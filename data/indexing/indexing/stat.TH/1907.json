[{"id": "1907.00004", "submitter": "Junmo Song", "authors": "Junmo Song and Jiwon Kang", "title": "Test for parameter change in the presence of outliers: the density power\n  divergence based approach", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study considers the problem of testing for a parameter change in the\npresence of outliers. For this, we propose a robust test using the objective\nfunction of minimum density power divergence estimator (MDPDE) by Basu et al.\n(Biometrika, 1998), and then derive its limiting null distribution. Our test\nprocedure can be naturally extended to any parametric model to which MDPDE can\nbe applied. To illustrate this, we apply our test procedure to GARCH models. We\ndemonstrate the validity and robustness of the proposed test through a\nsimulation study. In a real data application to the Hang Seng index, our test\nlocates some change-points that are not detected by the previous tests such as\nthe score test and the residual-based CUSUM test.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:39:56 GMT"}, {"version": "v2", "created": "Fri, 30 Aug 2019 02:25:11 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 14:59:11 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Song", "Junmo", ""], ["Kang", "Jiwon", ""]]}, {"id": "1907.00085", "submitter": "Guenther Walther", "authors": "Jiyao Kou and Guenther Walther", "title": "Large-scale inference with block structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The detection of weak and rare effects in large amounts of data arises in a\nnumber of modern data analysis problems. Known results show that in this\nsituation the potential of statistical inference is severely limited by the\nlarge-scale multiple testing that is inherent in these problems. Here we show\nthat fundamentally more powerful statistical inference is possible when there\nis some structure in the signal that can be exploited, e.g. if the signal is\nclustered in many small blocks, as is the case in some relevant applications.\nWe derive the detection boundary in such a situation where we allow both the\nnumber of blocks and the block length to grow polynomially with sample size. We\nderive these results both for the univariate and the multivariate settings as\nwell as for the problem of detecting clusters in a network. These results\nrecover as special cases the heterogeneous mixture detection problem [1] where\nthere is no structure in the signal, as well as scan problem [2] where the\nsignal comprises a single interval. We develop methodology that allows optimal\nadaptive detection in the general setting, thus exploiting the structure if it\nis present without incurring a relevant penalty in the case where there is no\nstructure. The advantage of this methodology can be considerable, as in the\ncase of no structure the means need to increase at the rate $\\sqrt{\\log n}$ to\nensure detection, while the presence of structure allows detection even if the\nmeans \\emph{decrease} at a polynomial rate.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 21:22:13 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Kou", "Jiyao", ""], ["Walther", "Guenther", ""]]}, {"id": "1907.00196", "submitter": "Alexander Bulinski", "authors": "Alexander Bulinski and Denis Dimitrov", "title": "Statistical estimation of the Kullback-Leibler divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wide conditions are provided to guarantee asymptotic unbiasedness and\nL^2-consistency of the introduced estimates of the Kullback-Leibler divergence\nfor probability measures in R^d having densities w.r.t. the Lebesgue measure.\nThese estimates are constructed by means of two independent collections of\ni.i.d. observations and involve the specified k-nearest neighbor statistics. In\nparticular, the established results are valid for estimates of the\nKullback-Leibler divergence between any two Gaussian measures in R^d with\nnondegenerate covariance matrices. As a byproduct we obtain new statements\nconcerning the Kozachenko-Leonenko estimators of the Shannon differential\nentropy.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 12:39:39 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Bulinski", "Alexander", ""], ["Dimitrov", "Denis", ""]]}, {"id": "1907.00288", "submitter": "Tomohiro Nishiyama", "authors": "Tomohiro Nishiyama", "title": "A New Lower Bound for Kullback-Leibler Divergence Based on\n  Hammersley-Chapman-Robbins Bound", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive a useful lower bound for the Kullback-Leibler\ndivergence (KL-divergence) based on the Hammersley-Chapman-Robbins bound\n(HCRB). The HCRB states that the variance of an estimator is bounded from below\nby the Chi-square divergence and the expectation value of the estimator. By\nusing the relation between the KL-divergence and the Chi-square divergence, we\nshow that the lower bound for the KL-divergence which only depends on the\nexpectation value and the variance of a function we choose. This lower bound\ncan also be derived from an information geometric approach. Furthermore, we\nshow that the equality holds for the Bernoulli distributions and show that the\ninequality converges to the Cram\\'{e}r-Rao bound when two distributions are\nvery close. We also describe application examples and examples of numerical\ncalculation.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 22:30:03 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 13:17:58 GMT"}, {"version": "v3", "created": "Sat, 2 Nov 2019 05:57:28 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Nishiyama", "Tomohiro", ""]]}, {"id": "1907.00296", "submitter": "Didong Li", "authors": "Didong Li and David B Dunson", "title": "Geodesic Distance Estimation with Spherelets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical and machine learning approaches rely on pairwise distances\nbetween data points. The choice of distance metric has a fundamental impact on\nperformance of these procedures, raising questions about how to appropriately\ncalculate distances. When data points are real-valued vectors, by far the most\ncommon choice is the Euclidean distance. This article is focused on the problem\nof how to better calculate distances taking into account the intrinsic geometry\nof the data, assuming data are concentrated near an unknown subspace or\nmanifold. The appropriate geometric distance corresponds to the length of the\nshortest path along the manifold, which is the geodesic distance. When the\nmanifold is unknown, it is challenging to accurately approximate the geodesic\ndistance. Current algorithms are either highly complex, and hence often\nimpractical to implement, or based on simple local linear approximations and\nshortest path algorithms that may have inadequate accuracy. We propose a simple\nand general alternative, which uses pieces of spheres, or spherelets, to\nlocally approximate the unknown subspace and thereby estimate the geodesic\ndistance through paths over spheres. Theory is developed showing lower error\nfor many manifolds, with applications in clustering, conditional density\nestimation and mean regression. The conclusion is supported through multiple\nsimulation examples and real data sets.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 23:37:02 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2020 06:08:35 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Li", "Didong", ""], ["Dunson", "David B", ""]]}, {"id": "1907.00399", "submitter": "Philip Dawid", "authors": "Philip Dawid, Macartan Humphreys and Monica Musio", "title": "Bounding Causes of Effects with Mediators", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose X and Y are binary exposure and outcome variables, and we have full\nknowledge of the distribution of Y, given application of X. From this we know\nthe average causal effect of X on Y. We are now interested in assessing, for a\ncase that was exposed and exhibited a positive outcome, whether it was the\nexposure that caused the outcome. The relevant \"probability of causation\", PC,\ntypically is not identified by the distribution of Y given X, but bounds can be\nplaced on it, and these bounds can be improved if we have further information\nabout the causal process. Here we consider cases where we know the\nprobabilistic structure for a sequence of complete mediators between X and Y.\nWe derive a general formula for calculating bounds on PC for any pattern of\ndata on the mediators (including the case with no data). We show that the\nlargest and smallest upper and lower bounds that can result from any complete\nmediation process can be obtained in processes with at most two steps. We also\nconsider homogeneous processes with many mediators. PC can sometimes be\nidentified as 0 with negative data, but it cannot be identified at 1 even with\npositive data on an infinite set of mediators. The results have implications\nfor learning about causation from knowledge of general processes and of data on\ncases.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jun 2019 15:41:36 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Dawid", "Philip", ""], ["Humphreys", "Macartan", ""], ["Musio", "Monica", ""]]}, {"id": "1907.00668", "submitter": "Sofiya Ostrovska", "authors": "Mohammed Khalleefah, Sofiya Ostrovska and Mehmet Turan", "title": "Power Lindley distribution and software metrics", "comments": "15 pages, 3 figures,4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Lindley distribution and its numerous generalizations are widely used in\nstatistical and engineering practice. Recently, a power transformation of\nLindley distribution, called the power Lindley distribution, has been\nintroduced by M. E. Ghitany et al., who initiated the investigation of its\nproperties and possible applications. In this article, new results on the power\nLindley distribution are presented. The focus of this work is on the\nmoment-(in)determinacy of the distribution for various values of the\nparameters. Afterwards, certain applications are provided to describe data sets\nof software metrics.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 11:27:49 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Khalleefah", "Mohammed", ""], ["Ostrovska", "Sofiya", ""], ["Turan", "Mehmet", ""]]}, {"id": "1907.00686", "submitter": "Nicolas Meyer", "authors": "Meyer Nicolas (LPSM (UMR\\_8001)), Olivier Wintenberger (LPSM\n  (UMR\\_8001))", "title": "Sparse regular variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regular variation provides a convenient theoretical framework to study large\nevents. In the multivariate setting, the dependence structure of the positive\nextremes is characterized by a measure - the spectral measure - defined on the\npositive orthant of the unit sphere. This measure gathers information on the\nlocalization of extreme events and has often a sparse support since severe\nevents do not simultaneously occur in all directions. However, it is defined\nthrough weak convergence which does not provide a natural way to capture this\nsparsity structure.In this paper, we introduce the notion of sparse regular\nvariation which allows to better learn the dependence structure of extreme\nevents. This concept is based on the Euclidean projection onto the simplex for\nwhich efficient algorithms are known. We prove that under mild assumptions\nsparse regular variation and regular variation are two equivalent notions and\nwe establish several results for sparsely regularly varying random vectors.\nFinally, we illustrate on numerical examples how this new concept allows one to\ndetect extremal directions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 12:11:57 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 09:55:28 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 18:43:38 GMT"}, {"version": "v4", "created": "Fri, 27 Nov 2020 15:52:29 GMT"}, {"version": "v5", "created": "Tue, 23 Feb 2021 08:33:40 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Nicolas", "Meyer", "", "LPSM"], ["Wintenberger", "Olivier", "", "LPSM"]]}, {"id": "1907.00723", "submitter": "Didi Lv", "authors": "Didi Lv and Xiaoqun Zhang", "title": "A greedy algorithm for sparse precision matrix approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precision matrix estimation is an important problem in statistical data\nanalysis. This paper introduces a fast sparse precision matrix estimation\nalgorithm, namely GISS$^{{\\rho}}$, which is originally introduced for\ncompressive sensing. The algorithm GISS$^{{\\rho}}$ is derived based on $l_1$\nminimization while with the computation advantage of greedy algorithms. We\nanalyze the asymptotic convergence rate of the proposed GISS$^{{\\rho}}$ for\nsparse precision matrix estimation and sparsity recovery properties with\nrespect to the stopping criteria. Finally, we numerically compare GISS$^{\\rho}$\nto other sparse recovery algorithms, such as ADMM and HTP in three settings of\nprecision matrix estimation. The numerical results show the advantages of the\nproposed algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 12:37:46 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Lv", "Didi", ""], ["Zhang", "Xiaoqun", ""]]}, {"id": "1907.00779", "submitter": "Roy Cerqueti", "authors": "Roy Cerqueti, Emilio De Santis", "title": "Constrained Monte Carlo Markov Chains on Graphs", "comments": "15 pages, no figures. arXiv admin note: substantial text overlap with\n  arXiv:1803.01738", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel theoretical Monte Carlo Markov chain procedure in\nthe framework of graphs. It specifically deals with the construction of a\nMarkov chain whose empirical distribution converges to a given reference one.\nThe Markov chain is constrained over an underlying graph, so that states are\nviewed as vertices and the transition between two states can have positive\nprobability only in presence of an edge connecting them. The analysis is\ncarried out on the basis of the relationship between the support of the target\ndistribution and the connectedness of the graph.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 13:56:12 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Cerqueti", "Roy", ""], ["De Santis", "Emilio", ""]]}, {"id": "1907.00790", "submitter": "Philipp di Dio", "authors": "Philipp J. di Dio", "title": "The multidimensional truncated Moment Problem: Shape and Gaussian\n  Mixture Reconstruction from Derivatives of Moments", "comments": "arXiv admin note: substantial text overlap with arXiv:1903.00598.\n  Author note: This is part II of arXiv:1903.00598. arXiv:1903.00598 was\n  extended and splitting it into two parts was necessary. Part I contains the\n  Caratheodory number from Hilbert functions parts (now arXiv:1903.00598v2).\n  Part II contains the (Gaussian) mixtures and shape reconstruction from\n  derivatives of moments parts", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA math.AG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the theory of derivatives of moments and (moment)\nfunctionals to represent moment functionals by Gaussian mixtures,\ncharacteristic functions of polytopes, and simple functions of polytopes. We\nstudy, among other measures, Gaussian mixtures, their reconstruction from\nmoments and especially the number of Gaussians needed to represent moment\nfunctionals. We find that there are moment functionals\n$L:\\mathbb{R}[x_1,\\dots,x_n]_{\\leq 2d}\\to\\mathbb{R}$ which can be represented\nby a sum of $\\binom{n+2d}{n} - n\\cdot \\binom{n+d}{n} + \\binom{n}{2}$ Gaussians\nbut not less. Hence, for any $d\\in\\mathbb{N}$ and $\\varepsilon>0$ we find an\n$n\\in\\mathbb{N}$ such that $L$ can be represented by a sum of\n$(1-\\varepsilon)\\cdot\\binom{n+2d}{n}$ Gaussians but not less. An upper bound is\n$\\binom{n+2d}{n}-1$.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jun 2019 10:21:01 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2019 17:00:11 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["di Dio", "Philipp J.", ""]]}, {"id": "1907.01110", "submitter": "Luke Prendergast", "authors": "Chandima N. P. G. Arachchige, Luke A. Prendergast, Robert G. Staudte", "title": "Robust analogs to the Coefficient of Variation", "comments": "30 pages, 2 figures Changed \"analogues\" to \"analogs\" in title to\n  match published version. Journal of Applied Statistics (2020)", "journal-ref": null, "doi": "10.1080/02664763.2020.1808599", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coefficient of variation (CV) is commonly used to measure relative\ndispersion. However, since it is based on the sample mean and standard\ndeviation, outliers can adversely affect the CV. Additionally, for skewed\ndistributions the mean and standard deviation do not have natural\ninterpretations and, consequently, neither does the CV. Here we investigate the\nextent to which quantile-based measures of relative dispersion can provide\nappropriate summary information as an alternative to the CV. In particular, we\ninvestigate two measures, the first being the interquartile range (in lieu of\nthe standard deviation), divided by the median (in lieu of the mean), and the\nsecond being the median absolute deviation (MAD), divided by the median, as\nrobust estimators of relative dispersion. In addition to comparing the\ninfluence functions of the competing estimators and their asymptotic biases and\nvariances, we compare interval estimators using simulation studies to assess\ncoverage.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 00:43:37 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 22:33:47 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2020 00:29:13 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Arachchige", "Chandima N. P. G.", ""], ["Prendergast", "Luke A.", ""], ["Staudte", "Robert G.", ""]]}, {"id": "1907.01145", "submitter": "Thomas Pumir", "authors": "Thomas Pumir, Amit Singer, Nicolas Boumal", "title": "The generalized orthogonal Procrustes problem in the high noise regime", "comments": null, "journal-ref": "Information and Inference: A Journal of the IMA, iaaa035, 2021", "doi": "10.1093/imaiai/iaaa035", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a cloud of points from numerous noisy\nobservations of that cloud after unknown rotations, and possibly reflections.\nThis is an instance of the general problem of estimation under group action,\noriginally inspired by applications in 3-D imaging and computer vision. We\nfocus on a regime where the noise level is larger than the magnitude of the\nsignal, so much so that the rotations cannot be estimated reliably. We propose\na simple and efficient procedure based on invariant polynomials (effectively:\nthe Gram matrices) to recover the signal, and we assess it against fundamental\nlimits of the problem that we derive. We show our approach adapts to the noise\nlevel and is statistically optimal (up to constants) for both the low and high\nnoise regimes. In studying the variance of our estimator, we encounter the\nquestion of the sensivity of a type of thin Cholesky factorization, for which\nwe provide an improved bound which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 03:25:26 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 17:10:50 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Pumir", "Thomas", ""], ["Singer", "Amit", ""], ["Boumal", "Nicolas", ""]]}, {"id": "1907.01223", "submitter": "Natalie Neumeyer", "authors": "Nick Kloodt, Natalie Neumeyer, Ingrid Van Keilegom", "title": "Specification testing in semi-parametric transformation models", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In transformation regression models the response is transformed before\nfitting a regression model to covariates and transformed response. We assume\nsuch a model where the errors are independent from the covariates and the\nregression function is modeled nonparametrically. We suggest a test for\ngoodness-of-fit of a parametric transformation class based on a distance\nbetween a nonparametric transformation estimator and the parametric class. We\npresent asymptotic theory under the null hypothesis of validity of the\nsemi-parametric model and under local alternatives. A bootstrap algorithm is\nsuggested in order to apply the test. We also consider relevant hypotheses to\ndistinguish between large and small distances of the parametric transformation\nclass to the `true' transformation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 08:13:13 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 12:10:54 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Kloodt", "Nick", ""], ["Neumeyer", "Natalie", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "1907.01306", "submitter": "Tobias Fissler", "authors": "Tobias Fissler, Jana Hlavinov\\'a, Birgit Rudloff", "title": "Elicitability and Identifiability of Systemic Risk Measures", "comments": "42 pages, 3 figures + supplementary material (6 pages, 2 figures)", "journal-ref": "Finance and Stochastics (2021), Volume 25, No. 1, 133-165", "doi": "10.1007/s00780-020-00446-z", "report-no": null, "categories": "math.ST q-fin.MF q-fin.RM q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification and scoring functions are statistical tools to assess the\ncalibration and the relative performance of risk measure estimates, e.g., in\nbacktesting. A risk measures is called identifiable (elicitable) it it admits a\nstrict identification function (strictly consistent scoring function). We\nconsider measures of systemic risk introduced in Feinstein, Rudloff and Weber\n(2017). Since these are set-valued, we work within the theoretical framework of\nFissler, Hlavinov\\'a and Rudloff (2019) for forecast evaluation of set-valued\nfunctionals. We construct oriented selective identification functions, which\ninduce a mixture representation of (strictly) consistent scoring functions.\nTheir applicability is demonstrated with a comprehensive simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 11:52:15 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 12:33:43 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Fissler", "Tobias", ""], ["Hlavinov\u00e1", "Jana", ""], ["Rudloff", "Birgit", ""]]}, {"id": "1907.01358", "submitter": "Giorgio Matteo Vitetta Prof.", "authors": "Giorgio M. Vitetta, Pasquale Di Viesti, Emilio Sirignano and Francesco\n  Montorsi", "title": "Multiple Bayesian Filtering as Message Passing", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.2965296", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, a general method for deriving filtering algorithms that\ninvolve a network of interconnected Bayesian filters is proposed. This method\nis based on the idea that the processing accomplished inside each of the\nBayesian filters and the interactions between them can be represented as\nmessage passing algorithms over a proper graphical model. The usefulness of our\nmethod is exemplified by developing new filtering techniques, based on the\ninterconnection of a particle filter and an extended Kalman filter, for\nconditionally linear Gaussian systems. Numerical results for two specific\ndynamic systems evidence that the devised algorithms can achieve a better\ncomplexity-accuracy tradeoff than marginalized particle filtering and multiple\nparticle filtering.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jul 2019 14:19:40 GMT"}, {"version": "v2", "created": "Wed, 24 Jul 2019 16:42:42 GMT"}, {"version": "v3", "created": "Thu, 25 Jul 2019 08:44:00 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Vitetta", "Giorgio M.", ""], ["Di Viesti", "Pasquale", ""], ["Sirignano", "Emilio", ""], ["Montorsi", "Francesco", ""]]}, {"id": "1907.01379", "submitter": "Lara Kassab", "authors": "Henry Adams, Mark Blumstein, Lara Kassab", "title": "Multidimensional Scaling on Metric Measure Spaces", "comments": "arXiv admin note: substantial text overlap with arXiv:1904.07763", "journal-ref": "Rocky Mountain Journal of Mathematics 50 (2020), 397-413", "doi": "10.1216/rmj.2020.50.397", "report-no": null, "categories": "math.ST math.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional scaling (MDS) is a popular technique for mapping a finite\nmetric space into a low-dimensional Euclidean space in a way that best\npreserves pairwise distances. We overview the theory of classical MDS, along\nwith its optimality properties and goodness of fit. Further, we present a\nnotion of MDS on infinite metric measure spaces that generalizes these\noptimality properties. As a consequence we can study the MDS embeddings of the\ngeodesic circle $S^1$ into $\\mathbb{R}^m$ for all $m$, and ask questions about\nthe MDS embeddings of the geodesic $n$-spheres $S^n$ into $\\mathbb{R}^m$.\nFinally, we address questions on convergence of MDS. For instance, if a\nsequence of metric measure spaces converges to a fixed metric measure space\n$X$, then in what sense do the MDS embeddings of these spaces converge to the\nMDS embedding of $X$?\n", "versions": [{"version": "v1", "created": "Sat, 29 Jun 2019 18:34:53 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Adams", "Henry", ""], ["Blumstein", "Mark", ""], ["Kassab", "Lara", ""]]}, {"id": "1907.01670", "submitter": "Linjun Zhang", "authors": "Xianli Zeng, Yingcun Xia, Linjun Zhang", "title": "Double Cross Validation for the Number of Factors in Approximate Factor\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the number of factors is essential to factor analysis. In this\npaper, we propose {an efficient cross validation (CV)} method to determine the\nnumber of factors in approximate factor models. The method applies CV twice,\nfirst along the directions of observations and then variables, and hence is\nreferred to hereafter as double cross-validation (DCV). Unlike most CV methods,\nwhich are prone to overfitting, the DCV is statistically consistent in\ndetermining the number of factors when both dimension of variables and sample\nsize are sufficiently large. Simulation studies show that DCV has outstanding\nperformance in comparison to existing methods in selecting the number of\nfactors, especially when the idiosyncratic error has heteroscedasticity, or\nheavy tail, or relatively large variance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 22:33:58 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Zeng", "Xianli", ""], ["Xia", "Yingcun", ""], ["Zhang", "Linjun", ""]]}, {"id": "1907.01672", "submitter": "John Storey", "authors": "Irineo Cabreros and John D. Storey", "title": "Causal models on probability spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe the interface between measure theoretic probability and causal\ninference by constructing causal models on probability spaces within the\npotential outcomes framework. We find that measure theory provides a precise\nand instructive language for causality and that consideration of the\nprobability spaces underlying causal models offers clarity into central\nconcepts of causal inference. By closely studying simple, instructive examples,\nwe demonstrate insights into causal effects, causal interactions, matching\nprocedures, and randomization. Additionally, we introduce a simple technique\nfor visualizing causal models on probability spaces that is useful both for\ngenerating examples and developing causal intuition. Finally, we provide an\naxiomatic framework for causality and make initial steps towards a formal\ntheory of general causal models.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jul 2019 22:58:36 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Cabreros", "Irineo", ""], ["Storey", "John D.", ""]]}, {"id": "1907.01715", "submitter": "Julia Gaudio", "authors": "David Gamarnik and Julia Gaudio", "title": "Sparse High-Dimensional Isotonic Regression", "comments": "28 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating an unknown coordinate-wise monotone\nfunction given noisy measurements, known as the isotonic regression problem.\nOften, only a small subset of the features affects the output. This motivates\nthe sparse isotonic regression setting, which we consider here. We provide an\nupper bound on the expected VC entropy of the space of sparse coordinate-wise\nmonotone functions, and identify the regime of statistical consistency of our\nestimator. We also propose a linear program to recover the active coordinates,\nand provide theoretical recovery guarantees. We close with experiments on\ncancer classification, and show that our method significantly outperforms\nstandard methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 02:52:57 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Gamarnik", "David", ""], ["Gaudio", "Julia", ""]]}, {"id": "1907.01758", "submitter": "Xiequan Fan", "authors": "J\\'er\\^ome Dedecker, Paul Doukhan, Xiequan Fan", "title": "Deviation inequalities for separately Lipschitz functionals of\n  composition of random functions", "comments": "18 pages", "journal-ref": "J. Math. Anal. Appl. 479(2019): 1549-1568", "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of non-homogeneous Markov chains, that contains many\nnatural examples. Next, using martingale methods, we establish some deviation\nand moment inequalities for separately Lipschitz functions of such a chain,\nunder moment conditions on some dominating random variables.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 06:38:26 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Dedecker", "J\u00e9r\u00f4me", ""], ["Doukhan", "Paul", ""], ["Fan", "Xiequan", ""]]}, {"id": "1907.01781", "submitter": "Lucie Bernard", "authors": "Lucie Bernard (IDP), Philippe Leduc (ST-TOURS)", "title": "Estimating a probability of failure with the convex order in computer\n  experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the estimation of a failure probability of an\nindustrial product. To be more specific, it is defined as the probability that\nthe output of a physical model, with random input variables, exceeds a\nthreshold. The model corresponds with an expensive to evaluate black-box\nfunction, so that classical Monte Carlo simulation methods cannot be applied.\nBayesian principles of the Kriging method are then used to design an estimator\nof the failure probability. From a numerical point of view, the practical use\nof this estimator is restricted. An alternative estimator is proposed, which is\nequivalent in term of bias. The main result of this paper concerns the\nexistence of a convex order inequality between these two estimators. This\ninequality allows to compare their efficiency and to quantify the uncertainty\non the results that these estimators provide. A sequential procedure for the\nconstruction of a design of computer experiments, based on the principle of the\nStepwise Uncertainty Reduction strategies, also results of the convex order\ninequality. The interest of this approach is highlighted through the study of a\nreal case from the company STMicroelectronics.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 07:50:38 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Bernard", "Lucie", "", "IDP"], ["Leduc", "Philippe", "", "ST-TOURS"]]}, {"id": "1907.01843", "submitter": "Sarat Babu Moka", "authors": "Sarat Moka, Dirk P. Kroese and Sandeep Juneja", "title": "Unbiased Estimation of the Reciprocal Mean for Non-negative Random\n  Variables", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many simulation problems require the estimation of a ratio of two\nexpectations. In recent years Monte Carlo estimators have been proposed that\ncan estimate such ratios without bias. We investigate the theoretical\nproperties of such estimators for the estimation of $\\beta = 1/\\mathbb{E}\\, Z$,\nwhere $Z \\geq 0$. The estimator, $\\widehat \\beta(w)$, is of the form $w/f_w(N)\n\\prod_{i=1}^N (1 - w\\, Z_i)$, where $w < 2\\beta$ and $N$ is any random variable\nwith probability mass function $f_w$ on the positive integers. For a fixed $w$,\nthe optimal choice for $f_w$ is well understood, but less so the choice of $w$.\nWe study the properties of $\\widehat \\beta(w)$ as a function of~$w$ and show\nthat its expected time variance product decreases as $w$ decreases, even though\nthe cost of constructing the estimator increases with $w$. We also show that\nthe estimator is asymptotically equivalent to the maximum likelihood (biased)\nratio estimator and establish practical confidence intervals.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 10:41:08 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Moka", "Sarat", ""], ["Kroese", "Dirk P.", ""], ["Juneja", "Sandeep", ""]]}, {"id": "1907.01945", "submitter": "Marco Geraci", "authors": "Marco Geraci, Alessio Farcomeni", "title": "Mid-quantile regression for discrete responses", "comments": "30 pages, 7 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop quantile regression methods for discrete responses by extending\nParzen's definition of marginal mid-quantiles. As opposed to existing\napproaches, which are based on either jittering or latent constructs, we use\ninterpolation and define the conditional mid-quantile function as the inverse\nof the conditional mid-distribution function. We propose a two-step estimator\nwhereby, in the first step, conditional mid-probabilities are obtained\nnonparametrically and, in the second step, regression coefficients are\nestimated by solving an implicit equation. When constraining the quantile index\nto a data-driven admissible range, the second-step estimating equation has a\nleast-squares type, closed-form solution. The proposed estimator is shown to be\nstrongly consistent and asymptotically normal. A simulation study and real data\napplications are presented. Our methods can be applied to a large variety of\ndiscrete responses, including binary, ordinal, and count variables.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 13:48:51 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Geraci", "Marco", ""], ["Farcomeni", "Alessio", ""]]}, {"id": "1907.02006", "submitter": "Johannes Wiesel", "authors": "Samuel N. Cohen, Martin N. A. Tegn\\'er and Johannes Wiesel", "title": "Bounding quantiles of Wasserstein distance between true and empirical\n  measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the empirical measure, $\\hat{\\mathbb{P}}_N$, associated to $N$\ni.i.d. samples of a given probability distribution $\\mathbb{P}$ on the unit\ninterval. For fixed $\\mathbb{P}$ the Wasserstein distance between\n$\\hat{\\mathbb{P}}_N$ and $\\mathbb{P}$ is a random variable on the sample space\n$[0,1]^N$. Our main result is that its normalised quantiles are asymptotically\nmaximised when $\\mathbb{P}$ is a convex combination between the uniform\ndistribution supported on the two points $\\{0,1\\}$ and the uniform distribution\non the unit interval $[0,1]$. This allows us to obtain explicit asymptotic\nconfidence regions for the underlying measure $\\mathbb{P}$.\n  We also suggest extensions to higher dimensions with numerical evidence.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 16:01:16 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Cohen", "Samuel N.", ""], ["Tegn\u00e9r", "Martin N. A.", ""], ["Wiesel", "Johannes", ""]]}, {"id": "1907.02033", "submitter": "Niklas Hohmann", "authors": "Niklas Hohmann", "title": "Large Deviations of the Estimated Cumulative Hazard Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Survivorship analysis allows to statistically analyze situations that can be\nmodeled as waiting times to an event. These waiting times are characterized by\nthe cumulative hazard rate, which can be estimated by the Nelson-Aalen\nestimator or diverse confidence estimators based on asymptotic statistics. To\nbetter understand the small sample properties of these estimators, the speed of\nconvergence of the estimate to the exact value is examined. This is done by\nderiving large deviation principles and their rate functions for the estimators\nand examining their properties. It is shown that these rate functions are\nasymmetric, leading to a tendency of the estimated cumulative hazard rate to\noverestimate the true cumulative hazard rate. This tendency is strongest in the\ncases of (1) small sample sizes and (2) low tail probabilities. Taking this\ntendency into account can improve risk assessments of rare events and of cases\nwhere only little data is available.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 16:54:34 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Hohmann", "Niklas", ""]]}, {"id": "1907.02095", "submitter": "Galen Reeves", "authors": "Galen Reeves and Henry Pfister", "title": "Understanding Phase Transitions via Mutual Information and MMSE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to understand and solve high-dimensional inference problems is\nessential for modern data science. This article examines high-dimensional\ninference problems through the lens of information theory and focuses on the\nstandard linear model as a canonical example that is both rich enough to be\npractically useful and simple enough to be studied rigorously. In particular,\nthis model can exhibit phase transitions where an arbitrarily small change in\nthe model parameters can induce large changes in the quality of estimates. For\nthis model, the performance of optimal inference can be studied using the\nreplica method from statistical physics but, until recently, it was not known\nif the resulting formulas were actually correct. In this chapter, we present a\ntutorial description of the standard linear model and its connection to\ninformation theory. We also describe the replica prediction for this model and\noutline the authors' recent proof that it is exact.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:33:42 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Reeves", "Galen", ""], ["Pfister", "Henry", ""]]}, {"id": "1907.02097", "submitter": "Yanhong Wu", "authors": "Yanhong Wu", "title": "Estimation of common change point and isolation of changed panels after\n  sequential detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quick detection of common changes is critical in sequential monitoring of\nmulti-stream data where a common change is referred as a change that only\noccurs in a portion of panels. After a common change is detected by using a\ncombined CUSUM-SR procedure, we first study the joint distribution for values\nof the CUSUM process and the estimated delay detection time for the unchanged\npanels. The BH method by using the asymptotic exponential property for the\nCUSUM process is developed to isolate the changed panels with the control on\nFDR. The common change point is then estimated based on the isolated changed\npanels. Simulation results show that the proposed method can also control the\nFNR by properly selecting FDR.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jul 2019 18:46:43 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Wu", "Yanhong", ""]]}, {"id": "1907.02306", "submitter": "Vincent Margot", "authors": "Vincent Margot (LPSM UMR 8001), Jean-Patrick Baudry (LPSM UMR 8001),\n  Fr\\'ed\\'eric Guilloux (LPSM UMR 8001), Olivier Wintenberger (LPSM UMR 8001)", "title": "Consistent Regression using Data-Dependent Coverings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a novel method to generate interpretable\nregression function estimators. The idea is based on called data-dependent\ncoverings. The aim is to extract from the data a covering of the feature space\ninstead of a partition. The estimator predicts the empirical conditional\nexpectation over the cells of the partitions generated from the coverings.\nThus, such estimator has the same form as those issued from data-dependent\npartitioning algorithms. We give sufficient conditions to ensure the\nconsistency, avoiding the sufficient condition of shrinkage of the cells that\nappears in the former literature. Doing so, we reduce the number of covering\nelements. We show that such coverings are interpretable and each element of the\ncovering is tagged as significant or insignificant. The proof of the\nconsistency is based on a control of the error of the empirical estimation of\nconditional expectations which is interesting on its own.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 09:54:44 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 12:48:27 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 13:36:45 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2020 12:52:06 GMT"}, {"version": "v5", "created": "Tue, 26 Jan 2021 08:43:34 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Margot", "Vincent", "", "LPSM UMR 8001"], ["Baudry", "Jean-Patrick", "", "LPSM UMR 8001"], ["Guilloux", "Fr\u00e9d\u00e9ric", "", "LPSM UMR 8001"], ["Wintenberger", "Olivier", "", "LPSM UMR 8001"]]}, {"id": "1907.02333", "submitter": "Brett Kolesnik", "authors": "Persi Diaconis, Brett Kolesnik", "title": "Randomized sequential importance sampling for estimating the number of\n  perfect matchings in bipartite graphs", "comments": "v2: minor improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study randomized sequential importance sampling algorithms\nfor estimating the number of perfect matchings in bipartite graphs. In\nanalyzing their performance, we establish various non-standard central limit\ntheorems. We expect our methods to be useful for other applied problems.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 11:40:59 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 09:51:24 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Diaconis", "Persi", ""], ["Kolesnik", "Brett", ""]]}, {"id": "1907.02347", "submitter": "Nicole B\\\"auerle", "authors": "Nicole B\\\"auerle and Ulrich Rieder", "title": "Markov Decision Processes under Ambiguity", "comments": null, "journal-ref": "Banach Center Publications 122 (2020), 25-39", "doi": "10.4064/bc122-2", "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistical Markov Decision Processes where the decision maker is\nrisk averse against model ambiguity. The latter is given by an unknown\nparameter which influences the transition law and the cost functions. Risk\naversion is either measured by the entropic risk measure or by the Average\nValue at Risk. We show how to solve these kind of problems using a general\nminimax theorem. Under some continuity and compactness assumptions we prove the\nexistence of an optimal (deterministic) policy and discuss its computation. We\nillustrate our results using an example from statistical decision theory.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 12:02:02 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["B\u00e4uerle", "Nicole", ""], ["Rieder", "Ulrich", ""]]}, {"id": "1907.02373", "submitter": "Janet Godolphin", "authors": "Janet Godolphin", "title": "Construction of Blocked Factorial Designs to Estimate Main Effects and\n  Selected Two-Factor Interactions", "comments": "24 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-level factorial designs are widely used in industrial experiments. For\nprocesses involving \\(n\\) factors, the construction of designs comprising\n\\(2^n\\) and \\(2^{n-p}\\) factorials, arranged in blocks of size \\(2^q\\) is\ninvestigated. The aim is to estimate all main effects and a selected subset of\ntwo-factor interactions. Designs constructed according to minimum aberration\ncriteria are shown to not necessarily be the most appropriate designs in this\nsituation. A design construction approach is proposed which exploits known\nresults on proper vertex colourings in graph theory. Examples are provided to\nillustrate the results and construction strategies.\n  Particular consideration is given to the special case of designs with blocks\nof size four and tables of designs are given for this block size.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 12:44:42 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Godolphin", "Janet", ""]]}, {"id": "1907.02435", "submitter": "Leonard Henckel", "authors": "Leonard Henckel, Emilija Perkovi\\'c, Marloes H. Maathuis", "title": "Graphical Criteria for Efficient Total Effect Estimation via Adjustment\n  in Causal Linear Models", "comments": "63 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate adjustment is a commonly used method for total causal effect\nestimation. In recent years, graphical criteria have been developed to identify\nall valid adjustment sets, that is, all covariate sets that can be used for\nthis purpose. Different valid adjustment sets typically provide total effect\nestimates of varying accuracies. Restricting ourselves to causal linear models,\nwe introduce a graphical criterion to compare the asymptotic variances provided\nby certain valid adjustment sets. We employ this result to develop two further\ngraphical tools. First, we introduce a simple variance reducing pruning\nprocedure for any given valid adjustment set. Second, we give a graphical\ncharacterization of a valid adjustment set that provides the optimal asymptotic\nvariance among all valid adjustment sets. Our results depend only on the\ngraphical structure and not on the specific error variances or edge\ncoefficients of the underlying causal linear model. They can be applied to\ndirected acyclic graphs (DAGs), completed partially directed acyclic graphs\n(CPDAGs) and maximally oriented partially directed acyclic graphs (maximal\nPDAGs). We present simulations and a real data example to support our results\nand show their practical applicability.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 14:53:12 GMT"}, {"version": "v2", "created": "Sat, 12 Dec 2020 22:06:03 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Henckel", "Leonard", ""], ["Perkovi\u0107", "Emilija", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "1907.02473", "submitter": "Richard Lockhart", "authors": "Richard A Lockhart", "title": "Bayes factors with (overly) informative priors", "comments": "1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Priors in which a large number of parameters are specified to be independent\nare dangerous; they make it hard to learn from data. I present a couple of\nexamples from the literature and work through a bit of large sample theory to\nshow what happens.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 15:56:43 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 04:44:04 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Lockhart", "Richard A", ""]]}, {"id": "1907.02496", "submitter": "Galen Reeves", "authors": "Galen Reeves and Vaishakhi Mayya and Alexander Volfovsky", "title": "The Geometry of Community Detection via the MMSE Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information-theoretic limits of community detection have been studied\nextensively for network models with high levels of symmetry or homogeneity. The\ncontribution of this paper is to study a broader class of network models that\nallow for variability in the sizes and behaviors of the different communities,\nand thus better reflect the behaviors observed in real-world networks. Our\nresults show that the ability to detect communities can be described succinctly\nin terms of a matrix of effective signal-to-noise ratios that provides a\ngeometrical representation of the relationships between the different\ncommunities. This characterization follows from a matrix version of the I-MMSE\nrelationship and generalizes the concept of an effective scalar signal-to-noise\nratio introduced in previous work. We provide explicit formulas for the\nasymptotic per-node mutual information and upper bounds on the minimum\nmean-squared error. The theoretical results are supported by numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jul 2019 17:16:28 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Reeves", "Galen", ""], ["Mayya", "Vaishakhi", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "1907.02652", "submitter": "Rui She", "authors": "Rui She, Shanyun Liu, Shuo Wan, Ke Xiong, and Pingyi Fan", "title": "Importance of Small Probability Events in Big Data: Information\n  Measures, Applications, and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications (e.g., anomaly detection and security systems) of smart\ncities, rare events dominate the importance of the total information of big\ndata collected by Internet of Things (IoTs). That is, it is pretty crucial to\nexplore the valuable information associated with the rare events involved in\nminority subsets of the voluminous amounts of data. To do so, how to\neffectively measure the information with importance of the small probability\nevents from the perspective of information theory is a fundamental question.\nThis paper first makes a survey of some theories and models with respect to\nimportance measures and investigates the relationship between subjective or\nsemantic importance and rare events in big data. Moreover, some applications\nfor message processing and data analysis are discussed in the viewpoint of\ninformation measures. In addition, based on rare events detection, some open\nchallenges related to information measures, such as smart cities, autonomous\ndriving, and anomaly detection in IoTs, are introduced which can be considered\nas future research directions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 02:03:57 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 02:41:21 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["She", "Rui", ""], ["Liu", "Shanyun", ""], ["Wan", "Shuo", ""], ["Xiong", "Ke", ""], ["Fan", "Pingyi", ""]]}, {"id": "1907.02707", "submitter": "Anatoli Juditsky", "authors": "Anatoli Juditsky (LJK), Alexander Nazin, Arkadi Nemirovsky, Alexandre\n  Tsybakov", "title": "Algorithms of Robust Stochastic Optimization Based on Mirror Descent\n  Method", "comments": "Automation and Remote Control / Avtomatika i Telemekhanika, MAIK\n  Nauka/Interperiodica, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to construction of robust non-Euclidean iterative\nalgorithms for convex composite stochastic optimization based on truncation of\nstochastic gradients. For such algorithms, we establish sub-Gaussian confidence\nbounds under weak assumptions about the tails of the noise distribution in\nconvex and strongly convex settings. Robust estimates of the accuracy of\ngeneral stochastic algorithms are also proposed.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 07:33:05 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Juditsky", "Anatoli", "", "LJK"], ["Nazin", "Alexander", ""], ["Nemirovsky", "Arkadi", ""], ["Tsybakov", "Alexandre", ""]]}, {"id": "1907.02708", "submitter": "Fritjof Freise", "authors": "Fritjof Freise, Norbert Gaffke, Rainer Schwabe", "title": "The adaptive Wynn-algorithm in generalized linear models with univariate\n  response", "comments": null, "journal-ref": "Annals of Statistics 2021, Vol. 49, No. 2, 702-722", "doi": "10.1214/20-AOS1974", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a nonlinear regression model the information matrices of designs depend\non the parameter of the model. The adaptive Wynn-algorithm for D-optimal design\nestimates the parameter at each step on the basis of the employed design points\nand observed responses so far, and selects the next design point as in the\nclassical Wynn-algorithm for D-optimal design. The name `Wynn-algorithm' is in\nhonor of Henry P. Wynn who established the latter `classical' algorithm in his\n1970 paper. The asymptotics of the sequences of designs and maximum likelihood\nestimates generated by the adaptive algorithm is studied for an important class\nof nonlinear regression models: generalized linear models whose (univariate)\nresponse variables follow a distribution from a one-parameter exponential\nfamily. Under the assumptions of compactness of the experimental region and of\nthe parameter space together with some natural continuity assumptions it is\nshown that the adaptive ML-estimators are strongly consistent and the design\nsequence is asymptotically locally D-optimal at the true parameter point. If\nthe true parameter point is an interior point of the parameter space then under\nsome smoothness assumptions the asymptotic normality of the adaptive\nML-estimators is obtained.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 07:36:28 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Freise", "Fritjof", ""], ["Gaffke", "Norbert", ""], ["Schwabe", "Rainer", ""]]}, {"id": "1907.02809", "submitter": "Matthieu Lerasle", "authors": "Antoine Havet, Matthieu Lerasle, Eric Moulines and Elodie Vernet", "title": "A quantitative Mc Diarmid's inequality for geometrically ergodic Markov\n  chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We state and prove a quantitative version of the bounded difference\ninequality for geometrically ergodic Markov chains.\n  Our proof uses the same martingale decomposition as \\cite{MR3407208} but,\ncompared to this paper, the exact coupling argument is modified to fill a gap\nbetween the strongly aperiodic case and the general aperiodic case.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 13:20:07 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Havet", "Antoine", ""], ["Lerasle", "Matthieu", ""], ["Moulines", "Eric", ""], ["Vernet", "Elodie", ""]]}, {"id": "1907.02912", "submitter": "Kayvan Sadeghi", "authors": "Kayvan Sadeghi", "title": "On Finite Exchangeability and Conditional Independence", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the independence structure of finitely exchangeable distributions\nover random vectors and random networks. In particular, we provide necessary\nand sufficient conditions for an exchangeable vector so that its elements are\ncompletely independent or completely dependent. We also provide a sufficient\ncondition for an exchangeable vector so that its elements are marginally\nindependent. We then generalize these results and conditions for exchangeable\nrandom networks. In this case, it is demonstrated that the situation is more\ncomplex. We show that the independence structure of exchangeable random\nnetworks lies in one of six regimes that are two-fold dual to one another,\nrepresented by undirected and bidirected independence graphs in graphical model\nsense with graphs that are complement of each other. In addition, under certain\nadditional assumptions, we provide necessary and sufficient conditions for the\nexchangeable network distributions to be faithful to each of these graphs.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 16:17:47 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 01:49:25 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 15:40:28 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Sadeghi", "Kayvan", ""]]}, {"id": "1907.03025", "submitter": "Wojciech Rejchel", "authors": "Piotr Pokarowski, Wojciech Rejchel, Agnieszka Soltys, Michal Frej and\n  Jan Mielniczuk", "title": "Improving Lasso for model selection and prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the Thresholded Lasso (TL), SCAD or MCP correct intrinsic\nestimation bias of the Lasso. In this paper we propose an alternative method of\nimproving the Lasso for predictive models with general convex loss functions\nwhich encompass normal linear models, logistic regression, quantile regression\nor support vector machines. For a given penalty we order the absolute values of\nthe Lasso non-zero coefficients and then select the final model from a small\nnested family by the Generalized Information Criterion. We derive exponential\nupper bounds on the selection error of the method. These results confirm that,\nat least for normal linear models, our algorithm seems to be the benchmark for\nthe theory of model selection as it is constructive, computationally efficient\nand leads to consistent model selection under weak assumptions. Constructivity\nof the algorithm means that, in contrast to the TL, SCAD or MCP, consistent\nselection does not rely on the unknown parameters as the cone invertibility\nfactor. Instead, our algorithm only needs the sample size, the number of\npredictors and an upper bound on the noise parameter. We show in numerical\nexperiments on synthetic and real-world data sets that an implementation of our\nalgorithm is more accurate than implementations of studied concave\nregularizations. Our procedure is contained in the R package \"DMRnet\" and\navailable on the CRAN repository.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 21:05:39 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 16:07:57 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Pokarowski", "Piotr", ""], ["Rejchel", "Wojciech", ""], ["Soltys", "Agnieszka", ""], ["Frej", "Michal", ""], ["Mielniczuk", "Jan", ""]]}, {"id": "1907.03028", "submitter": "Jamie Haddock", "authors": "Jamie Haddock, Denali Molitor, Deanna Needell, Sneha Sambandam, Joy\n  Song, Simon Sun", "title": "On Inferences from Completed Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion has become an extremely important technique as data\nscientists are routinely faced with large, incomplete datasets on which they\nwish to perform statistical inferences. We investigate how error introduced via\nmatrix completion affects statistical inference. Furthermore, we prove recovery\nerror bounds which depend upon the matrix recovery error for several common\nstatistical inferences. We consider matrix recovery via nuclear norm\nminimization and a variant, $\\ell_1$-regularized nuclear norm minimization for\ndata with a structured sampling pattern. Finally, we run a series of numerical\nexperiments on synthetic data and real patient surveys from MyLymeData, which\nillustrate the relationship between inference recovery error and matrix\nrecovery error. These results indicate that exact matrix recovery is often not\nnecessary to achieve small inference recovery error.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 21:30:13 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Haddock", "Jamie", ""], ["Molitor", "Denali", ""], ["Needell", "Deanna", ""], ["Sambandam", "Sneha", ""], ["Song", "Joy", ""], ["Sun", "Simon", ""]]}, {"id": "1907.03086", "submitter": "Neil Chada", "authors": "Neil K. Chada, Sari Lasanen, Lassi Roininen", "title": "Posterior Convergence Analysis of $\\alpha$-Stable Sheets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the theoretical understanding of $\\alpha$-stable\nsheets $U$ on $\\mathbb{R}^d$. Our motivation for this is in the context of\nBayesian inverse problems, where we consider these processes as prior\ndistributions, aiming to quantify information of the posterior. We derive\nconvergence results referring to finite-dimensional approximations of\ninfinite-dimensional random variables. In doing so we use a number of variants\nwhich these sheets can take, such as a stochastic integral representation, but\nalso random series expansions through Poisson processes. Our proofs will rely\non the fact of whether $U$ can omit $L^p$-sample paths. To aid with the\nconvergence of the finite approximations we provide a natural discretization to\nrepresent the prior. Aside from convergence of these stable sheets we address\nwhether both well-posedness and well-definiteness of the inverse problem can be\nattained.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 06:38:36 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 13:58:07 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 13:07:53 GMT"}, {"version": "v4", "created": "Thu, 18 Jul 2019 11:41:32 GMT"}, {"version": "v5", "created": "Mon, 12 Aug 2019 04:29:49 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Chada", "Neil K.", ""], ["Lasanen", "Sari", ""], ["Roininen", "Lassi", ""]]}, {"id": "1907.03087", "submitter": "Ankit Pensia", "authors": "Ankit Pensia, Varun Jog, Po-Ling Loh", "title": "Estimating location parameters in entangled single-sample distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the common mean of independently\nsampled data, where samples are drawn in a possibly non-identical manner from\nsymmetric, unimodal distributions with a common mean. This generalizes the\nsetting of Gaussian mixture modeling, since the number of distinct mixture\ncomponents may diverge with the number of observations. We propose an estimator\nthat adapts to the level of heterogeneity in the data, achieving\nnear-optimality in both the i.i.d. setting and some heterogeneous settings,\nwhere the fraction of ``low-noise'' points is as small as $\\frac{\\log n}{n}$.\nOur estimator is a hybrid of the modal interval, shorth, and median estimators\nfrom classical statistics; however, the key technical contributions rely on\nnovel empirical process theory results that we derive for independent but\nnon-i.i.d. data. In the multivariate setting, we generalize our theory to mean\nestimation for mixtures of radially symmetric distributions, and derive minimax\nlower bounds on the expected error of any estimator that is agnostic to the\nscales of individual data points. Finally, we describe an extension of our\nestimators applicable to linear regression. In the multivariate mean estimation\nand regression settings, we present computationally feasible versions of our\nestimators that run in time polynomial in the number of data points.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 06:39:25 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Pensia", "Ankit", ""], ["Jog", "Varun", ""], ["Loh", "Po-Ling", ""]]}, {"id": "1907.03170", "submitter": "Karl Oskar Ekvall", "authors": "Karl Oskar Ekvall and Galin L. Jones", "title": "Convergence Analysis of a Collapsed Gibbs Sampler for Bayesian Vector\n  Autoregressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the convergence properties of a collapsed Gibbs sampler for Bayesian\nvector autoregressions with predictors, or exogenous variables. The Markov\nchain generated by our algorithm is shown to be geometrically ergodic\nregardless of whether the number of observations in the underlying vector\nautoregression is small or large in comparison to the order and dimension of\nit. In a convergence complexity analysis, we also give conditions for when the\ngeometric ergodicity is asymptotically stable as the number of observations\ntends to infinity. Specifically, the geometric convergence rate is shown to be\nbounded away from unity asymptotically, either almost surely or with\nprobability tending to one, depending on what is assumed about the data\ngenerating process. This result is one of the first of its kind for practically\nrelevant Markov chain Monte Carlo algorithms. Our convergence results hold\nunder close to arbitrary model misspecification.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 19:08:11 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 08:16:11 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 07:01:27 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Ekvall", "Karl Oskar", ""], ["Jones", "Galin L.", ""]]}, {"id": "1907.03182", "submitter": "Maryam Aliakbarpour", "authors": "Maryam Aliakbarpour, Themis Gouleakis, John Peebles, Ronitt Rubinfeld,\n  Anak Yodpinyanee", "title": "Towards Testing Monotonicity of Distributions Over General Posets", "comments": "Appeared in COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider the sample complexity required for testing the\nmonotonicity of distributions over partial orders. A distribution $p$ over a\nposet is monotone if, for any pair of domain elements $x$ and $y$ such that $x\n\\preceq y$, $p(x) \\leq p(y)$. To understand the sample complexity of this\nproblem, we introduce a new property called bigness over a finite domain, where\nthe distribution is $T$-big if the minimum probability for any domain element\nis at least $T$. We establish a lower bound of $\\Omega(n/\\log n)$ for testing\nbigness of distributions on domains of size $n$. We then build on these lower\nbounds to give $\\Omega(n/\\log{n})$ lower bounds for testing monotonicity over a\nmatching poset of size $n$ and significantly improved lower bounds over the\nhypercube poset. We give sublinear sample complexity bounds for testing bigness\nand for testing monotonicity over the matching poset.\n  We then give a number of tools for analyzing upper bounds on the sample\ncomplexity of\n  the monotonicity testing problem.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 20:45:01 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Aliakbarpour", "Maryam", ""], ["Gouleakis", "Themis", ""], ["Peebles", "John", ""], ["Rubinfeld", "Ronitt", ""], ["Yodpinyanee", "Anak", ""]]}, {"id": "1907.03190", "submitter": "Maryam Aliakbarpour", "authors": "Maryam Aliakbarpour, Ravi Kumar, Ronitt Rubinfeld", "title": "Testing Mixtures of Discrete Distributions", "comments": "Appeared in COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been significant study on the sample complexity of testing\nproperties of distributions over large domains. For many properties, it is\nknown that the sample complexity can be substantially smaller than the domain\nsize. For example, over a domain of size $n$, distinguishing the uniform\ndistribution from distributions that are far from uniform in $\\ell_1$-distance\nuses only $O(\\sqrt{n})$ samples.\n  However, the picture is very different in the presence of arbitrary noise,\neven when the amount of noise is quite small. In this case, one must\ndistinguish if samples are coming from a distribution that is $\\epsilon$-close\nto uniform from the case where the distribution is $(1-\\epsilon)$-far from\nuniform. The latter task requires nearly linear in $n$ samples [Valiant 2008,\nValian and Valiant 2011].\n  In this work, we present a noise model that on one hand is more tractable for\nthe testing problem, and on the other hand represents a rich class of noise\nfamilies. In our model, the noisy distribution is a mixture of the original\ndistribution and noise, where the latter is known to the tester either\nexplicitly or via sample access; the form of the noise is also known a priori.\nFocusing on the identity and closeness testing problems leads to the following\nmixture testing question: Given samples of distributions $p, q_1,q_2$, can we\ntest if $p$ is a mixture of $q_1$ and $q_2$? We consider this general question\nin various scenarios that differ in terms of how the tester can access the\ndistributions, and show that indeed this problem is more tractable. Our results\nshow that the sample complexity of our testers are exactly the same as for the\nclassical non-mixture case.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jul 2019 21:24:54 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Aliakbarpour", "Maryam", ""], ["Kumar", "Ravi", ""], ["Rubinfeld", "Ronitt", ""]]}, {"id": "1907.03625", "submitter": "Harouna Sangar\\'e", "authors": "Harouna Sangar\\'e, Gane Samb Lo and Mamadou Cherif Moctar Traor\\'e", "title": "Arbitrary functional Glivenko-Cantelli classes and applications to\n  different types of dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a general strong law of large number proved by Sangar\\'e and Lo (2015)\nand the entropy numbers, we provide functional Glivenko-Cantelli (GC) classes\nfor arbitrary stationary real-valued random variables (rrv's). Next, the\ngeneral results are particularized for different types of dependence\n(association, $\\phi$-mixing, in particular) and compared with available results\nin the literature.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jul 2019 15:09:54 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 17:51:04 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 20:44:46 GMT"}, {"version": "v4", "created": "Sun, 11 Oct 2020 14:19:27 GMT"}, {"version": "v5", "created": "Thu, 15 Oct 2020 19:29:37 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Sangar\u00e9", "Harouna", ""], ["Lo", "Gane Samb", ""], ["Traor\u00e9", "Mamadou Cherif Moctar", ""]]}, {"id": "1907.03783", "submitter": "Guojun Zhang", "authors": "Guojun Zhang, Pascal Poupart and George Trimponias", "title": "Comparing EM with GD in Mixture Models of Two Components", "comments": "UAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expectation-maximization (EM) algorithm has been widely used in\nminimizing the negative log likelihood (also known as cross entropy) of mixture\nmodels. However, little is understood about the goodness of the fixed points it\nconverges to. In this paper, we study the regions where one component is\nmissing in two-component mixture models, which we call one-cluster regions. We\nanalyze the propensity of such regions to trap EM and gradient descent (GD) for\nmixtures of two Gaussians and mixtures of two Bernoullis. In the case of\nGaussian mixtures, EM escapes one-cluster regions exponentially fast, while GD\nescapes them linearly fast. In the case of mixtures of Bernoullis, we find that\nthere exist one-cluster regions that are stable for GD and therefore trap GD,\nbut those regions are unstable for EM, allowing EM to escape. Those regions are\nlocal minima that appear universally in experiments and can be arbitrarily bad.\nThis work implies that EM is less likely than GD to converge to certain bad\nlocal optima in mixture models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:00:32 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2019 03:28:45 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 15:13:38 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Zhang", "Guojun", ""], ["Poupart", "Pascal", ""], ["Trimponias", "George", ""]]}, {"id": "1907.03792", "submitter": "Marc Lelarge", "authors": "Marc Lelarge and Leo Miolane", "title": "Asymptotic Bayes risk for Gaussian mixture in a semi-supervised setting", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning (SSL) uses unlabeled data for training and has been\nshown to greatly improve performance when compared to a supervised approach on\nthe labeled data available. This claim depends both on the amount of labeled\ndata available and on the algorithm used.\n  In this paper, we compute analytically the gap between the best\nfully-supervised approach using only labeled data and the best semi-supervised\napproach using both labeled and unlabeled data. We quantify the best possible\nincrease in performance obtained thanks to the unlabeled data, i.e. we compute\nthe accuracy increase due to the information contained in the unlabeled data.\nOur work deals with a simple high-dimensional Gaussian mixture model for the\ndata in a Bayesian setting. Our rigorous analysis builds on recent theoretical\nbreakthroughs in high-dimensional inference and a large body of mathematical\ntools from statistical physics initially developed for spin glasses.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:08:05 GMT"}, {"version": "v2", "created": "Sat, 28 Sep 2019 21:26:23 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Lelarge", "Marc", ""], ["Miolane", "Leo", ""]]}, {"id": "1907.03813", "submitter": "Xiaoyi Gu", "authors": "Xiaoyi Gu, Leman Akoglu, Alessandro Rinaldo", "title": "Statistical Analysis of Nearest Neighbor Methods for Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nearest-neighbor (NN) procedures are well studied and widely used in both\nsupervised and unsupervised learning problems. In this paper we are concerned\nwith investigating the performance of NN-based methods for anomaly detection.\nWe first show through extensive simulations that NN methods compare favorably\nto some of the other state-of-the-art algorithms for anomaly detection based on\na set of benchmark synthetic datasets. We further consider the performance of\nNN methods on real datasets, and relate it to the dimensionality of the\nproblem. Next, we analyze the theoretical properties of NN-methods for anomaly\ndetection by studying a more general quantity called distance-to-measure (DTM),\noriginally developed in the literature on robust geometric and topological\ninference. We provide finite-sample uniform guarantees for the empirical DTM\nand use them to derive misclassification rates for anomalous observations under\nvarious settings. In our analysis we rely on Huber's contamination model and\nformulate mild geometric regularity assumptions on the underlying distribution\nof the data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 18:58:35 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Gu", "Xiaoyi", ""], ["Akoglu", "Leman", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1907.03875", "submitter": "Lorenzo Rosasco", "authors": "Enrico Cecini and Ernesto De Vito and Lorenzo Rosasco", "title": "Multi-Scale Vector Quantization with Reconstruction Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a multi-scale approach to vector quantization. We\ndevelop an algorithm, dubbed reconstruction trees, inspired by decision trees.\nHere the objective is parsimonious reconstruction of unsupervised data, rather\nthan classification. Contrasted to more standard vector quantization methods,\nsuch as K-means, the proposed approach leverages a family of given partitions,\nto quickly explore the data in a coarse to fine-- multi-scale-- fashion. Our\nmain technical contribution is an analysis of the expected distortion achieved\nby the proposed algorithm, when the data are assumed to be sampled from a fixed\nunknown distribution. In this context, we derive both asymptotic and finite\nsample results under suitable regularity assumptions on the distribution. As a\nspecial case, we consider the setting where the data generating distribution is\nsupported on a compact Riemannian sub-manifold. Tools from differential\ngeometry and concentration of measure are useful in our analysis.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 21:11:24 GMT"}, {"version": "v2", "created": "Wed, 4 Sep 2019 13:49:45 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Cecini", "Enrico", ""], ["De Vito", "Ernesto", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "1907.03888", "submitter": "Barnaby Rowe Dr", "authors": "Barnaby Rowe", "title": "Residual Entropy", "comments": "10 pages, 5 figures, updated to v2 with minor edits following\n  feedback; code used to generate all figures available at\n  https://github.com/barnabytprowe/residual-entropy", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe an approach to improving model fitting and model generalization\nthat considers the entropy of distributions of modelling residuals. We use\nsimple simulations to demonstrate the observational signatures of overfitting\non ordered sequences of modelling residuals, via the autocorrelation and power\nspectral density. These results motivate the conclusion that, as commonly\napplied, the least squares method assumes too much when it assumes that\nresiduals are uncorrelated for all possible models or values of the model\nparameters. We relax these too-stringent assumptions in favour of imposing an\nentropy prior on the (unknown, model-dependent, but potentially marginalizable)\ndistribution function for residuals. We recommend a simple extension to the\nMean Squared Error loss function that approximately incorporates this prior and\ncan be used immediately for modelling applications where meaningfully-ordered\nsequences of observations or training data can be defined.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jul 2019 22:04:21 GMT"}, {"version": "v2", "created": "Thu, 1 Aug 2019 23:08:05 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Rowe", "Barnaby", ""]]}, {"id": "1907.03955", "submitter": "XiaoMei Yang", "authors": "Xiaomei Yang and Zhiliang Deng", "title": "Bayesian approach for inverse obstacle scattering with Poisson data", "comments": "14 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an acoustic obstacle reconstruction problem with Poisson data.\nDue to the stochastic nature of the data, we tackle this problem in the\nframework of Bayesian inversion. The unknown obstacle is parameterized in its\nangular form. The prior for the parameterized unknown plays key role in the\nBayes reconstruction algorithm. The most popular used prior is the Gaussian.\nUnder the Gaussian prior assumption, we further suppose that the unknown\nsatisfies the total variation prior. With the hybrid prior, the well-posedness\nof the posterior distribution is discussed. The numerical examples verify the\neffectiveness of the proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 03:18:24 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Yang", "Xiaomei", ""], ["Deng", "Zhiliang", ""]]}, {"id": "1907.04027", "submitter": "Xiaoou Pan", "authors": "Xiaoou Pan, Qiang Sun and Wen-Xin Zhou", "title": "Iteratively Reweighted $\\ell_1$-Penalized Robust Regression", "comments": "62 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates tradeoffs among optimization errors, statistical\nrates of convergence and the effect of heavy-tailed errors for high-dimensional\nrobust regression with nonconvex regularization. When the additive errors in\nlinear models have only bounded second moment, we show that iteratively\nreweighted $\\ell_1$-penalized adaptive Huber regression estimator satisfies\nexponential deviation bounds and oracle properties, including the oracle\nconvergence rate and variable selection consistency, under a weak beta-min\ncondition. Computationally, we need as many as $O(\\log s + \\log\\log d)$\niterations to reach such an oracle estimator, where $s$ and $d$ denote the\nsparsity and ambient dimension, respectively. Extension to a general class of\nrobust loss functions is also considered. Numerical studies lend strong support\nto our methodology and theory.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 07:45:04 GMT"}, {"version": "v2", "created": "Sun, 8 Mar 2020 06:52:11 GMT"}, {"version": "v3", "created": "Tue, 29 Dec 2020 08:21:42 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Pan", "Xiaoou", ""], ["Sun", "Qiang", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1907.04044", "submitter": "Samuel Rosa", "authors": "Samuel Rosa", "title": "Optimal experimental designs for treatment contrasts in heteroscedastic\n  models with covariates", "comments": "21 pages, 2 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical trials, the response of a given subject often depends on the\nselected treatment as well as on some covariates. We study optimal approximate\ndesigns of experiments in the models with treatment and covariate effects. We\nallow for the variances of the responses to depend on the chosen treatments,\nwhich introduces heteroscedasticity into the models. For estimating systems of\ntreatment contrasts and linear functions of the covariates, we extend known\nresults on D-optimality of product designs by providing product designs that\nare optimal with respect to general eigenvalue-based criteria. In particular,\nA- and E-optimal product designs are obtained. We then formulate a method based\non linear programming for constructing optimal designs with smaller supports\nfrom the optimal product designs. The sparser designs can be more easily\nconverted to practically applicable exact designs. The provided results and the\nproposed sparsification method are demonstrated on some examples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 08:42:32 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Rosa", "Samuel", ""]]}, {"id": "1907.04303", "submitter": "Subhadip Pal", "authors": "Subhadip Pal, Subhajit Sengupta, Riten Mitra and Arunava Banerjee", "title": "A Bayesian Approach for Analyzing Data on the Stiefel Manifold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directional data emerges in a wide array of applications, ranging from\natmospheric sciences to medical imaging. Modeling such data, however, poses\nunique challenges by virtue of their being constrained to non-Euclidean spaces\nlike manifolds. Here, we present a unified Bayesian framework for inference on\nthe Stiefel manifold using the Matrix Langevin distribution. Specifically, we\npropose a novel family of conjugate priors and establish a number of\ntheoretical properties relevant to statistical inference. %Importantly, these\ninclude the propriety of these priors and concentration characterization.\nConjugacy enables the translation of these properties to their corresponding\nposteriors, which we exploit to develop the posterior inference scheme. For the\nimplementation of the posterior computation, including the posterior sampling,\nwe adopt a novel computational procedure for evaluating the hypergeometric\nfunction of matrix arguments that appears as normalization constants in the\nrelevant densities.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 17:41:09 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Pal", "Subhadip", ""], ["Sengupta", "Subhajit", ""], ["Mitra", "Riten", ""], ["Banerjee", "Arunava", ""]]}, {"id": "1907.04377", "submitter": "Nhat Ho", "authors": "Nhat Ho and Chiao-Yu Yang and Michael I. Jordan", "title": "Convergence Rates for Gaussian Mixtures of Experts", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a theoretical treatment of over-specified Gaussian mixtures of\nexperts with covariate-free gating networks. We establish the convergence rates\nof the maximum likelihood estimation (MLE) for these models. Our proof\ntechnique is based on a novel notion of \\emph{algebraic independence} of the\nexpert functions. Drawing on optimal transport theory, we establish a\nconnection between the algebraic independence and a certain class of partial\ndifferential equations (PDEs). Exploiting this connection allows us to derive\nconvergence rates and minimax lower bounds for parameter estimation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jul 2019 19:31:37 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Ho", "Nhat", ""], ["Yang", "Chiao-Yu", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1907.04467", "submitter": "Vrettos Moulos", "authors": "Vrettos Moulos and Venkat Anantharam", "title": "Optimal Chernoff and Hoeffding Bounds for Finite State Markov Chains", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops an optimal Chernoff type bound for the probabilities of\nlarge deviations of sums $\\sum_{k=1}^n f (X_k)$ where $f$ is a real-valued\nfunction and $(X_k)_{k \\in \\mathbb{Z}_{\\ge 0}}$ is a finite state Markov chain\nwith an arbitrary initial distribution and an irreducible transition\nprobability matrix satisfying a mild assumption on its positivity pattern,\nrelated to the function $f$ being considered. The novelty lies in this being a\nnon-asymptotic finite sample bound. Further, our bound is optimal in the large\ndeviations sense, attaining a constant prefactor and an exponential decay with\nthe optimal large deviations rate. Moreover, through a Pinsker type inequality\nand a Hoeffding type lemma, we are able to loosen up our Chernoff type bound to\na Hoeffding type bound and reveal the sub-Gaussian nature of the sums. Finally,\nunder the same mild assumption on the positivity pattern of the transition\nprobability matrix, we prove a uniform multiplicative ergodic theorem for the\nexponential family of tilted transition probability matrices corresponding to\n$f$.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 00:33:17 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2019 03:39:13 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Moulos", "Vrettos", ""], ["Anantharam", "Venkat", ""]]}, {"id": "1907.04481", "submitter": "Marcus A. Brubaker", "authors": "Priyank Jaini, Ivan Kobyzev, Yaoliang Yu, Marcus Brubaker", "title": "Tails of Lipschitz Triangular Flows", "comments": "Published at the 37th International Conference of Machine Learning,\n  (ICML 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the ability of popular flow based methods to capture\ntail-properties of a target density by studying the increasing triangular maps\nused in these flow methods acting on a tractable source density. We show that\nthe density quantile functions of the source and target density provide a\nprecise characterization of the slope of transformation required to capture\ntails in a target density. We further show that any Lipschitz-continuous\ntransport map acting on a source density will result in a density with similar\ntail properties as the source, highlighting the trade-off between a complex\nsource density and a sufficiently expressive transformation to capture\ndesirable properties of a target density. Subsequently, we illustrate that flow\nmodels like Real-NVP, MAF, and Glow as implemented originally lack the ability\nto capture a distribution with non-Gaussian tails. We circumvent this problem\nby proposing tail-adaptive flows consisting of a source distribution that can\nbe learned simultaneously with the triangular map to capture tail-properties of\na target density. We perform several synthetic and real-world experiments to\ncompliment our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 01:46:39 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 10:27:13 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 18:05:25 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Jaini", "Priyank", ""], ["Kobyzev", "Ivan", ""], ["Yu", "Yaoliang", ""], ["Brubaker", "Marcus", ""]]}, {"id": "1907.04956", "submitter": "Xianzhu Xiong", "authors": "Xianzhu Xiong, Meijuan Ou", "title": "Nonparametric estimation of the conditional density function with\n  right-censored and dependent data", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the local constant and the local linear estimators of\nthe conditional density function with right-censored data which exhibit some\ntype of dependence. It is assumed that the observations form a stationary\n$\\alpha-$mixing sequence. The asymptotic normality of the two estimators is\nestablished, which combined with the condition that\n$\\lim\\limits_{n\\rightarrow\\infty}nh_nb_n=\\infty$ implies the consistency of the\ntwo estimators and can be employed to construct confidence intervals for the\nconditional density function. The result on the local linear estimator of the\nconditional density function in Kim et al. (2010) is relaxed from the i.i.d.\nassumption to the $\\alpha-$mixing setting, and the result on the local linear\nestimator of the conditional density function in Spierdijk (2008) is relaxed\nfrom the $\\rho$-mixing assumption to the $\\alpha-$mixing setting. Finite sample\nbehavior of the estimators is investigated by simulations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jul 2019 23:37:40 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Xiong", "Xianzhu", ""], ["Ou", "Meijuan", ""]]}, {"id": "1907.05077", "submitter": "Nick Koning", "authors": "Nick Koning", "title": "Directing Power Towards Conic Parameter Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a high-dimensional parameter of interest, tests based on quadratic\nstatistics are known to have low power against subsets of the parameter space\n(henceforth, parameter subspaces). In addition, they typically involve an\ninverse covariance matrix which is difficult to estimate in high-dimensional\nsettings. I simultaneously address these two issues by proposing a novel test\nstatistic that is large in a conic parameter subspace of interest. This test\nstatistic generalizes the Wald statistic and nests many well-known test\nstatistics. For a given parameter subspace, the statistic is free of tuning\nparameters and suitable for high-dimensional settings if the subspace is\nsufficiently small. It can be computed using regularized linear regression,\nwhere the type of regularization and the regularization parameters are\ncompletely determined by the parameter subspace of interest. I illustrate the\nstatistic on subspaces that consist of sparse or nearly-sparse vectors, for\nwhich the computation corresponds to $\\ell_0$- and $\\ell_1$-regularized\nregression, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 09:56:23 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 15:37:18 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 15:38:53 GMT"}, {"version": "v4", "created": "Mon, 9 Sep 2019 14:26:55 GMT"}, {"version": "v5", "created": "Fri, 8 Nov 2019 23:15:58 GMT"}, {"version": "v6", "created": "Tue, 19 Nov 2019 16:11:03 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Koning", "Nick", ""]]}, {"id": "1907.05108", "submitter": "Adelaide Olivier", "authors": "Marie Doumic (MAMBA), Ad\\'ela\\\"ide Olivier (UP11 UFR Sciences), Lydia\n  Robert (MICALIS)", "title": "Estimating the division rate from indirect measurements of single cells", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Is it possible to estimate the dependence of a growing and dividing\npopulation on a given trait in the case where this trait is not directly\naccessible by experimental measurements, but making use of measurements of\nanother variable? This article adresses this general question for a very recent\nand popular model describing bacterial growth, the so-called incremental or\nadder model. In this model, the division rate depends on the increment of size\nbetween birth and division, whereas the most accessible trait is the size\nitself. We prove that estimating the division 10 rate from size measurements is\npossible, we state a reconstruction formula in a deterministic and then in a\nstatistical setting, and solve numerically the problem on simulated and\nexperimental data. Though this represents a severely ill-posed inverse problem,\nour numerical results prove to be satisfactory.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 11:05:33 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Doumic", "Marie", "", "MAMBA"], ["Olivier", "Ad\u00e9la\u00efde", "", "UP11 UFR Sciences"], ["Robert", "Lydia", "", "MICALIS"]]}, {"id": "1907.05226", "submitter": "Bharath Sriperumbudur", "authors": "Nicholas Sterge, Bharath Sriperumbudur, Lorenzo Rosasco and Alessandro\n  Rudi", "title": "Gain with no Pain: Efficient Kernel-PCA by Nystr\\\"om Sampling", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose and study a Nystr\\\"om based approach to efficient\nlarge scale kernel principal component analysis (PCA). The latter is a natural\nnonlinear extension of classical PCA based on considering a nonlinear feature\nmap or the corresponding kernel. Like other kernel approaches, kernel PCA\nenjoys good mathematical and statistical properties but, numerically, it scales\npoorly with the sample size. Our analysis shows that Nystr\\\"om sampling greatly\nimproves computational efficiency without incurring any loss of statistical\naccuracy. While similar effects have been observed in supervised learning, this\nis the first such result for PCA. Our theoretical findings, which are also\nillustrated by numerical results, are based on a combination of analytic and\nconcentration of measure techniques. Our study is more broadly motivated by the\nquestion of understanding the interplay between statistical and computational\nrequirements for learning.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 14:16:25 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Sterge", "Nicholas", ""], ["Sriperumbudur", "Bharath", ""], ["Rosasco", "Lorenzo", ""], ["Rudi", "Alessandro", ""]]}, {"id": "1907.05325", "submitter": "Andrew McRae", "authors": "Andrew D. McRae and Mark A. Davenport", "title": "Low-rank matrix completion and denoising under Poisson noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating a low-rank matrix from the\nobservation of all or a subset of its entries in the presence of Poisson noise.\nWhen we observe all entries, this is a problem of matrix denoising; when we\nobserve only a subset of the entries, this is a problem of matrix completion.\nIn both cases, we exploit an assumption that the underlying matrix is low-rank.\nSpecifically, we analyze several estimators, including a constrained\nnuclear-norm minimization program, nuclear-norm regularized least squares, and\na nonconvex constrained low-rank optimization problem. We show that for all\nthree estimators, with high probability, we have an upper error bound (in the\nFrobenius norm error metric) that depends on the matrix rank, the fraction of\nthe elements observed, and maximal row and column sums of the true matrix. We\nfurthermore show that the above results are minimax optimal (within a universal\nconstant) in classes of matrices with low rank and bounded row and column sums.\nWe also extend these results to handle the case of matrix multinomial denoising\nand completion.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 16:00:42 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 16:20:17 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["McRae", "Andrew D.", ""], ["Davenport", "Mark A.", ""]]}, {"id": "1907.05353", "submitter": "Shu Lu", "authors": "Shu Lu and Hongsheng Liu", "title": "Statistical inference for piecewise normal distributions and stochastic\n  variational inequalities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we first provide a method to compute confidence intervals for\nthe center of a piecewise normal distribution given a sample from this\ndistribution, under certain assumptions. We then extend this method to an\nasymptotic setting, and apply this method to compute confidence intervals for\nthe true solution of a stochastic variational inequality based on a solution to\na sample average approximation problem. The confidence intervals are computed\nwith simple formulas. Performance of the proposed method is tested with\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jul 2019 16:21:10 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Lu", "Shu", ""], ["Liu", "Hongsheng", ""]]}, {"id": "1907.05689", "submitter": "Tanut Treetanthiploet", "authors": "Samuel N. Cohen and Tanut Treetanthiploet", "title": "Gittins' theorem under uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR math.ST q-fin.CP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study dynamic allocation problems for discrete time multi-armed bandits\nunder uncertainty, based on the the theory of nonlinear expectations. We show\nthat, under strong independence of the bandits and with some relaxation in the\ndefinition of optimality, a Gittins allocation index gives optimal choices.\nThis involves studying the interaction of our uncertainty with controls which\ndetermine the filtration. We also run a simple numerical example which\nillustrates the interaction between the willingness to explore and uncertainty\naversion of the agent when making decisions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 12:01:27 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 15:27:13 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 00:42:01 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Cohen", "Samuel N.", ""], ["Treetanthiploet", "Tanut", ""]]}, {"id": "1907.05781", "submitter": "Robert Castelo", "authors": "Alberto Roverato, Robert Castelo", "title": "Path Weights in Concentration Graphs", "comments": "19 pages, 2 figures, 2 tables; revised manuscript after peer review;\n  added DOI", "journal-ref": "Biometrika, 107:705-722 (2020)", "doi": "10.1093/biomet/asaa010", "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A graphical model provides a compact and efficient representation of the\nassociation structure of a multivariate distribution by means of a graph.\nRelevant features of the distribution are represented by vertices, edges and\nother higher-order graphical structures, such as cliques or paths. Typically,\npaths play a central role in these models because they determine the\nindependence relationships among variables. However, while a theory of path\ncoefficients is available in models for directed graphs, little has been\ninvestigated about the strength of the association represented by a path in an\nundirected graph. Essentially, it has been shown that the covariance between\ntwo variables can be decomposed into a sum of weights associated with each of\nthe paths connecting the two variables in the corresponding concentration\ngraph. In this context, we consider concentration graph models and provide an\nextensive analysis of the properties of path weights and their interpretation.\nMore specifically, we give an interpretation of covariance weights through\ntheir factorisation into a partial covariance and an inflation factor. We then\nextend the covariance decomposition over the paths of an undirected graph to\nother measures of association, such as the marginal correlation coefficient and\na quantity that we call the inflated correlation. The usefulness of these\nfindings is illustrated with an application to the analysis of dietary intake\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 15:07:00 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:06:11 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Roverato", "Alberto", ""], ["Castelo", "Robert", ""]]}, {"id": "1907.05802", "submitter": "Domenico Marinucci", "authors": "Alessia Caponera and Domenico Marinucci", "title": "Asymptotics for Spherical Functional Autoregressions", "comments": "40 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a class of spherical functional autoregressive\nprocesses, and we discuss the estimation of the corresponding autoregressive\nkernels. In particular, we first establish a consistency result (in sup and\nmean-square norm), then a quantitative central limit theorem (in Wasserstein\ndistance), and finally a weak convergence result, under more restrictive\nregularity conditions. Our results are validated by a small numerical\ninvestigation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 15:41:28 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Caponera", "Alessia", ""], ["Marinucci", "Domenico", ""]]}, {"id": "1907.05918", "submitter": "Didong Li", "authors": "Minerva Mukhopadhyay, Didong Li and David B Dunson", "title": "Estimating densities with nonlinear support using Fisher-Gaussian\n  kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current tools for multivariate density estimation struggle when the density\nis concentrated near a nonlinear subspace or manifold. Most approaches require\nchoice of a kernel, with the multivariate Gaussian by far the most commonly\nused. Although heavy-tailed and skewed extensions have been proposed, such\nkernels cannot capture curvature in the support of the data. This leads to poor\nperformance unless the sample size is very large relative to the dimension of\nthe data. This article proposes a novel generalization of the Gaussian\ndistribution, which includes an additional curvature parameter. We refer to the\nproposed class as Fisher-Gaussian (FG) kernels, since they arise by sampling\nfrom a von Mises-Fisher density on the sphere and adding Gaussian noise. The FG\ndensity has an analytic form, and is amenable to straightforward implementation\nwithin Bayesian mixture models using Markov chain Monte Carlo. We provide\ntheory on large support, and illustrate gains relative to competitors in\nsimulated and real data applications.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jul 2019 18:45:27 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Mukhopadhyay", "Minerva", ""], ["Li", "Didong", ""], ["Dunson", "David B", ""]]}, {"id": "1907.06006", "submitter": "Mingming Li", "authors": "Mingming Li, Huafei Sun, Linyu Peng", "title": "Fisher-Rao Geometry and Jeffreys Prior for Pareto Distribution", "comments": null, "journal-ref": null, "doi": "10.1080/03610926.2020.1771593", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the Fisher-Rao geometry of the two-parameter\nfamily of Pareto distribution. We prove that its geometrical structure is\nisometric to the Poincar\\'e upper half-plane model, and then study the\ncorresponding geometrical features by presenting explicit expressions for\nconnection, curvature and geodesics. It is then applied to Bayesian inference\nby considering the Jeffreys prior determined by the volume form. In addition,\nthe posterior distribution from the prior is computed, providing a systematic\nmethod to the Bayesian inference for Pareto distribution.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 03:53:21 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Mingming", ""], ["Sun", "Huafei", ""], ["Peng", "Linyu", ""]]}, {"id": "1907.06074", "submitter": "Alexander Kolnogorov", "authors": "Alexander Kolnogorov", "title": "A new approach to Poissonian two-armed bandit problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a continuous time two-armed bandit problem in which incomes are\ndescribed by Poissonian processes. We develop Bayesian approach with arbitrary\nprior distribution. We present two versions of recursive equation for\ndetermination of Bayesian piece-wise constant strategy and Bayesian risk and\npartial differential equation in the limiting case. Unlike the previously\nconsidered Bayesian settings our description uses current history of the\nprocess and not evolution of the posterior distribution.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jul 2019 13:10:40 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Kolnogorov", "Alexander", ""]]}, {"id": "1907.06233", "submitter": "Martin Kroll", "authors": "Martin Kroll", "title": "Pointwise adaptive kernel density estimation under local approximate\n  differential privacy", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider non-parametric density estimation in the framework of local\napproximate differential privacy. In contrast to centralized privacy scenarios\nwith a trusted curator, in the local setup anonymization must be guaranteed\nalready on the individual data owners' side and therefore must precede any data\nmining tasks. Thus, the published anonymized data should be compatible with as\nmany statistical procedures as possible. We suggest adding Laplace noise and\nGaussian processes (both appropriately scaled) to kernel density estimators to\nobtain approximate differential private versions of the latter ones. We obtain\nminimax type results over Sobolev classes indexed by a smoothness parameter\n$s>1/2$ for the mean squared error at a fixed point. In particular, we show\nthat taking the average of private kernel density estimators from $n$ different\ndata owners attains the optimal rate of convergence if the bandwidth parameter\nis correctly specified. Notably, the optimal convergence rate in terms of the\nsample size $n$ is $n^{-(2s-1)/(2s+1)}$ under local differential privacy and\nthus deteriorated to the rate $n^{-(2s-1)/(2s)}$ which holds without privacy\nrestrictions. Since the optimal choice of the bandwidth parameter depends on\nthe smoothness $s$ and is thus not accessible in practice, adaptive methods for\nbandwidth selection are necessary and must, in the local privacy framework, be\nperformed directly on the anonymized data. We address this problem by means of\na variant of Lepski's method tailored to the privacy setup and obtain general\noracle inequalities for private kernel density estimators. In the Sobolev case,\nthe resulting adaptive estimator attains the optimal rate of convergence at\nleast up to extra logarithmic factors.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jul 2019 15:25:29 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Kroll", "Martin", ""]]}, {"id": "1907.06317", "submitter": "Gregory Cox", "authors": "Gregory Cox and Xiaoxia Shi", "title": "Simple Adaptive Size-Exact Testing for Full-Vector and Subvector\n  Inference in Moment Inequality Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple test for moment inequalities that has exact size in\nnormal models with known variance and has uniformly asymptotically exact size\nmore generally. The test compares the quasi-likelihood ratio statistic to a\nchi-squared critical value, where the degree of freedom is the rank of the\ninequalities that are active in finite samples. The test requires no simulation\nand thus is computationally fast and especially suitable for constructing\nconfidence sets for parameters by test inversion. It uses no tuning parameter\nfor moment selection and yet still adapts to the slackness of the moment\ninequalities. Furthermore, we show how the test can be easily adapted for\ninference on subvectors for the common empirical setting of conditional moment\ninequalities with nuisance parameters entering linearly.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 02:54:55 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 11:42:09 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Cox", "Gregory", ""], ["Shi", "Xiaoxia", ""]]}, {"id": "1907.06455", "submitter": "Radu Stoica", "authors": "R. Stoica (Universit\\'e de Lorraine), Madalina Deaconu\n  (TOSCA-NGE-POST), Anne Philippe (UN), Lluis Hurtado", "title": "Shadow Simulated Annealing algorithm: a new tool for global optimisation\n  and statistical inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a new global optimisation method that applies to a family\nof criteria that are not entirely known. This family includes the criteria\nobtained from the class of posteriors that have nor-malising constants that are\nanalytically not tractable. The procedure applies to posterior probability\ndensities that are continuously differen-tiable with respect to their\nparameters. The proposed approach avoids the re-sampling needed for the\nclassical Monte Carlo maximum likelihood inference, while providing the missing\nconvergence properties of the ABC based methods. Results on simulated data and\nreal data are presented. The real data application fits an inhomogeneous area\ninteraction point process to cosmological data. The obtained results validate\ntwo important aspects of the galaxies distribution in our Universe : proximity\nof the galaxies from the cosmic filament network together with territorial\nclustering at given range of interactions. Finally, conclusions and\nperspectives are depicted.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 11:58:33 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Stoica", "R.", "", "Universit\u00e9 de Lorraine"], ["Deaconu", "Madalina", "", "TOSCA-NGE-POST"], ["Philippe", "Anne", "", "UN"], ["Hurtado", "Lluis", ""]]}, {"id": "1907.06577", "submitter": "Fang Han", "authors": "Fang Han and Weibiao Wu", "title": "Probability inequalities for high dimensional time series under a\n  triangular array framework", "comments": "This is an invited short survey paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Study of time series data often involves measuring the strength of temporal\ndependence, on which statistical properties like consistency and central limit\ntheorem are built. Historically, various dependence measures have been\nproposed. In this note, we first survey some of the most well-used dependence\nmeasures as well as various probability and moment inequalities built upon them\nunder a high-dimensional triangular array time series setting. We then argue\nthat this triangular array setting will pose substantially new challenges to\nthe verification of some dependence conditions. In particular, ``textbook\nresults\" could now be misleading, and hence are recommended to be used with\ncaution.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jul 2019 16:36:06 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Han", "Fang", ""], ["Wu", "Weibiao", ""]]}, {"id": "1907.06933", "submitter": "Eni Musta", "authors": "C\\'ecile Durot and Eni Musta", "title": "On the $L_p$-error of the Grenander-type estimator in the Cox model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Cox regression model and study the asymptotic global behavior\nof the Grenander-type estimator for a monotone baseline hazard function. This\nmodel is not included in the general setting of Durot (2007). However, we show\nthat a similar central limit theorem holds for $L_p$-error of the\nGrenander-type estimator. We also propose a test procedure for a Weibull\nbaseline distribution, based on the $L_p$-distance between the Grenander\nestimator and a parametric estimator of the baseline hazard. Simulation studies\nare performed to investigate the performance of this test.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 10:40:58 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Durot", "C\u00e9cile", ""], ["Musta", "Eni", ""]]}, {"id": "1907.06965", "submitter": "Andreas Greven", "authors": "Andreas Greven", "title": "Stochastic Evolution of spatial populations: From configurations to\n  genealogies and back", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper reviews the results obtained for spatial population models and the\nevolution of the genealogies of these populations during the last decade by the\nauthor and his coworkers. The focus is on their large scale behaviour and on\nthe analysis of universality classes of large scale behaviour via the methods\nof the hierarchical mean-field limit or via the spatial continuum limit and\nfrom another angel, the finite system scheme . We use genealogical information\nto analyze the type and location structure and vice versa.\n  To apply this approach and to explain effects in biological situations we\nextend the classical model classes in new directions. Namely we look as\npopulation models here at: Fleming-Viot genealogies in continuum geographic\nspace, Cannings models with block resampling (reducing diversity),\nFisher-Wright diffusion with coloured seedbanks (enhancing diversity) and\nevolving genealogies of Fleming-Viot models with selection and rare mutation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 12:45:38 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Greven", "Andreas", ""]]}, {"id": "1907.07085", "submitter": "Sarah Ouadah", "authors": "C\\'eline L\\'evy-Leduc, Sarah Ouadah and Laure Sansonnet", "title": "Variable selection in sparse high-dimensional GLARMA models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel variable selection approach in the\nframework of sparse high-dimensional GLARMA models. It consists in combining\nthe estimation of the autoregressive moving average (ARMA) coefficients of\nthese models with regularized methods designed for Generalized Linear Models\n(GLM). The properties of our approach are investigated both from a theoretical\nand a numerical point of view. More precisely, we establish in a specific case\nthe consistency of the ARMA part coefficient estimators. We explain how to\nimplement our approach and we show that it is very attractive since it benefits\nfrom a low computational load. We also assess the performance of our\nmethodology using synthetic data and compare it with alternative approaches.\nOur numerical experiments show that combining the estimation of the ARMA part\ncoefficients with regularized methods designed for GLM dramatically improves\nthe variable selection performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 15:52:56 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 08:04:06 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["L\u00e9vy-Leduc", "C\u00e9line", ""], ["Ouadah", "Sarah", ""], ["Sansonnet", "Laure", ""]]}, {"id": "1907.07288", "submitter": "Fredrik S\\\"avje", "authors": "Fredrik S\\\"avje", "title": "On the inconsistency of matching without replacement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper shows that matching without replacement on propensity scores\nproduces estimators that generally are inconsistent for the average treatment\neffect of the treated. To achieve consistency, practitioners must either assume\nthat no units exist with propensity scores greater than one-half or assume that\nthere is no confounding among such units. The result is not driven by the use\nof propensity scores, and similar artifacts arise when matching on other scores\nas long as it is without replacement.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 23:50:56 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 12:54:02 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["S\u00e4vje", "Fredrik", ""]]}, {"id": "1907.07320", "submitter": "Sonja Petrovic", "authors": "Sonja Petrovi\\'c", "title": "What is... a Markov basis?", "comments": "AMS Notices piece", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short piece defines a Markov basis. The aim is to introduce the\nstatistical concept to mathematicians.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jul 2019 03:35:36 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Petrovi\u0107", "Sonja", ""]]}, {"id": "1907.07502", "submitter": "Cynthia Rush", "authors": "Zhiqi Bu, Jason Klusowski, Cynthia Rush, Weijie Su", "title": "Algorithmic Analysis and Statistical Estimation of SLOPE via Approximate\n  Message Passing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SLOPE is a relatively new convex optimization procedure for high-dimensional\nlinear regression via the sorted l1 penalty: the larger the rank of the fitted\ncoefficient, the larger the penalty. This non-separable penalty renders many\nexisting techniques invalid or inconclusive in analyzing the SLOPE solution. In\nthis paper, we develop an asymptotically exact characterization of the SLOPE\nsolution under Gaussian random designs through solving the SLOPE problem using\napproximate message passing (AMP). This algorithmic approach allows us to\napproximate the SLOPE solution via the much more amenable AMP iterates.\nExplicitly, we characterize the asymptotic dynamics of the AMP iterates relying\non a recently developed state evolution analysis for non-separable penalties,\nthereby overcoming the difficulty caused by the sorted l1 penalty. Moreover, we\nprove that the AMP iterates converge to the SLOPE solution in an asymptotic\nsense, and numerical simulations show that the convergence is surprisingly\nfast. Our proof rests on a novel technique that specifically leverages the\nSLOPE problem. In contrast to prior literature, our work not only yields an\nasymptotically sharp analysis but also offers an algorithmic, flexible, and\nconstructive approach to understanding the SLOPE problem.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 13:21:51 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Bu", "Zhiqi", ""], ["Klusowski", "Jason", ""], ["Rush", "Cynthia", ""], ["Su", "Weijie", ""]]}, {"id": "1907.07618", "submitter": "Gerardo Barrera Vargas", "authors": "Gerardo Barrera", "title": "Cut-off phenomenon for the maximum of a sampling of Ornstein-Uhlenbeck\n  processes", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we study the so-called cut-off phenomenon in the total\nvariation distance when $n\\to \\infty$ for the family of continuous-time\nstochastic processes indexed by $n\\in \\mathbb{N}$, \\[ \\left(\n\\mathcal{Z}^{(n)}_t= \\max\\limits_{j\\in \\{1,\\ldots,n\\}}{X^{(j)}_t}:t\\geq\n0\\right), \\] where $X^{(1)},\\ldots,X^{(n)}$ is a sampling of $n$ ergodic\nOrnstein-Uhlenbeck processes driven by stable processes of index $\\alpha$. It\nis not hard to see that for each $n\\in \\mathbb{N}$, $\\mathcal{Z}^{(n)}_t$\nconverges in the total variation distance to a limiting distribution\n$\\mathcal{Z}^{(n)}_\\infty$ as $t$ goes by. Using the asymptotic theory of\nextremes; in the Gaussian case we prove that the total variation distance\nbetween the distribution of $\\mathcal{Z}^{(n)}_t$ and its limiting distribution\n$\\mathcal{Z}^{(n)}_\\infty$ converges to a universal function in a constant time\nwindow around the cut-off time, a fact known as profile cut-off in the context\nof stochastic processes. On the other hand, in the heavy-tailed case we prove\nthat there is not cut-off.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 16:19:35 GMT"}, {"version": "v2", "created": "Fri, 27 Dec 2019 18:16:05 GMT"}, {"version": "v3", "created": "Sat, 5 Sep 2020 15:39:41 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Barrera", "Gerardo", ""]]}, {"id": "1907.07753", "submitter": "Jeremie Bigot", "authors": "J\\'er\\'emie Bigot and Camille Male", "title": "Freeness over the diagonal and outliers detection in deformed random\n  matrices with a variance profile", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the eigenvalue distribution of a GUE matrix with a variance profile\nthat is perturbed by an additive random matrix that may possess spikes. Our\napproach is guided by Voiculescu's notion of freeness with amalgamation over\nthe diagonal and by the notion of deterministic equivalent. This allows to\nderive a fixed point equation to approximate the spectral distribution of\ncertain deformed GUE matrices with a variance profile and to characterize the\nlocation of potential outliers in such models in a non-asymptotic setting. We\nalso consider the singular values distribution of a rectangular Gaussian random\nmatrix with a variance profile in a similar setting of additive perturbation.\nWe discuss the application of this approach to the study of low-rank matrix\ndenoising models in the presence of heteroscedastic noise, that is when the\namount of variance in the observed data matrix may change from entry to entry.\nNumerical experiments are used to illustrate our results.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jul 2019 20:55:22 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 19:48:13 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 16:04:28 GMT"}, {"version": "v4", "created": "Mon, 18 May 2020 09:49:24 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Bigot", "J\u00e9r\u00e9mie", ""], ["Male", "Camille", ""]]}, {"id": "1907.07869", "submitter": "Rajesh Sharma", "authors": "R.Sharma, R.Kumar, R.Saini and P.Devi", "title": "Bounds on Spreads of Matrices related to Fourth Central Moment. II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive some inequalities involving first four central moments of discrete\nand continuous distributions. Bounds for the eigenvalues and spread of a matrix\nare obtained when all its eigenvalues are real. Likewise, we discuss bounds for\nthe roots and span of a polynomial equation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 04:32:23 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Sharma", "R.", ""], ["Kumar", "R.", ""], ["Saini", "R.", ""], ["Devi", "P.", ""]]}, {"id": "1907.07915", "submitter": "Yunyi Zhang", "authors": "Yunyi Zhang, Tingting Wang, Dimitris N. Politis", "title": "Semi-parametric estimation and prediction intervals in state space\n  models", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Literatures in state space models focus on parametric inference and\nprediction, which fail if the state space model is not fully specified and the\nmaximum likelihood estimation does not work. In this paper, we assume the state\ntransition matrix and the distribution of state noises are unknown. Under this\nassumption, we provide methods to consistently estimate these terms. In\naddition, we introduce an algorithm to construct consistent prediction\nintervals for state vectors and future observations. We complement the\nasymptotic results with several numerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 07:55:40 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 09:45:32 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Zhang", "Yunyi", ""], ["Wang", "Tingting", ""], ["Politis", "Dimitris N.", ""]]}, {"id": "1907.08074", "submitter": "Stanislav Nagy", "authors": "Fr\\'ed\\'eric Ferraty and Stanislav Nagy", "title": "Scalar-on-function local linear regression and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regressing a scalar response on a random function is nowadays a common\nsituation. In the nonparametric setting, this paper paves the way for making\nthe local linear regression based on a projection approach a prominent method\nfor solving this regression problem. Our asymptotic results demonstrate that\nthe functional local linear regression outperforms its functional local\nconstant counterpart. Beyond the estimation of the regression operator itself,\nthe local linear regression is also a useful tool for predicting the functional\nderivative of the regression operator, a promising mathematical object on its\nown. The local linear estimator of the functional derivative is shown to be\nconsistent. On simulated datasets we illustrate good finite sample properties\nof both proposed methods. On a real data example of a single-functional index\nmodel we indicate how the functional derivative of the regression operator\nprovides an original and fast, widely applicable estimating method.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 14:28:18 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Ferraty", "Fr\u00e9d\u00e9ric", ""], ["Nagy", "Stanislav", ""]]}, {"id": "1907.08201", "submitter": "Jack Noonan", "authors": "Jack Noonan, Anatoly Zhigljavsky", "title": "Approximations for the boundary crossing probabilities of moving sums of\n  random variables", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.09229", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study approximations for the boundary crossing probabilities\nof moving sums of i.i.d. normal r.v. We approximate a discrete time problem\nwith a continuous time problem allowing us to apply established theory for\nstationary Gaussian processes. By then subsequently correcting approximations\nfor discrete time, we show that the developed approximations are very accurate\neven for small window length. Also, they have high accuracy when the original\nr.v. are not exactly normal and when the weights in the moving window are not\nall equal. We then provide accurate and simple approximations for ARL, the\naverage run length until crossing the boundary.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 16:03:05 GMT"}, {"version": "v2", "created": "Sat, 3 Aug 2019 15:27:24 GMT"}, {"version": "v3", "created": "Fri, 3 Jan 2020 10:17:22 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Noonan", "Jack", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "1907.08226", "submitter": "Stefano Sarao Mannelli", "authors": "Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent\n  Krzakala, and Lenka Zdeborov\\'a", "title": "Who is Afraid of Big Bad Minima? Analysis of Gradient-Flow in a Spiked\n  Matrix-Tensor Model", "comments": "9 pages, 4 figures + appendix. Appears in Proceedings of the Advances\n  in Neural Information Processing Systems 2019 (NeurIPS 2019)", "journal-ref": "Advances in Neural Information Processing Systems, pp. 8676-8686.\n  2019", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based algorithms are effective for many machine learning tasks, but\ndespite ample recent effort and some progress, it often remains unclear why\nthey work in practice in optimising high-dimensional non-convex functions and\nwhy they find good minima instead of being trapped in spurious ones.\n  Here we present a quantitative theory explaining this behaviour in a spiked\nmatrix-tensor model.\n  Our framework is based on the Kac-Rice analysis of stationary points and a\nclosed-form analysis of gradient-flow originating from statistical physics. We\nshow that there is a well defined region of parameters where the gradient-flow\nalgorithm finds a good global minimum despite the presence of exponentially\nmany spurious local minima.\n  We show that this is achieved by surfing on saddles that have strong negative\ndirection towards the global minima, a phenomenon that is connected to a\nBBP-type threshold in the Hessian describing the critical points of the\nlandscapes.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 18:11:24 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 13:43:09 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 15:08:26 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Mannelli", "Stefano Sarao", ""], ["Biroli", "Giulio", ""], ["Cammarota", "Chiara", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1907.08369", "submitter": "Naoya Yamaguchi", "authors": "Naoya Yamaguchi, Yuka Yamaguchi, and Ryuei Nishii", "title": "Minimizing the expected value of the asymmetric loss and an inequality\n  of the variance of the loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CA math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For some estimations and predictions, we solve minimization problems with\nasymmetric loss functions. Usually, we estimate the coefficient of regression\nfor these problems. In this paper, we do not make such the estimation, but\nrather give a solution by correcting any predictions so that the prediction\nerror follows a general normal distribution. In our method, we can not only\nminimize the expected value of the asymmetric loss, but also lower the variance\nof the loss.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jul 2019 01:24:40 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Yamaguchi", "Naoya", ""], ["Yamaguchi", "Yuka", ""], ["Nishii", "Ryuei", ""]]}, {"id": "1907.08417", "submitter": "Jeremie Bigot", "authors": "J\\'er\\'emie Bigot", "title": "Statistical data analysis in the Wasserstein space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned by statistical inference problems from a data set\nwhose elements may be modeled as random probability measures such as multiple\nhistograms or point clouds. We propose to review recent contributions in\nstatistics on the use of Wasserstein distances and tools from optimal transport\nto analyse such data. In particular, we highlight the benefits of using the\nnotions of barycenter and geodesic PCA in the Wasserstein space for the purpose\nof learning the principal modes of geometric variation in a dataset. In this\nsetting, we discuss existing works and we present some research perspectives\nrelated to the emerging field of statistical optimal transport.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 09:10:15 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 07:58:33 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Bigot", "J\u00e9r\u00e9mie", ""]]}, {"id": "1907.08627", "submitter": "Paula Saavedra-Nieves", "authors": "A. Rodr\\'iguez-Casal and P. Saavedra-Nieves", "title": "Extent of occurrence reconstruction using a new data-driven support\n  estimator", "comments": "arXiv admin note: text overlap with arXiv:1404.7397", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample of points from some unknown distribution, we propose a\nnew data-driven method for estimating its probability support S. Under the mild\nassumption that S is r-convex, the smallest r-convex set which contains the\nsample points is the natural estimator. The main problem for using this\nestimator in practice is that r is an unknown geometric characteristic of the\nset S. A stochastic algorithm is proposed for determining an optimal estimate\nof r from the data under mild regularity assumptions on the density function.\nThe resulting data-driven reconstruction of S attains the same convergence\nrates as the convex hull for estimating convex sets, but under a much more\nflexible smoothness shape condition. The new support estimator will be used for\nreconstructing the extent of occurrence of an assemblage of invasive plant\nspecies in the Azores archipelago.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 16:43:12 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Rodr\u00edguez-Casal", "A.", ""], ["Saavedra-Nieves", "P.", ""]]}, {"id": "1907.08646", "submitter": "John Lafferty", "authors": "Dana Yang, John Lafferty, David Pollard", "title": "Fair quantile regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is a tool for learning conditional distributions. In this\npaper we study quantile regression in the setting where a protected attribute\nis unavailable when fitting the model. This can lead to \"unfair'' quantile\nestimators for which the effective quantiles are very different for the\nsubpopulations defined by the protected attribute. We propose a procedure for\nadjusting the estimator on a heldout sample where the protected attribute is\navailable. The main result of the paper is an empirical process analysis\nshowing that the adjustment leads to a fair estimator for which the target\nquantiles are brought into balance, in a statistical sense that we call\n$\\sqrt{n}$-fairness. We illustrate the ideas and adjustment procedure on a\ndataset of 200,000 live births, where the objective is to characterize the\ndependence of the birth weights of the babies on demographic attributes of the\nbirth mother; the protected attribute is the mother's race.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jul 2019 18:52:35 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Yang", "Dana", ""], ["Lafferty", "John", ""], ["Pollard", "David", ""]]}, {"id": "1907.08742", "submitter": "Miles Lopes", "authors": "Miles E. Lopes", "title": "Estimating the Algorithmic Variance of Randomized Ensembles via the\n  Bootstrap", "comments": "53 pages", "journal-ref": "Annals of Statistics 47 (2019), no. 2, 1088--1112", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the methods of bagging and random forests are some of the most\nwidely used prediction methods, relatively little is known about their\nalgorithmic convergence. In particular, there are not many theoretical\nguarantees for deciding when an ensemble is \"large enough\" --- so that its\naccuracy is close to that of an ideal infinite ensemble. Due to the fact that\nbagging and random forests are randomized algorithms, the choice of ensemble\nsize is closely related to the notion of \"algorithmic variance\" (i.e. the\nvariance of prediction error due only to the training algorithm). In the\npresent work, we propose a bootstrap method to estimate this variance for\nbagging, random forests, and related methods in the context of classification.\nTo be specific, suppose the training dataset is fixed, and let the random\nvariable $Err_t$ denote the prediction error of a randomized ensemble of size\n$t$. Working under a \"first-order model\" for randomized ensembles, we prove\nthat the centered law of $Err_t$ can be consistently approximated via the\nproposed method as $t\\to\\infty$. Meanwhile, the computational cost of the\nmethod is quite modest, by virtue of an extrapolation technique. As a\nconsequence, the method offers a practical guideline for deciding when the\nalgorithmic fluctuations of $Err_t$ are negligible.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 02:55:41 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Lopes", "Miles E.", ""]]}, {"id": "1907.08743", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Yanjun Han, Ziteng Sun, and\n  Himanshu Tyagi", "title": "Domain Compression and its Application to Randomness-Optimal Distributed\n  Goodness-of-Fit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study goodness-of-fit of discrete distributions in the distributed\nsetting, where samples are divided between multiple users who can only release\na limited amount of information about their samples due to various information\nconstraints. Recently, a subset of the authors showed that having access to a\ncommon random seed (i.e., shared randomness) leads to a significant reduction\nin the sample complexity of this problem. In this work, we provide a complete\nunderstanding of the interplay between the amount of shared randomness\navailable, the stringency of information constraints, and the sample complexity\nof the testing problem by characterizing a tight trade-off between these three\nparameters. We provide a general distributed goodness-of-fit protocol that as a\nfunction of the amount of shared randomness interpolates smoothly between the\nprivate- and public-coin sample complexities. We complement our upper bound\nwith a general framework to prove lower bounds on the sample complexity of this\ntesting problems under limited shared randomness. Finally, we instantiate our\nbounds for the two archetypal information constraints of communication and\nlocal privacy, and show that our sample complexity bounds are optimal as a\nfunction of all the parameters of the problem, including the amount of shared\nrandomness.\n  A key component of our upper bounds is a new primitive of domain compression,\na tool that allows us to map distributions to a much smaller domain size while\npreserving their pairwise distances, using a limited amount of randomness.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 03:03:56 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Han", "Yanjun", ""], ["Sun", "Ziteng", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "1907.08766", "submitter": "Alfred Galichon", "authors": "Alfred Galichon", "title": "On the representation of the nested logit model", "comments": null, "journal-ref": null, "doi": "10.1017/S026646662000047X", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a two-line proof of a long-standing conjecture of Ben-Akiva and\nLerman (1985) regarding the random utility representation of the nested logit\nmodel, thus providing a renewed and straightforward textbook treatment of that\nmodel. As an application, we provide a closed-form formula for the correlation\nbetween two Fr\\'echet random variables coupled by a Gumbel copula.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 06:03:53 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 10:20:03 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 13:29:02 GMT"}, {"version": "v4", "created": "Sun, 8 Dec 2019 21:40:50 GMT"}, {"version": "v5", "created": "Mon, 12 Oct 2020 04:15:38 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Galichon", "Alfred", ""]]}, {"id": "1907.08790", "submitter": "Vaclav Kautsky", "authors": "V\\'aclav Kautsk\\'y, Zbyn\\v{e}k Koldovsk\\'y, Petr Tichavsk\\'y and\n  Vicente Zarzoso", "title": "Cram\\'er-Rao Bounds for Complex-Valued Independent Component Extraction:\n  Determined and Piecewise Determined Mixing Models", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": "10.1109/TSP.2020.3022827", "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Cram\\'er-Rao Lower Bound (CRLB) for the complex-valued\nBlind Source Extraction (BSE) problem based on the assumption that the target\nsignal is independent of the other signals. Two instantaneous mixing models are\nconsidered. First, we consider the standard determined mixing model used in\nIndependent Component Analysis (ICA) where the mixing matrix is square and\nnon-singular and the number of the latent sources is the same as that of the\nobserved signals. The CRLB for Independent Component Extraction (ICE) where the\nmixing matrix is re-parameterized in order to extract only one independent\ntarget source is computed. The target source is assumed to be non-Gaussian or\nnon-circular Gaussian while the other signals (background) are circular\nGaussian or non-Gaussian. The results confirm some previous observations known\nfor the real domain and bring new results for the complex domain. Also, the\nCRLB for ICE is shown to coincide with that for ICA when the non-Gaussianity of\nbackground is taken into account. %unless the assumed sources' distributions\nare misspecified. Second, we extend the CRLB analysis to piecewise determined\nmixing models. Here, the observed signals are assumed to obey the determined\nmixing model within short blocks where the mixing matrices can be varying from\nblock to block. However, either the mixing vector or the separating vector\ncorresponding to the target source is assumed to be constant across the blocks.\nThe CRLBs for the parameters of these models bring new performance bounds for\nthe BSE problem.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 10:02:50 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Kautsk\u00fd", "V\u00e1clav", ""], ["Koldovsk\u00fd", "Zbyn\u011bk", ""], ["Tichavsk\u00fd", "Petr", ""], ["Zarzoso", "Vicente", ""]]}, {"id": "1907.08880", "submitter": "Jiaming Xu", "authors": "Zhou Fan, Cheng Mao, Yihong Wu, and Jiaming Xu", "title": "Spectral Graph Matching and Regularized Quadratic Relaxations I: The\n  Gaussian Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph matching aims at finding the vertex correspondence between two\nunlabeled graphs that maximizes the total edge weight correlation. This amounts\nto solving a computationally intractable quadratic assignment problem. In this\npaper we propose a new spectral method, GRAph Matching by Pairwise\neigen-Alignments (GRAMPA). Departing from prior spectral approaches that only\ncompare top eigenvectors, or eigenvectors of the same order, GRAMPA first\nconstructs a similarity matrix as a weighted sum of outer products between all\npairs of eigenvectors of the two graphs, with weights given by a Cauchy kernel\napplied to the separation of the corresponding eigenvalues, then outputs a\nmatching by a simple rounding procedure. The similarity matrix can also be\ninterpreted as the solution to a regularized quadratic programming relaxation\nof the quadratic assignment problem.\n  For the Gaussian Wigner model in which two complete graphs on $n$ vertices\nhave Gaussian edge weights with correlation coefficient $1-\\sigma^2$, we show\nthat GRAMPA exactly recovers the correct vertex correspondence with high\nprobability when $\\sigma = O(\\frac{1}{\\log n})$. This matches the state of the\nart of polynomial-time algorithms, and significantly improves over existing\nspectral methods which require $\\sigma$ to be polynomially small in $n$. The\nsuperiority of GRAMPA is also demonstrated on a variety of synthetic and real\ndatasets, in terms of both statistical accuracy and computational efficiency.\nUniversality results, including similar guarantees for dense and sparse\nErd\\H{o}s-R\\'{e}nyi graphs, are deferred to the companion paper.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 23:36:41 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fan", "Zhou", ""], ["Mao", "Cheng", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1907.08883", "submitter": "Jiaming Xu", "authors": "Zhou Fan, Cheng Mao, Yihong Wu, and Jiaming Xu", "title": "Spectral Graph Matching and Regularized Quadratic Relaxations II:\n  Erd\\H{o}s-R\\'enyi Graphs and Universality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG math.SP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a new spectral graph matching algorithm, GRAph Matching by\nPairwise eigen-Alignments (GRAMPA), for recovering the latent vertex\ncorrespondence between two unlabeled, edge-correlated weighted graphs.\nExtending the exact recovery guarantees established in the companion paper for\nGaussian weights, in this work, we prove the universality of these guarantees\nfor a general correlated Wigner model. In particular, for two Erd\\H{o}s-R\\'enyi\ngraphs with edge correlation coefficient $1-\\sigma^2$ and average degree at\nleast $\\operatorname{polylog}(n)$, we show that GRAMPA exactly recovers the\nlatent vertex correspondence with high probability when $\\sigma \\lesssim\n1/\\operatorname{polylog}(n)$. Moreover, we establish a similar guarantee for a\nvariant of GRAMPA, corresponding to a tighter quadratic programming relaxation\nof the quadratic assignment problem. Our analysis exploits a resolvent\nrepresentation of the GRAMPA similarity matrix and local laws for the\nresolvents of sparse Wigner matrices.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jul 2019 23:50:02 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Fan", "Zhou", ""], ["Mao", "Cheng", ""], ["Wu", "Yihong", ""], ["Xu", "Jiaming", ""]]}, {"id": "1907.09083", "submitter": "Zhen Li", "authors": "Zhen Li and Eric Laber", "title": "Convergence Rates of Posterior Distributions in Markov Decision Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show the convergence rates of posterior distributions of\nthe model dynamics in a MDP for both episodic and continuous tasks. The\ntheoretical results hold for general state and action space and the parameter\nspace of the dynamics can be infinite dimensional. Moreover, we show the\nconvergence rates of posterior distributions of the mean accumulative reward\nunder a fixed or the optimal policy and of the regret bound. A variant of\nThompson sampling algorithm is proposed which provides both posterior\nconvergence rates for the dynamics and the regret-type bound. Then the previous\nresults are extended to Markov games. Finally, we show numerical results with\nthree simulation scenarios and conclude with discussions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 02:08:47 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Li", "Zhen", ""], ["Laber", "Eric", ""]]}, {"id": "1907.09232", "submitter": "Nicolas Marie", "authors": "Nicolas Marie", "title": "Nonparametric Estimation of the Trend in Reflected Fractional SDE", "comments": "10 pages", "journal-ref": "Statistics and Probability Letters 158, 8 pages, 2020", "doi": "10.1016/j.spl.2019.108659", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the consistency, a rate of convergence and the\nasymptotic distribution of a nonparametric estimator of the trend in the\nSkorokhod reflection problem defined by a fractional SDE and a Moreau sweeping\nprocess.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 11:11:16 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 12:38:28 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Marie", "Nicolas", ""]]}, {"id": "1907.09244", "submitter": "Aur\\'elien Bibaut", "authors": "Aur\\'elien F. Bibaut and Mark J. van der Laan", "title": "Fast rates for empirical risk minimization over c\\`adl\\`ag functions\n  with bounded sectional variation norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical risk minimization over classes functions that are bounded for some\nversion of the variation norm has a long history, starting with Total Variation\nDenoising (Rudin et al., 1992), and has been considered by several recent\narticles, in particular Fang et al., 2019 and van der Laan, 2015. In this\narticle, we consider empirical risk minimization over the class $\\mathcal{F}_d$\nof c\\`adl\\`ag functions over $[0,1]^d$ with bounded sectional variation norm\n(also called Hardy-Krause variation).\n  We show how a certain representation of functions in $\\mathcal{F}_d$ allows\nto bound the bracketing entropy of sieves of $\\mathcal{F}_d$, and therefore\nderive rates of convergence in nonparametric function estimation. Specifically,\nfor sieves whose growth is controlled by some rate $a_n$, we show that the\nempirical risk minimizer has rate of convergence $O_P(n^{-1/3} (\\log\nn)^{2(d-1)/3} a_n)$. Remarkably, the dimension only affects the rate in $n$\nthrough the logarithmic factor, making this method especially appropriate for\nhigh dimensional problems.\n  In particular, we show that in the case of nonparametric regression over\nsieves of c\\`adl\\`ag functions with bounded sectional variation norm, this\nupper bound on the rate of convergence holds for least-squares estimators,\nunder the random design, sub-exponential errors setting.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 11:36:32 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 17:50:22 GMT"}], "update_date": "2019-08-26", "authors_parsed": [["Bibaut", "Aur\u00e9lien F.", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1907.09477", "submitter": "Nan Zou", "authors": "Nan Zou, Stanislav Volgushev, Axel B\\\"ucher", "title": "Multiple block sizes and overlapping blocks for multivariate time series\n  extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Block maxima methods constitute a fundamental part of the statistical toolbox\nin extreme value analysis. However, most of the corresponding theory is derived\nunder the simplifying assumption that block maxima are independent observations\nfrom a genuine extreme value distribution. In practice however, block sizes are\nfinite and observations from different blocks are dependent. Theory respecting\nthe latter complications is not well developed, and, in the multivariate case,\nhas only recently been established for disjoint blocks of a single block size.\nWe show that using overlapping blocks instead of disjoint blocks leads to a\nuniform improvement in the asymptotic variance of the multivariate empirical\ndistribution function of rescaled block maxima and any smooth functionals\nthereof (such as the empirical copula), without any sacrifice in the asymptotic\nbias. We further derive functional central limit theorems for multivariate\nempirical distribution functions and empirical copulas that are uniform in the\nblock size parameter, which seems to be the first result of this kind for\nestimators based on block maxima in general. The theory allows for various\naggregation schemes over multiple block sizes, leading to substantial\nimprovements over the single block length case and opens the door to further\nmethodology developments. In particular, we consider bias correction procedures\nthat can improve the convergence rates of extreme-value estimators and shed\nsome new light on estimation of the second-order parameter when the main\npurpose is bias correction.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 15:21:59 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Zou", "Nan", ""], ["Volgushev", "Stanislav", ""], ["B\u00fccher", "Axel", ""]]}, {"id": "1907.09611", "submitter": "Jeffrey Miller", "authors": "Jeffrey W. Miller", "title": "Asymptotic normality, concentration, and coverage of generalized\n  posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generalized likelihoods are commonly used to obtain consistent estimators\nwith attractive computational and robustness properties. Formally, any\ngeneralized likelihood can be used to define a generalized posterior\ndistribution, but an arbitrarily defined \"posterior\" cannot be expected to\nappropriately quantify uncertainty in any meaningful sense. In this article, we\nprovide sufficient conditions under which generalized posteriors exhibit\nconcentration, asymptotic normality (Bernstein-von Mises), an asymptotically\ncorrect Laplace approximation, and asymptotically correct frequentist coverage.\nWe apply our results in detail to generalized posteriors for a wide array of\ngeneralized likelihoods, including pseudolikelihoods in general, the Gaussian\nMarkov random field pseudolikelihood, the fully observed Boltzmann machine\npseudolikelihood, the Ising model pseudolikelihood, the Cox proportional\nhazards partial likelihood, and a median-based likelihood for robust inference\nof location. Further, we show how our results can be used to easily establish\nthe asymptotics of standard posteriors for exponential families and generalized\nlinear models. We make no assumption of model correctness so that our results\napply with or without misspecification.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 22:28:47 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 00:47:10 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Miller", "Jeffrey W.", ""]]}, {"id": "1907.09617", "submitter": "Likun Zhang", "authors": "Likun Zhang, Benjamin A. Shaby, and Jennifer L. Wadsworth", "title": "Hierarchical Transformed Scale Mixtures for Flexible Modeling of Spatial\n  Extremes on Datasets with Many Locations", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1858838", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Flexible spatial models that allow transitions between tail dependence\nclasses have recently appeared in the literature. However, inference for these\nmodels is computationally prohibitive, even in moderate dimensions, due to the\nnecessity of repeatedly evaluating the multivariate Gaussian distribution\nfunction. In this work, we attempt to achieve truly high-dimensional inference\nfor extremes of spatial processes, while retaining the desirable flexibility in\nthe tail dependence structure, by modifying an established class of models\nbased on scale mixtures Gaussian processes. We show that the desired extremal\ndependence properties from the original models are preserved under the\nmodification, and demonstrate that the corresponding Bayesian hierarchical\nmodel does not involve the expensive computation of the multivariate Gaussian\ndistribution function. We fit our model to exceedances of a high threshold, and\nperform coverage analyses and cross-model checks to validate its ability to\ncapture different types of tail characteristics. We use a standard adaptive\nMetropolis algorithm for model fitting, and further accelerate the computation\nvia parallelization and Rcpp. Lastly, we apply the model to a dataset of a fire\nthreat index on the Great Plains region of the US, which is vulnerable to\nmassively destructive wildfires. We find that the joint tail of the fire threat\nindex exhibits a decaying dependence structure that cannot be captured by\nlimiting extreme value models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 22:44:08 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 20:09:32 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Zhang", "Likun", ""], ["Shaby", "Benjamin A.", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "1907.09698", "submitter": "Thomas Hogan", "authors": "Jes\\'us A. De Loera, Thomas A. Hogan", "title": "Stochastic Tverberg theorems and their applications in multi-class\n  logistic regression, data separability, and centerpoints of data", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new stochastic geometry theorems that give bounds on the\nprobability that $m$ random data classes all contain a point in common in their\nconvex hulls. We apply these stochastic separation theorems to obtain bounds on\nthe probability of existence of maximum likelihood estimators in multinomial\nlogistic regression. We also discuss connections to condition numbers for\nanalysis of steepest descent algorithms in logistic regression and to the\ncomputation of centerpoints of data clouds.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 05:21:12 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["De Loera", "Jes\u00fas A.", ""], ["Hogan", "Thomas A.", ""]]}, {"id": "1907.09762", "submitter": "Jean-Marc Bardet", "authors": "Jean-Marc Bardet (SAMM), Kare Kamila (SAMM), William Kengne (THEMA)", "title": "Consistent model selection criteria and goodness-of-fit test for affine\n  causal processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the model selection problem in a large class of causal\ntime series models, which includes both the ARMA or AR($\\infty$) processes, as\nwell as the GARCH or ARCH($\\infty$), APARCH, ARMA-GARCH and many others\nprocesses. To tackle this issue, we consider a penalized contrast based on the\nquasi-likelihood of the model. We provide sufficient conditions for the penalty\nterm to ensure the consistency of the proposed procedure as well as the\nconsistency and the asymptotic normality of the quasi-maximum likelihood\nestimator of the chosen model. It appears from these conditions that the\nBayesian Information Criterion (BIC) does not always guarantee the consistency.\nWe also propose a tool for diagnosing the goodness-of-fit of the chosen model\nbased on the portmanteau Test. Numerical simulations and an illustrative\nexample on the FTSE index are performed to highlight the obtained asymptotic\nresults, including a numerical evidence of the non consistency of the usual BIC\npenalty for order selection of an AR(p) models with ARCH($\\infty$) errors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 08:50:35 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Bardet", "Jean-Marc", "", "SAMM"], ["Kamila", "Kare", "", "SAMM"], ["Kengne", "William", "", "THEMA"]]}, {"id": "1907.10012", "submitter": "Chao Gao", "authors": "Haoyang Liu, Chao Gao, Richard J. Samworth", "title": "Minimax rates in sparse, high-dimensional changepoint detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the detection of a sparse change in a high-dimensional mean vector\nas a minimax testing problem. Our first main contribution is to derive the\nexact minimax testing rate across all parameter regimes for $n$ independent,\n$p$-variate Gaussian observations. This rate exhibits a phase transition when\nthe sparsity level is of order $\\sqrt{p \\log \\log (8n)}$ and has a very\ndelicate dependence on the sample size: in a certain sparsity regime it\ninvolves a triple iterated logarithmic factor in~$n$. Further, in a dense\nasymptotic regime, we identify the sharp leading constant, while in the\ncorresponding sparse asymptotic regime, this constant is determined to within a\nfactor of $\\sqrt{2}$. Extensions that cover spatial and temporal dependence,\nprimarily in the dense case, are also provided.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 17:16:05 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 18:34:23 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Liu", "Haoyang", ""], ["Gao", "Chao", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1907.10114", "submitter": "Behzad Mahmoudian", "authors": "Behzad Mahmoudian", "title": "Exploring the Distributional Properties of the Non-Gaussian Random Field\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the environmental modeling field, the exploratory analysis of responses\noften exhibits spatial correlation as well as some non-Gaussian attributes such\nas skewness and/or heavy-tailedness. Consequently, we propose a general spatial\nmodel based on scale-shape mixtures of the multivariate skew-normal\ndistribution. Intuitively, it incorporates distinct random effects to account\nfor the spatial dependencies not explained by a simple Gaussian random field\nmodel. Importantly, the proposed model is capable of generating a wide range of\nskewness and kurtosis levels. Meanwhile, we demonstrate that the skewness\nmixing can induce asymmetric tail dependence at sub-asymptotic and/or\nasymptotic levels.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 19:58:05 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Mahmoudian", "Behzad", ""]]}, {"id": "1907.10176", "submitter": "Etienne Roquain", "authors": "Tabea Rebafka, Etienne Roquain, Fanny Villers", "title": "Graph inference with clustering and false discovery rate control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a noisy version of the stochastic block model (NSBM) is\nintroduced and we investigate the three following statistical inferences in\nthis model: estimation of the model parameters, clustering of the nodes and\nidentification of the underlying graph. While the two first inferences are done\nby using a variational expectation-maximization (VEM) algorithm, the graph\ninference is done by controlling the false discovery rate (FDR), that is, the\naverage proportion of errors among the edges declared significant, and by\nmaximizing the true discovery rate (TDR), that is, the average proportion of\nedges declared significant among the true edges. Provided that the VEM\nalgorithm provides reliable parameter estimates and clustering, we\ntheoretically show that our procedure does control the FDR while satisfying an\noptimal TDR property, up to remainder terms that become small when the size of\nthe graph grows. Numerical experiments show that our method outperforms the\nclassical FDR controlling methods that ignore the underlying SBM topology. In\naddition, these simulations demonstrate that the FDR/TDR properties of our\nmethod are robust to model mis-specification, that is, are essentially\nmaintained outside our model.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 23:21:30 GMT"}], "update_date": "2019-07-25", "authors_parsed": [["Rebafka", "Tabea", ""], ["Roquain", "Etienne", ""], ["Villers", "Fanny", ""]]}, {"id": "1907.10318", "submitter": "Michael Choi", "authors": "Michael C.H. Choi", "title": "Universality of the Langevin diffusion as scaling limit of a family of\n  Metropolis-Hastings processes I: fixed dimension", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a target distribution $\\mu$ on a general state space $\\mathcal{X}$ and\na proposal Markov jump process with generator $Q$, the purpose of this paper is\nto investigate two universal properties enjoyed by two types of\nMetropolis-Hastings (MH) processes with generators $M_1(Q,\\mu)$ and\n$M_2(Q,\\mu)$ respectively. First, we motivate our study of $M_2$ by offering a\ngeometric interpretation of $M_1$, $M_2$ and their convex combinations as $L^1$\nminimizers between $Q$ and the set of $\\mu$-reversible generators of Markov\njump processes. Second, specializing into the case of $\\mathcal{X} =\n\\mathbb{R}^d$ along with a Gaussian proposal with vanishing variance and Gibbs\ntarget distribution, we prove that, upon appropriate scaling in time, the\nfamily of Markov jump processes corresponding to $M_1$, $M_2$ or their convex\ncombinations all converge weakly to an universal Langevin diffusion. While\n$M_1$ and $M_2$ are seemingly different stochastic dynamics, it is perhaps\nsurprising that they share these two universal properties. These two results\nare known for $M_1$ in Billera and Diaconis (2001) and Gelfand and Mitter\n(1991), and the counterpart results for $M_2$ and their convex combinations are\nnew.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 09:18:18 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 12:10:14 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Choi", "Michael C. H.", ""]]}, {"id": "1907.10592", "submitter": "Cathy Maugis-Rabusseau", "authors": "Yohann de Castro (ECL, ICJ), S\\'ebastien Gadat (TSE), Cl\\'ement\n  Marteau (ICJ), Cathy Maugis (IMT)", "title": "SuperMix: Sparse Regularization for Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the statistical estimation of a discrete mixing\nmeasure $\\mu$0 involved in a kernel mixture model. Using some recent advances\nin l1-regularization over the space of measures, we introduce a \"data fitting\nand regularization\" convex program for estimating $\\mu$0 in a grid-less manner\nfrom a sample of mixture law, this method is referred to as Beurling-LASSO. Our\ncontribution is twofold: we derive a lower bound on the bandwidth of our data\nfitting term depending only on the support of $\\mu$0 and its so-called \"minimum\nseparation\" to ensure quantitative support localization error bounds; and under\na so-called \"non-degenerate source condition\" we derive a non-asymptotic\nsupport stability property. This latter shows that for a sufficiently large\nsample size n, our estimator has exactly as many weighted Dirac masses as the\ntarget $\\mu$0 , converging in amplitude and localization towards the true ones.\nFinally, we also introduce some tractable algorithms for solving this convex\nprogram based on \"Sliding Frank-Wolfe\" or \"Conic Particle Gradient Descent\".\nStatistical performances of this estimator are investigated designing a\nso-called \"dual certificate\", which is appropriate to our setting. Some\nclassical situations, as e.g. mixtures of super-smooth distributions (e.g.\nGaussian distributions) or ordinary-smooth distributions (e.g. Laplace\ndistributions), are discussed at the end of the paper.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jul 2019 08:45:57 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 19:07:32 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["de Castro", "Yohann", "", "ECL, ICJ"], ["Gadat", "S\u00e9bastien", "", "TSE"], ["Marteau", "Cl\u00e9ment", "", "ICJ"], ["Maugis", "Cathy", "", "IMT"]]}, {"id": "1907.10679", "submitter": "Divine Wanduku", "authors": "Divine Wanduku and Chinmoy Rahul", "title": "Complete maximum likelihood estimation for SEIR epidemic models:\n  theoretical development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.DS math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a class of SEIR Markov chain models for infectious diseases\nobserved over discrete time in a random human population living in a closed\nenvironment. The population changes over time through random births, deaths,\nand transitions between states of the population. The SEIR models consist of\nrandom dynamical equations for each state (S, E, I and R) involving driving\nevents for the process. We characterize some special types of SEIR Markov chain\nmodels in the class including: (1) when birth and death are zero or non-zero,\nand (2) when the incubation and infectious periods are constant or random. A\ndetailed parameter estimation applying the maximum likelihood estimation\ntechnique and expectation maximization algorithm are presented for this study.\nNumerical simulation results are given to validate the epidemic models.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jul 2019 19:23:41 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 15:04:07 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Wanduku", "Divine", ""], ["Rahul", "Chinmoy", ""]]}, {"id": "1907.10821", "submitter": "Keith Levin", "authors": "Keith Levin, Elizaveta Levina", "title": "Bootstrapping Networks with Latent Space Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A core problem in statistical network analysis is to develop network\nanalogues of classical techniques. The problem of bootstrapping network data\nstands out as especially challenging, since typically one observes only a\nsingle network, rather than a sample. Here we propose two methods for obtaining\nbootstrap samples for networks drawn from latent space models. The first method\ngenerates bootstrap replicates of network statistics that can be represented as\nU-statistics in the latent positions, and avoids actually constructing new\nbootstrapped networks. The second method generates bootstrap replicates of\nwhole networks, and thus can be used for bootstrapping any network function.\nCommonly studied network quantities that can be represented as U-statistics\ninclude many popular summaries, such as average degree and subgraph counts, but\nother equally popular summaries, such as the clustering coefficient, are not\nexpressible as U-statistics and thus require the second bootstrap method. Under\nthe assumption of a random dot product graph, a type of latent space network\nmodel, we show consistency of the proposed bootstrap methods. We give\nmotivating examples throughout and demonstrate the effectiveness of our methods\non synthetic data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 03:40:27 GMT"}], "update_date": "2019-07-26", "authors_parsed": [["Levin", "Keith", ""], ["Levina", "Elizaveta", ""]]}, {"id": "1907.10905", "submitter": "Shuxiao Chen", "authors": "Shuxiao Chen, Edgar Dobriban, Jane H Lee", "title": "A Group-Theoretic Framework for Data Augmentation", "comments": "To appear in Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation is a widely used trick when training deep neural networks:\nin addition to the original data, properly transformed data are also added to\nthe training set. However, to the best of our knowledge, a clear mathematical\nframework to explain the performance benefits of data augmentation is not\navailable. In this paper, we develop such a theoretical framework. We show data\naugmentation is equivalent to an averaging operation over the orbits of a\ncertain group that keeps the data distribution approximately invariant. We\nprove that it leads to variance reduction. We study empirical risk\nminimization, and the examples of exponential families, linear regression, and\ncertain two-layer neural networks. We also discuss how data augmentation could\nbe used in problems with symmetry where other approaches are prevalent, such as\nin cryo-electron microscopy (cryo-EM).\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 08:58:59 GMT"}, {"version": "v2", "created": "Sat, 28 Dec 2019 19:29:42 GMT"}, {"version": "v3", "created": "Fri, 21 Feb 2020 20:50:50 GMT"}, {"version": "v4", "created": "Fri, 6 Nov 2020 19:48:43 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Chen", "Shuxiao", ""], ["Dobriban", "Edgar", ""], ["Lee", "Jane H", ""]]}, {"id": "1907.11024", "submitter": "Denis Belomestny", "authors": "Denis Belomestny, Alexander Goldenshluger", "title": "Density deconvolution under general assumptions on the distribution of\n  measurement errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the problem of density deconvolution under general\nassumptions on the measurement error distribution. Typically deconvolution\nestimators are constructed using Fourier transform techniques, and it is\nassumed that the characteristic function of the measurement errors does not\nhave zeros on the real line. This assumption is rather strong and is not\nfulfilled in many cases of interest. In this paper we develop a methodology for\nconstructing optimal density deconvolution estimators in the general setting\nthat covers vanishing and non--vanishing characteristic functions of the\nmeasurement errors. We derive upper bounds on the risk of the proposed\nestimators and provide sufficient conditions under which zeros of the\ncorresponding characteristic function have no effect on estimation accuracy.\nMoreover, we show that the derived conditions are also necessary in some\nspecific problem instances.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 13:21:29 GMT"}, {"version": "v2", "created": "Fri, 9 Aug 2019 14:05:16 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2020 12:03:25 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Belomestny", "Denis", ""], ["Goldenshluger", "Alexander", ""]]}, {"id": "1907.11038", "submitter": "Gunnar Taraldsen", "authors": "Gunnar Taraldsen", "title": "Conditional probability in Renyi spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1933 Kolmogorov constructed a general theory that defines the modern\nconcept of conditional probability. In 1955 Renyi fomulated a new axiomatic\ntheory for probability motivated by the need to include unbounded measures.\nThis note introduces a general concept of conditional probability in Renyi\nspaces.\n  Keywords: Measure theory; conditional probability space; conditional\nexpectation\n", "versions": [{"version": "v1", "created": "Mon, 22 Jul 2019 21:16:51 GMT"}, {"version": "v2", "created": "Sun, 28 Jul 2019 12:11:01 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Taraldsen", "Gunnar", ""]]}, {"id": "1907.11264", "submitter": "Mansour Naslcheraghi", "authors": "Leila Marandi, Mansour Naslcheraghi, Seyed Ali Ghorashi, Mohammad\n  Shikh-Bahaei", "title": "Delay Analysis in Full-Duplex Heterogeneous Cellular Networks", "comments": "This paper has been accepted for publication in the IEEE Transactions\n  on Vehicular Technology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous networks (HetNets) as a combination of macro cells and small\ncells are used to increase the cellular network's capacity, and present a\nperfect solution for high-speed communications. Increasing area spectrum\nefficiency and capacity of HetNets largely depends on the high speed of\nbackhaul links. One effective way which is currently utilized in HetNets is the\nuse of full-duplex (FD) technology that potentially doubles the spectral\nefficiency without the need for additional spectrum. On the other hand, one of\nthe most critical network design requirements is delay, which is a key\nrepresentation of the quality of service (QoS) in modern cellular networks. In\nthis paper, by utilizing tools from the stochastic geometry, we analyze the\nlocal delay for downlink (DL) channel, which is typically defined as the mean\nnumber of required time slots for a successful communication. Given imperfect\nself-interference (SI) cancellation in practical FD communications, we utilize\nduplex mode (half-duplex (HD) or FD) for each user based on the distance from\nits serving base station (BS). Further, we aim to investigate the energy\nefficiency (EE) for both duplexing modes, i.e., HD and FD, by considering local\ndelay. We conduct extensive simulations to validate system performance in terms\nof local delay versus different system key parameters.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 18:25:50 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Marandi", "Leila", ""], ["Naslcheraghi", "Mansour", ""], ["Ghorashi", "Seyed Ali", ""], ["Shikh-Bahaei", "Mohammad", ""]]}, {"id": "1907.11284", "submitter": "Nicolas Klutchnikoff", "authors": "Karine Bertin and Nicolas Klutchnikoff", "title": "Adaptive regression with Brownian path covariate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with estimation with functional covariates. More precisely,\nwe aim at estimating the regression function $m$ of a continuous outcome $Y$\nagainst a standard Wiener coprocess $W$. Following Cadre and Truquet (2015) and\nCadre, Klutchnikoff, and Massiot (2017) the Wiener-It\\^o decomposition of\n$m(W)$ is used to construct a family of estimators. The minimax rate of\nconvergence over specific smoothness classes is obtained. A data-driven\nselection procedure is defined following the ideas developed by Goldenshluger\nand Lepski (2011). An oracle-type inequality is obtained which leads to\nadaptive results.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 19:28:03 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 15:14:30 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Bertin", "Karine", ""], ["Klutchnikoff", "Nicolas", ""]]}, {"id": "1907.11331", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Nicolas Flammarion, Martin J. Wainwright, Peter L.\n  Bartlett", "title": "Improved Bounds for Discretization of Langevin Diffusions: Near-Optimal\n  Rates without Convexity", "comments": "Changes from v1: corrections in the proof of Lemma 6 and Lemma 10;\n  fixed some minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an improved analysis of the Euler-Maruyama discretization of the\nLangevin diffusion. Our analysis does not require global contractivity, and\nyields polynomial dependence on the time horizon. Compared to existing\napproaches, we make an additional smoothness assumption, and improve the\nexisting rate from $O(\\eta)$ to $O(\\eta^2)$ in terms of the KL divergence. This\nresult matches the correct order for numerical SDEs, without suffering from\nexponential time dependence. When applied to algorithms for sampling and\nlearning, this result simultaneously improves all those methods based on\nDalayan's approach.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 22:54:45 GMT"}, {"version": "v2", "created": "Mon, 4 Nov 2019 17:57:36 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Mou", "Wenlong", ""], ["Flammarion", "Nicolas", ""], ["Wainwright", "Martin J.", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1907.11391", "submitter": "Gabor Lugosi", "authors": "Gabor Lugosi and Shahar Mendelson", "title": "Robust multivariate mean estimation: the optimality of trimmed mean", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the mean of a random vector based on\ni.i.d. observations and adversarial contamination. We introduce a multivariate\nextension of the trimmed-mean estimator and show its optimal performance under\nminimal conditions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 06:30:04 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 13:27:59 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Lugosi", "Gabor", ""], ["Mendelson", "Shahar", ""]]}, {"id": "1907.11541", "submitter": "Mucyo Karemera", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser", "title": "Phase Transition Unbiased Estimation in High Dimensional Settings", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.04443", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important challenge in statistical analysis concerns the control of the\nfinite sample bias of estimators. For example, the maximum likelihood estimator\nhas a bias that can result in a significant inferential loss. This problem is\ntypically magnified in high-dimensional settings where the number of variables\n$p$ is allowed to diverge with the sample size $n$. However, it is generally\ndifficult to establish whether an estimator is unbiased and therefore its\nasymptotic order is a common approach used (in low-dimensional settings) to\nquantify the magnitude of the bias. As an alternative, we introduce a new and\nstronger property, possibly for high-dimensional settings, called phase\ntransition unbiasedness. An estimator satisfying this property is unbiased for\nall $n$ greater than a finite sample size $n^\\ast$. Moreover, we propose a\nphase transition unbiased estimator built upon the idea of matching an initial\nestimator computed on the sample and on simulated data. It is not required for\nthis initial estimator to be consistent and thus it can be chosen for its\ncomputational efficiency and/or for other desirable properties such as\nrobustness. This estimator can be computed using a suitable simulation based\nalgorithm, namely the iterative bootstrap, which is shown to converge\nexponentially fast. In addition, we demonstrate the consistency and the\nlimiting distribution of this estimator in high-dimensional settings. Finally,\nas an illustration, we use our approach to develop new estimators for the\nlogistic regression model, with and without random effects, that also enjoy\nother properties such as robustness to data contamination and are also not\naffected by the problem of separability. In a simulation exercise, the\ntheoretical results are confirmed in settings where the sample size is\nrelatively small compared to the model dimension.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 14:58:07 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2019 15:27:00 GMT"}, {"version": "v3", "created": "Fri, 1 Nov 2019 15:57:08 GMT"}], "update_date": "2019-11-04", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "1907.11547", "submitter": "Giorgio Matteo Vitetta Prof.", "authors": "Pasquale Di Viesti, Giorgio M. Vitetta and Emilio Sirignano", "title": "Double Bayesian Smoothing as Message Passing", "comments": "arXiv admin note: text overlap with arXiv:1902.05717 and\n  arXiv:1907.01358", "journal-ref": null, "doi": "10.1109/TSP.2019.2941064", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a novel method for developing filtering algorithms, based on the\ninterconnection of two Bayesian filters and called double Bayesian filtering,\nhas been proposed. In this manuscript we show that the same conceptual approach\ncan be exploited to devise a new smoothing method, called double Bayesian\nsmoothing. A double Bayesian smoother combines a double Bayesian filter,\nemployed in its forward pass, with the interconnection of two backward\ninformation filters used in its backward pass. As a specific application of our\ngeneral method, a detailed derivation of double Bayesian smoothing algorithms\nfor conditionally linear Gaussian systems is illustrated. Numerical results for\ntwo specific dynamic systems evidence that these algorithms can achieve a\nbetter complexity-accuracy tradeoff and tracking capability than other\nsmoothing techniques recently appeared in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jul 2019 08:38:55 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Di Viesti", "Pasquale", ""], ["Vitetta", "Giorgio M.", ""], ["Sirignano", "Emilio", ""]]}, {"id": "1907.11579", "submitter": "Iosif Pinelis", "authors": "Iosif Pinelis", "title": "An asymptotically optimal transform of Pearson's correlation statistic", "comments": "Revised version of a submitted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that for any correlation-parametrized model of dependence and any\ngiven significance level $\\alpha\\in(0,1)$, there is an asymptotically optimal\ntransform of Pearson's correlation statistic $R$, for which the generally\nleading error term for the normal approximation vanishes for all values\n$\\rho\\in(-1,1)$ of the correlation coefficient.\n  This general result is then applied to the bivariate normal (BVN) model of\ndependence and to what is referred to in this paper as the SquareV model. In\nthe BVN model, Pearson's $R$ turns out to be asymptotically optimal for a\nrather unusual significance level $\\alpha\\approx0.240$, whereas Fisher's\ntransform $R_F$ of $R$ is asymptotically optimal for the limit significance\nlevel $\\alpha=0$. In the SquareV model, Pearson's $R$ is asymptotically optimal\nfor a still rather high significance level $\\alpha\\approx0.159$, whereas\nFisher's transform $R_F$ of $R$ is not asymptotically optimal for any\n$\\alpha\\in[0,1]$. Moreover, it is shown that in both the BVN model and the\nSquareV model, the transform optimal for a given value of $\\alpha$ is in fact\nasymptotically better than $R$ and $R_F$ in wide ranges of values of the\nsignificance level, including $\\alpha$ itself.\n  Extensive computer simulations for the BVN and SquareV models of dependence\nare presented, which suggest that, for sample sizes $n\\ge100$ and significance\nlevels $\\alpha\\in\\{0.01,0.05\\}$, the mentioned asymptotically optimal transform\nof $R$ generally outperforms both Pearson's $R$ and Fisher's transform $R_F$ of\n$R$, the latter appearing generally much inferior to both $R$ and the\nasymptotically optimal transform of $R$ in the SquareV model.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 13:53:49 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Pinelis", "Iosif", ""]]}, {"id": "1907.11635", "submitter": "Alexander Wein", "authors": "Yunzi Ding, Dmitriy Kunisky, Alexander S. Wein, Afonso S. Bandeira", "title": "Subexponential-Time Algorithms for Sparse PCA", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational cost of recovering a unit-norm sparse principal\ncomponent $x \\in \\mathbb{R}^n$ planted in a random matrix, in either the Wigner\nor Wishart spiked model (observing either $W + \\lambda xx^\\top$ with $W$ drawn\nfrom the Gaussian orthogonal ensemble, or $N$ independent samples from\n$\\mathcal{N}(0, I_n + \\beta xx^\\top)$, respectively). Prior work has shown that\nwhen the signal-to-noise ratio ($\\lambda$ or $\\beta\\sqrt{N/n}$, respectively)\nis a small constant and the fraction of nonzero entries in the planted vector\nis $\\|x\\|_0 / n = \\rho$, it is possible to recover $x$ in polynomial time if\n$\\rho \\lesssim 1/\\sqrt{n}$. While it is possible to recover $x$ in exponential\ntime under the weaker condition $\\rho \\ll 1$, it is believed that\npolynomial-time recovery is impossible unless $\\rho \\lesssim 1/\\sqrt{n}$. We\ninvestigate the precise amount of time required for recovery in the \"possible\nbut hard\" regime $1/\\sqrt{n} \\ll \\rho \\ll 1$ by exploring the power of\nsubexponential-time algorithms, i.e., algorithms running in time\n$\\exp(n^\\delta)$ for some constant $\\delta \\in (0,1)$. For any $1/\\sqrt{n} \\ll\n\\rho \\ll 1$, we give a recovery algorithm with runtime roughly $\\exp(\\rho^2\nn)$, demonstrating a smooth tradeoff between sparsity and runtime. Our family\nof algorithms interpolates smoothly between two existing algorithms: the\npolynomial-time diagonal thresholding algorithm and the $\\exp(\\rho n)$-time\nexhaustive search algorithm. Furthermore, by analyzing the low-degree\nlikelihood ratio, we give rigorous evidence suggesting that the tradeoff\nachieved by our algorithms is optimal.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:45:13 GMT"}, {"version": "v2", "created": "Mon, 2 Mar 2020 02:24:47 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Ding", "Yunzi", ""], ["Kunisky", "Dmitriy", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "1907.11636", "submitter": "Alexander Wein", "authors": "Dmitriy Kunisky, Alexander S. Wein, Afonso S. Bandeira", "title": "Notes on Computational Hardness of Hypothesis Testing: Predictions using\n  the Low-Degree Likelihood Ratio", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These notes survey and explore an emerging method, which we call the\nlow-degree method, for predicting and understanding\nstatistical-versus-computational tradeoffs in high-dimensional inference\nproblems. In short, the method posits that a certain quantity -- the second\nmoment of the low-degree likelihood ratio -- gives insight into how much\ncomputational time is required to solve a given hypothesis testing problem,\nwhich can in turn be used to predict the computational hardness of a variety of\nstatistical inference tasks. While this method originated in the study of the\nsum-of-squares (SoS) hierarchy of convex programs, we present a self-contained\nintroduction that does not require knowledge of SoS. In addition to showing how\nto carry out predictions using the method, we include a discussion\ninvestigating both rigorous and conjectural consequences of these predictions.\n  These notes include some new results, simplified proofs, and refined\nconjectures. For instance, we point out a formal connection between spectral\nmethods and the low-degree likelihood ratio, and we give a sharp low-degree\nlower bound against subexponential-time algorithms for tensor PCA.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jul 2019 15:46:05 GMT"}], "update_date": "2019-07-29", "authors_parsed": [["Kunisky", "Dmitriy", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "1907.11958", "submitter": "Michael Law", "authors": "Michael Law and Ya'acov Ritov", "title": "Estimating the Random Effect in Big Data Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider three problems in high-dimensional Gaussian linear mixed models.\nWithout any assumptions on the design for the fixed effects, we construct an\nasymptotic $F$-statistic for testing whether a collection of random effects is\nzero, derive an asymptotic confidence interval for a single random effect at\nthe parametric rate $\\sqrt{n}$, and propose an empirical Bayes estimator for a\npart of the mean vector in ANOVA type models that performs asymptotically as\nwell as the oracle Bayes estimator. We support our results with numerical\nsimulations and provide comparisons with oracle estimators. The procedures\ndeveloped are applied to the Trends in International Mathematics and Sciences\nStudy (TIMSS) data.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jul 2019 18:21:10 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Law", "Michael", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1907.12116", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Michael I. Jordan, Tamara Broderick", "title": "A Higher-Order Swiss Army Infinitesimal Jackknife", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross validation (CV) and the bootstrap are ubiquitous model-agnostic tools\nfor assessing the error or variability of machine learning and statistical\nestimators. However, these methods require repeatedly re-fitting the model with\ndifferent weighted versions of the original dataset, which can be prohibitively\ntime-consuming. For sufficiently regular optimization problems the optimum\ndepends smoothly on the data weights, and so the process of repeatedly\nre-fitting can be approximated with a Taylor series that can be often evaluated\nrelatively quickly. The first-order approximation is known as the\n\"infinitesimal jackknife\" in the statistics literature and has been the subject\nof recent interest in machine learning for approximate CV. In this work, we\nconsider high-order approximations, which we call the \"higher-order\ninfinitesimal jackknife\" (HOIJ). Under mild regularity conditions, we provide a\nsimple recursive procedure to compute approximations of all orders with\nfinite-sample accuracy bounds. Additionally, we show that the HOIJ can be\nefficiently computed even in high dimensions using forward-mode automatic\ndifferentiation. We show that a linear approximation with bootstrap weights\napproximation is equivalent to those provided by asymptotic normal\napproximations. Consequently, the HOIJ opens up the possibility of enjoying\nhigher-order accuracy properties of the bootstrap using local approximations.\nConsistency of the HOIJ for leave-one-out CV under different asymptotic regimes\nfollows as corollaries from our finite-sample bounds under additional\nregularity assumptions. The generality of the computation and bounds motivate\nthe name \"higher-order Swiss Army infinitesimal jackknife.\"\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 17:49:48 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Giordano", "Ryan", ""], ["Jordan", "Michael I.", ""], ["Broderick", "Tamara", ""]]}, {"id": "1907.12159", "submitter": "Yariv Aizenbud", "authors": "Yariv Aizenbud and Barak Sober", "title": "Approximating the Span of Principal Components via Iterative\n  Least-Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the course of the last century, Principal Component Analysis (PCA) have\nbecome one of the pillars of modern scientific methods. Although PCA is\nnormally addressed as a statistical tool aiming at finding orthogonal\ndirections on which the variance is maximized, its first introduction by\nPearson at 1901 was done through defining a non-linear least-squares\nminimization problem of fitting a plane to scattered data points. Thus, it\nseems natural that PCA and linear least-squares regression are somewhat\nrelated, as they both aim at fitting planes to data points. In this paper, we\npresent a connection between the two approaches. Specifically, we present an\niterated linear least-squares approach, yielding a sequence of subspaces, which\nconverges to the space spanned by the leading principal components (i.e.,\nprincipal space).\n", "versions": [{"version": "v1", "created": "Sun, 28 Jul 2019 23:21:16 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Aizenbud", "Yariv", ""], ["Sober", "Barak", ""]]}, {"id": "1907.12203", "submitter": "Mingzhang Yin", "authors": "Mingzhang Yin, Y. X. Rachel Wang, Purnamrita Sarkar", "title": "A Theoretical Case Study of Structured Variational Inference for\n  Community Detection", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean-field variational inference (MFVI) has been widely applied in large\nscale Bayesian inference. However MFVI, which assumes a product distribution on\nthe latent variables, often leads to objective functions with many local\noptima, making optimization algorithms sensitive to initialization. In this\npaper, we study the advantage of structured variational inference for the two\nclass Stochastic Blockmodel. The variational distribution is constructed to\nhave pairwise dependency structure on the nodes of the network. We prove that,\nin a broad density regime and for general random initializations, unlike MFVI,\nthe class labels estimated from our method converge to the ground truth with\nhigh probability, when the model parameters are known, estimated within a\nreasonable range or jointly optimized with the variational parameters. In\naddition, empirically we demonstrate structured VI is more robust compared with\nMFVI when the graph is sparse and the signal to noise ratio is low. The paper\ntakes a first step towards understanding the importance of dependency structure\nin variational inference for community detection.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 03:59:44 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 03:45:27 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2019 03:56:30 GMT"}, {"version": "v4", "created": "Fri, 18 Oct 2019 05:30:40 GMT"}, {"version": "v5", "created": "Sun, 1 Mar 2020 03:01:51 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Yin", "Mingzhang", ""], ["Wang", "Y. X. Rachel", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "1907.12251", "submitter": "Jingming Wang", "authors": "Zhigang Bao, Xiucai Ding, Jingming Wang, Ke Wang", "title": "Principal components of spiked covariance matrices in the supercritical\n  regime", "comments": "This paper has been included in arXiv: 2008.11903 as a special case.\n  Hence, this paper will not be published separately", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the asymptotic behavior of the extreme eigenvalues\nand eigenvectors of the spiked covariance matrices, in the supercritical\nregime. Specifically, we derive the joint distribution of the extreme\neigenvalues and the generalized components of their associated eigenvectors in\nthis regime.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 07:48:01 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 09:07:37 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Bao", "Zhigang", ""], ["Ding", "Xiucai", ""], ["Wang", "Jingming", ""], ["Wang", "Ke", ""]]}, {"id": "1907.12528", "submitter": "Robert Lunde", "authors": "Robert Lunde and Purnamrita Sarkar", "title": "Subsampling Sparse Graphons Under Minimal Assumptions", "comments": "V2 is a streamlined/shorter version of V1", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a general theory for subsampling network data generated by the\nsparse graphon model. In contrast to previous work for networks, we demonstrate\nvalidity under minimal assumptions; the main requirement is weak convergence of\nthe functional of interest. We study the properties of two procedures: vertex\nsubsampling and $p$-subsampling. For the first, we prove validity under the\nmild condition that the number of subsampled vertices is $o(n)$. For the\nsecond, we establish validity under analogous conditions on the expected\nsubsample size. For both procedures, we also establish conditions under which\nuniform validity holds. Furthermore, under appropriate sparsity conditions, we\nderive limiting distributions for the nonzero eigenvalues of the adjacency\nmatrix of a low rank sparse graphon. Our weak convergence result immediately\nyields the validity of subsampling for the nonzero eigenvalues under suitable\nassumptions.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jul 2019 17:06:32 GMT"}, {"version": "v2", "created": "Sun, 25 Aug 2019 23:56:16 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Lunde", "Robert", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "1907.12732", "submitter": "Zijian Guo", "authors": "Zijian Guo, Cun-Hui Zhang", "title": "Local Inference in Additive Models with Decorrelated Local Linear\n  Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive models, as a natural generalization of linear regression, have\nplayed an important role in studying nonlinear relationships. Despite of a rich\nliterature and many recent advances on the topic, the statistical inference\nproblem in additive models is still relatively poorly understood. Motivated by\nthe inference for the exposure effect and other applications, we tackle in this\npaper the statistical inference problem for $f_1'(x_0)$ in additive models,\nwhere $f_1$ denotes the univariate function of interest and $f_1'(x_0)$ denotes\nits first order derivative evaluated at a specific point $x_0$. The main\nchallenge for this local inference problem is the understanding and control of\nthe additional uncertainty due to the need of estimating other components in\nthe additive model as nuisance functions. To address this, we propose a\ndecorrelated local linear estimator, which is particularly useful in reducing\nthe effect of the nuisance function estimation error on the estimation accuracy\nof $f'_1(x_0)$. We establish the asymptotic limiting distribution for the\nproposed estimator and then construct confidence interval and hypothesis\ntesting procedures for $f_1'(x_0)$. The variance level of the proposed\nestimator is of the same order as that of the local least squares in\nnonparametric regression, or equivalently the additive model with one\ncomponent, while the bias of the proposed estimator is jointly determined by\nthe statistical accuracies in estimating the nuisance functions and the\nrelationship between the variable of interest and the nuisance variables. The\nmethod is developed for general additive models and is demonstrated in the\nhigh-dimensional sparse setting.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 04:12:51 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Guo", "Zijian", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1907.12780", "submitter": "Baptiste Broto", "authors": "Baptiste Broto (LADIS), Fran\\c{c}ois Bachoc (IMT), Laura Clouvel,\n  Jean-Marc Martinez (DM2S)", "title": "Block-diagonal covariance estimation and application to the Shapley\n  effects in sensitivity analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to estimate block-diagonal covariance matrices for\nGaussian data in high dimension and in fixed dimension. We first estimate the\nblock-diagonal structure of the covariance matrix by theoretical and practical\nestimators which are consistent. We deduce that the suggested estimator of the\ncovariance matrix in high dimension converges with the same rate than if the\ntrue decomposition was known. In fixed dimension , we prove that the suggested\nestimator is asymptotically efficient. Then, we focus on the estimation of\nsensitivity indices called \"Shapley effects\", in the high-dimensional Gaussian\nlinear framework. From the estimated covariance matrix, we obtain an estimator\nof the Shapley effects with a relative error which goes to zero at the\nparametric rate up to a logarithm factor. Using the block-diagonal structure of\nthe estimated covariance matrix, this estimator is still available for\nthousands inputs variables, as long as the maximal block is not too large.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 08:44:47 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 10:59:47 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Broto", "Baptiste", "", "LADIS"], ["Bachoc", "Fran\u00e7ois", "", "IMT"], ["Clouvel", "Laura", "", "DM2S"], ["Martinez", "Jean-Marc", "", "DM2S"]]}, {"id": "1907.12795", "submitter": "J\\\"org Martin", "authors": "J\\\"org Martin, Clemens Elster", "title": "The variation of the posterior variance and Bayesian sample size\n  determination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian sample size determination using a criterion that\nutilizes the first two moments of the expected posterior variance. We study the\nresulting sample size in dependence on the chosen prior and explore the success\nrate for bounding the posterior variance below a prescribed limit under the\ntrue sampling distribution. Compared with sample size determination based on\nthe expected average of the posterior variance the proposed criterion leads to\nan increase in sample size and significantly improved success rates. Generic\nasymptotic properties are proven, such as an asymptotic expression for the\nsample size and a sort of phase transition. Our study is illustrated using two\nreal world datasets with Poisson and normally distributed data. Based on our\nresults some recommendations are given.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 09:19:46 GMT"}, {"version": "v2", "created": "Thu, 27 Feb 2020 09:55:35 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Martin", "J\u00f6rg", ""], ["Elster", "Clemens", ""]]}, {"id": "1907.13093", "submitter": "Jean-Jacques Forneron", "authors": "Jean-Jacques Forneron", "title": "Detecting Identification Failure in Moment Condition Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper develops an approach to detect identification failure in a large\nclass of moment condition models. This is achieved by introducing a\nquasi-Jacobian matrix which is asymptotically singular under weak or set\nidentification, and when local identification fails. In these settings,\nstandard asymptotics are not valid. Under (semi)-strong identification, where\nstandard asymptotics are valid, the quasi-Jacobian is asymptotically equivalent\nto the usual Jacobian and, after re-scaling, is asymptotically non-singular.\nHence, the eigenvalues of the quasi-Jacobian are informative about local and\nglobal identification failures and the directions in which identification\nfails. Building on these results, a simple test procedure with chi-squared\ndata-driven critical values is introduced. It yields uniformly valid subvector\ninferences under strong, semi-strong, and weak identification without a priori\nknowledge about the underlying identification structure. It is non-conservative\nunder strong identification. Monte-Carlo simulations and an application to the\nLong-Run Risks model illustrate the results.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jul 2019 17:26:06 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 01:07:30 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 18:39:44 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Forneron", "Jean-Jacques", ""]]}, {"id": "1907.13421", "submitter": "Jinguo Xian", "authors": "Dong Han, Fugee Tsung and Jinguo Xian", "title": "Optimal Sequential Tests for Monitoring Changes in the Distribution of\n  Finite Observation Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a method to construct the optimal sequential test for\nmonitoring the changes in the distribution of finite observation sequences with\na general dependence structure. This method allows us to prove that different\noptimal sequential tests can be constructed for different performance measures\nof detection delay times. We also provide a formula to calculate the value of\nthe generalized out-of-control average run length for every optimal sequential\ntest. Moreover, we show that there is an equivalent optimal control limit which\ndoes not depend on the test statistic directly when the post-change conditional\ndensities (probabilities) of the observation sequences do not depend on the\nchange time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 11:23:40 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Han", "Dong", ""], ["Tsung", "Fugee", ""], ["Xian", "Jinguo", ""]]}, {"id": "1907.13501", "submitter": "John Kent", "authors": "John T. Kent and Shambo Bhattacharjee and Weston R. Faber and Islam I.\n  Hussein", "title": "Observation-centered Kalman filters", "comments": "17 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various methods have been proposed for the nonlinear filtering problem,\nincluding the extended Kalman filter (EKF), iterated extended Kalman filter\n(IEKF), unscented Kalman filter (UKF) and iterated unscented Kalman filter\n(IUKF). In this paper two new nonlinear Kalman filters are proposed and\ninvestigated, namely the observation-centered extended Kalman filter (OCEKF)\nand observation-centered unscented Kalman filter (OCUKF). Although the UKF and\nEKF are common default choices for nonlinear filtering, there are situations\nwhere they are bad choices. Examples are given where the EKF and UKF perform\nvery poorly, and the IEKF and OCEKF perform well. In addition the IUKF and\nOCUKF are generally similar to the IEKF and OCEKF, and also perform well,\nthough care is needed in the choice of tuning parameters when the observation\nerror is small. The reasons for this behaviour are explored in detail.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 13:45:37 GMT"}, {"version": "v2", "created": "Tue, 20 Aug 2019 11:27:41 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 16:25:26 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Kent", "John T.", ""], ["Bhattacharjee", "Shambo", ""], ["Faber", "Weston R.", ""], ["Hussein", "Islam I.", ""]]}, {"id": "1907.13533", "submitter": "Lionel Truquet", "authors": "Lionel Truquet", "title": "Coupling and perturbation techniques for categorical time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general approach for studying autoregressive categorical time\nseries models with dependence of infinite order and defined conditional on an\nexogenous covariate process. To this end, we adapt a coupling approach,\ndeveloped in the literature for bounding the relaxation speed of a chain with\ncomplete connection and from which we derive a perturbation result for\nnon-homogenous versions of such chains. We then study stationarity, ergodicity\nand dependence properties of some chains with complete connections and\nexogenous covariates. As a consequence, we obtain a general framework for\nstudying some observation-driven time series models used both in statistics and\neconometrics but without theoretical support.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 14:43:27 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Truquet", "Lionel", ""]]}, {"id": "1907.13563", "submitter": "Francisco Javier Rubio", "authors": "David Rossell and Francisco Javier Rubio", "title": "Additive Bayesian variable selection under censoring and\n  misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the role of misspecification and censoring on Bayesian model\nselection in the contexts of right-censored survival and concave log-likelihood\nregression. Misspecification includes wrongly assuming the censoring mechanism\nto be non-informative. Emphasis is placed on additive accelerated failure time,\nCox proportional hazards and probit models. We offer a theoretical treatment\nthat includes local and non-local priors, and a general non-linear effect\ndecomposition to improve power-sparsity trade-offs. We discuss a fundamental\nquestion: what solution can one hope to obtain when (inevitably) models are\nmisspecified, and how to interpret it? Asymptotically, covariates that do not\nhave predictive power for neither the outcome nor (for survival data) censoring\ntimes, in the sense of reducing a likelihood-associated loss, are discarded.\nMisspecification and censoring have an asymptotically negligible effect on\nfalse positives, but their impact on power is exponential. We show that it can\nbe advantageous to consider simple models that are computationally practical\nyet attain good power to detect potentially complex effects, including the use\nof finite-dimensional basis to detect truly non-parametric effects. We also\ndiscuss algorithms to capitalize on sufficient statistics and fast likelihood\napproximations for Gaussian-based survival and binary models.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 15:43:40 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 22:19:24 GMT"}, {"version": "v3", "created": "Wed, 6 May 2020 18:39:21 GMT"}, {"version": "v4", "created": "Tue, 30 Mar 2021 15:18:10 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Rossell", "David", ""], ["Rubio", "Francisco Javier", ""]]}, {"id": "1907.13593", "submitter": "Robert McCann", "authors": "Tongseok Lim and Robert J McCann", "title": "Isodiametry, variance, and regular simplices from particle interactions", "comments": "22 pages, 5 figures. V4 differs from V3 by the inclusion of Remark\n  1.8, three references, and a smaller font. V3 differs from V2 by the a\n  reference to work of Sun et al discussed in Remarks 1.7 and 4.5. V2 differs\n  from V1 by the inclusion on an appendix referencing work of Jung (1901) and\n  the removal of some material into the separate preprint. arXiv:2001.11851", "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph math.AP math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a collection of particles interacting through an\nattractive-repulsive potential given as a difference of power laws and\nnormalized so that its unique minimum occurs at unit separation. For a range of\nexponents corresponding to mild repulsion and strong attraction, we show that\nthe minimum energy configuration is uniquely attained -- apart from\ntranslations and rotations -- by equidistributing the particles over the\nvertices of a regular top-dimensional simplex (i.e. an equilateral triangle in\ntwo dimensions and regular tetrahedron in three). If the attraction is not\nassumed to be strong, we show these configurations are at least local energy\nminimizers in the relevant $d_\\infty$ metric from optimal transportation, as\nare all of the other uncountably many unbalanced configurations with the same\nsupport. We infer the existence of phase transitions. The proof is based on a\nsimple isodiametric variance bound which characterizes regular simplices: it\nshows that among probability measures on ${\\mathbf R}^n$ whose supports have at\nmost unit diameter, the variance around the mean is maximized precisely by\nthose measures which assign mass $1/(n+1)$ to each vertex of a (unit-diameter)\nregular simplex.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 16:51:45 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 20:51:29 GMT"}, {"version": "v3", "created": "Sat, 29 Feb 2020 23:24:42 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2021 21:45:26 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Lim", "Tongseok", ""], ["McCann", "Robert J", ""]]}, {"id": "1907.13602", "submitter": "Richard Kueng", "authors": "Richard Kueng and Joel A. Tropp", "title": "Binary component decomposition Part II: The asymmetric case", "comments": "18(+9) pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of decomposing a low-rank matrix into a factor\nwith binary entries, either from $\\{\\pm 1\\}$ or from $\\{0,1\\}$, and an\nunconstrained factor. The research answers fundamental questions about the\nexistence and uniqueness of these decompositions. It also leads to tractable\nfactorization algorithms that succeed under a mild deterministic condition.\nThis work builds on a companion paper that addresses the related problem of\ndecomposing a low-rank positive-semidefinite matrix into symmetric binary\nfactors.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 17:07:08 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Kueng", "Richard", ""], ["Tropp", "Joel A.", ""]]}, {"id": "1907.13603", "submitter": "Richard Kueng", "authors": "Richard Kueng and Joel A. Tropp", "title": "Binary Component Decomposition Part I: The Positive-Semidefinite Case", "comments": "21(+4) pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.MG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of decomposing a low-rank\npositive-semidefinite matrix into symmetric factors with binary entries, either\n$\\{\\pm 1\\}$ or $\\{0,1\\}$. This research answers fundamental questions about the\nexistence and uniqueness of these decompositions. It also leads to tractable\nfactorization algorithms that succeed under a mild deterministic condition. A\ncompanion paper addresses the related problem of decomposing a low-rank\nrectangular matrix into a binary factor and an unconstrained factor.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 17:07:13 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Kueng", "Richard", ""], ["Tropp", "Joel A.", ""]]}, {"id": "1907.13616", "submitter": "Krishnakumar Balasubramanian", "authors": "Abhishek Roy, Krishnakumar Balasubramanian, Saeed Ghadimi, Prasant\n  Mohapatra", "title": "Multi-Point Bandit Algorithms for Nonstationary Online Nonconvex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandit algorithms have been predominantly analyzed in the convex setting with\nfunction-value based stationary regret as the performance measure. In this\npaper, motivated by online reinforcement learning problems, we propose and\nanalyze bandit algorithms for both general and structured nonconvex problems\nwith nonstationary (or dynamic) regret as the performance measure, in both\nstochastic and non-stochastic settings. First, for general nonconvex functions,\nwe consider nonstationary versions of first-order and second-order stationary\nsolutions as a regret measure, motivated by similar performance measures for\noffline nonconvex optimization. In the case of second-order stationary solution\nbased regret, we propose and analyze online and bandit versions of the cubic\nregularized Newton's method. The bandit version is based on estimating the\nHessian matrices in the bandit setting, based on second-order Gaussian Stein's\nidentity. Our nonstationary regret bounds in terms of second-order stationary\nsolutions have interesting consequences for avoiding saddle points in the\nbandit setting. Next, for weakly quasi convex functions and monotone weakly\nsubmodular functions we consider nonstationary regret measures in terms of\nfunction-values; such structured classes of nonconvex functions enable one to\nconsider regret measure defined in terms of function values, similar to convex\nfunctions. For this case of function-value, and first-order stationary solution\nbased regret measures, we provide regret bounds in both the low- and\nhigh-dimensional settings, for some scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 17:32:07 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 07:06:09 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Roy", "Abhishek", ""], ["Balasubramanian", "Krishnakumar", ""], ["Ghadimi", "Saeed", ""], ["Mohapatra", "Prasant", ""]]}, {"id": "1907.13630", "submitter": "Bryan Graham", "authors": "Bryan S. Graham, Fengshi Niu and James L. Powell", "title": "Kernel Density Estimation for Undirected Dyadic Data", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonparametric estimation of density functions for undirected dyadic\nrandom variables (i.e., random variables defined for all\nn\\overset{def}{\\equiv}\\tbinom{N}{2} unordered pairs of agents/nodes in a\nweighted network of order N). These random variables satisfy a local dependence\nproperty: any random variables in the network that share one or two indices may\nbe dependent, while those sharing no indices in common are independent. In this\nsetting, we show that density functions may be estimated by an application of\nthe kernel estimation method of Rosenblatt (1956) and Parzen (1962). We suggest\nan estimate of their asymptotic variances inspired by a combination of (i)\nNewey's (1994) method of variance estimation for kernel estimators in the\n\"monadic\" setting and (ii) a variance estimator for the (estimated) density of\na simple network first suggested by Holland and Leinhardt (1976). More unusual\nare the rates of convergence and asymptotic (normal) distributions of our\ndyadic density estimates. Specifically, we show that they converge at the same\nrate as the (unconditional) dyadic sample mean: the square root of the number,\nN, of nodes. This differs from the results for nonparametric estimation of\ndensities and regression functions for monadic data, which generally have a\nslower rate of convergence than their corresponding sample mean.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jul 2019 17:55:08 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Graham", "Bryan S.", ""], ["Niu", "Fengshi", ""], ["Powell", "James L.", ""]]}]