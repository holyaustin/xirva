[{"id": "2005.00035", "submitter": "Sourabh Bhattacharya", "authors": "Sucharita Roy and Sourabh Bhattacharya", "title": "Bayesian Characterizations of Properties of Stochastic Processes with\n  Applications", "comments": "Not yet complete, much work still needed. Feedback welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we primarily propose a novel Bayesian characterization of\nstationary and nonstationary stochastic processes. In practice, this theory\naims to distinguish between global stationarity and nonstationarity for both\nparametric and nonparametric stochastic processes. Interestingly, our theory\nbuilds on our previous work on Bayesian characterization of infinite series,\nwhich was applied to verification of the (in)famous Riemann Hypothesis. Thus,\nthere seems to be interesting and important connections between pure\nmathematics and Bayesian statistics, with respect to our proposed ideas. We\nvalidate our proposed method with simulation and real data experiments\nassociated with different setups. In particular, applications of our method\ninclude stationarity and nonstationarity determination in various time series\nmodels, spatial and spatio-temporal setups, and convergence diagnostics of\nMarkov Chain Monte Carlo. Our results demonstrate very encouraging performance,\neven in very subtle situations. Using similar principles, we also provide a\nnovel Bayesian characterization of mutual independence among any number of\nrandom variables, using which we characterize the properties of point\nprocesses, including characterizations of Poisson point processes, complete\nspatial randomness, stationarity and nonstationarity. Applications to\nsimulation experiments with ample Poisson and non-Poisson point process models\nagain indicate quite encouraging performance of our proposed ideas. We further\npropose a novel recursive Bayesian method for determination of frequencies of\noscillatory stochastic processes, based on our general principle. Simulation\nstudies and real data experiments with varieties of time series models\nconsisting of single and multiple frequencies bring out the worth of our\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 18:05:17 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Roy", "Sucharita", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2005.00066", "submitter": "Sourabh Bhattacharya", "authors": "Noirrit Kiran Chandra and Sourabh Bhattacharya", "title": "High-dimensional Asymptotic Theory of Bayesian Multiple Testing\n  Procedures Under General Dependent Setup and Possible Misspecification", "comments": "Feedback welcome. arXiv admin note: text overlap with\n  arXiv:1611.01369", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate the asymptotic properties of Bayesian\nmultiple testing procedures under general dependent setup, when the sample size\nand the number of hypotheses both tend to infinity. Specifically, we\ninvestigate strong consistency of the procedures and asymptotic properties of\ndifferent versions of false discovery and false non-discovery rates under the\nhigh dimensional setup. We particularly focus on a novel Bayesian non-marginal\nmultiple testing procedure and its associated error rates in this regard. Our\nresults show that the asymptotic convergence rates of the error rates are\ndirectly associated with the Kullback-Leibler divergence from the true model,\nand the results hold even when the postulated class of models is misspecified.\nFor illustration of our high-dimensional asymptotic theory, we consider a\nBayesian variable selection problem in a time-varying covariate selection\nframework, with autoregressive response variables. We particularly focus on the\nsetup where the number of hypotheses increases at a faster rate compared to the\nsample size, which is the so-called ultra-high dimensional situation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 19:28:27 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Chandra", "Noirrit Kiran", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2005.00188", "submitter": "Andriy Olenko", "authors": "Andriy Olenko, Dareen Omari", "title": "Reduction principle for functionals of strong-weak dependent vector\n  random fields", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the reduction principle for asymptotics of functionals of vector\nrandom fields with weakly and strongly dependent components. These functionals\ncan be used to construct new classes of random fields with skewed and\nheavy-tailed distributions. Contrary to the case of scalar long-range dependent\nrandom fields, it is shown that the asymptotic behaviour of such functionals is\nnot necessarily determined by the terms at their Hermite rank. The results are\nillustrated by an application to the first Minkowski functional of the Student\nrandom fields. Some simulation studies based on the theoretical findings are\nalso presented.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 02:56:01 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Olenko", "Andriy", ""], ["Omari", "Dareen", ""]]}, {"id": "2005.00234", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee and Sourabh Bhattacharya", "title": "Posterior Convergence of Nonparametric Binary and Poisson Regression\n  Under Possible Misspecifications", "comments": "Feedback welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate posterior convergence of nonparametric binary\nand Poisson regression under possible model misspecification, assuming general\nstochastic process prior with appropriate properties. Our model setup and\nobjective for binary regression is similar to that of Ghosal and Roy (2006)\nwhere the authors have used the approach of entropy bound and exponentially\nconsistent tests with the sieve method to achieve consistency with respect to\ntheir Gaussian process prior. In contrast, for both binary and Poisson\nregression, using general stochastic process prior, our approach involves\nverification of asymptotic equipartition property along with the method of\nsieve, which is a manoeuvre of the general results of Shalizi (2009), useful\neven for misspecified models. Moreover, we will establish not only posterior\nconsistency but also the rates at which the posterior probabilities converge,\nwhich turns out to be the Kullback-Leibler divergence rate. We also investgate\nthe traditional posterior convergence rates. Interestingly, from subjective\nBayesian viewpoint we will show that the posterior predictive distribution can\naccurately approximate the best possible predictive distribution in the sense\nthat the Hellinger distance, as well as the total variation distance between\nthe two distributions can tend to zero, in spite of misspecifications.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 05:55:19 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2005.00236", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee and Sourabh Bhattacharya", "title": "Posterior Consistency of Bayesian Inverse Regression and Inverse\n  Reference Distributions", "comments": "Feedback welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian inference in inverse regression problems where the\nobjective is to infer about unobserved covariates from observed responses and\ncovariates. We establish posterior consistency of such unobserved covariates in\nBayesian inverse regression problemsunder appropriate priors in a leave-one-out\ncross-validation setup. We relate this to posterior consistency of inverse\nreference distributions (Bhattacharya (2013)) for assessing model adequacy. We\nillustrate our theory and methods with various examples of Bayesian inverse\nregression, along with adequate simulation experiments.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 06:04:18 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2005.00511", "submitter": "Fan Yang", "authors": "Fan Yang, Sifan Liu, Edgar Dobriban, David P. Woodruff", "title": "How to reduce dimension with PCA and random projections?", "comments": "56 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In our \"big data\" age, the size and complexity of data is steadily\nincreasing. Methods for dimension reduction are ever more popular and useful.\nTwo distinct types of dimension reduction are \"data-oblivious\" methods such as\nrandom projections and sketching, and \"data-aware\" methods such as principal\ncomponent analysis (PCA). Both have their strengths, such as speed for random\nprojections, and data-adaptivity for PCA. In this work, we study how to combine\nthem to get the best of both. We study \"sketch and solve\" methods that take a\nrandom projection (or sketch) first, and compute PCA after. We compute the\nperformance of several popular sketching methods (random iid projections,\nrandom sampling, subsampled Hadamard transform, count sketch, etc) in a general\n\"signal-plus-noise\" (or spiked) data model. Compared to well-known works, our\nresults (1) give asymptotically exact results, and (2) apply when the signal\ncomponents are only slightly above the noise, but the projection dimension is\nnon-negligible. We also study stronger signals allowing more general covariance\nstructures. We find that (a) signal strength decreases under projection in a\ndelicate way depending on the structure of the data and the sketching method,\n(b) orthogonal projections are more accurate, (c) randomization does not hurt\ntoo much, due to concentration of measure, (d) count sketch can be improved by\na normalization method. Our results have implications for statistical learning\nand data analysis. We also illustrate that the results are highly accurate in\nsimulations and in analyzing empirical data.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 17:30:02 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 17:12:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yang", "Fan", ""], ["Liu", "Sifan", ""], ["Dobriban", "Edgar", ""], ["Woodruff", "David P.", ""]]}, {"id": "2005.00610", "submitter": "Joris Mooij", "authors": "Joris M. Mooij and Tom Claassen", "title": "Constraint-Based Causal Discovery using Partial Ancestral Graphs in the\n  presence of Cycles", "comments": "Major revision. To appear in Proceedings of the 36 th Conference on\n  Uncertainty in Artificial Intelligence (UAI), PMLR volume 124, 2020", "journal-ref": "Proceedings of Machine Learning Research 124 (2020) 1159-1168", "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While feedback loops are known to play important roles in many complex\nsystems, their existence is ignored in a large part of the causal discovery\nliterature, as systems are typically assumed to be acyclic from the outset.\nWhen applying causal discovery algorithms designed for the acyclic setting on\ndata generated by a system that involves feedback, one would not expect to\nobtain correct results. In this work, we show that---surprisingly---the output\nof the Fast Causal Inference (FCI) algorithm is correct if it is applied to\nobservational data generated by a system that involves feedback. More\nspecifically, we prove that for observational data generated by a simple and\n$\\sigma$-faithful Structural Causal Model (SCM), FCI is sound and complete, and\ncan be used to consistently estimate (i) the presence and absence of causal\nrelations, (ii) the presence and absence of direct causal relations, (iii) the\nabsence of confounders, and (iv) the absence of specific cycles in the causal\ngraph of the SCM. We extend these results to constraint-based causal discovery\nalgorithms that exploit certain forms of background knowledge, including the\ncausally sufficient setting (e.g., the PC algorithm) and the Joint Causal\nInference setting (e.g., the FCI-JCI algorithm).\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2020 21:10:31 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 21:28:26 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Mooij", "Joris M.", ""], ["Claassen", "Tom", ""]]}, {"id": "2005.00934", "submitter": "Kengne William", "authors": "William Kengne and Isidore S\\'eraphin Ngongo", "title": "Inference for nonstationary time series of counts with application to\n  change-point problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an integer-valued time series $Y=(Y_t)_{t\\in\\Z}$ where the models\nafter a time $k^*$ is Poisson autoregressive with the conditional mean that\ndepends on a parameter $\\theta^*\\in\\Theta\\subset\\R^d$. The structure of the\nprocess before $k^*$ is unknown;? it could be any other integer-valued time\nseries, that is, the process $Y$ could be nonstationary.? It is established\nthat the maximum likelihood estimator of $\\theta^*$ computed on the\nnonstationary observations is consistent and asymptotically normal. Next, we\ncarry out the sequential change-point detection in a large class of Poisson\nautoregressive models. We propose a monitoring scheme for detecting change in\nthe model. The procedure is based on an updated estimator which is computed\nwithout the historical observations. The asymptotic behavior of the detector is\nstudied, in particular, the above result on the inference in a nonstationary\nsetting are applied to prove that the proposed procedure is consistent. A\nsimulation study as well as a real data application are provided.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2020 22:41:15 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Kengne", "William", ""], ["Ngongo", "Isidore S\u00e9raphin", ""]]}, {"id": "2005.00971", "submitter": "Esam Mahdi", "authors": "Esam Mahdi", "title": "A Powerful Portmanteau Test for Detecting Nonlinearity in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new portmanteau test statistic is proposed for detecting nonlinearity in\ntime series data. In this paper, we elaborate on the Toeplitz autocorrelation\nmatrix to the autocorrelation and cross-correlation of residuals and squared\nresiduals block matrix. We derive a new portmanteau test statistic using the\nlog of the determinant of the mth autocorrelations and cross-correlations block\nmatrix. The asymptotic distribution of the proposed test statistic is derived\nas a linear combination of chi-squared distributions and can be approximated by\na gamma distribution. This test is applied to identify the linearity and\nnonlinearity dependency of some stationary time series models. It is shown that\nthe convergence of the new test to its asymptotic distribution is reasonable\nwith higher power than other tests in many situations. We demonstrate the\nefficiency of the proposed test by investigating linear and nonlinear effects\nin Vodafone Qatar and Nikkei-300 daily returns.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 02:56:08 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Mahdi", "Esam", ""]]}, {"id": "2005.00999", "submitter": "Fan Yang", "authors": "Fan Yang", "title": "Linear spectral statistics of eigenvectors of anisotropic sample\n  covariance matrices", "comments": "60 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider sample covariance matrices of the form $Q:=\\Sigma^{1/2} X X^*\n\\Sigma^{1/2}$, where $X=(x_{ij})$ is an $n\\times N$ random matrix whose entries\nare independent random variables with mean zero and variance $N^{-1}$, and\n$\\Sigma$ is a deterministic positive-definite matrix. We study the limiting\nbehavior of the eigenvectors of $Q$ through the so-called eigenvector empirical\nspectral distribution (VESD) $F_{\\mathbf u}$, which is an alternate form of\nempirical spectral distribution with weights given by $|\\mathbf u^\\top\n\\xi_k|^2$, where $\\mathbf u$ is any deterministic unit vector and $\\xi_k$ are\nthe eigenvectors of $Q$. We prove a functional central limit theorem for the\nlinear spectral statistics of $F_{\\mathbf u}$, indexed by functions with\nH{\\\"o}lder continuous derivatives. We show that the linear spectral statistics\nconverge to universal Gaussian processes both on global scales of order 1, and\non local scales that are much smaller than 1 and much larger than the typical\neigenvalues spacing $N^{-1}$. Moreover, we give explicit expressions for the\nmeans and covariance functions of the Gaussian processes, where the exact\ndependence on $\\Sigma$ and $\\mathbf u$ allows for more flexibility in the\napplications of VESD in statistical estimations of sample covariance matrices.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 06:17:19 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Yang", "Fan", ""]]}, {"id": "2005.01058", "submitter": "Emmanuel Caron", "authors": "Emmanuel Caron, J\\'er\\^ome Dedecker, Bertrand Michel", "title": "Gaussian linear model selection in a dependent context", "comments": "30 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the nonparametric linear model, when the error\nprocess is a dependent Gaussian process. We focus on the estimation of the mean\nvector via a model selection approach. We first give the general theoretical\nform of the penalty function, ensuring that the penalized estimator among a\ncollection of models satisfies an oracle inequality. Then we derive a penalty\nshape involving the spectral radius of the covariance matrix of the errors,\nwhich can be chosen proportional to the dimension when the error process is\nstationary and short range dependent. However, this penalty can be too rough in\nsome cases, in particular when the error process is long range dependent. In a\nsecond part, we focus on the fixed-design regression model assuming that the\nerror process is a stationary Gaussian process. We propose a model selection\nprocedure in order to estimate the mean function via piecewise polynomials on a\nregular partition, when the error process is either short range dependent, long\nrange dependent or anti-persistent. We present different kinds of penalties,\ndepending on the memory of the process. For each case, an adaptive estimator is\nbuilt, and the rates of convergence are computed. Thanks to several sets of\nsimulations, we study the performance of these different penalties for all\ntypes of errors (short memory, long memory and anti-persistent errors).\nFinally, we give an application of our method to the well-known Nile data,\nwhich clearly shows that the type of dependence of the error process must be\ntaken into account.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 11:33:06 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Caron", "Emmanuel", ""], ["Dedecker", "J\u00e9r\u00f4me", ""], ["Michel", "Bertrand", ""]]}, {"id": "2005.01168", "submitter": "Liangliang Zhang", "authors": "Yingjie Li, Liangliang Zhang and Tapabrata Maiti", "title": "High Dimensional Classification for Spatially Dependent Data with\n  Application to Neuroimaging", "comments": "58 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminating patients with Alzheimer's disease (AD) from healthy subjects\nis a crucial task in the research of Alzheimer's disease. The task can be\npotentially achieved by linear discriminant analysis (LDA), which is one of the\nmost classical and popular classification techniques. However, the\nclassification problem becomes challenging for LDA because of the\nhigh-dimensionally and the spatial dependency of the brain imaging data. To\naddress the challenges, researchers have proposed various ways to generalize\nLDA into high-dimensional context in recent years. However, these existing\nmethods did not reach any consensus on how to incorporate spatially dependent\nstructure. In light of the current needs and limitations, we propose a new\nclassification method, named as Penalized Maximum Likelihood Estimation LDA\n(PMLE-LDA). The proposed method uses $Mat\\acute{e}rn$ covariance function to\ndescribe the spatial correlation of brain regions. Additionally, PMLE is\ndesigned to model the sparsity of high-dimensional features. The spatial\nlocation information is used to address the singularity of the covariance.\nTapering technique is introduced to reduce computational burden. We show in\ntheory that the proposed method can not only provide consistent results of\nparameter estimation and feature selection, but also generate an asymptotically\noptimal classifier driven by high dimensional data with specific spatially\ndependent structure. Finally, the method is validated through simulations and\nan application into ADNI data for classifying Alzheimer's patients.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2020 19:23:28 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Li", "Yingjie", ""], ["Zhang", "Liangliang", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2005.01316", "submitter": "Koji Tsukuda", "authors": "Koji Tsukuda, Shun Matsuura", "title": "Limit theorem associated with Wishart matrices with application to\n  hypothesis testing for common principal components", "comments": "27 pages. Corrected some errors. Improved presentations", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study derives a new property of the Wishart distribution when the\ndegree-of-freedom and the size of the matrix parameter of the distribution grow\nsimultaneoulsy. Particularly, the asymptotic normality of the product of four\nindependent Wishart matrices is shown under a high dimensional asymptotic\nregime. As an application of the result, a statistical test procedure for the\ncommon principal components hypothesis is proposed. For this problem, the\nproposed test statistic is asymptotically normal under the null hypothesis. In\naddition, the proposed test statistic diverges to positive infinity in\nprobability under the alternative hypothesis.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 08:25:44 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 03:39:38 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Tsukuda", "Koji", ""], ["Matsuura", "Shun", ""]]}, {"id": "2005.01362", "submitter": "Jan van Waaij PhD", "authors": "J. van Waaij and B.J.K. Kleijn", "title": "Uncertainty quantification in the stochastic block model with an unknown\n  number of classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the frequentist properties of Bayesian statistical inference for the\nstochastic block model, with an unknown number of classes of varying sizes. We\nequip the space of vertex labellings with a prior on the number of classes and,\nconditionally, a prior on the labels. The number of classes may grow to\ninfinity as a function of the number of vertices, depending on the sparsity of\nthe graph. We derive non-asymptotic posterior contraction rates of the form\n$P_{\\theta_{0,n}}\\Pi_n(B_n\\mid X^n)\\le \\epsilon_n$, where $X^n$ is the observed\ngraph, generated according to $P_{\\theta_{0,n}}$, $B_n$ is either $\\{\\theta_{0,\nn}\\}$ or, in the very sparse case, a ball around $\\theta_{0,n}$ of known\nextent, and $\\epsilon_n$ is an explicit rate of convergence.\n  These results enable conversion of credible sets to confidence sets. In the\nsparse case, credible tests are shown to be confidence sets. In the very sparse\ncase, credible sets are enlarged to form confidence sets. Confidence levels are\nexplicit, for each $n$, as a function of the credible level and the rate of\nconvergence.\n  Hypothesis testing between the number of classes is considered with the help\nof posterior odds, and is shown to be consistent. Explicit upper bounds on\nerrors of the first and second type and an explicit lower bound on the power of\nthe tests are given.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 10:16:45 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["van Waaij", "J.", ""], ["Kleijn", "B. J. K.", ""]]}, {"id": "2005.01378", "submitter": "Yu Cheng", "authors": "Yu Cheng, Ilias Diakonikolas, Rong Ge, Mahdi Soltanolkotabi", "title": "High-Dimensional Robust Mean Estimation via Gradient Descent", "comments": "Under submission to ICML'20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of high-dimensional robust mean estimation in the\npresence of a constant fraction of adversarial outliers. A recent line of work\nhas provided sophisticated polynomial-time algorithms for this problem with\ndimension-independent error guarantees for a range of natural distribution\nfamilies.\n  In this work, we show that a natural non-convex formulation of the problem\ncan be solved directly by gradient descent. Our approach leverages a novel\nstructural lemma, roughly showing that any approximate stationary point of our\nnon-convex objective gives a near-optimal solution to the underlying robust\nestimation task. Our work establishes an intriguing connection between\nalgorithmic high-dimensional robust statistics and non-convex optimization,\nwhich may have broader applications to other robust estimation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 10:48:04 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Cheng", "Yu", ""], ["Diakonikolas", "Ilias", ""], ["Ge", "Rong", ""], ["Soltanolkotabi", "Mahdi", ""]]}, {"id": "2005.01417", "submitter": "Benjamin Roycraft", "authors": "Benjamin Roycraft, Johannes Krebs, Wolfgang Polonik", "title": "Bootstrapping Persistent Betti Numbers and Other Stabilizing Statistics", "comments": "59 pages, 3 figures. Restructured paper with alternate problem\n  settings moved to appendix. Rewrote data analysis and simulations study\n  sections to be more comprehensive, moved each to the end of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present contribution investigates multivariate bootstrap procedures for\ngeneral stabilizing statistics, with specific application to topological data\nanalysis. Existing limit theorems for topological statistics prove difficult to\nuse in practice for the construction of confidence intervals, motivating the\nuse of the bootstrap in this capacity. However, the standard nonparametric\nbootstrap does not directly provide for asymptotically valid confidence\nintervals in some situations. A smoothed bootstrap procedure, instead, is shown\nto give consistent estimation in these settings. The present work relates to\nother general results in the area of stabilizing statistics, including central\nlimit theorems for functionals of Poisson and Binomial processes in the\ncritical regime. Specific statistics considered include the persistent Betti\nnumbers of \\v{C}ech and Vietoris-Rips complexes over point sets in $\\mathbb\nR^d$, along with Euler characteristics, and the total edge length of the\n$k$-nearest neighbor graph. Special emphasis is made throughout to weakening\nthe necessary conditions needed to establish bootstrap consistency. In\nparticular, the assumption of a continuous underlying density is not required.\nA simulation study is provided to assess the performance of the smoothed\nbootstrap for finite sample sizes, and the method is further applied to the\ncosmic web dataset from the Sloan Digital Sky Survey (SDSS). Source code is\navailable at github.com/btroycraft/stabilizing_statistics_bootstrap.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 12:17:03 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 08:34:25 GMT"}, {"version": "v3", "created": "Fri, 26 Mar 2021 00:58:06 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Roycraft", "Benjamin", ""], ["Krebs", "Johannes", ""], ["Polonik", "Wolfgang", ""]]}, {"id": "2005.01559", "submitter": "Wenjia Wang", "authors": "Wenjia Wang, Yi-Hui Zhou", "title": "Reduced Rank Multivariate Kernel Ridge Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the multivariate regression, also referred to as multi-task learning in\nmachine learning, the goal is to recover a vector-valued function based on\nnoisy observations. The vector-valued function is often assumed to be of low\nrank. Although the multivariate linear regression is extensively studied in the\nliterature, a theoretical study on the multivariate nonlinear regression is\nlacking. In this paper, we study reduced rank multivariate kernel ridge\nregression, proposed by \\cite{mukherjee2011reduced}. We prove the consistency\nof the function predictor and provide the convergence rate. An algorithm based\non nuclear norm relaxation is proposed. A few numerical examples are presented\nto show the smaller mean squared prediction error comparing with the\nelementwise univariate kernel ridge regression.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:19:04 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Wang", "Wenjia", ""], ["Zhou", "Yi-Hui", ""]]}, {"id": "2005.01580", "submitter": "Navin Kashyap", "authors": "Navin Kashyap and Manjunath Krishnapur", "title": "How Many Modes Can a Mixture of Gaussians with Uniformly Bounded Means\n  Have?", "comments": "11 pages, 1 figure; this version is currently under review at\n  Information and Inference: A Journal of the IMA", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show, by an explicit construction, that a mixture of univariate Gaussian\ndensities with variance $1$ and means in $[-A,A]$ can have $\\Omega(A^2)$ modes.\nThis disproves a recent conjecture of Dytso, Yagli, Poor and Shamai\n\\cite{DYPS20} who showed that such a mixture can have at most $O(A^{2})$ modes\nand surmised that the upper bound could be improved to $O(A)$. Our result holds\neven if an additional variance constraint is imposed on the mixing\ndistribution. Extending the result to higher dimensions, we exhibit a mixture\nof Gaussians in $\\mathbb{R}^{d}$, with identity covariances and means inside\n$[-A,A]^{d}$, that has $\\Omega(A^{2d})$ modes.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2020 15:49:36 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 19:52:33 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 15:19:33 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kashyap", "Navin", ""], ["Krishnapur", "Manjunath", ""]]}, {"id": "2005.01895", "submitter": "Ping-Shou Zhong", "authors": "Shawn Santo and Ping-Shou Zhong", "title": "Homogeneity Tests of Covariance and Change-Points Identification for\n  High-Dimensional Functional Data", "comments": "The paper has 32 pages with 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference problems for high-dimensional (HD) functional data with\na dense number (T) of repeated measurements taken for a large number of p\nvariables from a small number of n experimental units. The spatial and temporal\ndependence, high dimensionality, and the dense number of repeated measurements\nall make theoretical studies and computation challenging. This paper has two\naims; our first aim is to solve the theoretical and computational challenges in\ndetecting and identifying change points among covariance matrices from HD\nfunctional data. The second aim is to provide computationally efficient and\ntuning-free tools with a guaranteed stochastic error control. The change point\ndetection procedure is developed in the form of testing the homogeneity of\ncovariance matrices. The weak convergence of the stochastic process formed by\nthe test statistics is established under the \"large p, large T and small n\"\nsetting. Under a mild set of conditions, our change point identification\nestimator is proven to be consistent for change points in any location of a\nsequence. Its rate of convergence depends on the data dimension, sample size,\nnumber of repeated measurements, and signal-to-noise ratio. We also show that\nour proposed computation algorithms can significantly reduce the computation\ntime and are applicable to real-world data such as fMRI data with a large\nnumber of HD repeated measurements. Simulation results demonstrate both finite\nsample performance and computational effectiveness of our proposed procedures.\nWe observe that the empirical size of the test is well controlled at the\nnominal level, and the locations of multiple change points can accurately be\nidentified. An application to fMRI data demonstrates that our proposed methods\ncan identify event boundaries in the preface of the movie Sherlock. Our\nproposed procedures are implemented in an R package TechPhD.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 01:04:58 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Santo", "Shawn", ""], ["Zhong", "Ping-Shou", ""]]}, {"id": "2005.01899", "submitter": "Zhou Zhou", "authors": "Zhou Zhou, Yang-Guan-Jian Guo and Hau-Tieng Wu", "title": "Frequency Detection and Change Point Estimation for Time Series of\n  Complex Oscillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider detecting the evolutionary oscillatory pattern of a signal when\nit is contaminated by non-stationary noises with complexly time-varying data\ngenerating mechanism. A high-dimensional dense progressive periodogram test is\nproposed to accurately detect all oscillatory frequencies. A further\nphase-adjusted local change point detection algorithm is applied in the\nfrequency domain to detect the locations at which the oscillatory pattern\nchanges. Our method is shown to be able to detect all oscillatory frequencies\nand the corresponding change points within an accurate range with a prescribed\nprobability asymptotically. This study is motivated by oscillatory frequency\nestimation and change point detection problems encountered in physiological\ntime series analysis. An application to spindle detection and estimation in\nsleep EEG data is used to illustrate the usefulness of the proposed\nmethodology. A Gaussian approximation scheme and an overlapping-block\nmultiplier bootstrap methodology for sums of complex-valued high dimensional\nnon-stationary time series without variance lower bounds are established, which\ncould be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 01:20:04 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Zhou", "Zhou", ""], ["Guo", "Yang-Guan-Jian", ""], ["Wu", "Hau-Tieng", ""]]}, {"id": "2005.02151", "submitter": "Vince Lyzinski", "authors": "Keith Levin, Carey E. Priebe, Vince Lyzinski", "title": "On the role of features in vertex nomination: Content and context\n  together are better (sometimes)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertex nomination is a lightly-supervised network information retrieval (IR)\ntask in which vertices of interest in one graph are used to query a second\ngraph to discover vertices of interest in the second graph. Similar to other IR\ntasks, the output of a vertex nomination scheme is a ranked list of the\nvertices in the second graph, with the heretofore unknown vertices of interest\nideally concentrating at the top of the list. Vertex nomination schemes provide\na useful suite of tools for efficiently mining complex networks for pertinent\ninformation. In this paper, we explore, both theoretically and practically, the\ndual roles of content (i.e., edge and vertex attributes) and context (i.e.,\nnetwork topology) in vertex nomination. We provide necessary and sufficient\nconditions under which vertex nomination schemes that leverage both content and\ncontext outperform schemes that leverage only content or context separately.\nWhile the joint utility of both content and context has been demonstrated\nempirically in the literature, the framework presented in this paper provides a\nnovel theoretical basis for understanding the potential complementary roles of\nnetwork features and topology.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 15:13:24 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 13:01:43 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Levin", "Keith", ""], ["Priebe", "Carey E.", ""], ["Lyzinski", "Vince", ""]]}, {"id": "2005.02511", "submitter": "Benjamin Draves", "authors": "Benjamin Draves and Daniel L. Sussman", "title": "Bias-Variance Tradeoffs in Joint Spectral Embeddings", "comments": "41 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent position models and their corresponding estimation procedures offer a\nstatistically principled paradigm for multiple network inference by translating\nmultiple network analysis problems to familiar task in multivariate statistics.\nLatent position estimation is a fundamental task in this framework yet most\nwork focus only on unbiased estimation procedures. We consider the\nramifications of utilizing biased latent position estimates in subsequent\nstatistical analysis in exchange for sizable variance reductions in finite\nnetworks. We establish an explicit bias-variance tradeoff for latent position\nestimates produced by the omnibus embedding of arXiv:1705.09355 in the presence\nof heterogeneous network data. We reveal an analytic bias expression, derive a\nuniform concentration bound on the residual term, and prove a central limit\ntheorem characterizing the distributional properties of these estimates. These\nexplicit bias and variance expressions enable us to show that the omnibus\nembedding estimates are often preferable to comparable estimators with respect\nto mean square error, state sufficient conditions for exact recovery in\ncommunity detection tasks, and develop a test statistic to determine whether\ntwo graphs share the same set of latent positions. These results are\ndemonstrated in several experimental settings where community detection\nalgorithms and hypothesis testing procedures utilizing the biased latent\nposition estimates are competitive, and oftentimes preferable, to unbiased\nlatent position estimates.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 21:45:24 GMT"}, {"version": "v2", "created": "Mon, 11 May 2020 13:57:50 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Draves", "Benjamin", ""], ["Sussman", "Daniel L.", ""]]}, {"id": "2005.02532", "submitter": "Yasutaka Shimizu", "authors": "Yasutaka Shimizu", "title": "Asymptotic distributions for estimated expected functionals of general\n  random elements", "comments": "Some simulation results are provided", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an estimation problem of expected functionals of a general random\nelement that values in a metric space. If the functional forms an explicit\nfunction of some unknown parameters, we can estimate it by plugging-in a\nsuitable estimator in to the function, and we can find the asymptotic\ndistribution. However, if the functional is implicit in the parameters, it\ncauses a problem of specifying asymptotic distribution. This paper gives a\ngeneral condition to specify the asymptotic distribution even if the functional\nis implicit in the parameters, and further investigates it in detail when the\nrandom elements are semimartingales.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2020 23:23:13 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 08:16:40 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 06:36:27 GMT"}, {"version": "v4", "created": "Sat, 11 Jul 2020 03:12:53 GMT"}, {"version": "v5", "created": "Tue, 14 Jul 2020 23:27:50 GMT"}, {"version": "v6", "created": "Tue, 1 Sep 2020 02:06:47 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Shimizu", "Yasutaka", ""]]}, {"id": "2005.02879", "submitter": "\\'Eric Marchand", "authors": "Aziz LMoudden and \\'Eric Marchand", "title": "Bayesian estimation and prediction for certain mixtures", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two vast families of mixture distributions and a given prior, we provide\nunified representations of posterior and predictive distributions. Model\napplications presented include bivariate mixtures of Gamma distributions\nlabelled as Kibble-type, non-central Chi-square and F distributions, the\ndistribution of $R^2$ in multiple regression, variance mixture of normal\ndistributions, and mixtures of location-scale exponential distributions\nincluding the multivariate Lomax distribution. An emphasis is also placed on\nanalytical representations and the relationships with a host of existing\ndistributions and several hypergeomtric functions of one or two variables.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 14:54:06 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 21:22:19 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["LMoudden", "Aziz", ""], ["Marchand", "\u00c9ric", ""]]}, {"id": "2005.02889", "submitter": "St\\'ephanie van der Pas", "authors": "Isma\\\"el Castillo and St\\'ephanie van der Pas", "title": "Multiscale Bayesian Survival Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian nonparametric inference in the right-censoring survival\nmodel, where modeling is made at the level of the hazard rate. We derive\nposterior limiting distributions for linear functionals of the hazard, and then\nfor `many' functionals simultaneously in appropriate multiscale spaces. As an\napplication, we derive Bernstein-von Mises theorems for the cumulative hazard\nand survival functions, which lead to asymptotically efficient confidence bands\nfor these quantities. Further, we show optimal posterior contraction rates for\nthe hazard in terms of the supremum norm. In medical studies, a popular\napproach is to model hazards a priori as random histograms with possibly\ndependent heights. This and more general classes of arbitrarily smooth prior\ndistributions are considered as applications of our theory. A sampler is\nprovided for possibly dependent histogram posteriors. Its finite sample\nproperties are investigated on both simulated and real data experiments.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 15:04:34 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 08:41:59 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 14:46:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Castillo", "Isma\u00ebl", ""], ["van der Pas", "St\u00e9phanie", ""]]}, {"id": "2005.02912", "submitter": "Oskar Laverny", "authors": "Oskar Laverny, Esterina Masiello, V\\'eronique Maume-Deschamps and\n  Didier Rulli\\`ere", "title": "Dependence structure estimation using Copula Recursive Trees", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmva.2021.104776", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct the COpula Recursive Tree (CORT) estimator: a flexible,\nconsistent, piecewise linear estimator of a copula, leveraging the patchwork\ncopula formalization and various piecewise constant density estimators. While\nthe patchwork structure imposes a grid, the CORT estimator is data-driven and\nconstructs the (possibly irregular) grid recursively from the data, minimizing\na chosen distance on the copula space. The addition of the copula constraints\nmakes usual density estimators unusable, whereas the CORT estimator is only\nconcerned with dependence and guarantees the uniformity of margins. Refinements\nsuch as localized dimension reduction and bagging are developed, analyzed, and\ntested through simulated data.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 15:33:45 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 15:54:04 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Laverny", "Oskar", ""], ["Masiello", "Esterina", ""], ["Maume-Deschamps", "V\u00e9ronique", ""], ["Rulli\u00e8re", "Didier", ""]]}, {"id": "2005.02952", "submitter": "Mickael Albertus", "authors": "Mickael Albertus", "title": "Exponential increase of the power of the independence and homogeneity\n  chi-square tests with auxiliary information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an extension of the work about the exponential increase of the\npower of two non-parametric tests: the $ Z $-test and the chi-square\ngoodness-of-fit test. Subject to having auxiliary information, it is possible\nto improve exponentially relative to the size of the sample the power of the\nfamous chi-square tests of independence and homogeneity. Improving the power of\nthese statistical tests by using auxiliary information makes it possible either\nto reduce the probability of accepting the null hypothesis under the\nalternative hypothesis, or to reduce the size of the sample necessary to reach\na predefined power. The suggested method is computational and some simple\nstatistical applications are presented to illustrate these results. The\nframework of this work is non-parametric, so it can be applied to any kind of\ndata and any area using statistics.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2020 16:47:46 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Albertus", "Mickael", ""]]}, {"id": "2005.03187", "submitter": "Roger Silva Ph.d", "authors": "Wagner Barreto-Souza, Gabriela Oliveira, Roger W.C. Silva", "title": "Convergence and inference for mixed Poisson random sums", "comments": "24", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we obtain the limit distribution for partial sums with a random\nnumber of terms following a class of mixed Poisson distributions. The resulting\nweak limit is a mixing between a normal distribution and an exponential family,\nwhich we call by normal exponential family (NEF) laws. A new stability concept\nis introduced and a relationship between {\\alpha}-stable distributions and NEF\nlaws is established. We propose estimation of the parameters of the NEF models\nthrough the method of moments and also by the maximum likelihood method, which\nis performed via an Expectation-Maximization algorithm. Monte Carlo simulation\nstudies are addressed to check the performance of the proposed estimators and\nan empirical illustration on financial market is presented.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 00:46:03 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Barreto-Souza", "Wagner", ""], ["Oliveira", "Gabriela", ""], ["Silva", "Roger W. C.", ""]]}, {"id": "2005.03223", "submitter": "Francesca Boso", "authors": "Francesca Boso and Daniel M. Tartakovsky", "title": "Learning on dynamic statistical manifolds", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": "10.1098/rspa.2020.0213", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperbolic balance laws with uncertain (random) parameters and inputs are\nubiquitous in science and engineering. Quantification of uncertainty in\npredictions derived from such laws, and reduction of predictive uncertainty via\ndata assimilation, remain an open challenge. That is due to nonlinearity of\ngoverning equations, whose solutions are highly non-Gaussian and often\ndiscontinuous. To ameliorate these issues in a computationally efficient way,\nwe use the method of distributions, which here takes the form of a\ndeterministic equation for spatiotemporal evolution of the cumulative\ndistribution function (CDF) of the random system state, as a means of forward\nuncertainty propagation. Uncertainty reduction is achieved by recasting the\nstandard loss function, i.e., discrepancy between observations and model\npredictions, in distributional terms. This step exploits the equivalence\nbetween minimization of the square error discrepancy and the Kullback-Leibler\ndivergence. The loss function is regularized by adding a Lagrangian constraint\nenforcing fulfillment of the CDF equation. Minimization is performed\nsequentially, progressively updating the parameters of the CDF equation as more\nmeasurements are assimilated.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 03:20:31 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Boso", "Francesca", ""], ["Tartakovsky", "Daniel M.", ""]]}, {"id": "2005.03576", "submitter": "Liron Ravner", "authors": "Yoshiaki Inoue, Liron Ravner, Michel Mandjes", "title": "Estimating customer impatience in a service system with unobserved\n  balking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies a service system in which arriving customers are provided\nwith information about the delay they will experience. Based on this\ninformation they decide to wait for service or to leave the system. The main\nobjective is to estimate the customers' patience-level distribution and the\ncorresponding potential arrival rate, using knowledge of the actual\nqueue-length process only. The main complication, and distinguishing feature of\nour setup, lies in the fact that customers who decide not to join are not\nobserved, but, remarkably, we manage to devise a procedure to estimate the load\nthey would generate. We express our system in terms of a multi-server queue\nwith a Poisson stream of customers, which allows us to evaluate the\ncorresponding likelihood function. Estimating the unknown parameters relying on\na maximum likelihood procedure, we prove strong consistency and derive the\nasymptotic distribution of the estimation error. Several applications and\nextensions of the method are discussed. The performance of our approach is\nfurther assessed through a series of numerical experiments. By fitting\nparameters of hyperexponential and generalized-hyperexponential distributions\nour method provides a robust estimation framework for any continuous\npatience-level distribution.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 16:02:57 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 10:00:11 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Inoue", "Yoshiaki", ""], ["Ravner", "Liron", ""], ["Mandjes", "Michel", ""]]}, {"id": "2005.03622", "submitter": "Alex Dytso", "authors": "Wei Cao, Alex Dytso, Michael Fau{\\ss}, H. Vincent Poor, and Gang Feng", "title": "Nonparametric Estimation of the Fisher Information and Its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimation of the Fisher information for\nlocation from a random sample of size $n$. First, an estimator proposed by\nBhattacharya is revisited and improved convergence rates are derived. Second, a\nnew estimator, termed a clipped estimator, is proposed. Superior upper bounds\non the rates of convergence can be shown for the new estimator compared to the\nBhattacharya estimator, albeit with different regularity conditions. Third,\nboth of the estimators are evaluated for the practically relevant case of a\nrandom variable contaminated by Gaussian noise. Moreover, using Brown's\nidentity, which relates the Fisher information and the minimum mean squared\nerror (MMSE) in Gaussian noise, two corresponding consistent estimators for the\nMMSE are proposed. Simulation examples for the Bhattacharya estimator and the\nclipped estimator as well as the MMSE estimators are presented. The examples\ndemonstrate that the clipped estimator can significantly reduce the required\nsample size to guarantee a specific confidence interval compared to the\nBhattacharya estimator.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:21:56 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Cao", "Wei", ""], ["Dytso", "Alex", ""], ["Fau\u00df", "Michael", ""], ["Poor", "H. Vincent", ""], ["Feng", "Gang", ""]]}, {"id": "2005.03631", "submitter": "Bhaswar Bhattacharya", "authors": "Somabha Mukherjee, Jaesung Son, and Bhaswar B. Bhattacharya", "title": "Phase Transitions of the Maximum Likelihood Estimates in the $p$-Spin\n  Curie-Weiss Model", "comments": "65 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math-ph math.MP math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of parameter estimation in the $p$-spin\nCurie-Weiss model, for $p \\geq 3$. We provide a complete description of the\nlimiting properties of the maximum likelihood (ML) estimates of the inverse\ntemperature and the magnetic field given a single realization from the $p$-spin\nCurie-Weiss model, complementing the well-known results in the 2-spin case\n(Comets and Gidas (1991)). Our results unearth various new phase transitions\nand surprising limit theorems, such as the existence of a 'critical' curve in\nthe parameter space, where the limiting distribution of the ML estimates is a\nmixture with both continuous and discrete components. The number of mixture\ncomponents is either two or three, depending on, among other things, the sign\nof one of the parameters and the parity of $p$. Another interesting revelation\nis the existence of certain 'special' points in the parameter space where the\nML estimates exhibit a superefficiency phenomenon, converging to a non-Gaussian\nlimiting distribution at rate $N^{\\frac{3}{4}}$. As a consequence of our\nanalysis, we obtain limit theorems for the average magnetization, results which\nare of independent interest, that provide key insights into the thermodynamic\nproperties of the model.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 17:36:57 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 04:08:02 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 00:05:59 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Mukherjee", "Somabha", ""], ["Son", "Jaesung", ""], ["Bhattacharya", "Bhaswar B.", ""]]}, {"id": "2005.03662", "submitter": "Kei Kobayashi", "authors": "Phillip Kerger and Kei Kobayashi", "title": "Parameter estimation for one-sided heavy-tailed distributions", "comments": "13 pages, 7 figures, to appear in Statistics & Probability Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stable subordinators, and more general subordinators possessing power law\nprobability tails, have been widely used in the context of subdiffusions, where\nparticles get trapped or immobile in a number of time periods, called constant\nperiods. The lengths of the constant periods follow a one-sided distribution\nwhich involves a parameter between 0 and 1 and whose first moment does not\nexist. This paper constructs an estimator for the parameter, applying the\nmethod of moments to the number of observed constant periods in a fixed time\ninterval. The resulting estimator is asymptotically unbiased and consistent,\nand it is well-suited for situations where multiple observations of the same\nsubdiffusion process are available. We present supporting numerical examples\nand an application to market price data for a low-volume stock.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 16:46:43 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Kerger", "Phillip", ""], ["Kobayashi", "Kei", ""]]}, {"id": "2005.03725", "submitter": "Martin Wainwright", "authors": "Max Rabinovich and Michael I. Jordan and Martin J. Wainwright", "title": "Lower bounds in multiple testing: A framework based on derandomized\n  proxies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large bulk of work in multiple testing has focused on specifying\nprocedures that control the false discovery rate (FDR), with relatively less\nattention being paid to the corresponding Type II error known as the false\nnon-discovery rate (FNR). A line of more recent work in multiple testing has\nbegun to investigate the tradeoffs between the FDR and FNR and to provide lower\nbounds on the performance of procedures that depend on the model structure.\nLacking thus far, however, has been a general approach to obtaining lower\nbounds for a broad class of models. This paper introduces an analysis strategy\nbased on derandomization, illustrated by applications to various concrete\nmodels. Our main result is meta-theorem that gives a general recipe for\nobtaining lower bounds on the combination of FDR and FNR. We illustrate this\nmeta-theorem by deriving explicit bounds for several models, including\ninstances with dependence, scale-transformed alternatives, and\nnon-Gaussian-like distributions. We provide numerical simulations of some of\nthese lower bounds, and show a close relation to the actual performance of the\nBenjamini-Hochberg (BH) algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2020 19:59:51 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["Rabinovich", "Max", ""], ["Jordan", "Michael I.", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "2005.03952", "submitter": "Chris Oates", "authors": "Marina Riabiz, Wilson Chen, Jon Cockayne, Pawel Swietach, Steven A.\n  Niederer, Lester Mackey, Chris. J. Oates", "title": "Optimal Thinning of MCMC Output", "comments": "To appear in the Journal of the Royal Statistical Society, Series B,\n  2021+", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of heuristics to assess the convergence and compress the output of\nMarkov chain Monte Carlo can be sub-optimal in terms of the empirical\napproximations that are produced. Typically a number of the initial states are\nattributed to \"burn in\" and removed, whilst the remainder of the chain is\n\"thinned\" if compression is also required. In this paper we consider the\nproblem of retrospectively selecting a subset of states, of fixed cardinality,\nfrom the sample path such that the approximation provided by their empirical\ndistribution is close to optimal. A novel method is proposed, based on greedy\nminimisation of a kernel Stein discrepancy, that is suitable for problems where\nheavy compression is required. Theoretical results guarantee consistency of the\nmethod and its effectiveness is demonstrated in the challenging context of\nparameter inference for ordinary differential equations. Software is available\nin the Stein Thinning package in Python, R and MATLAB.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 10:54:25 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 18:48:04 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 14:58:49 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Riabiz", "Marina", ""], ["Chen", "Wilson", ""], ["Cockayne", "Jon", ""], ["Swietach", "Pawel", ""], ["Niederer", "Steven A.", ""], ["Mackey", "Lester", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2005.04089", "submitter": "David Preinerstorfer", "authors": "Benedikt M. P\\\"otscher and David Preinerstorfer", "title": "How Reliable are Bootstrap-based Heteroskedasticity Robust Tests?", "comments": "54 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop theoretical finite-sample results concerning the size of wild\nbootstrap-based heteroskedasticity robust tests in linear regression models. In\nparticular, these results provide an efficient diagnostic check, which can be\nused to weed out tests that are unreliable for a given testing problem in the\nsense that they overreject substantially. This allows us to assess the\nreliability of a large variety of wild bootstrap-based tests in an extensive\nnumerical study.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2020 15:06:36 GMT"}], "update_date": "2020-05-11", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Preinerstorfer", "David", ""]]}, {"id": "2005.04362", "submitter": "Pengfei Li", "authors": "Meng Yuan, Pengfei Li and Changbao Wu", "title": "Semiparametric Inference of the Youden Index and the Optimal Cutoff\n  Point under Density Ratio Models", "comments": "37 pages, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Youden index is a popular summary statistic for receiver operating\ncharacteristic curve. It gives the optimal cutoff point of a biomarker to\ndistinguish the diseased and healthy individuals. In this paper, we propose to\nmodel the distributions of a biomarker for individuals in the healthy and\ndiseased groups via a semiparametric density ratio model. Based on this model,\nwe use the maximum empirical likelihood method to estimate the Youden index and\nthe optimal cutoff point. We further establish the asymptotic normality of the\nproposed estimators and construct valid confidence intervals for the Youden\nindex and the corresponding optimal cutoff point. The proposed method\nautomatically covers both cases when there is no lower limit of detection\n(LLOD) and when there is a fixed and finite LLOD for the biomarker. Extensive\nsimulation studies and a real data example are used to illustrate the\neffectiveness of the proposed method and its advantages over the existing\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 03:42:00 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Yuan", "Meng", ""], ["Li", "Pengfei", ""], ["Wu", "Changbao", ""]]}, {"id": "2005.04506", "submitter": "Laba Handique", "authors": "Laba Handique and Subrata Chakraborty", "title": "The Poisson Transmuted-G Family of Distributions: Its Properties and\n  Applications", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper introduces a new family of continuous distributions namely the\nPoison transmuted-G family of distribution is proposed by inducing two addition\nparameter on the base line G distribution. Some of its mathematical properties\nincluding explicit expressions for the moments generating function, order\nstatistics, Probability weighted moments, stress-strength reliability, residual\nlife, reversed residual life, R\\'enyi entropy and mean deviation are derived.\nSome special models of the new family are listed. Estimation of the model\nparameters by the maximum likelihood method is discussed. The advantage of the\nproposed family in data fitting is illustrated by means of two applications to\nfailure time data set.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2020 19:57:14 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Handique", "Laba", ""], ["Chakraborty", "Subrata", ""]]}, {"id": "2005.04549", "submitter": "Huiqin Xin", "authors": "Huiqin Xin, Sihai Dave Zhao", "title": "A nonparametric empirical Bayes approach to covariance matrix estimation", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an empirical Bayes method to estimate high-dimensional covariance\nmatrices. Our procedure centers on vectorizing the covariance matrix and\ntreating matrix estimation as a vector estimation problem. Drawing from the\ncompound decision theory literature, we introduce a new class of decision rules\nthat generalizes several existing procedures. We then use a nonparametric\nempirical Bayes g-modeling approach to estimate the oracle optimal rule in that\nclass. This allows us to let the data itself determine how best to shrink the\nestimator, rather than shrinking in a pre-determined direction such as toward a\ndiagonal matrix. Simulation results and a gene expression network analysis\nshows that our approach can outperform a number of state-of-the-art proposals\nin a wide range of settings, sometimes substantially.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 02:05:11 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 23:00:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xin", "Huiqin", ""], ["Zhao", "Sihai Dave", ""]]}, {"id": "2005.04761", "submitter": "Nestor Parolya Dr.", "authors": "Taras Bodnar, Solomiia Dmytriv, Yarema Okhrin, Nestor Parolya and\n  Wolfgang Schmid", "title": "Statistical inference for the EU portfolio in high dimensions", "comments": "27 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, using the shrinkage-based approach for portfolio weights and\nmodern results from random matrix theory we construct an effective procedure\nfor testing the efficiency of the expected utility (EU) portfolio and discuss\nthe asymptotic behavior of the proposed test statistic under the\nhigh-dimensional asymptotic regime, namely when the number of assets $p$\nincreases at the same rate as the sample size $n$ such that their ratio $p/n$\napproaches a positive constant $c\\in(0,1)$ as $n\\to\\infty$. We provide an\nextensive simulation study where the power function and receiver operating\ncharacteristic curves of the test are analyzed. In the empirical study, the\nmethodology is applied to the returns of S\\&P 500 constituents.\n", "versions": [{"version": "v1", "created": "Sun, 10 May 2020 19:39:02 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Bodnar", "Taras", ""], ["Dmytriv", "Solomiia", ""], ["Okhrin", "Yarema", ""], ["Parolya", "Nestor", ""], ["Schmid", "Wolfgang", ""]]}, {"id": "2005.04914", "submitter": "Jie Wu", "authors": "J. Wu, Z. Zheng, Y. Li, Y. Zhang", "title": "Scalable Interpretable Learning for Multi-Response Error-in-Variables\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corrupted data sets containing noisy or missing observations are prevalent in\nvarious contemporary applications such as economics, finance and\nbioinformatics. Despite the recent methodological and algorithmic advances in\nhigh-dimensional multi-response regression, how to achieve scalable and\ninterpretable estimation under contaminated covariates is unclear. In this\npaper, we develop a new methodology called convex conditioned sequential sparse\nlearning (COSS) for error-in-variables multi-response regression under both\nadditive measurement errors and random missing data. It combines the strengths\nof the recently developed sequential sparse factor regression and the nearest\npositive semi-definite matrix projection, thus enjoying stepwise convexity and\nscalability in large-scale association analyses. Comprehensive theoretical\nguarantees are provided and we demonstrate the effectiveness of the proposed\nmethodology through numerical studies.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 08:17:45 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["Wu", "J.", ""], ["Zheng", "Z.", ""], ["Li", "Y.", ""], ["Zhang", "Y.", ""]]}, {"id": "2005.05129", "submitter": "Sihong Shao", "authors": "Sihong Shao and Yunfeng Xiong", "title": "SPADE: Sequential-clustering Particle Annihilation via Discrepancy\n  Estimation", "comments": "Corrections only in the website abstract: Fix the display problem of\n  math notations and remove a typo. The manuscript with 38 pages and 9 tables\n  keeps unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST physics.comp-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an empirical signed measure $\\mu = \\frac{1}{N} \\left(\\sum_{i=1}^P\n\\delta_{x_i} - \\sum_{i=1}^M \\delta_{y_i}\\right)$, particle annihilation (PA)\nremoves $N_A$ particles from both $\\{x_i\\}_{i=1}^P$ and $\\{y_i\\}_{i=1}^M$\nsimultaneously, yielding another empirical signed measure $\\nu$ such that $\\int\nf d \\nu$ approximates to $\\int f d \\mu$ within an acceptable accuracy for\nsuitable test functions $f$. Such annihilation of particles carrying opposite\nimportance weights has been extensively utilized for alleviating the numerical\nsign problem in particle simulations. In this paper, we propose an algorithm\nfor PA in high-dimensional Euclidean space based on hybrid of clustering and\nmatching, dubbed the Sequential-clustering Particle Annihilation via\nDiscrepancy Estimation (SPADE). It consists of two steps: Adaptive clustering\nof particles via controlling their number-theoretic discrepancies, and\nindependent random matching among positive and negative particles in each\ncluster. Both deterministic error bounds by the Koksma-Hlawka inequality and\nnon-asymptotic random error bounds by concentration inequalities are proved to\nbe affected by two factors. One factor measures the irregularity of point\ndistributions and reflects their discrete nature. The other relies on the\nvariation of test function and is influenced by the continuity. Only the latter\nimplicitly depends on dimensionality $d$, implying that SPADE can be immune to\nthe curse of dimensionality for a wide class of test functions. Numerical\nexperiments up to $d=1080$ validate our theoretical discoveries.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 14:17:02 GMT"}, {"version": "v2", "created": "Tue, 12 May 2020 01:35:41 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Shao", "Sihong", ""], ["Xiong", "Yunfeng", ""]]}, {"id": "2005.05195", "submitter": "Ryan Cory-Wright", "authors": "Dimitris Bertsimas, Ryan Cory-Wright, Jean Pauphilet", "title": "Solving Large-Scale Sparse PCA to Certifiable (Near) Optimality", "comments": "Revision submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse principal component analysis (PCA) is a popular dimensionality\nreduction technique for obtaining principal components which are linear\ncombinations of a small subset of the original features. Existing approaches\ncannot supply certifiably optimal principal components with more than $p=100s$\nof variables. By reformulating sparse PCA as a convex mixed-integer\nsemidefinite optimization problem, we design a cutting-plane method which\nsolves the problem to certifiable optimality at the scale of selecting k=5\ncovariates from p=300 variables, and provides small bound gaps at a larger\nscale. We also propose a convex relaxation and greedy rounding scheme that\nprovides bound gaps of $1-2\\%$ in practice within minutes for $p=100$s or hours\nfor $p=1,000$s and is therefore a viable alternative to the exact method at\nscale. Using real-world financial and medical datasets, we illustrate our\napproach's ability to derive interpretable principal components tractably at\nscale.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 15:39:23 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 21:11:28 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 21:04:39 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Bertsimas", "Dimitris", ""], ["Cory-Wright", "Ryan", ""], ["Pauphilet", "Jean", ""]]}, {"id": "2005.05208", "submitter": "Robert Gaunt", "authors": "Andreas Anastasiou and Robert E. Gaunt", "title": "Wasserstein distance error bounds for the multivariate normal\n  approximation of the maximum likelihood estimator", "comments": "34 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain explicit Wasserstein distance error bounds between the distribution\nof the multi-parameter MLE and the multivariate normal distribution. Our\ngeneral bounds are given for possibly high-dimensional, independent and\nidentically distributed random vectors. Our general bounds are of the optimal\n$\\mathcal{O}(n^{-1/2})$ order. We apply our general bounds to derive\nWasserstein distance error bounds for the multivariate normal approximation of\nthe MLE in several settings; these being single-parameter exponential families,\nthe normal distribution under canonical parametrisation, and the multivariate\nnormal distribution under non-canonical parametrisation.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 15:53:11 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 13:21:05 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Anastasiou", "Andreas", ""], ["Gaunt", "Robert E.", ""]]}, {"id": "2005.05444", "submitter": "Yuzhou Gu", "authors": "Yuzhou Gu, Yury Polyanskiy", "title": "Non-linear Log-Sobolev inequalities for the Potts semigroup and\n  applications to reconstruction problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a Markov process with state space $[k]$, which jumps continuously to\na new state chosen uniformly at random and regardless of the previous state.\nThe collection of transition kernels (indexed by time $t\\ge 0$) is the Potts\nsemigroup. Diaconis and Saloff-Coste computed the maximum of the ratio of the\nrelative entropy and the Dirichlet form obtaining the constant $\\alpha_2$ in\nthe $2$-log-Sobolev inequality ($2$-LSI). In this paper, we obtain the best\npossible non-linear inequality relating entropy and the Dirichlet form (i.e.,\n$p$-NLSI, $p\\ge1$). As an example, we show $\\alpha_1 = 1+\\frac{1+o(1)}{\\log\nk}$. The more precise NLSIs have been shown by Polyanskiy and Samorodnitsky to\nimply various geometric and Fourier-analytic results.\n  Beyond the Potts semigroup, we also analyze Potts channels -- Markov\ntransition matrices $[k]\\times [k]$ constant on and off diagonal. (Potts\nsemigroup corresponds to a (ferromagnetic) subset of matrices with positive\nsecond eigenvalue). By integrating the $1$-NLSI we obtain the new strong data\nprocessing inequality (SDPI), which in turn allows us to improve results on\nreconstruction thresholds for Potts models on trees. A special case is the\nproblem of reconstructing color of the root of a $k$-colored tree given\nknowledge of colors of all the leaves. We show that to have a non-trivial\nreconstruction probability the branching number of the tree should be at least\n$$\\frac{\\log k}{\\log k - \\log(k-1)} = (1-o(1))k\\log k.$$ This extends previous\nresults (of Sly and Bhatnagar et al.) to general trees, and avoids the need for\nany specialized arguments. Similarly, we improve the state-of-the-art on\nreconstruction threshold for the stochastic block model with $k$ balanced\ngroups, for all $k\\ge 3$. These improvements advocate information-theoretic\nmethods as a useful complement to the conventional techniques originating from\nthe statistical physics.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2020 21:29:08 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Gu", "Yuzhou", ""], ["Polyanskiy", "Yury", ""]]}, {"id": "2005.05506", "submitter": "Eugene Katsevich", "authors": "Eugene Katsevich and Aaditya Ramdas", "title": "On the power of conditional independence testing under model-X", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For testing conditional independence (CI) of a response Y and a predictor X\ngiven covariates Z, the recently introduced model-X (MX) framework has been the\nsubject of active methodological research, especially in the context of MX\nknockoffs and their successful application to genome-wide association studies.\nIn this paper, we study the power of MX CI tests, yielding quantitative\nexplanations for empirically observed phenomena and novel insights to guide the\ndesign of MX methodology. We show that any valid MX CI test must also be valid\nconditionally on Y and Z; this conditioning allows us to reformulate the\nproblem as testing a point null hypothesis involving the conditional\ndistribution of X. The Neyman-Pearson lemma then implies that the conditional\nrandomization test (CRT) based on a likelihood statistic is the most powerful\nMX CI test against a point alternative. We also obtain a related optimality\nresult for MX knockoffs. Switching to an asymptotic framework with arbitrarily\ngrowing covariate dimension, we derive an expression for the limiting power of\nthe CRT against local semiparametric alternatives in terms of the prediction\nerror of the machine learning algorithm on which its test statistic is based.\nFinally, we exhibit a resampling-free test with uniform asymptotic Type-I error\ncontrol under the assumption that only the first two moments of X given Z are\nknown, a significant relaxation of the MX assumption.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 01:24:25 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 16:06:32 GMT"}, {"version": "v3", "created": "Sat, 24 Jul 2021 20:08:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Katsevich", "Eugene", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2005.05608", "submitter": "Alice Le Brigant", "authors": "Alice Le Brigant (SAMM), Stephen Preston, St\\'ephane Puechmorel (ENAC)", "title": "Fisher-Rao geometry of Dirichlet distributions", "comments": null, "journal-ref": "Differential Geometry and its Applications, Elsevier, 2021, 74,\n  pp.101702", "doi": "10.1016/j.difgeo.2020.101702", "report-no": null, "categories": "math.DG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the geometry induced by the Fisher-Rao metric on the\nparameter space of Dirichlet distributions. We show that this space is\ngeodesically complete and has everywhere negative sectional curvature. An\nimportant consequence of this negative curvature for applications is that the\nFr{\\'e}chet mean of a set of Dirichlet distributions is uniquely defined in\nthis geometry.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 08:27:57 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 15:19:27 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Brigant", "Alice Le", "", "SAMM"], ["Preston", "Stephen", "", "ENAC"], ["Puechmorel", "St\u00e9phane", "", "ENAC"]]}, {"id": "2005.06006", "submitter": "Jeffrey N\\\"af", "authors": "Loris Michel and Jeffrey N\\\"af", "title": "High Probability Lower Bounds for the Total Variation Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistics and machine learning communities have recently seen a growing\ninterest in classification-based approaches to two-sample testing. The outcome\nof a classification-based two-sample test remains a rejection decision, which\nis not always informative since the null hypothesis is seldom strictly true.\nTherefore, when a test rejects, it would be beneficial to provide an additional\nquantity serving as a refined measure of distributional difference. In this\nwork, we introduce a framework for the construction of high-probability lower\nbounds on the total variation distance. These bounds are based on a\none-dimensional projection, such as a classification or regression method, and\ncan be interpreted as the minimal fraction of samples pointing towards a\ndistributional difference. We further derive asymptotic power and detection\nrates of two proposed estimators and discuss potential uses through an\napplication to a reanalysis climate dataset\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 18:55:05 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 10:38:44 GMT"}, {"version": "v3", "created": "Fri, 14 May 2021 17:04:46 GMT"}, {"version": "v4", "created": "Wed, 2 Jun 2021 17:19:29 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Michel", "Loris", ""], ["N\u00e4f", "Jeffrey", ""]]}, {"id": "2005.06062", "submitter": "Simon Coste", "authors": "Charles Bordenave, Simon Coste, Raj Rao Nadakuditi", "title": "Detection thresholds in very sparse matrix completion", "comments": "84 pages, 10 pictures. Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $A$ be a rectangular matrix of size $m\\times n$ and $A_1$ be the random\nmatrix where each entry of $A$ is multiplied by an independent\n$\\{0,1\\}$-Bernoulli random variable with parameter $1/2$. This paper is about\nwhen, how and why the non-Hermitian eigen-spectra of the randomly induced\nasymmetric matrices $A_1 (A - A_1)^*$ and $(A-A_1)^*A_1$ captures more of the\nrelevant information about the principal component structure of $A$ than via\nits SVD or the eigen-spectra of $A A^*$ and $A^* A$, respectively. Hint: the\nasymmetry inducing randomness breaks the echo-chamber effect that cripples the\nSVD.\n  We illustrate the application of this striking phenomenon on the low-rank\nmatrix completion problem for the setting where each entry is observed with\nprobability $d/n$, including the very sparse regime where $d$ is of order $1$,\nwhere matrix completion via the SVD of $A$ fails or produces unreliable\nrecovery. We determine an asymptotically exact, matrix-dependent, non-universal\ndetection threshold above which reliable, statistically optimal matrix recovery\nusing a new, universal data-driven matrix-completion algorithm is possible.\nAveraging the left and right eigenvectors provably improves the recovered\nmatrix but not the detection threshold. We define another variant of this\nasymmetric procedure that bypasses the randomization step and has a detection\nthreshold that is smaller by a constant factor but with a computational cost\nthat is larger by a polynomial factor of the number of observed entries. Both\ndetection thresholds shatter the seeming barrier due to the well-known\ninformation theoretical limit $d \\asymp \\log n$ for matrix completion found in\nthe literature.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2020 21:39:24 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 08:04:43 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Bordenave", "Charles", ""], ["Coste", "Simon", ""], ["Nadakuditi", "Raj Rao", ""]]}, {"id": "2005.06158", "submitter": "Yuyuan Ouyang", "authors": "Zhulin He and Yuyuan Ouyang", "title": "An Asymptotic Result of Conditional Logistic Regression Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cluster-specific studies, ordinary logistic regression and conditional\nlogistic regression for binary outcomes provide maximum likelihood estimator\n(MLE) and conditional maximum likelihood estimator (CMLE), respectively. In\nthis paper, we show that CMLE is approaching to MLE asymptotically when each\nindividual data point is replicated infinitely many times. Our theoretical\nderivation is based on the observation that a term appearing in the conditional\naverage log-likelihood function is the coefficient of a polynomial, and hence\ncan be transformed to a complex integral by Cauchy's differentiation formula.\nThe asymptotic analysis of the complex integral can then be performed using the\nclassical method of steepest descent. Our result implies that CMLE can be\nbiased if individual weights are multiplied with a constant, and that we should\nbe cautious when assigning weights to cluster-specific studies.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 05:46:19 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["He", "Zhulin", ""], ["Ouyang", "Yuyuan", ""]]}, {"id": "2005.06371", "submitter": "Daisuke Kurisu", "authors": "Daisuke Kurisu", "title": "Nonparametric regression for locally stationary random fields under\n  stochastic sampling design", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we develop an asymptotic theory of nonparametric regression\nfor locally stationary random fields (LSRFs) $\\{{\\bf X}_{{\\bf s}, A_{n}}: {\\bf\ns} \\in R_{n} \\}$ in $\\mathbb{R}^{p}$ observed at irregularly spaced locations\nin $R_{n} =[0,A_{n}]^{d} \\subset \\mathbb{R}^{d}$. We first derive the uniform\nconvergence rate of general kernel estimators, followed by the asymptotic\nnormality of an estimator for the mean function of the model. Moreover, we\nconsider additive models to avoid the curse of dimensionality arising from the\ndependence of the convergence rate of estimators on the number of covariates.\nSubsequently, we derive the uniform convergence rate and joint asymptotic\nnormality of the estimators for additive functions. We also introduce\napproximately $m_{n}$-dependent RFs to provide examples of LSRFs. We find that\nthese RFs include a wide class of L\\'evy-driven moving average RFs.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 15:26:25 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 13:23:06 GMT"}, {"version": "v3", "created": "Thu, 21 May 2020 05:32:23 GMT"}, {"version": "v4", "created": "Tue, 23 Jun 2020 08:59:31 GMT"}, {"version": "v5", "created": "Tue, 15 Dec 2020 02:05:13 GMT"}, {"version": "v6", "created": "Wed, 24 Mar 2021 04:57:41 GMT"}, {"version": "v7", "created": "Thu, 17 Jun 2021 05:35:28 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Kurisu", "Daisuke", ""]]}, {"id": "2005.06417", "submitter": "Sushrut Karmalkar", "authors": "Ilias Diakonikolas, Samuel B. Hopkins, Daniel Kane, Sushrut Karmalkar", "title": "Robustly Learning any Clusterable Mixture of Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the efficient learnability of high-dimensional Gaussian mixtures in\nthe outlier-robust setting, where a small constant fraction of the data is\nadversarially corrupted. We resolve the polynomial learnability of this problem\nwhen the components are pairwise separated in total variation distance.\nSpecifically, we provide an algorithm that, for any constant number of\ncomponents $k$, runs in polynomial time and learns the components of an\n$\\epsilon$-corrupted $k$-mixture within information theoretically near-optimal\nerror of $\\tilde{O}(\\epsilon)$, under the assumption that the overlap between\nany pair of components $P_i, P_j$ (i.e., the quantity $1-TV(P_i, P_j)$) is\nbounded by $\\mathrm{poly}(\\epsilon)$.\n  Our separation condition is the qualitatively weakest assumption under which\naccurate clustering of the samples is possible. In particular, it allows for\ncomponents with arbitrary covariances and for components with identical means,\nas long as their covariances differ sufficiently. Ours is the first polynomial\ntime algorithm for this problem, even for $k=2$.\n  Our algorithm follows the Sum-of-Squares based proofs to algorithms approach.\nOur main technical contribution is a new robust identifiability proof of\nclusters from a Gaussian mixture, which can be captured by the constant-degree\nSum of Squares proof system. The key ingredients of this proof are a novel use\nof SoS-certifiable anti-concentration and a new characterization of pairs of\nGaussians with small (dimension-independent) overlap in terms of their\nparameter distance.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 16:44:12 GMT"}], "update_date": "2020-05-14", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Hopkins", "Samuel B.", ""], ["Kane", "Daniel", ""], ["Karmalkar", "Sushrut", ""]]}, {"id": "2005.06573", "submitter": "David Rindt", "authors": "David Rindt, Dino Sejdinovic and David Steinsaltz", "title": "Consistency of permutation tests for HSIC and dHSIC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hilbert--Schmidt Independence Criterion (HSIC) is a popular measure of\nthe dependency between two random variables. The statistic dHSIC is an\nextension of HSIC that can be used to test joint independence of $d$ random\nvariables. Such hypothesis testing for (joint) independence is often done using\na permutation test, which compares the observed data with randomly permuted\ndatasets. The main contribution of this work is proving that the power of such\nindependence tests converges to 1 as the sample size converges to infinity.\nThis answers a question that was asked in (Pfister, 2018) Additionally this\nwork proves correct type 1 error rate of HSIC and dHSIC permutation tests and\nprovides guidance on how to select the number of permutations one uses in\npractice. While correct type 1 error rate was already proved in (Pfister,\n2018), we provide a modified proof following (Berrett, 2019), which extends to\nthe case of non-continuous data. The number of permutations to use was studied\ne.g. by (Marozzi, 2004) but not in the context of HSIC and with a slight\ndifference in the estimate of the $p$-value and for permutations rather than\nvectors of permutations. While the last two points have limited novelty we\ninclude these to give a complete overview of permutation testing in the context\nof HSIC and dHSIC.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 20:26:41 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Rindt", "David", ""], ["Sejdinovic", "Dino", ""], ["Steinsaltz", "David", ""]]}, {"id": "2005.06623", "submitter": "Dmitry Burov", "authors": "Dmitry Burov, Dimitrios Giannakis, Krithika Manohar, Andrew Stuart", "title": "Kernel Analog Forecasting: Multiscale Test Problems", "comments": "30 pages, 14 figures; clarified several ambiguous parts, added\n  references, and a comparison with Lorenz' original method (Sec. 4.5)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DS physics.comp-ph physics.data-an stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven prediction is becoming increasingly widespread as the volume of\ndata available grows and as algorithmic development matches this growth. The\nnature of the predictions made, and the manner in which they should be\ninterpreted, depends crucially on the extent to which the variables chosen for\nprediction are Markovian, or approximately Markovian. Multiscale systems\nprovide a framework in which this issue can be analyzed. In this work kernel\nanalog forecasting methods are studied from the perspective of data generated\nby multiscale dynamical systems. The problems chosen exhibit a variety of\ndifferent Markovian closures, using both averaging and homogenization;\nfurthermore, settings where scale-separation is not present and the predicted\nvariables are non-Markovian, are also considered. The studies provide guidance\nfor the interpretation of data-driven prediction methods when used in practice.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2020 21:44:38 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 22:49:18 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Burov", "Dmitry", ""], ["Giannakis", "Dimitrios", ""], ["Manohar", "Krithika", ""], ["Stuart", "Andrew", ""]]}, {"id": "2005.06673", "submitter": "Serdar Y\\\"uksel", "authors": "Ian Hogeboom-Burr and Serdar Y\\\"uksel", "title": "Comparison of Information Structures for Zero-Sum Games and a Partial\n  Converse to Blackwell Ordering in Standard Borel Spaces", "comments": "To appear in SIAM Journal on Control and Optimization. A short\n  conference version appeared at the 2020 IEEE International Symposium on\n  Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical decision theory involving a single decision-maker, an\ninformation structure is said to be better than another one if for any cost\nfunction involving a hidden state variable and an action variable which is\nrestricted to be conditionally independent from the state given some\nmeasurement, the solution value under the former is not worse than that under\nthe latter. For finite spaces, a theorem due to Blackwell leads to a complete\ncharacterization on when one information structure is better than another. For\nstochastic games, in general, such an ordering is not possible since additional\ninformation can lead to equilibria perturbations with positive or negative\nvalues to a player. However, for zero-sum games in a finite probability space,\nP\\k{e}ski introduced a complete characterization of ordering of information\nstructures. In this paper, we obtain an infinite dimensional (standard Borel)\ngeneralization of P\\k{e}ski's result. A corollary is that more information\ncannot hurt a decision maker taking part in a zero-sum game. We establish two\nsupporting results which are essential and explicit though modest improvements\non prior literature: (i) a partial converse to Blackwell's ordering in the\nstandard Borel setup and (ii) an existence result for equilibria in zero-sum\ngames with incomplete information.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 00:15:57 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 20:08:34 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 20:11:34 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Hogeboom-Burr", "Ian", ""], ["Y\u00fcksel", "Serdar", ""]]}, {"id": "2005.06860", "submitter": "Aida Calvi\\~no", "authors": "Aida Calvi\\~no", "title": "Inference for a Step-Stress Model With Type-II and Progressive Type-II\n  Censoring and Lognormally Distributed Lifetimes", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accelerated life-testing (ALT) is a very useful technique for examining the\nreliability of highly reliable products. It allows testing the products at\nhigher than usual stress conditions to induce failures more quickly and\neconomically than under typical conditions. A special case of ALT are\nstep-stress tests that allow experimenter to increase the stress levels at\nfixed times. This paper deals with the multiple step step-stress model under\nthe cumulative exposure model with lognormally distributed lifetimes in the\npresence of Type-II and Progressive Type-II censoring. For this model, the\nmaximum likelihood estimates (MLE) of its parameters, as well as the\ncorresponding observed Fisher Information Matrix (FI), are derived. The\nlikelihood equations do not lead to closed-form expressions for the MLE, and\nthey need to be solved by means of an iterative procedure, such as the\nNewton-Raphson method. We then evaluate the bias and mean square error of the\nestimates and provide asymptotic and bootstrap confidence intervals. Finally,\nin order to asses the performance of the confidence intervals, a Monte Carlo\nsimulation study is conducted.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 10:30:00 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Calvi\u00f1o", "Aida", ""]]}, {"id": "2005.06869", "submitter": "Martin Wahl", "authors": "Martin Wahl", "title": "Lower bounds for invariant statistical models with applications to\n  principal component analysis", "comments": "42 pages, to appear in Annales de l'Institut Henri Poincar\\'e\n  Probabilit\\'es et Statistiques", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops nonasymptotic information inequalities for the estimation\nof the eigenspaces of a covariance operator. These results generalize previous\nlower bounds for the spiked covariance model, and they show that recent upper\nbounds for models with decaying eigenvalues are sharp. The proof relies on\nlower bound techniques based on group invariance arguments which can also deal\nwith a variety of other statistical models.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 11:02:39 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 08:18:33 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Wahl", "Martin", ""]]}, {"id": "2005.06883", "submitter": "Geoffrey McLachlan", "authors": "Sharon X. Lee, Geoffrey J. McLachlan", "title": "On mean and/or variance mixtures of normal distributions", "comments": "10 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric distributions are an important part of statistics. There is now a\nvoluminous literature on different fascinating formulations of flexible\ndistributions. We present a selective and brief overview of a small subset of\nthese distributions, focusing on those that are obtained by scaling the mean\nand/or covariance matrix of the (multivariate) normal distribution with some\nscaling variable(s). Namely, we consider the families of mean mixture, variance\nmixture, and mean-variance mixture of normal distributions. Its basic\nproperties, some notable special/limiting cases, and parameter estimation\nmethods are also described.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 11:39:38 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Lee", "Sharon X.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "2005.06905", "submitter": "Khalifa Es-Sebaiy", "authors": "Khalifa Es-Sebaiy, Jabrane Moustaaid", "title": "Optimal Berry-Ess\\'een bound for Maximum likelihood estimation of the\n  drift parameter in $ \\alpha $-Brownian bridge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $T>0,\\alpha>\\frac12$. In the present paper we consider the\n$\\alpha$-Brownian bridge defined as $dX_t=-\\alpha\\frac{X_t}{T-t}dt+dW_t,~ 0\\leq\nt< T$, where $W$ is a standard Brownian motion. We investigate the optimal rate\nof convergence to normality of the maximum likelihood estimator (MLE) for the\nparameter $ \\alpha $ based on the continuous observation $\\{X_s,0\\leq s\\leq\nt\\}$ as $t\\uparrow T$. We prove that an optimal rate of Kolmogorov distance for\ncentral limit theorem on the MLE is given by $\\frac{1}{\\sqrt{|\\log(T-t)|}}$, as\n$t\\uparrow T$. First we compute an upper bound and then find a lower bound with\nthe same speed using Corollary 1 and Corollary 2 of \\cite{kp-JVA},\nrespectively.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2020 12:12:57 GMT"}, {"version": "v2", "created": "Mon, 31 Aug 2020 17:53:55 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Es-Sebaiy", "Khalifa", ""], ["Moustaaid", "Jabrane", ""]]}, {"id": "2005.07397", "submitter": "Paul Doukhan M.", "authors": "J.-M. Bardet and P. Doukhan and O. Wintenberger", "title": "Contrast estimation of general locally stationary processes using\n  coupling", "comments": "42 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at providing statistical guarantees for a kernel based\nestimation of time varying parameters driving the dynamic of local stationary\nprocesses. We extend the results of Dahlhaus et al. (2018) considering the\nlocal stationary version of the infinite memory processes of Doukhan and\nWintenberger (2008). The estimators are computed as localized M-estimators of\nany contrast satisfying appropriate contraction conditions. We prove the\nuniform consistency and pointwise asymptotic normality of such kernel based\nestimators. We apply our result to usual contrasts such as least-square, least\nabsolute value, or quasi-maximum likelihood contrasts. Various local-stationary\nprocesses as ARMA, AR(infty), GARCH, ARCH(infty), ARMA-GARCH,\nLARCH(\\infty),..., and integer valued processes are also considered. Numerical\nexperiments demonstrate the efficiency of the estimators on both simulated and\nreal data sets.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 07:59:36 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 07:44:37 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Bardet", "J. -M.", ""], ["Doukhan", "P.", ""], ["Wintenberger", "O.", ""]]}, {"id": "2005.07450", "submitter": "Frank Werner", "authors": "Gytis Kulaitis, Axel Munk, Frank Werner", "title": "What is resolution? A statistical minimax testing perspective on\n  super-resolution microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a general rule of thumb the resolution of a light microscope (i.e. the\nability to discern objects) is predominantly described by the full width at\nhalf maximum (FWHM) of its point spread function (psf)---the diameter of the\nblurring density at half of its maximum. Classical wave optics suggests a\nlinear relationship between FWHM and resolution also manifested in the well\nknown Abbe and Rayleigh criteria, dating back to the end of 19th century.\nHowever, during the last two decades conventional light microscopy has\nundergone a shift from microscopic scales to nanoscales. This increase in\nresolution comes with the need to incorporate the random nature of observations\n(light photons) and challenges the classical view of discernability, as we\nargue in this paper. Instead, we suggest a statistical description of\nresolution obtained from such random data. Our notion of discernability is\nbased on statistical testing whether one or two objects with the same total\nintensity are present. For Poisson measurements we get linear dependence of the\n(minimax) detection boundary on the FWHM, whereas for a homogeneous Gaussian\nmodel the dependence of resolution is nonlinear. Hence, at small physical\nscales modeling by homogeneous gaussians is inadequate, although often\nimplicitly assumed in many reconstruction algorithms. In contrast, the Poisson\nmodel and its variance stabilized Gaussian approximation seem to provide a\nstatistically sound description of resolution at the nanoscale. Our theory is\nalso applicable to other imaging setups, such as telescopes.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 09:59:15 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 07:27:47 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Kulaitis", "Gytis", ""], ["Munk", "Axel", ""], ["Werner", "Frank", ""]]}, {"id": "2005.07557", "submitter": "Johannes Krebs", "authors": "Johannes Krebs, Benjamin Roycraft and Wolfgang Polonik", "title": "On approximation theorems for the Euler characteristic with applications\n  to the bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study approximation theorems for the Euler characteristic of the\nVietoris-Rips and Cech filtration. The filtration is obtained from a Poisson or\nbinomial sampling scheme in the critical regime. We apply our results to the\nsmooth bootstrap of the Euler characteristic and determine its rate of\nconvergence in the Kantorovich-Wasserstein distance and in the Kolmogorov\ndistance.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 14:09:49 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2020 13:30:11 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Krebs", "Johannes", ""], ["Roycraft", "Benjamin", ""], ["Polonik", "Wolfgang", ""]]}, {"id": "2005.07568", "submitter": "S{\\o}ren Wengel Mogensen", "authors": "S{\\o}ren Wengel Mogensen, Niels Richard Hansen", "title": "Graphical modeling of stochastic processes driven by correlated errors", "comments": "43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of graphs that represent local independence structures in\nstochastic processes allowing for correlated error processes. Several graphs\nmay encode the same local independencies and we characterize such equivalence\nclasses of graphs. In the worst case, the number of conditions in our\ncharacterizations grows superpolynomially as a function of the size of the node\nset in the graph. We show that deciding Markov equivalence is coNP-complete\nwhich suggests that our characterizations cannot be improved upon\nsubstantially. We prove a global Markov property in the case of a multivariate\nOrnstein-Uhlenbeck process which is driven by correlated Brownian motions.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 14:33:17 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 07:10:34 GMT"}], "update_date": "2020-09-14", "authors_parsed": [["Mogensen", "S\u00f8ren Wengel", ""], ["Hansen", "Niels Richard", ""]]}, {"id": "2005.07587", "submitter": "Yufei Yi", "authors": "Yufei Yi, Matey Neykov", "title": "Non-Sparse PCA in High Dimensions via Cone Projected Power Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a cone projected power iteration algorithm to\nrecover the first principal eigenvector from a noisy positive semidefinite\nmatrix. When the true principal eigenvector is assumed to belong to a convex\ncone, the proposed algorithm is fast and has a tractable error. Specifically,\nthe method achieves polynomial time complexity for certain convex cones\nequipped with fast projection such as the monotone cone. It attains a small\nerror when the noisy matrix has a small cone-restricted operator norm. We\nsupplement the above results with a minimax lower bound of the error under the\nspiked covariance model. Our numerical experiments on simulated and real data,\nshow that our method achieves shorter run time and smaller error in comparison\nto the ordinary power iteration and some sparse principal component analysis\nalgorithms if the principal eigenvector is in a convex cone.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 15:02:24 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 17:22:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yi", "Yufei", ""], ["Neykov", "Matey", ""]]}, {"id": "2005.07674", "submitter": "Pedro Ramos", "authors": "Pedro L. Ramos, Francisco A. Rodrigues, Eduardo Ramos, Dipak K. Dey,\n  Francisco Louzada", "title": "Power laws distributions in objective priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of objective prior in Bayesian applications has become a common\npractice to analyze data without subjective information. Formal rules usually\nobtain these priors distributions, and the data provide the dominant\ninformation in the posterior distribution. However, these priors are typically\nimproper and may lead to improper posterior. Here, we show, for a general\nfamily of distributions, that the obtained objective priors for the parameters\neither follow a power-law distribution or has an asymptotic power-law behavior.\nAs a result, we observed that the exponents of the model are between 0.5 and 1.\nUnderstand these behaviors allow us to easily verify if such priors lead to\nproper or improper posteriors directly from the exponent of the power-law. The\ngeneral family considered in our study includes essential models such as\nExponential, Gamma, Weibull, Nakagami-m, Haf-Normal, Rayleigh, Erlang, and\nMaxwell Boltzmann distributions, to list a few. In summary, we show that\ncomprehending the mechanisms describing the shapes of the priors provides\nessential information that can be used in situations where additional\ncomplexity is presented.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2020 17:45:11 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Ramos", "Pedro L.", ""], ["Rodrigues", "Francisco A.", ""], ["Ramos", "Eduardo", ""], ["Dey", "Dipak K.", ""], ["Louzada", "Francisco", ""]]}, {"id": "2005.07844", "submitter": "Debdeep Pati", "authors": "Anirban Bhattacharya and Debdeep Pati", "title": "Nonasymptotic Laplace approximation under model misspecification", "comments": "23 pages. Fixed minor technical glitches in the proof of Theorem 2 in\n  the updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present non-asymptotic two-sided bounds to the log-marginal likelihood in\nBayesian inference. The classical Laplace approximation is recovered as the\nleading term. Our derivation permits model misspecification and allows the\nparameter dimension to grow with the sample size. We do not make any\nassumptions about the asymptotic shape of the posterior, and instead require\ncertain regularity conditions on the likelihood ratio and that the posterior to\nbe sufficiently concentrated.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 02:16:34 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 14:53:03 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""]]}, {"id": "2005.07927", "submitter": "Mauricio Barahona", "authors": "Stamatina Lamprinakou, Emma McCoy, Mauricio Barahona, Axel Gandy, Seth\n  Flaxman, Sarah Filippi", "title": "BART-based inference for Poisson processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of Bayesian Additive Regression Trees (BART) has been\ndemonstrated in a variety of contexts including non parametric regression and\nclassification. Here we introduce a BART scheme for estimating the intensity of\ninhomogeneous Poisson Processes. Poisson intensity estimation is a vital task\nin various applications including medical imaging, astrophysics and network\ntraffic analysis. Our approach enables full posterior inference of the\nintensity in a nonparametric regression setting. We demonstrate the performance\nof our scheme through simulation studies on synthetic and real datasets in one\nand two dimensions, and compare our approach to alternative approaches.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 10:01:54 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Lamprinakou", "Stamatina", ""], ["McCoy", "Emma", ""], ["Barahona", "Mauricio", ""], ["Gandy", "Axel", ""], ["Flaxman", "Seth", ""], ["Filippi", "Sarah", ""]]}, {"id": "2005.07938", "submitter": "Jack Noonan", "authors": "Jack Noonan and Anatoly Zhigljavsky", "title": "Efficient quantization and weak covering of high dimensional cubes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathbb{Z}_n = \\{Z_1, \\ldots, Z_n\\}$ be a design; that is, a collection\nof $n$ points $Z_j \\in [-1,1]^d$. We study the quality of quantization of\n$[-1,1]^d$ by the points of $\\mathbb{Z}_n$ and the problem of quality of\ncovering of $[-1,1]^d$ by ${\\cal B}_d(\\mathbb{Z}_n,r)$, the union of balls\ncentred at $Z_j \\in \\mathbb{Z}_n$. We concentrate on the cases where the\ndimension $d$ is not small ($d\\geq 5$) and $n$ is not too large, $n \\leq 2^d$.\nWe define the design ${\\mathbb{D}_{n,\\delta}}$ as the maximum-resolution\n$2^{d-1}$ design defined on vertices of the cube $[-\\delta,\\delta]^d$, $0\\leq\n\\delta\\leq 1$. For this design, we derive a closed-form expression for the\nquantization error and very accurate approximations for {the coverage area}\nvol$([-1,1]^d \\cap {\\cal B}_d(\\mathbb{Z}_n,r)) $. We conjecture that the design\n${\\mathbb{D}_{n,\\delta}}$ with optimal $\\delta$ is the most efficient quantizer\nof $[-1,1]^d$ under the assumption $n \\leq 2^d$ and it is also makes a very\nefficient $(1-\\gamma)$-covering. We provide results of a large-scale numerical\ninvestigation confirming the accuracy of the developed approximations and the\nefficiency of the\n  designs ${\\mathbb{D}_{n,\\delta}}$.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 10:27:11 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Noonan", "Jack", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "2005.08099", "submitter": "Matthew Brennan", "authors": "Matthew Brennan, Guy Bresler", "title": "Reducibility and Statistical-Computational Gaps from Secret Leakage", "comments": "175 pages; subsumes preliminary draft arXiv:1908.06130; accepted for\n  presentation at the Conference on Learning Theory (COLT) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference problems with conjectured statistical-computational gaps are\nubiquitous throughout modern statistics, computer science and statistical\nphysics. While there has been success evidencing these gaps from the failure of\nrestricted classes of algorithms, progress towards a more traditional\nreduction-based approach to computational complexity in statistical inference\nhas been limited. Existing reductions have largely been limited to inference\nproblems with similar structure -- primarily mapping among problems\nrepresentable as a sparse submatrix signal plus a noise matrix, which are\nsimilar to the common hardness assumption of planted clique.\n  The insight in this work is that a slight generalization of the planted\nclique conjecture -- secret leakage planted clique -- gives rise to a variety\nof new average-case reduction techniques, yielding a web of reductions among\nproblems with very different structure. Using variants of the planted clique\nconjecture for specific forms of secret leakage planted clique, we deduce tight\nstatistical-computational tradeoffs for a diverse range of problems including\nrobust sparse mean estimation, mixtures of sparse linear regressions, robust\nsparse linear regression, tensor PCA, variants of dense $k$-block stochastic\nblock models, negatively correlated sparse PCA, semirandom planted dense\nsubgraph, detection in hidden partition models and a universality principle for\nlearning sparse mixtures. In particular, a $k$-partite hypergraph variant of\nthe planted clique conjecture is sufficient to establish all of our\ncomputational lower bounds. Our techniques also reveal novel connections to\ncombinatorial designs and to random matrix theory. This work gives the first\nevidence that an expanded set of hardness assumptions, such as for secret\nleakage planted clique, may be a key first step towards a more complete theory\nof reductions among statistical problems.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2020 20:56:09 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 21:59:41 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Brennan", "Matthew", ""], ["Bresler", "Guy", ""]]}, {"id": "2005.08136", "submitter": "Xinglong Li", "authors": "Xinglong Li and Trevor Campbell", "title": "Truncated Simulation and Inference in Edge-Exchangeable Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Edge-exchangeable probabilistic network models generate edges as an\ni.i.d.~sequence from a discrete measure, providing a simple means for\nstatistical inference of latent network properties. The measure is often\nconstructed using the self-product of a realization from a Bayesian\nnonparametric (BNP) discrete prior; but unlike in standard BNP models, the\nself-product measure prior is not conjugate the likelihood, hindering the\ndevelopment of exact simulation and inference algorithms. Approximation via\nfinite truncation of the discrete measure is a straightforward alternative, but\nincurs an unknown approximation error. In this paper, we develop methods for\nforward simulation and posterior inference in random self-product-measure\nmodels based on truncation, and provide theoretical guarantees on the quality\nof the results as a function of the truncation level. The techniques we present\nare general and extend to the broader class of discrete Bayesian nonparametric\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 00:32:30 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 15:49:30 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Li", "Xinglong", ""], ["Campbell", "Trevor", ""]]}, {"id": "2005.08155", "submitter": "Zhiqiang Tan", "authors": "Zhiqiang Tan and Xinwei Zhang", "title": "On loss functions and regret bounds for multi-category classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new approaches in multi-class settings for constructing proper\nscoring rules and hinge-like losses and establishing corresponding regret\nbounds with respect to the zero-one or cost-weighted classification loss. Our\nconstruction of losses involves deriving new inverse mappings from a concave\ngeneralized entropy to a loss through the use of a convex dissimilarity\nfunction related to the multi-distribution $f$-divergence. Moreover, we\nidentify new classes of multi-class proper scoring rules, which also recover\nand reveal interesting relationships between various composite losses currently\nin use. We establish new classification regret bounds in general for\nmulti-class proper scoring rules by exploiting the Bregman divergences of the\nassociated generalized entropies, and, as applications, provide simple\nmeaningful regret bounds for two specific classes of proper scoring rules.\nFinally, we derive new hinge-like convex losses, which are tighter convex\nextensions than related hinge-like losses and geometrically simpler with fewer\nnon-differentiable edges, while achieving similar regret bounds. We also\nestablish a general classification regret bound for all losses which induce the\nsame generalized entropy as the zero-one loss.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 03:12:52 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 19:14:54 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tan", "Zhiqiang", ""], ["Zhang", "Xinwei", ""]]}, {"id": "2005.08195", "submitter": "Pedro Ramos", "authors": "Eduardo Ramos, Pedro L. Ramos", "title": "Posterior properties of the Weibull distribution for censored data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Weibull distribution is one of the most used tools in reliability\nanalysis. In this paper, assuming a Bayesian approach, we propose necessary and\nsufficient conditions to verify when improper priors lead to proper posteriors\nfor the parameters of the Weibull distribution in the presence of complete or\nright-censored data. Additionally, we proposed sufficient conditions to verify\nif the obtained posterior moments are finite. These results can be achieved by\nchecking the behavior of the improper priors, which are applied in different\nobjective priors to illustrate the usefulness of the new results. As an\napplication of our theorem, we prove that if the improper prior leads to a\nproper posterior, the posterior mean, as well as other higher moments of the\nscale parameter, are not finite and, therefore, should not be used.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 08:44:45 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Ramos", "Eduardo", ""], ["Ramos", "Pedro L.", ""]]}, {"id": "2005.08373", "submitter": "Kevin Smith", "authors": "Kevin D. Smith", "title": "A Tutorial on Multivariate $k$-Statistics and their Computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document aims to provide an accessible tutorial on the unbiased\nestimation of multivariate cumulants, using $k$-statistics. We offer an\nexplicit and general formula for multivariate $k$-statistics of arbitrary\norder. We also prove that the $k$-statistics are unbiased, using M\\\"obius\ninversion and rudimentary combinatorics. Many detailed examples are considered\nthroughout the paper. We conclude with a discussion of $k$-statistics\ncomputation, including the challenge of time complexity, and we examine a\ncouple of possible avenues to improve the efficiency of this computation. The\npurpose of this document is threefold: to provide a clear introduction to\n$k$-statistics without relying on specialized tools like the umbral calculus;\nto construct an explicit formula for $k$-statistics that might facilitate\nfuture approximations and faster algorithms; and to serve as a companion paper\nto our Python library PyMoments, which implements this formula.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 21:20:40 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Smith", "Kevin D.", ""]]}, {"id": "2005.08397", "submitter": "Khalifa Es-Sebaiy", "authors": "Maoudo Faramba Balde, Rachid Belfadli, Khalifa Es-Sebaiy", "title": "Berry-Ess\\'een bound for drift estimation of fractional Ornstein\n  Uhlenbeck process of second kind", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we consider the Ornstein-Uhlenbeck process of the second\nkind defined as solution to the equation $dX_{t} = -\\alpha\nX_{t}dt+dY_{t}^{(1)},\n  \\ \\ X_{0}=0$, where $Y_{t}^{(1)}:=\\int_{0}^{t}e^{-s}dB^H_{a_{s}}$ with\n$a_{t}=He^{\\frac{t}{H}}$, and $B^H$ is a fractional Brownian motion with Hurst\nparameter $H\\in(\\frac12,1)$, whereas $\\alpha>0$ is unknown parameter to be\nestimated. We obtain the upper bound $O(1/\\sqrt{T})$ in Kolmogorov distance for\nnormal approximation of the least squares estimator of the drift parameter\n$\\alpha$ on the basis of the continuous observation $\\{X_t,t\\in[0,T]\\}$, as\n$T\\rightarrow\\infty$. Our method is based on the work of \\cite{kp-JVA}, which\nis proved using a combination of Malliavin calculus and Stein's method for\nnormal approximation.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2020 23:35:30 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Balde", "Maoudo Faramba", ""], ["Belfadli", "Rachid", ""], ["Es-Sebaiy", "Khalifa", ""]]}, {"id": "2005.08415", "submitter": "Wei Dai", "authors": "Ka Wai Tsang and Wei Dai", "title": "Selective Confidence Intervals for Martingale Regression Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of constructing confidence intervals\nfor coefficients of martingale regression models (in particular, time series\nmodels) after variable selection. Although constructing confidence intervals\nare common practice in statistical analysis, it is challenging in our framework\ndue to the data-dependence of the selected model and the correlation among the\nvariables being selected and not selected. We first introduce estimators for\nthe selected coefficients and show that it is consistent under martingale\nregression model, in which the observations can be dependent and the errors can\nbe heteroskedastic. Then we use the estimators together with a resampling\napproach to construct confidence intervals. Our simulation results show that\nour approach outperforms other existing approaches in various data structures.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 01:20:07 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Tsang", "Ka Wai", ""], ["Dai", "Wei", ""]]}, {"id": "2005.08602", "submitter": "Philipp Wacker", "authors": "Doris Schittenhelm and Philipp Wacker", "title": "Nested Sampling And Likelihood Plateaus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main idea of nested sampling is to substitute the high-dimensional\nlikelihood integral over the parameter space $\\Omega$ by an integral over the\nunit line $[0,1]$ by employing a push-forward with respect to a suitable\ntransformation. For this substitution, it is often implicitly or explicitly\nassumed that samples from the prior are uniformly distributed along this unit\nline after having been mapped by this transformation. We show that this\nassumption is wrong, especially in the case of a likelihood function with\nplateaus. Nevertheless, we show that the substitution enacted by nested\nsampling works because of more interesting reasons which we lay out. Although\nthis means that analytically, nested sampling can deal with plateaus in the\nlikelihood function, the actual performance of the algorithm suffers under such\na setting and the method fails to approximate the evidence, mean and variance\nappropriately. We suggest a robust implementation of nested sampling by a\nsimple decomposition idea which demonstrably overcomes this issue.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 11:34:48 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 08:46:40 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Schittenhelm", "Doris", ""], ["Wacker", "Philipp", ""]]}, {"id": "2005.08741", "submitter": "Samuel Rudy", "authors": "Samuel H. Rudy and Themistoklis P. Sapsis", "title": "Sparse Methods for Automatic Relevance Determination", "comments": null, "journal-ref": null, "doi": "10.1016/j.physd.2021.132843", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers methods for imposing sparsity in Bayesian regression with\napplications in nonlinear system identification. We first review automatic\nrelevance determination (ARD) and analytically demonstrate the need to\nadditional regularization or thresholding to achieve sparse models. We then\ndiscuss two classes of methods, regularization based and thresholding based,\nwhich build on ARD to learn parsimonious solutions to linear problems. In the\ncase of orthogonal covariates, we analytically demonstrate favorable\nperformance with regards to learning a small set of active terms in a linear\nsystem with a sparse solution. Several example problems are presented to\ncompare the set of proposed methods in terms of advantages and limitations to\nARD in bases with hundreds of elements. The aim of this paper is to analyze and\nunderstand the assumptions that lead to several algorithms and to provide\ntheoretical and empirical results so that the reader may gain insight and make\nmore informed choices regarding sparse Bayesian regression.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 14:08:49 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Rudy", "Samuel H.", ""], ["Sapsis", "Themistoklis P.", ""]]}, {"id": "2005.08794", "submitter": "Min Xu", "authors": "Harry Crane and Min Xu", "title": "Inference on the History of a Randomly Growing Tree", "comments": "36 pages; 7 figures; 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of infectious disease in a human community or the proliferation of\nfake news on social media can be modeled as a randomly growing tree-shaped\ngraph. The history of the random growth process is often unobserved but\ncontains important information such as the source of the infection. We consider\nthe problem of statistical inference on aspects of the latent history using\nonly a single snapshot of the final tree. Our approach is to apply random\nlabels to the observed unlabeled tree and analyze the resulting distribution of\nthe growth process, conditional on the final outcome. We show that this\nconditional distribution is tractable under a shape-exchangeability condition,\nwhich we introduce here, and that this condition is satisfied for many popular\nmodels for randomly growing trees such as uniform attachment, linear\npreferential attachment and uniform attachment on a $D$-regular tree. For\ninference of the root under shape-exchangeability, we propose O(n log n) time\nalgorithms for constructing confidence sets with valid frequentist coverage as\nwell as bounds on the expected size of the confidence sets. We also provide\nefficient sampling algorithms that extend our methods to a wide class of\ninference problems.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 15:15:15 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 00:28:22 GMT"}, {"version": "v3", "created": "Wed, 13 Jan 2021 19:24:59 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Crane", "Harry", ""], ["Xu", "Min", ""]]}, {"id": "2005.08904", "submitter": "Kristin Kirchner", "authors": "Kristin Kirchner and David Bolin", "title": "Necessary and sufficient conditions for asymptotically optimal linear\n  prediction of random fields on compact metric spaces", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal linear prediction (also known as kriging) of a random field\n$\\{Z(x)\\}_{x\\in\\mathcal{X}}$ indexed by a compact metric space\n$(\\mathcal{X},d_{\\mathcal{X}})$ can be obtained if the mean value function\n$m\\colon\\mathcal{X}\\to\\mathbb{R}$ and the covariance function\n$\\varrho\\colon\\mathcal{X}\\times\\mathcal{X}\\to\\mathbb{R}$ of $Z$ are known. We\nconsider the problem of predicting the value of $Z(x^*)$ at some location\n$x^*\\in\\mathcal{X}$ based on observations at locations $\\{x_j\\}_{j=1}^n$ which\naccumulate at $x^*$ as $n\\to\\infty$ (or, more generally, predicting $f(Z)$\nbased on $\\{ f_j(Z) \\}_{j=1}^n$ for linear functionals $f, f_1, \\ldots, f_n$).\nOur main result characterizes the asymptotic performance of linear predictors\n(as $n$ increases) based on an incorrect second order structure\n$(\\tilde{m},\\tilde{\\varrho})$, without any restrictive assumptions on $\\varrho,\n\\tilde{\\varrho}$ such as stationarity. We, for the first time, provide\nnecessary and sufficient conditions on $(\\tilde{m},\\tilde{\\varrho})$ for\nasymptotic optimality of the corresponding linear predictor holding uniformly\nwith respect to $f$. These general results are illustrated by an example on the\nsphere $\\mathbb{S}^2$ for the case of two isotropic covariance functions.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 17:24:45 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 16:05:58 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 11:43:18 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Kirchner", "Kristin", ""], ["Bolin", "David", ""]]}, {"id": "2005.09048", "submitter": "Luis Scoccola", "authors": "Alexander Rolle, Luis Scoccola", "title": "Stable and consistent density-based clustering", "comments": "32 pages, 7 figures. v2: improves exposition, adds computational\n  examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multiscale, consistent approach to density-based clustering that\nsatisfies stability theorems -- in both the input data and in the parameters --\nwhich hold without distributional assumptions. The stability in the input data\nis with respect to the Gromov--Hausdorff--Prokhorov distance on metric\nprobability spaces and interleaving distances between (multi-parameter)\nhierarchical clusterings we introduce. We prove stability results for standard\nsimplification procedures for hierarchical clusterings, which can be combined\nwith our approach to yield a stable flat clustering algorithm. We illustrate\nthe stability of the approach with computational examples. Our framework is\nbased on the concepts of persistence and interleaving distance from Topological\nData Analysis.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2020 19:45:04 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 17:38:01 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Rolle", "Alexander", ""], ["Scoccola", "Luis", ""]]}, {"id": "2005.09145", "submitter": "Yunyi Zhang", "authors": "Yunyi Zhang, Dimitris N. Politis", "title": "Bootstrap prediction intervals with asymptotic conditional validity and\n  unconditional guarantees", "comments": "27 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It can be argued that optimal prediction should take into account all\navailable data. Therefore, to evaluate a prediction interval's performance one\nshould employ conditional coverage probability, conditioning on all available\nobservations. Focusing on a linear model, we derive the asymptotic distribution\nof the difference between the conditional coverage probability of a nominal\nprediction interval and the conditional coverage probability of a prediction\ninterval obtained via a residual-based bootstrap. Applying this result, we show\nthat a prediction interval generated by the residual-based bootstrap has\napproximately 50% probability to yield conditional under-coverage. We then\ndevelop a new bootstrap algorithm that generates a prediction interval that\nasymptotically controls both the conditional coverage probability as well as\nthe possibility of conditional under-coverage. We complement the asymptotic\nresults with several finite-sample simulations.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 00:20:10 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 17:51:50 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhang", "Yunyi", ""], ["Politis", "Dimitris N.", ""]]}, {"id": "2005.09152", "submitter": "Anqi Dong", "authors": "Anqi Dong, Amirhossein Taghvaei, Tryphon T. Georgiou", "title": "Lasso formulation of the shortest path problem", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The shortest path problem is formulated as an $l_1$-regularized regression\nproblem, known as lasso. Based on this formulation, a connection is established\nbetween Dijkstra's shortest path algorithm and the least angle regression\n(LARS) for the lasso problem. Specifically, the solution path of the lasso\nproblem, obtained by varying the regularization parameter from infinity to zero\n(the regularization path), corresponds to shortest path trees that appear in\nthe bi-directional Dijkstra algorithm. Although Dijkstra's algorithm and the\nLARS formulation provide exact solutions, they become impractical when the size\nof the graph is exceedingly large. To overcome this issue, the alternating\ndirection method of multipliers (ADMM) is proposed to solve the lasso\nformulation. The resulting algorithm produces good and fast approximations of\nthe shortest path by sacrificing exactness that may not be absolutely essential\nin many applications. Numerical experiments are provided to illustrate the\nperformance of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 01:16:01 GMT"}, {"version": "v2", "created": "Fri, 22 May 2020 18:04:43 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Dong", "Anqi", ""], ["Taghvaei", "Amirhossein", ""], ["Georgiou", "Tryphon T.", ""]]}, {"id": "2005.09235", "submitter": "Guanyang Wang", "authors": "Guanyang Wang", "title": "On the Theoretical Properties of the Exchange Algorithm", "comments": "33 pages, 2 figures, typos fixed, include more examples, add\n  literature review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exchange algorithm is one of the most popular extensions of\nMetropolis-Hastings algorithm to sample from doubly-intractable distributions.\nHowever, theoretical exploration of exchange algorithm is very limited. For\nexample, natural questions like `Does exchange algorithm converge at a\ngeometric rate?' or `Does the exchange algorithm admit a Central Limit\nTheorem?' have not been answered. In this paper, we study the theoretical\nproperties of exchange algorithm, in terms of asymptotic variance and\nconvergence speed. We compare the exchange algorithm with the original\nMetropolis-Hastings algorithm and provide both necessary and sufficient\nconditions for geometric ergodicity of the exchange algorithm, which can be\napplied to various practical applications such as exponential random graph\nmodels and Ising models. A central limit theorem for the exchange algorithm is\nalso established. Meanwhile, a concrete example, involving the Binomial model\nwith conjugate and non-conjugate priors, is treated in detail with sharp\nconvergence rates. Our results justify the theoretical usefulness of the\nexchange algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 06:16:43 GMT"}, {"version": "v2", "created": "Sat, 23 May 2020 22:15:34 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 02:39:41 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Wang", "Guanyang", ""]]}, {"id": "2005.09577", "submitter": "Trisha Maitra Mrs", "authors": "Trisha Maitra and Sourabh Bhattacharya", "title": "Increasing Domain Infill Asymptotics for Stochastic Differential\n  Equations Driven by Fractional Brownian Motion", "comments": "Feedback welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although statistical inference in stochastic differential equations (SDEs)\ndriven by Wiener process has received significant attention in the literature,\ninference in those driven by fractional Brownian motion seem to have seen much\nless development in comparison, despite their importance in modeling long range\ndependence. In this article, we consider both classical and Bayesian inference\nin such fractional Brownian motion based SDEs. In particular, we consider\nasymptotic inference for two parameters in this regard; a multiplicative\nparameter associated with the drift function, and the so-called \"Hurst\nparameter\" of the fractional Brownian motion, when the time domain tends to\ninfinity. For unknown Hurst parameter, the likelihood does not lend itself\namenable to the popular Girsanov form, rendering usual asymptotic development\ndifficult. As such, we develop increasing domain infill asymptotic theory, by\ndiscretizing the SDE. In this setup, we establish consistency and asymptotic\nnormality of the maximum likelihood estimators, as well as consistency and\nasymptotic normality of the Bayesian posterior distributions. However,\nclassical or Bayesian asymptotic normality with respect to the Hurst parameter\ncould not be established. We supplement our theoretical investigations with\nsimulation studies in a non-asymptotic setup, prescribing suitable\nmethodologies for classical and Bayesian analyses of SDEs driven by fractional\nBrownian motion.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 16:42:47 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Maitra", "Trisha", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2005.09669", "submitter": "Sinho Chewi", "authors": "Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe\n  Rigollet, Austin J. Stromme", "title": "Exponential ergodicity of mirror-Langevin diffusions", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of sampling from ill-conditioned log-concave\ndistributions, we give a clean non-asymptotic convergence analysis of\nmirror-Langevin diffusions as introduced in Zhang et al. (2020). As a special\ncase of this framework, we propose a class of diffusions called Newton-Langevin\ndiffusions and prove that they converge to stationarity exponentially fast with\na rate which not only is dimension-free, but also has no dependence on the\ntarget distribution. We give an application of this result to the problem of\nsampling from the uniform distribution on a convex body using a strategy\ninspired by interior-point methods. Our general approach follows the recent\ntrend of linking sampling and optimization and highlights the role of the\nchi-squared divergence. In particular, it yields new results on the convergence\nof the vanilla Langevin diffusion in Wasserstein distance.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 18:00:52 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 22:39:02 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Chewi", "Sinho", ""], ["Gouic", "Thibaut Le", ""], ["Lu", "Chen", ""], ["Maunu", "Tyler", ""], ["Rigollet", "Philippe", ""], ["Stromme", "Austin J.", ""]]}, {"id": "2005.09711", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul, Hongjin Zhang, Konstantinos Tsampourakis and George\n  Michailidis", "title": "Inference on the Change Point for High Dimensional Dynamic Graphical\n  Models", "comments": "Software available upon request (built in R)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an estimator for the change point parameter for a dynamically\nevolving graphical model, and also obtain its asymptotic distribution under\nhigh dimensional scaling. To procure the latter result, we establish that the\nproposed estimator exhibits an $O_p(\\psi^{-2})$ rate of convergence, wherein\n$\\psi$ represents the jump size between the graphical model parameters before\nand after the change point. Further, it retains sufficient adaptivity against\nplug-in estimates of the graphical model parameters. We characterize the forms\nof the asymptotic distribution under the both a vanishing and a non-vanishing\nregime of the magnitude of the jump size. Specifically, in the former case it\ncorresponds to the argmax of a negative drift asymmetric two sided Brownian\nmotion, while in the latter case to the argmax of a negative drift asymmetric\ntwo sided random walk, whose increments depend on the distribution of the\ngraphical model. Easy to implement algorithms are provided for estimating the\nchange point and their performance assessed on synthetic data. The proposed\nmethodology is further illustrated on RNA-sequenced microbiome data and their\nchanges between young and older individuals.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 19:15:32 GMT"}, {"version": "v2", "created": "Fri, 18 Dec 2020 20:51:16 GMT"}, {"version": "v3", "created": "Sun, 21 Feb 2021 05:30:44 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Kaul", "Abhishek", ""], ["Zhang", "Hongjin", ""], ["Tsampourakis", "Konstantinos", ""], ["Michailidis", "George", ""]]}, {"id": "2005.09717", "submitter": "Anders Bredahl Kock", "authors": "Anders Bredahl Kock and David Preinerstorfer and Bezirgen Veliyev", "title": "Treatment recommendation with distributional targets", "comments": "Treatment allocation, best treatment identification, statistical\n  decision theory, pure exploration, nonparametric multi-armed bandits", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of a decision maker who must provide the best possible\ntreatment recommendation based on an experiment. The desirability of the\noutcome distribution resulting from the policy recommendation is measured\nthrough a functional capturing the distributional characteristic that the\ndecision maker is interested in optimizing. This could be, e.g., its inherent\ninequality, welfare, level of poverty or its distance to a desired outcome\ndistribution. If the functional of interest is not quasi-convex or if there are\nconstraints, the optimal recommendation may be a mixture of treatments. This\nvastly expands the set of recommendations that must be considered. We\ncharacterize the difficulty of the problem by obtaining maximal expected regret\nlower bounds. Furthermore, we propose two regret-optimal policies. The first\npolicy is static and thus applicable irrespectively of the subjects arriving\nsequentially or not in the course of the experimental phase. The second policy\ncan utilize that subjects arrive sequentially by successively eliminating\ninferior treatments and thus spends the sampling effort where it is most\nneeded.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 19:27:21 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 10:57:59 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Kock", "Anders Bredahl", ""], ["Preinerstorfer", "David", ""], ["Veliyev", "Bezirgen", ""]]}, {"id": "2005.09744", "submitter": "Roberto Vila Gabriel", "authors": "Helton Saulo, Roberto Vila, Leonardo Paiva, Narayanaswamy Balakrishnan", "title": "On a family of discrete log-symmetric distributions", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of continuous probability distributions has been widespread in\nproblems with purely discrete nature. In general, such distributions are not\nappropriate in this scenario. In this paper, we introduce a class of discrete\nand asymmetric distributions based on the family of continuous log-symmetric\ndistributions. Some properties are discussed as well as estimation by the\nmaximum likelihood method. A Monte Carlo simulation study is carried out to\nevaluate the performance of the estimators, and censored and uncensored data\nsets are used to illustrate the proposed methodology.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 20:34:15 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Saulo", "Helton", ""], ["Vila", "Roberto", ""], ["Paiva", "Leonardo", ""], ["Balakrishnan", "Narayanaswamy", ""]]}, {"id": "2005.09912", "submitter": "John Lafferty", "authors": "Chao Gao and John Lafferty", "title": "Model Repair: Robust Recovery of Over-Parameterized Statistical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new type of robust estimation problem is introduced where the goal is to\nrecover a statistical model that has been corrupted after it has been estimated\nfrom data. Methods are proposed for \"repairing\" the model using only the design\nand not the response values used to fit the model in a supervised learning\nsetting. Theory is developed which reveals that two important ingredients are\nnecessary for model repair---the statistical model must be over-parameterized,\nand the estimator must incorporate redundancy. In particular, estimators based\non stochastic gradient descent are seen to be well suited to model repair, but\nsparse estimators are not in general repairable. After formulating the problem\nand establishing a key technical lemma related to robust estimation, a series\nof results are presented for repair of over-parameterized linear models, random\nfeature models, and artificial neural networks. Simulation studies are\npresented that corroborate and illustrate the theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 08:41:56 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Gao", "Chao", ""], ["Lafferty", "John", ""]]}, {"id": "2005.09994", "submitter": "Ariane Hanebeck", "authors": "Ariane Hanebeck and Bernhard Klar", "title": "Smooth Distribution Function Estimation for Lifetime Distributions using\n  Szasz-Mirakyan Operators", "comments": "Small typo in Theorem 10: Now -1/12 instead of +1/12 in the term of\n  order $m^{-1}$", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new smooth estimator for continuous\ndistribution functions on the positive real half-line using Szasz-Mirakyan\noperators, similar to Bernstein's approximation theorem. We show that the\nproposed estimator outperforms the empirical distribution function in terms of\nasymptotic (integrated) mean-squared error, and generally compares favourably\nwith other competitors in theoretical comparisons. Also, we conduct the\nsimulations to demonstrate the finite sample performance of the proposed\nestimator.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 12:34:02 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2020 19:51:22 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2020 17:34:31 GMT"}, {"version": "v4", "created": "Tue, 8 Dec 2020 17:00:29 GMT"}, {"version": "v5", "created": "Wed, 27 Jan 2021 19:38:07 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Hanebeck", "Ariane", ""], ["Klar", "Bernhard", ""]]}, {"id": "2005.10018", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Revisiting Concentration of Missing Mass", "comments": "Added suplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of \\emph{missing mass concentration}, developing a new\nmethod of estimating concentration of heterogenic sums, in spirit of celebrated\nRosenthal's inequality. As a result we slightly improve the state-of-art bounds\ndue to Ben-Hamou at al., and simplify the proofs.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:56:00 GMT"}, {"version": "v2", "created": "Thu, 21 May 2020 16:03:44 GMT"}, {"version": "v3", "created": "Fri, 22 May 2020 15:43:42 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2005.10041", "submitter": "Fabian Telschow J. E.", "authors": "Fabian J.E. Telschow, Samuel Davenport, Armin Schwartzman", "title": "Functional delta residuals and applications to functional effect sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a functional central limit (fCLT) and a parameter transformation, we\nuse the functional delta method to construct random processes, called\nfunctional delta residuals, which asymptotically have the same covariance\nstructure as the transformed limit process. Moreover, we prove a multiplier\nbootstrap fCLT theorem for these transformed residuals and show how this can be\nused to construct simultaneous confidence bands for transformed functional\nparameters. As motivation for this methodology, we provide the formal\napplication of these residuals to a functional version of the effect size\nparameter Cohen's $d$, a problem appearing in current brain imaging\napplications. The performance and necessity of such residuals is illustrated in\na simulation experiment for the covering rate of simultaneous confidence bands\nfor the functional Cohen's $d$ parameter.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 13:38:03 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 13:33:48 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Telschow", "Fabian J. E.", ""], ["Davenport", "Samuel", ""], ["Schwartzman", "Armin", ""]]}, {"id": "2005.10561", "submitter": "Yury Polyanskiy", "authors": "Soham Jana, Yury Polyanskiy and Yihong Wu", "title": "Extrapolating the profile of a finite population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a prototypical problem in empirical Bayes. Namely, consider a\npopulation consisting of $k$ individuals each belonging to one of $k$ types\n(some types can be empty). Without any structural restrictions, it is\nimpossible to learn the composition of the full population having observed only\na small (random) subsample of size $m = o(k)$. Nevertheless, we show that in\nthe sublinear regime of $m =\\omega(k/\\log k)$, it is possible to consistently\nestimate in total variation the \\emph{profile} of the population, defined as\nthe empirical distribution of the sizes of each type, which determines many\nsymmetric properties of the population. We also prove that in the linear regime\nof $m=c k$ for any constant $c$ the optimal rate is $\\Theta(1/\\log k)$. Our\nestimator is based on Wolfowitz's minimum distance method, which entails\nsolving a linear program (LP) of size $k$. We show that there is a single\ninfinite-dimensional LP whose value simultaneously characterizes the risk of\nthe minimum distance estimator and certifies its minimax optimality. The sharp\nconvergence rate is obtained by evaluating this LP using complex-analytic\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 10:39:41 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Jana", "Soham", ""], ["Polyanskiy", "Yury", ""], ["Wu", "Yihong", ""]]}, {"id": "2005.10618", "submitter": "Kam\\'elia Daudel", "authors": "Kam\\'elia Daudel, Randal Douc and Fran\\c{c}ois Portier", "title": "Infinite-dimensional gradient-based descent for alpha-divergence\n  minimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the $(\\alpha, \\Gamma)$-descent, an iterative algorithm\nwhich operates on measures and performs $\\alpha$-divergence minimisation in a\nBayesian framework. This gradient-based procedure extends the commonly-used\nvariational approximation by adding a prior on the variational parameters in\nthe form of a measure. We prove that for a rich family of functions $\\Gamma$,\nthis algorithm leads at each step to a systematic decrease in the\n$\\alpha$-divergence and derive convergence results. Our framework recovers the\nEntropic Mirror Descent algorithm and provides an alternative algorithm that we\ncall the Power Descent. Moreover, in its stochastic formulation, the $(\\alpha,\n\\Gamma)$-descent allows to optimise the mixture weights of any given mixture\nmodel without any information on the underlying distribution of the variational\nparameters. This renders our method compatible with many choices of parameters\nupdates and applicable to a wide range of Machine Learning tasks. We\ndemonstrate empirically on both toy and real-world examples the benefit of\nusing the Power descent and going beyond the Entropic Mirror Descent framework,\nwhich fails as the dimension grows.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2020 15:05:38 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 07:24:16 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Daudel", "Kam\u00e9lia", ""], ["Douc", "Randal", ""], ["Portier", "Fran\u00e7ois", ""]]}, {"id": "2005.10650", "submitter": "Kay Bogerd", "authors": "Gianmarco Bet, Kay Bogerd, Rui M. Castro, Remco van der Hofstad", "title": "Detecting a botnet in a network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize the problem of detecting the presence of a botnet in a network\nas an hypothesis testing problem where we observe a single instance of a graph.\nThe null hypothesis, corresponding to the absence of a botnet, is modeled as a\nrandom geometric graph where every vertex is assigned a location on a\n$d$-dimensional torus and two vertices are connected when their distance is\nsmaller than a certain threshold. The alternative hypothesis is similar, except\nthat there is a small number of vertices, called the botnet, that ignore this\ngeometric structure and simply connect randomly to every other vertex with a\nprescribed probability.\n  We present two tests that are able to detect the presence of such a botnet.\nThe first test is based on the idea that botnet vertices tend to form large\nisolated stars that are not present under the null hypothesis. The second test\nuses the average graph distance, which becomes significantly shorter under the\nalternative hypothesis. We show that both these tests are asymptotically\noptimal. However, numerical simulations show that the isolated star test\nperforms significantly better than the average distance test on networks of\nmoderate size. Finally, we construct a robust scheme based on the isolated star\ntest that is also able to identify the vertices in the botnet.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 13:52:17 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 07:32:40 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Bet", "Gianmarco", ""], ["Bogerd", "Kay", ""], ["Castro", "Rui M.", ""], ["van der Hofstad", "Remco", ""]]}, {"id": "2005.10690", "submitter": "Laba Handique", "authors": "Laba Handique, Subrata Chakraborty and Farrukh Jamal", "title": "Beta Poisson-G Family of Distributions: Its Properties and Application\n  with Failure Time Data", "comments": "23 Pages, 7 Figures. arXiv admin note: substantial text overlap with\n  arXiv:2005.04506", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new generalization of the family of Poisson-G is called beta Poisson-G\nfamily of distribution. Useful expansions of the probability density function\nand the cumulative distribution function of the proposed family are derived and\nseen as infinite mixtures of the Poisson-G distribution. Moment generating\nfunction, power moments, entropy, quantile function, skewness and kurtosis are\ninvestigated. Numerical computation of moments, skewness, kurtosis and entropy\nare tabulated for select parameter values. Furthermore, estimation by methods\nof maximum likelihood is discussed. A simulation study is carried at under\nvarying sample size to assess the performance of this model. Finally\nsuitability check of the proposed model in comparison to its recently\nintroduced models is carried out by considering two real life data sets\nmodeling.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2020 17:03:43 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Handique", "Laba", ""], ["Chakraborty", "Subrata", ""], ["Jamal", "Farrukh", ""]]}, {"id": "2005.10743", "submitter": "Anru R. Zhang", "authors": "Yuetian Luo and Anru R. Zhang", "title": "Tensor Clustering with Planted Structures: Statistical Optimality and\n  Computational Limits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper studies the statistical and computational limits of high-order\nclustering with planted structures. We focus on two clustering models, constant\nhigh-order clustering (CHC) and rank-one higher-order clustering (ROHC), and\nstudy the methods and theory for testing whether a cluster exists (detection)\nand identifying the support of cluster (recovery).\n  Specifically, we identify the sharp boundaries of signal-to-noise ratio for\nwhich CHC and ROHC detection/recovery are statistically possible. We also\ndevelop the tight computational thresholds: when the signal-to-noise ratio is\nbelow these thresholds, we prove that polynomial-time algorithms cannot solve\nthese problems under the computational hardness conjectures of hypergraphic\nplanted clique (HPC) detection and hypergraphic planted dense subgraph (HPDS)\nrecovery. We also propose polynomial-time tensor algorithms that achieve\nreliable detection and recovery when the signal-to-noise ratio is above these\nthresholds. Both sparsity and tensor structures yield the computational\nbarriers in high-order tensor clustering. The interplay between them results in\nsignificant differences between high-order tensor clustering and matrix\nclustering in literature in aspects of statistical and computational phase\ntransition diagrams, algorithmic approaches, hardness conjecture, and proof\ntechniques. To our best knowledge, we are the first to give a thorough\ncharacterization of the statistical and computational trade-off for such a\ndouble computational-barrier problem. Finally, we provide evidence for the\ncomputational hardness conjectures of HPC detection and HPDS recovery.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 15:53:44 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2020 01:57:19 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Luo", "Yuetian", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2005.10761", "submitter": "Leighton Barnes", "authors": "Leighton Pate Barnes, Huseyin A. Inan, Berivan Isik, and Ayfer Ozgur", "title": "rTop-k: A Statistical Estimation Approach to Distributed SGD", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The large communication cost for exchanging gradients between different nodes\nsignificantly limits the scalability of distributed training for large-scale\nlearning models. Motivated by this observation, there has been significant\nrecent interest in techniques that reduce the communication cost of distributed\nStochastic Gradient Descent (SGD), with gradient sparsification techniques such\nas top-k and random-k shown to be particularly effective. The same observation\nhas also motivated a separate line of work in distributed statistical\nestimation theory focusing on the impact of communication constraints on the\nestimation efficiency of different statistical models. The primary goal of this\npaper is to connect these two research lines and demonstrate how statistical\nestimation models and their analysis can lead to new insights in the design of\ncommunication-efficient training techniques. We propose a simple statistical\nestimation model for the stochastic gradients which captures the sparsity and\nskewness of their distribution. The statistically optimal communication scheme\narising from the analysis of this model leads to a new sparsification technique\nfor SGD, which concatenates random-k and top-k, considered separately in the\nprior literature. We show through extensive experiments on both image and\nlanguage domains with CIFAR-10, ImageNet, and Penn Treebank datasets that the\nconcatenated application of these two sparsification methods consistently and\nsignificantly outperforms either method applied alone.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 16:27:46 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 21:26:06 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Barnes", "Leighton Pate", ""], ["Inan", "Huseyin A.", ""], ["Isik", "Berivan", ""], ["Ozgur", "Ayfer", ""]]}, {"id": "2005.10783", "submitter": "Leighton Barnes", "authors": "Leighton Pate Barnes, Wei-Ning Chen, and Ayfer Ozgur", "title": "Fisher information under local differential privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop data processing inequalities that describe how Fisher information\nfrom statistical samples can scale with the privacy parameter $\\varepsilon$\nunder local differential privacy constraints. These bounds are valid under\ngeneral conditions on the distribution of the score of the statistical model,\nand they elucidate under which conditions the dependence on $\\varepsilon$ is\nlinear, quadratic, or exponential. We show how these inequalities imply order\noptimal lower bounds for private estimation for both the Gaussian location\nmodel and discrete distribution estimation for all levels of privacy\n$\\varepsilon>0$. We further apply these inequalities to sparse Bernoulli models\nand demonstrate privacy mechanisms and estimators with order-matching squared\n$\\ell^2$ error.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 17:05:09 GMT"}], "update_date": "2020-05-22", "authors_parsed": [["Barnes", "Leighton Pate", ""], ["Chen", "Wei-Ning", ""], ["Ozgur", "Ayfer", ""]]}, {"id": "2005.10817", "submitter": "Alexander Wein", "authors": "Matthias L\\\"offler, Alexander S. Wein, Afonso S. Bandeira", "title": "Computationally efficient sparse clustering", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical and computational limits of clustering when the means of\nthe centres are sparse and their dimension is possibly much larger than the\nsample size. Our theoretical analysis focuses on the model $X_i = z_i \\theta +\n\\varepsilon_i, ~z_i \\in \\{-1,1\\}, ~\\varepsilon_i \\thicksim \\mathcal{N}(0,I)$,\nwhich has two clusters with centres $\\theta$ and $-\\theta$. We provide a finite\nsample analysis of a new sparse clustering algorithm based on sparse PCA and\nshow that it achieves the minimax optimal misclustering rate in the regime\n$\\|\\theta\\| \\rightarrow \\infty$.\n  Our results require the sparsity to grow slower than the square root of the\nsample size. Using a recent framework for computational lower bounds -- the\nlow-degree likelihood ratio -- we give evidence that this condition is\nnecessary for any polynomial-time clustering algorithm to succeed below the BBP\nthreshold. This complements existing evidence based on reductions and\nstatistical query lower bounds. Compared to these existing results, we cover a\nwider set of parameter regimes and give a more precise understanding of the\nruntime required and the misclustering error achievable. Our results imply that\na large class of tests based on low-degree polynomials fail to solve even the\nweak testing task.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2020 17:51:30 GMT"}, {"version": "v2", "created": "Mon, 25 May 2020 17:21:09 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 17:38:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["L\u00f6ffler", "Matthias", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "2005.10973", "submitter": "Masoud M Nasari", "authors": "Masoud M Nasari and Mohamedou Ould-Haye", "title": "A Consistent Estimator for Skewness of Partial Sums of Dependent Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an estimation method for the scaled skewness coefficient of the\nsample mean of short and long memory linear processes. This method can be\nextended to estimate higher moments such as curtosis coefficient of the sample\nmean. Also a general result on computing all asymptotic moments of partial sums\nis obtained, allowing in particular a much easier derivation of some existing\ncentral limit theorems for linear processes. The introduced skewness estimator\nprovides a tool to empirically examine the error of the central limit theorem\nfor long and short memory linear processes. We also show that, for both short\nand long memory linear processes, the skewness coefficient of the sample mean\nconverges to zero at the same rate as in the i.i.d. case.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 02:37:48 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Nasari", "Masoud M", ""], ["Ould-Haye", "Mohamedou", ""]]}, {"id": "2005.10993", "submitter": "Elvira Di Nardo Prof.", "authors": "E. Di Nardo", "title": "Polynomial traces and elementary symmetric functions in the latent roots\n  of a non-central Wishart matrix", "comments": null, "journal-ref": "Journal Multivariate Analysis, vol. 179, September (2020)", "doi": "10.1016/j.jmva.2020.104629", "report-no": null, "categories": "math.ST math.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypergeometric functions and zonal polynomials are the tools usually\naddressed in the literature to deal with the expected value of the elementary\nsymmetric functions in non-central Wishart latent roots. The method here\nproposed recovers the expected value of these symmetric functions by using the\numbral operator applied to the trace of suitable polynomial matrices and their\ncumulants. The employment of a suitable linear operator in place of\nhypergeometric functions and zonal polynomials was conjectured by de Waal in\n1972. Here we show how the umbral operator accomplishes this task and\nconsequently represents an alternative tool to deal with these symmetric\nfunctions. When special formal variables are plugged in the variables, the\nevaluation through the umbral operator deletes all the monomials in the latent\nroots except those contributing in the elementary symmetric functions.\nCumulants further simplify the computations taking advantage of the convolution\nstructure of the polynomial trace. Open problems are addressed at the end of\nthe paper.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 04:01:47 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Di Nardo", "E.", ""]]}, {"id": "2005.11028", "submitter": "Jesse Goodman", "authors": "Jesse Goodman", "title": "Asymptotic accuracy of the saddlepoint approximation for maximum\n  likelihood estimation", "comments": "78 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The saddlepoint approximation gives an approximation to the density of a\nrandom variable in terms of its moment generating function. When the underlying\nrandom variable is itself the sum of $n$ unobserved i.i.d.\\ terms, the basic\nclassical result is that the relative error in the density is of order $1/n$.\nIf instead the approximation is interpreted as a likelihood and maximised as a\nfunction of model parameters, the result is an approximation to the maximum\nlikelihood estimate (MLE) that can be much faster to compute than the true MLE.\nThis paper proves the analogous basic result for the approximation error\nbetween the saddlepoint MLE and the true MLE: subject to certain explicit\nidentifiability conditions, the error has asymptotic size $O(1/n^2)$ for some\nparameters, and $O(1/n^{3/2})$ or $O(1/n)$ for others. In all three cases, the\napproximation errors are asymptotically negligible compared to the inferential\nuncertainty.\n  The proof is based on a factorisation of the saddlepoint likelihood into an\nexact and approximate term, along with an analysis of the approximation error\nin the gradient of the log-likelihood. This factorisation also gives insight\ninto alternatives to the saddlepoint approximation, including a new and simpler\nsaddlepoint approximation, for which we derive analogous error bounds. As a\ncorollary of our results, we also obtain the asymptotic size of the MLE error\napproximation when the saddlepoint approximation is replaced by the normal\napproximation.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 06:44:41 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 22:16:49 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Goodman", "Jesse", ""]]}, {"id": "2005.11115", "submitter": "Ansgar Steland", "authors": "Ansgar Steland", "title": "Consistency of Extreme Learning Machines and Regression under\n  Non-Stationarity and Dependence for ML-Enhanced Moving Objects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised learning by extreme learning machines resp. neural networks with\nrandom weights is studied under a non-stationary spatial-temporal sampling\ndesign which especially addresses settings where an autonomous object moving in\na non-stationary spatial environment collects and analyzes data. The stochastic\nmodel especially allows for spatial heterogeneity and weak dependence. As\nefficient and computationally cheap learning methods (unconstrained) least\nsquares, ridge regression and $\\ell_s$-penalized least squares (including the\nLASSO) are studied. Consistency and asymptotic normality of the least squares\nand ridge regression estimates as well as corresponding consistency results for\nthe $\\ell_s$-penalty are shown under weak conditions. The resuts also cover\nbounds for the sample squared predicition error.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 11:29:15 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 17:05:07 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 09:56:40 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Steland", "Ansgar", ""]]}, {"id": "2005.11132", "submitter": "Holger Dette", "authors": "Holger Dette, Florian Heinrichs", "title": "A distribution free test for changes in the trend function of locally\n  stationary processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the common time series model $X_{i,n} = \\mu (i/n) + \\varepsilon_{i,n}$\nwith non-stationary errors we consider the problem of detecting a significant\ndeviation of the mean function $\\mu$ from a benchmark $g (\\mu )$ (such as the\ninitial value $\\mu (0)$ or the average trend $\\int_{0}^{1} \\mu (t) dt$). The\nproblem is motivated by a more realistic modelling of change point analysis,\nwhere one is interested in identifying relevant deviations in a smoothly\nvarying sequence of means $ (\\mu (i/n))_{i =1,\\ldots ,n }$ and cannot assume\nthat the sequence is piecewise constant. A test for this type of hypotheses is\ndeveloped using an appropriate estimator for the integrated squared deviation\nof the mean function and the threshold. By a new concept of self-normalization\nadapted to non-stationary processes an asymptotically pivotal test for the\nhypothesis of a relevant deviation is constructed. The results are illustrated\nby means of a simulation study and a data example.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 12:08:23 GMT"}], "update_date": "2020-05-25", "authors_parsed": [["Dette", "Holger", ""], ["Heinrichs", "Florian", ""]]}, {"id": "2005.11270", "submitter": "Yunzi Ding", "authors": "Yunzi Ding, Dmitriy Kunisky, Alexander S. Wein, Afonso S. Bandeira", "title": "The Average-Case Time Complexity of Certifying the Restricted Isometry\n  Property", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compressed sensing, the restricted isometry property (RIP) on $M \\times N$\nsensing matrices (where $M < N$) guarantees efficient reconstruction of sparse\nvectors. A matrix has the $(s,\\delta)$-$\\mathsf{RIP}$ property if behaves as a\n$\\delta$-approximate isometry on $s$-sparse vectors. It is well known that an\n$M\\times N$ matrix with i.i.d. $\\mathcal{N}(0,1/M)$ entries is\n$(s,\\delta)$-$\\mathsf{RIP}$ with high probability as long as $s\\lesssim\n\\delta^2 M/\\log N$. On the other hand, most prior works aiming to\ndeterministically construct $(s,\\delta)$-$\\mathsf{RIP}$ matrices have failed\nwhen $s \\gg \\sqrt{M}$. An alternative way to find an RIP matrix could be to\ndraw a random gaussian matrix and certify that it is indeed RIP. However, there\nis evidence that this certification task is computationally hard when $s \\gg\n\\sqrt{M}$, both in the worst case and the average case.\n  In this paper, we investigate the exact average-case time complexity of\ncertifying the RIP property for $M\\times N$ matrices with i.i.d.\n$\\mathcal{N}(0,1/M)$ entries, in the \"possible but hard\" regime $\\sqrt{M} \\ll\ns\\lesssim M/\\log N$. Based on analysis of the low-degree likelihood ratio, we\ngive rigorous evidence that subexponential runtime $N^{\\tilde\\Omega(s^2/M)}$ is\nrequired, demonstrating a smooth tradeoff between the maximum tolerated\nsparsity and the required computational power. This lower bound is essentially\ntight, matching the runtime of an existing algorithm due to Koiran and Zouzias.\nOur hardness result allows $\\delta$ to take any constant value in $(0,1)$,\nwhich captures the relevant regime for compressed sensing. This improves upon\nthe existing average-case hardness result of Wang, Berthet, and Plan, which is\nlimited to $\\delta = o(1)$.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 16:55:01 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2020 15:44:33 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 16:00:12 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Ding", "Yunzi", ""], ["Kunisky", "Dmitriy", ""], ["Wein", "Alexander S.", ""], ["Bandeira", "Afonso S.", ""]]}, {"id": "2005.11303", "submitter": "Nima Hejazi", "authors": "Ashkan Ertefaie, Nima S. Hejazi, Mark J. van der Laan", "title": "Nonparametric inverse probability weighted estimators based on the\n  highly adaptive lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse probability weighted estimators are the oldest and potentially most\ncommonly used class of procedures for the estimation of causal effects. By\nadjusting for selection biases via a weighting mechanism, these procedures\nestimate an effect of interest by constructing a pseudo-population in which\nselection biases are eliminated. Despite their ease of use, these estimators\nrequire the correct specification of a model for the weighting mechanism, are\nknown to be inefficient, and suffer from the curse of dimensionality. We\npropose a class of nonparametric inverse probability weighted estimators in\nwhich the weighting mechanism is estimated via undersmoothing of the highly\nadaptive lasso, a nonparametric regression function proven to converge at\n$n^{-1/3}$-rate to the true weighting mechanism. We demonstrate that our\nestimators are asymptotically linear with variance converging to the\nnonparametric efficiency bound. Unlike doubly robust estimators, our procedures\nrequire neither derivation of the efficient influence function nor\nspecification of the conditional outcome model. Our theoretical developments\nhave broad implications for the construction of efficient inverse probability\nweighted estimators in large statistical models and a variety of problem\nsettings. We assess the practical performance of our estimators in simulation\nstudies and demonstrate use of our proposed methodology with data from a\nlarge-scale epidemiologic study.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 17:49:46 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 07:35:38 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Ertefaie", "Ashkan", ""], ["Hejazi", "Nima S.", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "2005.11375", "submitter": "Yifan Chen", "authors": "Yifan Chen, Houman Owhadi, Andrew M. Stuart", "title": "Consistency of Empirical Bayes And Kernel Flow For Hierarchical\n  Parameter Estimation", "comments": "to appear in Mathematics of Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression has proven very powerful in statistics, machine\nlearning and inverse problems. A crucial aspect of the success of this\nmethodology, in a wide range of applications to complex and real-world\nproblems, is hierarchical modeling and learning of hyperparameters. The purpose\nof this paper is to study two paradigms of learning hierarchical parameters:\none is from the probabilistic Bayesian perspective, in particular, the\nempirical Bayes approach that has been largely used in Bayesian statistics; the\nother is from the deterministic and approximation theoretic view, and in\nparticular the kernel flow algorithm that was proposed recently in the machine\nlearning literature. Analysis of their consistency in the large data limit, as\nwell as explicit identification of their implicit bias in parameter learning,\nare established in this paper for a Mat\\'ern-like model on the torus. A\nparticular technical challenge we overcome is the learning of the regularity\nparameter in the Mat\\'ern-like field, for which consistency results have been\nvery scarce in the spatial statistics literature. Moreover, we conduct\nextensive numerical experiments beyond the Mat\\'ern-like model, comparing the\ntwo algorithms further. These experiments demonstrate learning of other\nhierarchical parameters, such as amplitude and lengthscale; they also\nillustrate the setting of model misspecification in which the kernel flow\napproach could show superior performance to the more traditional empirical\nBayes approach.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 20:17:48 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 23:57:46 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Chen", "Yifan", ""], ["Owhadi", "Houman", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "2005.11378", "submitter": "Rafal Kulik", "authors": "Youssouph Cissokho and Rafal Kulik", "title": "Estimation of cluster functionals for regularly varying time series:\n  sliding blocks estimators", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster indices describe extremal behaviour of stationary time series. We\nconsider their sliding blocks estimators. Using a modern theory of\nmultivariate, regularly varying time series, we obtain central limit theorems\nunder conditions that can be easily verified for a large class of models. In\nparticular, we show that in the Peak over Threshold framework, sliding and\ndisjoint blocks estimators have the same limiting variance.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 20:24:19 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Cissokho", "Youssouph", ""], ["Kulik", "Rafal", ""]]}, {"id": "2005.11411", "submitter": "Raaz Dwivedi", "authors": "Nhat Ho, Koulik Khamaru, Raaz Dwivedi, Martin J. Wainwright, Michael\n  I. Jordan, Bin Yu", "title": "Instability, Computational Efficiency and Statistical Accuracy", "comments": "First three authors contributed equally (listed in random order). 57\n  pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical estimators are defined as the fixed point of a\ndata-dependent operator, with estimators based on minimizing a cost function\nbeing an important special case. The limiting performance of such estimators\ndepends on the properties of the population-level operator in the idealized\nlimit of infinitely many samples. We develop a general framework that yields\nbounds on statistical accuracy based on the interplay between the deterministic\nconvergence rate of the algorithm at the population level, and its degree of\n(in)stability when applied to an empirical object based on $n$ samples. Using\nthis framework, we analyze both stable forms of gradient descent and some\nhigher-order and unstable algorithms, including Newton's method and its\ncubic-regularized variant, as well as the EM algorithm. We provide applications\nof our general results to several concrete classes of models, including\nGaussian mixture estimation, single-index models, and informative non-response\nmodels. We exhibit cases in which an unstable algorithm can achieve the same\nstatistical accuracy as a stable algorithm in exponentially fewer\nsteps---namely, with the number of iterations being reduced from polynomial to\nlogarithmic in sample size $n$.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2020 22:30:52 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Ho", "Nhat", ""], ["Khamaru", "Koulik", ""], ["Dwivedi", "Raaz", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""], ["Yu", "Bin", ""]]}, {"id": "2005.11510", "submitter": "Ionas Erb", "authors": "Ionas Erb and Nihat Ay", "title": "The Information-Geometric Perspective of Compositional Data Analysis", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information geometry uses the formal tools of differential geometry to\ndescribe the space of probability distributions as a Riemannian manifold with\nan additional dual structure. The formal equivalence of compositional data with\ndiscrete probability distributions makes it possible to apply the same\ndescription to the sample space of Compositional Data Analysis (CoDA). The\nlatter has been formally described as a Euclidean space with an orthonormal\nbasis featuring components that are suitable combinations of the original\nparts. In contrast to the Euclidean metric, the information-geometric\ndescription singles out the Fisher information metric as the only one keeping\nthe manifold's geometric structure invariant under equivalent representations\nof the underlying random variables. Well-known concepts that are valid in\nEuclidean coordinates, e.g., the Pythogorean theorem, are generalized by\ninformation geometry to corresponding notions that hold for more general\ncoordinates. In briefly reviewing Euclidean CoDA and, in more detail, the\ninformation-geometric approach, we show how the latter justifies the use of\ndistance measures and divergences that so far have received little attention in\nCoDA as they do not fit the Euclidean geometry favored by current thinking. We\nalso show how entropy and relative entropy can describe amalgamations in a\nsimple way, while Aitchison distance requires the use of geometric means to\nobtain more succinct relationships. We proceed to prove the information\nmonotonicity property for Aitchison distance. We close with some thoughts about\nnew directions in CoDA where the rich structure that is provided by information\ngeometry could be exploited.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2020 10:36:50 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 22:40:32 GMT"}, {"version": "v3", "created": "Tue, 27 Apr 2021 09:14:11 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Erb", "Ionas", ""], ["Ay", "Nihat", ""]]}, {"id": "2005.11635", "submitter": "Richard Rohwer", "authors": "Richard Rohwer", "title": "The Optimal 'AND'", "comments": "23 pages, 5 figures, to be submitted (shortened) to NeurIPS20", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LO math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint distribution $P(X,Y)$ cannot be determined from its marginals\n$P(X)$ and $P(Y)$ alone; one also needs one of the conditionals $P(X|Y)$ or\n$P(Y|X)$. But is there a best guess, given only the marginals? Here we answer\nthis question in the affirmative, obtaining in closed form the function of the\nmarginals that has the lowest expected Kullbach-Liebler (KL) divergence between\nthe unknown \"true\" joint probability and the function value. The expectation is\ntaken with respect to Jeffreys' non-informative prior over the possible joint\nprobability values, given the marginals. This distribution can also be used to\nobtain the expected information loss for any other \"aggregation operator\", as\nsuch estimators are often called in fuzzy logic, for any given pair of marginal\ninput values. This enables such such operators, including ours, to be compared\naccording to their expected loss under the minimal knowledge conditions we\nassume. We go on to develop a method for evaluating the expected accuracy of\nany aggregation operator in the absence of knowledge of its inputs. This\nrequires averaging the expected loss over all possible input pairs, weighted by\nan appropriate distribution. We obtain this distribution by marginalizing\nJeffreys' prior over the possible joint distributions (over the 3 functionally\nindependent coordinates of the space of joint distributions over two Boolean\nvariables) onto a joint distribution over the pair of marginal distributions, a\n2-dimensional space with one parameter for each marginal. We report the\nresulting input-averaged expected losses for a few commonly used operators, as\nwell as the optimal operator. Finally, we discuss the potential to develop our\nmethodology into a principled risk management approach to replace the often\nrather arbitrary conditional-independence assumptions made for probabilistic\ngraphical models.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 01:30:58 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Rohwer", "Richard", ""]]}, {"id": "2005.11641", "submitter": "Tudor Manole", "authors": "Tudor Manole, Abbas Khalili", "title": "Estimating the Number of Components in Finite Mixture Models via the\n  Group-Sort-Fuse Procedure", "comments": "79 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the number of components (or order) of a finite mixture model\nis a long standing and challenging problem in statistics. We propose the\nGroup-Sort-Fuse (GSF) procedure---a new penalized likelihood approach for\nsimultaneous estimation of the order and mixing measure in multidimensional\nfinite mixture models. Unlike methods which fit and compare mixtures with\nvarying orders using criteria involving model complexity, our approach directly\npenalizes a continuous function of the model parameters. More specifically,\ngiven a conservative upper bound on the order, the GSF groups and sorts mixture\ncomponent parameters to fuse those which are redundant. For a wide range of\nfinite mixture models, we show that the GSF is consistent in estimating the\ntrue mixture order and achieves the $n^{-1/2}$ convergence rate for parameter\nestimation up to polylogarithmic factors. The GSF is implemented for several\nunivariate and multivariate mixture models in the R package GroupSortFuse. Its\nfinite sample performance is supported by a thorough simulation study, and its\napplication is illustrated on two real data examples.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 02:38:12 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Manole", "Tudor", ""], ["Khalili", "Abbas", ""]]}, {"id": "2005.11720", "submitter": "Thibaut Le Gouic", "authors": "Thibaut Le Gouic and Jean-Michel Loubes and Philippe Rigollet", "title": "Projection to Fairness in Statistical Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of regression, we consider the fundamental question of making\nan estimator fair while preserving its prediction accuracy as much as possible.\nTo that end, we define its projection to fairness as its closest fair estimator\nin a sense that reflects prediction accuracy. Our methodology leverages tools\nfrom optimal transport to construct efficiently the projection to fairness of\nany given estimator as a simple post-processing step. Moreover, our approach\nprecisely quantifies the cost of fairness, measured in terms of prediction\naccuracy.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 11:20:07 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 09:16:22 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 17:46:21 GMT"}, {"version": "v4", "created": "Thu, 25 Jun 2020 17:02:18 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Gouic", "Thibaut Le", ""], ["Loubes", "Jean-Michel", ""], ["Rigollet", "Philippe", ""]]}, {"id": "2005.11774", "submitter": "Irina Blazhievska", "authors": "Irina Blazhievska", "title": "Cumulant methods in the estimation of response functions in\n  time-invariant linear systems", "comments": "17 pictures, 202 pages, in Ukrainian, Dissertation, Kyev Polytechnic\n  Institute (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thesis is devoted to the application of cumulant analysis in the estimation\nof impulse response functions for continuous time-invariant linear systems,\nincluding systems with inner noises. The main assumption of the work is the\nsecond-order integration of the impulse response function. Our study deals with\ncumulant analysis of sample cross-correlograms between stationary Gaussian\nstochastic processes. An important role was played by integral representations\nfor the higher-order cumulants of these second-order statistics. Using the\ndiagram formula, all representations are reduced to the finite sums of\nintegrals involving cyclic products of kernels. In the work we proved the\nconvergence to zero of the corresponding integrals. Then, since the Gaussian\ndistribution is uniquelly determined by its cumulants and also all higher-order\ncumulants of the estimators tend to zero, we establish the asymptotic normality\nof the integral-type cross-correlogram estimators.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 15:22:25 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Blazhievska", "Irina", ""]]}, {"id": "2005.11807", "submitter": "William Leeb", "authors": "William Leeb", "title": "Optimal singular value shrinkage for operator norm loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the denoising of low-rank matrices by singular value shrinkage.\nRecent work of Gavish and Donoho constructs a framework for finding optimal\nsingular value shrinkers for a wide class of loss functions. We use this\nframework to derive the optimal shrinker for operator norm loss. The optimal\nshrinker matches the shrinker proposed by Gavish and Donoho in the special case\nof square matrices, but differs for all other aspect ratios. We precisely\nquantify the gain in accuracy from using the optimal shrinker. We also show\nthat the optimal shrinker converges to the best linear predictor in the\nclassical regime of aspect ratio zero.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2020 17:38:30 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Leeb", "William", ""]]}, {"id": "2005.11878", "submitter": "Yuanhan Hu", "authors": "Mert Gurbuzbalaban, Yuanhan Hu", "title": "Fractional moment-preserving initialization schemes for training deep\n  neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A traditional approach to initialization in deep neural networks (DNNs) is to\nsample the network weights randomly for preserving the variance of\npre-activations. On the other hand, several studies show that during the\ntraining process, the distribution of stochastic gradients can be heavy-tailed\nespecially for small batch sizes. In this case, weights and therefore\npre-activations can be modeled with a heavy-tailed distribution that has an\ninfinite variance but has a finite (non-integer) fractional moment of order $s$\nwith $s<2$. Motivated by this fact, we develop initialization schemes for fully\nconnected feed-forward networks that can provably preserve any given moment of\norder $s \\in (0, 2]$ over the layers for a class of activations including ReLU,\nLeaky ReLU, Randomized Leaky ReLU, and linear activations. These generalized\nschemes recover traditional initialization schemes in the limit $s \\to 2$ and\nserve as part of a principled theory for initialization. For all these schemes,\nwe show that the network output admits a finite almost sure limit as the number\nof layers grows, and the limit is heavy-tailed in some settings. This sheds\nfurther light into the origins of heavy tail during signal propagation in DNNs.\nWe prove that the logarithm of the norm of the network outputs, if properly\nscaled, will converge to a Gaussian distribution with an explicit mean and\nvariance we can compute depending on the activation used, the value of s chosen\nand the network width. We also prove that our initialization scheme avoids\nsmall network output values more frequently compared to traditional approaches.\nFurthermore, the proposed initialization strategy does not have an extra cost\nduring the training procedure. We show through numerical experiments that our\ninitialization can improve the training and test performance.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 01:10:01 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 02:39:19 GMT"}, {"version": "v3", "created": "Tue, 2 Jun 2020 17:25:23 GMT"}, {"version": "v4", "created": "Mon, 8 Jun 2020 20:00:48 GMT"}, {"version": "v5", "created": "Sat, 13 Feb 2021 15:23:47 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Gurbuzbalaban", "Mert", ""], ["Hu", "Yuanhan", ""]]}, {"id": "2005.11879", "submitter": "Zhou Fan", "authors": "Zhou Fan and Zhichao Wang", "title": "Spectra of the Conjugate Kernel and Neural Tangent Kernel for\n  linear-width neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the eigenvalue distributions of the Conjugate Kernel and Neural\nTangent Kernel associated to multi-layer feedforward neural networks. In an\nasymptotic regime where network width is increasing linearly in sample size,\nunder random initialization of the weights, and for input samples satisfying a\nnotion of approximate pairwise orthogonality, we show that the eigenvalue\ndistributions of the CK and NTK converge to deterministic limits. The limit for\nthe CK is described by iterating the Marcenko-Pastur map across the hidden\nlayers. The limit for the NTK is equivalent to that of a linear combination of\nthe CK matrices across layers, and may be described by recursive fixed-point\nequations that extend this Marcenko-Pastur map. We demonstrate the agreement of\nthese asymptotic predictions with the observed spectra for both synthetic and\nCIFAR-10 training data, and we perform a small simulation to investigate the\nevolutions of these spectra over training.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 01:11:49 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 17:46:17 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 16:47:46 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Fan", "Zhou", ""], ["Wang", "Zhichao", ""]]}, {"id": "2005.11911", "submitter": "Zeyi Wang", "authors": "Zeyi Wang, Eric Bridgeford, Shangsi Wang, Joshua T. Vogelstein, Brian\n  Caffo", "title": "Statistical Analysis of Data Repeatability Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of modern data collection and processing techniques has seen the\nsize, scale, and complexity of data grow exponentially. A seminal step in\nleveraging these rich datasets for downstream inference is understanding the\ncharacteristics of the data which are repeatable -- the aspects of the data\nthat are able to be identified under a duplicated analysis. Conflictingly, the\nutility of traditional repeatability measures, such as the intraclass\ncorrelation coefficient, under these settings is limited. In recent work, novel\ndata repeatability measures have been introduced in the context where a set of\nsubjects are measured twice or more, including: fingerprinting, rank sums, and\ngeneralizations of the intraclass correlation coefficient. However, the\nrelationships between, and the best practices among these measures remains\nlargely unknown. In this manuscript, we formalize a novel repeatability\nmeasure, discriminability. We show that it is deterministically linked with the\ncorrelation coefficient under univariate random effect models, and has desired\nproperty of optimal accuracy for inferential tasks using multivariate\nmeasurements. Additionally, we overview and systematically compare\nrepeatability statistics using both theoretical results and simulations. We\nshow that the rank sum statistic is deterministically linked to a consistent\nestimator of discriminability. The power of permutation tests derived from\nthese measures are compared numerically under Gaussian and non-Gaussian\nsettings, with and without simulated batch effects. Motivated by both\ntheoretical and empirical results, we provide methodological recommendations\nfor each benchmark setting to serve as a resource for future analyses. We\nbelieve these recommendations will play an important role towards improving\nrepeatability in fields such as functional magnetic resonance imaging,\ngenomics, pharmacology, and more.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 03:47:09 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 19:38:04 GMT"}, {"version": "v3", "created": "Thu, 20 Aug 2020 08:57:51 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Wang", "Zeyi", ""], ["Bridgeford", "Eric", ""], ["Wang", "Shangsi", ""], ["Vogelstein", "Joshua T.", ""], ["Caffo", "Brian", ""]]}, {"id": "2005.11986", "submitter": "Jean Bertoin", "authors": "Jean Bertoin", "title": "How linear reinforcement affects Donsker's Theorem for empirical\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reinforcement algorithm introduced by H.A. Simon \\cite{Simon} produces a\nsequence of uniform random variables with memory as follows. At each step, with\na fixed probability $p\\in(0,1)$, $\\hat U_{n+1}$ is sampled uniformly from $\\hat\nU_1, \\ldots, \\hat U_n$, and with complementary probability $1-p$, $\\hat\nU_{n+1}$ is a new independent uniform variable. The Glivenko-Cantelli theorem\nremains valid for the reinforced empirical measure, but not the Donsker\ntheorem. Specifically, we show that the sequence of empirical processes\nconverges in law to a Brownian bridge only up to a constant factor when\n$p<1/2$, and that a further rescaling is needed when $p>1/2$ and the limit is\nthen a bridge with exchangeable increments and discontinuous paths. This is\nrelated to earlier limit theorems for correlated Bernoulli processes, the\nso-called elephant random walk, and more generally step reinforced random\nwalks.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 09:07:09 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Bertoin", "Jean", ""]]}, {"id": "2005.12068", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh, Erica Ponzi, Torkjel Sandanger, Magne Thoresen", "title": "Robust Sure Independence Screening for Non-polynomial dimensional\n  Generalized Linear Models", "comments": "Pre-print; Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variable screening in ultra-high dimensional\ngeneralized linear models (GLMs) of non-polynomial orders. Since the popular\nSIS approach is extremely unstable in the presence of contamination and noise,\nwe discuss a new robust screening procedure based on the minimum density power\ndivergence estimator (MDPDE) of the marginal regression coefficients. Our\nproposed screening procedure performs well under pure and contaminated data\nscenarios. We provide a theoretical motivation for the use of marginal MDPDEs\nfor variable screening from both population as well as sample aspects; in\nparticular, we prove that the marginal MDPDEs are uniformly consistent leading\nto the sure screening property of our proposed algorithm. Finally, we propose\nan appropriate MDPDE based extension for robust conditional screening in GLMs\nalong with the derivation of its sure screening property. Our proposed methods\nare illustrated through extensive numerical studies along with an interesting\nreal data application.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 12:18:10 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 12:48:08 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ghosh", "Abhik", ""], ["Ponzi", "Erica", ""], ["Sandanger", "Torkjel", ""], ["Thoresen", "Magne", ""]]}, {"id": "2005.12093", "submitter": "Paul Doukhan M.", "authors": "Paul Doukhan and Naushad Mamode Khan and Michael H. Neumann", "title": "Mixing properties of Skellam-GARCH processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider integer-valued GARCH processes, where the count variable\nconditioned on past values of the count and state variables follows a so-called\nSkellam distribution. Using arguments for contractive Markov chains we prove\nthat the process has a unique stationary regime. Furthermore, we show\nasymptotic regularity ($\\beta$-mixing) with geometrically decaying coefficients\nfor the count process. These probabilistic results are complemented by a\nstatistical analysis, a few simulations as well as an application to recent\nCOVID-19 data.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 13:16:30 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 09:51:26 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Doukhan", "Paul", ""], ["Khan", "Naushad Mamode", ""], ["Neumann", "Michael H.", ""]]}, {"id": "2005.12395", "submitter": "Davide Viviano Mr.", "authors": "Davide Viviano, Jelena Bradic", "title": "Fair Policy Targeting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major concerns of targeting interventions on individuals in social\nwelfare programs is discrimination: individualized treatments may induce\ndisparities on sensitive attributes such as age, gender, or race. This paper\naddresses the question of the design of fair and efficient treatment allocation\nrules. We adopt the non-maleficence perspective of \"first do no harm\": we\npropose to select the fairest allocation within the Pareto frontier. We provide\nenvy-freeness justifications to novel counterfactual notions of fairness. We\ndiscuss easy-to-implement estimators of the policy function, by casting the\noptimization into a mixed-integer linear program formulation. We derive regret\nbounds on the unfairness of the estimated policy function, and small sample\nguarantees on the Pareto frontier. Finally, we illustrate our method using an\napplication from education economics.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 20:45:25 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 17:53:09 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Viviano", "Davide", ""], ["Bradic", "Jelena", ""]]}, {"id": "2005.12415", "submitter": "Jason Sun", "authors": "Daqian Sun, Martin T. Wells", "title": "Robust Matrix Completion with Mixed Data Types", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the matrix completion problem of recovering a structured low rank\nmatrix with partially observed entries with mixed data types. Vast majority of\nthe solutions have proposed computationally feasible estimators with strong\nstatistical guarantees for the case where the underlying distribution of data\nin the matrix is continuous. A few recent approaches have extended using\nsimilar ideas these estimators to the case where the underlying distributions\nbelongs to the exponential family. Most of these approaches assume that there\nis only one underlying distribution and the low rank constraint is regularized\nby the matrix Schatten Norm. We propose a computationally feasible statistical\napproach with strong recovery guarantees along with an algorithmic framework\nsuited for parallelization to recover a low rank matrix with partially observed\nentries for mixed data types in one step. We also provide extensive simulation\nevidence that corroborate our theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2020 21:35:10 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Sun", "Daqian", ""], ["Wells", "Martin T.", ""]]}, {"id": "2005.12479", "submitter": "Takeru Matsuda", "authors": "Takeru Matsuda, William E. Strawderman", "title": "Estimation under matrix quadratic loss and matrix superharmonicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate estimation of a normal mean matrix under the matrix quadratic\nloss. Improved estimation under the matrix quadratic loss implies improved\nestimation of any linear combination of the columns. First, an unbiased\nestimate of risk is derived and the Efron--Morris estimator is shown to be\nminimax. Next, a notion of \\textit{matrix superharmonicity} for matrix-variate\nfunctions is introduced and shown to have analogous properties with usual\nsuperharmonic functions, which may be of independent interest. Then, we show\nthat the generalized Bayes estimator with respect to a matrix superharmonic\nprior is minimax. We also provide a class of matrix superharmonic priors that\nincludes the previously proposed generalization of Stein's prior. Numerical\nresults demonstrate that matrix superharmonic priors work well for low rank\nmatrices.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 01:55:24 GMT"}, {"version": "v2", "created": "Fri, 25 Dec 2020 03:36:20 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 04:10:30 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Matsuda", "Takeru", ""], ["Strawderman", "William E.", ""]]}, {"id": "2005.12601", "submitter": "Thomas Berrett", "authors": "Thomas B. Berrett and Cristina Butucea", "title": "Locally private non-asymptotic testing of discrete distributions is\n  faster using interactive mechanisms", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find separation rates for testing multinomial or more general discrete\ndistributions under the constraint of local differential privacy. We construct\nefficient randomized algorithms and test procedures, in both the case where\nonly non-interactive privacy mechanisms are allowed and also in the case where\nall sequentially interactive privacy mechanisms are allowed. The separation\nrates are faster in the latter case. We prove general information theoretical\nbounds that allow us to establish the optimality of our algorithms among all\npairs of privacy mechanisms and test procedures, in most usual cases.\nConsidered examples include testing uniform, polynomially and exponentially\ndecreasing distributions.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 09:43:08 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Butucea", "Cristina", ""]]}, {"id": "2005.12720", "submitter": "Valentin Courgeau", "authors": "Valentin Courgeau, Almut E. D. Veraart", "title": "Likelihood theory for the Graph Ornstein-Uhlenbeck process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of modelling restricted interactions between\ncontinuously-observed time series as given by a known static graph (or network)\nstructure. For this purpose, we define a parametric multivariate Graph\nOrnstein-Uhlenbeck (GrOU) process driven by a general L\\'evy process to study\nthe momentum and network effects amongst nodes, effects that quantify the\nimpact of a node on itself and that of its neighbours, respectively. We derive\nthe maximum likelihood estimators (MLEs) and their usual properties (existence,\nuniqueness and efficiency) along with their asymptotic normality and\nconsistency. Additionally, an Adaptive Lasso approach, or a penalised\nlikelihood scheme, infers both the graph structure along with the GrOU\nparameters concurrently and is shown to satisfy similar properties. Finally, we\nshow that the asymptotic theory extends to the case when stochastic volatility\nmodulation of the driving L\\'evy process is considered.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 13:46:35 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 14:19:08 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Courgeau", "Valentin", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "2005.12900", "submitter": "Yuxin Chen", "authors": "Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, Yuxin Chen", "title": "Breaking the Sample Size Barrier in Model-Based Reinforcement Learning\n  with a Generative Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the sample efficiency of reinforcement learning in a\n$\\gamma$-discounted infinite-horizon Markov decision process (MDP) with state\nspace $\\mathcal{S}$ and action space $\\mathcal{A}$, assuming access to a\ngenerative model. Despite a number of prior work tackling this problem, a\ncomplete picture of the trade-offs between sample complexity and statistical\naccuracy is yet to be determined. In particular, prior results suffer from a\nsample size barrier, in the sense that their claimed statistical guarantees\nhold only when the sample size exceeds at least\n$\\frac{|\\mathcal{S}||\\mathcal{A}|}{(1-\\gamma)^2}$ (up to some log factor). The\ncurrent paper overcomes this barrier by certifying the minimax optimality of\nmodel-based reinforcement learning as soon as the sample size exceeds the order\nof $\\frac{|\\mathcal{S}||\\mathcal{A}|}{1-\\gamma}$ (modulo some log factor). More\nspecifically, a perturbed model-based planning algorithm provably finds an\n$\\varepsilon$-optimal policy with an order of $\\frac{|\\mathcal{S}||\\mathcal{A}|\n}{(1-\\gamma)^3\\varepsilon^2}\\log\\frac{|\\mathcal{S}||\\mathcal{A}|}{(1-\\gamma)\\varepsilon}$\nsamples for any $\\varepsilon \\in (0, \\frac{1}{1-\\gamma}]$. Along the way, we\nderive improved (instance-dependent) guarantees for model-based policy\nevaluation. To the best of our knowledge, this work provides the first\nminimax-optimal guarantee in a generative model that accommodates the entire\nrange of sample sizes (beyond which finding a meaningful policy is information\ntheoretically impossible).\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 17:53:18 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 17:50:39 GMT"}, {"version": "v3", "created": "Thu, 3 Sep 2020 02:02:42 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Li", "Gen", ""], ["Wei", "Yuting", ""], ["Chi", "Yuejie", ""], ["Gu", "Yuantao", ""], ["Chen", "Yuxin", ""]]}, {"id": "2005.13011", "submitter": "Sara Algeri", "authors": "Sara Algeri and Xiangyu Zhang", "title": "Exhaustive goodness-of-fit via smoothed inference and graphics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical tests of goodness-of-fit aim to validate the conformity of a\npostulated model to the data under study. Given their inferential nature, they\ncan be considered a crucial step in confirmatory data analysis. In their\nstandard formulation, however, they do not allow exploring how the hypothesized\nmodel deviates from the truth nor do they provide any insight into how the\nrejected model could be improved to better fit the data. The main goal of this\nwork is to establish a comprehensive framework for goodness-of-fit which\nnaturally integrates modeling, estimation, inference, and graphics. Modeling\nand estimation focus on a novel formulation of smooth tests that easily extends\nto arbitrary distributions, either continuous or discrete. Inference and\nadequate post-selection adjustments are performed via a specially designed\nsmoothed bootstrap and the results are summarized via an exhaustive graphical\ntool called CD-plot.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 20:14:17 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 18:21:35 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Algeri", "Sara", ""], ["Zhang", "Xiangyu", ""]]}, {"id": "2005.13078", "submitter": "Sakshi Arya", "authors": "Sakshi Arya and Yuhong Yang", "title": "To update or not to update? Delayed Nonparametric Bandits with\n  Randomized Allocation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Delayed rewards problem in contextual bandits has been of interest in various\npractical settings. We study randomized allocation strategies and provide an\nunderstanding on how the exploration-exploitation tradeoff is affected by\ndelays in observing the rewards. In randomized strategies, the extent of\nexploration-exploitation is controlled by a user-determined exploration\nprobability sequence. In the presence of delayed rewards, one may choose\nbetween using the original exploration sequence that updates at every time\npoint or update the sequence only when a new reward is observed, leading to two\ncompeting strategies. In this work, we show that while both strategies may lead\nto strong consistency in allocation, the property holds for a wider scope of\nsituations for the latter. However, for finite sample performance, we\nillustrate that both strategies have their own advantages and disadvantages,\ndepending on the severity of the delay and underlying reward generating\nmechanisms.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2020 23:06:20 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Arya", "Sakshi", ""], ["Yang", "Yuhong", ""]]}, {"id": "2005.13274", "submitter": "Wai Leong Ng", "authors": "Wai Leong Ng, Chun Yip Yau", "title": "Asymptotic Spectral Theory for Spatial Data", "comments": "33 pages, 4 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the asymptotic theory for spectral analysis of\nstationary random fields, including linear and nonlinear fields. Asymptotic\nproperties of Fourier coefficients and periodograms, including limiting\ndistributions of Fourier coefficients, and the uniform consistency of kernel\nspectral density estimators are obtained under various mild conditions on\nmoments and dependence structures. The validity of the aforementioned\nasymptotic results for estimated spatial fields is also established.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 10:46:14 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 10:11:09 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ng", "Wai Leong", ""], ["Yau", "Chun Yip", ""]]}, {"id": "2005.13650", "submitter": "Daniel Fraiman", "authors": "In\\'es Armend\\'ariz, Pablo A. Ferrari, Daniel Fraiman, Jos\\'e M.\n  Mart\\'inez, Silvina Ponce Dawson", "title": "Group testing with nested pools", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST physics.data-an q-bio.QM stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to identify the infected individuals of a population, their samples\nare divided in equally sized groups called pools and a single laboratory test\nis applied to each pool. Individuals whose samples belong to pools that test\nnegative are declared healthy, while each pool that tests positive is divided\ninto smaller, equally sized pools which are tested in the next stage. This\nscheme is called adaptive, because the composition of the pools at each stage\ndepends on results from previous stages, and nested because each pool is a\nsubset of a pool of the previous stage. Is the infection probability $p$ is not\nsmaller than $1-3^{-1/3}$ it is best to test each sample (no pooling). If\n$p<1-3^{-1/3}$, we compute the mean $D_k(m,p)$ and the variance of the number\nof tests per individual as a function of the pool sizes $m=(m_1,\\dots,m_k)$ in\nthe first $k$ stages; in the $(k+1)$-th stage all remaining samples are tested.\nThe case $k=1$ was proposed by Dorfman in his seminal paper in 1943. The goal\nis to minimize $D_k(m,p)$, which is called the cost associated to~$m$. We show\nthat for $p\\in (0, 1-3^{-1/3})$ the optimal choice is one of four possible\nschemes, which are explicitly described. For $p>2^{-51}$ we show overwhelming\nnumerical evidence that the best choice is $(3^k\\text{ or\n}3^{k-1}4,3^{k-1},\\dots,3^2,3 )$, with a precise description of the range of\n$p$'s where each holds. We then focus on schemes of the type $(3^k,\\dots,3)$,\nand estimate that the cost of the best scheme of this type for $p$, determined\nby the choice of $k=k_3(p)$, is of order $O\\big(p\\log(1/p)\\big)$. This is the\nsame order as that of the cost of the optimal scheme, and the difference of\nthese costs is explicitly bounded. As an example, for $p=0.02$ the optimal\nchoice is $k=3$, $m=(27,9,3)$, with cost $0.20$; that is, the mean number of\ntests required to screen 100 individuals is 20.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2020 21:00:31 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2020 23:43:00 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 22:17:46 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Armend\u00e1riz", "In\u00e9s", ""], ["Ferrari", "Pablo A.", ""], ["Fraiman", "Daniel", ""], ["Mart\u00ednez", "Jos\u00e9 M.", ""], ["Dawson", "Silvina Ponce", ""]]}, {"id": "2005.13805", "submitter": "Rizky Reza Fauzi", "authors": "Rizky Reza Fauzi, Yoshihiko Maesono", "title": "Boundary-free Estimators of the Mean Residual Life Function by\n  Transformation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two new kernel-type estimators of the mean residual life function\n$m_X(t)$ of bounded or half-bounded interval supported distributions. Though\nnot as severe as the boundary problems in the kernel density estimation,\neliminating the boundary bias problems that occur in the naive kernel estimator\nof the mean residual life function is needed. In this article, we utilize the\nproperty of bijective transformation. Furthermore, our proposed methods\npreserve the mean value property, which cannot be done by the naive kernel\nestimator. Some simulation results showing the estimators' performances and a\nreal data analysis will be presented in the last part of this article.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 06:54:37 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Fauzi", "Rizky Reza", ""], ["Maesono", "Yoshihiko", ""]]}, {"id": "2005.13846", "submitter": "Masatoshi Goda", "authors": "Masatoshi Goda", "title": "Hawkes process and Edgeworth expansion with application to maximum\n  likelihood estimator", "comments": null, "journal-ref": "Statistical Inference for Stochastic Processes. 24 (2021) 277-325", "doi": "10.1007/s11203-021-09237-5", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hawks process is a point process with a self-exciting property. It has\nbeen used to model earthquakes, social media events, infections, etc., and is\ngetting a lot of attention. However, as a real problem, there are often\nsituations where we can not obtain data with sufficient observation time. In\nsuch cases, it is not appropriate to approximate the error distribution of an\nestimator by the normal distribution. To overcome this problem, we provide a\nrigorous mathematical foundation of the theory for the higher-order asymptotic\nbehavior of the one-dimensional Hawkes process with an exponential kernel. As\nan important application, we give the second-order asymptotic distribution for\nthe maximum likelihood estimator of the exponential Hawkes process.\nFurthermore, we also present the simulation results.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 08:54:12 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 13:33:08 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Goda", "Masatoshi", ""]]}, {"id": "2005.13951", "submitter": "Fr\\'ed\\'eric Pro\\\"ia", "authors": "Fr\\'ed\\'eric Pro\\\"ia, Marius Soltane", "title": "Comments on the presence of serial correlation in the random\n  coefficients of an autoregressive process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an RCAR$(p)$ process and we establish that the standard\nestimation lacks consistency as soon as there exists a nonzero serial\ncorrelation in the coefficients. We give the correct asymptotic behavior and\nsome simulations come to illustrate the results.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 12:38:32 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:26:08 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Pro\u00efa", "Fr\u00e9d\u00e9ric", ""], ["Soltane", "Marius", ""]]}, {"id": "2005.14073", "submitter": "Banghua Zhu", "authors": "Banghua Zhu, Jiantao Jiao and Jacob Steinhardt", "title": "Robust estimation via generalized quasi-gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore why many recently proposed robust estimation problems are\nefficiently solvable, even though the underlying optimization problems are\nnon-convex. We study the loss landscape of these robust estimation problems,\nand identify the existence of \"generalized quasi-gradients\". Whenever these\nquasi-gradients exist, a large family of low-regret algorithms are guaranteed\nto approximate the global minimum; this includes the commonly-used filtering\nalgorithm.\n  For robust mean estimation of distributions under bounded covariance, we show\nthat any first-order stationary point of the associated optimization problem is\nan {approximate global minimum} if and only if the corruption level $\\epsilon <\n1/3$. Consequently, any optimization algorithm that aproaches a stationary\npoint yields an efficient robust estimator with breakdown point $1/3$. With\ncareful initialization and step size, we improve this to $1/2$, which is\noptimal.\n  For other tasks, including linear regression and joint mean and covariance\nestimation, the loss landscape is more rugged: there are stationary points\narbitrarily far from the global minimum. Nevertheless, we show that generalized\nquasi-gradients exist and construct efficient algorithms. These algorithms are\nsimpler than previous ones in the literature, and for linear regression we\nimprove the estimation error from $O(\\sqrt{\\epsilon})$ to the optimal rate of\n$O(\\epsilon)$ for small $\\epsilon$ assuming certified hypercontractivity. For\nmean estimation with near-identity covariance, we show that a simple gradient\ndescent algorithm achieves breakdown point $1/3$ and iteration complexity\n$\\tilde{O}(d/\\epsilon^2)$.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2020 15:14:33 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["Zhu", "Banghua", ""], ["Jiao", "Jiantao", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2005.14508", "submitter": "Lixing Zhu", "authors": "Keli Guo (1), Chuyun Ye (3), Jun Fan (1), Lixing Zhu (1 and 2) ((1)\n  Department of Mathematics, Hong Kong Baptist University, Hong Kong, (2)\n  Center for Statistics and Data Science, Beijing Normal University, Zhuhai,\n  China, (3) School of Statistics, Beijing Normal University, Beijing, China)", "title": "Doubly robust estimation of average treatment effect revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research described herewith is to re-visit the classical doubly robust\nestimation of average treatment effect by conducting a systematic study on the\ncomparisons, in the sense of asymptotic efficiency, among all possible\ncombinations of the estimated propensity score and outcome regression. To this\nend, we consider all nine combinations under, respectively, parametric,\nnonparametric and semiparametric structures. The comparisons provide useful\ninformation on when and how to efficiently utilize the model structures in\npractice. Further, when there is model-misspecification, either propensity\nscore or outcome regression, we also give the corresponding comparisons. Three\nphenomena are observed. Firstly, when all models are correctly specified, any\ncombination can achieve the same semiparametric efficiency bound, which\ncoincides with the existing results of some combinations. Secondly, when the\npropensity score is correctly modeled and estimated, but the outcome regression\nis misspecified parametrically or semiparametrically, the asymptotic variance\nis always larger than or equal to the semiparametric efficiency bound. Thirdly,\nin contrast, when the propensity score is misspecified parametrically or\nsemiparametrically, while the outcome regression is correctly modeled and\nestimated, the asymptotic variance is not necessarily larger than the\nsemiparametric efficiency bound. In some cases, the \"super-efficiency\"\nphenomenon occurs. We also conduct a small numerical study.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 11:46:15 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Guo", "Keli", "", "1 and 2"], ["Ye", "Chuyun", "", "1 and 2"], ["Fan", "Jun", "", "1 and 2"], ["Zhu", "Lixing", "", "1 and 2"]]}, {"id": "2005.14599", "submitter": "Teppei Ogihara", "authors": "Masaaki Fukasawa and Teppei Ogihara", "title": "Malliavin calculus techniques for local asymptotic mixed normality and\n  their application to degenerate diffusions", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sufficient conditions for a local asymptotic mixed normality\nproperty of statistical models. We develop a scheme with the $L^2$ regularity\ncondition proposed by Jeganathan [\\textit{Sankhya Ser. A} \\textbf{44} (1982)\n173--212] so that it is applicable to high-frequency observations of stochastic\nprocesses. Moreover, by combining with Malliavin calculus techniques by Gobet\n[\\textit{Bernoulli} \\textbf{7} (2001) 899--912, 2001], we introduce tractable\nsufficient conditions for smooth observations in the Malliavin sense, which do\nnot require Aronson-type estimates of the transition density function. Our\nresults, unlike those in the literature, can be applied even when the\ntransition density function has zeros. For an application, we show the local\nasymptotic mixed normality property of degenerate (hypoelliptic) diffusion\nmodels under high-frequency observations, in both complete and partial\nobservation frameworks. The former and the latter extend previous results for\nelliptic diffusions and for integrated diffusions, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 14:36:03 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 12:35:38 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Fukasawa", "Masaaki", ""], ["Ogihara", "Teppei", ""]]}]