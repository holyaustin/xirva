[{"id": "1511.00028", "submitter": "Gourab Mukherjee", "authors": "Gourab Mukherjee, Lawrence D. Brown and Paat Rusmevichientong", "title": "Efficient Empirical Bayes prediction under check loss using Asymptotic\n  Risk Estimates", "comments": "massively revamped; new insights, proofs and applied examples added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel Empirical Bayes methodology for prediction under check\nloss in high-dimensional Gaussian models. The check loss is a piecewise linear\nloss function having differential weights for measuring the amount of\nunderestimation or overestimation. Prediction under it differs in fundamental\naspects from estimation or prediction under weighted-quadratic losses. Because\nof the nature of this loss, our inferential target is a pre-chosen quantile of\nthe predictive distribution rather than the mean of the predictive\ndistribution. We develop a new method for constructing uniformly efficient\nasymptotic risk estimates which are then minimized to produce effective linear\nshrinkage predictive rules. In calculating the magnitude and direction of\nshrinkage, our proposed predictive rules incorporate the asymmetric nature of\nthe loss function and are shown to be asymptotically optimal. Using numerical\nexperiments we compare the performance of our method with traditional Empirical\nBayes procedures and obtain encouraging results.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 20:57:45 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 00:50:37 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Mukherjee", "Gourab", ""], ["Brown", "Lawrence D.", ""], ["Rusmevichientong", "Paat", ""]]}, {"id": "1511.00253", "submitter": "Lorenzo Mercuri", "authors": "Stefano M. Iacus, Lorenzo Mercuri, Edit Rroji", "title": "Discrete time approximation of a COGARCH(p,q) model and its estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we construct a sequence of discrete time stochastic processes\nthat converges in probability and in the Skorokhod metric to a COGARCH(p,q)\nmodel. The result is useful for the estimation of the continuous model defined\nfor irregularly spaced time series data. The estimation procedure is based on\nthe maximization of a pseudo log-likelihood function and is implemented in the\nyuima package.\n", "versions": [{"version": "v1", "created": "Sun, 1 Nov 2015 14:23:39 GMT"}, {"version": "v2", "created": "Sun, 6 Dec 2015 11:06:23 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Iacus", "Stefano M.", ""], ["Mercuri", "Lorenzo", ""], ["Rroji", "Edit", ""]]}, {"id": "1511.00420", "submitter": "Holger Drees", "authors": "Holger Drees", "title": "Bootstrapping Empirical Processes of Cluster Functionals with\n  Application to Extremograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the extreme value analysis of time series, not only the tail behavior is\nof interest, but also the serial dependence plays a crucial role. Drees and\nRootz\\'en (2010) established limit theorems for a general class of empirical\nprocesses of so-called cluster functionals which can be used to analyse various\naspects of the extreme value behavior of mixing time series. However, usually\nthe limit distribution is too complex to enable a direct construction of\nconfidence regions. Therefore, we suggest a multiplier block bootstrap analog\nto the empirical processes of cluster functionals. It is shown that under\nvirtually the same conditions as used by Drees and Rootz\\'en (2010),\nconditionally on the data, the bootstrap processes converge to the same limit\ndistribution. These general results are applied to construct confidence regions\nfor the empirical extremogram introduced by Davis and Mikosch (2009). In a\nsimulation study, the confidence intervals constructed by our multiplier block\nbootstrap approach compare favorably to the stationary bootstrap proposed by\nDavis et al.\\ (2012).\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 09:25:58 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Drees", "Holger", ""]]}, {"id": "1511.00507", "submitter": "Guillaume Chauvet", "authors": "H\\'el\\`ene Juillard and Guillaume Chauvet and Anne Ruiz-Gazen", "title": "Estimation under cross-classified sampling with application to a\n  childhood survey", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross-classified sampling design consists in drawing samples from a\ntwo-dimension population, independently in each dimension. Such design is\ncommonly used in consumer price index surveys and has been recently applied to\ndraw a sample of babies in the French ELFE survey, by crossing a sample of\nmaternity units and a sample of days. We propose to derive a general theory of\nestimation for this sampling design. We consider the Horvitz-Thompson estimator\nfor a total, and show that the cross-classified design will usually result in a\nloss of efficiency as compared to the widespread two-stage design. We obtain\nthe asymptotic distribution of the Horvitz-Thompson estimator, and several\nunbiased variance estimators. Facing the problem of possibly negative values,\nwe propose simplified non-negative variance estimators and study their bias\nunder a super-population model. The proposed estimators are compared for totals\nand ratios on simulated data. An application on real data from the ELFE survey\nis also presented, and we make some recommendations. Supplementary materials\nare available online.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 14:11:01 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Juillard", "H\u00e9l\u00e8ne", ""], ["Chauvet", "Guillaume", ""], ["Ruiz-Gazen", "Anne", ""]]}, {"id": "1511.00730", "submitter": "Linglong Kong", "authors": "Qianchuan He and Linglong Kong and Yanhua Wang and Sijian Wang and\n  Timothy A. Chan and Eric Holland", "title": "Regularized quantile regression under heterogeneous sparsity with\n  application to quantitative genetic traits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Genetic studies often involve quantitative traits. Identifying genetic\nfeatures that influence quantitative traits can help to uncover the etiology of\ndiseases. Quantile regression method considers the conditional quantiles of the\nresponse variable, and is able to characterize the underlying regression\nstructure in a more comprehensive manner. On the other hand, genetic studies\noften involve high dimensional genomic features, and the underlying regression\nstructure may be heterogeneous in terms of both effect sizes and sparsity. To\naccount for the potential genetic heterogeneity, including the heterogeneous\nsparsity, a regularized quantile regression method is introduced. The\ntheoretical property of the proposed method is investigated, and its\nperformance is examined through a series of simulation studies. A real dataset\nis analyzed to demonstrate the application of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 22:22:33 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["He", "Qianchuan", ""], ["Kong", "Linglong", ""], ["Wang", "Yanhua", ""], ["Wang", "Sijian", ""], ["Chan", "Timothy A.", ""], ["Holland", "Eric", ""]]}, {"id": "1511.00734", "submitter": "Anders Lindquist", "authors": "Anders Lindquist and Giorgio Picci", "title": "Modeling of Stationary Periodic Time Series by ARMA Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a survey of some recent results on the rational circulant covariance\nextension problem: Given a partial sequence $(c_0,c_1,\\dots,c_n)$ of covariance\nlags $c_k=\\mathbb{E}\\{y(t+k)\\overline{y(t)}\\}$ emanating from a stationary\nperiodic process $\\{y(t)\\}$ with period $2N>2n$, find all possible rational\nspectral functions of $\\{y(t)\\}$ of degree at most $2n$ or, equivalently, all\nbilateral and unilateral ARMA models of order at most $n$, having this partial\ncovariance sequence. Each representation is obtained as the solution of a pair\nof dual convex optimization problems. This theory is then reformulated in terms\nof circulant matrices and the connections to reciprocal processes and the\ncovariance selection problem is explained. Next it is shown how the theory can\nbe extended to the multivariate case. Finally, an application to image\nprocessing is presented.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 22:43:43 GMT"}, {"version": "v2", "created": "Thu, 17 Dec 2015 09:41:52 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Lindquist", "Anders", ""], ["Picci", "Giorgio", ""]]}, {"id": "1511.00820", "submitter": "Anne Marie Svane", "authors": "Julia H\\\"orrmann, Anne Marie Svane", "title": "Local digital algorithms applied to Boolean models", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.CG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the estimation of specific intrinsic volumes of stationary\nBoolean models by local digital algorithms; that is, by weighted sums of $n\n\\times\\ldots \\times n$ configuration counts. We show that asymptotically\nunbiased estimators for the specific surface area or integrated mean curvature\ndo not exist if the dimension is at least two or three, respectively. For\n3-dimensional stationary, isotropic Boolean models, we derive asymptotically\nunbiased estimators for the specific surface area and integrated mean\ncurvature. For a Boolean model with balls as grains we even obtain an\nasymptotically unbiased estimator for the specific Euler characteristic.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 09:08:16 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["H\u00f6rrmann", "Julia", ""], ["Svane", "Anne Marie", ""]]}, {"id": "1511.00860", "submitter": "Daniel Commenges", "authors": "Daniel Commenges", "title": "Information Theory and Statistics: an overview", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an overview of the role of information theory in statistics, and\nparticularly in biostatistics. We recall the basic quantities in information\ntheory; entropy, cross-entropy, conditional entropy, mutual information and\nKullback-Leibler risk. Then we examine the role of information theory in\nestimation theory, where the log-klikelihood can be identified as being an\nestimator of a cross-entropy. Then the basic quantities are extended to\nestimators, leading to criteria for estimator selection, such as Akaike\ncriterion and its extensions. Finally we investigate the use of these concepts\nin Bayesian theory; the cross-entropy of the predictive distribution can be\nused for model selection; a cross-validation estimator of this cross-entropy is\nfound to be equivalent to the pseudo-Bayes factor.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 11:20:26 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Commenges", "Daniel", ""]]}, {"id": "1511.00926", "submitter": "Nathan Owen", "authors": "N. E. Owen, P. Challenor, P. P. Menon, S. Bennani", "title": "Comparison of surrogate-based uncertainty quantification methods for\n  computationally expensive simulators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial chaos and Gaussian process emulation are methods for\nsurrogate-based uncertainty quantification, and have been developed\nindependently in their respective communities over the last 25 years. Despite\ntackling similar problems in the field, to our knowledge there has yet to be a\ncritical comparison of the two approaches in the literature. We begin by\nproviding a detailed description of polynomial chaos and Gaussian process\napproaches for building a surrogate model of a black-box function. The accuracy\nof each surrogate method is then tested and compared for two simulators used in\nindustry: a land-surface model (adJULES) and a launch vehicle controller\n(VEGACONTROL). We analyse surrogates built on experimental designs of various\nsize and type to investigate their performance in a range of modelling\nscenarios. Specifically, polynomial chaos and Gaussian process surrogates are\nbuilt on Sobol sequence and tensor grid designs. Their accuracy is measured by\ntheir ability to estimate the mean, standard deviation, exceedance\nprobabilities and probability density function of the simulator output, as well\nas a root mean square error metric, based on an independent validation design.\nWe find that one method does not unanimously outperform the other, but\nadvantages can be gained in some cases, such that the preferred method depends\non the modelling goals of the practitioner. Our conclusions are likely to\ndepend somewhat on the modelling choices for the surrogates as well as the\ndesign strategy. We hope that this work will spark future comparisons of the\ntwo methods in their more advanced formulations and for different sampling\nstrategies.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 14:39:33 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 16:47:37 GMT"}, {"version": "v3", "created": "Fri, 14 Oct 2016 11:20:59 GMT"}, {"version": "v4", "created": "Fri, 13 Jan 2017 13:29:12 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Owen", "N. E.", ""], ["Challenor", "P.", ""], ["Menon", "P. P.", ""], ["Bennani", "S.", ""]]}, {"id": "1511.01009", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro, G\\'abor Lugosi, Nicolas Verzelen", "title": "Detecting a Path of Correlations in a Network", "comments": "arXiv admin note: text overlap with arXiv:1504.06984", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting an anomaly in the form of a path of\ncorrelations hidden in white noise. We provide a minimax lower bound and a test\nthat, under mild assumptions, is able to achieve the lower bound up to a\nmultiplicative constant.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 17:44:40 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 10:06:12 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Lugosi", "G\u00e1bor", ""], ["Verzelen", "Nicolas", ""]]}, {"id": "1511.01017", "submitter": "Ali Mousavi", "authors": "Ali Mousavi, Arian Maleki, Richard G. Baraniuk", "title": "Consistent Parameter Estimation for LASSO and Approximate Message\n  Passing", "comments": "arXiv admin note: text overlap with arXiv:1309.5979", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a vector $\\beta_o \\in \\mathbb{R}^p$\nfrom $n$ random and noisy linear observations $y= X\\beta_o + w$, where $X$ is\nthe measurement matrix and $w$ is noise. The LASSO estimate is given by the\nsolution to the optimization problem $\\hat{\\beta}_{\\lambda} = \\arg \\min_{\\beta}\n\\frac{1}{2} \\|y-X\\beta\\|_2^2 + \\lambda \\| \\beta \\|_1$. Among the iterative\nalgorithms that have been proposed for solving this optimization problem,\napproximate message passing (AMP) has attracted attention for its fast\nconvergence. Despite significant progress in the theoretical analysis of the\nestimates of LASSO and AMP, little is known about their behavior as a function\nof the regularization parameter $\\lambda$, or the thereshold parameters\n$\\tau^t$. For instance the following basic questions have not yet been studied\nin the literature: (i) How does the size of the active set\n$\\|\\hat{\\beta}^\\lambda\\|_0/p$ behave as a function of $\\lambda$? (ii) How does\nthe mean square error $\\|\\hat{\\beta}_{\\lambda} - \\beta_o\\|_2^2/p$ behave as a\nfunction of $\\lambda$? (iii) How does $\\|\\beta^t - \\beta_o \\|_2^2/p$ behave as\na function of $\\tau^1, \\ldots, \\tau^{t-1}$? Answering these questions will help\nin addressing practical challenges regarding the optimal tuning of $\\lambda$ or\n$\\tau^1, \\tau^2, \\ldots$. This paper answers these questions in the asymptotic\nsetting and shows how these results can be employed in deriving simple and\ntheoretically optimal approaches for tuning the parameters $\\tau^1, \\ldots,\n\\tau^t$ for AMP or $\\lambda$ for LASSO. It also explores the connection between\nthe optimal tuning of the parameters of AMP and the optimal tuning of LASSO.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 18:05:21 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2015 16:20:58 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Mousavi", "Ali", ""], ["Maleki", "Arian", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1511.01437", "submitter": "Sourav Chatterjee", "authors": "Sourav Chatterjee, Persi Diaconis", "title": "The sample size required in importance sampling", "comments": "37 pages, 4 figures. To appear in Ann. App. Probab", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.NA math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of importance sampling is to estimate the expected value of a given\nfunction with respect to a probability measure $\\nu$ using a random sample of\nsize $n$ drawn from a different probability measure $\\mu$. If the two measures\n$\\mu$ and $\\nu$ are nearly singular with respect to each other, which is often\nthe case in practice, the sample size required for accurate estimation is\nlarge. In this article it is shown that in a fairly general setting, a sample\nof size approximately $\\exp(D(\\nu||\\mu))$ is necessary and sufficient for\naccurate estimation by importance sampling, where $D(\\nu||\\mu)$ is the\nKullback-Leibler divergence of $\\mu$ from $\\nu$. In particular, the required\nsample size exhibits a kind of cut-off in the logarithmic scale. The theory is\napplied to obtain a general formula for the sample size required in importance\nsampling for one-parameter exponential families (Gibbs measures).\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 19:06:25 GMT"}, {"version": "v2", "created": "Sun, 15 Nov 2015 04:48:41 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 03:36:08 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Chatterjee", "Sourav", ""], ["Diaconis", "Persi", ""]]}, {"id": "1511.01478", "submitter": "Joshua Loftus", "authors": "Joshua R. Loftus and Jonathan E. Taylor", "title": "Selective inference in regression models with groups of variables", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a general mathematical framework for selective inference with\nsupervised model selection procedures characterized by quadratic forms in the\noutcome variable. Forward stepwise with groups of variables is an important\nspecial case as it allows models with categorical variables or factors. Models\ncan be chosen by AIC, BIC, or a fixed number of steps. We provide an exact\nsignificance test for each group of variables in the selected model based on an\nappropriately truncated $\\chi$ or $F$ distribution for the cases of known and\nunknown $\\sigma^2$ respectively. An efficient software implementation is\navailable as a package in the R statistical programming language.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 20:56:25 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Loftus", "Joshua R.", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1511.01752", "submitter": "Olivier Wintenberger", "authors": "Olivier Wintenberger (LSTA)", "title": "Exponential inequalities for unbounded functions of geometrically\n  ergodic Markov chains. Applications to quantitative error bounds for\n  regenerative Metropolis algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this note is to investigate the concentration properties of\nunbounded functions of geometrically ergodic Markov chains. We derive\nconcentration properties of centered functions with respect to the square of\nthe Lyapunov's function in the drift condition satisfied by the Markov chain.\nWe apply the new exponential inequalities to derive confidence intervals for\nMCMC algorithms. Quantitative error bounds are providing for the regenerative\nMetropolis algorithm of [5].\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 14:13:18 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 14:22:52 GMT"}], "update_date": "2016-09-14", "authors_parsed": [["Wintenberger", "Olivier", "", "LSTA"]]}, {"id": "1511.01803", "submitter": "Nurzhan Nurushev", "authors": "Eduard Belitser and Nurzhan Nurushev", "title": "Needles and straw in a haystack: robust confidence for possibly sparse\n  sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the general signal+noise model we construct an empirical Bayes posterior\nwhich we then use for uncertainty quantification for the unknown, possibly\nsparse, signal. We introduce a novel excessive bias restriction (EBR)\ncondition, which gives rise to a new slicing of the entire space that is\nsuitable for uncertainty quantification. Under EBR and some mild conditions on\nthe noise, we establish the local (oracle) optimality of the proposed\nconfidence ball. In passing, we also get the local optimal (oracle) results for\nestimation and posterior contraction problems. Adaptive minimax results (also\nfor the estimation and posterior contraction problems) over various sparsity\nclasses follow from our local results.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 16:33:02 GMT"}, {"version": "v2", "created": "Mon, 4 Apr 2016 08:58:50 GMT"}, {"version": "v3", "created": "Fri, 11 Nov 2016 14:44:53 GMT"}, {"version": "v4", "created": "Sat, 26 Nov 2016 16:59:21 GMT"}, {"version": "v5", "created": "Wed, 15 Feb 2017 16:36:43 GMT"}, {"version": "v6", "created": "Sun, 11 Mar 2018 18:34:54 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Belitser", "Eduard", ""], ["Nurushev", "Nurzhan", ""]]}, {"id": "1511.01863", "submitter": "Anders Eklund", "authors": "Anders Eklund, Thomas Nichols, Hans Knutsson", "title": "Can parametric statistical methods be trusted for fMRI based group\n  studies?", "comments": null, "journal-ref": "PNAS (2016), vol. 113 no. 28, 7900 - 7905", "doi": "10.1073/pnas.1602413113", "report-no": null, "categories": "stat.AP math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The most widely used task fMRI analyses use parametric methods that depend on\na variety of assumptions. While individual aspects of these fMRI models have\nbeen evaluated, they have not been evaluated in a comprehensive manner with\nempirical data. In this work, a total of 2 million random task fMRI group\nanalyses have been performed using resting state fMRI data, to compute\nempirical familywise error rates for the software packages SPM, FSL and AFNI,\nas well as a standard non-parametric permutation method. While there is some\nvariation, for a nominal familywise error rate of 5% the parametric statistical\nmethods are shown to be conservative for voxel-wise inference and invalid for\ncluster-wise inference; in particular, cluster size inference with a cluster\ndefining threshold of p = 0.01 generates familywise error rates up to 60%. We\nconduct a number of follow up analyses and investigations that suggest the\ncause of the invalid cluster inferences is spatial auto correlation functions\nthat do not follow the assumed Gaussian shape. By comparison, the\nnon-parametric permutation test, which is based on a small number of\nassumptions, is found to produce valid results for voxel as well as cluster\nwise inference. Using real task data, we compare the results between one\nparametric method and the permutation test, and find stark differences in the\nconclusions drawn between the two using cluster inference. These findings speak\nto the need of validating the statistical methods being used in the\nneuroimaging field.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:34:57 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Eklund", "Anders", ""], ["Nichols", "Thomas", ""], ["Knutsson", "Hans", ""]]}, {"id": "1511.01957", "submitter": "Weijie Su", "authors": "Weijie Su, Malgorzata Bogdan, Emmanuel Candes", "title": "False Discoveries Occur Early on the Lasso Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression settings where explanatory variables have very low correlations\nand there are relatively few effects, each of large magnitude, we expect the\nLasso to find the important variables with few errors, if any. This paper shows\nthat in a regime of linear sparsity---meaning that the fraction of variables\nwith a non-vanishing effect tends to a constant, however small---this cannot\nreally be the case, even when the design variables are stochastically\nindependent. We demonstrate that true features and null features are always\ninterspersed on the Lasso path, and that this phenomenon occurs no matter how\nstrong the effect sizes are. We derive a sharp asymptotic trade-off between\nfalse and true positive rates or, equivalently, between measures of type I and\ntype II errors along the Lasso path. This trade-off states that if we ever want\nto achieve a type II error (false negative rate) under a critical value, then\nanywhere on the Lasso path the type I error (false positive rate) will need to\nexceed a given threshold so that we can never have both errors at a low level\nat the same time. Our analysis uses tools from approximate message passing\n(AMP) theory as well as novel elements to deal with a possibly adaptive\nselection of the Lasso regularizing parameter.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 23:51:51 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2015 04:06:10 GMT"}, {"version": "v3", "created": "Sun, 29 Nov 2015 07:08:38 GMT"}, {"version": "v4", "created": "Thu, 15 Sep 2016 02:45:47 GMT"}], "update_date": "2016-09-16", "authors_parsed": [["Su", "Weijie", ""], ["Bogdan", "Malgorzata", ""], ["Candes", "Emmanuel", ""]]}, {"id": "1511.01975", "submitter": "Varun  Jog", "authors": "Varun Jog and Po-Ling Loh", "title": "Persistence of centrality in random growing trees", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DM cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate properties of node centrality in random growing tree models.\nWe focus on a measure of centrality that computes the maximum subtree size of\nthe tree rooted at each node, with the most central node being the tree\ncentroid. For random trees grown according to a preferential attachment model,\na uniform attachment model, or a diffusion processes over a regular tree, we\nprove that a single node persists as the tree centroid after a finite number of\nsteps, with probability 1. Furthermore, this persistence property generalizes\nto the top $K \\ge 1$ nodes with respect to the same centrality measure. We also\nestablish necessary and sufficient conditions for the size of an initial seed\ngraph required to ensure persistence of a particular node with probability\n$1-\\epsilon$, as a function of $\\epsilon$: In the case of preferential and\nuniform attachment models, we derive bounds for the size of an initial hub\nconstructed around the special node. In the case of a diffusion process over a\nregular tree, we derive bounds for the radius of an initial ball centered\naround the special node. Our necessary and sufficient conditions match up to\nconstant factors for preferential attachment and diffusion tree models.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 03:09:31 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Jog", "Varun", ""], ["Loh", "Po-Ling", ""]]}, {"id": "1511.02112", "submitter": "Matthieu Lerasle", "authors": "M Lerasle (JAD), N Magalh\\~A{\\pounds}es (UPMC), P Reynaud-Bouret (JAD)", "title": "Optimal kernel selection for density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new general kernel selection rules thanks to penalized\nleast-squares criteria. We derive optimal oracle inequalities using adequate\nconcentration tools. We also investigate the problem of minimal penalty as\ndescribed in [BM07].\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 15:21:16 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Lerasle", "M", "", "JAD"], ["Magalh\u00c3\u00a3es", "N", "", "UPMC"], ["Reynaud-Bouret", "P", "", "JAD"]]}, {"id": "1511.02270", "submitter": "Matey Neykov", "authors": "Matey Neykov, Qian Lin, Jun S. Liu", "title": "Signed Support Recovery for Single Index Models in High-Dimensions", "comments": "38 pages, 7 figures; 1 table; data set analysis added; typos\n  corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the support recovery problem for single index models\n$Y=f(\\boldsymbol{X}^{\\intercal} \\boldsymbol{\\beta},\\varepsilon)$, where $f$ is\nan unknown link function, $\\boldsymbol{X}\\sim N_p(0,\\mathbb{I}_{p})$ and\n$\\boldsymbol{\\beta}$ is an $s$-sparse unit vector such that\n$\\boldsymbol{\\beta}_{i}\\in \\{\\pm\\frac{1}{\\sqrt{s}},0\\}$. In particular, we look\ninto the performance of two computationally inexpensive algorithms: (a) the\ndiagonal thresholding sliced inverse regression (DT-SIR) introduced by Lin et\nal. (2015); and (b) a semi-definite programming (SDP) approach inspired by\nAmini & Wainwright (2008). When $s=O(p^{1-\\delta})$ for some $\\delta>0$, we\ndemonstrate that both procedures can succeed in recovering the support of\n$\\boldsymbol{\\beta}$ as long as the rescaled sample size\n$\\kappa=\\frac{n}{s\\log(p-s)}$ is larger than a certain critical threshold. On\nthe other hand, when $\\kappa$ is smaller than a critical value, any algorithm\nfails to recover the support with probability at least $\\frac{1}{2}$\nasymptotically. In other words, we demonstrate that both DT-SIR and the SDP\napproach are optimal (up to a scalar) for recovering the support of\n$\\boldsymbol{\\beta}$ in terms of sample size. We provide extensive simulations,\nas well as a real dataset application to help verify our theoretical\nobservations.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 00:23:53 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2016 03:01:04 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Neykov", "Matey", ""], ["Lin", "Qian", ""], ["Liu", "Jun S.", ""]]}, {"id": "1511.02381", "submitter": "Shahab Asoodeh", "authors": "Shahab Asoodeh, Mario Diaz, Fady Alajaji, and Tam\\'as Linder", "title": "Information Extraction Under Privacy Constraints", "comments": "55 pages, 6 figures. Improved the organization and added detailed\n  literature review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A privacy-constrained information extraction problem is considered where for\na pair of correlated discrete random variables $(X,Y)$ governed by a given\njoint distribution, an agent observes $Y$ and wants to convey to a potentially\npublic user as much information about $Y$ as possible without compromising the\namount of information revealed about $X$. To this end, the so-called {\\em\nrate-privacy function} is introduced to quantify the maximal amount of\ninformation (measured in terms of mutual information) that can be extracted\nfrom $Y$ under a privacy constraint between $X$ and the extracted information,\nwhere privacy is measured using either mutual information or maximal\ncorrelation. Properties of the rate-privacy function are analyzed and\ninformation-theoretic and estimation-theoretic interpretations of it are\npresented for both the mutual information and maximal correlation privacy\nmeasures. It is also shown that the rate-privacy function admits a closed-form\nexpression for a large family of joint distributions of $(X,Y)$. Finally, the\nrate-privacy function under the mutual information privacy measure is\nconsidered for the case where $(X,Y)$ has a joint probability density function\nby studying the problem where the extracted information is a uniform\nquantization of $Y$ corrupted by additive Gaussian noise. The asymptotic\nbehavior of the rate-privacy function is studied as the quantization resolution\ngrows without bound and it is observed that not all of the properties of the\nrate-privacy function carry over from the discrete to the continuous case.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 17:27:44 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 01:46:42 GMT"}, {"version": "v3", "created": "Sun, 17 Jan 2016 20:39:48 GMT"}], "update_date": "2016-01-19", "authors_parsed": [["Asoodeh", "Shahab", ""], ["Diaz", "Mario", ""], ["Alajaji", "Fady", ""], ["Linder", "Tam\u00e1s", ""]]}, {"id": "1511.02515", "submitter": "Alisa Kirichenko", "authors": "Alisa Kirichenko, Harry van Zanten", "title": "Estimating a smooth function on a large graph by Bayesian Laplacian\n  regularisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a Bayesian approach to estimating a smooth function in the context\nof regression or classification problems on large graphs. We derive theoretical\nresults that show how asymptotically optimal Bayesian regularization can be\nachieved under an asymptotic shape assumption on the underlying graph and a\nsmoothness condition on the target function, both formulated in terms of the\ngraph Laplacian. The priors we study are randomly scaled Gaussians with\nprecision operators involving the Laplacian of the graph.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 19:11:11 GMT"}, {"version": "v2", "created": "Sun, 5 Mar 2017 10:13:11 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Kirichenko", "Alisa", ""], ["van Zanten", "Harry", ""]]}, {"id": "1511.02534", "submitter": "Chen Wang", "authors": "Z. D. Bai, Chen Wang, Ya Xue and Matthew Harding", "title": "Order Determination of Large Dimensional Dynamic Factor Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following dynamic factor model: $\\mathbf{R}_t=\\sum_{i=0}^q\n\\mathbf{\\Lambda}_i \\mathbf{f}_{t-i}+\\mathbf{e}_t,t=1,...,T$, where\n$\\mathbf{\\Lambda}_i$ is an $n\\times k$ loading matrix of full rank,\n$\\{\\mathbf{f}_t\\}$ are i.i.d. $k\\times1$-factors, and $\\mathbf{e}_t$ are\nindependent $n\\times1$ white noises. Now, assuming that $n/T\\to c>0$, we want\nto estimate the orders $k$ and $q$ respectively. Define a random matrix\n$$\\mathbf{\\Phi}_n(\\tau)=\\frac{1}{2T}\\sum_{j=1}^T (\\mathbf{R}_j\n\\mathbf{R}_{j+\\tau}^* + \\mathbf{R}_{j+\\tau} \\mathbf{R}_j^*),$$ where $\\tau\\ge\n0$ is an integer. When there are no factors, the matrix $\\Phi_{n}(\\tau)$\nreduces to $$\\mathbf{M}_n(\\tau) = \\frac{1}{2T} \\sum_{j=1}^T (\\mathbf{e}_j\n\\mathbf{e}_{j+\\tau}^* + \\mathbf{e}_{j+\\tau} \\mathbf{e}_j^*).$$ When $\\tau=0$,\n$\\mathbf{M}_n(\\tau)$ reduces to the usual sample covariance matrix whose ESD\ntends to the well known MP law and $\\mathbf{\\Phi}_n(0)$ reduces to the standard\nspike model. Hence the number $k(q+1)$ can be estimated by the number of spiked\neigenvalues of $\\mathbf{\\Phi}_n(0)$. To obtain separate estimates of $k$ and\n$q$ , we have employed the spectral analysis of $\\mathbf{M}_n(\\tau)$ and\nestablished the spiked model analysis for $\\mathbf{\\Phi}_n(\\tau)$.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 21:55:08 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 11:26:05 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Bai", "Z. D.", ""], ["Wang", "Chen", ""], ["Xue", "Ya", ""], ["Harding", "Matthew", ""]]}, {"id": "1511.02609", "submitter": "Martin Wendler", "authors": "B\\'eatrice Bucchia, Martin Wendler", "title": "Change-Point Detection and Bootstrap for Hilbert Space Valued Random\n  Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of testing for the presence of epidemic changes in random fields\nis investigated. In order to be able to deal with general changes in the\nmarginal distribution, a Cram\\'er-von Mises type test is introduced which is\nbased on Hilbert space theory. A functional central limit theorem for\n$\\rho$-mixing Hilbert space valued random fields is proven. In order to avoid\nthe estimation of the long-run variance and obtain critical values, Shao's\ndependent wild bootstrap method is adapted to this context. For this, a joint\nfunctional central limit theorem for the original and the bootstrap sample is\nshown. Finally, the theoretic results are supplemented by a short simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 09:25:55 GMT"}, {"version": "v2", "created": "Thu, 20 Oct 2016 11:16:01 GMT"}], "update_date": "2016-10-21", "authors_parsed": [["Bucchia", "B\u00e9atrice", ""], ["Wendler", "Martin", ""]]}, {"id": "1511.02705", "submitter": "Gabriel Peyre", "authors": "Jonathan Vacher (CEREMADE), Andrew Meso (INT), Laurent U Perrinet\n  (INT), Gabriel Peyr\\'e (CEREMADE)", "title": "Biologically Inspired Dynamic Textures for Probing Motion Perception", "comments": "Twenty-ninth Annual Conference on Neural Information Processing\n  Systems (NIPS), Dec 2015, Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perception is often described as a predictive process based on an optimal\ninference with respect to a generative model. We study here the principled\nconstruction of a generative model specifically crafted to probe motion\nperception. In that context, we first provide an axiomatic, biologically-driven\nderivation of the model. This model synthesizes random dynamic textures which\nare defined by stationary Gaussian distributions obtained by the random\naggregation of warped patterns. Importantly, we show that this model can\nequivalently be described as a stochastic partial differential equation. Using\nthis characterization of motion in images, it allows us to recast motion-energy\nmodels into a principled Bayesian inference framework. Finally, we apply these\ntextures in order to psychophysically probe speed perception in humans. In this\nframework, while the likelihood is derived from the generative model, the prior\nis estimated from the observed results and accounts for the perceptual bias in\na principled fashion.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 14:50:25 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Vacher", "Jonathan", "", "CEREMADE"], ["Meso", "Andrew", "", "INT"], ["Perrinet", "Laurent U", "", "INT"], ["Peyr\u00e9", "Gabriel", "", "CEREMADE"]]}, {"id": "1511.02729", "submitter": "Benjamin Guedj", "authors": "Benjamin Guedj and Sylvain Robbiano", "title": "PAC-Bayesian High Dimensional Bipartite Ranking", "comments": null, "journal-ref": "Journal of Statistical Planning and Inference (2018), vol. 196,\n  70--86", "doi": "10.1016/j.jspi.2017.10.010", "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper is devoted to the bipartite ranking problem, a classical\nstatistical learning task, in a high dimensional setting. We propose a scoring\nand ranking strategy based on the PAC-Bayesian approach. We consider nonlinear\nadditive scoring functions, and we derive non-asymptotic risk bounds under a\nsparsity assumption. In particular, oracle inequalities in probability holding\nunder a margin condition assess the performance of our procedure, and prove its\nminimax optimality. An MCMC-flavored algorithm is proposed to implement our\nmethod, along with its behavior on synthetic and real-life datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 16:01:26 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 21:46:13 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Guedj", "Benjamin", ""], ["Robbiano", "Sylvain", ""]]}, {"id": "1511.02775", "submitter": "Cheng Li", "authors": "Cheng Li, Lizhen Lin, David B. Dunson", "title": "On Posterior Consistency of Tail Index for Bayesian Kernel Mixture\n  Models", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymptotic theory of tail index estimation has been studied extensively in\nthe frequentist literature on extreme values, but rarely in the Bayesian\ncontext. We investigate whether popular Bayesian kernel mixture models are able\nto support heavy tailed distributions and consistently estimate the tail index.\nWe show that posterior inconsistency in tail index is surprisingly common for\nboth parametric and nonparametric mixture models. We then present a set of\nsufficient conditions under which posterior consistency in tail index can be\nachieved, and verify these conditions for Pareto mixture models under general\nmixing priors.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 17:33:06 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 02:31:19 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 01:43:12 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Li", "Cheng", ""], ["Lin", "Lizhen", ""], ["Dunson", "David B.", ""]]}, {"id": "1511.02980", "submitter": "Giorgos Afendras", "authors": "Georgios Afendras and Marianthi Markatou", "title": "Optimality of Training/Test Size and Resampling Effectiveness of\n  Cross-Validation Estimators of the Generalization Error", "comments": "53 pages, 6 figures, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important question in constructing Cross Validation (CV) estimators of the\ngeneralization error is whether rules can be established that allow \"optimal\"\nselection of the size of the training set, for fixed sample size $n$. We define\nthe {\\it resampling effectiveness} of random CV estimators of the\ngeneralization error as the ratio of the limiting value of the variance of the\nCV estimator over the estimated from the data variance. The variance and the\ncovariance of different average test set errors are independent of their\nindices, thus, the resampling effectiveness depends on the correlation and the\nnumber of repetitions used in the random CV estimator. We discuss statistical\nrules to define optimality and obtain the \"optimal\" training sample size as the\nsolution of an appropriately formulated optimization problem. We show that in a\nbroad class of loss functions the optimal training size equals half of the\ntotal sample size, independently of the data distribution. We optimally select\nthe number of folds in $k$-fold cross validation and offer a computational\nprocedure for obtaining the optimal splitting in the case of classification\n(via logistic regression). We substantiate our claims both, theoretically and\nempirically.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 03:11:02 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Afendras", "Georgios", ""], ["Markatou", "Marianthi", ""]]}, {"id": "1511.03229", "submitter": "Aravindan Vijayaraghavan", "authors": "Konstantin Makarychev, Yury Makarychev and Aravindan Vijayaraghavan", "title": "Learning Communities in the Presence of Errors", "comments": "34 pages. Appearing in the Conference on Learning Theory (COLT)'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning communities in the presence of modeling\nerrors and give robust recovery algorithms for the Stochastic Block Model\n(SBM). This model, which is also known as the Planted Partition Model, is\nwidely used for community detection and graph partitioning in various fields,\nincluding machine learning, statistics, and social sciences. Many algorithms\nexist for learning communities in the Stochastic Block Model, but they do not\nwork well in the presence of errors.\n  In this paper, we initiate the study of robust algorithms for partial\nrecovery in SBM with modeling errors or noise. We consider graphs generated\naccording to the Stochastic Block Model and then modified by an adversary. We\nallow two types of adversarial errors, Feige---Kilian or monotone errors, and\nedge outlier errors. Mossel, Neeman and Sly (STOC 2015) posed an open question\nabout whether an almost exact recovery is possible when the adversary is\nallowed to add $o(n)$ edges. Our work answers this question affirmatively even\nin the case of $k>2$ communities.\n  We then show that our algorithms work not only when the instances come from\nSBM, but also work when the instances come from any distribution of graphs that\nis $\\epsilon m$ close to SBM in the Kullback---Leibler divergence. This result\nalso works in the presence of adversarial errors. Finally, we present almost\ntight lower bounds for two communities.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 19:03:47 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 18:51:52 GMT"}, {"version": "v3", "created": "Fri, 24 Jun 2016 04:02:00 GMT"}], "update_date": "2016-06-27", "authors_parsed": [["Makarychev", "Konstantin", ""], ["Makarychev", "Yury", ""], ["Vijayaraghavan", "Aravindan", ""]]}, {"id": "1511.03334", "submitter": "Rajen Shah", "authors": "Rajen D. Shah and Peter B\\\"uhlmann", "title": "Goodness of fit tests for high-dimensional linear models", "comments": "42 pages, 12 figures", "journal-ref": null, "doi": "10.1111/rssb.12234", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a framework for constructing goodness of fit tests in\nboth low and high-dimensional linear models. We advocate applying regression\nmethods to the scaled residuals following either an ordinary least squares or\nLasso fit to the data, and using some proxy for prediction error as the final\ntest statistic. We call this family Residual Prediction (RP) tests. We show\nthat simulation can be used to obtain the critical values for such tests in the\nlow-dimensional setting, and demonstrate using both theoretical results and\nextensive numerical studies that some form of the parametric bootstrap can do\nthe same when the high-dimensional linear model is under consideration. We show\nthat RP tests can be used to test for significance of groups or individual\nvariables as special cases, and here they compare favourably with state of the\nart methods, but we also argue that they can be designed to test for as diverse\nmodel misspecifications as heteroscedasticity and nonlinearity.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 23:22:55 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2016 11:26:39 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 11:35:14 GMT"}, {"version": "v4", "created": "Sat, 8 Apr 2017 13:20:03 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Shah", "Rajen D.", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1511.03463", "submitter": "Dimitris Kugiumtzis", "authors": "Elsa Siggiridou and Dimitris Kugiumtzis", "title": "Granger Causality in Multi-variate Time Series using a Time Ordered\n  Restricted Vector Autoregressive Model", "comments": "15 pages, 4 tables, 6 figures", "journal-ref": null, "doi": "10.1109/TSP.2015.2500893", "report-no": null, "categories": "stat.ME math.ST physics.data-an stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Granger causality has been used for the investigation of the inter-dependence\nstructure of the underlying systems of multi-variate time series. In\nparticular, the direct causal effects are commonly estimated by the conditional\nGranger causality index (CGCI). In the presence of many observed variables and\nrelatively short time series, CGCI may fail because it is based on vector\nautoregressive models (VAR) involving a large number of coefficients to be\nestimated. In this work, the VAR is restricted by a scheme that modifies the\nrecently developed method of backward-in-time selection (BTS) of the lagged\nvariables and the CGCI is combined with BTS. Further, the proposed approach is\ncompared favorably to other restricted VAR representations, such as the\ntop-down strategy, the bottom-up strategy, and the least absolute shrinkage and\nselection operator (LASSO), in terms of sensitivity and specificity of CGCI.\nThis is shown by using simulations of linear and nonlinear, low and\nhigh-dimensional systems and different time series lengths. For nonlinear\nsystems, CGCI from the restricted VAR representations are compared with\nanalogous nonlinear causality indices. Further, CGCI in conjunction with BTS\nand other restricted VAR representations is applied to multi-channel scalp\nelectroencephalogram (EEG) recordings of epileptic patients containing\nepileptiform discharges. CGCI on the restricted VAR, and BTS in particular,\ncould track the changes in brain connectivity before, during and after\nepileptiform discharges, which was not possible using the full VAR\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 11:35:21 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Siggiridou", "Elsa", ""], ["Kugiumtzis", "Dimitris", ""]]}, {"id": "1511.03551", "submitter": "Jonathan Rougier", "authors": "Jonathan Rougier", "title": "Exchangeability, the 'Histogram Theorem', and population inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some practical results are derived for population inference based on a\nsample, under the two qualitative conditions of 'ignorability' and\nexchangeability. These are the 'Histogram Theorem', for predicting the outcome\nof a non-sampled member of the population, and its application to inference\nabout the population, both without and with groups. There are discussions of\nparametric versus non-parametric models, and different approaches to\nmarginalisation. An Appendix gives a self-contained proof of the Representation\nTheorem for finite exchangeable sequences.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 16:08:30 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Rougier", "Jonathan", ""]]}, {"id": "1511.03592", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "The Fourier Transform of Poisson Multinomial Distributions and its\n  Algorithmic Applications", "comments": "68 pages, full version of STOC 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.GT cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(n, k)$-Poisson Multinomial Distribution (PMD) is a random variable of\nthe form $X = \\sum_{i=1}^n X_i$, where the $X_i$'s are independent random\nvectors supported on the set of standard basis vectors in $\\mathbb{R}^k.$ In\nthis paper, we obtain a refined structural understanding of PMDs by analyzing\ntheir Fourier transform. As our core structural result, we prove that the\nFourier transform of PMDs is {\\em approximately sparse}, i.e., roughly\nspeaking, its $L_1$-norm is small outside a small set. By building on this\nresult, we obtain the following applications:\n  {\\bf Learning Theory.} We design the first computationally efficient learning\nalgorithm for PMDs with respect to the total variation distance. Our algorithm\nlearns an arbitrary $(n, k)$-PMD within variation distance $\\epsilon$ using a\nnear-optimal sample size of $\\widetilde{O}_k(1/\\epsilon^2),$ and runs in time\n$\\widetilde{O}_k(1/\\epsilon^2) \\cdot \\log n.$ Previously, no algorithm with a\n$\\mathrm{poly}(1/\\epsilon)$ runtime was known, even for $k=3.$\n  {\\bf Game Theory.} We give the first efficient polynomial-time approximation\nscheme (EPTAS) for computing Nash equilibria in anonymous games. For normalized\nanonymous games with $n$ players and $k$ strategies, our algorithm computes a\nwell-supported $\\epsilon$-Nash equilibrium in time $n^{O(k^3)} \\cdot\n(k/\\epsilon)^{O(k^3\\log(k/\\epsilon)/\\log\\log(k/\\epsilon))^{k-1}}.$ The best\nprevious algorithm for this problem had running time $n^{(f(k)/\\epsilon)^k},$\nwhere $f(k) = \\Omega(k^{k^2})$, for any $k>2.$\n  {\\bf Statistics.} We prove a multivariate central limit theorem (CLT) that\nrelates an arbitrary PMD to a discretized multivariate Gaussian with the same\nmean and covariance, in total variation distance. Our new CLT strengthens the\nCLT of Valiant and Valiant by completely removing the dependence on $n$ in the\nerror bound.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 18:00:37 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 19:42:04 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1511.03641", "submitter": "Gautam Kamath", "authors": "Constantinos Daskalakis, Anindya De, Gautam Kamath, Christos Tzamos", "title": "A Size-Free CLT for Poisson Multinomials and its Applications", "comments": "To appear in STOC 2016", "journal-ref": null, "doi": "10.1145/2897518.2897519", "report-no": null, "categories": "cs.DS cs.GT cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An $(n,k)$-Poisson Multinomial Distribution (PMD) is the distribution of the\nsum of $n$ independent random vectors supported on the set ${\\cal\nB}_k=\\{e_1,\\ldots,e_k\\}$ of standard basis vectors in $\\mathbb{R}^k$. We show\nthat any $(n,k)$-PMD is ${\\rm poly}\\left({k\\over \\sigma}\\right)$-close in total\nvariation distance to the (appropriately discretized) multi-dimensional\nGaussian with the same first two moments, removing the dependence on $n$ from\nthe Central Limit Theorem of Valiant and Valiant. Interestingly, our CLT is\nobtained by bootstrapping the Valiant-Valiant CLT itself through the structural\ncharacterization of PMDs shown in recent work by Daskalakis, Kamath, and\nTzamos. In turn, our stronger CLT can be leveraged to obtain an efficient PTAS\nfor approximate Nash equilibria in anonymous games, significantly improving the\nstate of the art, and matching qualitatively the running time dependence on $n$\nand $1/\\varepsilon$ of the best known algorithm for two-strategy anonymous\ngames. Our new CLT also enables the construction of covers for the set of\n$(n,k)$-PMDs, which are proper and whose size is shown to be essentially\noptimal. Our cover construction combines our CLT with the Shapley-Folkman\ntheorem and recent sparsification results for Laplacian matrices by Batson,\nSpielman, and Srivastava. Our cover size lower bound is based on an algebraic\ngeometric construction. Finally, leveraging the structural properties of the\nFourier spectrum of PMDs we show that these distributions can be learned from\n$O_k(1/\\varepsilon^2)$ samples in ${\\rm poly}_k(1/\\varepsilon)$-time, removing\nthe quasi-polynomial dependence of the running time on $1/\\varepsilon$ from the\nalgorithm of Daskalakis, Kamath, and Tzamos.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 20:27:33 GMT"}, {"version": "v2", "created": "Thu, 16 Jun 2016 05:02:43 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["De", "Anindya", ""], ["Kamath", "Gautam", ""], ["Tzamos", "Christos", ""]]}, {"id": "1511.03738", "submitter": "David Burstein", "authors": "David Burstein and Jonathan Rubin", "title": "Degree switching and partitioning for enumerating graphs to arbitrary\n  orders of accuracy", "comments": "24 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.DM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel method for constructing asymptotics (to arbitrary\naccuracy) for the number of directed graphs that realize a fixed bidegree\nsequence $d = a \\times b$ with maximum degree $d_{max}=O(S^{\\frac{1}{2}-\\tau})$\nfor an arbitrarily small positive number $\\tau$, where $S$ is the number edges\nspecified by $d$. Our approach is based on two key steps, graph partitioning\nand degree preserving switches. The former idea allows us to relate enumeration\nresults for given sequences to those for sequences that are especially easy to\nhandle, while the latter facilitates expansions based on numbers of shared\nneighbors of pairs of nodes. While we focus primarily on directed graphs\nallowing loops, our results can be extended to other cases, including bipartite\ngraphs, as well as directed and undirected graphs without loops. In addition,\nwe can relax the constraint that $d_{max} = O(S^{\\frac{1}{2}-\\tau})$ and\nreplace it with $a_{max} b_{max} = O(S^{1-\\tau})$. where $a_{max}$ and\n$b_{max}$ are the maximum values for $a$ and $b$ respectively. The previous\nbest results, from Greenhill et al., only allow for $d_{max} =\no(S^{\\frac{1}{3}})$ or alternatively $a_{max} b_{max} = o(S^{\\frac{2}{3}})$.\nSince in many real world networks, $d_{max}$ scales larger than\n$o(S^{\\frac{1}{3}})$, we expect that this work will be helpful for various\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 00:56:35 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2016 18:15:50 GMT"}], "update_date": "2016-10-24", "authors_parsed": [["Burstein", "David", ""], ["Rubin", "Jonathan", ""]]}, {"id": "1511.03803", "submitter": "Weijie Su", "authors": "Cynthia Dwork and Weijie Su and Li Zhang", "title": "Private False Discovery Rate Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the first differentially private algorithms for controlling the\nfalse discovery rate (FDR) in multiple hypothesis testing, with essentially no\nloss in power under certain conditions. Our general approach is to adapt a\nwell-known variant of the Benjamini-Hochberg procedure (BHq), making each step\ndifferentially private. This destroys the classical proof of FDR control. To\nprove FDR control of our method, (a) we develop a new proof of the original\n(non-private) BHq algorithm and its robust variants -- a proof requiring only\nthe assumption that the true null test statistics are independent, allowing for\narbitrary correlations between the true nulls and false nulls. This assumption\nis fairly weak compared to those previously shown in the vast literature on\nthis topic, and explains in part the empirical robustness of BHq. Then (b) we\nrelate the FDR control properties of the differentially private version to the\ncontrol properties of the non-private version. \\end{enumerate} We also present\na low-distortion \"one-shot\" differentially private primitive for \"top $k$\"\nproblems, e.g., \"Which are the $k$ most popular hobbies?\" (which we apply to:\n\"Which hypotheses have the $k$ most significant $p$-values?\"), and use it to\nget a faster privacy-preserving instantiation of our general approach at little\ncost in accuracy. The proof of privacy for the one-shot top~$k$ algorithm\nintroduces a new technique of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 07:31:55 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Dwork", "Cynthia", ""], ["Su", "Weijie", ""], ["Zhang", "Li", ""]]}, {"id": "1511.03977", "submitter": "Fabian Dunker", "authors": "Fabian Dunker", "title": "Nonparametric instrumental variable regression and quantile regression\n  with full independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of endogeneity in statistics and econometrics is often handled by\nintroducing instrumental variables (IV) which are assumed to be mean\nindependent of some regressors or other observables. When full independence of\nIV's and observables is assumed, nonparametric IV regression models and\nnonparametric demand models lead to nonlinear integral equations with unknown\nintegral kernels. We prove convergence rates for the mean integrated square\nerror of the iteratively regularized Newton method applied to these problems.\nCompared to related results we derive stronger convergence results that rely on\nweaker nonlinearity restrictions. We demonstrate in numerical simulations for a\nnonparametric IV regression that the method produces better results than the\nstandard model.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 17:26:47 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 23:42:46 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Dunker", "Fabian", ""]]}, {"id": "1511.03982", "submitter": "Pau Closas", "authors": "Adri\\`a Gusi-Amig\\'o, Pau Closas, and Luc Vandendorpe", "title": "Mean Square Error bounds for parameter estimation under model\n  misspecification", "comments": "Submitted for publication in the IEEE Transactions on Signal\n  Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parameter estimation, assumptions about the model are typically considered\nwhich allow us to build optimal estimation methods under many statistical\nsenses. However, it is usually the case where such models are inaccurately\nknown or not capturing the complexity of the observed phenomenon. A natural\nquestion arises to whether we can find fundamental estimation bounds under\nmodel mismatches. This paper derives a general bound on the mean square error\n(MSE) following the Ziv-Zakai methodology for the widely used additive Gaussian\nmodel. The general result accounts for erroneous functionals, hyperparameters,\nand distributions differing from the Gaussian. The result is then\nparticularized to gain some insight into specific problems and some\nillustrative examples demonstrate the predictive capabilities of the bound.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 17:39:43 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2015 10:18:58 GMT"}], "update_date": "2015-12-14", "authors_parsed": [["Gusi-Amig\u00f3", "Adri\u00e0", ""], ["Closas", "Pau", ""], ["Vandendorpe", "Luc", ""]]}, {"id": "1511.04033", "submitter": "Emilie Devijver", "authors": "Emilie Devijver, M\\'elina Gallopin", "title": "Block-diagonal covariance selection for high-dimensional Gaussian\n  graphical models", "comments": "Accepted in JASA", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models are widely utilized to infer and visualize networks\nof dependencies between continuous variables. However, inferring the graph is\ndifficult when the sample size is small compared to the number of variables. To\nreduce the number of parameters to estimate in the model, we propose a\nnon-asymptotic model selection procedure supported by strong theoretical\nguarantees based on an oracle inequality and a minimax lower bound. The\ncovariance matrix of the model is approximated by a block-diagonal matrix. The\nstructure of this matrix is detected by thresholding the sample covariance\nmatrix, where the threshold is selected using the slope heuristic. Based on the\nblock-diagonal structure of the covariance matrix, the estimation problem is\ndivided into several independent problems: subsequently, the network of\ndependencies between variables is inferred using the graphical lasso algorithm\nin each block. The performance of the procedure is illustrated on simulated\ndata. An application to a real gene expression dataset with a limited sample\nsize is also presented: the dimension reduction allows attention to be\nobjectively focused on interactions among smaller subsets of genes, leading to\na more parsimonious and interpretable modular network.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 19:56:45 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2016 10:56:02 GMT"}, {"version": "v3", "created": "Thu, 29 Sep 2016 08:33:19 GMT"}], "update_date": "2016-09-30", "authors_parsed": [["Devijver", "Emilie", ""], ["Gallopin", "M\u00e9lina", ""]]}, {"id": "1511.04066", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Properly Learning Poisson Binomial Distributions in Almost Polynomial\n  Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an algorithm for properly learning Poisson binomial distributions. A\nPoisson binomial distribution (PBD) of order $n$ is the discrete probability\ndistribution of the sum of $n$ mutually independent Bernoulli random variables.\nGiven $\\widetilde{O}(1/\\epsilon^2)$ samples from an unknown PBD $\\mathbf{p}$,\nour algorithm runs in time $(1/\\epsilon)^{O(\\log \\log (1/\\epsilon))}$, and\noutputs a hypothesis PBD that is $\\epsilon$-close to $\\mathbf{p}$ in total\nvariation distance. The previously best known running time for properly\nlearning PBDs was $(1/\\epsilon)^{O(\\log(1/\\epsilon))}$.\n  As one of our main contributions, we provide a novel structural\ncharacterization of PBDs. We prove that, for all $\\epsilon >0,$ there exists an\nexplicit collection $\\cal{M}$ of $(1/\\epsilon)^{O(\\log \\log (1/\\epsilon))}$\nvectors of multiplicities, such that for any PBD $\\mathbf{p}$ there exists a\nPBD $\\mathbf{q}$ with $O(\\log(1/\\epsilon))$ distinct parameters whose\nmultiplicities are given by some element of ${\\cal M}$, such that $\\mathbf{q}$\nis $\\epsilon$-close to $\\mathbf{p}$. Our proof combines tools from Fourier\nanalysis and algebraic geometry.\n  Our approach to the proper learning problem is as follows: Starting with an\naccurate non-proper hypothesis, we fit a PBD to this hypothesis. More\nspecifically, we essentially start with the hypothesis computed by the\ncomputationally efficient non-proper learning algorithm in our recent\nwork~\\cite{DKS15}. Our aforementioned structural characterization allows us to\nreduce the corresponding fitting problem to a collection of\n$(1/\\epsilon)^{O(\\log \\log(1/\\epsilon))}$ systems of low-degree polynomial\ninequalities. We show that each such system can be solved in time\n$(1/\\epsilon)^{O(\\log \\log(1/\\epsilon))}$, which yields the overall running\ntime of our algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 20:47:37 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1511.04125", "submitter": "Emmanuel Tsukerman", "authors": "Bernd Sturmfels, Emmanuel Tsukerman, Lauren Williams", "title": "Symmetric matrices, Catalan paths, and correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kenyon and Pemantle (2014) gave a formula for the entries of a square matrix\nin terms of connected principal and almost-principal minors. Each entry is an\nexplicit Laurent polynomial whose terms are the weights of domino tilings of a\nhalf Aztec diamond. They conjectured an analogue of this parametrization for\nsymmetric matrices, where the Laurent monomials are indexed by Catalan paths.\nIn this paper we prove the Kenyon-Pemantle conjecture, and apply this to a\nstatistics problem pioneered by Joe (2006). Correlation matrices are\nrepresented by an explicit bijection from the cube to the elliptope.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 23:33:59 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Sturmfels", "Bernd", ""], ["Tsukerman", "Emmanuel", ""], ["Williams", "Lauren", ""]]}, {"id": "1511.04128", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Jonathan M. Lilly", "title": "A Widely Linear Complex Autoregressive Process of Order One", "comments": "Link to published version:\n  http://ieeexplore.ieee.org/abstract/document/7539658/", "journal-ref": "IEEE Transactions on Signal Processing, 64(23), 6200-6210, 2016", "doi": "10.1109/TSP.2016.2599503", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple stochastic process for modeling improper or noncircular\ncomplex-valued signals. The process is a natural extension of a complex-valued\nautoregressive process, extended to include a widely linear autoregressive\nterm. This process can then capture elliptical, as opposed to circular,\nstochastic oscillations in a bivariate signal. The process is order one and is\nmore parsimonious than alternative stochastic modeling approaches in the\nliterature. We provide conditions for stationarity, and derive the form of the\ncovariance and relation sequence of this model. We describe how parameter\nestimation can be efficiently performed both in the time and frequency domain.\nWe demonstrate the practical utility of the process in capturing elliptical\noscillations that are naturally present in seismic signals.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 23:44:04 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 11:01:55 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 17:23:31 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Lilly", "Jonathan M.", ""]]}, {"id": "1511.04144", "submitter": "Chao Gao", "authors": "Mengjie Chen, Chao Gao, Zhao Ren", "title": "A General Decision Theory for Huber's $\\epsilon$-Contamination Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today's data pose unprecedented challenges to statisticians. It may be\nincomplete, corrupted or exposed to some unknown source of contamination. We\nneed new methods and theories to grapple with these challenges. Robust\nestimation is one of the revived fields with potential to accommodate such\ncomplexity and glean useful information from modern datasets. Following our\nrecent work on high dimensional robust covariance matrix estimation, we\nestablish a general decision theory for robust statistics under Huber's\n$\\epsilon$-contamination model. We propose a solution using Scheff{\\'e}\nestimate to a robust two-point testing problem that leads to the construction\nof robust estimators adaptive to the proportion of contamination. Applying the\ngeneral theory, we construct robust estimators for nonparametric density\nestimation, sparse linear regression and low-rank trace regression. We show\nthat these new estimators achieve the minimax rate with optimal dependence on\nthe contamination proportion. This testing procedure, Scheff{\\'e} estimate,\nalso enjoys an optimal rate in the exponent of the testing error, which may be\nof independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 02:36:39 GMT"}, {"version": "v2", "created": "Mon, 16 Jan 2017 14:03:19 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Chen", "Mengjie", ""], ["Gao", "Chao", ""], ["Ren", "Zhao", ""]]}, {"id": "1511.04162", "submitter": "Takuya Ura", "authors": "Takuya Ura", "title": "Heterogeneous Treatment Effects with Mismeasured Endogenous Treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the identifying power of an instrumental variable in the\nnonparametric heterogeneous treatment effect framework when a binary treatment\nis mismeasured and endogenous. Using a binary instrumental variable, I\ncharacterize the sharp identified set for the local average treatment effect\nunder the exclusion restriction of an instrument and the deterministic\nmonotonicity of the true treatment in the instrument. Even allowing for general\nmeasurement error (e.g., the measurement error is endogenous), it is still\npossible to obtain finite bounds on the local average treatment effect.\nNotably, the Wald estimand is an upper bound on the local average treatment\neffect, but it is not the sharp bound in general. I also provide a confidence\ninterval for the local average treatment effect with uniformly asymptotically\nvalid size control. Furthermore, I demonstrate that the identification strategy\nof this paper offers a new use of repeated measurements for tightening the\nidentified set.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 05:30:41 GMT"}, {"version": "v2", "created": "Mon, 30 May 2016 21:40:23 GMT"}, {"version": "v3", "created": "Fri, 19 May 2017 17:41:23 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Ura", "Takuya", ""]]}, {"id": "1511.04220", "submitter": "Leonidas Pitsoulis", "authors": "G. Zioutas, C. Chatzinakos, T.D. Nguyen and L. Pitsoulis", "title": "Optimization techniques for multivariate least trimmed absolute\n  deviation estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a dataset an outlier can be defined as an observation that it is\nunlikely to follow the statistical properties of the majority of the data.\nComputation of the location estimate of is fundamental in data analysis, and it\nis well known in statistics that classical methods, such as taking the sample\naverage, can be greatly affected by the presence of outliers in the data. Using\nthe median instead of the mean can partially resolve this issue but not\ncompletely. For the univariate case, a robust version of the median is the\nLeast Trimmed Absolute Deviation (LTAD) robust estimator introduced\nin~\\cite{Tableman1994}, which has desirable asymptotic properties such as\nrobustness, consistently, high breakdown and normality. There are different\ngeneralizations of the LTAD for multivariate data, depending on the choice of\nnorm. In~\\cite{ChaPitZiou:2015} we present such a generalization using the\nEuclidean norm and propose a solution technique for the resulting combinatorial\noptimization problem, based on a necessary condition, that results in a highly\nconvergent local search algorithm. In this subsequent work we use the $L^1$\nnorm to generalize the LTAD to higher dimensions, and show that the resulting\nmixed integer programming problem has an integral relaxation, after applying an\nappropriate data transformation. Moreover, we utilize the structure of the\nproblem to show that the resulting LP's can be solved efficiently using a\nsubgradient optimization approach. The robust statistical properties of the\nproposed estimator are verified by extensive computational results.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 10:09:44 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Zioutas", "G.", ""], ["Chatzinakos", "C.", ""], ["Nguyen", "T. D.", ""], ["Pitsoulis", "L.", ""]]}, {"id": "1511.04635", "submitter": "Adam Jaeger", "authors": "Adam Jaeger and Nicole Lazar", "title": "Composite Empirical Likelihood", "comments": "Paper has been repeatedly rejected as having no real world use", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood function plays a pivotal role in statistical inference; it is\nadaptable to a wide range of models and the resultant estimators are known to\nhave good properties. However, these results hinge on correct specification of\nthe data generating mechanism. Many modern problems involve extremely\ncomplicated distribution functions, which may be difficult -- if not impossible\n-- to express explicitly. This is a serious barrier to the likelihood approach,\nwhich requires not only the specification of a distribution, but the correct\ndistribution. Non-parametric methods are one way to avoid the problem of having\nto specify a particular data generating mechanism, but can be computationally\nintensive, reducing their accessibility for large data problems. We propose a\nnew approach that combines multiple non-parametric likelihood-type components\nto build a data-driven approximation of the true function. The new construct\nbuilds on empirical and composite likelihood, taking advantage of the strengths\nof each. Specifically, from empirical likelihood we borrow the ability to avoid\na parametric specification, and from composite likelihood we utilize multiple\nlikelihood components. We will examine the theoretical properties of this\ncomposite empirical likelihood, both for purposes of application and to compare\nproperties to other established likelihood methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 23:26:41 GMT"}, {"version": "v2", "created": "Wed, 30 Dec 2015 17:23:27 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 18:40:41 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Jaeger", "Adam", ""], ["Lazar", "Nicole", ""]]}, {"id": "1511.04903", "submitter": "Olivier Wintenberger", "authors": "Rafal Kulik, Philippe Soulier (MODAL'X), Olivier Wintenberger (LPSM\n  UMR 8001), Rafa Kulik", "title": "The tail empirical process of regularly varying functions of\n  geometrically ergodic Markov chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stationary regularly varying time series which can be\nexpressedas a function of a geometrically ergodic Markov chain. We obtain\npractical conditionsfor the weak convergence of the tail array sums and\nfeasible estimators ofcluster statistics. These conditions include the\nso-called geometric drift or Foster-Lyapunovcondition and can be easily checked\nfor most usual time series models witha Markovian structure. We illustrate\nthese conditions on several models and statisticalapplications. A\ncounterexample is given to show a different limiting behaviorwhen the geometric\ndrift condition is not fulfilled.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 10:34:32 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 12:14:32 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Kulik", "Rafal", "", "MODAL'X"], ["Soulier", "Philippe", "", "MODAL'X"], ["Wintenberger", "Olivier", "", "LPSM\n  UMR 8001"], ["Kulik", "Rafa", ""]]}, {"id": "1511.05201", "submitter": "Matthew Aldridge", "authors": "Matthew Aldridge", "title": "The capacity of Bernoulli nonadaptive group testing", "comments": "7 pages, 1 figure", "journal-ref": "IEEE Transactions on Information Theory, 63:11, 7142-7148, 2017", "doi": "10.1109/TIT.2017.2748564", "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonadaptive group testing with Bernoulli tests, where each item\nis placed in each test independently with some fixed probability. We give a\ntight threshold on the maximum number of tests required to find the defective\nset under optimal Bernoulli testing. Achievability is given by a result of\nScarlett and Cevher; here we give a converse bound showing that this result is\nbest possible. Our new converse requires three parts: a typicality bound\ngeneralising the trivial counting bound, a converse on the COMP algorithm of\nChan et al, and a bound on the SSS algorithm similar to that given by Aldridge,\nBaldassini, and Johnson. Our result has a number of important corollaries, in\nparticular that, in denser cases, Bernoulli nonadaptive group testing is\nstrictly worse than the best adaptive strategies.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 22:10:13 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 17:01:44 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Aldridge", "Matthew", ""]]}, {"id": "1511.05240", "submitter": "Richard Combes", "authors": "Richard Combes", "title": "An extension of McDiarmid's inequality", "comments": "Note (4 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an extension of McDiarmid's inequality for functions $f$ with\nbounded differences on a high probability set ${\\cal Y}$ (instead of almost\nsurely). The behavior of $f$ outside ${\\cal Y}$ may be arbitrary. The proof is\nshort and elementary, and relies on an extension argument similar to\nKirszbraun's theorem.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 01:14:51 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Combes", "Richard", ""]]}, {"id": "1511.05254", "submitter": "Hamid Javadi", "authors": "Hamid Javadi and Andrea Montanari", "title": "A Statistical Model for Motifs Detection", "comments": "40 pages, 1 pdf figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a statistical model for the problem of finding subgraphs with\nspecified topology in an otherwise random graph. This task plays an important\nrole in the analysis of social and biological networks. In these types of\nnetworks, small subgraphs with a specific structure have important functional\nroles, and they are referred to as `motifs.'\n  Within this model, one or multiple copies of a subgraph is added (`planted')\nin an Erd\\H{o}s-Renyi random graph with $n$ vertices and edge probability\n$q_0$. We ask whether the resulting graph can be distinguished reliably from a\npure Erd\\H{o}s-Renyi random graph, and we present two types of result. First we\ninvestigate the question from a purely statistical perspective, and ask whether\nthere is any test that can distinguish between the two graph models. We provide\nnecessary and sufficient conditions that are essentially tight for small enough\nsubgraphs.\n  Next we study two polynomial-time algorithms for solving the same problem: a\nspectral algorithm, and a semidefinite programming (SDP) relaxation. For the\nspectral algorithm, we establish sufficient conditions under which it\ndistinguishes the two graph models with high probability. Under the same\nconditions the spectral algorithm indeed identifies the hidden subgraph.\n  The spectral algorithm is substantially sub-optimal with respect to the\noptimal test. We show that a similar gap is present for the more sophisticated\nSDP approach.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 02:38:03 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 00:48:38 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Javadi", "Hamid", ""], ["Montanari", "Andrea", ""]]}, {"id": "1511.05333", "submitter": "Moritz Jirak", "authors": "Moritz Jirak", "title": "Uniform change point tests in high dimension", "comments": "Clarified Assumption 4.3 and Theorems 4.4, 4.6 and 4.8. The results\n  themselves are unchanged", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 6, 2451-2483", "doi": "10.1214/15-AOS1347", "report-no": "IMS-AOS-AOS1347", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider $d$ dependent change point tests, each based on a CUSUM-statistic.\nWe provide an asymptotic theory that allows us to deal with the maximum over\nall test statistics as both the sample size $n$ and $d$ tend to infinity. We\nachieve this either by a consistent bootstrap or an appropriate limit\ndistribution. This allows for the construction of simultaneous confidence bands\nfor dependent change point tests, and explicitly allows us to determine the\nlocation of the change both in time and coordinates in high-dimensional time\nseries. If the underlying data has sample size greater or equal $n$ for each\ntest, our conditions explicitly allow for the large $d$ small $n$ situation,\nthat is, where $n/d\\to0$. The setup for the high-dimensional time series is\nbased on a general weak dependence concept. The conditions are very flexible\nand include many popular multivariate linear and nonlinear models from the\nliterature, such as ARMA, GARCH and related models. The construction of the\ntests is completely nonparametric, difficulties associated with parametric\nmodel selection, model fitting and parameter estimation are avoided. Among\nother things, the limit distribution for $\\max_{1\\leq h\\leq d}\\sup_{0\\leq\nt\\leq1}\\vert \\mathcal{W}_{t,h}-t\\mathcal{W}_{1,h}\\vert$ is established, where\n$\\{\\mathcal{W}_{t,h}\\}_{1\\leq h\\leq d}$ denotes a sequence of dependent\nBrownian motions. As an application, we analyze all S&P 500 companies over a\nperiod of one year.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 10:15:14 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2016 22:00:04 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 22:00:34 GMT"}], "update_date": "2017-12-07", "authors_parsed": [["Jirak", "Moritz", ""]]}, {"id": "1511.05337", "submitter": "Guillaume Chauvet", "authors": "Guillaume Chauvet", "title": "Coupling methods for multistage sampling", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1348 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 6, 2484-2506", "doi": "10.1214/15-AOS1348", "report-no": "IMS-AOS-AOS1348", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multistage sampling is commonly used for household surveys when there exists\nno sampling frame, or when the population is scattered over a wide area.\nMultistage sampling usually introduces a complex dependence in the selection of\nthe final units, which makes asymptotic results quite difficult to prove. In\nthis work, we consider multistage sampling with simple random without\nreplacement sampling at the first stage, and with an arbitrary sampling design\nfor further stages. We consider coupling methods to link this sampling design\nto sampling designs where the primary sampling units are selected\nindependently. We first generalize a method introduced by [Magyar Tud. Akad.\nMat. Kutat\\'{o} Int. K\\\"{o}zl. 5 (1960) 361-374] to get a coupling with\nmultistage sampling and Bernoulli sampling at the first stage, which leads to a\ncentral limit theorem for the Horvitz--Thompson estimator. We then introduce a\nnew coupling method with multistage sampling and simple random with replacement\nsampling at the first stage. When the first-stage sampling fraction tends to\nzero, this method is used to prove consistency of a with-replacement bootstrap\nfor simple random without replacement sampling at the first stage, and\nconsistency of bootstrap variance estimators for smooth functions of totals.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 10:21:53 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Chauvet", "Guillaume", ""]]}, {"id": "1511.05491", "submitter": "Pamela Llop", "authors": "Liliana Forzani, Rodrigo Garc\\'ia Arancibia, Pamela Llop, Diego\n  Tomassi", "title": "Supervised dimension reduction for ordinal predictors", "comments": "33 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications involving ordinal predictors, common approaches to reduce\ndimensionality are either extensions of unsupervised techniques such as\nprincipal component analysis, or variable selection procedures that rely on\nmodeling the regression function. In this paper, a supervised dimension\nreduction method tailored to ordered categorical predictors is introduced. It\nuses a model-based dimension reduction approach, inspired by extending\nsufficient dimension reductions to the context of latent Gaussian variables.\nThe reduction is chosen without modeling the response as a function of the\npredictors and does not impose any distributional assumption on the response or\non the response given the predictors. A likelihood-based estimator of the\nreduction is derived and an iterative expectation-maximization type algorithm\nis proposed to alleviate the computational load and thus make the method more\npractical. A regularized estimator, which simultaneously achieves variable\nselection and dimension reduction, is also presented. Performance of the\nproposed method is evaluated through simulations and a real data example for\nsocioeconomic index construction, comparing favorably to widespread use\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 17:58:44 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2016 17:06:15 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 15:21:37 GMT"}, {"version": "v4", "created": "Thu, 12 Oct 2017 13:52:14 GMT"}], "update_date": "2017-10-13", "authors_parsed": [["Forzani", "Liliana", ""], ["Arancibia", "Rodrigo Garc\u00eda", ""], ["Llop", "Pamela", ""], ["Tomassi", "Diego", ""]]}, {"id": "1511.05741", "submitter": "Erwan Scornet", "authors": "G\\'erard Biau (LSTA), Erwan Scornet (LSTA)", "title": "A Random Forest Guided Tour", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random forest algorithm, proposed by L. Breiman in 2001, has been\nextremely successful as a general-purpose classification and regression method.\nThe approach, which combines several randomized decision trees and aggregates\ntheir predictions by averaging, has shown excellent performance in settings\nwhere the number of variables is much larger than the number of observations.\nMoreover, it is versatile enough to be applied to large-scale problems, is\neasily adapted to various ad-hoc learning tasks, and returns measures of\nvariable importance. The present article reviews the most recent theoretical\nand methodological developments for random forests. Emphasis is placed on the\nmathematical forces driving the algorithm, with special attention given to the\nselection of parameters, the resampling mechanism, and variable importance\nmeasures. This review is intended to provide non-experts easy access to the\nmain ideas.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 11:34:43 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Biau", "G\u00e9rard", "", "LSTA"], ["Scornet", "Erwan", "", "LSTA"]]}, {"id": "1511.05780", "submitter": "Johanna  Kappus", "authors": "Johanna Kappus", "title": "Nonparametric estimation for irregularly sampled L\\'evy processes", "comments": "24 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric statistical inference for L\\'evy processes sampled\nirregularly, at low frequency. The estimation of the jump dynamics as well as\nthe estimation of the distributional density are investigated.\n  Non-asymptotic risk bounds are derived and the corresponding rates of\nconvergence are discussed under global as well as local regularity assumptions.\nMoreover, minimax optimality is proved for the estimator of the jump measure.\nSome numerical examples are given to illustrate the practical performance of\nthe estimation procedure.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 13:54:27 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 11:44:57 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Kappus", "Johanna", ""]]}, {"id": "1511.05948", "submitter": "Matyas Barczy", "authors": "Matyas Barczy, Balazs Nyul, Gyula Pap", "title": "Least squares estimation for the subcritical Heston model based on\n  continuous time observations", "comments": "22 pages. arXiv admin note: text overlap with arXiv:1310.4783", "journal-ref": "Journal of Statistical Theory and Practice 3(1), (2019), 13:18", "doi": null, "report-no": null, "categories": "math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove strong consistency and asymptotic normality of least squares\nestimators for the subcritical Heston model based on continuous time\nobservations. We also present some numerical illustrations of our results.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 20:53:10 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 09:38:31 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 17:55:30 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Barczy", "Matyas", ""], ["Nyul", "Balazs", ""], ["Pap", "Gyula", ""]]}, {"id": "1511.06014", "submitter": "Tor Lattimore", "authors": "Tor Lattimore", "title": "Regret Analysis of the Finite-Horizon Gittins Index Strategy for\n  Multi-Armed Bandits", "comments": "32 pages, to appear in COLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I analyse the frequentist regret of the famous Gittins index strategy for\nmulti-armed bandits with Gaussian noise and a finite horizon. Remarkably it\nturns out that this approach leads to finite-time regret guarantees comparable\nto those available for the popular UCB algorithm. Along the way I derive\nfinite-time bounds on the Gittins index that are asymptotically exact and may\nbe of independent interest. I also discuss some computational issues and\npresent experimental results suggesting that a particular version of the\nGittins index strategy is a modest improvement on existing algorithms with\nfinite-time regret guarantees such as UCB and Thompson sampling.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 22:52:26 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2016 01:31:35 GMT"}, {"version": "v3", "created": "Fri, 27 May 2016 22:07:00 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Lattimore", "Tor", ""]]}, {"id": "1511.06028", "submitter": "Michal Koles\\'ar", "authors": "Timothy B. Armstrong and Michal Koles\\'ar", "title": "Optimal inference in a class of regression models", "comments": "39 pages plus supplementary materials", "journal-ref": "Econometrica, Volume 86, Issue 2, March 2018, Pages 655-683", "doi": "10.3982/ECTA14434", "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing confidence intervals (CIs) for a\nlinear functional of a regression function, such as its value at a point, the\nregression discontinuity parameter, or a regression coefficient in a linear or\npartly linear regression. Our main assumption is that the regression function\nis known to lie in a convex function class, which covers most smoothness and/or\nshape assumptions used in econometrics. We derive finite-sample optimal CIs and\nsharp efficiency bounds under normal errors with known variance. We show that\nthese results translate to uniform (over the function class) asymptotic results\nwhen the error distribution is not known. When the function class is\ncentrosymmetric, these efficiency bounds imply that minimax CIs are close to\nefficient at smooth regression functions. This implies, in particular, that it\nis impossible to form CIs that are tighter using data-dependent tuning\nparameters, and maintain coverage over the whole function class. We specialize\nour results to inference on the regression discontinuity parameter, and\nillustrate them in simulations and an empirical application.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 00:03:37 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 20:07:14 GMT"}, {"version": "v3", "created": "Mon, 22 May 2017 16:07:56 GMT"}, {"version": "v4", "created": "Wed, 22 Nov 2017 05:57:54 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Armstrong", "Timothy B.", ""], ["Koles\u00e1r", "Michal", ""]]}, {"id": "1511.06190", "submitter": "Kai Zhang", "authors": "Kai Zhang and Lawrence D. Brown and Edward George and Linda Zhao", "title": "Uniform Correlation Mixture of Bivariate Normal Distributions and\n  Hypercubically-contoured Densities That Are Marginally Normal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bivariate normal density with unit variance and correlation $\\rho$ is\nwell-known. We show that by integrating out $\\rho$, the result is a function of\nthe maximum norm. The Bayesian interpretation of this result is that if we put\na uniform prior over $\\rho$, then the marginal bivariate density depends only\non the maximal magnitude of the variables. The square-shaped isodensity contour\nof this resulting marginal bivariate density can also be regarded as the\nequally-weighted mixture of bivariate normal distributions over all possible\ncorrelation coefficients. This density links to the Khintchine mixture method\nof generating random variables. We use this method to construct the higher\ndimensional generalizations of this distribution. We further show that for each\ndimension, there is a unique multivariate density that is a differentiable\nfunction of the maximum norm and is marginally normal, and the bivariate\ndensity from the integral over $\\rho$ is its special case in two dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 14:57:49 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Zhang", "Kai", ""], ["Brown", "Lawrence D.", ""], ["George", "Edward", ""], ["Zhao", "Linda", ""]]}, {"id": "1511.06198", "submitter": "Kai Zhang", "authors": "Kai Zhang", "title": "Spherical Cap Packing Asymptotics and Rank-Extreme Detection", "comments": "14 pages; 1 figure. Accepted Jan 31, 2017 by IEEE Transactions on\n  Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2017.2700202", "report-no": null, "categories": "math.ST cs.IT math.IT physics.data-an stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the spherical cap packing problem with a probabilistic approach.\nSuch probabilistic considerations result in an asymptotic sharp universal\nuniform bound on the maximal inner product between any set of unit vectors and\na stochastically independent uniformly distributed unit vector. When the set of\nunit vectors are themselves independently uniformly distributed, we further\ndevelop the extreme value distribution limit of the maximal inner product,\nwhich characterizes its uncertainty around the bound.\n  As applications of the above asymptotic results, we derive (1) an asymptotic\nsharp universal uniform bound on the maximal spurious correlation, as well as\nits uniform convergence in distribution when the explanatory variables are\nindependently Gaussian distributed; and (2) an asymptotic sharp universal bound\non the maximum norm of a low-rank elliptically distributed vector, as well as\nrelated limiting distributions. With these results, we develop a fast detection\nmethod for a low-rank structure in high-dimensional Gaussian data without using\nthe spectrum information.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 15:08:10 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 23:52:16 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Zhang", "Kai", ""]]}, {"id": "1511.06259", "submitter": "Ilaria Giulini", "authors": "Ilaria Giulini", "title": "Robust dimension-free Gram operator estimates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the question of estimating the Gram operator by\na robust estimator from an i.i.d. sample in a separable Hilbert space and we\npresent uniform bounds that hold under weak moment assumptions. The approach\nconsists in first obtaining non-asymptotic dimension-free bounds in\nfinite-dimensional spaces using some PAC-Bayesian inequalities related to\nGaussian perturbations of the parameter and then in generalizing the results in\na separable Hilbert space. We show both from a theoretical point of view and\nwith the help of some simulations that such a robust estimator improves the\nbehavior of the classical empirical one in the case of heavy tail data\ndistributions.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 17:03:01 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 07:41:33 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Giulini", "Ilaria", ""]]}, {"id": "1511.06263", "submitter": "Ilaria Giulini", "authors": "Ilaria Giulini", "title": "PAC-Bayesian bounds for Principal Component Analysis in Hilbert spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on some new robust estimators of the covariance matrix, we propose\nstable versions of Principal Component Analysis (PCA) and we qualify it\nindependently of the dimension of the ambient space. We first provide a robust\nestimator of the orthogonal projector on the largest eigenvectors of the\ncovariance matrix. The behavior of such an estimator is related to the size of\nthe gap in the spectrum of the covariance matrix and in particular a large gap\nis needed in order to get a good approximation. To avoid the assumption of a\nlarge eigengap in the spectrum of the covariance matrix we propose a robust\nversion of PCA that consists in performing a smooth cut-off of the spectrum via\na Lipschitz function. We provide bounds on the approximation error in terms of\nthe operator norm and of the Frobenius norm.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 17:14:34 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Giulini", "Ilaria", ""]]}, {"id": "1511.06268", "submitter": "Lukas Steinberger", "authors": "Lukas Steinberger", "title": "The relative effects of dimensionality and multiplicity of hypotheses on\n  the F-test in linear regression", "comments": null, "journal-ref": "Electron. J. Statist. 10 (2016) 2584-2640", "doi": "10.1214/16-EJS1186", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, several authors have re-examined the power of the classical F-test\nin linear regression in a `large-p, large-n' framework (cf. Zhong and Chen\n(2011), Wang and Cui (2013)). They highlight the loss of power as the number of\nregressors p increases relative to sample size n. These papers essentially\nfocus only on the overall test of the null hypothesis that all p slope\ncoefficients are equal to zero. Here, we consider the general case of testing q\nlinear hypotheses on the (p+1)-dimensional regression parameter vector that\nincludes p slope coefficients and an intercept parameter. In the case of\nGaussian design, we describe the dependence of the local asymptotic power\nfunction on both the relative number of parameters p and the number of\nhypotheses q being tested, showing that the negative effect of dimensionality\nis less severe if the number of hypotheses is small. Using the recent work of\nSrivastava and Vershynin (2013) on high dimensional sample covariance matrices\nwe are also able to substantially generalize previous results for non-Gaussian\nregressors.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 17:30:48 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 14:36:17 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Steinberger", "Lukas", ""]]}, {"id": "1511.06340", "submitter": "Yanwei Fu", "authors": "Yanwei Fu and De-An Huang and Leonid Sigal", "title": "Robust Classification by Pre-conditioned LASSO and Transductive\n  Diffusion Component Analysis", "comments": "we will significantly change the content of this paper which makes it\n  another paper. In order not to misleading, we decided to withdraw it. The\n  updated version can not be shared currently, for some reason. We will update\n  it once it is OK to be shared", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning-based recognition approaches require large-scale\ndatasets with large number of labelled training images. However, such datasets\nare inherently difficult and costly to collect and annotate. Hence there is a\ngreat and growing interest in automatic dataset collection methods that can\nleverage the web. % which are collected % in a cheap, efficient and yet\nunreliable way. Collecting datasets in this way, however, requires robust and\nefficient ways for detecting and excluding outliers that are common and\nprevalent. % Outliers are thus a % prominent treat of using these dataset. So\nfar, there have been a limited effort in machine learning community to directly\ndetect outliers for robust classification. Inspired by the recent work on\nPre-conditioned LASSO, this paper formulates the outlier detection task using\nPre-conditioned LASSO and employs \\red{unsupervised} transductive diffusion\ncomponent analysis to both integrate the topological structure of the data\nmanifold, from labeled and unlabeled instances, and reduce the feature\ndimensionality. Synthetic experiments as well as results on two real-world\nclassification tasks show that our framework can robustly detect the outliers\nand improve classification.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:13:51 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2019 02:06:46 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Fu", "Yanwei", ""], ["Huang", "De-An", ""], ["Sigal", "Leonid", ""]]}, {"id": "1511.06525", "submitter": "Samuel Rosa", "authors": "Samuel Rosa, Radoslav Harman", "title": "Optimal Approximate Designs for Comparison with Control in\n  Dose-Escalation Studies", "comments": "22 pages. Compared to the previous version: extended section 3.3,\n  added section 6 Discussion, some corrections made, proofs moved to appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an experiment, where a new drug is tested for the first time on\nhuman subjects - healthy volunteers. Such experiments are often performed as\ndose-escalation studies: a set of increasing doses is pre-selected, individuals\nare grouped into cohorts, and in each cohort, the dose number $i$ can be\nadministered only if the dose number $i-1$ has already been tested in the\nprevious cohort. If an adverse effect of a dose is observed, the experiment\nstops and thus no subjects are exposed to higher doses. In this paper, we\nassume that the response is affected both by the dose or placebo effects as\nwell as by the cohort effects. We provide optimal approximate designs for\nselected optimality criteria ($E$-, $MV$- and $LV$-optimality) for estimating\nthe effects of the drug doses compared with the placebo. In particular, we\nobtain the optimality of Senn designs and extended Senn designs with respect to\nmultiple criteria.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 08:45:38 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2016 11:17:57 GMT"}], "update_date": "2016-08-17", "authors_parsed": [["Rosa", "Samuel", ""], ["Harman", "Radoslav", ""]]}, {"id": "1511.06540", "submitter": "Weihua Deng Professor", "authors": "Weihua Deng, Wanli Wang, Xinchun Tian, Yujiang Wu", "title": "Effects of the tempered aging and its Fokker-Planck equation", "comments": "12 pages, 14 figures", "journal-ref": "Journal of Statistical Physics, 164(2), 377-398, 2016", "doi": "10.1007/s10955-016-1547-3", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the renewal processes, if the waiting time probability density function is\na tempered power-law distribution, then the process displays a transition\ndynamics; and the transition time depends on the parameter $\\lambda$ of the\nexponential cutoff. In this paper, we discuss the aging effects of the renewal\nprocess with the tempered power-law waiting time distribution. By using the\naging renewal theory, the $p$-th moment of the number of renewal events\n$n_a(t_a, t)$ in the interval $(t_a, t_a+t)$ is obtained for both the weakly\nand strongly aged systems; and the corresponding surviving probabilities are\nalso investigated. We then further analyze the tempered aging continuous time\nrandom walk and its Einstein relation, and the mean square displacement is\nattained. Moreover, the tempered aging diffusion equation is derived.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 09:51:53 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Deng", "Weihua", ""], ["Wang", "Wanli", ""], ["Tian", "Xinchun", ""], ["Wu", "Yujiang", ""]]}, {"id": "1511.06544", "submitter": "Fran\\c{c}ois Portier", "authors": "Fran\\c{c}ois Portier and Johan Segers", "title": "On the weak convergence of the empirical conditional copula under a\n  simplifying assumption", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the copula of the conditional distribution of two random variables given\na covariate does not depend on the value of the covariate, two conflicting\nintuitions arise about the best possible rate of convergence attainable by\nnonparametric estimators of that copula. In the end, any such estimator must be\nbased on the marginal conditional distribution functions of the two dependent\nvariables given the covariate, and the best possible rates for estimating such\nlocalized objects is slower than the parametric one. However, the invariance of\nthe conditional copula given the value of the covariate suggests the\npossibility of parametric convergence rates. The more optimistic intuition is\nshown to be correct, confirming a conjecture supported by extensive Monte Carlo\nsimulations by I. Hobaek Haff and J. Segers [Computational Statistics and Data\nAnalysis 84:1--13, 2015] and improving upon the nonparametric rate obtained\ntheoretically by I. Gijbels, M. Omelka and N. Veraverbeke [Scandinavian Journal\nof Statistics 2015, to appear]. The novelty of the proposed approach lies in\nthe double smoothing procedure employed for the estimator of the marginal\ncumulative distribution functions. Under mild conditions on the bandwidth\nsequence, the estimator is shown to take values in a certain class of smooth\nfunctions, the class having sufficiently small entropy for empirical process\narguments to work. The copula estimator itself is asymptotically\nundistinguishable from a kind of oracle empirical copula, making it appear as\nif the marginal conditional distribution functions were known.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 10:06:15 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 15:16:29 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Portier", "Fran\u00e7ois", ""], ["Segers", "Johan", ""]]}, {"id": "1511.06813", "submitter": "Robin Evans", "authors": "Robin J. Evans and Thomas S. Richardson", "title": "Smooth, identifiable supermodels of discrete DAG models with latent\n  variables", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a parameterization of the discrete nested Markov model, which is a\nsupermodel that approximates DAG models (Bayesian network models) with latent\nvariables. Such models are widely used in causal inference and machine\nlearning. We explicitly evaluate their dimension, show that they are curved\nexponential families of distributions, and fit them to data. The\nparameterization avoids the irregularities and unidentifiability of latent\nvariable models. The parameters used are all fully identifiable and\ncausally-interpretable quantities.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 01:47:34 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 22:37:10 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Evans", "Robin J.", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1511.07030", "submitter": "Deborah Schneider-Luftman Ms", "authors": "D.Schneider-Luftman, A.T. Walden", "title": "Partial Coherence Estimation via Spectral Matrix Shrinkage under\n  Quadratic Loss", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2582464", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial coherence is an important quantity derived from spectral or precision\nmatrices and is used in seismology, meteorology, oceanography, neuroscience and\nelsewhere. If the number of complex degrees of freedom only slightly exceeds\nthe dimension of the multivariate stationary time series, spectral matrices are\npoorly conditioned and shrinkage techniques suggest themselves. When true\npartial coherencies are quite large then for shrinkage estimators of the\ndiagonal weighting kind it is shown empirically that the minimization of risk\nusing quadratic loss (QL) leads to oracle partial coherence estimators superior\nto those derived by minimizing risk using Hilbert-Schmidt (HS) loss. When true\npartial coherencies are small the methods behave similarly. We derive two new\nQL estimators for spectral matrices, and new QL and HS estimators for precision\nmatrices. In addition for the full estimation (non-oracle) case where certain\ntrace expressions must also be estimated, we examine the behaviour of three\ndifferent QL estimators, the precision matrix one seeming particularly robust\nand reliable. For the empirical study we carry out exact simulations derived\nfrom real EEG data for two individuals, one having large, and the other small,\npartial coherencies. This ensures our study covers cases of real-world\nrelevance.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 16:35:20 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Schneider-Luftman", "D.", ""], ["Walden", "A. T.", ""]]}, {"id": "1511.07036", "submitter": "Hazhir Homei", "authors": "Hazhir Homei", "title": "Another Generalization of Unimodality", "comments": "To Appear in the Communications in Statistics - Theory and Methods", "journal-ref": null, "doi": "10.1080/03610926.2015.1006788", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general characterization for \\alpha-unimodal distributions was provided by\nAlamatsaz (1985) who later introduced a multivariate extension of them\n(Alamatsaz 1993). Here, by solving the related equations, another\ngeneralization for unimodality is presented. As a result of this\ngeneralization, a simpler proof of a conjecture, as well as a characterization\nfor generalized arcsin distributions and some generalizations of the author's\nearlier works, have been obtained. Last, but not the least, it is shown that\nsome elementary methods can be more powerful than some more advanced\ntechniques.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 17:23:22 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Homei", "Hazhir", ""]]}, {"id": "1511.07050", "submitter": "Julia Benditkis", "authors": "Julia Benditkis, Philipp Heesen and Arnold Janssen", "title": "The False Discovery Rate (FDR) of Multiple Tests in a Class Room Lecture", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple tests are designed to test a whole collection of null hypotheses\nsimultaneously. Their quality is often judged by the false discovery rate\n(FDR), i.e. the expectation of the quotient of the number of false rejections\ndivided by the amount of all rejections. The widely cited Benjamini and\nHochberg (BH) step up multiple test controls the FDR under various regularity\nassumptions. In this note we present a rapid approach to the BH step up and\nstep down tests. Also sharp FDR inequalities are discussed for dependent\np-values and examples and counter-examples are considered. In particular, the\nBonferroni bound is sharp under dependence for control of the family-wise error\nrate.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 18:41:30 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Benditkis", "Julia", ""], ["Heesen", "Philipp", ""], ["Janssen", "Arnold", ""]]}, {"id": "1511.07091", "submitter": "Akimichi Takemura", "authors": "Akimichi Takemura", "title": "Exponential decay rate of partial autocorrelation coefficients of ARMA\n  and short-memory processes", "comments": null, "journal-ref": "Statistics and Probability Letters 110 (2016) 207-210", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a short proof of the fact that the exponential decay rate of\npartial autocorrelation coefficients of a short-memory process, in particular\nan ARMA process, is equal to the exponential decay rate of the coefficients of\nits infinite autoregressive representation.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 01:16:30 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2016 08:03:46 GMT"}], "update_date": "2016-02-09", "authors_parsed": [["Takemura", "Akimichi", ""]]}, {"id": "1511.07098", "submitter": "Adolfo Quiroz", "authors": "Alf Onshuus and Adolfo J. Quiroz", "title": "Metric Entropy estimation using o-minimality Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown how tools from the area of Model Theory, specifically from the\nTheory of o-minimality, can be used to prove that a class of functions is\nVC-subgraph (in the sense of Dudley, 1987), and therefore satisfies a uniform\npolynomial metric entropy bound. We give examples where the use of these\nmethods significantly improves the existing metric entropy bounds. The methods\nproposed here can be applied to finite dimensional parametric families of\nfunctions without the need for the parameters to live in a compact set, as is\nsometimes required in theorems that produce similar entropy bounds (for\ninstance Theorem 19.7 of van der Vaart, 1998).\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 02:36:30 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Onshuus", "Alf", ""], ["Quiroz", "Adolfo J.", ""]]}, {"id": "1511.07384", "submitter": "Fatma Zehra Do\\u{g}ru", "authors": "Fatma Zehra Do\\u{g}ru and Olcay Arslan", "title": "Robust mixture regression modeling based on the Generalized M\n  (GM)-estimation method", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bai (2010) and Bai et al. (2012) proposed robust mixture regression method\nbased on the M regression estimation. However, the M-estimators are robust\nagainst the outliers in response variables, but they are not robust against the\noutliers in explanatory variables (leverage points). In this paper, we propose\na robust mixture regression procedure to handle the outliers and the leverage\npoints, simultaneously. Our proposed mixture regression method is based on the\nGM regression estimation. We give an Expectation Maximization (EM) type\nalgorithm to compute estimates for the parameters of interest. We provide a\nsimulation study and a real data example to assess the robustness performance\nof the proposed method against the outliers and the leverage points.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 19:53:04 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Do\u011fru", "Fatma Zehra", ""], ["Arslan", "Olcay", ""]]}, {"id": "1511.07428", "submitter": "Ananda Theertha Suresh", "authors": "Alon Orlitsky, Ananda Theertha Suresh, Yihong Wu", "title": "Estimating the number of unseen species: A bird in the hand is worth\n  $\\log n $ in the bush", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the number of unseen species is an important problem in many\nscientific endeavors. Its most popular formulation, introduced by Fisher, uses\n$n$ samples to predict the number $U$ of hitherto unseen species that would be\nobserved if $t\\cdot n$ new samples were collected. Of considerable interest is\nthe largest ratio $t$ between the number of new and existing samples for which\n$U$ can be accurately predicted.\n  In seminal works, Good and Toulmin constructed an intriguing estimator that\npredicts $U$ for all $t\\le 1$, thereby showing that the number of species can\nbe estimated for a population twice as large as that observed. Subsequently\nEfron and Thisted obtained a modified estimator that empirically predicts $U$\neven for some $t>1$, but without provable guarantees.\n  We derive a class of estimators that $\\textit{provably}$ predict $U$ not just\nfor constant $t>1$, but all the way up to $t$ proportional to $\\log n$. This\nshows that the number of species can be estimated for a population $\\log n$\ntimes larger than that observed, a factor that grows arbitrarily large as $n$\nincreases. We also show that this range is the best possible and that the\nestimators' mean-square error is optimal up to constants for any $t$. Our\napproach yields the first provable guarantee for the Efron-Thisted estimator\nand, in addition, a variant which achieves stronger theoretical and\nexperimental performance than existing methodologies on a variety of synthetic\nand real datasets.\n  The estimators we derive are simple linear estimators that are computable in\ntime proportional to $n$. The performance guarantees hold uniformly for all\ndistributions, and apply to all four standard sampling models commonly used\nacross various scientific disciplines: multinomial, Poisson, hypergeometric,\nand Bernoulli product.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 20:58:55 GMT"}, {"version": "v2", "created": "Mon, 29 Feb 2016 20:58:19 GMT"}, {"version": "v3", "created": "Thu, 3 Mar 2016 02:52:44 GMT"}], "update_date": "2016-03-04", "authors_parsed": [["Orlitsky", "Alon", ""], ["Suresh", "Ananda Theertha", ""], ["Wu", "Yihong", ""]]}, {"id": "1511.07613", "submitter": "Axel B\\\"ucher", "authors": "Axel B\\\"ucher and Johan Segers", "title": "Maximum likelihood estimation for the Fr\\'echet distribution based on\n  block maxima extracted from a time series", "comments": "30 pages + 7 pages supplement, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The block maxima method in extreme-value analysis proceeds by fitting an\nextreme-value distribution to a sample of block maxima extracted from an\nobserved stretch of a time series. The method is usually validated under two\nsimplifying assumptions: the block maxima should be distributed according to an\nextreme-value distribution and the sample of block maxima should be\nindependent. Both assumptions are only approximately true.\n  For general triangular arrays of block maxima attracted to the Fr\\'echet\ndistribution, consistency and asymptotic normality is established for the\nmaximum likelihood estimator of the parameters of the limiting Fr\\'echet\ndistribution. The results are specialized to the setting of block maxima\nextracted from a strictly stationary time series. The case where the underlying\nrandom variables are independent and identically distributed is further worked\nout in detail. The results are illustrated by theoretical examples and Monte\nCarlo simulations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 09:08:38 GMT"}, {"version": "v2", "created": "Fri, 16 Sep 2016 14:08:24 GMT"}], "update_date": "2016-09-19", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Segers", "Johan", ""]]}, {"id": "1511.07903", "submitter": "Ahmad AlAmmouri", "authors": "Ahmad AlAmmouri, Hesham ElSawy, Mohamed-Slim Alouini", "title": "Flexible Design for $\\alpha$-Duplex Communications in Multi-Tier\n  Cellular Networks", "comments": "Submitted to Tcom", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backward compatibility is an essential ingredient for the success of new\ntechnologies. In the context of in-band full-duplex (FD) communication, FD base\nstations (BSs) should support half-duplex (HD) users' equipment (UEs) without\nsacrificing the foreseen FD gains. This paper presents flexible and tractable\nmodeling framework for multi-tier cellular networks with FD BSs and FD/HD UEs.\nThe presented model is based on stochastic geometry and accounts for the\nintrinsic vulnerability of uplink transmissions. The results show that FD UEs\nare not necessarily required to harvest rate gains from FD BSs. In particular,\nthe results show that adding FD UEs to FD BSs offers a maximum of $5\\%$ rate\ngain over FD BSs and HD UEs case if multi-user diversity is exploited, which is\na marginal gain compared to the burden required to implement FD transceivers at\nthe UEs' side. To this end, we shed light on practical scenarios where HD UEs\noperation with FD BSs outperforms the operation when both the BSs and UEs are\nFD and we find a closed form expression for the critical value of the\nself-interference attenuation power required for the FD UEs to outperform HD\nUEs.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 22:36:00 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 14:25:38 GMT"}, {"version": "v3", "created": "Wed, 20 Apr 2016 08:42:23 GMT"}], "update_date": "2016-04-21", "authors_parsed": [["AlAmmouri", "Ahmad", ""], ["ElSawy", "Hesham", ""], ["Alouini", "Mohamed-Slim", ""]]}, {"id": "1511.08102", "submitter": "Matey Neykov", "authors": "Matey Neykov, Jun S. Liu, Tianxi Cai", "title": "L1-Regularized Least Squares for Support Recovery of High Dimensional\n  Single Index Models with Gaussian Designs", "comments": "36 pages; 6 figures; typos corrected; clearer notation introduced", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that for a certain class of single index models (SIMs) $Y =\nf(\\boldsymbol{X}_{p \\times 1}^\\intercal\\boldsymbol{\\beta}_0, \\varepsilon)$,\nsupport recovery is impossible when $\\boldsymbol{X} \\sim \\mathcal{N}(0,\n\\mathbb{I}_{p \\times p})$ and a model complexity adjusted sample size is below\na critical threshold. Recently, optimal algorithms based on Sliced Inverse\nRegression (SIR) were suggested. These algorithms work provably under the\nassumption that the design $\\boldsymbol{X}$ comes from an i.i.d. Gaussian\ndistribution. In the present paper we analyze algorithms based on covariance\nscreening and least squares with $L_1$ penalization (i.e. LASSO) and\ndemonstrate that they can also enjoy optimal (up to a scalar) rescaled sample\nsize in terms of support recovery, albeit under slightly different assumptions\non $f$ and $\\varepsilon$ compared to the SIR based algorithms. Furthermore, we\nshow more generally, that LASSO succeeds in recovering the signed support of\n$\\boldsymbol{\\beta}_0$ if $\\boldsymbol{X} \\sim \\mathcal{N}(0,\n\\boldsymbol{\\Sigma})$, and the covariance $\\boldsymbol{\\Sigma}$ satisfies the\nirrepresentable condition. Our work extends existing results on the support\nrecovery of LASSO for the linear model, to a more general class of SIMs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 16:00:44 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 22:28:16 GMT"}, {"version": "v3", "created": "Thu, 23 Jun 2016 02:46:01 GMT"}], "update_date": "2016-06-24", "authors_parsed": [["Neykov", "Matey", ""], ["Liu", "Jun S.", ""], ["Cai", "Tianxi", ""]]}, {"id": "1511.08271", "submitter": "Tyrus Berry", "authors": "Tyrus Berry and Timothy Sauer", "title": "Density Estimation on Manifolds with Boundary", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density estimation is a crucial component of many machine learning methods,\nand manifold learning in particular, where geometry is to be constructed from\ndata alone. A significant practical limitation of the current density\nestimation literature is that methods have not been developed for manifolds\nwith boundary, except in simple cases of linear manifolds where the location of\nthe boundary is assumed to be known. We overcome this limitation by developing\na density estimation method for manifolds with boundary that does not require\nany prior knowledge of the location of the boundary. To accomplish this we\nintroduce statistics that provably estimate the distance and direction of the\nboundary, which allows us to apply a cut-and-normalize boundary correction. By\ncombining multiple cut-and-normalize estimators we introduce a consistent\nkernel density estimator that has uniform bias, at interior and boundary\npoints, on manifolds with boundary.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 02:30:14 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2015 22:13:25 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2016 15:12:33 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Berry", "Tyrus", ""], ["Sauer", "Timothy", ""]]}, {"id": "1511.08291", "submitter": "Paul Kabaila", "authors": "Paul Kabaila, Rheanna Mainzer and Davide Farchione", "title": "Conditional assessment of the impact of a Hausman pretest on confidence\n  intervals", "comments": null, "journal-ref": "Conditional assessment of the impact of a Hausman pretest on\n  confidence intervals. Statistica Neerlandica (2017)", "doi": "10.1111/stan.12109", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assess the impact of a Hausman pretest, applied to panel data, on a\nconfidence interval for the slope, conditional on the observed values of the\ntime-varying covariate. This assessment has the advantages that it (a) relates\nto the values of this covariate at hand, (b) is valid irrespective of how this\ncovariate is generated, (c) uses finite sample results and (d) results in an\nassessment that is determined by the values of this covariate and only 2\nunknown parameters. Our conditional analysis shows that the confidence interval\nconstructed after a Hausman pretest should not be used.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 05:01:26 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Kabaila", "Paul", ""], ["Mainzer", "Rheanna", ""], ["Farchione", "Davide", ""]]}, {"id": "1511.08327", "submitter": "Nathalie Villa-Vialaneix", "authors": "Robin Genuer (ISPED, SISTM), Jean-Michel Poggi (UPD5, LM-Orsay),\n  Christine Tuleau-Malot (JAD), Nathalie Villa-Vialaneix (MIAT INRA)", "title": "Random Forests for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big Data is one of the major challenges of statistical science and has\nnumerous consequences from algorithmic and theoretical viewpoints. Big Data\nalways involve massive data but they also often include online data and data\nheterogeneity. Recently some statistical methods have been adapted to process\nBig Data, like linear regression models, clustering methods and bootstrapping\nschemes. Based on decision trees combined with aggregation and bootstrap ideas,\nrandom forests were introduced by Breiman in 2001. They are a powerful\nnonparametric statistical method allowing to consider in a single and versatile\nframework regression problems, as well as two-class and multi-class\nclassification problems. Focusing on classification problems, this paper\nproposes a selective review of available proposals that deal with scaling\nrandom forests to Big Data problems. These proposals rely on parallel\nenvironments or on online adaptations of random forests. We also describe how\nrelated quantities -- such as out-of-bag error and variable importance -- are\naddressed in these methods. Then, we formulate various remarks for random\nforests in the Big Data context. Finally, we experiment five variants on two\nmassive datasets (15 and 120 millions of observations), a simulated one as well\nas real world data. One variant relies on subsampling while three others are\nrelated to parallel implementations of random forests and involve either\nvarious adaptations of bootstrap to Big Data or to \"divide-and-conquer\"\napproaches. The fifth variant relates on online learning of random forests.\nThese numerical experiments lead to highlight the relative performance of the\ndifferent variants, as well as some of their limitations.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 09:04:47 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 14:51:57 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Genuer", "Robin", "", "ISPED, SISTM"], ["Poggi", "Jean-Michel", "", "UPD5, LM-Orsay"], ["Tuleau-Malot", "Christine", "", "JAD"], ["Villa-Vialaneix", "Nathalie", "", "MIAT INRA"]]}, {"id": "1511.08369", "submitter": "Iv\\'an D\\'iaz", "authors": "Iv\\'an D\\'iaz, Marco Carone, and Mark J. van der Laan", "title": "Second-Order Inference for the Mean of a Variable Missing at Random", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a second-order estimator of the mean of a variable subject to\nmissingness, under the missing at random assumption. The estimator improves\nupon existing methods by using an approximate second-order expansion of the\nparameter functional, in addition to the first-order expansion employed by\nstandard doubly robust methods. This results in weaker assumptions about the\nconvergence rates necessary to establish consistency, local efficiency, and\nasymptotic linearity. The general estimation strategy is developed under the\ntargeted minimum loss-based estimation (TMLE) framework. We present a\nsimulation comparing the sensitivity of the first and second order estimators\nto the convergence rate of the initial estimators of the outcome regression and\nmissingness score. In our simulation, the second-order TMLE improved the\ncoverage probability of a confidence interval by up to 85%. In addition, we\npresent a first-order estimator inspired by a second-order expansion of the\nparameter functional. This estimator only requires one-dimensional smoothing,\nwhereas implementation of the second-order TMLE generally requires kernel\nsmoothing on the covariate space. The first-order estimator proposed is\nexpected to have improved finite sample performance compared to existing\nfirst-order estimators. In our simulations, the proposed first-order estimator\nimproved the coverage probability by up to 90%. We provide an illustration of\nour methods using a publicly available dataset to determine the effect of an\nanticoagulant on health outcomes of patients undergoing percutaneous coronary\nintervention. We provide R code implementing the proposed estimator.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 12:22:07 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""], ["Carone", "Marco", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1511.08404", "submitter": "Iv\\'an D\\'iaz", "authors": "Iv\\'an D\\'iaz, Elizabeth Colantuoni, Daniel F. Hanley, and Michael\n  Rosenblum", "title": "Improved Precision in the Analysis of Randomized Trials with Survival\n  Outcomes, without Assuming Proportional Hazards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new estimator of the restricted mean survival time in randomized\ntrials where there is right censoring that may depend on treatment and baseline\nvariables. The proposed estimator leverages prognostic baseline variables to\nobtain equal or better asymptotic precision compared to traditional estimators.\nUnder regularity conditions and random censoring within strata of treatment and\nbaseline variables, the proposed estimator has the following features: (i) it\nis interpretable under violations of the proportional hazards assumption; (ii)\nit is consistent and at least as precise as the Kaplan-Meier estimator under\nindependent censoring; (iii) it remains consistent under violations of\nindependent censoring (unlike the Kaplan-Meier estimator) when either the\ncensoring or survival distributions are estimated consistently; and (iv) it\nachieves the nonparametric efficiency bound when both of these distributions\nare consistently estimated. We illustrate the performance of our method using\nsimulations based on resampling data from a completed, phase 3 randomized\nclinical trial of a new surgical treatment for stroke; the proposed estimator\nachieves a 12% gain in relative efficiency compared to the Kaplan-Meier\nestimator. The proposed estimator has potential advantages over existing\napproaches for randomized trials with time-to-event outcomes, since existing\nmethods either rely on model assumptions that are untenable in many\napplications, or lack some of the efficiency and consistency properties\n(i)-(iv). We focus on estimation of the restricted mean survival time, but our\nmethods may be adapted to estimate any treatment effect measure defined as a\nsmooth contrast between the survival curves for each study arm. We provide R\ncode to implement the estimator.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 14:49:00 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 00:19:04 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""], ["Colantuoni", "Elizabeth", ""], ["Hanley", "Daniel F.", ""], ["Rosenblum", "Michael", ""]]}, {"id": "1511.08591", "submitter": "Stefan Rass", "authors": "Stefan Rass", "title": "On Game-Theoretic Risk Management (Part Two) -- Algorithms to Compute\n  Nash-Equilibria in Games with Distributions as Payoffs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.GT math.ST q-fin.EC q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The game-theoretic risk management framework put forth in the precursor work\n\"Towards a Theory of Games with Payoffs that are Probability-Distributions\"\n(arXiv:1506.07368 [q-fin.EC]) is herein extended by algorithmic details on how\nto compute equilibria in games where the payoffs are probability distributions.\nOur approach is \"data driven\" in the sense that we assume empirical data\n(measurements, simulation, etc.) to be available that can be compiled into\ndistribution models, which are suitable for efficient decisions about\npreferences, and setting up and solving games using these as payoffs. While\npreferences among distributions turn out to be quite simple if nonparametric\nmethods (kernel density estimates) are used, computing Nash-equilibria in games\nusing such models is discovered as inefficient (if not impossible). In fact, we\ngive a counterexample in which fictitious play fails to converge for the\n(specifically unfortunate) choice of payoff distributions in the game, and\nintroduce a suitable tail approximation of the payoff densities to tackle the\nissue. The overall procedure is essentially a modified version of fictitious\nplay, and is herein described for standard and multicriteria games, to\niteratively deliver an (approximate) Nash-equilibrium. An exact method using\nlinear programming is also given.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 09:39:06 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2020 07:18:26 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Rass", "Stefan", ""]]}, {"id": "1511.08624", "submitter": "Elodie Vernet", "authors": "Elodie Vernet", "title": "Non Parametric Hidden Markov Models with Finite State Space: Posterior\n  Concentration Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of non parametric hidden Markov models with finite state space is\nflourishing in practice while few theoretical guarantees are known in this\nframework. Here, we study asymptotic guarantees for these models in the\nBayesian framework. We obtain posterior concentration rates with respect to the\n$L_1$-norm on joint marginal densities of consecutive observations in a general\ntheorem. We apply this theorem to two cases and obtain minimax concentration\nrates. We consider discrete observations with emission distributions\ndistributed from a Dirichlet process and continuous observations with emission\ndistributions distributed from Dirichlet process mixtures of Gaussian\ndistributions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 11:27:25 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Vernet", "Elodie", ""]]}, {"id": "1511.08627", "submitter": "Matias Heikkil\\\"a", "authors": "Matias Heikkil\\\"a, Yves Dominicy, Pauliina Ilmonen", "title": "On Asymptotic Properties of the Separating Hill Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling and understanding multivariate extreme events is challenging, but of\ngreat importance in various applications - e.g. in biostatistics, climatology,\nand finance. The separating Hill estimator can be used in estimating the\nextreme value index of a heavy tailed multivariate elliptical distribution. We\nconsider the asymptotic behavior of the separating Hill estimator under\nestimated location and scatter. The asymptotic properties of the separating\nHill estimator are known under elliptical distribution with known location and\nscatter. However, the effect of estimation of the location and scatter has\npreviously been examined only in a simulation study. We show, analytically,\nthat the separating Hill estimator is consistent and asymptotically normal\nunder estimated location and scatter, when certain mild conditions are met.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 11:34:55 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 15:31:49 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Heikkil\u00e4", "Matias", ""], ["Dominicy", "Yves", ""], ["Ilmonen", "Pauliina", ""]]}, {"id": "1511.08677", "submitter": "Henryk Z\\\"ahle", "authors": "Volker Kr\\\"atschmer, Alexander Schied, Henryk Z\\\"ahle", "title": "Domains of weak continuity of statistical functionals with a view toward\n  robust statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many standard estimators such as several maximum likelihood estimators or the\nempirical estimator for any law-invariant convex risk measure are not\n(qualitatively) robust in the classical sense. However, these estimators may\nnevertheless satisfy a weak (Kr\\\"atschmer/Schied/Z\\\"ahle 2012, 2014) or a local\n(Z\\\"ahle 2016) robustness property on relevant sets of distributions. One aim\nof our paper is to identify sets of local robustness, and to explain the\nbenefit of the knowledge of such sets. For instance, we will be able to\ndemonstrate that many maximum likelihood estimators are robust on their natural\nparametric domains. A second aim consists in extending the general theory of\nrobust estimation to our local framework. In particular we provide a\ncorresponding Hampel-type theorem linking local robustness of a plug-in\nestimator with a certain continuity condition.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 14:09:17 GMT"}, {"version": "v2", "created": "Mon, 20 Jun 2016 15:18:25 GMT"}], "update_date": "2016-06-21", "authors_parsed": [["Kr\u00e4tschmer", "Volker", ""], ["Schied", "Alexander", ""], ["Z\u00e4hle", "Henryk", ""]]}, {"id": "1511.08698", "submitter": "Edgar Alan Muro Jimenez", "authors": "Alan Muro, Sara van de Geer", "title": "Concentration behavior of the penalized least squares estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the standard nonparametric regression model and take as estimator\nthe penalized least squares function. In this article, we study the trade-off\nbetween closeness to the true function and complexity penalization of the\nestimator, where complexity is described by a seminorm on a class of functions.\nFirst, we present an exponential concentration inequality revealing the\nconcentration behavior of the trade-off of the penalized least squares\nestimator around a nonrandom quantity, where such quantity depends on the\nproblem under consideration. Then, under some conditions and for the proper\nchoice of the tuning parameter, we obtain bounds for this nonrandom quantity.\nWe illustrate our results with some examples that include the smoothing splines\nestimator.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 15:13:49 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 17:19:09 GMT"}], "update_date": "2016-10-20", "authors_parsed": [["Muro", "Alan", ""], ["van de Geer", "Sara", ""]]}, {"id": "1511.08762", "submitter": "Tijl De Bie", "authors": "Tijl De Bie, Jefrey Lijffijt, Raul Santos-Rodriguez, Bo Kang", "title": "Informative Data Projections: A Framework and Two Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for Projection Pursuit aim to facilitate the visual exploration of\nhigh-dimensional data by identifying interesting low-dimensional projections. A\nmajor challenge is the design of a suitable quality metric of projections,\ncommonly referred to as the projection index, to be maximized by the Projection\nPursuit algorithm. In this paper, we introduce a new information-theoretic\nstrategy for tackling this problem, based on quantifying the amount of\ninformation the projection conveys to a user given their prior beliefs about\nthe data. The resulting projection index is a subjective quantity, explicitly\ndependent on the intended user. As a useful illustration, we developed this\nidea for two particular kinds of prior beliefs. The first kind leads to PCA\n(Principal Component Analysis), shining new light on when PCA is (not)\nappropriate. The second kind leads to a novel projection index, the\nmaximization of which can be regarded as a robust variant of PCA. We show how\nthis projection index, though non-convex, can be effectively maximized using a\nmodified power method as well as using a semidefinite programming relaxation.\nThe usefulness of this new projection index is demonstrated in comparative\nempirical experiments against PCA and a popular Projection Pursuit method.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 17:53:46 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["De Bie", "Tijl", ""], ["Lijffijt", "Jefrey", ""], ["Santos-Rodriguez", "Raul", ""], ["Kang", "Bo", ""]]}, {"id": "1511.08893", "submitter": "Francesco Buscemi", "authors": "Francesco Buscemi", "title": "Degradable channels, less noisy channels, and quantum statistical\n  morphisms: an equivalence relation", "comments": "v2: 15 pages, no figures, published version; v1: 14 pages, no figures", "journal-ref": "Problems of Information Transmission, volume 53, issue 3, pages\n  201-213 (October 2016)", "doi": "10.1134/S0032946016030017", "report-no": null, "categories": "quant-ph cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two partial orderings among communication channels, namely, `being degradable\ninto' and `being less noisy than,' are reconsidered in the light of recent\nresults about statistical comparisons of quantum channels. Though our analysis\ncovers at once both classical and quantum channels, we also provide a separate\ntreatment of classical noisy channels, and show how, in this case, an\nalternative self-contained proof can be constructed, with its own particular\nmerits with respect to the general result.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 12:31:56 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 03:31:59 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Buscemi", "Francesco", ""]]}, {"id": "1511.08963", "submitter": "Bryon Aragam", "authors": "Bryon Aragam, Arash A. Amini, Qing Zhou", "title": "Learning Directed Acyclic Graphs with Penalized Neighbourhood Regression", "comments": "54 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a family of regularized score-based estimators for learning the\nstructure of a directed acyclic graph (DAG) for a multivariate normal\ndistribution from high-dimensional data with $p\\gg n$. Our main results\nestablish support recovery guarantees and deviation bounds for a family of\npenalized least-squares estimators under concave regularization without\nassuming prior knowledge of a variable ordering. These results apply to a\nvariety of practical situations that allow for arbitrary nondegenerate\ncovariance structures as well as many popular regularizers including the MCP,\nSCAD, $\\ell_{0}$ and $\\ell_{1}$. The proof relies on interpreting a DAG as a\nrecursive linear structural equation model, which reduces the estimation\nproblem to a series of neighbourhood regressions. We provide a novel\nstatistical analysis of these neighbourhood problems, establishing uniform\ncontrol over the superexponential family of neighbourhoods associated with a\nGaussian distribution. We then apply these results to study the statistical\nproperties of score-based DAG estimators, learning causal DAGs, and inferring\nconditional independence relations via graphical models. Our results\nyield---for the first time---finite-sample guarantees for structure learning of\nGaussian DAGs in high-dimensions via score-based estimation.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 03:52:28 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2015 19:41:19 GMT"}, {"version": "v3", "created": "Mon, 2 Oct 2017 02:59:27 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Aragam", "Bryon", ""], ["Amini", "Arash A.", ""], ["Zhou", "Qing", ""]]}, {"id": "1511.09078", "submitter": "Damian Brzyski", "authors": "Damian Brzyski and Weijie Su and Ma{\\l}gorzata Bogdan", "title": "Group SLOPE - adaptive selection of groups of predictors", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sorted L-One Penalized Estimation is a relatively new convex optimization\nprocedure which allows for adaptive selection of regressors under sparse high\ndimensional designs. Here we extend the idea of SLOPE to deal with the\nsituation when one aims at selecting whole groups of explanatory variables\ninstead of single regressors. This approach is particularly useful when\nvariables in the same group are strongly correlated and thus true predictors\nare difficult to distinguish from their correlated \"neighbors\"'. We formulate\nthe respective convex optimization problem, gSLOPE (group SLOPE), and propose\nan efficient algorithm for its solution. We also define a notion of the group\nfalse discovery rate (gFDR) and provide a choice of the sequence of tuning\nparameters for gSLOPE so that gFDR is provably controlled at a prespecified\nlevel if the groups of variables are orthogonal to each other. Moreover, we\nprove that the resulting procedure adapts to unknown sparsity and is\nasymptotically minimax with respect to the estimation of the proportions of\nvariance of the response variable explained by regressors from different\ngroups. We also provide a method for the choice of the regularizing sequence\nwhen variables in different groups are not orthogonal but statistically\nindependent and illustrate its good properties with computer simulations.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 19:59:19 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Brzyski", "Damian", ""], ["Su", "Weijie", ""], ["Bogdan", "Ma\u0142gorzata", ""]]}, {"id": "1511.09298", "submitter": "Hilmar Mai", "authors": "Denis Belomestny, Hilmar Mai, John Schoenmakers", "title": "Generalized Post-Widder inversion formula with application to statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we derive an inversion formula for the Laplace transform of a\ndensity observed on a curve in the complex domain, which generalizes the well\nknown Post-Widder formula. We establish convergence of our inversion method and\nderive the corresponding convergence rates for the case of a Laplace transform\nof a smooth density. As an application we consider the problem of statistical\ninference for variance-mean mixture models. We construct a nonparametric\nestimator for the mixing density based on the generalized Post-Widder formula,\nderive bounds for its root mean square error and give a brief numerical\nexample.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 13:24:36 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Belomestny", "Denis", ""], ["Mai", "Hilmar", ""], ["Schoenmakers", "John", ""]]}, {"id": "1511.09433", "submitter": "Joel Tropp", "authors": "Samet Oymak and Joel A. Tropp", "title": "Universality laws for randomized dimension reduction, with applications", "comments": "v2 and v3 with technical corrections. Code for reproducing figures\n  available at http://users.cms.caltech.edu/~jtropp/", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction is the process of embedding high-dimensional data into a\nlower dimensional space to facilitate its analysis. In the Euclidean setting,\none fundamental technique for dimension reduction is to apply a random linear\nmap to the data. This dimension reduction procedure succeeds when it preserves\ncertain geometric features of the set.\n  The question is how large the embedding dimension must be to ensure that\nrandomized dimension reduction succeeds with high probability.\n  This paper studies a natural family of randomized dimension reduction maps\nand a large class of data sets. It proves that there is a phase transition in\nthe success probability of the dimension reduction map as the embedding\ndimension increases. For a given data set, the location of the phase transition\nis the same for all maps in this family. Furthermore, each map has the same\nstability properties, as quantified through the restricted minimum singular\nvalue. These results can be viewed as new universality laws in high-dimensional\nstochastic geometry.\n  Universality laws for randomized dimension reduction have many applications\nin applied mathematics, signal processing, and statistics. They yield design\nprinciples for numerical linear algebra algorithms, for compressed sensing\nmeasurement ensembles, and for random linear codes. Furthermore, these results\nhave implications for the performance of statistical estimation methods under a\nlarge class of random experimental designs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 19:14:23 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 18:22:19 GMT"}, {"version": "v3", "created": "Sun, 17 Sep 2017 00:45:33 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Oymak", "Samet", ""], ["Tropp", "Joel A.", ""]]}]