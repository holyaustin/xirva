[{"id": "2104.00119", "submitter": "Philip Dawid", "authors": "A. Philip Dawid and Monica Musio", "title": "Effects of Causes and Causes of Effects", "comments": "28 pages. Invited paper for \"Annual Review of Statistics and its\n  Application\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe and contrast two distinct problem areas for statistical\ncausality: studying the likely effects of an intervention (\"effects of\ncauses\"), and studying whether there is a causal link between the observed\nexposure and outcome in an individual case (\"causes of effects\"). For each of\nthese, we introduce and compare various formal frameworks that have been\nproposed for that purpose, including the decision-theoretic approach,\nstructural equations, structural and stochastic causal models, and potential\noutcomes. It is argued that counterfactual concepts are unnecessary for\nstudying effects of causes, but are needed for analysing causes of effects.\nThey are however subject to a degree of arbitrariness, which can be reduced,\nthough not in general eliminated, by taking account of additional structure in\nthe problem.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 21:06:36 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Dawid", "A. Philip", ""], ["Musio", "Monica", ""]]}, {"id": "2104.00245", "submitter": "Zhe Zhang", "authors": "Zhe Zhang and Linjun Zhang", "title": "High-Dimensional Differentially-Private EM Algorithm: Methods and\n  Near-Optimal Statistical Guarantees", "comments": "45 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a general framework to design differentially\nprivate expectation-maximization (EM) algorithms in high-dimensional latent\nvariable models, based on the noisy iterative hard-thresholding. We derive the\nstatistical guarantees of the proposed framework and apply it to three specific\nmodels: Gaussian mixture, mixture of regression, and regression with missing\ncovariates. In each model, we establish the near-optimal rate of convergence\nwith differential privacy constraints, and show the proposed algorithm is\nminimax rate optimal up to logarithm factors. The technical tools developed for\nthe high-dimensional setting are then extended to the classic low-dimensional\nlatent variable models, and we propose a near rate-optimal EM algorithm with\ndifferential privacy guarantees in this setting. Simulation studies and real\ndata analysis are conducted to support our results.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 04:08:34 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Zhang", "Zhe", ""], ["Zhang", "Linjun", ""]]}, {"id": "2104.00277", "submitter": "Adrian Riekert", "authors": "Arnulf Jentzen, Adrian Riekert", "title": "A proof of convergence for stochastic gradient descent in the training\n  of artificial neural networks with ReLU activation for constant target\n  functions", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG cs.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we study the stochastic gradient descent (SGD) optimization\nmethod in the training of fully-connected feedforward artificial neural\nnetworks with ReLU activation. The main result of this work proves that the\nrisk of the SGD process converges to zero if the target function under\nconsideration is constant. In the established convergence result the considered\nartificial neural networks consist of one input layer, one hidden layer, and\none output layer (with $d \\in \\mathbb{N}$ neurons on the input layer, $H \\in\n\\mathbb{N}$ neurons on the hidden layer, and one neuron on the output layer).\nThe learning rates of the SGD process are assumed to be sufficiently small and\nthe input data used in the SGD process to train the artificial neural networks\nis assumed to be independent and identically distributed.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 06:28:30 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Jentzen", "Arnulf", ""], ["Riekert", "Adrian", ""]]}, {"id": "2104.00383", "submitter": "Leonard Monsaingeon", "authors": "L\\'eonard Monsaingeon, Dmitry Vorotnikov", "title": "Schr\\\"odinger encounters Fisher and Rao: a survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.FA math.IT math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note we review the dynamical Schr\\\"odinger problem on the\nnon-commutative Fisher-Rao space of positive semi-definite matrix-valued\nmeasures. The presentation is meant to be self-contained, and we discuss in\nparticular connections with Gaussian optimal transport, entropy, and quantum\nFisher information.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 10:36:51 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Monsaingeon", "L\u00e9onard", ""], ["Vorotnikov", "Dmitry", ""]]}, {"id": "2104.00581", "submitter": "Wenyang Huang", "authors": "Huiwen Wang, Wenyang Huang, Shanshan Wang", "title": "Forecasting open-high-low-close data contained in candlestick chart", "comments": "35 pages, 11 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the (open-high-low-close)OHLC data contained in candlestick chart\nis of great practical importance, as exemplified by applications in the field\nof finance. Typically, the existence of the inherent constraints in OHLC data\nposes great challenge to its prediction, e.g., forecasting models may yield\nunrealistic values if these constraints are ignored. To address it, a novel\ntransformation approach is proposed to relax these constraints along with its\nexplicit inverse transformation, which ensures the forecasting models obtain\nmeaningful openhigh-low-close values. A flexible and efficient framework for\nforecasting the OHLC data is also provided. As an example, the detailed\nprocedure of modelling the OHLC data via the vector auto-regression (VAR) model\nand vector error correction (VEC) model is given. The new approach has high\npractical utility on account of its flexibility, simple implementation and\nstraightforward interpretation. Extensive simulation studies are performed to\nassess the effectiveness and stability of the proposed approach. Three\nfinancial data sets of the Kweichow Moutai, CSI 100 index and 50 ETF of Chinese\nstock market are employed to document the empirical effect of the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 09:01:40 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Huiwen", ""], ["Huang", "Wenyang", ""], ["Wang", "Shanshan", ""]]}, {"id": "2104.00673", "submitter": "Stephen Bates", "authors": "Stephen Bates and Trevor Hastie and Robert Tibshirani", "title": "Cross-validation: what does it estimate and how well does it do it?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-validation is a widely-used technique to estimate prediction error, but\nits behavior is complex and not fully understood. Ideally, one would like to\nthink that cross-validation estimates the prediction error for the model at\nhand, fit to the training data. We prove that this is not the case for the\nlinear model fit by ordinary least squares; rather it estimates the average\nprediction error of models fit on other unseen training sets drawn from the\nsame population. We further show that this phenomenon occurs for most popular\nestimates of prediction error, including data splitting, bootstrapping, and\nMallow's Cp. Next, the standard confidence intervals for prediction error\nderived from cross-validation may have coverage far below the desired level.\nBecause each data point is used for both training and testing, there are\ncorrelations among the measured accuracies for each fold, and so the usual\nestimate of variance is too small. We introduce a nested cross-validation\nscheme to estimate this variance more accurately, and show empirically that\nthis modification leads to intervals with approximately correct coverage in\nmany examples where traditional cross-validation intervals fail. Lastly, our\nanalysis also shows that when producing confidence intervals for prediction\naccuracy with simple data splitting, one should not re-fit the model on the\ncombined data, since this invalidates the confidence intervals.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:58:54 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 16:51:23 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bates", "Stephen", ""], ["Hastie", "Trevor", ""], ["Tibshirani", "Robert", ""]]}, {"id": "2104.00753", "submitter": "Steve Huntsman", "authors": "Steve Huntsman", "title": "Sampling and statistical physics via symmetry", "comments": "Proceedings of Les Houches 2020 school on Joint Structures and Common\n  Foundations of Statistical Physics, Information Geometry and Inference for\n  Learning", "journal-ref": null, "doi": "10.1007/978-3-030-77957-3_20", "report-no": null, "categories": "cond-mat.stat-mech math-ph math.DS math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate both Markov chain Monte Carlo (MCMC) sampling algorithms and\nbasic statistical physics in terms of elementary symmetries. This perspective\non sampling yields derivations of well-known MCMC algorithms and a new parallel\nalgorithm that appears to converge more quickly than current state of the art\nmethods. The symmetry perspective also yields a parsimonious framework for\nstatistical physics and a practical approach to constructing meaningful notions\nof effective temperature and energy directly from time series data. We apply\nthese latter ideas to Anosov systems.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 20:24:36 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Huntsman", "Steve", ""]]}, {"id": "2104.00846", "submitter": "Tianyu Zhang", "authors": "Tianyu Zhang and Noah Simon", "title": "A Sieve Stochastic Gradient Descent Estimator for Online Nonparametric\n  Regression in Sobolev ellipsoids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The goal of regression is to recover an unknown underlying function that best\nlinks a set of predictors to an outcome from noisy observations. In\nnon-parametric regression, one assumes that the regression function belongs to\na pre-specified infinite dimensional function space (the hypothesis space). In\nthe online setting, when the observations come in a stream, it is\ncomputationally-preferable to iteratively update an estimate rather than\nrefitting an entire model repeatedly. Inspired by nonparametric sieve\nestimation and stochastic approximation methods, we propose a sieve stochastic\ngradient descent estimator (Sieve-SGD) when the hypothesis space is a Sobolev\nellipsoid. We show that Sieve-SGD has rate-optimal MSE under a set of simple\nand direct conditions. We also show that the Sieve-SGD estimator can be\nconstructed with low time expense, and requires almost minimal memory usage\namong all statistically rate-optimal estimators, under some conditions on the\ndistribution of the predictors.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 01:52:47 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Zhang", "Tianyu", ""], ["Simon", "Noah", ""]]}, {"id": "2104.00986", "submitter": "Daniel Straub", "authors": "Daniel Straub, Max Ehre, Iason Papaioannou", "title": "Decision-theoretic reliability sensitivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and discuss sensitivity metrics for reliability analysis, which\nare based on the value of information. These metrics are easier to interpret\nthan other existing sensitivity metrics in the context of a specific decision\nand they are applicable to any type of reliability assessment, including those\nwith dependent inputs. We develop computational strategies that enable\nefficient evaluation of these metrics, in some scenarios without additional\nruns of the deterministic model. The metrics are investigated by application to\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 11:13:24 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Straub", "Daniel", ""], ["Ehre", "Max", ""], ["Papaioannou", "Iason", ""]]}, {"id": "2104.01067", "submitter": "Lionel Truquet", "authors": "Zinsou Max Debaly and Lionel Truquet", "title": "Multivariate time series models for mixed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general approach for modeling the dynamic of multivariate time\nseries when the data are of mixed type (binary/count/continuous). Our method is\nquite flexible and conditionally on past values, each coordinate at time $t$\ncan have a distribution compatible with a standard univariate time series model\nsuch as GARCH, ARMA, INGARCH or logistic models whereas past values of the\nother coordinates play the role of exogenous covariates in the dynamic. The\nsimultaneous dependence in the multivariate time series can be modeled with a\ncopula. Additional exogenous covariates are also allowed in the dynamic. We\nfirst study usual stability properties of these models and then show that\nautoregressive parameters can be consistently estimated equation-by-equation\nusing a pseudo-maximum likelihood method, leading to a fast implementation even\nwhen the number of time series is large. Moreover, we prove consistency results\nwhen a parametric copula model is fitted to the time series and in the case of\nGaussian copulas, we show that the likelihood estimator of the correlation\nmatrix is strongly consistent. We carefully check all our assumptions for two\nprototypical examples: a GARCH/INGARCH model and logistic/log-linear INGARCH\nmodel. Our results are illustrated with numerical experiments as well as two\nreal data sets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 14:33:41 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Debaly", "Zinsou Max", ""], ["Truquet", "Lionel", ""]]}, {"id": "2104.01144", "submitter": "Nicolas Marie", "authors": "Nicolas Marie", "title": "Projection Estimators of the Stationary Density of a Differential\n  Equation Driven by the Fractional Brownian Motion", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with projection estimators of the density of the stationary\nsolution $X$ to a differential equation driven by the fractional Brownian\nmotion under a dissipativity condition on the drift function. A model selection\nmethod is provided and, thanks to the concentration inequality for Lipschitz\nfunctionals of discrete samples of $X$ proved in Bertin et al. (2020), an\noracle inequality is established for the adaptive estimator.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 16:53:45 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Marie", "Nicolas", ""]]}, {"id": "2104.01280", "submitter": "Sjoerd Dirksen", "authors": "Sjoerd Dirksen, Johannes Maly, Holger Rauhut", "title": "Covariance estimation under one-bit quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classical problem of estimating the covariance matrix of a\nsubgaussian distribution from i.i.d. samples in the novel context of coarse\nquantization, i.e., instead of having full knowledge of the samples, they are\nquantized to one or two bits per entry. This problem occurs naturally in signal\nprocessing applications. We introduce new estimators in two different\nquantization scenarios and derive non-asymptotic estimation error bounds in\nterms of the operator norm. In the first scenario we consider a simple,\nscale-invariant one-bit quantizer and derive an estimation result for the\ncorrelation matrix of a centered Gaussian distribution. In the second scenario,\nwe add random dithering to the quantizer. In this case we can accurately\nestimate the full covariance matrix of a general subgaussian distribution by\ncollecting two bits per entry of each sample. In both scenarios, our bounds\napply to masked covariance estimation. We demonstrate the near-optimality of\nour error bounds by deriving corresponding (minimax) lower bounds and using\nnumerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 23:59:04 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dirksen", "Sjoerd", ""], ["Maly", "Johannes", ""], ["Rauhut", "Holger", ""]]}, {"id": "2104.01346", "submitter": "Ruth Heller", "authors": "Ruth Heller and Abba Krieger and Saharon Rosset", "title": "Optimal multiple testing and design in clinical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A central goal in designing clinical trials is to find the test that\nmaximizes power (or equivalently minimizes required sample size) for finding a\ntrue research hypothesis subject to the constraint of type I error. When there\nis more than one test, such as in clinical trials with multiple endpoints, the\nissues of optimal design and optimal policies become more complex. In this\npaper we address the question of how such optimal tests should be defined and\nhow they can be found. We review different notions of power and how they relate\nto study goals, and also consider the requirements of type I error control and\nthe nature of the policies. This leads us to formulate the optimal policy\nproblem as an explicit optimization problem with objective and constraints\nwhich describe its specific desiderata. We describe a complete solution for\nderiving optimal policies for two hypotheses, which have desired monotonicity\nproperties, and are computationally simple. For some of the optimization\nformulations this yields optimal policies that are identical to existing\npolicies, such as Hommel's procedure or the procedure of Bittman et al. (2009),\nwhile for others it yields completely novel and more powerful policies than\nexisting ones. We demonstrate the nature of our novel policies and their\nimproved power extensively in simulation and on the APEX study (Cohen et al.,\n2016).\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 08:54:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Heller", "Ruth", ""], ["Krieger", "Abba", ""], ["Rosset", "Saharon", ""]]}, {"id": "2104.01450", "submitter": "Yaakov Malinovsky", "authors": "Yaakov Malinovsky and John W. Moon", "title": "On the negative dependence inequalities and maximal score in round-robin\n  tournament", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Huber's (1963) inequality for the joint distribution function of\nnegative dependent scores in the round-robin tournament. As a byproduct, this\nextension implies convergence in probability of the maximal score in a\nround-robin tournament in a more general setting.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 17:08:20 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Malinovsky", "Yaakov", ""], ["Moon", "John W.", ""]]}, {"id": "2104.01573", "submitter": "Maliheh Heidari", "authors": "Maliheh Heidari, Md Abu Manju, Pieta C.IJzerman-Boon and Edwin R. van\n  den Heuvel", "title": "D-optimal designs for the Mitscherlich non-linear regression function", "comments": "23 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mitscherlich's function is a well-known three-parameter non-linear regression\nfunction that quantifies the relation between a stimulus or a time variable and\na response. Optimal designs for this function have been constructed only for\nnormally distributed responses with homoscedastic variances. In this paper, we\nconstruct D-optimal designs for discrete and continuous responses having their\ndistribution function in the exponential family. We also demonstrate the\nconnection with D-optimality for weighted linear regression.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 09:34:27 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Heidari", "Maliheh", ""], ["Manju", "Md Abu", ""], ["IJzerman-Boon", "Pieta C.", ""], ["Heuvel", "Edwin R. van den", ""]]}, {"id": "2104.01603", "submitter": "Konstantinos Vamvourellis", "authors": "Konstantinos Vamvourellis, Kostas Kalogeropoulos and Irini Moustaki", "title": "Generalised Bayesian Structural Equation Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a generalised framework for Bayesian Structural Equation Modelling\n(SEM) that can be applied to a variety of data types. The introduced framework\nfocuses on the approximate zero approach, according to which a hypothesised\nstructure is formulated with approximate rather than exact zero. It extends\npreviously suggested models by \\citeA{MA12} and can handle continuous, binary,\nand ordinal data. Moreover, we propose a novel model assessment paradigm aiming\nto address shortcomings of posterior predictive $p-$values, which provide the\ndefault metric of fit for Bayesian SEM. The introduced model assessment\nprocedure monitors the out-of-sample predictive performance of the model in\nquestion, and draws from a list of principles to answer whether the\nhypothesised theory is supported by the data. We incorporate scoring rules and\ncross-validation to supplement existing model assessment metrics for Bayesian\nSEM. The methodology is illustrated in continuous and categorical data examples\nvia simulation experiments as well as real-world applications on the `Big-5'\npersonality scale and the Fagerstrom test for nicotine dependence.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 12:44:05 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Vamvourellis", "Konstantinos", ""], ["Kalogeropoulos", "Kostas", ""], ["Moustaki", "Irini", ""]]}, {"id": "2104.01648", "submitter": "Martin Molina-Fructuoso", "authors": "Martin Molina-Fructuoso and Ryan Murray", "title": "Tukey Depths and Hamilton-Jacobi Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread application of modern machine learning has increased the need\nfor robust statistical algorithms. This work studies one such fundamental\nstatistical measure known as the Tukey depth. We study the problem in the\ncontinuum (population) limit. In particular, we derive the associated necessary\nconditions, which take the form of a first-order partial differential equation.\nWe discuss the classical interpretation of this necessary condition as the\nviscosity solution of a Hamilton-Jacobi equation, but with a non-classical\nHamiltonian with discontinuous dependence on the gradient at zero. We prove\nthat this equation possesses a unique viscosity solution and that this solution\nalways bounds the Tukey depth from below. In certain cases, we prove that the\nTukey depth is equal to the viscosity solution, and we give some illustrations\nof standard numerical methods from the optimal control community which deal\ndirectly with the partial differential equation. We conclude by outlining\nseveral promising research directions both in terms of new numerical algorithms\nand theoretical challenges.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 17:13:50 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Molina-Fructuoso", "Martin", ""], ["Murray", "Ryan", ""]]}, {"id": "2104.01863", "submitter": "Matteo Barigozzi", "authors": "Matteo Barigozzi and Matteo Farn\\`e", "title": "An algebraic estimator for large spectral density matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new estimator of high-dimensional spectral density matrices,\ncalled UNshrunk ALgebraic Spectral Estimator (UNALSE), under the assumption of\nan underlying low rank plus sparse structure, as typically assumed in dynamic\nfactor models. The UNALSE is computed by minimizing a quadratic loss under a\nnuclear norm plus $l_1$ norm constraint to control the latent rank and the\nresidual sparsity pattern. The loss function requires as input the classical\nsmoothed periodogram estimator and two threshold parameters, the choice of\nwhich is thoroughly discussed. We prove consistency of UNALSE as both the\ndimension $p$ and the sample size $T$ diverge to infinity, as well as algebraic\nconsistency, i.e., the recovery of latent rank and residual sparsity pattern\nwith probability one. The finite sample properties of UNALSE are studied by\nmeans of an extended simulation exercise as well as an empirical analysis of US\nmacroeconomic data.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 11:57:20 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Barigozzi", "Matteo", ""], ["Farn\u00e8", "Matteo", ""]]}, {"id": "2104.01883", "submitter": "Alex Dytso", "authors": "Alex Dytso, H. Vincent Poor, Shlomo Shamai (Shitz)", "title": "A General Derivative Identity for the Conditional Mean Estimator in\n  Gaussian Noise and Some Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consider a channel ${\\bf Y}={\\bf X}+ {\\bf N}$ where ${\\bf X}$ is an\n$n$-dimensional random vector, and ${\\bf N}$ is a Gaussian vector with a\ncovariance matrix ${\\bf \\mathsf{K}}_{\\bf N}$. The object under consideration in\nthis paper is the conditional mean of ${\\bf X}$ given ${\\bf Y}={\\bf y}$, that\nis ${\\bf y} \\to E[{\\bf X}|{\\bf Y}={\\bf y}]$. Several identities in the\nliterature connect $E[{\\bf X}|{\\bf Y}={\\bf y}]$ to other quantities such as the\nconditional variance, score functions, and higher-order conditional moments.\nThe objective of this paper is to provide a unifying view of these identities.\n  In the first part of the paper, a general derivative identity for the\nconditional mean is derived. Specifically, for the Markov chain ${\\bf U}\n\\leftrightarrow {\\bf X} \\leftrightarrow {\\bf Y}$, it is shown that the Jacobian\nof $E[{\\bf U}|{\\bf Y}={\\bf y}]$ is given by ${\\bf \\mathsf{K}}_{{\\bf N}}^{-1}\n{\\bf Cov} ( {\\bf X}, {\\bf U} | {\\bf Y}={\\bf y})$.\n  In the second part of the paper, via various choices of ${\\bf U}$, the new\nidentity is used to generalize many of the known identities and derive some new\nones. First, a simple proof of the Hatsel and Nolte identity for the\nconditional variance is shown. Second, a simple proof of the recursive identity\ndue to Jaffer is provided. Third, a new connection between the conditional\ncumulants and the conditional expectation is shown. In particular, it is shown\nthat the $k$-th derivative of $E[X|Y=y]$ is the $(k+1)$-th conditional\ncumulant.\n  The third part of the paper considers some applications. In a first\napplication, the power series and the compositional inverse of $E[X|Y=y]$ are\nderived. In a second application, the distribution of the estimator error\n$(X-E[X|Y])$ is derived. In a third application, we construct consistent\nestimators (empirical Bayes estimators) of the conditional cumulants from an\ni.i.d. sequence $Y_1,...,Y_n$.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 12:48:28 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dytso", "Alex", "", "Shitz"], ["Poor", "H. Vincent", "", "Shitz"], ["Shamai", "Shlomo", "", "Shitz"]]}, {"id": "2104.01986", "submitter": "Nabarun Deb", "authors": "Nabarun Deb, Bhaswar B. Bhattacharya, and Bodhisattva Sen", "title": "Efficiency Lower Bounds for Distribution-Free Hotelling-Type Two-Sample\n  Tests Based on Optimal Transport", "comments": "60 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wilcoxon rank-sum test is one of the most popular distribution-free\nprocedures for testing the equality of two univariate probability\ndistributions. One of the main reasons for its popularity can be attributed to\nthe remarkable result of Hodges and Lehmann (1956), which shows that the\nasymptotic relative efficiency of Wilcoxon's test with respect to Student's\n$t$-test, under location alternatives, never falls below 0.864, despite the\nformer being exactly distribution-free for all sample sizes. Even more striking\nis the result of Chernoff and Savage (1958), which shows that the efficiency of\na Gaussian score transformed Wilcoxon's test, against the $t$-test, is lower\nbounded by 1. In this paper we study the two-sample problem in the multivariate\nsetting and propose distribution-free analogues of the Hotelling $T^2$ test\n(the natural multidimensional counterpart of Student's $t$-test) based on\noptimal transport and obtain extensions of the above celebrated results over\nvarious natural families of multivariate distributions. Our proposed tests are\nconsistent against a general class of alternatives and satisfy Hodges-Lehmann\nand Chernoff-Savage-type efficiency lower bounds, despite being entirely\nagnostic to the underlying data generating mechanism. In particular, a\ncollection of our proposed tests suffer from no loss in asymptotic efficiency,\nwhen compared to Hotelling $T^2$. To the best of our knowledge, these are the\nfirst collection of multivariate, nonparametric, exactly distribution-free\ntests that provably achieve such attractive efficiency lower bounds. We also\ndemonstrate the broader scope of our methods in optimal transport based\nnonparametric inference by constructing exactly distribution-free multivariate\ntests for mutual independence, which suffer from no loss in asymptotic\nefficiency against the classical Wilks' likelihood ratio test, under Konijn\nalternatives.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 16:25:22 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Deb", "Nabarun", ""], ["Bhattacharya", "Bhaswar B.", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "2104.01987", "submitter": "Weijie J. Su", "authors": "Jinshuo Dong, Aaron Roth, Weijie J. Su", "title": "Rejoinder: Gaussian Differential Privacy", "comments": "Updated the references. Rejoinder to discussions on Gaussian\n  Differential Privacy, read to the Royal Statistical Society in December 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this rejoinder, we aim to address two broad issues that cover most\ncomments made in the discussion. First, we discuss some theoretical aspects of\nour work and comment on how this work might impact the theoretical foundation\nof privacy-preserving data analysis. Taking a practical viewpoint, we next\ndiscuss how f-differential privacy (f-DP) and Gaussian differential privacy\n(GDP) can make a difference in a range of applications.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 16:27:56 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 02:40:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Dong", "Jinshuo", ""], ["Roth", "Aaron", ""], ["Su", "Weijie J.", ""]]}, {"id": "2104.02020", "submitter": "Dootika Vats", "authors": "Sanket Agrawal, Dootika Vats, Krzysztof {\\L}atuszy\\'nski, Gareth O.\n  Roberts", "title": "Optimal Scaling of MCMC Beyond Metropolis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of optimally scaling the proposal distribution in a Markov chain\nMonte Carlo algorithm is critical to the quality of the generated samples. Much\nwork has gone into obtaining such results for various Metropolis-Hastings (MH)\nalgorithms. Recently, acceptance probabilities other than MH are being employed\nin problems with intractable target distributions. There is little resource\navailable on tuning the Gaussian proposal distributions for this situation. We\nobtain optimal scaling results for a general class of acceptance functions,\nwhich includes Barker's and Lazy-MH acceptance functions. In particular,\noptimal values for Barker's algorithm are derived and are found to be\nsignificantly different from that obtained for MH algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:19:35 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Agrawal", "Sanket", ""], ["Vats", "Dootika", ""], ["\u0141atuszy\u0144ski", "Krzysztof", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "2104.02105", "submitter": "Olha Bodnar", "authors": "Olha Bodnar and Taras Bodnar", "title": "Objective Bayesian meta-analysis based on generalized multivariate\n  random effects model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective Bayesian inference procedures are derived for the parameters of the\nmultivariate random effects model generalized to elliptically contoured\ndistributions. The posterior for the overall mean vector and the between-study\ncovariance matrix is deduced by assigning two noninformative priors to the\nmodel parameter, namely the Berger and Bernardo reference prior and the\nJeffreys prior, whose analytical expressions are obtained under weak\ndistributional assumptions. It is shown that the only condition needed for the\nposterior to be proper is that the sample size is larger than the dimension of\nthe data-generating model, independently of the class of elliptically contoured\ndistributions used in the definition of the generalized multivariate random\neffects model. The theoretical findings of the paper are applied to real data\nconsisting of ten studies about the effectiveness of hypertension treatment for\nreducing blood pressure where the treatment effects on both the systolic blood\npressure and diastolic blood pressure are investigated.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:19:49 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Bodnar", "Olha", ""], ["Bodnar", "Taras", ""]]}, {"id": "2104.02147", "submitter": "Henry-Louis de Kergorlay", "authors": "Henry-Louis de Kergorlay", "title": "Which Sampling Densities are Suitable for Spectral Clustering on\n  Unbounded Domains?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a random geometric graph with vertices sampled from a probability\nmeasure supported on $\\mathbb R^d$, and study its connectivity. We show the\ngraph is typically disconnected, unless the sampling density has\nsuperexponential decay. In the later setting, we identify an asymptotic\nthreshold value for the radius parameter of the graph such that, for radius\nvalues beyond the threshold, some concentration properties hold for the sampled\npoints of the graph, while the graph is disconnected for radius values below\nthe same threshold. Properties of point processes are well-known to be closely\nrelated to the analysis of geometric learning problems, such as spectral\nclustering. This work can be seen as a first step towards understanding the\nconsistency of spectral clustering when the probability measure has unbounded\nsupport. In particular, we narrow down the setting under which spectral\nclustering algorithms on $\\mathbb R^d$ may be expected to achieve consistency,\nto a sufficiently fast decay of the sampling density (superexponential) and a\nsufficiently slowly decaying radius parameter value as a function of $n$, the\nnumber of sampled points.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:37:54 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["de Kergorlay", "Henry-Louis", ""]]}, {"id": "2104.02292", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Guillaume Boglioni Beaulieu and Pierre Lafaye de Micheaux and\n  Fr\\'ed\\'eric Ouimet", "title": "Counterexamples to the classical Central Limit Theorem for triplewise\n  independent random variables having a common arbitrary margin", "comments": "25 pages, 5 figures, 1 table. arXiv admin note: text overlap with\n  arXiv:2003.01350", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct explicitly two sequences of triplewise independent random\nvariables having a common but arbitrary marginal distribution $F$ (satisfying\nvery mild conditions) for which a Central Limit Theorem (CLT) does not hold. We\nobtain, in closed form, the asymptotic distributions of the sample means of\nthose sequences, which are seen to depend on the specific choice of $F$. This\nallows us to illustrate the extent of the `failure' of the classical CLT under\ntriplewise independence. Our methodology is simple and can also be used to\ncreate, for any integer $K$, new $K$-tuplewise independent but dependent\nsequences (which are useful to assess the ability of independence tests to\ndetect complex dependence). For $K \\geq 4$, it appears that the sequences thus\ncreated do verify a CLT, and we explain heuristically why this is the case.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 05:16:04 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Beaulieu", "Guillaume Boglioni", ""], ["de Micheaux", "Pierre Lafaye", ""], ["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2104.02422", "submitter": "Matteo Farn\\`e Dr.", "authors": "Matteo Farn\\`e and Angela Montanari", "title": "Large factor model estimation by nuclear norm plus $l_1$ norm\n  penalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper provides a comprehensive estimation framework via nuclear norm\nplus $l_1$ norm penalization for high-dimensional approximate factor models\nwith a sparse residual covariance. The underlying assumptions allow for\nnon-pervasive latent eigenvalues and a prominent residual covariance pattern.\nIn that context, existing approaches based on principal components may lead to\nmisestimate the latent rank, due to the numerical instability of sample\neigenvalues. On the contrary, the proposed optimization problem retrieves the\nlatent covariance structure and exactly recovers the latent rank and the\nresidual sparsity pattern. Conditioning on them, the asymptotic rates of the\nsubsequent ordinary least squares estimates of loadings and factor scores are\nprovided, the recovered latent eigenvalues are shown to be maximally\nconcentrated and the estimates of factor scores via Bartlett's and Thompson's\nmethods are proved to be the most precise given the data. The validity of\noutlined results is highlighted in an exhaustive simulation study and in a real\nfinancial data example.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 10:56:09 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Farn\u00e8", "Matteo", ""], ["Montanari", "Angela", ""]]}, {"id": "2104.02427", "submitter": "Claudio Durastanti Dr.", "authors": "Claudio Durastanti and Nicola Turchi", "title": "Nonparametric needlet estimation for partial derivatives of a\n  probability density function on the $d$-torus", "comments": "28 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the estimation of the partial derivatives of a\nprobability density function of directional data on the $d$-dimensional torus\nwithin the local thresholding framework. The estimators here introduced are\nbuilt by means of the toroidal needlets, a class of wavelets characterized by\nexcellent concentration properties in both the real and the harmonic domains.\nIn particular, we discuss the convergence rates of the $L^p$-risks for these\nestimators, investigating on their minimax properties and proving their\noptimality over a scale of Besov spaces, here taken as nonparametric regularity\nfunction spaces.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:20:15 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 07:15:46 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Durastanti", "Claudio", ""], ["Turchi", "Nicola", ""]]}, {"id": "2104.02507", "submitter": "Subhodh Kotekal", "authors": "Subhodh Kotekal", "title": "Statistical Limits of Sparse Mixture Detection", "comments": "70 pages; minor typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting a general sparse mixture and obtain an\nexplicit characterization of the phase transition under some conditions,\ngeneralizing the univariate results of Cai and Wu. Additionally, we provide a\nsufficient condition for the adaptive optimality of a Higher Criticism type\ntesting statistic formulated by Gao and Ma. In the course of establishing these\nresults, we offer a unified perspective through the large deviations theory.\nThe phase transition and adaptive optimality we establish are direct\nconsequences of the large deviation principle of the normalized log-likelihood\nratios between the null and the signal distributions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:39:16 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 23:15:15 GMT"}, {"version": "v3", "created": "Wed, 26 May 2021 00:43:50 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Kotekal", "Subhodh", ""]]}, {"id": "2104.02640", "submitter": "TrungTin Nguyen", "authors": "TrungTin Nguyen, Hien Duy Nguyen, Faicel Chamroukhi and Florence\n  Forbes", "title": "A non-asymptotic penalization criterion for model selection in mixture\n  of experts models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of experts (MoE) is a popular class of models in statistics and\nmachine learning that has sustained attention over the years, due to its\nflexibility and effectiveness. We consider the Gaussian-gated localized MoE\n(GLoME) regression model for modeling heterogeneous data. This model poses\nchallenging questions with respect to the statistical estimation and model\nselection problems, including feature selection, both from the computational\nand theoretical points of view. We study the problem of estimating the number\nof components of the GLoME model, in a penalized maximum likelihood estimation\nframework. We provide a lower bound on the penalty that ensures a weak oracle\ninequality is satisfied by our estimator. To support our theoretical result, we\nperform numerical experiments on simulated and real data, which illustrate the\nperformance of our finite-sample oracle inequality.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:24:55 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Nguyen", "TrungTin", ""], ["Nguyen", "Hien Duy", ""], ["Chamroukhi", "Faicel", ""], ["Forbes", "Florence", ""]]}, {"id": "2104.02734", "submitter": "Jack Noonan", "authors": "Jack Noonan", "title": "Online change-point detection for a transient change", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a popular online change-point problem of detecting a transient\nchange in distributions of i.i.d. random variables. For this change-point\nproblem, several change-point procedures are formulated and some advanced\nresults for a particular procedure are surveyed. Some new approximations for\nthe average run length to false alarm are offered and the power of these\nprocedures for detecting a transient change in mean of a sequence of normal\nrandom variables is compared.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 18:03:29 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Noonan", "Jack", ""]]}, {"id": "2104.02929", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Andrew Ying, Ilya Shpitser, Eric Tchetgen Tchetgen", "title": "Minimax Kernel Machine Learning for a Class of Doubly Robust Functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A moment function is called doubly robust if it is comprised of two nuisance\nfunctions and the estimator based on it is a consistent estimator of the target\nparameter even if one of the nuisance functions is misspecified. In this paper,\nwe consider a class of doubly robust moment functions originally introduced in\n(Robins et al., 2008). We demonstrate that this moment function can be used to\nconstruct estimating equations for the nuisance functions. The main idea is to\nchoose each nuisance function such that it minimizes the dependency of the\nexpected value of the moment function to the other nuisance function. We\nimplement this idea as a minimax optimization problem. We then provide\nconditions required for asymptotic linearity of the estimator of the parameter\nof interest, which are based on the convergence rate of the product of the\nerrors of the nuisance functions, as well as the local ill-posedness of a\nconditional expectation operator. The convergence rates of the nuisance\nfunctions are analyzed using the modern techniques in statistical learning\ntheory based on the Rademacher complexity of the function spaces. We\nspecifically focus on the case that the function spaces are reproducing kernel\nHilbert spaces, which enables us to use its spectral properties to analyze the\nconvergence rates. As an application of the proposed methodology, we consider\nthe parameter of average causal effect both in presence and absence of latent\nconfounders. For the case of presence of latent confounders, we use the\nrecently proposed proximal causal inference framework of (Miao et al., 2018;\nTchetgen Tchetgen et al., 2020), and hence our results lead to a robust\nnon-parametric estimator for average causal effect in this framework.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 05:52:15 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Ying", "Andrew", ""], ["Shpitser", "Ilya", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "2104.02943", "submitter": "Myrto Limnios", "authors": "St\\'ephan Cl\\'emen\\c{c}on (LTCI), Myrto Limnios (CB), Nicolas Vayatis\n  (CB)", "title": "Concentration Inequalities for Two-Sample Rank Processes with\n  Application to Bipartite Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ROC curve is the gold standard for measuring the performance of a\ntest/scoring statistic regarding its capacity to discriminate between two\nstatistical populations in a wide variety of applications, ranging from anomaly\ndetection in signal processing to information retrieval, through medical\ndiagnosis. Most practical performance measures used in scoring/ranking\napplications such as the AUC, the local AUC, the p-norm push, the DCG and\nothers, can be viewed as summaries of the ROC curve. In this paper, the fact\nthat most of these empirical criteria can be expressed as two-sample linear\nrank statistics is highlighted and concentration inequalities for collections\nof such random variables, referred to as two-sample rank processes here, are\nproved, when indexed by VC classes of scoring functions. Based on these\nnonasymptotic bounds, the generalization capacity of empirical maximizers of a\nwide class of ranking performance criteria is next investigated from a\ntheoretical perspective. It is also supported by empirical evidence through\nconvincing numerical experiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 06:31:06 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"], ["Limnios", "Myrto", "", "CB"], ["Vayatis", "Nicolas", "", "CB"]]}, {"id": "2104.02960", "submitter": "Sagnik Nandy", "authors": "Zongming Ma and Sagnik Nandy", "title": "Community Detection with Contextual Multilayer Networks", "comments": "76 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study community detection when we observe $m$ sparse\nnetworks and a high dimensional covariate matrix, all encoding the same\ncommunity structure among $n$ subjects. In the asymptotic regime where the\nnumber of features $p$ and the number of subjects $n$ grows proportionally, we\nderive an exact formula of asymptotic minimum mean square error (MMSE) for\nestimating the common community structure in the balanced two block case. The\nformula implies the necessity of integrating information from multiple data\nsources. Consequently, it induces a sharp threshold of phase transition between\nthe regime where detection (i.e., weak recovery) is possible and the regime\nwhere no procedure performs better than a random guess. The asymptotic MMSE\ndepends on the covariate signal-to-noise ratio in a more subtle way than the\nphase transition threshold does. In the special case of $m=1$, our asymptotic\nMMSE formula complements the pioneering work of Deshpande et. al. (2018) which\nfound the sharp threshold when $m=1$.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:26:19 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 09:12:25 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Ma", "Zongming", ""], ["Nandy", "Sagnik", ""]]}, {"id": "2104.02978", "submitter": "Masaaki Imaizumi", "authors": "Tomoya Wakayama, Masaaki Imaizumi", "title": "Fast Convergence on Perfect Classification for Functional Data", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we investigate the availability of approaching to perfect\nclassification on functional data with finite samples. The seminal work\n(Delaigle and Hall (2012)) showed that classification on functional data is\neasier to define on a perfect classifier than on finite-dimensional data. This\nresult is based on their finding that a sufficient condition for the existence\nof a perfect classifier, named a Delaigle--Hall (DH) condition, is only\navailable for functional data. However, there is a danger that a large sample\nsize is required to achieve the perfect classification even though the DH\ncondition holds because a convergence of misclassification errors of functional\ndata is significantly slow. Specifically, a minimax rate of the convergence of\nerrors with functional data has a logarithm order in the sample size. This\nstudy solves this complication by proving that the DH condition also achieves\nfast convergence of the misclassification error in sample size. Therefore, we\nstudy a classifier with empirical risk minimization using reproducing kernel\nHilbert space (RKHS) and analyse its convergence rate under the DH condition.\nThe result shows that the convergence speed of the misclassification error by\nthe RKHS classifier has an exponential order in sample size. Technically, the\nproof is based on the following points: (i) connecting the DH condition and a\nmargin of classifiers, and (ii) handling metric entropy of functional data.\nExperimentally, we validate that the DH condition and the associated margin\ncondition have a certain impact on the convergence rate of the RKHS classifier.\nWe also find that some of the other classifiers for functional data have a\nsimilar property.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 08:00:54 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wakayama", "Tomoya", ""], ["Imaizumi", "Masaaki", ""]]}, {"id": "2104.03200", "submitter": "Friedrich Teuscher Dr.", "authors": "Friedrich Teuscher", "title": "The quantification of Simpsons paradox and other contributions to\n  contingency table theory", "comments": "36 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of contingency tables is a powerful statistical tool used in\nexperiments with categorical variables. This study improves parts of the theory\nunderlying the use of contingency tables. Specifically, the linkage\ndisequilibrium parameter as a measure of two-way interactions applied to\nthree-way tables makes it possible to quantify Simpsons paradox by a simple\nformula. With tests on three-way interactions, there is only one that\ndetermines whether the partial interactions of all variables agree or whether\nthere is at least one variable whose partial interactions disagree. To date,\nthere has been no test available that determines whether the partial\ninteractions of a certain variable agree or disagree, and the presented work\ncloses this gap. This work reveals the relation of the multiplicative and the\nadditive measure of a three-way interaction. Another contribution addresses the\nquestion of which cells in a contingency table are fixed when the first- and\nsecond-order marginal totals are given. The proposed procedure not only detects\nfixed zero counts but also fixed positive counts. This impacts the\ndetermination of the degrees of freedom. Furthermore, limitations of methods\nthat simulate contingency tables with given pairwise associations are\naddressed.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:42:21 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Teuscher", "Friedrich", ""]]}, {"id": "2104.03251", "submitter": "Zacharie Naulet", "authors": "Stefano Favaro and Zacharie Naulet", "title": "Near-optimal estimation of the unseen under regularly varying tail\n  populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ samples from a population of individuals belonging to different\nspecies, what is the number $U$ of hitherto unseen species that would be\nobserved if $\\lambda n$ new samples were collected? This is an important\nproblem in many scientific endeavors, and it has been the subject of recent\nworks introducing non-parametric estimators of $U$ that are minimax\nnear-optimal and consistent all the way up to $\\lambda\\asymp\\log n$. These\nworks do not rely on any assumption on the underlying unknown distribution $p$\nof the population, and therefore, while providing a theory in its greatest\ngenerality, worst-case distributions may severely hamper the estimation of $U$\nin concrete applications. In this paper, we consider the problem of\nstrengthening the non-parametric framework for estimating $U$. Inspired by the\nestimation of rare probabilities in extreme value theory, and motivated by the\nubiquitous power-law type distributions in many natural and social phenomena,\nwe make use of a semi-parametric assumption regular variation of index\n$\\alpha\\in(0,1)$ for the tail behaviour of $p$. Under this assumption, we\nintroduce an estimator of $U$ that is simple, linear in the sampling\ninformation, computationally efficient, and scalable to massive datasets. Then,\nuniformly over our class of regularly varying tail distributions, we show that\nthe proposed estimator has provable guarantees: i) it is minimax near-optimal,\nup to a power of $\\log n$ factor; ii) it is consistent all of the way up to\n$\\log \\lambda\\asymp n^{\\alpha/2}/\\sqrt{\\log n}$, and this range is the best\npossible. This work presents the first study on the estimation of the unseen\nunder regularly varying tail distributions. A numerical illustration of our\nmethodology is presented for synthetic data and real data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:55:55 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 08:23:36 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Favaro", "Stefano", ""], ["Naulet", "Zacharie", ""]]}, {"id": "2104.03298", "submitter": "Changxiao Cai", "authors": "Gen Li, Changxiao Cai, Yuantao Gu, H. Vincent Poor, Yuxin Chen", "title": "Minimax Estimation of Linear Functions of Eigenvectors in the Face of\n  Small Eigen-Gaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Eigenvector perturbation analysis plays a vital role in various statistical\ndata science applications. A large body of prior works, however, focused on\nestablishing $\\ell_{2}$ eigenvector perturbation bounds, which are often highly\ninadequate in addressing tasks that rely on fine-grained behavior of an\neigenvector. This paper makes progress on this by studying the perturbation of\nlinear functions of an unknown eigenvector. Focusing on two fundamental\nproblems -- matrix denoising and principal component analysis -- in the\npresence of Gaussian noise, we develop a suite of statistical theory that\ncharacterizes the perturbation of arbitrary linear functions of an unknown\neigenvector. In order to mitigate a non-negligible bias issue inherent to the\nnatural \"plug-in\" estimator, we develop de-biased estimators that (1) achieve\nminimax lower bounds for a family of scenarios (modulo some logarithmic\nfactor), and (2) can be computed in a data-driven manner without sample\nsplitting. Noteworthily, the proposed estimators are nearly minimax optimal\neven when the associated eigen-gap is substantially smaller than what is\nrequired in prior theory.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:55:10 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Li", "Gen", ""], ["Cai", "Changxiao", ""], ["Gu", "Yuantao", ""], ["Poor", "H. Vincent", ""], ["Chen", "Yuxin", ""]]}, {"id": "2104.03397", "submitter": "Andrew McCormack", "authors": "Andrew McCormack and Peter Hoff", "title": "Equivariant Estimation of Fr\\'echet Means", "comments": "31 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fr\\'echet mean generalizes the concept of a mean to a metric space\nsetting. In this work we consider equivariant estimation of Fr\\'echet means for\nparametric models on metric spaces that are Riemannian manifolds. The geometry\nand symmetry of such a space is encoded by its isometry group. Estimators that\nare equivariant under the isometry group take into account the symmetry of the\nmetric space. For some models there exists an optimal equivariant estimator,\nwhich necessarily will perform as well or better than other common equivariant\nestimators, such as the maximum likelihood estimator or the sample Fr\\'echet\nmean. We derive the general form of this minimum risk equivariant estimator and\nin a few cases provide explicit expressions for it. In other models the\nisometry group is not large enough relative to the parametric family of\ndistributions for there to exist a minimum risk equivariant estimator. In such\ncases, we introduce an adaptive equivariant estimator that uses the data to\nselect a submodel for which there is an MRE. Simulations results show that the\nadaptive equivariant estimator performs favorably relative to alternative\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 21:06:06 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["McCormack", "Andrew", ""], ["Hoff", "Peter", ""]]}, {"id": "2104.03417", "submitter": "Yanqing Yin", "authors": "Yanqing Yin", "title": "Spectral statistics of high dimensional sample covariance matrix with\n  unbounded population spectral norm", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we establish some new central limit theorems for certain\nspectral statistics of a high-dimensional sample covariance matrix under a\ndivergent spectral norm population model. This model covers the divergent\nspiked population model as a special case. Meanwhile, the number of the spiked\neigenvalues can either be fixed or grow to infinity. It is seen from our\ntheorems that the divergence of population spectral norm affects the\nfluctuations of the linear spectral statistics in a fickle way, depending on\nthe divergence rate.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 22:19:30 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Yin", "Yanqing", ""]]}, {"id": "2104.03436", "submitter": "David Frazier", "authors": "David T. Frazier, Christopher Drovandi, and David J. Nott", "title": "Synthetic Likelihood in Misspecified Models: Consequences and\n  Corrections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse the behaviour of the synthetic likelihood (SL) method when the\nmodel generating the simulated data differs from the actual data generating\nprocess. One of the most common methods to obtain SL-based inferences is via\nthe Bayesian posterior distribution, with this method often referred to as\nBayesian synthetic likelihood (BSL). We demonstrate that when the model is\nmisspecified, the BSL posterior can be poorly behaved, placing significant\nposterior mass on values of the model parameters that do not represent the true\nfeatures observed in the data. Theoretical results demonstrate that in\nmisspecified models the BSL posterior can display a wide range of behaviours\ndepending on the level of model misspecification, including being\nasymptotically non-Gaussian. Our results suggest that a recently proposed\nrobust BSL approach can ameliorate this behavior and deliver reliable posterior\ninference under model misspecification. We document all theoretical results\nusing a simple running example.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 00:07:48 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Frazier", "David T.", ""], ["Drovandi", "Christopher", ""], ["Nott", "David J.", ""]]}, {"id": "2104.03464", "submitter": "Yufei Yi", "authors": "Yufei Yi, Matey Neykov", "title": "A New Perspective on Debiasing Linear Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an abstract procedure for debiasing constrained or\nregularized potentially high-dimensional linear models. It is elementary to\nshow that the proposed procedure can produce $\\frac{1}{\\sqrt{n}}$-confidence\nintervals for individual coordinates (or even bounded contrasts) in models with\nunknown covariance, provided that the covariance has bounded spectrum. While\nthe proof of the statistical guarantees of our procedure is simple, its\nimplementation requires more care due to the complexity of the optimization\nprograms we need to solve. We spend the bulk of this paper giving examples in\nwhich the proposed algorithm can be implemented in practice. One fairly general\nclass of instances which are amenable to applications of our procedure include\nconvex constrained least squares. We are able to translate the procedure to an\nabstract algorithm over this class of models, and we give concrete examples\nwhere efficient polynomial time methods for debiasing exist. Those include the\nconstrained version of LASSO, regression under monotone constraints, regression\nwith positive monotone constraints and non-negative least squares. In addition,\nwe show that our abstract procedure can be applied to efficiently debias SLOPE\nand square-root SLOPE, among other popular regularized procedures under certain\nassumptions. We provide thorough simulation results in support of our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 01:41:55 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Yi", "Yufei", ""], ["Neykov", "Matey", ""]]}, {"id": "2104.03529", "submitter": "Didong Li", "authors": "Didong Li, Wenpin Tang and Sudipto Banerjee", "title": "Fixed-Domain Inference for Gausian Processes with Mat\\'ern Covariogram\n  on Compact Riemannian Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Gaussian processes are widely employed as versatile modeling and predictive\ntools in spatial statistics, functional data analysis, computer modeling and in\ndiverse applications of machine learning. Such processes have been widely\nstudied over Euclidean spaces, where they are constructed using specified\ncovariance functions or covariograms. These functions specify valid stochastic\nprocesses that can be used to model complex dependencies in spatial statistics\nand other machine learning contexts. Valid (positive definite) covariance\nfunctions have been extensively studied for Gaussian processes on Euclidean\nspaces. Such investigations have focused, among other aspects, on the\nidentifiability and consistency of covariance parameters as well as the problem\nof spatial interpolation and prediction within the fixed-domain or infill\nparadigm of asymptotic inference. This manuscript undertakes analogous\ntheoretical developments for Gaussian processes constructed over Riemannian\nmanifolds. We begin by establishing formal notions and conditions for the\nequivalence of two Gaussian random measures on compact manifolds. We build upon\nrecently introduced Mat\\'ern covariograms on compact Riemannian manifold,\nderive the microergodic parameter and formally establish the consistency of\nmaximum likelihood estimators and the asymptotic optimality of the best linear\nunbiased predictor (BLUP). The circle and sphere are studied as two specific\nexamples of compact Riemannian manifolds with numerical experiments that\nillustrate the theory.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:13:32 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Didong", ""], ["Tang", "Wenpin", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "2104.03716", "submitter": "Fatemeh Gharari", "authors": "Fatemeh Gharari and Masoud Ganji", "title": "A new stochastic order based on discrete Laplace transform and some\n  ordering results of the order statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper aims to study a new stochastic order based upon discrete Laplace\ntransforms. By this order, in a setup where the sample size is random, having\ndiscrete delta and nabla distributions, we obtain some ordering results\ninvolving ordinary and fractional order statistics. Some applications in\nfrailty models and reliability are presented as well.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:09:31 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Gharari", "Fatemeh", ""], ["Ganji", "Masoud", ""]]}, {"id": "2104.03966", "submitter": "Anne Sabourin", "authors": "St\\'ephan Cl\\'emen\\c{c}on and Hamid Jalalzai and Anne Sabourin and\n  Johan Segers", "title": "Concentration bounds for the empirical angular measure with statistical\n  learning applications", "comments": "30 pages (main paper), 15 pages (supplement), 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The angular measure on the unit sphere characterizes the first-order\ndependence structure of the components of a random vector in extreme regions\nand is defined in terms of standardized margins. Its statistical recovery is an\nimportant step in learning problems involving observations far away from the\ncenter. In the common situation when the components of the vector have\ndifferent distributions, the rank transformation offers a convenient and robust\nway of standardizing data in order to build an empirical version of the angular\nmeasure based on the most extreme observations. However, the study of the\nsampling distribution of the resulting empirical angular measure is\nchallenging. It is the purpose of the paper to establish finite-sample bounds\nfor the maximal deviations between the empirical and true angular measures,\nuniformly over classes of Borel sets of controlled combinatorial complexity.\nThe bounds are valid with high probability and scale essentially as the square\nroot of the effective sample size, up to a logarithmic factor. Discarding the\nmost extreme observations yields a truncated version of the empirical angular\nmeasure for which the logarithmic factor in the concentration bound is replaced\nby a factor depending on the truncation level. The bounds are applied to\nprovide performance guarantees for two statistical learning procedures tailored\nto extreme regions of the input space and built upon the empirical angular\nmeasure: binary classification in extreme regions through empirical risk\nminimization and unsupervised anomaly detection through minimum-volume sets of\nthe sphere.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 18:41:29 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Cl\u00e9men\u00e7on", "St\u00e9phan", ""], ["Jalalzai", "Hamid", ""], ["Sabourin", "Anne", ""], ["Segers", "Johan", ""]]}, {"id": "2104.04047", "submitter": "Mingao Yuan", "authors": "Mingao Yuan and Zuofeng Shang", "title": "Heterogeneous Dense Subhypergraph Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of testing the existence of a heterogeneous dense\nsubhypergraph. The null hypothesis corresponds to a heterogeneous\nErd\\\"{o}s-R\\'{e}nyi uniform random hypergraph and the alternative hypothesis\ncorresponds to a heterogeneous uniform random hypergraph that contains a dense\nsubhypergraph. We establish detection boundaries when the edge probabilities\nare known and construct an asymptotically powerful test for distinguishing the\nhypotheses. We also construct an adaptive test which does not involve edge\nprobabilities, and hence, is more practically useful.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:44:22 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Yuan", "Mingao", ""], ["Shang", "Zuofeng", ""]]}, {"id": "2104.04176", "submitter": "Jose Julian Pavon-Espa\\~nol", "authors": "F. Delgado-Vences and J.J. Pavon-Espa\\~nol", "title": "Statistical inference for a stochastic wave equation with Malliavin\n  calculus", "comments": "32 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1904.10884", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study asymptotic properties of the maximum likelihood\nestimator (MLE) for the speed of a stochastic wave equation. We follow a\nwell-known spectral approach to write the solution as a Fourier series, then we\nproject the solution to a $N$-finite dimensional space and find the estimator\nas a function of the time and $N$. We then show consistency of the MLE using\nclassical stochastic analysis. Afterward we prove the asymptotic normality\nusing the Malliavin-Stein method. We also study asymptotic properties of a\ndiscretized version of the MLE for the parameter. We provide this asymptotic\nanalysis of the proposed estimator as the number of Fourier modes, $N$, used in\nthe estimation and the observation time go to infinity. Finally, we illustrate\nthe theoretical results with some numerical experiments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 03:24:09 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Delgado-Vences", "F.", ""], ["Pavon-Espa\u00f1ol", "J. J.", ""]]}, {"id": "2104.04186", "submitter": "Liyan Xie", "authors": "Liyan Xie, Shaofeng Zou, Yao Xie, Venugopal V. Veeravalli", "title": "Sequential (Quickest) Change Detection: Classical Results and New\n  Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online detection of changes in stochastic systems, referred to as sequential\nchange detection or quickest change detection, is an important research topic\nin statistics, signal processing, and information theory, and has a wide range\nof applications. This survey starts with the basics of sequential change\ndetection, and then moves on to generalizations and extensions of sequential\nchange detection theory and methods. We also discuss some new dimensions that\nemerge at the intersection of sequential change detection with other areas,\nalong with a selection of modern applications and remarks on open questions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 04:35:16 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Xie", "Liyan", ""], ["Zou", "Shaofeng", ""], ["Xie", "Yao", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "2104.04244", "submitter": "Konstantin Donhauser", "authors": "Konstantin Donhauser, Mingqi Wu and Fanny Yang", "title": "How rotational invariance of common kernels prevents generalization in\n  high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel ridge regression is well-known to achieve minimax optimal rates in\nlow-dimensional settings. However, its behavior in high dimensions is much less\nunderstood. Recent work establishes consistency for kernel regression under\ncertain assumptions on the ground truth function and the distribution of the\ninput data. In this paper, we show that the rotational invariance property of\ncommonly studied kernels (such as RBF, inner product kernels and\nfully-connected NTK of any depth) induces a bias towards low-degree polynomials\nin high dimensions. Our result implies a lower bound on the generalization\nerror for a wide range of distributions and various choices of the scaling for\nkernels with different eigenvalue decays. This lower bound suggests that\ngeneral consistency results for kernel ridge regression in high dimensions\nrequire a more refined analysis that depends on the structure of the kernel\nbeyond its eigenvalue decay.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 08:27:37 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Donhauser", "Konstantin", ""], ["Wu", "Mingqi", ""], ["Yang", "Fanny", ""]]}, {"id": "2104.04335", "submitter": "Topi Halme", "authors": "Topi Halme, Eyal Nitzan and Visa Koivunen", "title": "Bayesian Method for Spatial Change-Point Detection of Propagating Event", "comments": "13 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid detection of spatial events that propagate across a sensor network is\nof wide interest in many modern applications. In particular, in communications,\nradar, environmental monitoring, and biosurveillance, we may observe\npropagating fields or particles. In this paper, we propose Bayesian single and\nmultiple change-point detection procedures for the rapid detection of\npropagating spatial events. It is assumed that the spatial event propagates\nacross a network of sensors according to the physical properties of the source\ncausing the event. The multisensor system configuration is arbitrary and\nsensors may be mobile. We begin by considering a single spatial event and are\ninterested in detecting this event as quickly as possible, while statistically\ncontrolling the probability of false alarm. Using a dynamic programming\nframework we derive the structure of the optimal procedure, which minimizes the\naverage detection delay (ADD) subject to a false alarm probability upper bound.\nIn the rare event regime, the optimal procedure converges to a more practical\nthreshold test on the posterior probability of the change point. A convenient\nrecursive computation of this posterior probability is derived by using the\npropagation pattern of the spatial event. The ADD of the posterior probability\nthreshold test is analyzed in the asymptotic regime. Then, we take a multiple\nhypothesis testing (MHT) approach and develop a procedure for the detection of\nmultiple propagating spatial events in parallel. The proposed parallel\nprocedure controls the overall false discovery rate (FDR) under prespecified\nupper bound. Simulations are conducted to verify the theoretical findings. It\nis shown that exploiting the spatial properties of the event improves the ADD\ncompared to procedures that do not properly take advantage of the spatial\ninformation.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 12:41:34 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Halme", "Topi", ""], ["Nitzan", "Eyal", ""], ["Koivunen", "Visa", ""]]}, {"id": "2104.04416", "submitter": "Timoth\\'ee Mathieu", "authors": "Timoth\\'ee Mathieu", "title": "Concentration study of M-estimators using the influence function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new finite-sample analysis of M-estimators of locations in\n$\\mathbb{R}^d$ using the tool of the influence function. In particular, we show\nthat the deviations of an M-estimator can be controlled thanks to its influence\nfunction (or its score function) and then, we use concentration inequality on\nM-estimators to investigate the robust estimation of the mean in high dimension\nin a corrupted setting (adversarial corruption setting) for bounded and\nunbounded score functions. For a sample of size $n$ and covariance matrix\n$\\Sigma$, we attain the minimax speed\n$\\sqrt{Tr(\\Sigma)/n}+\\sqrt{\\|\\Sigma\\|_{op}\\log(1/\\delta)/n}$ with probability\nlarger than $1-\\delta$ in a heavy-tailed setting. One of the major advantages\nof our approach compared to others recently proposed is that our estimator is\ntractable and fast to compute even in very high dimension with a complexity of\n$O(nd\\log(Tr(\\Sigma)))$ where $n$ is the sample size and $\\Sigma$ is the\ncovariance matrix of the inliers. In practice, the code that we make available\nfor this article proves to be very fast.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:11:02 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Mathieu", "Timoth\u00e9e", ""]]}, {"id": "2104.04472", "submitter": "Hamdi Raissi", "authors": "Valentin Patilea and Hamdi Ra\\\"issi", "title": "Powers correlation analysis of non-stationary illiquid assets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the higher order dynamics of individual illiquid stocks are\ninvestigated. We show that considering the classical powers correlation could\nlead to a spurious assessment of the volatility persistency or long memory\nvolatility effects, if the zero returns probability is non-constant over time.\nIn other words, the classical tools are not able to distinguish between\nlong-run volatility effects, such as IGARCH, and the case where the zero\nreturns are not evenly distributed over time. As a consequence, tools that are\nrobust to changes in the degree of illiquidity are proposed. Since a\ntime-varying zero returns probability could potentially be accompanied by a\nnon-constant unconditional variance, we then develop powers correlations that\nare also robust in such a case. In addition, note that the tools proposed in\nthe paper offer a rigorous analysis of the short-run volatility effects, while\nthe use of the classical power correlations lead to doubtful conclusions. The\nMonte Carlo experiments, and the study of the absolute value correlation of\nsome stocks taken from the Chilean financial market, suggest that the\nvolatility effects are only short-run in many cases.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:41:53 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Patilea", "Valentin", ""], ["Ra\u00efssi", "Hamdi", ""]]}, {"id": "2104.04647", "submitter": "Peng Ding", "authors": "Fangzhou Su and Peng Ding", "title": "Model-assisted analyses of cluster-randomized experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cluster-randomized experiments are widely used due to their logistical\nconvenience and policy relevance. To analyze them properly, we must address the\nfact that the treatment is assigned at the cluster level instead of the\nindividual level. Standard analytic strategies are regressions based on\nindividual data, cluster averages, and cluster totals, which differ when the\ncluster sizes vary. These methods are often motivated by models with strong and\nunverifiable assumptions, and the choice among them can be subjective. Without\nany outcome modeling assumption, we evaluate these regression estimators and\nthe associated robust standard errors from a design-based perspective where\nonly the treatment assignment itself is random and controlled by the\nexperimenter. We demonstrate that regression based on cluster averages targets\na weighted average treatment effect, regression based on individual data is\nsuboptimal in terms of efficiency, and regression based on cluster totals is\nconsistent and more efficient with a large number of clusters. We highlight the\ncritical role of covariates in improving estimation efficiency, and illustrate\nthe efficiency gain via both simulation studies and data analysis. Moreover, we\nshow that the robust standard errors are convenient approximations to the true\nasymptotic standard errors under the design-based perspective. Our theory holds\neven when the outcome models are misspecified, so it is model-assisted rather\nthan model-based. We also extend the theory to a wider class of weighted\naverage treatment effects.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 23:58:42 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Su", "Fangzhou", ""], ["Ding", "Peng", ""]]}, {"id": "2104.04660", "submitter": "Nour Hawila", "authors": "Nour Hawila and Arthur Berg", "title": "Exact-corrected confidence interval for risk difference in\n  noninferiority binomial trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel confidence interval estimator is proposed for the risk difference in\nnoninferiority binomial trials. The confidence interval is consistent with an\nexact unconditional test that preserves the type-1 error, and has improved\npower, particularly for smaller sample sizes, compared to the confidence\ninterval by Chan & Zhang (1999). The improved performance of the proposed\nconfidence interval is theoretically justified and demonstrated with\nsimulations and examples. An R package is also distributed that implements the\nproposed methods along with other confidence interval estimators.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 01:16:39 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 16:16:25 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Hawila", "Nour", ""], ["Berg", "Arthur", ""]]}, {"id": "2104.04716", "submitter": "Jesper Riis-Vestergaard S{\\o}rensen", "authors": "Denis Chetverikov and Jesper Riis-Vestergaard S{\\o}rensen", "title": "Analytic and Bootstrap-after-Cross-Validation Methods for Selecting\n  Penalty Parameters of High-Dimensional M-Estimators", "comments": "63 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop two new methods for selecting the penalty parameter for the\n$\\ell^1$-penalized high-dimensional M-estimator, which we refer to as the\nanalytic and bootstrap-after-cross-validation methods. For both methods, we\nderive nonasymptotic error bounds for the corresponding $\\ell^1$-penalized\nM-estimator and show that the bounds converge to zero under mild conditions,\nthus providing a theoretical justification for these methods. We demonstrate\nvia simulations that the finite-sample performance of our methods is much\nbetter than that of previously available and theoretically justified methods.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 09:05:41 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 11:18:59 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Chetverikov", "Denis", ""], ["S\u00f8rensen", "Jesper Riis-Vestergaard", ""]]}, {"id": "2104.04734", "submitter": "Xiaozhuo Zhang", "authors": "Xiaozhuo Zhang, Zhiqiang Hou, Zhidong Bai and Jiang Hu", "title": "Spiked eigenvalues of noncentral Fisher matrix with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we investigate the asymptotic behavior of spiked eigenvalues\nof the noncentral Fisher matrix defined by ${\\mathbf F}_p={\\mathbf C}_n(\\mathbf\nS_N)^{-1}$, where ${\\mathbf C}_n$ is a noncentral sample covariance matrix\ndefined by $(\\mathbf \\Xi+\\mathbf X)(\\mathbf \\Xi+\\mathbf X)^*/n$ and $\\mathbf\nS_N={\\mathbf Y}{\\mathbf Y}^*/N$. The matrices $\\mathbf X$ and $\\mathbf Y$ are\ntwo independent {Gaussian} arrays, with respective $p\\times n$ and $p\\times N$\nand the Gaussian entries of them are \\textit {independent and identically\ndistributed} (i.i.d.) with mean $0$ and variance $1$. When $p$, $n$, and $N$\ngrow to infinity proportionally, we establish a phase transition of the spiked\neigenvalues of $\\mathbf F_p$. Furthermore, we derive the \\textit{central\nlimiting theorem} (CLT) for the spiked eigenvalues of $\\mathbf F_p$. As an\naccessory to the proof of the above results, the fluctuations of the spiked\neigenvalues of ${\\mathbf C}_n$ are studied, which should have its own\ninterests. Besides, we develop the limits and CLT for the sample canonical\ncorrelation coefficients by the results of the spiked noncentral Fisher matrix\nand give three consistent estimators, including the population spiked\neigenvalues and the population canonical correlation coefficients.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 10:54:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhang", "Xiaozhuo", ""], ["Hou", "Zhiqiang", ""], ["Bai", "Zhidong", ""], ["Hu", "Jiang", ""]]}, {"id": "2104.04863", "submitter": "L\\'aszl\\'o Viharos", "authors": "Lillian Achola Oluoch, L\\'aszl\\'o Viharos", "title": "Asymptotic distributions for weighted power sums of extreme values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_{1,n}\\le\\cdots\\le X_{n,n}$ be the order statistics of $n$ independent\nrandom variables with a common distribution function $F$ having right heavy\ntail with tail index $\\gamma$. Given known constants $d_{i,n}$, $1\\le i\\le n$,\nconsider the weighted power sums\n$\\sum^{k_n}_{i=1}d_{n+1-i,n}\\log^pX_{n+1-i,n}$, where $p>0$ and the $k_n$ are\npositive integers such that $k_n\\to\\infty$ and $k_n/n\\to0$ as $n\\to\\infty$.\nUnder some constraints on the weights $d_{i,n}$, we prove asymptotic normality\nfor the power sums over the whole heavy-tail model. We apply the obtained\nresult to construct a new class of estimators for the parameter $\\gamma$.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 21:04:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Oluoch", "Lillian Achola", ""], ["Viharos", "L\u00e1szl\u00f3", ""]]}, {"id": "2104.04882", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Fr\\'ed\\'eric Ouimet", "title": "A symmetric matrix-variate normal local approximation for the Wishart\n  distribution and some applications", "comments": "12 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The noncentral Wishart distribution has become more mainstream in statistics\nas the prevalence of applications involving sample covariances with underlying\nmultivariate Gaussian populations as dramatically increased since the advent of\ncomputers. Multiple sources in the literature deal with local approximations of\nthe noncentral Wishart distribution with respect to its central counterpart.\nHowever, no source has yet has developed explicit local approximations for the\n(central) Wishart distribution in terms of a normal analogue, which is\nimportant since Gaussian distributions are at the heart of the asymptotic\ntheory for many statistical methods. In this note, we prove a precise\nasymptotic expansion for the ratio of the Wishart density to the symmetric\nmatrix-variate normal density with the same mean and covariances. The result is\nthen used to derive an upper bound on the total variation between the\ncorresponding probability measures and to find the asymptotic variance of a new\ndensity estimator on the space of positive definite matrices with a Wishart\nasymmetric kernel.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 00:13:38 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 09:12:56 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2104.04910", "submitter": "Yifan Li", "authors": "Yifan Li, Reg Kulperger and Hao Yu", "title": "Semi-$G$-normal: a Hybrid between Normal and $G$-normal", "comments": "102 pages, 8 figures, a comprehensive document for conference\n  discussions, to be divided later for publications, readers may navigate to\n  the parts they are interested in by the table of contents", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The $G$-expectation framework is a generalization of the classical\nprobabilistic system motivated by Knightian uncertainty, where the $G$-normal\nplays a central role. However, from a statistical perspective, $G$-normal\ndistributions look quite different from the classical normal ones. For\ninstance, its uncertainty is characterized by a set of distributions which\ncovers not only classical normal with different variances, but additional\ndistributions typically having non-zero skewness. The $G$-moments of\n$G$-normals are defined by a class of fully nonlinear PDEs called $G$-heat\nequations. To understand $G$-normal in a probabilistic and stochastic way that\nis more friendly to statisticians and practitioners, we introduce a\nsubstructure called semi-$G$-normal, which behaves like a hybrid between normal\nand $G$-normal: it has variance uncertainty but zero-skewness. We will show\nthat the non-zero skewness arises when we impose the $G$-version sequential\nindependence on the semi-$G$-normal. More importantly, we provide a series of\nrepresentations of random vectors with semi-$G$-normal marginals under various\ntypes of independence. Each of these representations under a typical order of\nindependence is closely related to a class of state-space volatility models\nwith a common graphical structure. In short, semi-$G$-normal gives a\n(conceptual) transition from classical normal to $G$-normal, allowing us a\nbetter understanding of the distributional uncertainty of $G$-normal and the\nsequential independence.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 04:04:17 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 10:33:54 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Li", "Yifan", ""], ["Kulperger", "Reg", ""], ["Yu", "Hao", ""]]}, {"id": "2104.05021", "submitter": "Soham Sarkar", "authors": "Soham Sarkar and Victor M. Panaretos", "title": "CovNet: Covariance Networks for Functional Data on Multidimensional\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Covariance estimation is ubiquitous in functional data analysis. Yet, the\ncase of functional observations over multidimensional domains introduces\ncomputational and statistical challenges, rendering the standard methods\neffectively inapplicable. To address this problem, we introduce Covariance\nNetworks (CovNet) as a modeling and estimation tool. The CovNet model is\nuniversal -- it can be used to approximate any covariance up to desired\nprecision. Moreover, the model can be fitted efficiently to the data and its\nneural network architecture allows us to employ modern computational tools in\nthe implementation. The CovNet model also admits a closed-form\neigen-decomposition, which can be computed efficiently, without constructing\nthe covariance itself. This facilitates easy storage and subsequent\nmanipulation in the context of the CovNet. Moreover, we establish consistency\nof the proposed estimator and derive its rate of convergence. The usefulness of\nthe proposed method is demonstrated by means of an extensive simulation study.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 14:40:41 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sarkar", "Soham", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2104.05087", "submitter": "Orestis Plevrakis", "authors": "Orestis Plevrakis", "title": "Learning from Censored and Dependent Data: The case of Linear Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Observations from dynamical systems often exhibit irregularities, such as\ncensoring, where values are recorded only if they fall within a certain range.\nCensoring is ubiquitous in practice, due to saturating sensors,\nlimit-of-detection effects, and image-frame effects. In light of recent\ndevelopments on learning linear dynamical systems (LDSs), and on censored\nstatistics with independent data, we revisit the decades-old problem of\nlearning an LDS, from censored observations (Lee and Maddala (1985); Zeger and\nBrookmeyer (1986)). Here, the learner observes the state $x_t \\in \\mathbb{R}^d$\nif and only if $x_t$ belongs to some set $S_t \\subseteq \\mathbb{R}^d$. We\ndevelop the first computationally and statistically efficient algorithm for\nlearning the system, assuming only oracle access to the sets $S_t$. Our\nalgorithm, Stochastic Online Newton with Switching Gradients, is a novel\nsecond-order method that builds on the Online Newton Step (ONS) of Hazan et al.\n(2007). Our Switching-Gradient scheme does not always use (stochastic)\ngradients of the function we want to optimize, which we call \"censor-aware\"\nfunction. Instead, in each iteration, it performs a simple test to decide\nwhether to use the censor-aware, or another \"censor-oblivious\" function, for\ngetting a stochastic gradient.\n  In our analysis, we consider a \"generic\" Online Newton method, which uses\narbitrary vectors instead of gradients, and we prove an error-bound for it.\nThis can be used to appropriately design these vectors, leading to our\nSwitching-Gradient scheme. This framework significantly deviates from the\nrecent long line of works on censored statistics (e.g., Daskalakis et al.\n(2018); Kontonis et al. (2019); Daskalakis et al. (2019)), which apply\nStochastic Gradient Descent (SGD), and their analysis reduces to establishing\nconditions for off-the-shelf SGD-bounds.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 19:55:24 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Plevrakis", "Orestis", ""]]}, {"id": "2104.05441", "submitter": "Marcus Kaiser", "authors": "Marcus Kaiser, Maksim Sipos", "title": "Unsuitability of NOTEARS for Causal Graph Discovery", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal Discovery methods aim to identify a DAG structure that represents\ncausal relationships from observational data. In this article, we stress that\nit is important to test such methods for robustness in practical settings. As\nour main example, we analyze the NOTEARS method, for which we demonstrate a\nlack of scale-invariance. We show that NOTEARS is a method that aims to\nidentify a parsimonious DAG from the data that explains the residual variance.\nWe conclude that NOTEARS is not suitable for identifying truly causal\nrelationships from the data.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 13:09:10 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 10:38:53 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kaiser", "Marcus", ""], ["Sipos", "Maksim", ""]]}, {"id": "2104.05886", "submitter": "Zuheng Xu", "authors": "Zuheng Xu, Trevor Campbell", "title": "The computational asymptotics of Gaussian variational inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a popular alternative to Markov chain Monte Carlo\nmethods that constructs a Bayesian posterior approximation by minimizing a\ndiscrepancy to the true posterior within a pre-specified family. This converts\nBayesian inference into an optimization problem, enabling the use of simple and\nscalable stochastic optimization algorithms. However, a key limitation of\nvariational inference is that the optimal approximation is typically not\ntractable to compute; even in simple settings the problem is nonconvex. Thus,\nrecently developed statistical guarantees -- which all involve the (data)\nasymptotic properties of the optimal variational distribution -- are not\nreliably obtained in practice. In this work, we provide two major\ncontributions: a theoretical analysis of the asymptotic convexity properties of\nvariational inference in the popular setting with a Gaussian family; and\nconsistent stochastic variational inference (CSVI), an algorithm that exploits\nthese properties to find the optimal approximation in the asymptotic regime.\nCSVI consists of a tractable initialization procedure that finds the local\nbasin of the optimal solution, and a scaled gradient descent algorithm that\nstays locally confined to that basin. Experiments on nonconvex synthetic and\nreal-data examples show that compared with standard stochastic gradient\ndescent, CSVI improves the likelihood of obtaining the globally optimal\nposterior approximation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 01:23:34 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Xu", "Zuheng", ""], ["Campbell", "Trevor", ""]]}, {"id": "2104.06110", "submitter": "Kazuki Okamura", "authors": "Yuichi Akaoka, Kazuki Okamura, Yoshiki Otobe", "title": "Limit theorems for quasi-arithmetic means of random variables with\n  applications to point estimations for the Cauchy distribution", "comments": "23 pages, reference added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We establish some limit theorems for quasi-arithmetic means of random\nvariables. This class of means contains the arithmetic, geometric and harmonic\nmeans. Our feature is that the generators of quasi-arithmetic means are allowed\nto be complex-valued, which makes considerations for quasi-arithmetic means of\nrandom variables which could take negative values possible. Our motivation for\nthe limit theorems is finding simple estimators of the parameters of the Cauchy\ndistribution. By applying the limit theorems, we obtain some closed-form\nunbiased strongly-consistent estimators for the joint of the location and scale\nparameters of the Cauchy distribution, which are easy to compute and analyze.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 11:30:53 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 14:17:56 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Akaoka", "Yuichi", ""], ["Okamura", "Kazuki", ""], ["Otobe", "Yoshiki", ""]]}, {"id": "2104.06112", "submitter": "Kazuki Okamura", "authors": "Yuichi Akaoka, Kazuki Okamura, Yoshiki Otobe", "title": "Bahadur efficiency of the maximum likelihood estimator and one-step\n  estimator for quasi-arithmetic means of the Cauchy distribution", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Some quasi-arithmetic means of random variables easily give unbiased strongly\nconsistent closed-form estimators of the joint of the location and scale\nparameters of the Cauchy distribution. The one-step estimators of those\nquasi-arithmetic means of the Cauchy distribution are considered. We establish\nthe Bahadur efficiency of the maximum likelihood estimator and the one-step\nestimators. We also show that the rate of the convergence of the mean-squared\nerrors achieves the Cramer-Rao bound. Our results are also applicable to the\ncircular Cauchy distribution.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 11:38:28 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Akaoka", "Yuichi", ""], ["Okamura", "Kazuki", ""], ["Otobe", "Yoshiki", ""]]}, {"id": "2104.06124", "submitter": "Kazuki Okamura", "authors": "Yuichi Akaoka, Kazuki Okamura, Yoshiki Otobe", "title": "Confidence disc for Cauchy distributions", "comments": "14 pages, 6 figures, Subsection 5.6 revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We will construct a confidence region of parameters for $N$ samples from\nCauchy distributed random variables. Although Cauchy distribution has two\nparameters, a location parameter $\\mu \\in \\mathbb{R}$ and a scale parameter\n$\\sigma > 0$, we will infer them at once by regarding them as a single complex\nparameter $\\gamma := \\mu + i \\sigma$. Therefore the region should be a domain\nin the complex plane and we will give a simple and concrete formula to give the\nregion as a disc.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 11:55:49 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 10:20:27 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Akaoka", "Yuichi", ""], ["Okamura", "Kazuki", ""], ["Otobe", "Yoshiki", ""]]}, {"id": "2104.06130", "submitter": "Kazuki Okamura", "authors": "Kazuki Okamura, Yoshiki Otobe", "title": "Characterizations of the maximum likelihood estimator of the Cauchy\n  distribution", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider characterizations of the maximal likelihood estimator (MLE) of\nsamples from the Cauchy distribution. We characterize the MLE as an attractive\nfixed point of a holomorphic map on the upper-half plane. We show that the\niteration of the holomorphic function starting at every point in the upper-half\nplane converges to the MLE exponentially fast. We can also characterize the MLE\nas a unique root in the upper-half plane of a certain univariate polynomial\nover $\\mathbb R$. By this polynomial, we can derive the closed-form formulae\nfor samples of size three and four, and furthermore show that for samples of\nsize five, there is no algebraic closed-form formula.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 12:05:59 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Okamura", "Kazuki", ""], ["Otobe", "Yoshiki", ""]]}, {"id": "2104.06355", "submitter": "Marat Burnashev V.", "authors": "M. V. Burnashev", "title": "On Minimax Detection of Gaussian Stochastic Sequences and Gaussian\n  Stationary Signals", "comments": "Preliminary version. Problems of Information Transmission, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Minimax detection of Gaussian stochastic sequences (signals) with unknown\ncovariance matrices is studied. For a fixed false alarm probability (1-st kind\nerror probability), the performance of the minimax detection is being\ncharacterized by the best exponential decay rate of the miss probability (2-nd\nkind error probability) as the length of the observation interval tends to\ninfinity. Our goal is to find the largest set of covariance matrices such that\nthe minimax robust testing of this set (composite hypothesis) can be replaced\nwith testing of only one specific covariance matrix (simple hypothesis) without\nany loss in detection characteristics. In this paper, we completely describe\nthis maximal set of covariance matrices. Some corollaries address minimax\ndetection of the Gaussian stochastic signals embedded in the White Gaussian\nnoise and detection of the Gaussian stationary signals.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:02:49 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Burnashev", "M. V.", ""]]}, {"id": "2104.06667", "submitter": "Abhishek Chakrabortty", "authors": "Yuqian Zhang, Abhishek Chakrabortty and Jelena Bradic", "title": "Double Robust Semi-Supervised Inference for the Mean: Selection Bias\n  under MAR Labeling with Decaying Overlap", "comments": "37 pages; (Supplement: 43 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised (SS) inference has received much attention in recent years.\nApart from a moderate-sized labeled data, L, the SS setting is characterized by\nan additional, much larger sized, unlabeled data, U. The setting of |U| >> |L|,\nmakes SS inference unique and different from the standard missing data\nproblems, owing to natural violation of the so-called 'positivity' or 'overlap'\nassumption. However, most of the SS literature implicitly assumes L and U to be\nequally distributed, i.e., no selection bias in the labeling. Inferential\nchallenges in missing at random (MAR) type labeling allowing for selection\nbias, are inevitably exacerbated by the decaying nature of the propensity score\n(PS). We address this gap for a prototype problem, the estimation of the\nresponse's mean. We propose a double robust SS (DRSS) mean estimator and give a\ncomplete characterization of its asymptotic properties. The proposed estimator\nis consistent as long as either the outcome or the PS model is correctly\nspecified. When both models are correctly specified, we provide inference\nresults with a non-standard consistency rate that depends on the smaller size\n|L|. The results are also extended to causal inference with imbalanced\ntreatment groups. Further, we provide several novel choices of models and\nestimators of the decaying PS, including a novel offset logistic model and a\nstratified labeling model. We present their properties under both high and low\ndimensional settings. These may be of independent interest. Lastly, we present\nextensive simulations and also a real data application.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 07:27:27 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhang", "Yuqian", ""], ["Chakrabortty", "Abhishek", ""], ["Bradic", "Jelena", ""]]}, {"id": "2104.06708", "submitter": "Jian Huang", "authors": "Yuling Jiao, Guohao Shen, Yuanyuan Lin and Jian Huang", "title": "Deep Nonparametric Regression on Approximately Low-dimensional Manifolds", "comments": "Yuling Jiao and Guohao Shen contributed equally to this work.\n  Co-corresponding authors: Yuanyuan Lin (Email: ylin@sta.cuhk.edu.hk) and Jian\n  Huang (Email: jian-huang@uiowa.edu)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the properties of nonparametric least squares\nregression using deep neural networks. We derive non-asymptotic upper bounds\nfor the prediction error of the empirical risk minimizer for feedforward deep\nneural regression. Our error bounds achieve the minimax optimal rate and\nsignificantly improve over the existing ones in the sense that they depend\nlinearly or quadratically on the dimension d of the predictor, instead of\nexponentially on d. We show that the neural regression estimator can circumvent\nthe curse of dimensionality under the assumption that the predictor is\nsupported on an approximate low-dimensional manifold. This assumption differs\nfrom the structural condition imposed on the target regression function and is\nweaker and more realistic than the exact low-dimensional manifold support\nassumption in the existing literature. We investigate how the prediction error\nof the neural regression estimator depends on the structure of neural networks\nand propose a notion of network relative efficiency between two types of neural\nnetworks, which provides a quantitative measure for evaluating the relative\nmerits of different network structures. Our results are derived under weaker\nassumptions on the data distribution, the target regression function and the\nneural network structure than those in the existing literature.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:08:30 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 23:54:11 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Jiao", "Yuling", ""], ["Shen", "Guohao", ""], ["Lin", "Yuanyuan", ""], ["Huang", "Jian", ""]]}, {"id": "2104.06839", "submitter": "Fabio Cuzzolin", "authors": "Fabio Cuzzolin", "title": "Uncertainty measures: The big picture", "comments": "18 pages, 1 table, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Probability theory is far from being the most general mathematical theory of\nuncertainty. A number of arguments point at its inability to describe\nsecond-order ('Knightian') uncertainty. In response, a wide array of theories\nof uncertainty have been proposed, many of them generalisations of classical\nprobability. As we show here, such frameworks can be organised into clusters\nsharing a common rationale, exhibit complex links, and are characterised by\ndifferent levels of generality. Our goal is a critical appraisal of the current\nlandscape in uncertainty theory.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 13:11:20 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Cuzzolin", "Fabio", ""]]}, {"id": "2104.06911", "submitter": "Zijian Guo", "authors": "Zijian Guo", "title": "Post-selection Problems for Causal Inference with Invalid Instruments: A\n  Solution Using Searching and Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variable method is among the most commonly used causal inference\napproaches for analyzing observational studies with unmeasured confounders.\nDespite its popularity, the instruments' invalidity is a major concern for\npractical applications and a fast-growing area of research is inference for the\ncausal effect with possibly invalid instruments. In this paper, we construct\nuniformly valid confidence intervals for the causal effect when the instruments\nare possibly invalid. We illustrate the post-selection problem of existing\ninference methods relying on instrument selection. Our proposal is to search\nfor the value of the treatment effect such that a sufficient amount of\ncandidate instruments are taken as valid. We further devise a novel sampling\nmethod, which, together with searching, lead to a more precise confidence\ninterval. Our proposed searching and sampling confidence intervals are shown to\nbe uniformly valid under the finite-sample majority and plurality rules. We\ncompare our proposed methods with existing inference methods over a large set\nof simulation studies and apply them to study the effect of the triglyceride\nlevel on the glucose level over a mouse data set.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 15:03:22 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Guo", "Zijian", ""]]}, {"id": "2104.07028", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "On Missing Mass Variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The missing mass refers to the probability of elements not observed in a\nsample, and since the work of Good and Turing during WWII, has been studied\nextensively in many areas including ecology, linguistic, networks and\ninformation theory.\n  This work determines what is the \\emph{maximal variance of the missing mass},\nfor any sample and alphabet sizes. The result helps in understanding the\nmissing mass concentration properties.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:20:15 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2104.07317", "submitter": "Pengkun Yang", "authors": "Yihong Wu, Pengkun Yang", "title": "Polynomial methods in statistical inference: theory and practice", "comments": "Foundations and Trends in Communications and Information Theory: Vol.\n  17: No. 4, pp 402-586, 2020. ISBN to printed book: 978-1-68083-730-8. arXiv\n  admin note: text overlap with arXiv:1807.07237", "journal-ref": null, "doi": "10.1561/0100000095", "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This survey provides an exposition of a suite of techniques based on the\ntheory of polynomials, collectively referred to as polynomial methods, which\nhave recently been applied to address several challenging problems in\nstatistical inference successfully. Topics including polynomial approximation,\npolynomial interpolation and majorization, moment space and positive\npolynomials, orthogonal polynomials and Gaussian quadrature are discussed, with\ntheir major probabilistic and statistical applications in property estimation\non large domains and learning mixture models. These techniques provide useful\ntools not only for the design of highly practical algorithms with provable\noptimality, but also for establishing the fundamental limits of the inference\nproblems through the method of moment matching. The effectiveness of the\npolynomial method is demonstrated in concrete problems such as entropy and\nsupport size estimation, distinct elements problem, and learning Gaussian\nmixture models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 09:13:51 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 07:10:05 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Wu", "Yihong", ""], ["Yang", "Pengkun", ""]]}, {"id": "2104.07328", "submitter": "Miles Lopes", "authors": "Junwen Yao and Miles E. Lopes", "title": "Rates of Bootstrap Approximation for Eigenvalues in High-Dimensional PCA", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the context of principal components analysis (PCA), the bootstrap is\ncommonly applied to solve a variety of inference problems, such as constructing\nconfidence intervals for the eigenvalues of the population covariance matrix\n$\\Sigma$. However, when the data are high-dimensional, there are relatively few\ntheoretical guarantees that quantify the performance of the bootstrap. Our aim\nin this paper is to analyze how well the bootstrap can approximate the joint\ndistribution of the leading eigenvalues of the sample covariance matrix\n$\\hat\\Sigma$, and we establish non-asymptotic rates of approximation with\nrespect to the multivariate Kolmogorov metric. Under certain assumptions, we\nshow that the bootstrap can achieve the dimension-free rate of\n${\\tt{r}}(\\Sigma)/\\sqrt n$ up to logarithmic factors, where ${\\tt{r}}(\\Sigma)$\nis the effective rank of $\\Sigma$, and $n$ is the sample size. From a\nmethodological standpoint, our work also illustrates that applying a\ntransformation to the eigenvalues of $\\hat\\Sigma$ before bootstrapping is an\nimportant consideration in high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 09:28:23 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yao", "Junwen", ""], ["Lopes", "Miles E.", ""]]}, {"id": "2104.07359", "submitter": "Takuo Matsubara", "authors": "Takuo Matsubara, Jeremias Knoblauch, Fran\\c{c}ois-Xavier Briol, Chris.\n  J. Oates", "title": "Robust Generalised Bayesian Inference for Intractable Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalised Bayesian inference updates prior beliefs using a loss function,\nrather than a likelihood, and can therefore be used to confer robustness\nagainst possible misspecification of the likelihood. Here we consider\ngeneralised Bayesian inference with a Stein discrepancy as a loss function,\nmotivated by applications in which the likelihood contains an intractable\nnormalisation constant. In this context, the Stein discrepancy circumvents\nevaluation of the normalisation constant and produces generalised posteriors\nthat are either closed form or accessible using standard Markov chain Monte\nCarlo. On a theoretical level, we show consistency, asymptotic normality, and\nbias-robustness of the generalised posterior, highlighting how these properties\nare impacted by the choice of Stein discrepancy. Then, we provide numerical\nexperiments on a range of intractable distributions, including applications to\nkernel-based exponential family models and non-Gaussian graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:31:22 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Matsubara", "Takuo", ""], ["Knoblauch", "Jeremias", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2104.07552", "submitter": "Mariya Naumova", "authors": "Vladimir Gurvich, Mariya Naumova", "title": "Logical contradictions in the One-way ANOVA and Tukey-Kramer multiple\n  comparisons tests with more than two groups of observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We show that the One-way ANOVA and Tukey-Kramer (TK) tests agree on any\nsample with two groups. This result is based on a simple identity connecting\nthe Fisher-Snedecor and studentized probabilistic distributions and is proven\nwithout any additional assumptions; in particular, the standard ANOVA\nassumptions (independence, normality, and homoscedasticity (INAH)) are not\nneeded. In contrast, it is known that for a sample with k > 2 groups of\nobservations, even under the INAH assumptions, with the same significance level\n$\\alpha$, the above two tests may give opposite results: (i) ANOVA rejects its\nnull hypothesis $H_0^{A}: \\mu_1 = \\ldots = \\mu_k$, while the TK one,\n$H_0^{TK}(i,j): \\mu_i = \\mu_j$, is not rejected for any pair $i, j \\in \\{1,\n\\ldots, k\\}$; (ii) the TK test rejects $H_0^{TK}(i,j)$ for a pair $(i, j)$\n(with $i \\neq j$) while ANOVA does not reject $H_0^{A}$. We construct two large\ninfinite pseudo-random families of samples of both types satisfying INAH: in\ncase (i) for any $k \\geq 3$ and in case (ii) for some larger $k$. Furthermore,\nin case (ii) ANOVA, being restricted to the pair of groups $(i,j)$, may reject\nequality $\\mu_i = \\mu_j$ with the same $\\alpha$. This is an obvious\ncontradiction, since $\\mu_1 = \\ldots = \\mu_k$ implies $\\mu_i = \\mu_j$ for all\n$i, j \\in \\{1, \\ldots, k\\}.$ Similar contradictory examples are constructed for\nthe Multivariable Linear Regression (MLR). However, for these constructions it\nseems difficult to verify the Gauss-Markov assumptions, which are standardly\nrequired for MLR.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:06:57 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 22:26:09 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Gurvich", "Vladimir", ""], ["Naumova", "Mariya", ""]]}, {"id": "2104.07752", "submitter": "Fabrizio Leisen", "authors": "Patrizia Berti, Emanuela Dreassi, Fabrizio Leisen, Luca Pratelli,\n  Pietro Rigo", "title": "New perspectives on knockoffs construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\Lambda$ be the collection of all probability distributions for\n$(X,\\widetilde{X})$, where $X$ is a fixed random vector and $\\widetilde{X}$\nranges over all possible knockoff copies of $X$ (in the sense of\n\\cite{CFJL18}). Three topics are developed in this note: (i) A new\ncharacterization of $\\Lambda$ is proved; (ii) A certain subclass of $\\Lambda$,\ndefined in terms of copulas, is introduced; (iii) The (meaningful) special case\nwhere the components of $X$ are conditionally independent is treated in depth.\nIn real problems, after observing $X=x$, each of points (i)-(ii)-(iii) may be\nuseful to generate a value $\\widetilde{x}$ for $\\widetilde{X}$ conditionally on\n$X=x$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:20:00 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Berti", "Patrizia", ""], ["Dreassi", "Emanuela", ""], ["Leisen", "Fabrizio", ""], ["Pratelli", "Luca", ""], ["Rigo", "Pietro", ""]]}, {"id": "2104.07773", "submitter": "Emma Jingfei Zhang", "authors": "Biao Cai, Jingfei Zhang and Will Wei Sun", "title": "Heterogeneous Tensor Mixture Models in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of jointly modeling and clustering populations of\ntensors by introducing a flexible high-dimensional tensor mixture model with\nheterogeneous covariances. The proposed mixture model exploits the intrinsic\nstructures of tensor data, and is assumed to have means that are low-rank and\ninternally sparse as well as heterogeneous covariances that are separable and\nconditionally sparse. We develop an efficient high-dimensional\nexpectation-conditional-maximization (HECM) algorithm that breaks the\nchallenging optimization in the M-step into several simpler conditional\noptimization problems, each of which is convex, admits regularization and has\nclosed-form updating formulas. We show that the proposed HECM algorithm, with\nan appropriate initialization, converges geometrically to a neighborhood that\nis within statistical precision of the true parameter. Such a theoretical\nanalysis is highly nontrivial due to the dual non-convexity arising from both\nthe EM-type estimation and the non-convex objective function in the M-step. The\nefficacy of our proposed method is demonstrated through simulation studies and\nan application to an autism spectrum disorder study, where our analysis\nidentifies important brain regions for diagnosis.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:06:16 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Cai", "Biao", ""], ["Zhang", "Jingfei", ""], ["Sun", "Will Wei", ""]]}, {"id": "2104.07870", "submitter": "Ery Arias-Castro", "authors": "Ery Arias-Castro, Wanli Qiao, Lin Zheng", "title": "Estimation of the Global Mode of a Density: Minimaxity, Adaptation, and\n  Computational Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of the global mode of a density under some decay\nrate condition around the global mode. We show that the maximum of a histogram,\nwith proper choice of bandwidth, achieves the minimax rate that we establish\nfor the setting that we consider. This is based on knowledge of the decay rate.\nAddressing the situation where the decay rate is unknown, we propose a\nmultiscale variant consisting in the recursive refinement of a histogram, which\nis shown to be minimax adaptive. These methods run in linear time, and we prove\nin an appendix that this is best possible: There is no estimation procedure\nthat runs in sublinear time that achieves the minimax rate.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 03:08:24 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Qiao", "Wanli", ""], ["Zheng", "Lin", ""]]}, {"id": "2104.08018", "submitter": "Jean-Yves Brua", "authors": "Ouerdia Arkoun, Jean-Yves Brua and Serguei Pergamenshchikov", "title": "Adaptive efficient robust sequential analysis for autoregressive big\n  data models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we consider high dimension models based on dependent\nobservations defined through autoregressive processes. For such models we\ndevelop an adaptive efficient estimation method via the robust sequential model\nselection procedures. To this end, firstly, using the Van Trees inequality, we\nobtain a sharp lower bound for robust risks in an explicit form given by the\nfamous Pinsker constant. It should be noted, that for such models this constant\nis calculated for the first time. Then, using the weighted least square method\nand sharp non asymptotic oracle inequalities we provide the efficiency property\nin the minimax sense for the proposed estimation procedure, i.e. we establish,\nthat the upper bound for its risk coincides with the obtained lower bound. It\nshould be emphasized that this property is obtained without using sparse\nconditions and in the adaptive setting when the parameter dimension and model\nregularity are unknown.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:29:20 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Arkoun", "Ouerdia", ""], ["Brua", "Jean-Yves", ""], ["Pergamenshchikov", "Serguei", ""]]}, {"id": "2104.08069", "submitter": "Susanne Trick", "authors": "Susanne Trick, Frank J\\\"akel, Constantin A. Rothkopf", "title": "A Bivariate Beta Distribution with Arbitrary Beta Marginals and its\n  Generalization to a Correlated Dirichlet Distribution", "comments": "18 pages, 4 figures; revised affiliations", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a bivariate beta distribution that can model arbitrary\nbeta-distributed marginals with a positive correlation. The distribution is\nconstructed from six independent gamma-distributed random variates. We show how\nthe parameters of the distribution can be fit to data using moment matching.\nPrevious work used an approximate and sometimes inaccurate method to compute\nthe covariance. Here, we derive all product moments and the exact covariance,\nwhich can easily be computed numerically. The bivariate case can be generalized\nto a multivariate distribution with arbitrary beta-distributed marginals.\nFurthermore, we generalize the distribution from two marginal beta to two\nmarginal Dirichlet distributions. The resulting correlated Dirichlet\ndistribution makes it possible to model two correlated Dirichlet-distributed\nrandom vectors.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 12:34:34 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 12:56:21 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Trick", "Susanne", ""], ["J\u00e4kel", "Frank", ""], ["Rothkopf", "Constantin A.", ""]]}, {"id": "2104.08279", "submitter": "Stephen Bates", "authors": "Stephen Bates, Emmanuel Cand\\`es, Lihua Lei, Yaniv Romano, Matteo\n  Sesia", "title": "Testing for Outliers with Conformal p-values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies the construction of p-values for nonparametric outlier\ndetection, taking a multiple-testing perspective. The goal is to test whether\nnew independent samples belong to the same distribution as a reference data set\nor are outliers. We propose a solution based on conformal inference, a broadly\napplicable framework which yields p-values that are marginally valid but\nmutually dependent for different test points. We prove these p-values are\npositively dependent and enable exact false discovery rate control, although in\na relatively weak marginal sense. We then introduce a new method to compute\np-values that are both valid conditionally on the training data and independent\nof each other for different test points; this paves the way to stronger type-I\nerror guarantees. Our results depart from classical conformal inference as we\nleverage concentration inequalities rather than combinatorial arguments to\nestablish our finite-sample guarantees. Furthermore, our techniques also yield\na uniform confidence bound for the false positive rate of any outlier detection\nalgorithm, as a function of the threshold applied to its raw statistics.\nFinally, the relevance of our results is demonstrated by numerical experiments\non real and simulated data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:59:21 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 16:31:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bates", "Stephen", ""], ["Cand\u00e8s", "Emmanuel", ""], ["Lei", "Lihua", ""], ["Romano", "Yaniv", ""], ["Sesia", "Matteo", ""]]}, {"id": "2104.08324", "submitter": "Cynthia Rush", "authors": "Marco Avella Medina and Jos\\'e Luis Montiel Olea and Cynthia Rush and\n  Amilcar Velez", "title": "On the Robustness to Misspecification of $\\alpha$-Posteriors and Their\n  Variational Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $\\alpha$-posteriors and their variational approximations distort standard\nposterior inference by downweighting the likelihood and introducing variational\napproximation errors. We show that such distortions, if tuned appropriately,\nreduce the Kullback-Leibler (KL) divergence from the true, but perhaps\ninfeasible, posterior distribution when there is potential parametric model\nmisspecification. To make this point, we derive a Bernstein-von Mises theorem\nshowing convergence in total variation distance of $\\alpha$-posteriors and\ntheir variational approximations to limiting Gaussian distributions. We use\nthese distributions to evaluate the KL divergence between true and reported\nposteriors. We show this divergence is minimized by choosing $\\alpha$ strictly\nsmaller than one, assuming there is a vanishingly small probability of model\nmisspecification. The optimized value becomes smaller as the the\nmisspecification becomes more severe. The optimized KL divergence increases\nlogarithmically in the degree of misspecification and not linearly as with the\nusual posterior.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 19:11:53 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Medina", "Marco Avella", ""], ["Olea", "Jos\u00e9 Luis Montiel", ""], ["Rush", "Cynthia", ""], ["Velez", "Amilcar", ""]]}, {"id": "2104.08361", "submitter": "Brian Fitzpatrick", "authors": "Brian Fitzpatrick, James Loughman, Daniel Ian Flitcroft", "title": "A Multiple Regression-Enhanced Convolution Estimator for the Density of\n  a Response Variable in the Presence of Additional Covariate Information", "comments": "Working draft for reference on ARVO 2021 conference poster, 34 pages;\n  refined many sections of previous working draft, 31 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a convolution estimator for estimating the density\nof a response variable that employs an underlying multiple regression framework\nto enhance the accuracy of density estimates through the incorporation of\nauxiliary information. Suppose we have a sample consisting of $N$ complete case\nobservations of a response variable and an associated set of covariates, along\nwith an additional sample consisting of $M$ observations of the covariates\nonly. We show that the mean square error of the multiple regression-enhanced\nconvolution estimator converges as $O(N^{-1})$ towards zero, and moreover, for\na large fixed $N$, that the mean square error converges as $O(M^{-4/5})$\ntowards an $O(N^{-1})$ constant. This is the first time that the convergence of\na convolution estimator with respect to the amount of additional covariate\ninformation has been established. In contrast to convolution estimators based\non the Nadaraya-Watson estimator for a nonlinear regression model, the multiple\nregression-enhanced convolution estimator proposed in this paper does not\nsuffer from the curse of dimensionality. It is particularly useful for\nscenarios in which one wants to estimate the density of a response variable\nthat is challenging to measure, while being in possession of a large amount of\nadditional covariate information. In fact, an application of this type from the\nfield of ophthalmology motivated our work in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 20:37:41 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 10:44:52 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Fitzpatrick", "Brian", ""], ["Loughman", "James", ""], ["Flitcroft", "Daniel Ian", ""]]}, {"id": "2104.08525", "submitter": "Sangita Das", "authors": "Sangita Das and Suchandan Kayal", "title": "On comparison of the second-order statistics from independent and\n  interdependent exponentiated location-scale distributed random variables", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider two batches of independent or interdependent exponentiated\nlocation-scale distributed heterogeneous random variables. This article\ninvestigates ordering results for the second-order statistics from these\nbatches when a vector of parameters is switched to another vector of parameters\nin the specified model. Sufficient conditions for the usual stochastic order\nand the hazard rate order are derived. Some applications of the established\nresults are presented.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 12:28:50 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Das", "Sangita", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2104.08611", "submitter": "Sangita Das", "authors": "Sangita Das and Suchandan Kayal", "title": "Some new ordering results on stochastic comparisons of second largest\n  order statistics from independent and interdependent heterogeneous\n  distributions", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The second-largest order statistic is of special importance in reliability\ntheory since it represents the time to failure of a $2$-out-of-$n$ system.\nConsider two $2$-out-of-$n$ systems with heterogeneous random lifetimes. The\nlifetimes are assumed to follow heterogeneous general exponentiated\nlocation-scale models. In this communication, the usual stochastic and reversed\nhazard rate orders between the systems' lifetimes are established under two\ncases. For the case of independent random lifetimes, the usual stochastic order\nand the reversed hazard rate order between the second-largest order statistics\nare obtained by using the concept of vector majorization and related orders.\nFor the dependent case, the conditions under which the usual stochastic order\nbetween the second-largest order statistics holds are investigated. To\nillustrate the theoretical findings, some special cases of the exponentiated\nlocation-scale model are considered.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:26:10 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Das", "Sangita", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2104.08687", "submitter": "Dan M. Kluger", "authors": "Dan M. Kluger and Art B. Owen", "title": "A central limit theorem for the Benjamini-Hochberg false discovery\n  proportion under a factor model", "comments": "Main changes in version 2: i) restated Corollary 1 in a way that is\n  clearer and easier to use, ii) removed a regularity condition for our\n  theorems (in particular we removed Condition 2 from version 1), and iii) we\n  added a couple of remarks (namely, Remark 1 and 6 in version 2). Throughout\n  the text we also fixed typos, improved clarity, and added a some additional\n  commentary and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Benjamini-Hochberg (BH) procedure remains widely popular despite having\nlimited theoretical guarantees in the commonly encountered scenario of\ncorrelated test statistics. Of particular concern is the possibility that the\nmethod could exhibit bursty behavior, meaning that it might typically yield no\nfalse discoveries while occasionally yielding both a large number of false\ndiscoveries and a false discovery proportion (FDP) that far exceeds its own\nwell controlled mean. In this paper, we investigate which test statistic\ncorrelation structures lead to bursty behavior and which ones lead to well\ncontrolled FDPs. To this end, we develop a central limit theorem for the FDP in\na multiple testing setup where the test statistic correlations can be either\nshort-range or long-range as well as either weak or strong. The theorem and our\nsimulations from a data-driven factor model suggest that the BH procedure\nexhibits severe burstiness when the test statistics have many strong,\nlong-range correlations, but does not otherwise.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 02:52:35 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 00:56:54 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kluger", "Dan M.", ""], ["Owen", "Art B.", ""]]}, {"id": "2104.08931", "submitter": "David Hirshberg", "authors": "David A. Hirshberg", "title": "Least Squares with Error in Variables", "comments": "40 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Error-in-variables regression is a common ingredient in treatment effect\nestimators using panel data. This includes synthetic control estimators,\ncounterfactual time series forecasting estimators, and combinations. We study\nhigh-dimensional least squares with correlated error-in-variables with a focus\non these uses. We use our results to derive conditions under which the\nsynthetic control estimator is asymptotically unbiased and normal with\nestimable variance, permitting inference without assuming time-stationarity,\nunit-exchangeability, or the absence of weak factors. These results hold in an\nasymptotic regime in which the number of pre-treatment periods goes to infinity\nand the number of control units can be much larger $(p \\gg n)$.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 18:50:37 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hirshberg", "David A.", ""]]}, {"id": "2104.08959", "submitter": "TrungTin Nguyen", "authors": "TrungTin Nguyen, Faicel Chamroukhi, Hien Duy Nguyen, Florence Forbes", "title": "Non-asymptotic model selection in block-diagonal mixture of polynomial\n  experts models", "comments": "Corrected typos. Extended results from arXiv:2104.02640. arXiv admin\n  note: substantial text overlap with arXiv:2104.02640", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection, via penalized likelihood type criteria, is a standard task\nin many statistical inference and machine learning problems. Progress has led\nto deriving criteria with asymptotic consistency results and an increasing\nemphasis on introducing non-asymptotic criteria. We focus on the problem of\nmodeling non-linear relationships in regression data with potential hidden\ngraph-structured interactions between the high-dimensional predictors, within\nthe mixture of experts modeling framework. In order to deal with such a complex\nsituation, we investigate a block-diagonal localized mixture of polynomial\nexperts (BLoMPE) regression model, which is constructed upon an inverse\nregression and block-diagonal structures of the Gaussian expert covariance\nmatrices. We introduce a penalized maximum likelihood selection criterion to\nestimate the unknown conditional density of the regression model. This model\nselection criterion allows us to handle the challenging problem of inferring\nthe number of mixture components, the degree of polynomial mean functions, and\nthe hidden block-diagonal structures of the covariance matrices, which reduces\nthe number of parameters to be estimated and leads to a trade-off between\ncomplexity and sparsity in the model. In particular, we provide a strong\ntheoretical guarantee: a finite-sample oracle inequality satisfied by the\npenalized maximum likelihood estimator with a Jensen-Kullback-Leibler type\nloss, to support the introduced non-asymptotic model selection criterion. The\npenalty shape of this criterion depends on the complexity of the considered\nrandom subcollection of BLoMPE models, including the relevant graph structures,\nthe degree of polynomial mean functions, and the number of mixture components.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 21:32:20 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 21:05:06 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Nguyen", "TrungTin", ""], ["Chamroukhi", "Faicel", ""], ["Nguyen", "Hien Duy", ""], ["Forbes", "Florence", ""]]}, {"id": "2104.09401", "submitter": "Maximilian Wechsung", "authors": "Maximilian Wechsung, Frank Konietschke", "title": "Distribution-free multivariate inference about diagnostic classifiers\n  based on partial areas under their receiver operating characteristic curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The receiver operating characteristic curve is a widely used performance\nindicator for diagnostic tests. By its nature some segments of the curve are\nmore relevant for clinical applications than others. A suitably specified\npartial area under the curve aggregates the information carried by the\nclinically relevant segments. Our main result shows joint asymptotic normality\nof vectors of possibly dependent distribution-free estimators of these partial\nareas. We additionally show correctness of the empirical bootstrap in this\nsituation and use it to construct asymptotically correct multiple contrast\ntests for partial areas under receiver operating characteristic curves. Our\nanalytical results indicate that a partial area under the curve may be\npreferable to the widely used total area under the curve as a basis for\nperformance comparisons between two diagnostic tests.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:43:10 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wechsung", "Maximilian", ""], ["Konietschke", "Frank", ""]]}, {"id": "2104.09485", "submitter": "Martin Kroll", "authors": "Holger Dette and Martin Kroll", "title": "Asymptotic equivalence for nonparametric regression with dependent\n  errors: Gauss-Markov processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For the class of Gauss-Markov processes we study the problem of asymptotic\nequivalence of the nonparametric regression model with errors given by the\nincrements of the process and the continuous time model, where a whole path of\na sum of a deterministic signal and the Gauss-Markov process can be observed.\nIn particular we provide sufficient conditions such that asymptotic equivalence\nof the two models holds for functions from a given class, and we verify these\nfor the special cases of Sobolev ellipsoids and H\\\"older classes with\nsmoothness index $> 1/2$ under mild assumptions on the Gauss-Markov process at\nhand. To derive these results, we develop an explicit characterization of the\nreproducing kernel Hilbert space associated with the Gauss-Markov process, that\nhinges on a characterization of such processes by a property of the\ncorresponding covariance kernel introduced by Doob. In order to demonstrate\nthat the given assumptions on the Gauss-Markov process are in some sense sharp\nwe also show that asymptotic equivalence fails to hold for the special case of\nBrownian bridge. Our results demonstrate that the well-known asymptotic\nequivalence of the Gaussian white noise model and the nonparametric regression\nmodel with independent standard normal distributed errors can be extended to a\nbroad class of models with dependent data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:46:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dette", "Holger", ""], ["Kroll", "Martin", ""]]}, {"id": "2104.09665", "submitter": "Allen Liu", "authors": "Allen Liu, Ankur Moitra", "title": "Learning GMMs with Nearly Optimal Robustness Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we solve the problem of robustly learning a high-dimensional\nGaussian mixture model with $k$ components from $\\epsilon$-corrupted samples up\nto accuracy $\\widetilde{O}(\\epsilon)$ in total variation distance for any\nconstant $k$ and with mild assumptions on the mixture. This robustness\nguarantee is optimal up to polylogarithmic factors. At the heart of our\nalgorithm is a new way to relax a system of polynomial equations which\ncorresponds to solving an improper learning problem where we are allowed to\noutput a Gaussian mixture model whose weights are low-degree polynomials.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 22:14:54 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Liu", "Allen", ""], ["Moitra", "Ankur", ""]]}, {"id": "2104.09778", "submitter": "Wenjia Wang", "authors": "Wenjia Wang and Bing-Yi Jing", "title": "Convergence of Gaussian process regression: Optimality, robustness, and\n  relationship with kernel ridge regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate Gaussian process regression used to recover a\nfunction based on noisy observations. We derive upper and lower error bounds\nfor Gaussian process regression with possibly misspecified correlation\nfunctions. The optimal convergence rate can be attained even if the smoothness\nof the imposed correlation function exceeds that of the true correlation\nfunction and the sampling scheme is quasi-uniform. As byproducts, we also\nobtain convergence rates of kernel ridge regression with misspecified kernel\nfunction, where the underlying truth is a deterministic function. The\nconvergence rates of Gaussian process regression and kernel ridge regression\nare closely connected, which is aligned with the relationship between sample\npaths of Gaussian process and the corresponding reproducing kernel Hilbert\nspace.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 06:27:14 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "Wenjia", ""], ["Jing", "Bing-Yi", ""]]}, {"id": "2104.10240", "submitter": "Mohsen Rezapour", "authors": "Bahareh Afhami, Mohsen Rezapour, Mohsen Madadi, and Vahed Maroufy", "title": "Portfolio Selection under Multivariate Merton Model with Correlated Jump\n  Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio selection in the periodic investment of securities modeled by a\nmultivariate Merton model with dependent jumps is considered. The optimization\nframework is designed to maximize expected terminal wealth when portfolio risk\nis measured by the Condition-Value-at-Risk ($CVaR$). Solving the portfolio\noptimization problem by Monte Carlo simulation often requires intensive and\ntime-consuming computation; hence a faster and more efficient portfolio\noptimization method based on closed-form comonotonic bounds for the risk\nmeasure $CVaR$ of the terminal wealth is proposed.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 20:36:23 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Afhami", "Bahareh", ""], ["Rezapour", "Mohsen", ""], ["Madadi", "Mohsen", ""], ["Maroufy", "Vahed", ""]]}, {"id": "2104.10334", "submitter": "Sylvia Klosin", "authors": "Sylvia Klosin", "title": "Automatic Double Machine Learning for Continuous Treatment Effects", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce and prove asymptotic normality for a new\nnonparametric estimator of continuous treatment effects. Specifically, we\nestimate the average dose-response function - the expected value of an outcome\nof interest at a particular level of the treatment level. We utilize tools from\nboth the double debiased machine learning (DML) and the automatic double\nmachine learning (ADML) literatures to construct our estimator. Our estimator\nutilizes a novel debiasing method that leads to nice theoretical stability and\nbalancing properties. In simulations our estimator performs well compared to\ncurrent methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:17:40 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Klosin", "Sylvia", ""]]}, {"id": "2104.10335", "submitter": "Arkaprava Roy", "authors": "Arkaprava Roy, Shubhashis Ghosal", "title": "Optimal Bayesian Smoothing of Functional Observations over a Large Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern contexts, some types of data are observed in high-resolution,\nessentially continuously in time. Such data units are best described as taking\nvalues in a space of functions. Subject units carrying the observations may\nhave intrinsic relations among themselves, and are best described by the nodes\nof a large graph. It is often sensible to think that the underlying signals in\nthese functional observations vary smoothly over the graph, in that neighboring\nnodes have similar underlying signals. This qualitative information allows\nborrowing of strength over neighboring nodes and consequently leads to more\naccurate inference. In this paper, we consider a model with Gaussian functional\nobservations and adopt a Bayesian approach to smoothing over the nodes of the\ngraph. We characterize the minimax rate of estimation in terms of the\nregularity of the signals and their variation across nodes quantified in terms\nof the graph Laplacian. We show that an appropriate prior constructed from the\ngraph Laplacian can attain the minimax bound, while using a mixture prior, the\nminimax rate up to a logarithmic factor can be attained simultaneously for all\npossible values of functional and graphical smoothness. We also show that in\nthe fixed smoothness setting, an optimal sized credible region has arbitrarily\nhigh frequentist coverage. A simulation experiment demonstrates that the method\nperforms better than potential competing methods like the random forest. The\nmethod is also applied to a dataset on daily temperatures measured at several\nweather stations in the US state of North Carolina.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:22:22 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 03:01:31 GMT"}, {"version": "v3", "created": "Mon, 19 Jul 2021 21:36:01 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Roy", "Arkaprava", ""], ["Ghosal", "Shubhashis", ""]]}, {"id": "2104.10471", "submitter": "Biqiang Mu", "authors": "Biqiang Mu, Tianshi Chen, Lennart Ljung", "title": "On the Asymptotic Optimality of Cross-Validation based Hyper-parameter\n  Estimators for Regularized Least Squares Regression Problems", "comments": "36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymptotic optimality (a.o.) of various hyper-parameter estimators with\ndifferent optimality criteria has been studied in the literature for\nregularized least squares regression problems. The estimators include e.g., the\nmaximum (marginal) likelihood method, $C_p$ statistics, and generalized cross\nvalidation method, and the optimality criteria are based on e.g., the\ninefficiency, the expectation inefficiency and the risk. In this paper, we\nconsider the regularized least squares regression problems with fixed number of\nregression parameters, choose the optimality criterion based on the risk, and\nstudy the a.o. of several cross validation (CV) based hyper-parameter\nestimators including the leave $k$-out CV method, generalized CV method,\n$r$-fold CV method and hold out CV method. We find the former three methods can\nbe a.o. under mild assumptions, but not the last one, and we use Monte Carlo\nsimulations to illustrate the efficacy of our findings.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 11:35:39 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 01:38:48 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Mu", "Biqiang", ""], ["Chen", "Tianshi", ""], ["Ljung", "Lennart", ""]]}, {"id": "2104.10554", "submitter": "Hengrui Cai", "authors": "Hengrui Cai, Wenbin Lu, Rui Song", "title": "Calibrated Optimal Decision Making with Multiple Data Sources and\n  Limited Outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimal decision-making problem in a primary sample of\ninterest with multiple auxiliary sources available. The outcome of interest is\nlimited in the sense that it is only observed in the primary sample. In\nreality, such multiple data sources may belong to different populations and\nthus cannot be combined directly. This paper proposes a novel calibrated\noptimal decision rule (CODR) to address the limited outcome, by leveraging the\nshared pattern in multiple data sources. Under a mild and testable assumption\nthat the conditional means of intermediate outcomes in different samples are\nequal given baseline covariates and the treatment information, we can show that\nthe calibrated mean outcome of interest under the CODR is unbiased and more\nefficient than using the primary sample solely. Extensive experiments on\nsimulated datasets demonstrate empirical validity and improvement of the\nproposed CODR, followed by a real application on the MIMIC-III as the primary\nsample with auxiliary data from eICU.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:24:17 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cai", "Hengrui", ""], ["Lu", "Wenbin", ""], ["Song", "Rui", ""]]}, {"id": "2104.10573", "submitter": "Hengrui Cai", "authors": "Hengrui Cai, Rui Song, Wenbin Lu", "title": "GEAR: On Optimal Decision Making with Auxiliary Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized optimal decision making, finding the optimal decision rule (ODR)\nbased on individual characteristics, has attracted increasing attention\nrecently in many fields, such as education, economics, and medicine. Current\nODR methods usually require the primary outcome of interest in samples for\nassessing treatment effects, namely the experimental sample. However, in many\nstudies, treatments may have a long-term effect, and as such the primary\noutcome of interest cannot be observed in the experimental sample due to the\nlimited duration of experiments, which makes the estimation of ODR impossible.\nThis paper is inspired to address this challenge by making use of an auxiliary\nsample to facilitate the estimation of ODR in the experimental sample. We\npropose an auGmented inverse propensity weighted Experimental and Auxiliary\nsample-based decision Rule (GEAR) by maximizing the augmented inverse\npropensity weighted value estimator over a class of decision rules using the\nexperimental sample, with the primary outcome being imputed based on the\nauxiliary sample. The asymptotic properties of the proposed GEAR estimators and\ntheir associated value estimators are established. Simulation studies are\nconducted to demonstrate its empirical validity with a real AIDS application.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:59:25 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cai", "Hengrui", ""], ["Song", "Rui", ""], ["Lu", "Wenbin", ""]]}, {"id": "2104.10601", "submitter": "Mika Meitz", "authors": "Mika Meitz", "title": "Statistical inference for generative adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies generative adversarial networks (GANs) from a statistical\nperspective. A GAN is a popular machine learning method in which the parameters\nof two neural networks, a generator and a discriminator, are estimated to solve\na particular minimax problem. This minimax problem typically has a multitude of\nsolutions and the focus of this paper are the statistical properties of these\nsolutions. We address two key issues for the generator and discriminator\nnetwork parameters, consistent estimation and confidence sets. We first show\nthat the set of solutions to the sample GAN problem is a (Hausdorff) consistent\nestimator of the set of solutions to the corresponding population GAN problem.\nWe then devise a computationally intensive procedure to form confidence sets\nand show that these sets contain the population GAN solutions with the desired\ncoverage probability. The assumptions employed in our results are weak and hold\nin many practical GAN applications. To the best of our knowledge, this paper\nprovides the first results on statistical inference for GANs in the empirically\nrelevant case of multiple solutions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 15:59:12 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Meitz", "Mika", ""]]}, {"id": "2104.10618", "submitter": "Qingyuan Zhao", "authors": "Yao Zhang, Qingyuan Zhao", "title": "Multiple conditional randomization tests", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for (multiple) conditional randomization tests\nthat incorporate several important ideas in the recent literature. We establish\na general sufficient condition on the construction of multiple conditional\nrandomization tests under which their p-values are \"independent\", in the sense\nthat their joint distribution stochastically dominates the product of uniform\ndistributions under the null. Conceptually, we argue that randomization should\nbe understood as the mode of inference precisely based on randomization. We\nshow that under a change of perspective, many existing statistical methods,\nincluding permutation tests for (conditional) independence and conformal\nprediction, are special cases of the general conditional randomization test.\nThe versatility of our framework is further illustrated with an example\nconcerning lagged treatment effects in stepped-wedge randomized trials.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 16:25:43 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 15:41:46 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Zhang", "Yao", ""], ["Zhao", "Qingyuan", ""]]}, {"id": "2104.10633", "submitter": "Wing Hung Wong", "authors": "Wing Hung Wong", "title": "A calculus for causal inference with instrumental variables", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Under a general structural equation framework for causal inference, we\nprovide a definition of the causal effect of a variable X on another variable\nY, and propose an approach to estimate this causal effect via the use of\ninstrumental variables.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 16:59:12 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 06:02:33 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wong", "Wing Hung", ""]]}, {"id": "2104.10651", "submitter": "Fabio Cuzzolin", "authors": "Fabio Cuzzolin", "title": "A geometric approach to conditioning belief functions", "comments": "43 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Conditioning is crucial in applied science when inference involving time\nseries is involved. Belief calculus is an effective way of handling such\ninference in the presence of epistemic uncertainty -- unfortunately, different\napproaches to conditioning in the belief function framework have been proposed\nin the past, leaving the matter somewhat unsettled. Inspired by the geometric\napproach to uncertainty, in this paper we propose an approach to the\nconditioning of belief functions based on geometrically projecting them onto\nthe simplex associated with the conditioning event in the space of all belief\nfunctions. We show here that such a geometric approach to conditioning often\nproduces simple results with straightforward interpretations in terms of\ndegrees of belief. This raises the question of whether classical approaches,\nsuch as for instance Dempster's conditioning, can also be reduced to some form\nof distance minimisation in a suitable space. The study of families of\ncombination rules generated by (geometric) conditioning rules appears to be the\nnatural prosecution of the presented research.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 17:24:19 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cuzzolin", "Fabio", ""]]}, {"id": "2104.10750", "submitter": "Ksheera Sagar K N", "authors": "Ksheera Sagar K. N, Sayantan Banerjee, Jyotishka Datta, Anindya Bhadra", "title": "Precision Matrix Estimation under the Horseshoe-like Prior-Penalty Dual", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of precision matrix estimation in a multivariate Gaussian model\nis fundamental to network estimation. Although there exist both Bayesian and\nfrequentist approaches to this, it is difficult to obtain good Bayesian and\nfrequentist properties under the same prior-penalty dual, complicating\njustification. It is well known, for example, that the Bayesian version of the\npopular lasso estimator has poor posterior concentration properties. To bridge\nthis gap for the precision matrix estimation problem, our contribution is a\nnovel prior-penalty dual that closely approximates the popular graphical\nhorseshoe prior and penalty, and performs well in both Bayesian and frequentist\nsenses. A chief difficulty with the horseshoe prior is a lack of closed form\nexpression of the density function, which we overcome in this article, allowing\nus to directly study the penalty function. In terms of theory, we establish\nposterior convergence rate of the precision matrix that matches the oracle\nrate, in addition to the frequentist consistency of the maximum a posteriori\nestimator. In addition, our results also provide theoretical justifications for\npreviously developed approaches that have been unexplored so far, e.g. for the\ngraphical horseshoe prior. Computationally efficient Expectation Conditional\nMaximization and Markov chain Monte Carlo algorithms are developed respectively\nfor the penalized likelihood and fully Bayesian estimation problems, using the\nsame latent variable framework. In numerical experiments, the horseshoe-based\napproaches echo their superior theoretical properties by comprehensively\noutperforming the competing methods. A protein-protein interaction network\nestimation in B-cell lymphoma is considered to validate the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:30:38 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["N", "Ksheera Sagar K.", ""], ["Banerjee", "Sayantan", ""], ["Datta", "Jyotishka", ""], ["Bhadra", "Anindya", ""]]}, {"id": "2104.10808", "submitter": "Modou Ngom", "authors": "Moumouni Diallo, Modou Ngom, Akim Adekpedjou, Gane Samb Lo", "title": "Second order Expansions for Extreme Quantiles of Burr Distributions and\n  Asymptotic Theory of Record Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the Burr distributions Family which contains\ntwelve members. Second order expansions of quantiles of the Burr's\ndistributions are provided on which may be based statistical methods, in\nparticular in extreme value theory. Beyond the proper interest of these\nexpansions, we apply them to characterize the asymptotic laws of their records\nof Burr's distributions, lead to new statistical tests.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 00:56:13 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Diallo", "Moumouni", ""], ["Ngom", "Modou", ""], ["Adekpedjou", "Akim", ""], ["Lo", "Gane Samb", ""]]}, {"id": "2104.11294", "submitter": "Philip Etter", "authors": "Philip Etter, Lexing Ying", "title": "Operator Augmentation for General Noisy Matrix Systems", "comments": "arXiv admin note: text overlap with arXiv:2010.09656", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the computational sciences, one must often estimate model parameters from\ndata subject to noise and uncertainty, leading to inaccurate results. In order\nto improve the accuracy of models with noisy parameters, we consider the\nproblem of reducing error in a linear system with the operator corrupted by\nnoise. To address this problem, we extend the elliptic operator augmentation\nframework (Etter, Ying 2020) to the general nonsymmetric matrix case. We show\nthat under the conditions of right-hand-side isotropy and noise symmetry that\nthe optimal operator augmentation factor for the residual error is always\npositive, thereby making the framework amenable to a necessary bootstrapping\nstep. While the above conditions are unnecessary for positive optimal\naugmentation factor in the elliptic case, we provide counter-examples that\nillustrate their necessity when applied to general matrices. When the noise in\nthe operator is small, however, we show that the condition of noise symmetry is\nunnecessary. Finally, we demonstrate through numerical experiments on Markov\nchain problems that operator augmentation can significantly reduce error in\nnoisy matrix systems -- even when the aforementioned conditions are not met.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 19:28:46 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Etter", "Philip", ""], ["Ying", "Lexing", ""]]}, {"id": "2104.11329", "submitter": "Christian Bick", "authors": "Christian Bick, Elizabeth Gross, Heather A. Harrington, and Michael T.\n  Schaub", "title": "What are higher-order networks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI math.CO math.DS math.ST nlin.AO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling complex systems and data using the language of graphs and networks\nhas become an essential topic across a range of different disciplines.\nArguably, this network-based perspective derives is success from the relative\nsimplicity of graphs: A graph consists of nothing more than a set of vertices\nand a set of edges, describing relationships between pairs of such vertices.\nThis simple combinatorial structure makes graphs interpretable and flexible\nmodeling tools. The simplicity of graphs as system models, however, has been\nscrutinized in the literature recently. Specifically, it has been argued from a\nvariety of different angles that there is a need for higher-order networks,\nwhich go beyond the paradigm of modeling pairwise relationships, as\nencapsulated by graphs. In this survey article we take stock of these recent\ndevelopments. Our goals are to clarify (i) what higher-order networks are, (ii)\nwhy these are interesting objects of study, and (iii) how they can be used in\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 22:25:12 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bick", "Christian", ""], ["Gross", "Elizabeth", ""], ["Harrington", "Heather A.", ""], ["Schaub", "Michael T.", ""]]}, {"id": "2104.11358", "submitter": "Giovanni Motta", "authors": "Giovanni Motta", "title": "Joint Mean-Vector and Var-Matrix estimation for Locally Stationary\n  VAR(1) processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the last two decades, locally stationary processes have been widely\nstudied in the time series literature. In this paper we consider the\nlocally-stationary vector-auto-regression model of order one, or LS-VAR(1), and\nestimate its parameters by weighted least squares. The LS-VAR(1) we consider\nallows for a smoothly time-varying non-diagonal VAR matrix, as well as for a\nsmoothly time-varying non-zero mean. The weighting scheme is based on kernel\nsmoothers. The time-varying mean and the time-varying VAR matrix are estimated\njointly, and the definition of the local-linear weighting matrix is provided in\nclosed-from. The quality of the estimated curves is illustrated through\nsimulation results.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 00:37:17 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Motta", "Giovanni", ""]]}, {"id": "2104.11371", "submitter": "Laura Dumitrescu", "authors": "L. Dumitrescu and D. Harcourt", "title": "Nonparametric estimation of marginal distributions for unordered pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the estimation of the marginal distributions for\npairs of data are recorded, with unobserved order in each pair. New estimators\nare proposed and their asymptotic properties are established, by proving a\nGlivenko-Cantelli theorem and a functional central limit result. Results from a\nsimulation study are included and we illustrate the applicability of the method\non the homologous chromosomes data.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 01:44:04 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Dumitrescu", "L.", ""], ["Harcourt", "D.", ""]]}, {"id": "2104.11438", "submitter": "Yozo Tonaki", "authors": "Yozo Tonaki and Masayuki Uchida", "title": "Change point inference in ergodic diffusion processes based on high\n  frequency data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with the change point problem in ergodic diffusion processes based on\nhigh frequency data. Tonaki et al. (2020, 2021) studied the change point\nproblem for the ergodic diffusion process model. However, the change point\nproblem for the drift parameter when the diffusion parameter changes is still\nopen. Therefore, we consider the change detection and the change point\nestimation for the drift parameter taking into account that there is a change\npoint in the diffusion parameter. Moreover, we examine the performance of the\ntests and the estimation with numerical simulations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 07:03:27 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Tonaki", "Yozo", ""], ["Uchida", "Masayuki", ""]]}, {"id": "2104.11496", "submitter": "Lukas Trottner", "authors": "S\\\"oren Christensen, Claudia Strauch and Lukas Trottner", "title": "Learning to reflect: A unifying approach for data-driven stochastic\n  control strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic optimal control problems have a long tradition in applied\nprobability, with the questions addressed being of high relevance in a\nmultitude of fields. Even though theoretical solutions are well understood in\nmany scenarios, their practicability suffers from the assumption of known\ndynamics of the underlying stochastic process, raising the statistical\nchallenge of developing purely data-driven strategies. For the mathematically\nseparated classes of continuous diffusion processes and L\\'evy processes, we\nshow that developing efficient strategies for related singular stochastic\ncontrol problems can essentially be reduced to finding rate-optimal estimators\nwith respect to the sup-norm risk of objects associated to the invariant\ndistribution of ergodic processes which determine the theoretical solution of\nthe control problem. From a statistical perspective, we exploit the exponential\n$\\beta$-mixing property as the common factor of both scenarios to drive the\nconvergence analysis, indicating that relying on general stability properties\nof Markov processes is a sufficiently powerful and flexible approach to treat\ncomplex applications requiring statistical methods. We show moreover that in\nthe L\\'evy case $-$ even though per se jump processes are more difficult to\nhandle both in statistics and control theory $-$ a fully data-driven strategy\nwith regret of significantly better order than in the diffusion case can be\nconstructed.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 09:33:15 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Christensen", "S\u00f6ren", ""], ["Strauch", "Claudia", ""], ["Trottner", "Lukas", ""]]}, {"id": "2104.11547", "submitter": "Patrick Forr\\'e", "authors": "Patrick Forr\\'e", "title": "Transitional Conditional Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develope the framework of transitional conditional independence. For this\nwe introduce transition probability spaces and transitional random variables.\nThese constructions will generalize, strengthen and unify previous notions of\n(conditional) random variables and non-stochastic variables, (extended)\nstochastic conditional independence and some form of functional conditional\nindependence. Transitional conditional independence is asymmetric in general\nand it will be shown that it satisfies all desired relevance relations in terms\nof left and right versions of the separoid rules, except symmetry, on standard,\nanalytic and universal measurable spaces. As a preparation we prove a\ndisintegration theorem for transition probabilities, i.e. the existence and\nessential uniqueness of (regular) conditional Markov kernels, on those spaces.\nTransitional conditional independence will be able to express classical\nstatistical concepts like sufficiency, adequacy and ancillarity. As an\napplication, we will then show how transitional conditional independence can be\nused to prove a directed global Markov property for causal graphical models\nthat allow for non-stochastic input variables in strong generality. This will\nthen also allow us to show the main rules of causal do-calculus, relating\nobservational and interventional distributions, in such measure theoretic\ngenerality.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 11:52:15 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Forr\u00e9", "Patrick", ""]]}, {"id": "2104.11643", "submitter": "Fabrizio Leisen", "authors": "Patrizia Berti, Emanuela Dreassi, Fabrizio Leisen, Pietro Rigo, Luca\n  Pratelli", "title": "Bayesian predictive inference without a prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(X_n:n\\ge 1)$ be a sequence of random observations. Let\n$\\sigma_n(\\cdot)=P\\bigl(X_{n+1}\\in\\cdot\\mid X_1,\\ldots,X_n\\bigr)$ be the $n$-th\npredictive distribution and $\\sigma_0(\\cdot)=P(X_1\\in\\cdot)$ the marginal\ndistribution of $X_1$. In a Bayesian framework, to make predictions on $(X_n)$,\none only needs the collection $\\sigma=(\\sigma_n:n\\ge 0)$. Because of the\nIonescu-Tulcea theorem, $\\sigma$ can be assigned directly, without passing\nthrough the usual prior/posterior scheme. One main advantage is that no prior\nprobability has to be selected. In this paper, $\\sigma$ is subjected to two\nrequirements: (i) The resulting sequence $(X_n)$ is conditionally identically\ndistributed, in the sense of Berti, Pratelli and Rigo (2004); (ii) Each\n$\\sigma_{n+1}$ is a simple recursive update of $\\sigma_n$. Various new $\\sigma$\nsatisfying (i)-(ii) are introduced and investigated. For such $\\sigma$, the\nasymptotics of $\\sigma_n$, as $n\\rightarrow\\infty$, is determined. In some\ncases, the probability distribution of $(X_n)$ is also evaluated.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:51:07 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 13:02:43 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Berti", "Patrizia", ""], ["Dreassi", "Emanuela", ""], ["Leisen", "Fabrizio", ""], ["Rigo", "Pietro", ""], ["Pratelli", "Luca", ""]]}, {"id": "2104.12174", "submitter": "Ivan Y. Tyukin", "authors": "Ivan Y. Tyukin, Alexander N. Gorban, Muhammad H. Alkhudaydi, Qinghua\n  Zhou", "title": "Demystification of Few-shot and One-shot Learning", "comments": "IEEE International Joint Conference on Neural Networks, IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot and one-shot learning have been the subject of active and intensive\nresearch in recent years, with mounting evidence pointing to successful\nimplementation and exploitation of few-shot learning algorithms in practice.\nClassical statistical learning theories do not fully explain why few- or\none-shot learning is at all possible since traditional generalisation bounds\nnormally require large training and testing samples to be meaningful. This\nsharply contrasts with numerous examples of successful one- and few-shot\nlearning systems and applications.\n  In this work we present mathematical foundations for a theory of one-shot and\nfew-shot learning and reveal conditions specifying when such learning schemes\nare likely to succeed. Our theory is based on intrinsic properties of\nhigh-dimensional spaces. We show that if the ambient or latent decision space\nof a learning machine is sufficiently high-dimensional than a large class of\nobjects in this space can indeed be easily learned from few examples provided\nthat certain data non-concentration conditions are met.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 14:47:05 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 12:25:59 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tyukin", "Ivan Y.", ""], ["Gorban", "Alexander N.", ""], ["Alkhudaydi", "Muhammad H.", ""], ["Zhou", "Qinghua", ""]]}, {"id": "2104.12232", "submitter": "Subhabrata Sen", "authors": "Sumit Mukherjee and Subhabrata Sen", "title": "Variational Inference in high-dimensional linear regression", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional Bayesian linear regression with product priors.\nUsing the nascent theory of non-linear large deviations (Chatterjee and\nDembo,2016), we derive sufficient conditions for the leading-order correctness\nof the naive mean-field approximation to the log-normalizing constant of the\nposterior distribution. Subsequently, assuming a true linear model for the\nobserved data, we derive a limiting infinite dimensional variational formula\nfor the log normalizing constant of the posterior. Furthermore, we establish\nthat under an additional \"separation\" condition, the variational problem has a\nunique optimizer, and this optimizer governs the probabilistic properties of\nthe posterior distribution. We provide intuitive sufficient conditions for the\nvalidity of this \"separation\" condition. Finally, we illustrate our results on\nconcrete examples with specific design matrices.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 19:09:38 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Mukherjee", "Sumit", ""], ["Sen", "Subhabrata", ""]]}, {"id": "2104.12260", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban", "title": "Consistency of invariance-based randomization tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Invariance-based randomization tests -- such as permutation tests -- are an\nimportant and widely used class of statistical methods. They allow drawing\ninferences with few assumptions on the data distribution. Most work focuses on\ntheir type I error control properties, while their consistency properties are\nmuch less understood.\n  We develop a general framework and a set of results on the consistency of\ninvariance-based randomization tests in signal-plus-noise models. Our framework\nis grounded in the deep mathematical area of representation theory. We allow\nthe transforms to be general compact topological groups, such as rotation\ngroups. Moreover, we allow actions by general linear group representations.\n  We apply our framework to a number of fundamental and highly important\nproblems in statistics, including sparse vector detection, testing for low-rank\nmatrices in noise, sparse detection in linear regression, symmetric submatrix\ndetection, and two-sample testing. Perhaps surprisingly, we find that\nrandomization tests can adapt to problem structure and detect signals at the\nsame rate as tests with full knowledge of the noise distribution.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 21:21:36 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Dobriban", "Edgar", ""]]}, {"id": "2104.12314", "submitter": "Wanli Qiao", "authors": "Wanli Qiao and Wolfgang Polonik", "title": "Algorithms for ridge estimation with convergence guarantees", "comments": "41 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of filamentary structure from a point cloud is discussed. The\nfilaments are modeled as ridge lines or higher dimensional ridges of an\nunderlying density. We propose two novel algorithms, and provide theoretical\nguarantees for their convergences. We consider the new algorithms as\nalternatives to the Subspace Constraint Mean Shift (SCMS) algorithm that do not\nsuffer from a shortcoming of the SCMS that is also revealed in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 01:57:04 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Qiao", "Wanli", ""], ["Polonik", "Wolfgang", ""]]}, {"id": "2104.12552", "submitter": "Koki Shimizu", "authors": "Aya Shinozaki, Koki Shimizu, Hiroki Hashiguchi", "title": "Generalized heterogeneous hypergeometric functions and the distribution\n  of the largest eigenvalue of an elliptical Wishart matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we derive the exact distributions of eigenvalues of a singular\nWishart matrix under an elliptical model. We define generalized heterogeneous\nhypergeometric functions with two matrix arguments and provide convergence\nconditions for these functions. The joint density of eigenvalues and the\ndistribution function of the largest eigenvalue for a singular elliptical\nWishart matrix are represented by these functions. Numerical computations for\nthe distribution of the largest eigenvalue were conducted under the\nmatrix-variate $t$ and Kotz-type models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 13:20:39 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Shinozaki", "Aya", ""], ["Shimizu", "Koki", ""], ["Hashiguchi", "Hiroki", ""]]}, {"id": "2104.12588", "submitter": "Dursun Bulutoglu A", "authors": "Dursun A. Bulutoglu, Kenneth J. Ryan", "title": "Algorithms for finding generalized minimum aberration designs", "comments": "15 pages. arXiv admin note: text overlap with arXiv:1501.02281", "journal-ref": "Journal of Complexity 31 (4) (2015) 577-589", "doi": "10.1016/j.jco.2014.12.001", "report-no": null, "categories": "stat.CO math.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical design of experiments is widely used in scientific and industrial\ninvestigations. A generalized minimum aberration (GMA) orthogonal array is\noptimum under the well-established, so-called GMA criterion, and such an array\ncan extract as much information as possible at a fixed cost. Finding GMA arrays\nis an open (yet fundamental) problem in design of experiments because\nconstructing such arrays becomes intractable as the number of runs and factors\nincrease. We develop two directed enumeration algorithms that call the integer\nprogramming with isomorphism pruning algorithm of Margot (2007) for the purpose\nof finding GMA arrays. Our results include 16 GMA arrays that were not\npreviously in the literature, along with documentation of the efficiencies that\nmade the required calculations possible within a reasonable budget of computer\ntime. We also validate heuristic algorithms against a GMA array catalog, by\nshowing that they quickly output near GMA arrays, and then use the heuristics\nto find near GMA arrays when enumeration is computationally burdensome.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:42:43 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bulutoglu", "Dursun A.", ""], ["Ryan", "Kenneth J.", ""]]}, {"id": "2104.12597", "submitter": "Benedikt M. P\\\"otscher", "authors": "Benedikt M. P\\\"otscher and David Preinerstorfer", "title": "Valid Heteroskedasticity Robust Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Tests based on heteroskedasticity robust standard errors are an important\ntechnique in econometric practice. Choosing the right critical value, however,\nis not all that simple: Conventional critical values based on asymptotics often\nlead to severe size distortions; and so do existing adjustments including the\nbootstrap. To avoid these issues, we suggest to use smallest size-controlling\ncritical values, the generic existence of which we prove in this article.\nFurthermore, sufficient and often also necessary conditions for their existence\nare given that are easy to check. Granted their existence, these critical\nvalues are the canonical choice: larger critical values result in unnecessary\npower loss, whereas smaller critical values lead to over-rejections under the\nnull hypothesis, make spurious discoveries more likely, and thus are invalid.\nWe suggest algorithms to numerically determine the proposed critical values and\nprovide implementations in accompanying software. Finally, we numerically study\nthe behavior of the proposed testing procedures, including their power\nproperties.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:01:40 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Preinerstorfer", "David", ""]]}, {"id": "2104.12929", "submitter": "Xiaohui Chen", "authors": "Jinyuan Chang, Xiaohui Chen, Mingcong Wu", "title": "Central Limit Theorems for High Dimensional Dependent Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by statistical inference problems in high-dimensional time series\nanalysis, we derive non-asymptotic error bounds for Gaussian approximations of\nsums of high-dimensional dependent random vectors on hyper-rectangles, simple\nconvex sets and sparsely convex sets. We investigate the quantitative effect of\ntemporal dependence on the rates of convergence to normality over three\ndifferent dependency frameworks ($\\alpha$-mixing, $m$-dependent, and physical\ndependence measure). In particular, we establish new error bounds under the\n$\\alpha$-mixing framework and derive faster rate over existing results under\nthe physical dependence measure. To implement the proposed results in practical\nstatistical inference problems, we also derive a data-driven parametric\nbootstrap procedure based on a kernel-type estimator for the long-run\ncovariance matrices.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 01:08:27 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Chang", "Jinyuan", ""], ["Chen", "Xiaohui", ""], ["Wu", "Mingcong", ""]]}, {"id": "2104.12938", "submitter": "Matieyendou Lamboni", "authors": "Matieyendou Lamboni", "title": "On dependent generalized sensitivity indices and asymptotic\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we propose a novel methodology for better performing\nuncertainty and sensitivity analysis for complex mathematical models under\nconstraints and/or with dependent input variables, including correlated\nvariables. Our approach allows for assessing the single, overall and\ninteractions effects of any subset of input variables, that account for the\ndependencies structures inferred by the constraints. Using the variance as\nimportance measure among others, we define the main-effect and total\nsensitivity indices of input(s) with the former index less than the latter. We\nalso derive the consistent estimators and asymptotic distributions of such\nindices by distinguishing the case of the multivariate and/or functional\noutputs, including spatio-temporal models and dynamic models.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 01:57:56 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Lamboni", "Matieyendou", ""]]}, {"id": "2104.13132", "submitter": "Michael Frank", "authors": "Lutz Klotz and Michael Frank", "title": "Stability of trigonometric approximation in $L^p$ and applications to\n  prediction theory", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\Gamma$ be an LCA group and $(\\mu_n)$ be a sequence of bounded regular\nBorel measures on $\\Gamma$ tending to a measure $\\mu_0$. Let $G$ be the dual\ngroup of $\\Gamma$, $S$ be a non-empty subset of $G \\setminus \\{ 0 \\}$, and\n$[{\\mathcal T}(S)]_{\\mu_n,p}$ the subspace of $L^p(\\mu_n)$, $p \\in (0,\\infty)$,\nspanned by the characters of $\\Gamma$ which are generated by the elements of\n$S$. The limit behaviour of the sequence of metric projections of the function\n$1$ onto $[{\\mathcal T}(S)]_{\\mu_n,p}$ as well as of the sequence of the\ncorresponding approximation errors are studied. The results are applied to\nobtain stability theorems for prediction of weakly stationary or harmonizable\nsymmetric $p$-stable stochastic processes. Along with the general problem the\nparticular cases of linear interpolation or extrapolation as well as of a\nfinite or periodic observation set are studied in detail and compared to each\nother.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 12:22:57 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Klotz", "Lutz", ""], ["Frank", "Michael", ""]]}, {"id": "2104.13140", "submitter": "John Kent", "authors": "Kanti V. Mardia and Stuart Barber and Philippa M. Burdett and John T.\n  Kent and Thomas Hamelryck", "title": "Mixture models for spherical data with applications to protein\n  bioinformatics", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finite mixture models are fitted to spherical data. Kent distributions are\nused for the components of the mixture because they allow considerable\nflexibility. Previous work on such mixtures has used an approximate maximum\nlikelihood estimator for the parameters of a single component. However, the\napproximation causes problems when using the EM algorithm to estimate the\nparameters in a mixture model. Hence the exact maximum likelihood estimator is\nused here for the individual components. This paper is motivated by a\nchallenging prize problem in structural bioinformatics of how proteins fold. It\nis known that hydrogen bonds play a key role in the folding of a protein. We\nexplore this hydrogen bond geometry using a data set describing bonds between\ntwo amino acids in proteins. An appropriate coordinate system to represent the\nhydrogen bond geometry is proposed, with each bond represented as a point on a\nsphere. We fit mixtures of Kent distributions to different subsets of the\nhydrogen bond data to gain insight into how the secondary structure elements\nbond together, since the distribution of hydrogen bonds depends on which\nsecondary structure elements are involved.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 12:34:53 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Mardia", "Kanti V.", ""], ["Barber", "Stuart", ""], ["Burdett", "Philippa M.", ""], ["Kent", "John T.", ""], ["Hamelryck", "Thomas", ""]]}, {"id": "2104.13208", "submitter": "Jean-Jil Duchamps", "authors": "Cl\\'ement Dombry and Jean-Jil Duchamps", "title": "Infinitesimal gradient boosting", "comments": "44 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We define infinitesimal gradient boosting as a limit of the popular\ntree-based gradient boosting algorithm from machine learning. The limit is\nconsidered in the vanishing-learning-rate asymptotic, that is when the learning\nrate tends to zero and the number of gradient trees is rescaled accordingly.\nFor this purpose, we introduce a new class of randomized regression trees\nbridging totally randomized trees and Extra Trees and using a softmax\ndistribution for binary splitting. Our main result is the convergence of the\nassociated stochastic algorithm and the characterization of the limiting\nprocedure as the unique solution of a nonlinear ordinary differential equation\nin a infinite dimensional function space. Infinitesimal gradient boosting\ndefines a smooth path in the space of continuous functions along which the\ntraining error decreases, the residuals remain centered and the total variation\nis well controlled.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:09:05 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Dombry", "Cl\u00e9ment", ""], ["Duchamps", "Jean-Jil", ""]]}, {"id": "2104.13281", "submitter": "Leon Bungert", "authors": "Leon Bungert, Philipp Wacker", "title": "Complete Deterministic Dynamics and Spectral Decomposition of the\n  Ensemble Kalman Inversion", "comments": "Extended the analysis to mean field and averaged stochastic EKI", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ensemble Kalman inversion (EKI), proposed by Iglesias et al. for the\nsolution of Bayesian inverse problems of type $y=A u^\\dagger +\\varepsilon$,\nwith $u^\\dagger$ being an unknown parameter and $y$ a given datum, is a\npowerful tool usually derived from a sequential Monte Carlo point of view. It\ndescribes the dynamics of an ensemble of particles $\\{u^j(t)\\}_{j=1}^J$, whose\ninitial empirical measure is sampled from the prior, evolving over an\nartificial time $t$ towards an approximate solution of the inverse problem.\n  Using spectral techniques, we provide a complete description of the\n\\new{deterministic} dynamics of EKI and their asymptotic behavior in parameter\nspace. In particular, we analyze dynamics of deterministic EKI, averaged\nquantities of stochastic EKI, and mean-field EKI. We show that in the linear\nGaussian regime, the Bayesian posterior can only be recovered with the\nmean-field limit and not with finite sample sizes or deterministic EKI.\nFurthermore, we show that -- even in the deterministic case -- residuals in\nparameter space do not decrease monotonously in the Euclidean norm and suggest\na problem-adapted norm, where monotonicity can be proved. Finally, we derive a\nsystem of ordinary differential equations governing the spectrum and\neigenvectors of the covariance matrix.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 15:49:42 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 17:47:38 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Bungert", "Leon", ""], ["Wacker", "Philipp", ""]]}, {"id": "2104.13419", "submitter": "Bryant Davis", "authors": "Bryant Davis and James P. Hobert", "title": "Approximating the Spectral Gap of the P\\'olya-Gamma Gibbs Sampler", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The self-adjoint, positive Markov operator defined by the P\\'olya-Gamma Gibbs\nsampler (under a proper normal prior) is shown to be trace-class, which implies\nthat all non-zero elements of its spectrum are eigenvalues. Consequently, the\nspectral gap is $1-\\lambda_*$, where $\\lambda_* \\in [0,1)$ is the second\nlargest eigenvalue. A method of constructing an asymptotically valid confidence\ninterval for an upper bound on $\\lambda_*$ is developed by adapting the\nclassical Monte Carlo technique of Qin et al. (2019) to the P\\'olya-Gamma Gibbs\nsampler. The results are illustrated using the German credit data. It is also\nshown that, in general, uniform ergodicity does not imply the trace-class\nproperty, nor does the trace-class property imply uniform ergodicity.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 18:34:34 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Davis", "Bryant", ""], ["Hobert", "James P.", ""]]}, {"id": "2104.13440", "submitter": "Lorenzo Trapani", "authors": "Lajos Horvath and Lorenzo Trapani", "title": "Changepoint detection in random coefficient autoregressive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.OT stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a family of CUSUM-based statistics to detect the presence of\nchangepoints in the deterministic part of the autoregressive parameter in a\nRandom Coefficient AutoRegressive (RCA) sequence. In order to ensure the\nability to detect breaks at sample endpoints, we thoroughly study weighted\nCUSUM statistics, analysing the asymptotics for virtually all possible weighing\nschemes, including the standardised CUSUM process (for which we derive a\nDarling-Erdos theorem) and even heavier weights (studying the so-called R\\'enyi\nstatistics). Our results are valid irrespective of whether the sequence is\nstationary or not, and no prior knowledge of stationarity or lack thereof is\nrequired. Technically, our results require strong approximations which, in the\nnonstationary case, are entirely new. Similarly, we allow for\nheteroskedasticity of unknown form in both the error term and in the stochastic\npart of the autoregressive coefficient, proposing a family of test statistics\nwhich are robust to heteroskedasticity, without requiring any prior knowledge\nas to the presence or type thereof. Simulations show that our procedures work\nvery well in finite samples. We complement our theory with applications to\nfinancial, economic and epidemiological time series.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 19:27:33 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Horvath", "Lajos", ""], ["Trapani", "Lorenzo", ""]]}, {"id": "2104.13517", "submitter": "Ji Oon Lee", "authors": "Ji Hyung Jung, Hye Won Chung, Ji Oon Lee", "title": "Detection of Signal in the Spiked Rectangular Models", "comments": "38 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting signals in the rank-one\nsignal-plus-noise data matrix models that generalize the spiked Wishart\nmatrices. We show that the principal component analysis can be improved by\npre-transforming the matrix entries if the noise is non-Gaussian. As an\nintermediate step, we prove a sharp phase transition of the largest eigenvalues\nof spiked rectangular matrices, which extends the Baik-Ben Arous-P\\'ech\\'e\n(BBP) transition. We also propose a hypothesis test to detect the presence of\nsignal with low computational complexity, based on the linear spectral\nstatistics, which minimizes the sum of the Type-I and Type-II errors when the\nnoise is Gaussian.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 01:15:45 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Jung", "Ji Hyung", ""], ["Chung", "Hye Won", ""], ["Lee", "Ji Oon", ""]]}, {"id": "2104.13628", "submitter": "Quanquan Gu", "authors": "Yuan Cao and Quanquan Gu and Mikhail Belkin", "title": "Risk Bounds for Over-parameterized Maximum Margin Classification on\n  Sub-Gaussian Mixtures", "comments": "35 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern machine learning systems such as deep neural networks are often highly\nover-parameterized so that they can fit the noisy training data exactly, yet\nthey can still achieve small test errors in practice. In this paper, we study\nthis \"benign overfitting\" (Bartlett et al. (2020)) phenomenon of the maximum\nmargin classifier for linear classification problems. Specifically, we consider\ndata generated from sub-Gaussian mixtures, and provide a tight risk bound for\nthe maximum margin linear classifier in the over-parameterized setting. Our\nresults precisely characterize the condition under which benign overfitting can\noccur in linear classification problems, and improve on previous work. They\nalso have direct implications for over-parameterized logistic regression.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 08:25:16 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Cao", "Yuan", ""], ["Gu", "Quanquan", ""], ["Belkin", "Mikhail", ""]]}, {"id": "2104.13705", "submitter": "Suchandan Kayal", "authors": "Suchandan Kayal", "title": "Failure extropy, dynamic failure extropy and their weighted versions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Extropy was introduced as a dual complement of the Shannon entropy. In this\ninvestigation, we consider failure extropy and its dynamic version. Various\nbasic properties of these measures are presented. It is shown that the dynamic\nfailure extropy characterizes the distribution function uniquely. We also\nconsider weighted versions of these measures. Several virtues of the weighted\nmeasures are explored. Finally, nonparametric estimators are introduced based\non the empirical distribution function.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 11:05:37 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Kayal", "Suchandan", ""]]}, {"id": "2104.13753", "submitter": "Alexander Dunlap", "authors": "Alexander Dunlap and Jean-Christophe Mourrat", "title": "Sum-of-norms clustering does not separate nearby balls", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-of-norms clustering is a popular convexification of $K$-means clustering.\nWe show that, if the dataset is made of a large number of independent random\nvariables distributed according to the uniform measure on the union of two\ndisjoint balls of unit radius, and if the balls are sufficiently close to one\nanother, then sum-of-norms clustering will typically fail to recover the\ndecomposition of the dataset into two clusters. As the dimension tends to\ninfinity, this happens even when the distance between the centers of the two\nballs is taken to be as large as $2\\sqrt{2}$. In order to show this, we\nintroduce and analyze a continuous version of sum-of-norms clustering, where\nthe dataset is replaced by a general measure. In particular, we state and prove\na local-global characterization of the clustering that seems to be new even in\nthe case of discrete datapoints.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 13:35:17 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Dunlap", "Alexander", ""], ["Mourrat", "Jean-Christophe", ""]]}, {"id": "2104.13789", "submitter": "Kengne William", "authors": "Mamadou Lamine Diop and William Kengne", "title": "A general procedure for change-point detection in multivariate time\n  series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the change-point detection in multivariate continuous and integer\nvalued time series. We propose a Wald-type statistic based on the estimator\nperformed by a general contrast function; which can be constructed from the\nlikelihood, a quasi-likelihood, a least squares method, etc.\n  Sufficient conditions are provided to ensure that the statistic convergences\nto a well-known distribution under the null hypothesis (of no change) and\ndiverges to infinity under the alternative; which establishes the consistency\nof the procedure. Some examples are detailed to illustrate the scope of\napplication of the proposed procedure.\n  Simulation experiments are conducted to illustrate the asymptotic results.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:23:29 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Diop", "Mamadou Lamine", ""], ["Kengne", "William", ""]]}, {"id": "2104.13796", "submitter": "Bennet Str\\\"oh", "authors": "Annemarie Bitter, Robert Stelzer and Bennet Str\\\"oh", "title": "Continuous-time locally stationary time series models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the classical definition of locally stationary processes in\ndiscrete-time to the continuous-time setting and obtain equivalent\nrepresentations in the time and frequency domain. From this, a unique\ntime-varying spectral density is derived using the Wigner-Ville spectrum. As an\nexample, we investigate time-varying L\\'evy-driven state space processes,\nincluding the class of time-varying L\\'evy-driven CARMA processes. First, the\nconnection between these two classes of processes is examined. Considering a\nsequence of time-varying L\\'evy-driven state space processes, we then give\nsufficient conditions on the coefficient functions that ensure local\nstationarity with respect to the given definition.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:34:03 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Bitter", "Annemarie", ""], ["Stelzer", "Robert", ""], ["Str\u00f6h", "Bennet", ""]]}, {"id": "2104.13855", "submitter": "Jorge Ignacio Gonz\\'alez C\\'azares", "authors": "David Bang, Jorge Ignacio Gonz\\'alez C\\'azares, Aleksandar Mijatovi\\'c", "title": "A Gaussian approximation theorem for L\\'evy processes", "comments": "4 pages", "journal-ref": "Statistics & Probability Letters (2021)", "doi": "10.1016/j.spl.2021.109187", "report-no": "109187", "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Without higher moment assumptions, this note establishes the decay of the\nKolmogorov distance in a central limit theorem for L\\'evy processes. This\ntheorem can be viewed as a continuous-time extension of the classical random\nwalk result by Friedman, Katz and Koopmans.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:02:18 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 21:19:57 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bang", "David", ""], ["C\u00e1zares", "Jorge Ignacio Gonz\u00e1lez", ""], ["Mijatovi\u0107", "Aleksandar", ""]]}, {"id": "2104.13881", "submitter": "Jason Klusowski M", "authors": "Jason M. Klusowski", "title": "Universal Consistency of Decision Trees in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper shows that decision trees constructed with Classification and\nRegression Trees (CART) methodology are universally consistent in an additive\nmodel context, even when the number of predictor variables scales exponentially\nwith the sample size, under certain $1$-norm sparsity constraints. The\nconsistency is universal in the sense that there are no a priori assumptions on\nthe distribution of the predictor variables. Amazingly, this adaptivity to\n(approximate or exact) sparsity is achieved with a single tree, as opposed to\nwhat might be expected for an ensemble. Finally, we show that these qualitative\nproperties of individual trees are inherited by Breiman's random forests.\nAnother surprise is that consistency holds even when the \"mtry\" tuning\nparameter vanishes as a fraction of the number of predictor variables, thus\nspeeding up computation of the forest. A key step in the analysis is the\nestablishment of an oracle inequality, which precisely characterizes the\ngoodness-of-fit and complexity tradeoff for a misspecified model.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:59:03 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 16:04:44 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 14:32:02 GMT"}, {"version": "v4", "created": "Mon, 21 Jun 2021 01:31:29 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Klusowski", "Jason M.", ""]]}, {"id": "2104.14012", "submitter": "Leszek Szczecinski", "authors": "Leszek Szczecinski and Rapha\\\"elle Tihon", "title": "Simplified Kalman filter for online rating: one-fits-all approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we deal with the problem of rating in sports, where the skills\nof the players/teams are inferred from the observed outcomes of the games. Our\nfocus is on the online rating algorithms which estimate the skills after each\nnew game by exploiting the probabilistic models of the relationship between the\nskills and the game outcome. We propose a Bayesian approach which may be seen\nas an approximate Kalman filter and which is generic in the sense that it can\nbe used with any skills-outcome model and can be applied in the individual --\nas well as in the group-sports. We show how the well-know algorithms (such as\nthe Elo, the Glicko, and the TrueSkill algorithms) may be seen as instances of\nthe one-fits-all approach we propose. In order to clarify the conditions under\nwhich the gains of the Bayesian approach over the simpler solutions can\nactually materialize, we critically compare the known and the new algorithms by\nmeans of numerical examples using the synthetic as well as the empirical data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 20:44:10 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Szczecinski", "Leszek", ""], ["Tihon", "Rapha\u00eblle", ""]]}, {"id": "2104.14023", "submitter": "Gilles Mordant", "authors": "Gilles Mordant, Johan Segers", "title": "Measuring dependence between random vectors via optimal transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To quantify the dependence between two random vectors of possibly different\ndimensions, we propose to rely on the properties of the 2-Wasserstein distance.\nWe first propose two coefficients that are based on the Wasserstein distance\nbetween the actual distribution and a reference distribution with independent\ncomponents. The coefficients are normalized to take values between 0 and 1,\nwhere 1 represents the maximal amount of dependence possible given the two\nmultivariate margins. We then make a quasi-Gaussian assumption that yields two\nadditional coefficients rooted in the same ideas as the first two. These\ndifferent coefficients are more amenable for distributional results and admit\nattractive formulas in terms of the joint covariance or correlation matrix.\nFurthermore, maximal dependence is proved to occur at the covariance matrix\nwith minimal von Neumann entropy given the covariance matrices of the two\nmultivariate margins. This result also helps us revisit the RV coefficient by\nproposing a sharper normalisation. The two coefficients based on the\nquasi-Gaussian approach can be estimated easily via the empirical covariance\nmatrix. The estimators are asymptotically normal and their asymptotic variances\nare explicit functions of the covariance matrix, which can thus be estimated\nconsistently too. The results extend to the Gaussian copula case, in which case\nthe estimators are rank-based. The results are illustrated through theoretical\nexamples, Monte Carlo simulations, and a case study involving\nelectroencephalography data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 21:11:43 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Mordant", "Gilles", ""], ["Segers", "Johan", ""]]}, {"id": "2104.14071", "submitter": "Haijun Li", "authors": "Haijun Li", "title": "On Rapid Variation of Multivariate Probability Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate rapid variation describes decay rates of joint light tails of a\nmultivariate distribution. We impose a local uniformity condition to control\ndecay variation of distribution tails along different directions, and using\nhigher-order tail dependence of copulas, we prove that a rapidly varying\nmultivariate density implies rapid variation of the joint distribution tails.\nAs a corollary, rapid variation of skew-elliptical distributions is established\nunder the assumption that the underlying density generators belong to the\nmax-domain of attraction of the Gumbel distribution.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 01:42:04 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Li", "Haijun", ""]]}, {"id": "2104.14091", "submitter": "Manjari Das", "authors": "Manjari Das and Edward H. Kennedy", "title": "Doubly robust capture-recapture methods for estimating population size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimation of population size using incomplete lists (also called the\ncapture-recapture problem) has a long history across many biological and social\nsciences. For example, human rights and other groups often construct partial\nand overlapping lists of victims of armed conflicts, with the hope of using\nthis information to estimate the total number of victims. Earlier statistical\nmethods for this setup either use potentially restrictive parametric\nassumptions, or else rely on typically suboptimal plug-in-type nonparametric\nestimators; however, both approaches can lead to substantial bias, the former\nvia model misspecification and the latter via smoothing. Under an identifying\nassumption that two lists are conditionally independent given measured\ncovariate information, we make several contributions. First we derive the\nnonparametric efficiency bound for estimating the capture probability, which\nindicates the best possible performance of any estimator, and sheds light on\nthe statistical limits of capture-recapture methods. Then we present a new\nestimator, and study its finite-sample properties, showing that it has a double\nrobustness property new to capture-recapture, and that it is near-optimal in a\nnon-asymptotic sense, under relatively mild nonparametric conditions. Next, we\ngive a method for constructing confidence intervals for total population size\nfrom generic capture probability estimators, and prove non-asymptotic\nnear-validity. Finally, we study our methods in simulations, and apply them to\nestimate the number of killings and disappearances attributable to different\ngroups in Peru during its internal armed conflict between 1980 and 2000.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 03:24:47 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Das", "Manjari", ""], ["Kennedy", "Edward H.", ""]]}, {"id": "2104.14706", "submitter": "Yonglong Li", "authors": "Yonglong Li, Vincent Y. F. Tan, and Marco Tomamichel", "title": "Optimal Adaptive Strategies for Sequential Quantum Hypothesis Testing", "comments": "31 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.IT math-ph math.IT math.MP math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We consider sequential hypothesis testing between two quantum states using\nadaptive and non-adaptive strategies. In this setting, samples of an unknown\nstate are requested sequentially and a decision to either continue or to accept\none of the two hypotheses is made after each test. Under the constraint that\nthe number of samples is bounded, either in expectation or with high\nprobability, we exhibit adaptive strategies that minimize both types of\nmisidentification errors. Namely, we show that these errors decrease\nexponentially (in the stopping time) with decay rates given by the measured\nrelative entropies between the two states. Moreover, if we allow joint\nmeasurements on multiple samples, the rates are increased to the respective\nquantum relative entropies. We also fully characterize the achievable error\nexponents for non-adaptive strategies and provide numerical evidence showing\nthat adaptive measurements are necessary to achieve our bounds under some\nadditional assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 00:52:48 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Li", "Yonglong", ""], ["Tan", "Vincent Y. F.", ""], ["Tomamichel", "Marco", ""]]}, {"id": "2104.14712", "submitter": "Youngjo Lee", "authors": "Yudi Pawitan, Hangbin Lee and Youngjo Lee", "title": "Epistemic confidence, the Dutch Book and relevant subsets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use a logical device called the Dutch Book to establish epistemic\nconfidence, defined as the sense of confidence \\emph{in an observed} confidence\ninterval. This epistemic property is unavailable -- or even denied -- in\northodox frequentist inference. In financial markets, including the betting\nmarket, the Dutch Book is also known as arbitrage or risk-free profitable\ntransaction. A numerical confidence is deemed epistemic if its use as a betting\nprice is protected from the Dutch Book by an external agent. Theoretically, to\nconstruct the Dutch Book, the agent must exploit unused information available\nin any relevant subset. Pawitan and Lee (2021) showed that confidence is an\nextended likelihood, and the likelihood principle states that the likelihood\ncontains all the information in the data, hence leaving no relevant subset.\nIntuitively, this implies that confidence associated with the full likelihood\nis protected from the Dutch Book, and hence is epistemic. Our aim is to provide\nthe theoretical support for this intuitive notion.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 01:11:13 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 05:52:02 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Pawitan", "Yudi", ""], ["Lee", "Hangbin", ""], ["Lee", "Youngjo", ""]]}, {"id": "2104.14737", "submitter": "Whitney Newey", "authors": "Victor Chernozhukov, Whitney K. Newey, Victor Quintas-Martinez,\n  Vasilis Syrgkanis", "title": "Automatic Debiased Machine Learning via Neural Nets for Generalized\n  Linear Regression", "comments": "arXiv admin note: text overlap with arXiv:1809.05224", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give debiased machine learners of parameters of interest that depend on\ngeneralized linear regressions, which regressions make a residual orthogonal to\nregressors. The parameters of interest include many causal and policy effects.\nWe give neural net learners of the bias correction that are automatic in only\ndepending on the object of interest and the regression residual. Convergence\nrates are given for these neural nets and for more general learners of the bias\ncorrection. We also give conditions for asymptotic normality and consistent\nasymptotic variance estimation of the learner of the object of interest.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 03:18:54 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Newey", "Whitney K.", ""], ["Quintas-Martinez", "Victor", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "2104.14888", "submitter": "B.L.S. Prakasa Rao", "authors": "B.L.S. Prakasa Rao", "title": "Maximum likelihood estimation for stochastic differential equations\n  driven by a mixed fractional Brownian motion with random effects", "comments": "arXiv admin note: substantial text overlap with arXiv:1902.08375,\n  arXiv:2103.05264", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We discuss maximum likelihood estimation of parameters for models governed by\na stochastic differential equation driven by a mixed fractional Brownian motion\nwith random effects.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 10:21:36 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Rao", "B. L. S. Prakasa", ""]]}, {"id": "2104.14977", "submitter": "Yikun Zhang", "authors": "Yikun Zhang and Yen-Chi Chen", "title": "Linear Convergence of the Subspace Constrained Mean Shift Algorithm:\n  From Euclidean to Directional Data", "comments": "75 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies linear convergence of the subspace constrained mean shift\n(SCMS) algorithm, a well-known algorithm for identifying a density ridge\ndefined by a kernel density estimator. By arguing that the SCMS algorithm is a\nspecial variant of a subspace constrained gradient ascent (SCGA) algorithm with\nan adaptive step size, we derive linear convergence of such SCGA algorithm.\nWhile the existing research focuses mainly on density ridges in the Euclidean\nspace, we generalize density ridges and the SCMS algorithm to directional data.\nIn particular, we establish the stability theorem of density ridges with\ndirectional data and prove the linear convergence of our proposed directional\nSCMS algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 01:46:35 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhang", "Yikun", ""], ["Chen", "Yen-Chi", ""]]}, {"id": "2104.15127", "submitter": "Michael J. Feldman", "authors": "Michael J. Feldman", "title": "Spiked Singular Values and Vectors under Extreme Aspect Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The behavior of the leading singular values and vectors of noisy low-rank\nmatrices is fundamental to many statistical and scientific problems.\nTheoretical understanding currently derives from asymptotic analysis under one\nof two regimes: ${\\it classical}$, with a fixed number of rows, large number of\ncolumns or vice versa; and ${\\it proportional}$, with large numbers of rows and\ncolumns, proportional to one another. This paper is concerned with the ${\\it\ndisproportional}$ regime, where the matrix is either `tall and narrow' or\n`short and wide': we study sequences of matrices of size $n \\times m_n$ with\naspect ratio $ n/m_n \\rightarrow 0$ or $n/m_n \\rightarrow \\infty$ as $n\n\\rightarrow \\infty$. This regime has important `big data' applications.\n  Theory derived here shows that the displacement of the empirical singular\nvalues and vectors from their noise-free counterparts and the associated phase\ntransitions -- well-known under proportional growth asymptotics -- still occur\nin the disproportionate setting. They must be quantified, however, on a novel\nscale of measurement that adjusts with the changing aspect ratio as the matrix\nsize increases. In this setting, the top singular vectors corresponding to the\nlonger of the two matrix dimensions are asymptotically uncorrelated with the\nnoise-free signal.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 17:31:10 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 17:47:55 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Feldman", "Michael J.", ""]]}, {"id": "2104.15140", "submitter": "Nabarun Deb", "authors": "Arnab Auddy, Nabarun Deb, and Sagnik Nandy", "title": "Exact Detection Thresholds for Chatterjee's Correlation", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Chatterjee (2021) introduced a new rank-based correlation\ncoefficient which can be used to test for independence between two random\nvariables. His test has already attracted much attention as it is\ndistribution-free, consistent against all fixed alternatives, asymptotically\nnormal under the null hypothesis of independence and computable in (near)\nlinear time; thereby making it appropriate for large-scale applications.\nHowever, not much is known about the power properties of this test beyond\nconsistency against fixed alternatives. In this paper, we bridge this gap by\nobtaining the asymptotic distribution of Chatterjee's correlation under any\nchanging sequence of alternatives \"converging\" to the null hypothesis (of\nindependence). We further obtain a general result that gives exact detection\nthresholds and limiting power for Chatterjee's test of independence under\nnatural nonparametric alternatives \"converging\" to the null. As applications of\nthis general result, we prove a non-standard $n^{-1/4}$ detection boundary for\nthis test and compute explicitly the limiting local power on the detection\nboundary, for popularly studied alternatives in literature such as mixture\nmodels, rotation models and noisy nonparametric regression. Moreover our\nconvergence results provide explicit finite sample bounds that depend on the\n\"distance\" between the null and the alternative. Our proof techniques rely on\nsecond order Poincar\\'{e} type inequalities and a non-asymptotic projection\ntheorem.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 17:58:50 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Auddy", "Arnab", ""], ["Deb", "Nabarun", ""], ["Nandy", "Sagnik", ""]]}]