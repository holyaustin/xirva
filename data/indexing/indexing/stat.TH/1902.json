[{"id": "1902.00050", "submitter": "Bruno Remillard", "authors": "Bouchra R. Nasri and Bruno N. Remillard", "title": "On the monotonicity of copula-based conditional distributions", "comments": "Some work has been done by other authors. We need to add these\n  references and modify the paper accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we find necessary and sufficient conditions so that\ncopula-based conditional distributions of a response variable with respect to\ncovariates, are ordered with respect to the simple stochastic order introduced\nby Lehmann. These conditions do not depend on the marginal distributions of the\nrandom variables. As a result, we have conditions to ensure that the\nconditional mean and conditional quantiles are monotonic with respect to the\ncovariates.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 19:55:42 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 19:52:14 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Nasri", "Bouchra R.", ""], ["Remillard", "Bruno N.", ""]]}, {"id": "1902.00080", "submitter": "Geoffrey Wolfer", "authors": "Geoffrey Wolfer and Aryeh Kontorovich", "title": "Minimax Testing of Identity to a Reference Ergodic Markov Chain", "comments": "A previous version of this print contained a mistake in a proof. We\n  have now fixed it", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exhibit an efficient procedure for testing, based on a single long state\nsequence, whether an unknown Markov chain is identical to or $\\varepsilon$-far\nfrom a given reference chain. We obtain nearly matching (up to logarithmic\nfactors) upper and lower sample complexity bounds for our notion of distance,\nwhich is based on total variation. Perhaps surprisingly, we discover that the\nsample complexity depends solely on the properties of the known reference chain\nand does not involve the unknown chain at all, which is not even assumed to be\nergodic.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 21:24:54 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 11:21:16 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 18:52:29 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Wolfer", "Geoffrey", ""], ["Kontorovich", "Aryeh", ""]]}, {"id": "1902.00104", "submitter": "Enrico Au-Yeung", "authors": "Enrico Au-Yeung, Greg Zanotti", "title": "Phase Transition in the Recovery of Rank One Matrices Corrupted by\n  Gaussian Noise", "comments": "This version is a replacement of the earlier version of the\n  manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In datasets where the number of parameters is fixed and the number of samples\nis large, principal component analysis (PCA) is a powerful dimension reduction\ntool. However, in many contemporary datasets, when the number of parameters is\ncomparable to the sample size, PCA can be misleading. A closely related problem\nis the following: is it possible to recover a rank-one matrix in the presence\nof a large amount of noise? In both situations, there is a phase transition in\nthe eigen-structure of the matrix.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2019 22:24:21 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 05:28:19 GMT"}, {"version": "v3", "created": "Wed, 13 Feb 2019 07:06:46 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Au-Yeung", "Enrico", ""], ["Zanotti", "Greg", ""]]}, {"id": "1902.00139", "submitter": "Stefano Sarao Mannelli", "authors": "Stefano Sarao Mannelli, Florent Krzakala, Pierfrancesco Urbani, and\n  Lenka Zdeborov\\'a", "title": "Passed & Spurious: Descent Algorithms and Local Minima in Spiked\n  Matrix-Tensor Models", "comments": "12 pages + appendix, 10 figures. Appears in Proceedings of the\n  International Conference on Machine Learning (ICML 2019)", "journal-ref": "International Conference on Machine Learning, 4333-4342 (ICML\n  2019)", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we analyse quantitatively the interplay between the loss\nlandscape and performance of descent algorithms in a prototypical inference\nproblem, the spiked matrix-tensor model. We study a loss function that is the\nnegative log-likelihood of the model. We analyse the number of local minima at\na fixed distance from the signal/spike with the Kac-Rice formula, and locate\ntrivialization of the landscape at large signal-to-noise ratios. We evaluate in\na closed form the performance of a gradient flow algorithm using\nintegro-differential PDEs as developed in physics of disordered systems for the\nLangevin dynamics. We analyze the performance of an approximate message passing\nalgorithm estimating the maximum likelihood configuration via its state\nevolution. We conclude by comparing the above results: while we observe a\ndrastic slow down of the gradient flow dynamics even in the region where the\nlandscape is trivial, both the analyzed algorithms are shown to perform well\neven in the part of the region of parameters where spurious local minima are\npresent.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 00:12:37 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 09:16:43 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 08:13:43 GMT"}, {"version": "v4", "created": "Mon, 20 Jan 2020 15:10:12 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Mannelli", "Stefano Sarao", ""], ["Krzakala", "Florent", ""], ["Urbani", "Pierfrancesco", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1902.00161", "submitter": "Andrea Arf\\'e", "authors": "Andrea Arf\\'e and Brian Alexander and Lorenzo Trippa", "title": "Optimality of testing procedures for survival data", "comments": "Accepted for publication in Biometrics on May 27, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most statistical tests for treatment effects used in randomized clinical\ntrials with survival outcomes are based on the proportional hazards assumption,\nwhich often fails in practice. Data from early exploratory studies may provide\nevidence of non-proportional hazards which can guide the choice of alternative\ntests in the design of practice-changing confirmatory trials. We study a test\nto detect treatment effects in a late-stage trial which accounts for the\ndeviations from proportional hazards suggested by early-stage data. Conditional\non early-stage data, among all tests which control the frequentist Type I error\nrate at a fixed $\\alpha$ level, our testing procedure maximizes the Bayesian\nprediction of the finite-sample power. Hence, the proposed test provides a\nuseful benchmark for other tests commonly used in presence of non-proportional\nhazards, for example weighted log-rank tests. We illustrate the approach in a\nsimulations based on data from a published cancer immunotherapy phase III\ntrial.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 03:27:31 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 20:57:36 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 12:52:05 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Arf\u00e9", "Andrea", ""], ["Alexander", "Brian", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "1902.00180", "submitter": "Chul-Ho Lee", "authors": "Chul-Ho Lee, Min Kang, Do Young Eun", "title": "Non-Markovian Monte Carlo on Directed Graphs", "comments": "Accepted to appear in ACM SIGMETRICS/IFIP Performance 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Chain Monte Carlo (MCMC) has been the de facto technique for sampling\nand inference of large graphs such as online social networks. At the heart of\nMCMC lies the ability to construct an ergodic Markov chain that attains any\ngiven stationary distribution $\\boldsymbol{\\pi}$, often in the form of random\nwalks or crawling agents on the graph. Most of the works around MCMC, however,\npresume that the graph is undirected or has reciprocal edges, and become\ninapplicable when the graph is directed and non-reciprocal. Here we develop a\nsimilar framework for directed graphs, which we call Non-Markovian Monte Carlo\n(NMMC), by establishing a mapping to convert $\\boldsymbol{\\pi}$ into the\nquasi-stationary distribution of a carefully constructed transient Markov chain\non an extended state space. As applications, we demonstrate how to achieve any\ngiven distribution $\\boldsymbol{\\pi}$ on a directed graph and estimate the\neigenvector centrality using a set of non-Markovian, history-dependent random\nwalks on the same graph in a distributed manner. We also provide numerical\nresults on various real-world directed graphs to confirm our theoretical\nfindings, and present several practical enhancements to make our NMMC method\nready for practical use in most directed graphs. To the best of our knowledge,\nthe proposed NMMC framework for directed graphs is the first of its kind,\nunlocking all the limitations set by the standard MCMC methods for undirected\ngraphs.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 04:44:11 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Lee", "Chul-Ho", ""], ["Kang", "Min", ""], ["Eun", "Do Young", ""]]}, {"id": "1902.00194", "submitter": "Raaz Dwivedi", "authors": "Raaz Dwivedi, Nhat Ho, Koulik Khamaru, Martin J. Wainwright, Michael\n  I. Jordan, Bin Yu", "title": "Sharp Analysis of Expectation-Maximization for Weakly Identifiable\n  Models", "comments": "30 pages, 4 figures. The first three authors contributed equally to\n  this work. To appear in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of weakly identifiable location-scale mixture models for\nwhich the maximum likelihood estimates based on $n$ i.i.d. samples are known to\nhave lower accuracy than the classical $n^{- \\frac{1}{2}}$ error. We\ninvestigate whether the Expectation-Maximization (EM) algorithm also converges\nslowly for these models. We provide a rigorous characterization of EM for\nfitting a weakly identifiable Gaussian mixture in a univariate setting where we\nprove that the EM algorithm converges in order $n^{\\frac{3}{4}}$ steps and\nreturns estimates that are at a Euclidean distance of order ${ n^{-\n\\frac{1}{8}}}$ and ${ n^{-\\frac{1} {4}}}$ from the true location and scale\nparameter respectively. Establishing the slow rates in the univariate setting\nrequires a novel localization argument with two stages, with each stage\ninvolving an epoch-based argument applied to a different surrogate EM operator\nat the population level. We demonstrate several multivariate ($d \\geq 2$)\nexamples that exhibit the same slow rates as the univariate case. We also prove\nslow statistical rates in higher dimensions in a special case, when the fitted\ncovariance is constrained to be a multiple of the identity.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 06:07:08 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 01:22:51 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 19:42:45 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Dwivedi", "Raaz", ""], ["Ho", "Nhat", ""], ["Khamaru", "Koulik", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""], ["Yu", "Bin", ""]]}, {"id": "1902.00214", "submitter": "Alexander Kolnogorov", "authors": "Alexander Kolnogorov and Sergey Garbar", "title": "Multi-Armed Bandit Problem and Batch UCB Rule", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain the upper bound of the loss function for a strategy in the\nmulti-armed bandit problem with Gaussian distributions of incomes. Considered\nstrategy is an asymptotic generalization of the strategy proposed by J. Bather\nfor the multi-armed bandit problem and using UCB rule, i.e. choosing the action\ncorresponding to the maximum of the upper bound of the confidence interval of\nthe current estimate of the expected value of one-step income. Results are\nobtained with the help of invariant description of the control on the unit\nhorizon in the domain of close distributions because just there the loss\nfunction attains its maximal values. UCB rule is widely used in machine\nlearning. It can be also used for the batch data processing optimization if\nthere are two alternative processing methods available with different a priori\nunknown efficiencies.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 08:12:42 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Kolnogorov", "Alexander", ""], ["Garbar", "Sergey", ""]]}, {"id": "1902.00431", "submitter": "Marianna Pensky", "authors": "Majid Noroozi, Ramchandra Rimal, Marianna Pensky", "title": "Estimation and Clustering in Popularity Adjusted Stochastic Block Model", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers the Popularity Adjusted Block model (PABM) introduced by\nSengupta and Chen (2018). We argue that the main appeal of the PABM is the\nflexibility of the spectral properties of the graph which makes the PABM an\nattractive choice for modeling networks that appear in biological sciences. We\nexpand the theory of PABM to the case of an arbitrary number of communities\nwhich possibly grows with a number of nodes in the network and is not assumed\nto be known. We produce the estimators of the probability matrix and the\ncommunity structure and provide non-asymptotic upper bounds for the estimation\nand the clustering errors. We use the Sparse Subspace Clustering (SSC) approach\nto partition the network into communities, the approach that, to the best of\nour knowledge, has not been used for clustering network data. The theory is\nsupplemented by a simulation study. In addition, we show advantages of the PABM\nfor modeling a butterfly similarity network and a human brain functional\nnetwork.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 16:05:56 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2019 02:37:06 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 16:03:58 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Noroozi", "Majid", ""], ["Rimal", "Ramchandra", ""], ["Pensky", "Marianna", ""]]}, {"id": "1902.00481", "submitter": "Gleb Oshanin", "authors": "D. Krapf, N. Lukat, E. Marinari, R. Metzler, G. Oshanin, C.\n  Selhuber-Unkel, A. Squarcini, L. Stadler, M. Weiss, and X. Xu", "title": "Spectral content of a single non-Brownian trajectory", "comments": "13 pages, 5 figures, Supplemental Material can be found at\n  https://journals.aps.org/prx/supplemental/10.1103/PhysRevX.9.011019/prx_SM_final.pdf", "journal-ref": "Phys. Rev. X 9, 011019 (2019)", "doi": "10.1103/PhysRevX.9.011019", "report-no": null, "categories": "cond-mat.stat-mech math.PR math.ST physics.bio-ph stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time-dependent processes are often analysed using the power spectral density\n(PSD), calculated by taking an appropriate Fourier transform of individual\ntrajectories and finding the associated ensemble-average. Frequently, the\navailable experimental data sets are too small for such ensemble averages, and\nhence it is of a great conceptual and practical importance to understand to\nwhich extent relevant information can be gained from $S(f,T)$, the PSD of a\nsingle trajectory. Here we focus on the behavior of this random,\nrealization-dependent variable, parametrized by frequency $f$ and\nobservation-time $T$, for a broad family of anomalous diffusions---fractional\nBrownian motion (fBm) with Hurst-index $H$---and derive exactly its probability\ndensity function. We show that $S(f,T)$ is proportional---up to a random\nnumerical factor whose universal distribution we determine---to the\nensemble-averaged PSD. For subdiffusion ($H<1/2$) we find that $S(f,T)\\sim\nA/f^{2H+1}$ with random-amplitude $A$. In sharp contrast, for superdiffusion\n$(H>1/2)$ $S(f,T)\\sim BT^{2H-1}/f^2$ with random amplitude $B$. Remarkably, for\n$H>1/2$ the PSD exhibits the same frequency-dependence as Brownian motion, a\ndeceptive property that may lead to false conclusions when interpreting\nexperimental data. Notably, for $H>1/2$ the PSD is ageing and is dependent on\n$T$. Our predictions for both sub- and superdiffusion are confirmed by\nexperiments in live cells and in agarose hydrogels, and by extensive\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 17:58:24 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Krapf", "D.", ""], ["Lukat", "N.", ""], ["Marinari", "E.", ""], ["Metzler", "R.", ""], ["Oshanin", "G.", ""], ["Selhuber-Unkel", "C.", ""], ["Squarcini", "A.", ""], ["Stadler", "L.", ""], ["Weiss", "M.", ""], ["Xu", "X.", ""]]}, {"id": "1902.00535", "submitter": "Qing Zhou", "authors": "Kun Zhou, Ker-Chau Li, and Qing Zhou", "title": "Honest confidence sets for high-dimensional regression by projection and\n  shrinkage", "comments": "36 pages, 7 figures", "journal-ref": "Journal of the American Statistical Association, 2021", "doi": "10.1080/01621459.2021.1938581", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of honesty in constructing confidence sets arises in nonparametric\nregression. While optimal rate in nonparametric estimation can be achieved and\nutilized to construct sharp confidence sets, severe degradation of confidence\nlevel often happens after estimating the degree of smoothness. Similarly, for\nhigh-dimensional regression, oracle inequalities for sparse estimators could be\nutilized to construct sharp confidence sets. Yet the degree of sparsity itself\nis unknown and needs to be estimated, causing the honesty problem. To resolve\nthis issue, we develop a novel method to construct honest confidence sets for\nsparse high-dimensional linear regression. The key idea in our construction is\nto separate signals into a strong and a weak group, and then construct\nconfidence sets for each group separately. This is achieved by a projection and\nshrinkage approach, the latter implemented via Stein estimation and the\nassociated Stein unbiased risk estimate. Our confidence set is honest over the\nfull parameter space without any sparsity constraints, while its diameter\nadapts to the optimal rate of $n^{-1/4}$ when the true parameter is indeed\nsparse. Through extensive numerical comparisons, we demonstrate that our method\noutperforms other competitors with big margins for finite samples, including\noracle methods built upon the true sparsity of the underlying model.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 19:38:24 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 16:53:56 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Zhou", "Kun", ""], ["Li", "Ker-Chau", ""], ["Zhou", "Qing", ""]]}, {"id": "1902.00582", "submitter": "John Duchi", "authors": "John Duchi, Ryan Rogers", "title": "Lower Bounds for Locally Private Estimation via Communication Complexity", "comments": "To appear in Conference on Learning Theory 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop lower bounds for estimation under local privacy\nconstraints---including differential privacy and its relaxations to approximate\nor R\\'{e}nyi differential privacy---by showing an equivalence between private\nestimation and communication-restricted estimation problems. Our results apply\nto arbitrarily interactive privacy mechanisms, and they also give sharp lower\nbounds for all levels of differential privacy protections, that is, privacy\nmechanisms with privacy levels $\\varepsilon \\in [0, \\infty)$. As a particular\nconsequence of our results, we show that the minimax mean-squared error for\nestimating the mean of a bounded or Gaussian random vector in $d$ dimensions\nscales as $\\frac{d}{n} \\cdot \\frac{d}{ \\min\\{\\varepsilon, \\varepsilon^2\\}}$.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 23:01:20 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 22:48:06 GMT"}, {"version": "v3", "created": "Wed, 10 Apr 2019 16:58:18 GMT"}, {"version": "v4", "created": "Sun, 5 May 2019 19:30:02 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Duchi", "John", ""], ["Rogers", "Ryan", ""]]}, {"id": "1902.00600", "submitter": "Marc Vuffray", "authors": "Marc Vuffray, Sidhant Misra, Andrey Y. Lokhov", "title": "Efficient Learning of Discrete Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models are useful tools for describing structured high-dimensional\nprobability distributions. Development of efficient algorithms for learning\ngraphical models with least amount of data remains an active research topic.\nReconstruction of graphical models that describe the statistics of discrete\nvariables is a particularly challenging problem, for which the maximum\nlikelihood approach is intractable. In this work, we provide the first\nsample-efficient method based on the Interaction Screening framework that\nallows one to provably learn fully general discrete factor models with\nnode-specific discrete alphabets and multi-body interactions, specified in an\narbitrary basis. We identify a single condition related to model\nparametrization that leads to rigorous guarantees on the recovery of model\nstructure and parameters in any error norm, and is readily verifiable for a\nlarge class of models. Importantly, our bounds make explicit distinction\nbetween parameters that are proper to the model and priors used as an input to\nthe algorithm. Finally, we show that the Interaction Screening framework\nincludes all models previously considered in the literature as special cases,\nand for which our analysis shows a systematic improvement in sample complexity.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 00:54:57 GMT"}, {"version": "v2", "created": "Thu, 16 Jul 2020 06:31:44 GMT"}], "update_date": "2020-07-17", "authors_parsed": [["Vuffray", "Marc", ""], ["Misra", "Sidhant", ""], ["Lokhov", "Andrey Y.", ""]]}, {"id": "1902.00653", "submitter": "Catia  Scricciolo", "authors": "Catia Scricciolo", "title": "On asymptotically efficient maximum likelihood estimation of linear\n  functionals in Laplace measurement error models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimation of linear functionals in the inverse problem of\ndeconvolution is considered. Given observations of a random sample from a\ndistribution $P_0\\equiv P_{F_0}$ indexed by a (potentially\ninfinite-dimensional) parameter $F_0$, which is the distribution of the latent\nvariable in a standard additive Laplace measurement error model, one wants to\nestimate a linear functional of $F_0$. Asymptotically efficient maximum\nlikelihood estimation (MLE) of integral linear functionals of the mixing\ndistribution $F_0$ in a convolution model with the Laplace kernel density is\ninvestigated. Situations are distinguished in which the functional of interest\ncan be consistently estimated at $n^{-1/2}$-rate by the plug-in MLE, which is\nasymptotically normal and efficient, in the sense of achieving the variance\nlower bound, from those in which no integral linear functional can be estimated\nat parametric rate, which precludes any possibility for asymptotic efficiency.\nThe $\\sqrt{n}$-convergence of the MLE, valid in the case of a degenerate mixing\ndistribution at a single location point, fails in general, as does asymptotic\nnormality. It is shown that there exists no regular estimator sequence for\nintegral linear functionals of the mixing distribution that, when recentered\nabout the estimand and $\\sqrt{n}$-rescaled, is asymptotically efficient,\n\\emph{viz}., has Gaussian limit distribution with minimum variance. One can\nthus only expect estimation with some slower rate and, often, with a\nnon-Gaussian limit distribution.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 06:55:59 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Scricciolo", "Catia", ""]]}, {"id": "1902.00734", "submitter": "Nicolas Marie", "authors": "Fabienne Comte and Nicolas Marie", "title": "Bandwidth Selection for the Wolverton-Wagner Estimator", "comments": "26 pages, 5 figures", "journal-ref": "Journal of Statistical Planning and Inference 207, 198-214, 2020", "doi": "10.1016/j.jspi.2019.12.003", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $n$ independent random variables having the same H\\\"older continuous\ndensity, this paper deals with controls of the Wolverton-Wagner's estimator MSE\nand MISE. Then, for a bandwidth $h_n(\\beta)$, estimators of $\\beta$ are\nobtained by a Goldenshluger-Lepski type method and a Lacour-Massart-Rivoirard\ntype method. Some numerical experiments are provided for this last method.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 15:28:25 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2019 12:47:21 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Comte", "Fabienne", ""], ["Marie", "Nicolas", ""]]}, {"id": "1902.00746", "submitter": "Jaehyeok Shin", "authors": "Jaehyeok Shin, Aaditya Ramdas, Alessandro Rinaldo", "title": "On the bias, risk and consistency of sample means in multi-armed bandits", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sample mean is among the most well studied estimators in statistics,\nhaving many desirable properties such as unbiasedness and consistency. However,\nwhen analyzing data collected using a multi-armed bandit (MAB) experiment, the\nsample mean is biased and much remains to be understood about its properties.\nFor example, when is it consistent, how large is its bias, and can we bound its\nmean squared error? This paper delivers a thorough and systematic treatment of\nthe bias, risk and consistency of MAB sample means. Specifically, we identify\nfour distinct sources of selection bias (sampling, stopping, choosing and\nrewinding) and analyze them both separately and together. We further\ndemonstrate that a new notion of \\emph{effective sample size} can be used to\nbound the risk of the sample mean under suitable loss functions. We present\nseveral carefully designed examples to provide intuition on the different\nsources of selection bias we study. Our treatment is nonparametric and\nalgorithm-agnostic, meaning that it is not tied to a specific algorithm or\ngoal. In a nutshell, our proofs combine variational representations of\ninformation-theoretic divergences with new martingale concentration\ninequalities.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 16:23:08 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 07:11:31 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 03:41:45 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Shin", "Jaehyeok", ""], ["Ramdas", "Aaditya", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1902.00768", "submitter": "Ross Boczar", "authors": "Max Simchowitz, Ross Boczar, Benjamin Recht", "title": "Learning Linear Dynamical Systems with Semi-Parametric Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze a simple prefiltered variation of the least squares estimator for\nthe problem of estimation with biased, semi-parametric noise, an error model\nstudied more broadly in causal statistics and active learning. We prove an\noracle inequality which demonstrates that this procedure provably mitigates the\nvariance introduced by long-term dependencies. We then demonstrate that\nprefiltered least squares yields, to our knowledge, the first algorithm that\nprovably estimates the parameters of partially-observed linear systems that\nattains rates which do not not incur a worst-case dependence on the rate at\nwhich these dependencies decay. The algorithm is provably consistent even for\nsystems which satisfy the weaker marginal stability condition obeyed by many\nclassical models based on Newtonian mechanics. In this context, our\nsemi-parametric framework yields guarantees for both stochastic and worst-case\nnoise.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 19:06:28 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Simchowitz", "Max", ""], ["Boczar", "Ross", ""], ["Recht", "Benjamin", ""]]}, {"id": "1902.00791", "submitter": "Marta Crispino", "authors": "Julyan Arbel, Marta Crispino and St\\'ephane Girard", "title": "Dependence properties and Bayesian inference for asymmetric multivariate\n  copulas", "comments": null, "journal-ref": "Journal of Multivariate Analysis, 2019", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a broad class of asymmetric copulas introduced by Liebscher (2008)\nas a combination of multiple - usually symmetric - copulas. The main thrust of\nthe paper is to provide new theoretical properties including exact tail\ndependence expressions and stability properties. A subclass of Liebscher\ncopulas obtained by combining Fr\\'echet copulas is studied in more details. We\nestablish further dependence properties for copulas of this class and show that\nthey are characterized by an arbitrary number of singular components.\nFurthermore, we introduce a novel iterative representation for general\nLiebscher copulas which de facto insures uniform margins, thus relaxing a\nconstraint of Liebscher's original construction. Besides, we show that this\niterative construction proves useful for inference by developing an Approximate\nBayesian computation sampling scheme. This inferential procedure is\ndemonstrated on simulated data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 20:56:31 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 07:36:08 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Arbel", "Julyan", ""], ["Crispino", "Marta", ""], ["Girard", "St\u00e9phane", ""]]}, {"id": "1902.00814", "submitter": "Andr\\'as Gily\\'en", "authors": "Andr\\'as Gily\\'en and Tongyang Li", "title": "Distributional property testing in a quantum world", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in statistics and learning theory is to test properties\nof distributions. We show that quantum computers can solve such problems with\nsignificant speed-ups. In particular, we give fast quantum algorithms for\ntesting closeness between unknown distributions, testing independence between\ntwo distributions, and estimating the Shannon / von Neumann entropy of\ndistributions. The distributions can be either classical or quantum, however\nour quantum algorithms require coherent quantum access to a process preparing\nthe samples. Our results build on the recent technique of quantum singular\nvalue transformation, combined with more standard tricks such as\ndivide-and-conquer. The presented approach is a natural fit for distributional\nproperty testing both in the classical and the quantum case, demonstrating the\nfirst speed-ups for testing properties of density operators that can be\naccessed coherently rather than only via sampling; for classical distributions\nour algorithms significantly improve the precision dependence of some earlier\nresults.\n", "versions": [{"version": "v1", "created": "Sat, 2 Feb 2019 23:43:53 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Gily\u00e9n", "Andr\u00e1s", ""], ["Li", "Tongyang", ""]]}, {"id": "1902.00819", "submitter": "Sakshi Arya", "authors": "Sakshi Arya, Yuhong Yang", "title": "Randomized Allocation with Nonparametric Estimation for Contextual\n  Multi-Armed Bandits with Delayed Rewards", "comments": "Added simulations and some minor typographical changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a multi-armed bandit problem with covariates in a setting where\nthere is a possible delay in observing the rewards. Under some mild assumptions\non the probability distributions for the delays and using an appropriate\nrandomization to select the arms, the proposed strategy is shown to be strongly\nconsistent.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 00:05:17 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 02:21:54 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 22:28:58 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Arya", "Sakshi", ""], ["Yang", "Yuhong", ""]]}, {"id": "1902.00832", "submitter": "Xiang Cheng", "authors": "Xiang Cheng, Peter L. Bartlett, Michael I. Jordan", "title": "Quantitative Weak Convergence for Discrete Stochastic Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we quantitative convergence in $W_2$ for a family of\nLangevin-like stochastic processes that includes stochastic gradient descent\nand related gradient-based algorithms. Under certain regularity assumptions, we\nshow that the iterates of these stochastic processes converge to an invariant\ndistribution at a rate of $\\tilde{O}\\lrp{1/\\sqrt{k}}$ where $k$ is the number\nof steps; this rate is provably tight up to log factors. Our result reduces to\na quantitative form of the classical Central Limit Theorem in the special case\nwhen the potential is quadratic.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 02:42:59 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 05:50:04 GMT"}], "update_date": "2019-07-03", "authors_parsed": [["Cheng", "Xiang", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1902.00892", "submitter": "Ruth Heller", "authors": "Ruth Heller and Saharon Rosset", "title": "Optimal control of false discovery criteria in the two-group model", "comments": null, "journal-ref": "J R Stat Soc Series B (2020)", "doi": "10.1111/rssb.12403", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The highly influential two-group model in testing a large number of\nstatistical hypotheses assumes that the test statistics are drawn independently\nfrom a mixture of a high probability null distribution and a low probability\nalternative. Optimal control of the marginal false discovery rate (mFDR), in\nthe sense that it provides maximal power (expected true discoveries) subject to\nmFDR control, is known to be achieved by thresholding the local false discovery\nrate (locFDR), i.e., the probability of the hypothesis being null given the set\nof test statistics, with a fixed threshold. We address the challenge of\ncontrolling optimally the popular false discovery rate (FDR) or positive FDR\n(pFDR) rather than mFDR in the general two-group model, which also allows for\ndependence between the test statistics. These criteria are less conservative\nthan the mFDR criterion, so they make more rejections in expectation. We derive\ntheir optimal multiple testing (OMT) policies, which turn out to be\nthresholding the locFDR with a threshold that is a function of the entire set\nof statistics. We develop an efficient algorithm for finding these policies,\nand use it for problems with thousands of hypotheses. We illustrate these\nprocedures on gene expression studies.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 13:32:12 GMT"}, {"version": "v2", "created": "Thu, 25 Jul 2019 13:30:21 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 17:10:51 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Heller", "Ruth", ""], ["Rosset", "Saharon", ""]]}, {"id": "1902.00917", "submitter": "Ben Boukai", "authors": "Benzion Boukai and Yue Zhang", "title": "Recycled Two-Stage Estimation in Nonlinear Mixed Effects Regression\n  Models", "comments": "36 pages including 15 Tables, 7 Figures with 4 Theorem and 11 Lemas", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a re-sampling scheme for estimation of the population parameters\nin the mixed effects nonlinear regression models of the type use for example in\nclinical pharmacokinetics, say. We provide an estimation procedure which {\\it\nrecycles}, via random weighting, the relevant two-stage parameters estimates to\nconstruct consistent estimates of the sampling distribution of the various\nestimates. We establish the asymptotic consistency and asymptotic normality of\nthe resampled estimates and demonstrate the applicability of the {\\it\nrecycling} approach in a small simulation study and via example.\n", "versions": [{"version": "v1", "created": "Sun, 3 Feb 2019 16:15:13 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Boukai", "Benzion", ""], ["Zhang", "Yue", ""]]}, {"id": "1902.01021", "submitter": "Tomohiro Nishiyama", "authors": "Tomohiro Nishiyama", "title": "$L^p$-norm inequality using q-moment and its applications", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a measurable function on a set which has a finite measure, an inequality\nholds between two Lp-norms. In this paper, we show similar inequalities for the\nEuclidean space and the Lebesgue measure by using a q-moment which is a moment\nof an escort distribution. As applications of these inequalities, we first\nderive upper bounds for the Renyi and the Tsallis entropies with given q-moment\nand derive an inequality between two Renyi entropies. Second, we derive an\nupper bound for the probability of a subset in the Euclidean space with given\nLp-norm on the same set.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 03:25:04 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 13:39:02 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Nishiyama", "Tomohiro", ""]]}, {"id": "1902.01070", "submitter": "Sylvain Le Corff", "authors": "Elisabeth Gassiat (LMO), Sylvain Le Corff (CITI, TIPIC-SAMOVAR), Luc\n  Leh\\'ericy (LMO)", "title": "Identifiability and consistent estimation of nonparametric translation\n  hidden Markov models with general state space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers hidden Markov models where the observations are given as\nthe sum of a latent state which lies in a general state space and some\nindependent noise with unknown distribution. It is shown that these fully\nnonparametric translation models are identifiable with respect to both the\ndistribution of the latent variables and the distribution of the noise, under\nmostly a light tail assumption on the latent variables. Two nonparametric\nestimation methods are proposed and we prove that the corresponding estimators\nare consistent for the weak convergence topology. These results are illustrated\nwith numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 07:56:13 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 12:38:20 GMT"}, {"version": "v3", "created": "Wed, 29 Jan 2020 13:51:13 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Gassiat", "Elisabeth", "", "LMO"], ["Corff", "Sylvain Le", "", "CITI, TIPIC-SAMOVAR"], ["Leh\u00e9ricy", "Luc", "", "LMO"]]}, {"id": "1902.01075", "submitter": "Claire Lacour", "authors": "Suzanne Varet (LM-Orsay), Claire Lacour (LAMA), Pascal Massart\n  (LM-Orsay), Vincent Rivoirard (CEREMADE)", "title": "Numerical performance of Penalized Comparison to Overfitting for\n  multivariate kernel density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel density estimation is a well known method involving a smoothing\nparameter (the bandwidth) that needs to be tuned by the user. Although this\nmethod has been widely used the bandwidth selection remains a challenging issue\nin terms of balancing algorithmic performance and statistical relevance. The\npurpose of this paper is to compare a recently developped bandwidth selection\nmethod for kernel density estimation to those which are commonly used by now\n(at least those which are implemented in the R-package). This new method is\ncalled Penalized Comparison to Overfitting (PCO). It has been proposed by some\nof the authors of this paper in a previous work devoted to its statistical\nrelevance from a purely theoretical perspective. It is compared here to other\nusual bandwidth selection methods for univariate and also multivariate kernel\ndensity estimation on the basis of intensive simulation studies. In particular,\ncross-validation and plug-in criteria are numerically investigated and compared\nto PCO. The take home message is that PCO can outperform the classical methods\nwithout algorithmic additionnal cost.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 08:14:46 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Varet", "Suzanne", "", "LM-Orsay"], ["Lacour", "Claire", "", "LAMA"], ["Massart", "Pascal", "", "LM-Orsay"], ["Rivoirard", "Vincent", "", "CEREMADE"]]}, {"id": "1902.01132", "submitter": "Agnieszka Goroncy", "authors": "Agnieszka Goroncy", "title": "Optimal upper bounds on expected kth record values from IGFR\n  distributions", "comments": "The final version of this paper will be published in Statistics", "journal-ref": null, "doi": "10.1080/02331888.2019.1580282", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper concerns the optimal upper bounds on the expectations of the kth\nrecord values (k >= 1) centered about the sample mean. We consider the case,\nwhen the records are based on the infinite sequence of the independent\nidentically distributed random variables, which distribution function is\nrestricted to the family of distributions with the increasing generalized\nfailure rate (IGFR). Such a class can be defined in terms of the convex orders\nof some distribution functions. Particularly important examples of IGFR class\nare the distributions with the increasing density (ID) and increasing failure\nrate (IFR). Presented bounds were obtained with use of the projection method,\nand are expressed in the scale units based on the standard deviation of the\nunderlying distribution function.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 11:41:45 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Goroncy", "Agnieszka", ""]]}, {"id": "1902.01136", "submitter": "Javier  Carcamo", "authors": "Javier C\\'arcamo, Luis-Alberto Rodr\\'iguez and Antonio Cuevas", "title": "Directional differentiability for supremum-type functionals: statistical\n  applications", "comments": "30 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that various functionals related to the supremum of a real function\ndefined on an arbitrary set or a measure space are Hadamard directionally\ndifferentiable. We specifically consider the supremum norm, the supremum, the\ninfimum, and the amplitude of a function. The (usually non-linear) derivatives\nof these maps adopt simple expressions under suitable assumptions on the\nunderlying space. As an application, we improve and extend to the\nmultidimensional case the results in \\cite{Raghavachari} regarding the limiting\ndistributions of Kolmogorov-Smirnov type statistics under the alternative\nhypothesis. Similar results are obtained for analogous statistics associated\nwith copulas. We additionally solve an open problem about the Berk-Jones\nstatistic proposed by \\cite{Jager-Wellner-2004}. Finally, the asymptotic\ndistribution of maximum mean discrepancies over Donsker classes of functions is\nderived.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 11:51:41 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 13:48:17 GMT"}], "update_date": "2019-12-18", "authors_parsed": [["C\u00e1rcamo", "Javier", ""], ["Rodr\u00edguez", "Luis-Alberto", ""], ["Cuevas", "Antonio", ""]]}, {"id": "1902.01141", "submitter": "{\\L}ukasz Rajkowski", "authors": "{\\L}ukasz Rajkowski, John Noble", "title": "A note on the geometry of the MAP partition in Conjugate Exponential\n  Bayesian Mixture Models", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the geometry of the maximal a posteriori (MAP) partition in\nthe Bayesian Mixture Model where the component and the base distributions are\nchosen from conjugate exponential families. We prove that in this case the\nclusters are separated by the contour lines of a linear functional of the\nsufficient statistic. As a particular example, we describe Bayesian Mixture of\nNormals with Normal-inverse-Wishart prior on the component mean and covariance,\nin which the clusters in any MAP partition are separated by a quadratic\nsurface. In connection with results of Rajkowski (2018), where the linear\nseparability of clusters in the Bayesian Mixture Model with a fixed component\ncovariance matrix was proved, it gives a nice Bayesian analogue of the\ngeometric properties of Fisher Discriminant Analysis (LDA and QDA).\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 12:10:09 GMT"}, {"version": "v2", "created": "Mon, 24 Jun 2019 14:20:50 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Rajkowski", "\u0141ukasz", ""], ["Noble", "John", ""]]}, {"id": "1902.01215", "submitter": "Subhajit Goswami", "authors": "Sabyasachi Chatterjee, Subhajit Goswami", "title": "New Risk Bounds for 2D Total Variation Denoising", "comments": "66 pages, 4 figures. Author names are sorted alphabetically. The\n  current version is published in the IEEE Transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2021.3059657", "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2D Total Variation Denoising (TVD) is a widely used technique for image\ndenoising. It is also an important nonparametric regression method for\nestimating functions with heterogenous smoothness. Recent results have shown\nthe TVD estimator to be nearly minimax rate optimal for the class of functions\nwith bounded variation. In this paper, we complement these worst case\nguarantees by investigating the adaptivity of the TVD estimator to functions\nwhich are piecewise constant on axis aligned rectangles. We rigorously show\nthat, when the truth is piecewise constant, the ideally tuned TVD estimator\nperforms better than in the worst case. We also study the issue of choosing the\ntuning parameter. In particular, we propose a fully data driven version of the\nTVD estimator which enjoys similar worst case risk guarantees as the ideally\ntuned TVD estimator.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 14:32:29 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 22:00:11 GMT"}, {"version": "v3", "created": "Mon, 26 Aug 2019 21:10:01 GMT"}, {"version": "v4", "created": "Thu, 4 Mar 2021 11:06:15 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Chatterjee", "Sabyasachi", ""], ["Goswami", "Subhajit", ""]]}, {"id": "1902.01219", "submitter": "Joseph Lam-Weil", "authors": "Joseph Lam-Weil, Alexandra Carpentier, Bharath K. Sriperumbudur", "title": "Local minimax rates for closeness testing of discrete distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the closeness testing problem for discrete distributions. The\ngoal is to distinguish whether two samples are drawn from the same unspecified\ndistribution, or whether their respective distributions are separated in\n$L_1$-norm. In this paper, we focus on adapting the rate to the shape of the\nunderlying distributions, i.e. we consider \\textit{a local minimax setting}. We\nprovide, to the best of our knowledge, the first local minimax rate for the\nseparation distance up to logarithmic factors, together with a test that\nachieves it. In view of the rate, closeness testing turns out to be\nsubstantially harder than the related one-sample testing problem over a wide\nrange of cases.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 12:42:12 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 14:59:51 GMT"}, {"version": "v3", "created": "Tue, 19 Jan 2021 15:47:42 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Lam-Weil", "Joseph", ""], ["Carpentier", "Alexandra", ""], ["Sriperumbudur", "Bharath K.", ""]]}, {"id": "1902.01224", "submitter": "Geoffrey Wolfer", "authors": "Geoffrey Wolfer and Aryeh Kontorovich", "title": "Estimating the Mixing Time of Ergodic Markov Chains", "comments": "Upload the extended version submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the mixing time $t_{\\mathsf{mix}}$ of an\narbitrary ergodic finite-state Markov chain from a single trajectory of length\n$m$. The reversible case was addressed by Hsu et al. [2019], who left the\ngeneral case as an open problem. In the reversible case, the analysis is\ngreatly facilitated by the fact that the Markov operator is self-adjoint, and\nWeyl's inequality allows for a dimension-free perturbation analysis of the\nempirical eigenvalues. As Hsu et al. point out, in the absence of reversibility\n(which induces asymmetric pair probabilities matrices), the existing\nperturbation analysis has a worst-case exponential dependence on the number of\nstates $d$. Furthermore, even if an eigenvalue perturbation analysis with\nbetter dependence on $d$ were available, in the non-reversible case the\nconnection between the spectral gap and the mixing time is not nearly as\nstraightforward as in the reversible case. Our key insight is to estimate the\npseudo-spectral gap $\\gamma_{\\mathsf{ps}}$ instead, which allows us to overcome\nthe loss of symmetry and to achieve a polynomial dependence on the minimal\nstationary probability $\\pi_\\star$ and $\\gamma_{\\mathsf{ps}}$. Additionally, in\nthe reversible case, we obtain simultaneous nearly (up to logarithmic factors)\nminimax rates in $t_{\\mathsf{mix}}$ and precision $\\varepsilon$, closing a gap\nin Hsu et al., who treated $\\varepsilon$ as constant in the lower bounds.\nFinally, we construct fully empirical confidence intervals for\n$\\gamma_{\\mathsf{ps}}$, which shrink to zero at a rate of roughly $1/\\sqrt{m}$,\nand improve the state of the art in even the reversible case.\n", "versions": [{"version": "v1", "created": "Fri, 1 Feb 2019 08:13:58 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 09:07:31 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 13:43:25 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Wolfer", "Geoffrey", ""], ["Kontorovich", "Aryeh", ""]]}, {"id": "1902.01237", "submitter": "Marco Oesting", "authors": "Marco Oesting and Alexander Schnurr", "title": "Ordinal Patterns in Clusters of Subsequent Extremes of Regularly Varying\n  Time Series", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate temporal clusters of extremes defined as\nsubsequent exceedances of high thresholds in a stationary time series. Two\nmeaningful features of these clusters are the probability distribution of the\ncluster size and the ordinal patterns within a cluster. Since these patterns\ntake only the ordinal structure of consecutive data points into account the\nmethod is robust under monotone transformations and measurement errors. We\nverify the existence of the corresponding limit distributions in the framework\nof regularly varying time series, develop non-parametric estimators and show\ntheir asymptotic normality under appropriate mixing conditions. The performance\nof the estimators is demonstrated in a simulated example and a real data\napplication to discharge data of the river Rhine.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 15:07:37 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 11:19:42 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Oesting", "Marco", ""], ["Schnurr", "Alexander", ""]]}, {"id": "1902.01373", "submitter": "Krishnakumar Balasubramanian", "authors": "Abhishek Roy, Lingqing Shen, Krishnakumar Balasubramanian, Saeed\n  Ghadimi", "title": "Stochastic Zeroth-order Discretizations of Langevin Diffusions for\n  Bayesian Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discretizations of Langevin diffusions provide a powerful method for sampling\nand Bayesian inference. However, such discretizations require evaluation of the\ngradient of the potential function. In several real-world scenarios, obtaining\ngradient evaluations might either be computationally expensive, or simply\nimpossible. In this work, we propose and analyze stochastic zeroth-order\nsampling algorithms for discretizing overdamped and underdamped Langevin\ndiffusions. Our approach is based on estimating the gradients, based on\nGaussian Stein's identities, widely used in the stochastic optimization\nliterature. We provide a comprehensive sample complexity analysis -- number\nnoisy function evaluations to be made to obtain an $\\epsilon$-approximate\nsample in Wasserstein distance -- of stochastic zeroth-order discretizations of\nboth overdamped and underdamped Langevin diffusions, under various noise\nmodels. We also propose a variable selection technique based on zeroth-order\ngradient estimates and establish its theoretical guarantees. Our theoretical\ncontributions extend the practical applicability of sampling algorithms to the\nnoisy black-box and high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 18:40:38 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2019 17:36:14 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 14:52:37 GMT"}, {"version": "v4", "created": "Sun, 17 Jan 2021 19:34:00 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Roy", "Abhishek", ""], ["Shen", "Lingqing", ""], ["Balasubramanian", "Krishnakumar", ""], ["Ghadimi", "Saeed", ""]]}, {"id": "1902.01456", "submitter": "Jean-Jacques Forneron", "authors": "Jean-Jacques Forneron", "title": "A Sieve-SMM Estimator for Dynamic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a Sieve Simulated Method of Moments (Sieve-SMM) estimator\nfor the parameters and the distribution of the shocks in nonlinear dynamic\nmodels where the likelihood and the moments are not tractable. An important\nconcern with SMM, which matches sample with simulated moments, is that a\nparametric distribution is required. However, economic quantities that depend\non this distribution, such as welfare and asset-prices, can be sensitive to\nmisspecification. The Sieve-SMM estimator addresses this issue by flexibly\napproximating the distribution of the shocks with a Gaussian and tails mixture\nsieve. The asymptotic framework provides consistency, rate of convergence and\nasymptotic normality results, extending existing results to a new framework\nwith more general dynamics and latent variables. An application to asset\npricing in a production economy shows a large decline in the estimates of\nrelative risk-aversion, highlighting the empirical relevance of\nmisspecification bias.\n", "versions": [{"version": "v1", "created": "Mon, 4 Feb 2019 20:51:02 GMT"}, {"version": "v2", "created": "Wed, 22 Jul 2020 13:51:54 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Forneron", "Jean-Jacques", ""]]}, {"id": "1902.01575", "submitter": "Emilie Kaufmann", "authors": "Lilian Besson (IRISA), Emilie Kaufmann (CNRS, CRIStAL, Scool),\n  Odalric-Ambrym Maillard (Scool), Julien Seznec (Scool)", "title": "Efficient Change-Point Detection for Tackling Piecewise-Stationary\n  Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GLR-klUCB, a novel algorithm for the piecewise iid\nnon-stationary bandit problem with bounded rewards. This algorithm combines an\nefficient bandit algorithm, kl-UCB, with an efficient, parameter-free,\nchangepoint detector, the Bernoulli Generalized Likelihood Ratio Test, for\nwhich we provide new theoretical guarantees of independent interest. Unlike\nprevious non-stationary bandit algorithms using a change-point detector,\nGLR-klUCB does not need to be calibrated based on prior knowledge on the arms'\nmeans. We prove that this algorithm can attain a $O(\\sqrt{TA\n\\Upsilon_T\\log(T)})$ regret in $T$ rounds on some \"easy\" instances, where A is\nthe number of arms and $\\Upsilon_T$ the number of change-points, without prior\nknowledge of $\\Upsilon_T$. In contrast with recently proposed algorithms that\nare agnostic to $\\Upsilon_T$, we perform a numerical study showing that\nGLR-klUCB is also very efficient in practice, beyond easy instances.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 07:37:48 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 11:12:25 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Besson", "Lilian", "", "IRISA"], ["Kaufmann", "Emilie", "", "CNRS, CRIStAL, Scool"], ["Maillard", "Odalric-Ambrym", "", "Scool"], ["Seznec", "Julien", "", "Scool"]]}, {"id": "1902.01596", "submitter": "Pierre Neuvial", "authors": "Christophe Ambroise (LaMME), Alia Dehman, Pierre Neuvial (IMT),\n  Guillem Rigaill (IPS2, LaMME), Nathalie Vialaneix (MIAT INRA)", "title": "Adjacency-constrained hierarchical clustering of a band similarity\n  matrix with application to Genomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Genomic data analyses such as Genome-Wide Association Studies\n(GWAS) or Hi-C studies are often faced with the problem of partitioning\nchromosomes into successive regions based on a similarity matrix of\nhigh-resolution, locus-level measurements. An intuitive way of doing this is to\nperform a modified Hierarchical Agglomerative Clustering (HAC), where only\nadjacent clusters (according to the ordering of positions within a chromosome)\nare allowed to be merged. A major practical drawback of this method is its\nquadratic time and space complexity in the number of loci, which is typically\nof the order of 10^4 to 10^5 for each chromosome. Results: By assuming that the\nsimilarity between physically distant objects is negligible, we propose an\nimplementation of this adjacency-constrained HAC with quasi-linear complexity.\nOur illustrations on GWAS and Hi-C datasets demonstrate the relevance of this\nassumption, and show that this method highlights biologically meaningful\nsignals. Thanks to its small time and memory footprint, the method can be run\non a standard laptop in minutes or even seconds. Availability and\nImplementation: Software and sample data are available as an R package,\nadjclust, that can be downloaded from the Comprehensive R Archive Network\n(CRAN).\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 09:01:24 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Ambroise", "Christophe", "", "LaMME"], ["Dehman", "Alia", "", "IMT"], ["Neuvial", "Pierre", "", "IMT"], ["Rigaill", "Guillem", "", "IPS2, LaMME"], ["Vialaneix", "Nathalie", "", "MIAT INRA"]]}, {"id": "1902.01753", "submitter": "Ji Xu", "authors": "Ji Xu and Arian Maleki and Kamiar Rahnama Rad and Daniel Hsu", "title": "Consistent Risk Estimation in Moderately High-Dimensional Linear\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk estimation is at the core of many learning systems. The importance of\nthis problem has motivated researchers to propose different schemes, such as\ncross validation, generalized cross validation, and Bootstrap. The theoretical\nproperties of such estimates have been extensively studied in the\nlow-dimensional settings, where the number of predictors $p$ is much smaller\nthan the number of observations $n$. However, a unifying methodology\naccompanied with a rigorous theory is lacking in high-dimensional settings.\nThis paper studies the problem of risk estimation under the moderately\nhigh-dimensional asymptotic setting $n,p \\rightarrow \\infty$ and $n/p\n\\rightarrow \\delta>1$ ($\\delta$ is a fixed number), and proves the consistency\nof three risk estimates that have been successful in numerical studies, i.e.,\nleave-one-out cross validation (LOOCV), approximate leave-one-out (ALO), and\napproximate message passing (AMP)-based techniques. A corner stone of our\nanalysis is a bound that we obtain on the discrepancy of the `residuals'\nobtained from AMP and LOOCV. This connection not only enables us to obtain a\nmore refined information on the estimates of AMP, ALO, and LOOCV, but also\noffers an upper bound on the convergence rate of each estimate.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 15:52:39 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 03:57:47 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 17:38:15 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Xu", "Ji", ""], ["Maleki", "Arian", ""], ["Rad", "Kamiar Rahnama", ""], ["Hsu", "Daniel", ""]]}, {"id": "1902.01778", "submitter": "Jonathan Niles-Weed", "authors": "Jonathan Niles-Weed, Quentin Berthet", "title": "Minimax estimation of smooth densities in Wasserstein distance", "comments": "v4 adds adaptive estimator (Theorem 2) and tight upper bound for\n  unbounded measures (Theorem 6)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonparametric density estimation problems where error is measured in\nthe Wasserstein distance, a metric on probability distributions popular in many\nareas of statistics and machine learning. We give the first minimax-optimal\nrates for this problem for general Wasserstein distances, and show that, unlike\nclassical nonparametric density estimation, these rates depend on whether the\ndensities in question are bounded below. Motivated by variational problems\ninvolving the Wasserstein distance, we also show how to construct discretely\nsupported measures, suitable for computational purposes, which achieve the\nminimax rates. Our main technical tool is an inequality giving a nearly tight\ndual characterization of the Wasserstein distances in terms of Besov norms.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 16:40:12 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 23:31:57 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 21:52:56 GMT"}, {"version": "v4", "created": "Tue, 28 Apr 2020 22:42:44 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Niles-Weed", "Jonathan", ""], ["Berthet", "Quentin", ""]]}, {"id": "1902.01902", "submitter": "Prateek Jaiswal", "authors": "Prateek Jaiswal, Vinayak A. Rao and Harsha Honnappa", "title": "Asymptotic Consistency of $\\alpha-$R\\'enyi-Approximate Posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the asymptotic consistency properties of $\\alpha$-R\\'enyi\napproximate posteriors, a class of variational Bayesian methods that\napproximate an intractable Bayesian posterior with a member of a tractable\nfamily of distributions, the member chosen to minimize the $\\alpha$-R\\'enyi\ndivergence from the true posterior. Unique to our work is that we consider\nsettings with $\\alpha > 1$, resulting in approximations that upperbound the\nlog-likelihood, and consequently have wider spread than traditional variational\napproaches that minimize the Kullback-Liebler (KL) divergence from the\nposterior. Our primary result identifies sufficient conditions under which\nconsistency holds, centering around the existence of a 'good' sequence of\ndistributions in the approximating family that possesses, among other\nproperties, the right rate of convergence to a limit distribution. We further\ncharacterize the good sequence by demonstrating that a sequence of\ndistributions that converges too quickly cannot be a good sequence. We also\nextend our analysis to the setting where $\\alpha$ equals one, corresponding to\nthe minimizer of the reverse KL divergence, and to models with local latent\nvariables. We also illustrate the existence of good sequence with a number of\nexamples. Our results complement a growing body of work focused on the\nfrequentist properties of variational Bayesian methods.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 20:41:39 GMT"}, {"version": "v2", "created": "Fri, 22 Feb 2019 02:02:35 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 17:45:09 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Jaiswal", "Prateek", ""], ["Rao", "Vinayak A.", ""], ["Honnappa", "Harsha", ""]]}, {"id": "1902.01911", "submitter": "Andreas Maurer", "authors": "Andreas Maurer, Massimiliano Pontil", "title": "Uniform concentration and symmetrization for weak interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method to derive uniform bounds with Gaussian and Rademacher complexities\nis extended to the case where the sample average is replaced by a nonlinear\nstatistic. Tight bounds are obtained for U-statistics, smoothened L-statistics\nand error functionals of l2-regularized algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 5 Feb 2019 21:16:55 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 22:36:41 GMT"}, {"version": "v3", "created": "Wed, 8 May 2019 19:30:27 GMT"}, {"version": "v4", "created": "Fri, 10 May 2019 06:27:14 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Maurer", "Andreas", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "1902.01998", "submitter": "Yeshwanth Cherapanamjeri", "authors": "Yeshwanth Cherapanamjeri, Nicolas Flammarion, Peter L. Bartlett", "title": "Fast Mean Estimation with Sub-Gaussian Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an estimator for the mean of a random vector in $\\mathbb{R}^d$\nthat can be computed in time $O(n^4+n^2d)$ for $n$ i.i.d.~samples and that has\nerror bounds matching the sub-Gaussian case. The only assumptions we make about\nthe data distribution are that it has finite mean and covariance; in\nparticular, we make no assumptions about higher-order moments. Like the\npolynomial time estimator introduced by Hopkins, 2018, which is based on the\nsum-of-squares hierarchy, our estimator achieves optimal statistical efficiency\nin this challenging setting, but it has a significantly faster runtime and a\nsimpler analysis.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 01:33:05 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Flammarion", "Nicolas", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1902.01999", "submitter": "Yeshwanth Cherapanamjeri", "authors": "Yeshwanth Cherapanamjeri, Peter L. Bartlett", "title": "Testing Markov Chains without Hitting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of identity testing of markov chains. In this setting,\nwe are given access to a single trajectory from a markov chain with unknown\ntransition matrix $Q$ and the goal is to determine whether $Q = P$ for some\nknown matrix $P$ or $\\text{Dist}(P, Q) \\geq \\epsilon$ where $\\text{Dist}$ is\nsuitably defined. In recent work by Daskalakis, Dikkala and Gravin, 2018, it\nwas shown that it is possible to distinguish between the two cases provided the\nlength of the observed trajectory is at least super-linear in the hitting time\nof $P$ which may be arbitrarily large.\n  In this paper, we propose an algorithm that avoids this dependence on hitting\ntime thus enabling efficient testing of markov chains even in cases where it is\ninfeasible to observe every state in the chain. Our algorithm is based on\ncombining classical ideas from approximation algorithms with techniques for the\nspectral analysis of markov chains.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 01:33:43 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Cherapanamjeri", "Yeshwanth", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1902.02117", "submitter": "Ismihan Bayramoglu (Bairamov)", "authors": "Ismihan Bairamov", "title": "Joint distribution of a random sample and an order statistic: A new\n  approach with an application in reliability analysis", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the joint distribution of elements of a random sample\nand an order statistic of the same sample. \\ The motivation for this work stems\nfrom the important problem in reliability analysis, to estimate the number of\ninspections we need in order to detect failed components in a coherent system.\nWe consider an $(n-r+1)$-out-of-$n$ system, which is intact until at least\n$n-r+1$ of the components are alive, and it fails if the number of failed\ncomponents exceeds $r$. The life time of the system is the $% r$th order\nstatistic. Assuming that some of the components failed but the system \\ is\nstill functioning, \\ using the results presented in this paper it is possible\nto find an expected value of the number of inspections we need to do for\ndetecting certain number of failed components.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 11:25:12 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 13:30:27 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Bairamov", "Ismihan", ""]]}, {"id": "1902.02238", "submitter": "Geoffrey Chinot", "authors": "Geoffrey Chinot", "title": "Robust learning and complexity dependent bounds for regularized problems", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Regularized Empirical Risk Minimizers (RERM) and minmax\nMedian-Of-Means (MOM) estimators where the regularization function\n$\\phi(\\cdot)$ is an even convex function. We obtain bounds on the\n$L_2$-estimation error and the excess risk that depend on $\\phi(f^*)$, where\n$f^*$ is the minimizer of the risk over a class $F$. The estimators are based\non loss functions that are both Lipschitz and convex. Results for the RERM are\nderived under weak assumptions on the outputs and a sub-Gaussian assumption on\nthe class $\\{ (f-f^*)(X), f \\in F \\}$. Similar results are shown for minmax MOM\nestimators in a close setting where outliers may corrupt the dataset and where\nthe class $\\{ (f-f^*)(X), f \\in F \\}$ is only supposed to satisfy weak moment\nassumptions, relaxing the sub-Gaussian and the i.i.d hypothesis necessary for\nRERM. The analysis of RERM and minmax MOM estimators with Lipschitz and convex\nloss funtions is based on a weak local Bernstein Assumption. We obtain two\n\"meta theorems\" that we use to study linear estimators regularized by the\nElastic Net. We also examine Support Vector Machines (SVM), where no\nsub-Gaussian assumption is required and when the target $Y$ can be\nheavy-tailed, improving the existing literature.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 15:42:44 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 02:37:08 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 00:38:15 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Chinot", "Geoffrey", ""]]}, {"id": "1902.02408", "submitter": "James Sharpnack", "authors": "James Sharpnack", "title": "Weak consistency of the 1-nearest neighbor measure with applications to\n  missing data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data is partially missing at random, imputation and importance weighting\nare often used to estimate moments of the unobserved population. In this paper,\nwe study 1-nearest neighbor (1NN) importance weighting, which estimates moments\nby replacing missing data with the complete data that is the nearest neighbor\nin the non-missing covariate space. We define an empirical measure, the 1NN\nmeasure, and show that it is weakly consistent for the measure of the missing\ndata. The main idea behind this result is that the 1NN measure is performing\ninverse probability weighting in the limit. We study applications to missing\ndata and mitigating the impact of covariate shift in prediction tasks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Feb 2019 22:00:14 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 22:09:05 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Sharpnack", "James", ""]]}, {"id": "1902.02723", "submitter": "Hailin Sang", "authors": "Aleksandr Beknazaryan, Hailin Sang and Yimin Xiao", "title": "Cram\\'er Type Moderate Deviations for Random Fields", "comments": "23 pages, to appear at Journal of Applied Probability", "journal-ref": "J. Appl. Probab. 56 (2019) 223-245", "doi": "10.1017/jpr.2019.15", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Cram\\'er type moderate deviation for partial sums of random\nfields by applying the conjugate method. The results are applicable to the\npartial sums of linear random fields with short or long memory and to\nnonparametric regression with random field errors.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 16:43:40 GMT"}], "update_date": "2019-07-22", "authors_parsed": [["Beknazaryan", "Aleksandr", ""], ["Sang", "Hailin", ""], ["Xiao", "Yimin", ""]]}, {"id": "1902.02761", "submitter": "Fang Han", "authors": "Yandi Shen, Fang Han, Daniela Witten", "title": "Tail behavior of dependent V-statistics and its applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish exponential inequalities and Cramer-type moderate deviation\ntheorems for a class of V-statistics under strong mixing conditions. Our theory\nis developed via kernel expansion based on random Fourier features. This type\nof expansion is new and useful for handling many notorious classes of kernels.\nWhile the developed theory has a number of applications, we apply it to\nlasso-type semiparametric regression estimation and high-dimensional multiple\nhypothesis testing.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 18:38:11 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Shen", "Yandi", ""], ["Han", "Fang", ""], ["Witten", "Daniela", ""]]}, {"id": "1902.02852", "submitter": "Yining Wang", "authors": "Yaonan Jin, Yingkai Li, Yining Wang, Yuan Zhou", "title": "On Asymptotically Tight Tail Bounds for Sums of Geometric and\n  Exponential Random Variables", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note we prove bounds on the upper and lower probability tails of sums\nof independent geometric or exponentially distributed random variables. We also\nprove negative results showing that our established tail bounds are\nasymptotically tight.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 21:30:27 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Jin", "Yaonan", ""], ["Li", "Yingkai", ""], ["Wang", "Yining", ""], ["Zhou", "Yuan", ""]]}, {"id": "1902.02885", "submitter": "Shiyun Chen", "authors": "Shiyun Chen, Shiva Kasiviswanathan", "title": "Contextual Online False Discovery Rate Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypothesis testing, a situation when we wish to consider many\nhypotheses, is a core problem in statistical inference that arises in almost\nevery scientific field. In this setting, controlling the false discovery rate\n(FDR), which is the expected proportion of type I error, is an important\nchallenge for making meaningful inferences. In this paper, we consider the\nproblem of controlling FDR in an online manner. Concretely, we consider an\nordered, possibly infinite, sequence of hypotheses, arriving one at each\ntimestep, and for each hypothesis we observe a p-value along with a set of\nfeatures specific to that hypothesis. The decision whether or not to reject the\ncurrent hypothesis must be made immediately at each timestep, before the next\nhypothesis is observed. The model of multi-dimensional feature set provides a\nvery general way of leveraging the auxiliary information in the data which\nhelps in maximizing the number of discoveries.\n  We propose a new class of powerful online testing procedures, where the\nrejections thresholds (significance levels) are learnt sequentially by\nincorporating contextual information and previous results. We prove that any\nrule in this class controls online FDR under some standard assumptions. We then\nfocus on a subclass of these procedures, based on weighting significance\nlevels, to derive a practical algorithm that learns a parametric weight\nfunction in an online fashion to gain more discoveries. We also theoretically\nprove, in a stylized setting, that our proposed procedures would lead to an\nincrease in the achieved statistical power over a popular online testing\nprocedure proposed by Javanmard & Montanari (2018). Finally, we demonstrate the\nfavorable performance of our procedure, by comparing it to state-of-the-art\nonline multiple testing procedures, on both synthetic data and real data\ngenerated from different applications.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 23:24:19 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 23:09:05 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Chen", "Shiyun", ""], ["Kasiviswanathan", "Shiva", ""]]}, {"id": "1902.02890", "submitter": "Leighton Barnes", "authors": "Leighton Pate Barnes, Yanjun Han, and Ayfer Ozgur", "title": "Lower Bounds for Learning Distributions under Communication Constraints\n  via Fisher Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning high-dimensional, nonparametric and\nstructured (e.g. Gaussian) distributions in distributed networks, where each\nnode in the network observes an independent sample from the underlying\ndistribution and can use $k$ bits to communicate its sample to a central\nprocessor. We consider three different models for communication. Under the\nindependent model, each node communicates its sample to a central processor by\nindependently encoding it into $k$ bits. Under the more general sequential or\nblackboard communication models, nodes can share information interactively but\neach node is restricted to write at most $k$ bits on the final transcript. We\ncharacterize the impact of the communication constraint $k$ on the minimax risk\nof estimating the underlying distribution under $\\ell^2$ loss. We develop\nminimax lower bounds that apply in a unified way to many common statistical\nmodels and reveal that the impact of the communication constraint can be\nqualitatively different depending on the tail behavior of the score function\nassociated with each model. A key ingredient in our proofs is a geometric\ncharacterization of Fisher information from quantized samples.\n", "versions": [{"version": "v1", "created": "Thu, 7 Feb 2019 23:58:09 GMT"}, {"version": "v2", "created": "Fri, 31 May 2019 20:51:46 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Barnes", "Leighton Pate", ""], ["Han", "Yanjun", ""], ["Ozgur", "Ayfer", ""]]}, {"id": "1902.02920", "submitter": "Katsumi Shimotsu", "authors": "Hiroyuki Kasahara and Katsumi Shimotsu", "title": "Testing the Order of Multivariate Normal Mixture Models", "comments": "54 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixtures of multivariate normal distributions have been widely used in\nempirical applications in diverse fields such as statistical genetics and\nstatistical finance. Testing the number of components in multivariate normal\nmixture models is a long-standing challenge even in the most important case of\ntesting homogeneity. This paper develops likelihood-based tests of the null\nhypothesis of $M_0$ components against the alternative hypothesis of $M_0 + 1$\ncomponents for a general $M_0 \\geq 1$. For heteroscedastic normal mixtures, we\npropose an EM test and derive the asymptotic distribution of the EM test\nstatistic. For homoscedastic normal mixtures, we derive the asymptotic\ndistribution of the likelihood ratio test statistic. We also derive the\nasymptotic distribution of the likelihood ratio test statistic and EM test\nstatistic under local alternatives and show the validity of parametric\nbootstrap. The simulations show that the proposed test has good finite sample\nsize and power properties.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 02:43:11 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Kasahara", "Hiroyuki", ""], ["Shimotsu", "Katsumi", ""]]}, {"id": "1902.03000", "submitter": "Yacouba Boubacar Mainassara", "authors": "Yacouba Boubacar Ma\\\"inassara (UFC), Abdoulkarim Ilmi Amir (LMB)", "title": "Distribution of residual autocorrelations for multiplicative seasonal\n  ARMA models with uncorrelated but non-independent error terms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider portmanteau tests for testing the adequacy of\nmultiplicative seasonal autoregressive moving-average (SARMA) models under the\nassumption that the errors are uncorrelated but not necessarily independent.We\nrelax the standard independence assumption on the error term in order to extend\nthe range of application of the SARMA models.We study the asymptotic\ndistributions of residual and normalized residual empirical autocovariances and\nautocorrelations underweak assumptions on the noise. We establish the\nasymptotic behaviour of the proposed statistics. A set of Monte Carlo\nexperiments and an application to monthly mean total sunspot number are\npresented.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 10:07:55 GMT"}], "update_date": "2019-02-11", "authors_parsed": [["Ma\u00efnassara", "Yacouba Boubacar", "", "UFC"], ["Amir", "Abdoulkarim Ilmi", "", "LMB"]]}, {"id": "1902.03046", "submitter": "Ulysse Marteau-Ferey", "authors": "Ulysse Marteau-Ferey (PSL, SIERRA), Dmitrii Ostrovskii (PSL, SIERRA),\n  Francis Bach (PSL, SIERRA), Alessandro Rudi (PSL, SIERRA)", "title": "Beyond Least-Squares: Fast Rates for Regularized Empirical Risk\n  Minimization through Self-Concordance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning methods based on the regularization of a convex\nempirical risk by a squared Hilbertian norm, a setting that includes linear\npredictors and non-linear predictors through positive-definite kernels. In\norder to go beyond the generic analysis leading to convergence rates of the\nexcess risk as $O(1/\\sqrt{n})$ from $n$ observations, we assume that the\nindividual losses are self-concordant, that is, their third-order derivatives\nare bounded by their second-order derivatives. This setting includes\nleast-squares, as well as all generalized linear models such as logistic and\nsoftmax regression. For this class of losses, we provide a bias-variance\ndecomposition and show that the assumptions commonly made in least-squares\nregression, such as the source and capacity conditions, can be adapted to\nobtain fast non-asymptotic rates of convergence by improving the bias terms,\nthe variance terms or both.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 12:18:05 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 13:37:59 GMT"}, {"version": "v3", "created": "Tue, 18 Jun 2019 14:40:41 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Marteau-Ferey", "Ulysse", "", "PSL, SIERRA"], ["Ostrovskii", "Dmitrii", "", "PSL, SIERRA"], ["Bach", "Francis", "", "PSL, SIERRA"], ["Rudi", "Alessandro", "", "PSL, SIERRA"]]}, {"id": "1902.03056", "submitter": "Philippe Toint", "authors": "Z. Luo and L. Qi and Ph. L. Toint", "title": "Bernstein Concentration Inequalities for Tensors via Einstein Products", "comments": "12 pages", "journal-ref": "Frontiers of Mathematics in China, vol. 5(2), pp. 367-384, 2020", "doi": null, "report-no": null, "categories": "math.ST math.NA math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generalization of the Bernstein matrix concentration inequality to random\ntensors of general order is proposed. This generalization is based on the use\nof Einstein products between tensors, from which a strong link can be\nestablished between matrices and tensors, in turn allowing exploitation of\nexisting results for the former.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 12:33:08 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Luo", "Z.", ""], ["Qi", "L.", ""], ["Toint", "Ph. L.", ""]]}, {"id": "1902.03086", "submitter": "Dmitrii Ostrovskii", "authors": "Dmitrii Ostrovskii (PSL, SIERRA, USC), Alessandro Rudi (PSL, SIERRA)", "title": "Affine Invariant Covariance Estimation for Heavy-Tailed Distributions", "comments": null, "journal-ref": "32nd Annual Conference on Learning Theory (COLT), 2019, Jun 2019,\n  Phoenix, United States", "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we provide an estimator for the covariance matrix of a\nheavy-tailed multivariate distributionWe prove that the proposed estimator\n$\\widehat{\\mathbf{S}}$ admits an \\textit{affine-invariant} bound of the form\n\\[(1-\\varepsilon) \\mathbf{S} \\preccurlyeq \\widehat{\\mathbf{S}} \\preccurlyeq\n(1+\\varepsilon) \\mathbf{S}\\]in high probability, where $\\mathbf{S}$ is the\nunknown covariance matrix, and $\\preccurlyeq$ is the positive semidefinite\norder on symmetric matrices. The result only requires the existence of\nfourth-order moments, and allows for $\\varepsilon = O(\\sqrt{\\kappa^4\nd\\log(d/\\delta)/n})$ where $\\kappa^4$ is a measure of kurtosis of the\ndistribution, $d$ is the dimensionality of the space, $n$ is the sample size,\nand $1-\\delta$ is the desired confidence level. More generally, we can allow\nfor regularization with level $\\lambda$, then $d$ gets replaced with the\ndegrees of freedom number. Denoting $\\text{cond}(\\mathbf{S})$ the condition\nnumber of $\\mathbf{S}$, the computational cost of the novel estimator is $O(d^2\nn + d^3\\log(\\text{cond}(\\mathbf{S})))$, which is comparable to the cost of the\nsample covariance estimator in the statistically interesing regime $n \\ge d$.\nWe consider applications of our estimator to eigenvalue estimation with\nrelative error, and to ridge regression with heavy-tailed random design.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 14:13:24 GMT"}, {"version": "v2", "created": "Tue, 24 Sep 2019 09:38:41 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Ostrovskii", "Dmitrii", "", "PSL, SIERRA, USC"], ["Rudi", "Alessandro", "", "PSL, SIERRA"]]}, {"id": "1902.03241", "submitter": "Natsumi Makigusa", "authors": "Natsumi Makigusa and Kanta Naito", "title": "Asymptotics and practical aspects of testing normality with kernel\n  methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with testing normality in a Hilbert space based on\nthe maximum mean discrepancy. Specifically, we discuss the behavior of the test\nfrom two standpoints: asymptotics and practical aspects. Asymptotic normality\nof the test under a fixed alternative hypothesis is developed, which implies\nthat the test has consistency. Asymptotic distribution of the test under a\nsequence of local alternatives is also derived, from which asymptotic null\ndistribution of the test is obtained. A concrete expression for the integral\nkernel associated with the null distribution is derived under the use of the\nGaussian kernel, allowing the implementation of a reliable approximation of the\nnull distribution. Simulations and applications to real data sets are reported\nwith emphasis on high-dimension low-sample size cases.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 04:11:33 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Makigusa", "Natsumi", ""], ["Naito", "Kanta", ""]]}, {"id": "1902.03291", "submitter": "Xianyang Zhang", "authors": "Changbo Zhu, Shun Yao, Xianyang Zhang and Xiaofeng Shao", "title": "Distance-based and RKHS-based Dependence Metrics in High Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study distance covariance, Hilbert-Schmidt covariance (aka\nHilbert-Schmidt independence criterion [Gretton et al. (2008)]) and related\nindependence tests under the high dimensional scenario. We show that the sample\ndistance/Hilbert-Schmidt covariance between two random vectors can be\napproximated by the sum of squared componentwise sample cross-covariances up to\nan asymptotically constant factor, which indicates that the\ndistance/Hilbert-Schmidt covariance based test can only capture linear\ndependence in high dimension. As a consequence, the distance correlation based\nt-test developed by Szekely and Rizzo (2013) for independence is shown to have\ntrivial limiting power when the two random vectors are nonlinearly dependent\nbut component-wisely uncorrelated. This new and surprising phenomenon, which\nseems to be discovered for the first time, is further confirmed in our\nsimulation study. As a remedy, we propose tests based on an aggregation of\nmarginal sample distance/Hilbert-Schmidt covariances and show their superior\npower behavior against their joint counterparts in simulations. We further\nextend the distance correlation based t-test to those based on Hilbert-Schmidt\ncovariance and marginal distance/Hilbert-Schmidt covariance. A novel unified\napproach is developed to analyze the studentized sample\ndistance/Hilbert-Schmidt covariance as well as the studentized sample marginal\ndistance covariance under both null and alternative hypothesis. Our theoretical\nand simulation results shed light on the limitation of distance/Hilbert-Schmidt\ncovariance when used jointly in the high dimensional setting and suggest the\naggregation of marginal distance/Hilbert-Schmidt covariance as a useful\nalternative.\n", "versions": [{"version": "v1", "created": "Fri, 8 Feb 2019 21:01:29 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Zhu", "Changbo", ""], ["Yao", "Shun", ""], ["Zhang", "Xianyang", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "1902.03347", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen", "title": "Asymptotic normality of the time-domain generalized least squares\n  estimator for linear regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In linear models, the generalized least squares (GLS) estimator is applicable\nwhen the structure of the error dependence is known. When it is unknown, such\nstructure must be approximated and estimated in a manner that may lead to\nmisspecification. The large-sample analysis of incorrectly-specified GLS (IGLS)\nestimators requires careful asymptotic manipulations. When performing\nestimation in the frequency domain, the asymptotic normality of the IGLS\nestimator, under the so-called Grenander assumptions, has been proved for a\nbroad class of error dependence models. Under the same assumptions, asymptotic\nnormality results for the time-domain IGLS estimator are only available for a\nlimited class of error structures. We prove that the time-domain IGLS estimator\nis asymptotically normal for a general class of dependence models.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 01:21:51 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Nguyen", "Hien D", ""]]}, {"id": "1902.03418", "submitter": "Holger Dette", "authors": "Tim Kutta, Nicolai Bissantz, Justin Chown and Holger Dette", "title": "The empirical process of residuals from an inverse regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate an indirect regression model characterized by\nthe Radon transformation. This model is useful for recovery of medical images\nobtained by computed tomography scans. The indirect regression function is\nestimated using a series estimator motivated by a spectral cut-off technique.\nFurther, we investigate the empirical process of residuals from this\nregression, and show that it satsifies a functional central limit theorem.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 12:38:55 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Kutta", "Tim", ""], ["Bissantz", "Nicolai", ""], ["Chown", "Justin", ""], ["Dette", "Holger", ""]]}, {"id": "1902.03460", "submitter": "Tze Siong Lau", "authors": "Tze Siong Lau, Wee Peng Tay", "title": "Quickest Change Detection in the Presence of a Nuisance Change", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2939080", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the quickest change detection problem in which both nuisance and critical\nchanges may occur, the objective is to detect the critical change as quickly as\npossible without raising an alarm when either there is no change or a nuisance\nchange has occurred. A window-limited sequential change detection procedure\nbased on the generalized likelihood ratio test statistic is proposed. A\nrecursive update scheme for the proposed test statistic is developed and is\nshown to be asymptotically optimal under mild technical conditions. In the\nscenario where the post-change distribution belongs to a parametrized family, a\ngeneralized stopping time and a lower bound on its average run length are\nderived. The proposed stopping rule is compared with the FMA stopping time and\nthe naive 2-stage procedure that detects the nuisance or critical change using\nseparate CuSum stopping procedures for the nuisance and critical changes.\nSimulations demonstrate that the proposed rule outperforms the FMA stopping\ntime and the 2-stage procedure, and experiments on a real dataset on bearing\nfailure verify the performance of the proposed stopping time.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 17:59:26 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 05:28:29 GMT"}, {"version": "v3", "created": "Wed, 13 Feb 2019 06:50:25 GMT"}, {"version": "v4", "created": "Wed, 29 May 2019 16:40:21 GMT"}, {"version": "v5", "created": "Sun, 14 Jul 2019 16:39:46 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Lau", "Tze Siong", ""], ["Tay", "Wee Peng", ""]]}, {"id": "1902.03511", "submitter": "Shashank Singh", "authors": "Ananya Uppal, Shashank Singh, Barnab\\'as P\\'oczos", "title": "Nonparametric Density Estimation & Convergence Rates for GANs under\n  Besov IPM Losses", "comments": "Advances in Neural Information Processing Systems. 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating a nonparametric probability density under\na large family of losses called Besov IPMs, which include, for example,\n$\\mathcal{L}^p$ distances, total variation distance, and generalizations of\nboth Wasserstein and Kolmogorov-Smirnov distances. For a wide variety of\nsettings, we provide both lower and upper bounds, identifying precisely how the\nchoice of loss function and assumptions on the data interact to determine the\nminimax optimal convergence rate. We also show that linear distribution\nestimates, such as the empirical distribution or kernel density estimator,\noften fail to converge at the optimal rate. Our bounds generalize, unify, or\nimprove several recent and classical results. Moreover, IPMs can be used to\nformalize a statistical model of generative adversarial networks (GANs). Thus,\nwe show how our results imply bounds on the statistical error of a GAN,\nshowing, for example, that GANs can strictly outperform the best linear\nestimator.\n", "versions": [{"version": "v1", "created": "Sat, 9 Feb 2019 23:24:43 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 20:02:36 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 20:29:19 GMT"}, {"version": "v4", "created": "Mon, 13 Jan 2020 15:12:33 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Uppal", "Ananya", ""], ["Singh", "Shashank", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1902.03537", "submitter": "Matthew Hirn", "authors": "Michael Perlmutter and Jieqian He and Matthew Hirn", "title": "Scattering Statistics of Generalized Spatial Poisson Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a machine learning model for the analysis of randomly generated\ndiscrete signals, which we model as the points of a homogeneous or\ninhomogeneous, compound Poisson point process. Like the wavelet scattering\ntransform introduced by S. Mallat, our construction is a mathematical model of\nconvolutional neural networks and is naturally invariant to translations and\nreflections. Our model replaces wavelets with Gabor-type measurements and\ntherefore decouples the roles of scale and frequency. We show that, with\nsuitably chosen nonlinearities, our measurements distinguish Poisson point\nprocesses from common self-similar processes, and separate different types of\nPoisson point processes based on the first and second moments of the arrival\nintensity $\\lambda(t)$, as well as the absolute moments of the charges\nassociated to each point.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 04:32:32 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Perlmutter", "Michael", ""], ["He", "Jieqian", ""], ["Hirn", "Matthew", ""]]}, {"id": "1902.03622", "submitter": "Gilles Ducharme", "authors": "Gilles R. Ducharme, Pierre Lafaye de Micheaux", "title": "A goodness-of-fit test for elliptical distributions with diagnostic\n  capabilities", "comments": "35 p. pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a smooth test of goodness-of-fit for elliptical\ndistributions. The test is adaptively omnibus, invariant to affine-linear\ntransformations and has a convenient expression that can be broken into\ncomponents. These components have diagnostic capabilities and can be used to\nidentify specific departures. This helps in correcting the null model when the\ntest rejects. As an example, the results are applied to the multivariate normal\ndistribution for which the R package ECGofTestDx is available. It is shown that\nthe proposed test strategy encompasses and generalizes a number of existing\napproaches. Some other cases are studied, such as the bivariate Laplace,\nlogistic and Pearson type II distribution. A simulation experiment shows the\nusefulness of the diagnostic tools.\n", "versions": [{"version": "v1", "created": "Sun, 10 Feb 2019 15:48:05 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Ducharme", "Gilles R.", ""], ["de Micheaux", "Pierre Lafaye", ""]]}, {"id": "1902.03773", "submitter": "Shaojun Guo", "authors": "Shaojun Guo, Dong Li, Muyi Li", "title": "Strict Stationarity Testing and GLAD Estimation of Double Autoregressive\n  Models", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we develop a tractable procedure for testing strict\nstationarity in a double autoregressive model and formulate the problem as\ntesting if the top Lyapunov exponent is negative. Without strict stationarity\nassumption, we construct a consistent estimator of the associated top Lyapunov\nexponent and employ a random weighting approach for its variance estimation,\nwhich in turn are used in a t-type test. We also propose a GLAD estimation for\nparameters of interest, relaxing key assumptions on the commonly used QMLE. All\nestimators, except for the intercept, are shown to be consistent and\nasymptotically normal in both stationary and explosive situations. The\nfinite-sample performance of the proposed procedures is evaluated via Monte\nCarlo simulation studies and a real dataset of interest rates is analyzed.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 08:28:52 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Guo", "Shaojun", ""], ["Li", "Dong", ""], ["Li", "Muyi", ""]]}, {"id": "1902.03795", "submitter": "Ye Zhang", "authors": "Y. Zhang, Z. Yao, P. Forssen, and T. Fornstedt", "title": "Estimating the Rate Constant from Biosensor Data via an Adaptive\n  Variational Bayesian Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The means to obtain the rate constants of a chemical reaction is a\nfundamental open problem in both science and the industry. Traditional\ntechniques for finding rate constants require either chemical modifications of\nthe reactants or indirect measurements. The rate constant map method is a\nmodern technique to study binding equilibrium and kinetics in chemical\nreactions. Finding a rate constant map from biosensor data is an ill-posed\ninverse problem that is usually solved by regularization. In this work, rather\nthan finding a deterministic regularized rate constant map that does not\nprovide uncertainty quantification of the solution, we develop an adaptive\nvariational Bayesian approach to estimate the distribution of the rate constant\nmap, from which some intrinsic properties of a chemical reaction can be\nexplored, including information about rate constants. Our new approach is more\nrealistic than the existing approaches used for biosensors and allows us to\nestimate the dynamics of the interactions, which are usually hidden in a\ndeterministic approximate solution. We verify the performance of the new\nproposed method by numerical simulations, and compare it with the Markov chain\nMonte Carlo algorithm. The results illustrate that the variational method can\nreliably capture the posterior distribution in a computationally efficient way.\nFinally, the developed method is also tested on the real biosensor data\n(parathyroid hormone), where we provide two novel analysis tools~-- the\nthresholding contour map and the high order moment map -- to estimate the\nnumber of interactions as well as their rate constants.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 10:03:55 GMT"}, {"version": "v2", "created": "Sat, 6 Jul 2019 12:52:05 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Zhang", "Y.", ""], ["Yao", "Z.", ""], ["Forssen", "P.", ""], ["Fornstedt", "T.", ""]]}, {"id": "1902.03809", "submitter": "Yuta Koike", "authors": "Yuta Koike", "title": "High-dimensional central limit theorems for homogeneous sums", "comments": "47 pages. Some errors have been corrected. A reference related to\n  Corollary 3.1 has been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a quantitative version of de Jong's central limit theorem\nfor homogeneous sums in a high-dimensional setting. More precisely, under\nappropriate moment assumptions, we establish an upper bound for the Kolmogorov\ndistance between a multi-dimensional vector of homogeneous sums and a Gaussian\nvector so that the bound depends polynomially on the logarithm of the dimension\nand is governed by the fourth cumulants and the maximal influences of the\ncomponents. As a corollary, we obtain high-dimensional versions of fourth\nmoment theorems, universality results and Peccati-Tudor type theorems for\nhomogeneous sums. We also sharpen some existing (quantitative) central limit\ntheorems by applications of our result.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 10:39:14 GMT"}, {"version": "v2", "created": "Sat, 23 Feb 2019 07:55:33 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Koike", "Yuta", ""]]}, {"id": "1902.03885", "submitter": "Salem Said", "authors": "Salem Said and Jonathan H. Manton", "title": "The Riemannian barycentre as a proxy for global optimisation", "comments": "V1 ; extended version of conference paper ; GSI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $M$ be a simply-connected compact Riemannian symmetric space, and $U$ a\ntwice-differentiable function on $M$, with unique global minimum at $x^* \\in\nM$. The idea of the present work is to replace the problem of searching for the\nglobal minimum of $U$, by the problem of finding the Riemannian barycentre of\nthe Gibbs distribution $P_{\\scriptscriptstyle{T}} \\propto \\exp(-U/T)$. In other\nwords, instead of minimising the function $U$ itself, to minimise\n$\\mathcal{E}_{\\scriptscriptstyle{T}}(x) = \\frac{1}{2}\\int d^{\\scriptscriptstyle\n2}(x,z)P_{\\scriptscriptstyle{T}}(dz)$, where $d(\\cdot,\\cdot)$ denotes\nRiemannian distance. The following original result is proved : if $U$ is\ninvariant by geodesic symmetry about $x^*$, then for each $\\delta < \\frac{1}{2}\nr_{\\scriptscriptstyle cx}$ ($r_{\\scriptscriptstyle cx}$ the convexity radius of\n$M$), there exists $T_{\\scriptscriptstyle \\delta}$ such that $T \\leq\nT_{\\scriptscriptstyle \\delta}$ implies $\\mathcal{E}_{\\scriptscriptstyle{T}}$ is\nstrongly convex on the geodesic ball $B(x^*,\\delta)\\,$, and $x^*$ is the unique\nglobal minimum of $\\mathcal{E}_{\\scriptscriptstyle{T\\,}}$. Moreover, this\n$T_{\\scriptscriptstyle \\delta}$ can be computed explicitly. This result gives\nrise to a general algorithm for black-box optimisation, which is briefly\ndescribed, and will be further explored in future work.\n", "versions": [{"version": "v1", "created": "Mon, 11 Feb 2019 14:07:36 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Said", "Salem", ""], ["Manton", "Jonathan H.", ""]]}, {"id": "1902.04304", "submitter": "Hannes Leeb", "authors": "Hannes Leeb, Lukas Steinberger", "title": "Statistical inference with F-statistics when fitting simple models to\n  high-dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study linear subset regression in the context of the high-dimensional\noverall model $y = \\vartheta+\\theta' z + \\epsilon$ with univariate response $y$\nand a $d$-vector of random regressors $z$, independent of $\\epsilon$. Here,\n\"high-dimensional\" means that the number $d$ of available explanatory variables\nis much larger than the number $n$ of observations. We consider simple linear\nsub-models where $y$ is regressed on a set of $p$ regressors given by $x =\nM'z$, for some $d \\times p$ matrix $M$ of full rank $p < n$. The corresponding\nsimple model, i.e., $y=\\alpha+\\beta' x + e$, can be justified by imposing\nappropriate restrictions on the unknown parameter $\\theta$ in the overall\nmodel; otherwise, this simple model can be grossly misspecified. In this paper,\nwe establish asymptotic validity of the standard $F$-test on the surrogate\nparameter $\\beta$, in an appropriate sense, even when the simple model is\nmisspecified.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 09:52:48 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Leeb", "Hannes", ""], ["Steinberger", "Lukas", ""]]}, {"id": "1902.04489", "submitter": "Tobias Fissler", "authors": "Tobias Fissler and Johanna F. Ziegel", "title": "Evaluating Range Value at Risk Forecasts", "comments": "25 pages, 2 figures An earlier version of this paper was circulated\n  under the name 'Elicitability of Range Value at Risk'. The presentation has\n  been made more concise and minor errors have been corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.MF q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The debate of what quantitative risk measure to choose in practice has mainly\nfocused on the dichotomy between Value at Risk (VaR) -- a quantile -- and\nExpected Shortfall (ES) -- a tail expectation. Range Value at Risk (RVaR) is a\nnatural interpolation between these two prominent risk measures, which\nconstitutes a tradeoff between the sensitivity of the latter and the robustness\nof the former, turning it into a practically relevant risk measure on its own.\nAs such, there is a need to statistically validate RVaR forecasts and to\ncompare and rank the performance of different RVaR models, tasks subsumed under\nthe term 'backtesting' in finance. The predictive performance is best evaluated\nand compared in terms of strictly consistent loss or scoring functions. That\nis, functions which are minimised in expectation by the correct RVaR forecast.\nMuch like ES, it has been shown recently that RVaR does not admit strictly\nconsistent scoring functions, i.e., it is not elicitable. Mitigating this\nnegative result, this paper shows that a triplet of RVaR with two VaR\ncomponents at different levels is elicitable. We characterise the class of\nstrictly consistent scoring functions for this triplet. Additional properties\nof these scoring functions are examined, including the diagnostic tool of\nMurphy diagrams. The results are illustrated with a simulation study, and we\nput our approach in perspective with respect to the classical approach of\ntrimmed least squares in robust regression.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 16:46:58 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 15:41:17 GMT"}, {"version": "v3", "created": "Sat, 28 Nov 2020 15:11:59 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Fissler", "Tobias", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "1902.04496", "submitter": "Sera Aylin Cakiroglu", "authors": "Sera Aylin Cakiroglu and Peter J Cameron", "title": "Optimal BIBD-extended designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balanced incomplete block designs (BIBDs) are a class of designs with v\ntreatments and b blocks of size k that are optimal with regards to a wide range\nof optimality criteria, but it is not clear which designs to choose for\ncombinations of v, b and k when BIBDs do not exist.\n  In 1992, Cheng showed that for sufficiently large b, the designs which are\noptimal with respect to commonly used criteria (including the A- and D-\ncriteria) must be found among (M.S)-optimal designs. In particular, this result\nconfirmed the conjecture of John and Mitchell in 1977 on the optimality of\nregular graph designs (RGDs) in the case of large numbers of blocks.\n  We investigate the effect of extending known optimal binary designs by\nrepeatedly adding the blocks of a BIBD and find boundaries for the number of\nblock so that these BIBD-extended designs are optimal. In particular, we will\nstudy the designs for k=2 and b=v-1 and b=v: in these cases the A- and\nD-optimal designs are not the same but we show that this changes after adding\nblocks of a BIBD and the same design becomes A- and D-optimal amongst the\ncollection of extended designs. Finally, we characterise those RGDs that give\nrise to A- and D-optimal extended designs and extend a result on the\nD-optimality of the a group-divisible design to A- and D-optimality amongst\nBIBD-extended designs.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 17:01:43 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Cakiroglu", "Sera Aylin", ""], ["Cameron", "Peter J", ""]]}, {"id": "1902.04553", "submitter": "Ramya Korlakai Vinayak", "authors": "Ramya Korlakai Vinayak, Weihao Kong, Gregory Valiant and Sham M.\n  Kakade", "title": "Maximum Likelihood Estimation for Learning Populations of Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a setting with $N$ independent individuals, each with an unknown\nparameter, $p_i \\in [0, 1]$ drawn from some unknown distribution $P^\\star$.\nAfter observing the outcomes of $t$ independent Bernoulli trials, i.e., $X_i\n\\sim \\text{Binomial}(t, p_i)$ per individual, our objective is to accurately\nestimate $P^\\star$. This problem arises in numerous domains, including the\nsocial sciences, psychology, health-care, and biology, where the size of the\npopulation under study is usually large while the number of observations per\nindividual is often limited. Our main result shows that, in the regime where $t\n\\ll N$, the maximum likelihood estimator (MLE) is both statistically minimax\noptimal and efficiently computable. Precisely, for sufficiently large $N$, the\nMLE achieves the information theoretic optimal error bound of\n$\\mathcal{O}(\\frac{1}{t})$ for $t < c\\log{N}$, with regards to the earth\nmover's distance (between the estimated and true distributions). More\ngenerally, in an exponentially large interval of $t$ beyond $c \\log{N}$, the\nMLE achieves the minimax error bound of $\\mathcal{O}(\\frac{1}{\\sqrt{t\\log\nN}})$. In contrast, regardless of how large $N$ is, the naive \"plug-in\"\nestimator for this problem only achieves the sub-optimal error of\n$\\Theta(\\frac{1}{\\sqrt{t}})$.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 18:55:31 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Vinayak", "Ramya Korlakai", ""], ["Kong", "Weihao", ""], ["Valiant", "Gregory", ""], ["Kakade", "Sham M.", ""]]}, {"id": "1902.04650", "submitter": "Arnak Dalalyan S.", "authors": "Amir-Hossein Bateni and Arnak S. Dalalyan", "title": "Confidence regions and minimax rates in outlier-robust estimation on the\n  probability simplex", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the mean of a distribution supported by\nthe $k$-dimensional probability simplex in the setting where an $\\varepsilon$\nfraction of observations are subject to adversarial corruption. A simple\nparticular example is the problem of estimating the distribution of a discrete\nrandom variable. Assuming that the discrete variable takes $k$ values, the\nunknown parameter $\\boldsymbol \\theta$ is a $k$-dimensional vector belonging to\nthe probability simplex. We first describe various settings of contamination\nand discuss the relation between these settings. We then establish minimax\nrates when the quality of estimation is measured by the total-variation\ndistance, the Hellinger distance, or the $\\mathbb L^2$-distance between two\nprobability measures. We also provide confidence regions for the unknown mean\nthat shrink at the minimax rate. Our analysis reveals that the minimax rates\nassociated to these three distances are all different, but they are all\nattained by the sample average. Furthermore, we show that the latter is\nadaptive to the possible sparsity of the unknown vector. Some numerical\nexperiments illustrating our theoretical findings are reported.\n", "versions": [{"version": "v1", "created": "Tue, 12 Feb 2019 21:52:30 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 10:08:24 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Bateni", "Amir-Hossein", ""], ["Dalalyan", "Arnak S.", ""]]}, {"id": "1902.04749", "submitter": "Xiaoling Dou", "authors": "Xiaoling Dou, Satoshi Kuriki, Gwo Dong Lin, Donald Richards", "title": "Dependence Properties of B-Spline Copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct by using B-spline functions a class of copulas that includes the\nBernstein copulas arising in Baker's distributions. The range of correlation of\nthe B-spline copulas is examined, and the Frechet--Hoeffding upper bound is\nproved to be attained when the number of B-spline functions goes to infinity.\nAs the B-spline functions are well-known to be an order-complete weak\nTchebycheff system from which the property of total positivity of any order\nfollows for the maximum correlation case, the results given here extend\nclassical results for the Bernstein copulas. In addition, we derive in terms of\nthe Stirling numbers of the second kind an explicit formula for the moments of\nthe related B-spline functions for nonnegative real numbers.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 05:45:24 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Dou", "Xiaoling", ""], ["Kuriki", "Satoshi", ""], ["Lin", "Gwo Dong", ""], ["Richards", "Donald", ""]]}, {"id": "1902.04962", "submitter": "Claudia Kluppelberg", "authors": "Claudia Kl\\\"uppelberg and Viet Son Pham", "title": "Estimation of causal CARMA random fields", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We estimate model parameters of L\\'evy-driven causal CARMA random fields by\nfitting the empirical variogram to the theoretical counterpart using a weighted\nleast squares (WLS) approach. Subsequent to deriving asymptotic results for the\nvariogram estimator, we show strong consistency and asymptotic normality of the\nparameter estimator. Furthermore, we conduct a simulation study to assess the\nquality of the WLS estimator for finite samples. For the simulation we utilize\nnumerical approximation schemes based on truncation and discretization of\nstochastic integrals and we analyze the associated simulation errors in detail.\nFinally, we apply our results to real data of the cosmic microwave background.\n", "versions": [{"version": "v1", "created": "Wed, 13 Feb 2019 15:47:45 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Kl\u00fcppelberg", "Claudia", ""], ["Pham", "Viet Son", ""]]}, {"id": "1902.05180", "submitter": "Vedhas Pandit", "authors": "Vedhas Pandit, Bj\\\"orn Schuller", "title": "The Many-to-Many Mapping Between the Concordance Correlation Coefficient\n  and the Mean Square Error", "comments": "Why this discovery, or the mapping formulation is important:\n  MSE1<MSE2 does not necessarily mean CCC1>CCC2. In other words, MSE\n  minimisation does not necessarily guarantee CCC maximisation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the mapping between two of the most pervasive utility functions,\nthe mean square error ($MSE$) and the concordance correlation coefficient (CCC,\n$\\rho_c$). Despite its drawbacks, $MSE$ is one of the most popular performance\nmetrics (and a loss function); along with lately $\\rho_c$ in many of the\nsequence prediction challenges. Despite the ever-growing simultaneous usage,\ne.g., inter-rater agreement, assay validation, a mapping between the two\nmetrics is missing, till date. While minimisation of $L_p$ norm of the errors\nor of its positive powers (e.g., $MSE$) is aimed at $\\rho_c$ maximisation, we\nreason the often-witnessed ineffectiveness of this popular loss function with\ngraphical illustrations. The discovered formula uncovers not only the\ncounterintuitive revelation that `$MSE_1<MSE_2$' does not imply\n`$\\rho_{c_1}>\\rho_{c_2}$', but also provides the precise range for the $\\rho_c$\nmetric for a given $MSE$. We discover the conditions for $\\rho_c$ optimisation\nfor a given $MSE$; and as a logical next step, for a given set of errors. We\ngeneralise and discover the conditions for any given $L_p$ norm, for an even p.\nWe present newly discovered, albeit apparent, mathematical paradoxes. The study\ninspires and anticipates a growing use of $\\rho_c$-inspired loss functions\ne.g., $\\left|\\frac{MSE}{\\sigma_{XY}}\\right|$, replacing the traditional\n$L_p$-norm loss functions in multivariate regressions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 01:36:27 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 00:20:05 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2019 01:55:21 GMT"}, {"version": "v4", "created": "Mon, 4 May 2020 22:11:09 GMT"}, {"version": "v5", "created": "Fri, 12 Jun 2020 18:58:02 GMT"}, {"version": "v6", "created": "Wed, 1 Jul 2020 18:01:35 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Pandit", "Vedhas", ""], ["Schuller", "Bj\u00f6rn", ""]]}, {"id": "1902.05261", "submitter": "Hajo Holzmann", "authors": "Hajo Holzmann and Alexander Meister", "title": "Rate-optimal nonparametric estimation for random coefficient regression\n  models", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random coefficient regression models are a popular tool for analyzing\nunobserved heterogeneity, and have seen renewed interest in the recent\neconometric literature. In this paper we obtain the optimal pointwise\nconvergence rate for estimating the density in the linear random coefficient\nmodel over H\\\"older smoothness classes, and in particular show how the tail\nbehavior of the design density impacts this rate. In contrast to previous\nsuggestions, the estimator that we propose and that achieves the optimal\nconvergence rate does not require dividing by a nonparametric density estimate.\nThe optimal choice of the tuning parameters in the estimator depends on the\ntail parameter of the design density and on the smoothness level of the\nH\\\"older class, and we also study adaptive estimation with respect to both\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 08:41:00 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 10:22:29 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Holzmann", "Hajo", ""], ["Meister", "Alexander", ""]]}, {"id": "1902.05354", "submitter": "Zacharie Naulet", "authors": "Federico Camerlenghi, Stefano Favaro, Zacharie Naulet, Francesca\n  Panero", "title": "Optimal disclosure risk assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protection against disclosure is a legal and ethical obligation for agencies\nreleasing microdata files for public use. Consider a microdata sample of size\n$n$ from a finite population of size $\\bar{n}=n+\\lambda n$, with $\\lambda>0$,\nsuch that each record contains two disjoint types of information: identifying\ncategorical information and sensitive information. Any decision about releasing\ndata is supported by the estimation of measures of disclosure risk, which are\nfunctionals of the number of sample records with a unique combination of values\nof identifying variables. The most common measure is arguably the number\n$\\tau_{1}$ of sample unique records that are population uniques. In this paper,\nwe first study nonparametric estimation of $\\tau_{1}$ under the Poisson\nabundance model for sample records. We introduce a class of linear estimators\nof $\\tau_{1}$ that are simple, computationally efficient and scalable to\nmassive datasets, and we give uniform theoretical guarantees for them. In\nparticular, we show that they provably estimate $\\tau_{1}$ all of the way up to\nthe sampling fraction $(\\lambda+1)^{-1}\\propto (\\log n)^{-1}$, with vanishing\nnormalized mean-square error (NMSE) for large $n$. We then establish a lower\nbound for the minimax NMSE for the estimation of $\\tau_{1}$, which allows us to\nshow that: i) $(\\lambda+1)^{-1}\\propto (\\log n)^{-1}$ is the smallest possible\nsampling fraction; ii) estimators' NMSE is near optimal, in the sense of\nmatching the minimax lower bound, for large $n$. This is the main result of our\npaper, and it provides a precise answer to an open question about the\nfeasibility of nonparametric estimation of $\\tau_{1}$ under the Poisson\nabundance model and for a sampling fraction $(\\lambda+1)^{-1}<1/2$.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 13:53:43 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Camerlenghi", "Federico", ""], ["Favaro", "Stefano", ""], ["Naulet", "Zacharie", ""], ["Panero", "Francesca", ""]]}, {"id": "1902.05404", "submitter": "Abhishake Rastogi", "authors": "Abhishake Rastogi, Gilles Blanchard and Peter Math\\'e", "title": "Convergence analysis of Tikhonov regularization for non-linear\n  statistical inverse learning problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a non-linear statistical inverse learning problem, where we observe\nthe noisy image of a quantity through a non-linear operator at some random\ndesign points. We consider the widely used Tikhonov regularization (or method\nof regularization, MOR) approach to reconstruct the estimator of the quantity\nfor the non-linear ill-posed inverse problem. The estimator is defined as the\nminimizer of a Tikhonov functional, which is the sum of a data misfit term and\na quadratic penalty term. We develop a theoretical analysis for the minimizer\nof the Tikhonov regularization scheme using the ansatz of reproducing kernel\nHilbert spaces. We discuss optimal rates of convergence for the proposed\nscheme, uniformly over classes of admissible solutions, defined through\nappropriate source conditions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 15:00:09 GMT"}, {"version": "v2", "created": "Fri, 1 Mar 2019 13:52:27 GMT"}], "update_date": "2019-03-04", "authors_parsed": [["Rastogi", "Abhishake", ""], ["Blanchard", "Gilles", ""], ["Math\u00e9", "Peter", ""]]}, {"id": "1902.05520", "submitter": "Iosif Pinelis", "authors": "Iosif Pinelis", "title": "Generalized semimodularity: order statistics", "comments": "To appear in the proceedings of the conference High Dimensional\n  Probability 8, held in Oaxaca (Mexico) in 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A notion of generalized $n$-semimodularity is introduced, which extends that\nof (sub/super)mod\\-ularity in four ways at once. The main result of this paper,\nstating that every generalized $(n\\colon\\!2)$-semimodular function on the $n$th\nCartesian power of a distributive lattice is generalized $n$-semimodular, may\nbe considered a multi/infinite-dimensional analogue of the well-known Muirhead\nlemma in the theory of Schur majorization. This result is also similar to a\ndiscretized version of the well-known theorem due to Lorentz, which latter was\ngiven only for additive-type functions. Illustrations of our main result are\npresented for counts of combinations of faces of a polytope; one-sided\npotentials; multiadditive forms, including multilinear ones -- in particular,\npermanents of rectangular matrices and elementary symmetric functions; and\nassociation inequalities for order statistics. Based on an extension of the FKG\ninequality due to Rinott \\& Saks and Aharoni \\& Keich, applications to\ncorrelation inequalities for order statistics are given as well.\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 17:56:19 GMT"}], "update_date": "2019-02-15", "authors_parsed": [["Pinelis", "Iosif", ""]]}, {"id": "1902.05616", "submitter": "Yury Polyanskiy", "authors": "Yury Polyanskiy and Yihong Wu", "title": "Dualizing Le Cam's method for functional estimation, with applications\n  to estimating the unseens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Le Cam's method (or the two-point method) is a commonly used tool for\nobtaining statistical lower bound and especially popular for functional\nestimation problems. This work aims to explain and give conditions for the\ntightness of Le Cam's lower bound in functional estimation from the perspective\nof convex duality. Under a variety of settings it is shown that the\nmaximization problem that searches for the best two-point lower bound, upon\ndualizing, becomes a minimization problem that optimizes the bias-variance\ntradeoff among a family of estimators.\n  For estimating linear functionals of a distribution our work strengthens\nprior results of Donoho-Liu \\cite{DL91} (for quadratic loss) by dropping the\nH\\\"olderian assumption on the modulus of continuity. For exponential families\nour results extend those of Juditsky-Nemirovski \\cite{JN09} by characterizing\nthe minimax risk for the quadratic loss under weaker assumptions on the\nexponential family.\n  We also provide an extension to the high-dimensional setting for estimating\nseparable functionals. Notably, coupled with tools from complex analysis, this\nmethod is particularly effective for characterizing the ``elbow effect'' -- the\nphase transition from parametric to nonparametric rates. As the main\napplication we derive sharp minimax rates in the Distinct elements problem\n(given a fraction $p$ of colored balls from an urn containing $d$ balls, the\noptimal error of estimating the number of distinct colors is $\\tilde\n\\Theta(d^{-\\frac{1}{2}\\min\\{\\frac{p}{1-p},1\\}})$) and the Fisher's species\nproblem (given $n$ iid observations from an unknown distribution, the optimal\nprediction error of the number of unseen symbols in the next (unobserved) $r\n\\cdot n$ observations is $\\tilde\n\\Theta(n^{-\\min\\{\\frac{1}{r+1},\\frac{1}{2}\\}})$).\n", "versions": [{"version": "v1", "created": "Thu, 14 Feb 2019 21:51:18 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 14:54:52 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Polyanskiy", "Yury", ""], ["Wu", "Yihong", ""]]}, {"id": "1902.05813", "submitter": "Qianqian Zhu", "authors": "Qianqian Zhu and Guodong Li", "title": "Quantile double autoregression", "comments": "This article has 46 pages, 6 tables and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many financial time series have varying structures at different quantile\nlevels, and also exhibit the phenomenon of conditional heteroscedasticity at\nthe same time. In the meanwhile, it is still lack of a time series model to\naccommodate both of the above features simultaneously. This paper fills the gap\nby proposing a novel conditional heteroscedastic model, which is called the\nquantile double autoregression. The strict stationarity of the new model is\nderived, and a self-weighted conditional quantile estimation is suggested. Two\npromising properties of the original double autoregressive model are shown to\nbe preserved. Based on the quantile autocorrelation function and self-weighting\nconcept, two portmanteau tests are constructed, and they can be used in\nconjunction to check the adequacy of fitted conditional quantiles. The\nfinite-sample performance of the proposed inference tools is examined by\nsimulation studies, and the necessity of the new model is further demonstrated\nby analyzing the S&P500 Index.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 14:07:46 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 14:29:28 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Zhu", "Qianqian", ""], ["Li", "Guodong", ""]]}, {"id": "1902.05853", "submitter": "Stilian Stoev", "authors": "Robert Yuen, Stilian Stoev, Dan Cooley", "title": "Distributionally Robust Inference for Extreme Value-at-Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under general multivariate regular variation conditions, the extreme\nValue-at-Risk of a portfolio can be expressed as an integral of a known kernel\nwith respect to a generally unknown spectral measure supported on the unit\nsimplex. The estimation of the spectral measure is challenging in practice and\nvirtually impossible in high dimensions. This motivates the problem studied in\nthis work, which is to find universal lower and upper bounds of the extreme\nValue-at-Risk under practically estimable constraints. That is, we study the\ninfimum and supremum of the extreme Value-at-Risk functional, over the infinite\ndimensional space of all possible spectral measures that meet a finite set of\nconstraints. We focus on extremal coefficient constraints, which are popular\nand easy to interpret in practice. Our contributions are twofold. Firstly, we\nshow that optimization problems over an infinite dimensional space of spectral\nmeasures are in fact dual problems to linear semi-infinite programs (LSIPs) --\nlinear optimization problems in an Euclidean space with an uncountable set of\nlinear constraints. This allows us to prove that the optimal solutions are in\nfact attained by discrete spectral measures supported on finitely many atoms.\nSecond, in the case of balanced portfolia, we establish further structural\nresults for the lower bounds as well as closed form solutions for both the\nlower- and upper-bounds of extreme Value-at-Risk in the special case of a\nsingle extremal coefficient constraint. The solutions unveil important\nconnections to the Tawn-Molchanov max-stable models. The results are\nillustrated with a real data example.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 15:38:37 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 18:09:03 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2020 15:44:28 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Yuen", "Robert", ""], ["Stoev", "Stilian", ""], ["Cooley", "Dan", ""]]}, {"id": "1902.06002", "submitter": "Jonathan Scarlett", "authors": "Matthew Aldridge, Oliver Johnson, Jonathan Scarlett", "title": "Group Testing: An Information Theory Perspective", "comments": "Survey paper, 140 pages, 19 figures. To be published in Foundations\n  and Trends in Communications and Information Theory", "journal-ref": "Foundations and Trends in Communications and Information Theory:\n  Vol. 15: No. 3-4, pp 196-392, 2019", "doi": "10.1561/0100000099", "report-no": null, "categories": "cs.IT cs.DM math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The group testing problem concerns discovering a small number of defective\nitems within a large population by performing tests on pools of items. A test\nis positive if the pool contains at least one defective, and negative if it\ncontains no defectives. This is a sparse inference problem with a combinatorial\nflavour, with applications in medical testing, biology, telecommunications,\ninformation technology, data science, and more. In this monograph, we survey\nrecent developments in the group testing problem from an information-theoretic\nperspective. We cover several related developments: efficient algorithms with\npractical storage and computation requirements, achievability bounds for\noptimal decoding methods, and algorithm-independent converse bounds. We assess\nthe theoretical guarantees not only in terms of scaling laws, but also in terms\nof the constant factors, leading to the notion of the {\\em rate} of group\ntesting, indicating the amount of information learned per test. Considering\nboth noiseless and noisy settings, we identify several regimes where existing\nalgorithms are provably optimal or near-optimal, as well as regimes where there\nremains greater potential for improvement. In addition, we survey results\nconcerning a number of variations on the standard group testing problem,\nincluding partial recovery criteria, adaptive algorithms with a limited number\nof stages, constrained test designs, and sublinear-time algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 15 Feb 2019 23:04:24 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 22:43:13 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 04:11:06 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Aldridge", "Matthew", ""], ["Johnson", "Oliver", ""], ["Scarlett", "Jonathan", ""]]}, {"id": "1902.06015", "submitter": "Song Mei", "authors": "Song Mei, Theodor Misiakiewicz, Andrea Montanari", "title": "Mean-field theory of two-layers neural networks: dimension-free bounds\n  and kernel limit", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.stat-mech cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning two layer neural networks using stochastic gradient\ndescent. The mean-field description of this learning dynamics approximates the\nevolution of the network weights by an evolution in the space of probability\ndistributions in $R^D$ (where $D$ is the number of parameters associated to\neach neuron). This evolution can be defined through a partial differential\nequation or, equivalently, as the gradient flow in the Wasserstein space of\nprobability distributions. Earlier work shows that (under some regularity\nassumptions), the mean field description is accurate as soon as the number of\nhidden units is much larger than the dimension $D$. In this paper we establish\nstronger and more general approximation guarantees. First of all, we show that\nthe number of hidden units only needs to be larger than a quantity dependent on\nthe regularity properties of the data, and independent of the dimensions. Next,\nwe generalize this analysis to the case of unbounded activation functions,\nwhich was not covered by earlier bounds. We extend our results to noisy\nstochastic gradient descent.\n  Finally, we show that kernel ridge regression can be recovered as a special\nlimit of the mean field analysis.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 00:01:01 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Mei", "Song", ""], ["Misiakiewicz", "Theodor", ""], ["Montanari", "Andrea", ""]]}, {"id": "1902.06021", "submitter": "Enguerrand Horel", "authors": "Enguerrand Horel, Kay Giesecke", "title": "Significance Tests for Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a pivotal test to assess the statistical significance of the\nfeature variables in a single-layer feedforward neural network regression\nmodel. We propose a gradient-based test statistic and study its asymptotics\nusing nonparametric techniques. Under technical conditions, the limiting\ndistribution is given by a mixture of chi-square distributions. The tests\nenable one to discern the impact of individual variables on the prediction of a\nneural network. The test statistic can be used to rank variables according to\ntheir influence. Simulation results illustrate the computational efficiency and\nthe performance of the test. An empirical application to house price valuation\nhighlights the behavior of the test using actual data.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 01:28:05 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 07:23:02 GMT"}, {"version": "v3", "created": "Sun, 8 Nov 2020 21:10:43 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Horel", "Enguerrand", ""], ["Giesecke", "Kay", ""]]}, {"id": "1902.06043", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle, Jerome P. Reiter", "title": "Sequentially additive nonignorable missing data modeling using auxiliary\n  marginal information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of missingness mechanisms, called sequentially additive\nnonignorable, for modeling multivariate data with item nonresponse. These\nmechanisms explicitly allow the probability of nonresponse for each variable to\ndepend on the value of that variable, thereby representing nonignorable\nmissingness mechanisms. These missing data models are identified by making use\nof auxiliary information on marginal distributions, such as marginal\nprobabilities for multivariate categorical variables or moments for numeric\nvariables. We present theory proving identification results, and illustrate the\nuse of these mechanisms in an application.\n", "versions": [{"version": "v1", "created": "Sat, 16 Feb 2019 05:11:12 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Sadinle", "Mauricio", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1902.06441", "submitter": "Melisande Albert", "authors": "M\\'elisande Albert (IMT, INSA Toulouse), B\\'eatrice Laurent (IMT, INSA\n  Toulouse), Amandine Marrel, Anouar Meynaoui (IMT, INSA Toulouse)", "title": "Adaptive test of independence based on HSIC measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependence measures based on reproducing kernel Hilbert spaces, also known as\nHilbert-Schmidt Independence Criterion and denoted HSIC, are widely used to\nstatistically decide whether or not two random vectors are dependent. Recently,\nnon-parametric HSIC-based statistical tests of independence have been\nperformed. However, these tests lead to the question of the choice of the\nkernels associated to the HSIC. In particular, there is as yet no method to\nobjectively select specific kernels with theoretical guarantees in terms of\nfirst and second kind errors. One of the main contributions of this work is to\ndevelop a new HSIC-based aggregated procedure which avoids such a kernel\nchoice, and to provide theoretical guarantees for this procedure. To achieve\nthis, we first introduce non-asymptotic single tests based on Gaussian kernels\nwith a given bandwidth, which are of prescribed level $\\alpha \\in (0,1)$. From\na theoretical point of view, we upper-bound their uniform separation rate of\ntesting over Sobolev and Nikol'skii balls. Then, we aggregate several single\ntests, and obtain similar upper-bounds for the uniform separation rate of the\naggregated procedure over the same regularity spaces. Another main contribution\nis that we provide a lower-bound for the non-asymptotic minimax separation rate\nof testing over Sobolev balls, and deduce that the aggregated procedure is\nadaptive in the minimax sense over such regularity spaces. Finally, from a\npractical point of view, we perform numerical studies in order to assess the\nefficiency of our aggregated procedure and compare it to existing independence\ntests in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 08:00:31 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 13:42:39 GMT"}, {"version": "v3", "created": "Fri, 30 Aug 2019 07:27:33 GMT"}, {"version": "v4", "created": "Tue, 1 Oct 2019 14:49:53 GMT"}, {"version": "v5", "created": "Tue, 12 Jan 2021 10:01:52 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Albert", "M\u00e9lisande", "", "IMT, INSA Toulouse"], ["Laurent", "B\u00e9atrice", "", "IMT, INSA\n  Toulouse"], ["Marrel", "Amandine", "", "IMT, INSA Toulouse"], ["Meynaoui", "Anouar", "", "IMT, INSA Toulouse"]]}, {"id": "1902.06603", "submitter": "Jeffrey Negrea", "authors": "Jeffrey Negrea", "title": "Optimal Scaling and Shaping of Random Walk Metropolis via Diffusion\n  Limits of Block-I.I.D. Targets", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work extends Roberts et al. (1997) by considering limits of Random Walk\nMetropolis (RWM) applied to block IID target distributions, with corresponding\nblock-independent proposals. The extension verifies the robustness of the\noptimal scaling heuristic, to tune the acceptance rate to $\\approx0.234$, for\nany choice of proposal shaping. We upgrade the form of weak convergence from a\nfinite-dimensional subprocess to the infinite dimensional process. We show that\nthe optimal shaping (in terms of the decay of autocorrelations of linear\nfunctions) is the variance of the target distribution. We show that this choice\ncoincides with the optimal shaping in terms of spectral gaps in special cases\nwhere they can be computed. Lastly, we provide some negative guarantees,\nshowing that RWM performance degrades with higher-order dependence. In such\ncases, no tuning of RWM will yield performance comparable to an IID target.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 15:05:50 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Negrea", "Jeffrey", ""]]}, {"id": "1902.06622", "submitter": "Tadeusz Inglot", "authors": "Tadeusz Inglot", "title": "Intermediate efficiency of tests under heavy-tailed alternatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for local alternatives which are not square integrable the\nintermediate (or Kallenberg) efficiency of the Neyman-Pearson test for\nuniformity with respect to the classical Kolmogorov-Smirnov test is equal to\ninfinity. Contrary to this, for local square integrable alternatives the\nintermediate efficiency is finite and can be explicitly calculated.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 15:48:14 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Inglot", "Tadeusz", ""]]}, {"id": "1902.06800", "submitter": "Abram Kagan", "authors": "Abram M. Kagan", "title": "The KLR-theorem revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For independent random variables $X_1,\\ldots, X_n;Y_1,\\ldots, Y_n$ with all\n$X_i$ identically distributed and same for $Y_j$, we study the relation\n\\[E\\{a\\bar X + b\\bar Y|X_1 -\\bar X +Y_1 -\\bar Y,\\ldots,X_n -\\bar X +Y_n -\\bar\nY\\}={\\rm const}\\] with $a, b$ some constants. It is proved that for $n\\geq 3$\nand $ab>0$ the relation holds iff $X_i$ and $Y_j$ are Gaussian.\\\\ A new\ncharacterization arises in case of $a=1, b= -1$. In this case either $X_i$ or\n$Y_j$ or both have a Gaussian component. It is the first (at least known to the\nauthor) case when presence of a Gaussian component is a characteristic\nproperty.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 20:59:34 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Kagan", "Abram M.", ""]]}, {"id": "1902.06802", "submitter": "Abram Kagan", "authors": "Abram M. Kagan", "title": "Efficiency requires innovation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In estimation a parameter $\\theta\\in{\\mathbb R}$ from a sample\n$(x_1,\\ldots,x_n)$ from a population $P_{\\theta}$ a simple way of incorporating\na new observation $x_{n+1}$ into an estimator $\\tilde\\theta_{n} =\n\\tilde\\theta_{n}(x_1,\\ldots,x_n)$ is transforming $\\tilde\\theta_n$ to what we\ncall the {\\it jackknife extension} $\\tilde\\theta_{n+1}^{(e)} =\n\\tilde\\theta_{n+1}^{(e)}(x_1,\\ldots,x_n,x_{n+1})$, \\[\\tilde\\theta_{n+1}^{(e)} =\n\\{\\tilde\\theta_n (x_1 ,\\ldots,x_n)+ \\tilde\\theta_n (x_{n+1},x_2 ,\\ldots,x_n) +\n\\ldots + \\tilde\\theta_n (x_1 ,\\ldots,x_{n-1},x_{n+1})\\}/(n+1).\\] Though\n$\\tilde\\theta_{n+1}^{(e)}$ lacks an innovation the statistician could expect\nfrom a larger data set, it is still better than $\\tilde\\theta_n$, \\[{\\rm\nvar}(\\tilde\\theta_{n+1}^{(e)})\\leq\\frac{n}{n+1} {\\rm var}(\\tilde\\theta_n).\\]\nHowever, an estimator obtained by jackknife extension for all $n$ is\nasymptotically efficient only for samples from exponential families. For a\ngeneral $P_{\\theta}$, asymptotically efficient estimators require innovation\nwhen a new observation is added to the data. Some examples illustrate the\nconcept.\n", "versions": [{"version": "v1", "created": "Mon, 18 Feb 2019 21:07:54 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Kagan", "Abram M.", ""]]}, {"id": "1902.06846", "submitter": "Jinchi Lv", "authors": "Jianqing Fan, Yingying Fan, Xiao Han, Jinchi Lv", "title": "Asymptotic Theory of Eigenvectors for Random Matrices with Diverging\n  Spikes", "comments": "78 pages, 3 figures; Journal of the American Statistical Association,\n  to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing the asymptotic distributions of eigenvectors for large random\nmatrices poses important challenges yet can provide useful insights into a\nrange of statistical applications. To this end, in this paper we introduce a\ngeneral framework of asymptotic theory of eigenvectors (ATE) for large spiked\nrandom matrices with diverging spikes and heterogeneous variances, and\nestablish the asymptotic properties of the spiked eigenvectors and eigenvalues\nfor the scenario of the generalized Wigner matrix noise. Under some mild\nregularity conditions, we provide the asymptotic expansions for the spiked\neigenvalues and show that they are asymptotically normal after some\nnormalization. For the spiked eigenvectors, we establish asymptotic expansions\nfor the general linear combination and further show that it is asymptotically\nnormal after some normalization, where the weight vector can be arbitrary. We\nalso provide a more general asymptotic theory for the spiked eigenvectors using\nthe bilinear form. Simulation studies verify the validity of our new\ntheoretical results. Our family of models encompasses many popularly used ones\nsuch as the stochastic block models with or without overlapping communities for\nnetwork analysis and the topic models for text analysis, and our general theory\ncan be exploited for statistical inference in these large-scale applications.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 00:21:13 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 16:46:48 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Fan", "Jianqing", ""], ["Fan", "Yingying", ""], ["Han", "Xiao", ""], ["Lv", "Jinchi", ""]]}, {"id": "1902.06916", "submitter": "Matthew Brennan", "authors": "Matthew Brennan, Guy Bresler, Wasim Huleihel", "title": "Universality of Computational Lower Bounds for Submatrix Detection", "comments": "46 pages, accepted for presentation at Conference on Learning Theory\n  (COLT) 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.LG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the general submatrix detection problem, the task is to detect the\npresence of a small $k \\times k$ submatrix with entries sampled from a\ndistribution $\\mathcal{P}$ in an $n \\times n$ matrix of samples from\n$\\mathcal{Q}$. This formulation includes a number of well-studied problems,\nsuch as biclustering when $\\mathcal{P}$ and $\\mathcal{Q}$ are Gaussians and the\nplanted dense subgraph formulation of community detection when the submatrix is\na principal minor and $\\mathcal{P}$ and $\\mathcal{Q}$ are Bernoulli random\nvariables. These problems all seem to exhibit a universal phenomenon: there is\na statistical-computational gap depending on $\\mathcal{P}$ and $\\mathcal{Q}$\nbetween the minimum $k$ at which this task can be solved and the minimum $k$ at\nwhich it can be solved in polynomial time. Our main result is to tightly\ncharacterize this computational barrier as a tradeoff between $k$ and the KL\ndivergences between $\\mathcal{P}$ and $\\mathcal{Q}$ through average-case\nreductions from the planted clique conjecture. These computational lower bounds\nhold given mild assumptions on $\\mathcal{P}$ and $\\mathcal{Q}$ arising\nnaturally from classical binary hypothesis testing. Our results recover and\ngeneralize the planted clique lower bounds for Gaussian biclustering in Ma-Wu\n(2015) and Brennan et al. (2018) and for the sparse and general regimes of\nplanted dense subgraph in Hajek et al. (2015) and Brennan et al. (2018). This\nyields the first universality principle for computational lower bounds obtained\nthrough average-case reductions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 06:37:02 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 05:01:18 GMT"}, {"version": "v3", "created": "Sat, 1 Jun 2019 17:05:34 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Brennan", "Matthew", ""], ["Bresler", "Guy", ""], ["Huleihel", "Wasim", ""]]}, {"id": "1902.06931", "submitter": "Erwan Scornet", "authors": "Julie Josse (CMAP, XPOP), Nicolas Prost (CMAP, XPOP, PARIETAL), Erwan\n  Scornet (X, CMAP), Ga\\\"el Varoquaux (PARIETAL)", "title": "On the consistency of supervised learning with missing values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many application settings, the data have missing entries which make\nanalysis challenging. An abundant literature addresses missing values in an\ninferential framework: estimating parameters and their variance from incomplete\ntables. Here, we consider supervised-learning settings: predicting a target\nwhen missing values appear in both training and testing data. We show the\nconsistency of two approaches in prediction. A striking result is that the\nwidely-used method of imputing with a constant, such as the mean prior to\nlearning is consistent when missing values are not informative. This contrasts\nwith inferential settings where mean imputation is pointed at for distorting\nthe distribution of the data. That such a simple approach can be consistent is\nimportant in practice. We also show that a predictor suited for complete\nobservations can predict optimally on incomplete data,through multiple\nimputation.Finally, to compare imputation with learning directly with a model\nthat accounts for missing values, we analyze further decision trees. These can\nnaturally tackle empirical risk minimization with missing values, due to their\nability to handle the half-discrete nature of incomplete variables. After\ncomparing theoretically and empirically different missing values strategies in\ntrees, we recommend using the \"missing incorporated in attribute\" method as it\ncan handle both non-informative and informative missing values.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 07:27:19 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 15:26:55 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 15:12:20 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Josse", "Julie", "", "CMAP, XPOP"], ["Prost", "Nicolas", "", "CMAP, XPOP, PARIETAL"], ["Scornet", "Erwan", "", "X, CMAP"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"]]}, {"id": "1902.06972", "submitter": "Thomas Lugrin", "authors": "Thomas Lugrin, Anthony C. Davison, Jonathan A. Tawn", "title": "Penultimate Analysis of the Conditional Multivariate Extremes Tail Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for extreme values are generally derived from limit results, which are\nmeant to be good enough approximations when applied to finite samples.\nDepending on the speed of convergence of the process underlying the data, these\napproximations may fail to represent subasymptotic features present in the\ndata, and thus may introduce bias. The case of univariate maxima has been\nwidely explored in the literature, a prominent example being the slow\nconvergence to their Gumbel limit of Gaussian maxima, which are better\napproximated by a negative Weibull distribution at finite levels. In the\ncontext of subasymptotic multivariate extremes, research has only dealt with\nspecific cases related to componentwise maxima and multivariate regular\nvariation. This paper explores the conditional extremes model (Heffernan and\nTawn, 2004) in order to shed light on its finite-sample behaviour and to reduce\nthe bias of extrapolations beyond the range of the available data. We identify\nsecond-order features for different types of conditional copulas, and obtain\nresults that echo those from the univariate context. These results suggest\npossible extensions of the conditional tail model, which will enable it to be\nfitted at less extreme thresholds.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 09:48:14 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Lugrin", "Thomas", ""], ["Davison", "Anthony C.", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "1902.07030", "submitter": "Anouar Meynaoui", "authors": "Anouar Meynaoui (INSA Toulouse), Amandine Marrel, B\\'eatrice Laurent\n  (IMT, INSA Toulouse)", "title": "New statistical methodology for second level global sensitivity analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis (GSA) of numerical simulators aims at studying\nthe global impact of the input uncertainties on the output. To perform the GSA,\nstatistical tools based on inputs/output dependence measures are commonly used.\nWe focus here on dependence measures based on reproducing kernel Hilbert\nspaces: the Hilbert-Schmidt Independence Criterion denoted HSIC. Sometimes, the\nprobability distributions modeling the uncertainty of inputs may be themselves\nuncertain and it is important to quantify the global impact of this uncertainty\non GSA results. We call it here the second-level global sensitivity analysis\n(GSA2). However, GSA2, when performed with a double Monte Carlo loop, requires\na large number of model evaluations which is intractable with CPU time\nexpensive simulators. To cope with this limitation, we propose a new\nstatistical methodology based on a single Monte Carlo loop with a limited\ncalculation budget. Firstly, we build a unique sample of inputs from a well\nchosen probability distribution and the associated code outputs are computed.\nFrom this inputs/output sample, we perform GSA for various assumed probability\ndistributions of inputs by using weighted HSIC measures estimators. Statistical\nproperties of these weighted esti-mators are demonstrated. Finally, we define 2\nnd-level HSIC-based measures between the probability distributions of inputs\nand GSA results, which constitute GSA2 indices. The efficiency of our GSA2\nmethodology is illustrated on an analytical example, thereby comparing several\ntechnical options. Finally, an application to a test case simulating a severe\naccidental scenario on nuclear reactor is provided.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 12:51:03 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Meynaoui", "Anouar", "", "INSA Toulouse"], ["Marrel", "Amandine", "", "IMT, INSA Toulouse"], ["Laurent", "B\u00e9atrice", "", "IMT, INSA Toulouse"]]}, {"id": "1902.07062", "submitter": "Chenguang Liu", "authors": "Chenguang Liu", "title": "Statistical inference for a partially observed interacting system of\n  Hawkes processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We observe the actions of a $K$ sub-sample of $N$ individuals up to time $t$\nfor some large $K<N$. We model the relationships of individuals by i.i.d.\nBernoulli($p$)-random variables, where $p\\in (0,1]$ is an unknown parameter.\nThe rate of action of each individual depends on some unknown parameter $\\mu>\n0$ and on the sum of some function $\\phi$ of the ages of the actions of the\nindividuals which influence him. The function $\\phi$ is unknown but we assume\nit rapidly decays. The aim of this paper is to estimate the parameter $p$\nasymptotically as $N\\to \\infty$, $K\\to \\infty$, and $t\\to \\infty$. Let $m_t$ be\nthe average number of actions per individual up to time $t$. In the subcritical\ncase, where $m_t$ increases linearly, we build an estimator of $p$ with the\nrate of convergence\n$\\frac{1}{\\sqrt{K}}+\\frac{N}{m_t\\sqrt{K}}+\\frac{N}{K\\sqrt{m_t}}$. In the\nsupercritical case, where $m_{t}$ increases exponentially fast, we build an\nestimator of $p$ with the rate of convergence\n$\\frac{1}{\\sqrt{K}}+\\frac{N}{m_{t}\\sqrt{K}}$.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 14:09:50 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2020 14:59:45 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Liu", "Chenguang", ""]]}, {"id": "1902.07190", "submitter": "Firas Khasawneh", "authors": "Jose A. Perea and Elizabeth Munch and Firas A. Khasawneh", "title": "Approximating Continuous Functions on Persistence Diagrams Using\n  Template Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG math.AT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The persistence diagram is an increasingly useful tool from Topological Data\nAnalysis, but its use alongside typical machine learning techniques requires\nmathematical finesse. The most success to date has come from methods that map\npersistence diagrams into $\\mathbb{R}^n$, in a way which maximizes the\nstructure preserved. This process is commonly referred to as featurization. In\nthis paper, we describe a mathematical framework for featurization using\ntemplate functions. These functions are general as they are only required to be\ncontinuous and compactly supported. We discuss two realizations: tent\nfunctions, which emphasize the local contributions of points in a persistence\ndiagram, and interpolating polynomials, which capture global pairwise\ninteractions. We combine the resulting features with classification and\nregression algorithms on several examples including shape data and the Rossler\nsystem. Our results show that using template functions yields high accuracy\nrates that match and often exceed those of existing featurization methods. One\ncounter-intuitive observation is that in most cases using interpolating\npolynomials, where each point contributes globally to the feature vector,\nyields significantly better results than using tent functions, where the\ncontribution of each point is localized. Along the way, we provide a complete\ncharacterization of compactness in the space of persistence diagrams.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 18:43:14 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 19:15:17 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Perea", "Jose A.", ""], ["Munch", "Elizabeth", ""], ["Khasawneh", "Firas A.", ""]]}, {"id": "1902.07284", "submitter": "Matthew Reimherr", "authors": "Matthew Reimherr, Bharath Sriperumbudur, and Hyun Bin Kang", "title": "Optimal Function-on-Scalar Regression over Complex Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of estimating function-on-scalar\nregression models when the functions are observed over multi-dimensional or\nmanifold domains and with potentially multivariate output. We establish the\nminimax rates of convergence and present an estimator based on reproducing\nkernel Hilbert spaces that achieves the minimax rate. To better interpret the\nderived rates, we extend well-known links between RKHS and Sobolev spaces to\nthe case where the domain is a compact Riemannian manifold. This is\naccomplished using an interesting connection to Weyl's Law from partial\ndifferential equations. We conclude with a numerical study and an application\nto 3D facial imaging.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 21:04:26 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Reimherr", "Matthew", ""], ["Sriperumbudur", "Bharath", ""], ["Kang", "Hyun Bin", ""]]}, {"id": "1902.07324", "submitter": "Dmitriy Kunisky", "authors": "Afonso S. Bandeira, Dmitriy Kunisky, Alexander S. Wein", "title": "Computational Hardness of Certifying Bounds on Constrained PCA Problems", "comments": "Submitted version (minor text revisions)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random $n \\times n$ symmetric matrix $\\boldsymbol W$ drawn from the\nGaussian orthogonal ensemble (GOE), we consider the problem of certifying an\nupper bound on the maximum value of the quadratic form $\\boldsymbol x^\\top\n\\boldsymbol W \\boldsymbol x$ over all vectors $\\boldsymbol x$ in a constraint\nset $\\mathcal{S} \\subset \\mathbb{R}^n$. For a certain class of normalized\nconstraint sets $\\mathcal{S}$ we show that, conditional on certain\ncomplexity-theoretic assumptions, there is no polynomial-time algorithm\ncertifying a better upper bound than the largest eigenvalue of $\\boldsymbol W$.\nA notable special case included in our results is the hypercube $\\mathcal{S} =\n\\{ \\pm 1 / \\sqrt{n}\\}^n$, which corresponds to the problem of certifying bounds\non the Hamiltonian of the Sherrington-Kirkpatrick spin glass model from\nstatistical physics.\n  Our proof proceeds in two steps. First, we give a reduction from the\ndetection problem in the negatively-spiked Wishart model to the above\ncertification problem. We then give evidence that this Wishart detection\nproblem is computationally hard below the classical spectral threshold, by\nshowing that no low-degree polynomial can (in expectation) distinguish the\nspiked and unspiked models. This method for identifying computational\nthresholds was proposed in a sequence of recent works on the sum-of-squares\nhierarchy, and is believed to be correct for a large class of problems. Our\nproof can be seen as constructing a distribution over symmetric matrices that\nappears computationally indistinguishable from the GOE, yet is supported on\nmatrices whose maximum quadratic form over $\\boldsymbol x \\in \\mathcal{S}$ is\nmuch larger than that of a GOE matrix.\n", "versions": [{"version": "v1", "created": "Tue, 19 Feb 2019 22:18:46 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 04:46:25 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Kunisky", "Dmitriy", ""], ["Wein", "Alexander S.", ""]]}, {"id": "1902.07380", "submitter": "Matthew Brennan", "authors": "Matthew Brennan, Guy Bresler", "title": "Optimal Average-Case Reductions to Sparse PCA: From Weak Assumptions to\n  Strong Hardness", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past decade, sparse principal component analysis has emerged as an\narchetypal problem for illustrating statistical-computational tradeoffs. This\ntrend has largely been driven by a line of research aiming to characterize the\naverage-case complexity of sparse PCA through reductions from the planted\nclique (PC) conjecture - which conjectures that there is no polynomial-time\nalgorithm to detect a planted clique of size $K = o(N^{1/2})$ in\n$\\mathcal{G}(N, \\frac{1}{2})$. All previous reductions to sparse PCA either\nfail to show tight computational lower bounds matching existing algorithms or\nshow lower bounds for formulations of sparse PCA other than its canonical\ngenerative model, the spiked covariance model. Also, these lower bounds all\nquickly degrade with the exponent in the PC conjecture. Specifically, when only\ngiven the PC conjecture up to $K = o(N^\\alpha)$ where $\\alpha < 1/2$, there is\nno sparsity level $k$ at which these lower bounds remain tight. If $\\alpha \\le\n1/3$ these reductions fail to even show the existence of a\nstatistical-computational tradeoff at any sparsity $k$. We give a reduction\nfrom PC that yields the first full characterization of the computational\nbarrier in the spiked covariance model, providing tight lower bounds at all\nsparsities $k$. We also show the surprising result that weaker forms of the PC\nconjecture up to clique size $K = o(N^\\alpha)$ for any given $\\alpha \\in (0,\n1/2]$ imply tight computational lower bounds for sparse PCA at sparsities $k =\no(n^{\\alpha/3})$. This shows that even a mild improvement in the signal\nstrength needed by the best known polynomial-time sparse PCA algorithms would\nimply that the hardness threshold for PC is subpolynomial. This is the first\ninstance of a suboptimal hardness assumption implying optimal lower bounds for\nanother problem in unsupervised learning.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 02:35:39 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Brennan", "Matthew", ""], ["Bresler", "Guy", ""]]}, {"id": "1902.07425", "submitter": "Robert Lunde", "authors": "Robert Lunde", "title": "Sample Splitting and Weak Assumption Inference For Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inference after model selection under weak\nassumptions in the time series setting. Even when the data are not independent,\nwe show that sample splitting remains asymptotically valid as long as the\nprocess satisfies appropriate weak dependence conditions and the functional of\ninterest is suitably well-behaved. In addition, if the inference targets are\nappropriately defined, we demonstrate that valid statistical inference is\npossible without assuming stationarity. As a working example, we consider\npost-selection inference for regression coefficients under a random design\nassumption, in which the pair $(Y_i, X_i) \\in \\mathbb{R}^{p_n}$ is assumed to\nbe an observation from a weakly dependent triangular array. We establish\n(asymptotic) sample splitting validity for regression coefficients under both\n$\\beta$-mixing and $\\tau$-dependence assumptions.\n  To facilitate statistical inference in the non-stationary, weakly dependent\nregime, we extend a central limit theorem of Doukhan and Wintenberger (2007).\nTo extend their result, we derive some properties of the variance of a\nnormalized sum of a weakly dependent process. In particular, we show that,\nunder very general conditions, the variance is often well-approximated by\nindependent blocks. Using this result, we derive the validity of the block\nmultiplier bootstrap under $\\theta$-dependence and demonstrate the validity of\nan inference procedure that combines sample splitting with the bootstrap under\nweak assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 06:28:41 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2019 22:04:09 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Lunde", "Robert", ""]]}, {"id": "1902.07698", "submitter": "Cong Ma", "authors": "Yuxin Chen, Yuejie Chi, Jianqing Fan, Cong Ma, Yuling Yan", "title": "Noisy Matrix Completion: Understanding Statistical Guarantees for Convex\n  Relaxation via Nonconvex Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies noisy low-rank matrix completion: given partial and noisy\nentries of a large low-rank matrix, the goal is to estimate the underlying\nmatrix faithfully and efficiently. Arguably one of the most popular paradigms\nto tackle this problem is convex relaxation, which achieves remarkable efficacy\nin practice. However, the theoretical support of this approach is still far\nfrom optimal in the noisy setting, falling short of explaining its empirical\nsuccess.\n  We make progress towards demystifying the practical efficacy of convex\nrelaxation vis-\\`a-vis random noise. When the rank and the condition number of\nthe unknown matrix are bounded by a constant, we demonstrate that the convex\nprogramming approach achieves near-optimal estimation errors --- in terms of\nthe Euclidean loss, the entrywise loss, and the spectral norm loss --- for a\nwide range of noise levels. All of this is enabled by bridging convex\nrelaxation with the nonconvex Burer-Monteiro approach, a seemingly distinct\nalgorithmic paradigm that is provably robust against noise. More specifically,\nwe show that an approximate critical point of the nonconvex formulation serves\nas an extremely tight approximation of the convex solution, thus allowing us to\ntransfer the desired statistical guarantees of the nonconvex approach to its\nconvex counterpart.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 18:51:50 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 14:01:40 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Chen", "Yuxin", ""], ["Chi", "Yuejie", ""], ["Fan", "Jianqing", ""], ["Ma", "Cong", ""], ["Yan", "Yuling", ""]]}, {"id": "1902.07780", "submitter": "Dionissios Hristopulos Prof.", "authors": "Dionissios T. Hristopulos, Vasiliki D. Agou", "title": "Stochastic Local Interaction Model with Sparse Precision Matrix for\n  Space-Time Interpolation", "comments": "29 pages, 10 figures", "journal-ref": "Spatial Statistics, Available online 31 December 2019, 100403", "doi": "10.1016/j.spasta.2019.100403", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of geostatistical and machine learning methods based on\nGaussian processes to big space-time data is beset by the requirement for\nstoring and numerically inverting large and dense covariance matrices.\nComputationally efficient representations of space-time correlations can be\nconstructed using local models of conditional dependence which can reduce the\ncomputational load. We formulate a stochastic local interaction model for\nregular and scattered space-time data that incorporates interactions within\ncontrolled space-time neighborhoods. The strength of the interaction and the\nsize of the neighborhood are defined by means of kernel functions and adaptive\nlocal bandwidths. Compactly supported kernels lead to finite-size local\nneighborhoods and consequently to sparse precision matrices that admit explicit\nexpression. Hence, the stochastic local interaction model's requirements for\nstorage are modest and the costly covariance matrix inversion is not needed. We\nalso derive a semi-explicit prediction equation and express the conditional\nvariance of the prediction in terms of the diagonal of the precision matrix.\nFor data on regular space-time lattices, the stochastic local interaction model\nis equivalent to a Gaussian Markov Random Field.\n", "versions": [{"version": "v1", "created": "Wed, 20 Feb 2019 21:12:16 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 17:30:10 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Hristopulos", "Dionissios T.", ""], ["Agou", "Vasiliki D.", ""]]}, {"id": "1902.07954", "submitter": "Shinpei Imori", "authors": "Shinpei Imori and Hidetoshi Shimodaira", "title": "An information criterion for auxiliary variable selection in incomplete\n  data analysis", "comments": null, "journal-ref": null, "doi": "10.3390/e21030281", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference is considered for variables of interest, called primary\nvariables, when auxiliary variables are observed along with the primary\nvariables. We consider the setting of incomplete data analysis, where some\nprimary variables are not observed. Utilizing a parametric model of joint\ndistribution of primary and auxiliary variables, it is possible to improve the\nestimation of parametric model for the primary variables when the auxiliary\nvariables are closely related to the primary variables. However, the estimation\naccuracy reduces when the auxiliary variables are irrelevant to the primary\nvariables. For selecting useful auxiliary variables, we formulate the problem\nas model selection, and propose an information criterion for predicting primary\nvariables by leveraging auxiliary variables. The proposed information criterion\nis an asymptotically unbiased estimator of the Kullback-Leibler divergence for\ncomplete data of primary variables under some reasonable conditions. We also\nclarify an asymptotic equivalence between the proposed information criterion\nand a variant of leave-one-out cross validation. Performance of our method is\ndemonstrated via a simulation study and a real data example.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 10:47:53 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 12:04:34 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Imori", "Shinpei", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "1902.08364", "submitter": "Muneya Matsui", "authors": "Muneya Matsui and Rasmus S{\\o}ndergaard Pedersen", "title": "Characterization of the tail behavior of a class of BEKK processes: A\n  stochastic recurrence equation approach", "comments": "Keywords: Regular variation, GARCH, BEKK, stochastic recurrence\n  equation. Statistics classified with econometrics, JEL: C32 and C58, 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new, mild conditions for strict stationarity and ergodicity of a\nclass of BEKK processes. By exploiting that the processes can be represented as\nmultivariate stochastic recurrence equations, we characterize the tail behavior\nof the associated stationary laws. Specifically, we show that the each\ncomponent of the BEKK processes is regularly varying with some tail index. In\ngeneral, the tail index differs along the components, which contrasts most of\nthe existing literature on the tail behavior of multivariate GARCH processes.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 05:09:02 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Matsui", "Muneya", ""], ["Pedersen", "Rasmus S\u00f8ndergaard", ""]]}, {"id": "1902.08500", "submitter": "Yury Kutoyants", "authors": "Yury A. Kutoyants", "title": "On Parameter Estimation of Hidden Ergodic Ornstein-Uhlenbeck Process", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of parameter estimation for the partially observed\nlinear stochastic differential equation. We assume that the unobserved\nOrnstein-Uhlenbeck process depends on some unknown parameter and estimate the\nunobserved process and the unknown parameter simultaneously. We construct the\ntwo-step MLE-process for the estimator of the parameter and describe its large\nsample asymptotic properties, including consistency and asymptotic normality.\nUsing the Kalman-Bucy filtering equations we construct recurrent estimators of\nthe state and the parameter.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 14:14:04 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Kutoyants", "Yury A.", ""]]}, {"id": "1902.08590", "submitter": "Lisandro Ferm\\'in", "authors": "H\\'ector Araya, Natalia Bahamonde, Lisandro Ferm\\'in, Tania Roa,\n  Soledad Torres", "title": "Parameter estimation for random sampled Regression Model with Long\n  Memory Noise", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present the least squares estimator for the drift\nparameter in a linear regression model driven by the increment of a fractional\nBrownian motion sampled at random times. For two different random times,\nJittered and renewal process sampling, consistency of the estimator is proven.\nA simulation study is provided to illustrate the performance of the estimator\nunder different values of the Hurst parameter H.\n", "versions": [{"version": "v1", "created": "Fri, 22 Feb 2019 18:14:38 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Araya", "H\u00e9ctor", ""], ["Bahamonde", "Natalia", ""], ["Ferm\u00edn", "Lisandro", ""], ["Roa", "Tania", ""], ["Torres", "Soledad", ""]]}, {"id": "1902.08885", "submitter": "Pierre C. Bellec", "authors": "Pierre C. Bellec and Cun-Hui Zhang", "title": "De-Biasing The Lasso With Degrees-of-Freedom Adjustment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies schemes to de-bias the Lasso in a linear model\n$y=X\\beta+\\epsilon$ where the goal is to construct confidence intervals for\n$a_0^T\\beta$ in a direction $a_0$, where $X$ has iid $N(0,\\Sigma)$ rows. We\nshow that previously analyzed propositions to de-bias the Lasso require a\nmodification in order to enjoy efficiency in a full range of sparsity. This\nmodification takes the form of a degrees-of-freedom adjustment that accounts\nfor the dimension of the model selected by Lasso.\n  Let $s_0$ be the true sparsity. If $\\Sigma$ is known and the ideal score\nvector proportional to $X\\Sigma^{-1}a_0$ is used, the unadjusted de-biasing\nschemes proposed previously enjoy efficiency if $s_0\\lll n^{2/3}$. However, if\n$s_0\\ggg n^{2/3}$, the unadjusted schemes cannot be efficient in certain $a_0$:\nthen it is necessary to modify existing procedures by a degrees-of-freedom\nadjustment. This modification grants asymptotic efficiency for any $a_0$ when\n$s_0/p\\to 0$ and $s_0\\log(p/s_0)/n \\to 0$.\n  If $\\Sigma$ is unknown, efficiency is granted for general $a_0$ when\n$$\\frac{s_0\\log p}{n}+\\min\\Big\\{\\frac{s_\\Omega\\log\np}{n},\\frac{\\|\\Sigma^{-1}a_0\\|_1\\sqrt{\\log p}}{\\|\\Sigma^{-1/2}a_0\\|_2 \\sqrt\nn}\\Big\\}+\\frac{\\min(s_\\Omega,s_0)\\log p}{\\sqrt n}\\to0$$ where\n$s_\\Omega=\\|\\Sigma^{-1}a_0\\|_0$, provided that the de-biased estimate is\nmodified with the degrees-of-freedom adjustment. The dependence in\n$s_0,s_\\Omega$ and $\\|\\Sigma^{-1}a_0\\|_1$ is optimal. Our estimated score\nvector provides a novel methodology to handle dense $a_0$.\n  Our analysis shows that the degrees-of-freedom adjustment is not needed when\nthe initial bias in direction $a_0$ is small, which is granted under stringent\nconditions on $\\Sigma^{-1}$. The main proof argument is an interpolation path\nsimilar to that typically used to derive Slepian's lemma. It yields a new\n$\\ell_\\infty$ error bound for the Lasso which is of independent interest.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 03:28:27 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 19:03:21 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 12:44:53 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Bellec", "Pierre C.", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1902.08936", "submitter": "Francisco Novoa Mu\\~noz", "authors": "Francisco Novoa-Mu\\~noz", "title": "Goodness-of-fit Tests for the Bivariate Poisson Distribution", "comments": "30 pages, 13 tables", "journal-ref": null, "doi": null, "report-no": "2588495", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bivariate Poisson distribution is commonly used to model bivariate count\ndata. In this paper we study a goodness-of-fit test for this distribution. We\nalso provide a review of the existing tests for the bivariate Poisson\ndistribution, and its multivariate extension. The proposed test is consistent\nagainst any fixed alternative. It is also able to detect local alternatives\nconverging to the null at the rate $n^{-\\frac{1}{2}}$. The bootstrap can be\nemployed to consistently estimate the null distribution of the test statistic.\nThrough a simulation study we investigated the goodness of the bootstrap\napproximation and the power for finite sample sizes.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 12:53:52 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Novoa-Mu\u00f1oz", "Francisco", ""]]}, {"id": "1902.09024", "submitter": "Timo Klock", "authors": "Zeljko Kereta, Timo Klock, Valeriya Naumova", "title": "Nonlinear generalization of the monotone single index model", "comments": "37 pages, 23 figures, 4 table", "journal-ref": "Information and Inference: A Journal of the IMA (2020)", "doi": "10.1093/imaiai/iaaa013", "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single index model is a powerful yet simple model, widely used in statistics,\nmachine learning, and other scientific fields. It models the regression\nfunction as $g(<a,x>)$, where a is an unknown index vector and x are the\nfeatures. This paper deals with a nonlinear generalization of this framework to\nallow for a regressor that uses multiple index vectors, adapting to local\nchanges in the responses. To do so we exploit the conditional distribution over\nfunction-driven partitions, and use linear regression to locally estimate index\nvectors. We then regress by applying a kNN type estimator that uses a localized\nproxy of the geodesic metric. We present theoretical guarantees for estimation\nof local index vectors and out-of-sample prediction, and demonstrate the\nperformance of our method with experiments on synthetic and real-world data\nsets, comparing it with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 24 Feb 2019 22:13:33 GMT"}, {"version": "v2", "created": "Thu, 5 Sep 2019 18:23:04 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Kereta", "Zeljko", ""], ["Klock", "Timo", ""], ["Naumova", "Valeriya", ""]]}, {"id": "1902.09169", "submitter": "Leo Pasquazzi", "authors": "Leo Pasquazzi", "title": "Weak convergence theory for Poisson sampling designs", "comments": "This article is an updated version of arXiv:1902.09169v1 [math.ST]\n  and of arXiv:1902.09169v2 [math.ST]", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides some general theorems about unconditional and conditional\nweak convergence of empirical processes in the case of Poisson sampling\ndesigns. The theorems presented in this work are stronger than previously\npublished results. Their proofs are based on the symmetrization technique and\non a contraction principle.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 09:55:36 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 13:31:18 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 14:21:44 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Pasquazzi", "Leo", ""]]}, {"id": "1902.09230", "submitter": "Marco Oesting", "authors": "Marco Oesting, Martin Schlather, Claudia Schillings", "title": "Sampling Sup-Normalized Spectral Functions for Brown-Resnick Processes", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sup-normalized spectral functions form building blocks of max-stable and\nPareto processes and therefore play an important role in modeling spatial\nextremes. For one of the most popular examples, the Brown-Resnick process,\nsimulation is not straightforward. In this paper, we generalize two approaches\nfor simulation via Markov Chain Monte Carlo methods and rejection sampling by\nintroducing new classes of proposal densities. In both cases, we provide an\noptimal choice of the proposal density with respect to sampling efficiency. The\nperformance of the procedures is demonstrated in an example.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 12:34:24 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Oesting", "Marco", ""], ["Schlather", "Martin", ""], ["Schillings", "Claudia", ""]]}, {"id": "1902.09276", "submitter": "Shovan Chowdhury", "authors": "Asok K. Nanda, Sanjib Gayen, and Shovan Chowdhury", "title": "Errors Due to Departure from Independence in Exponential Series System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reliability and life testing when the exponentially distributed components\nare put in series, it is generally assumed that the lifetimes of the components\nare independently distributed, which leads to some errors if they are not\nactually independent. In this paper, we study the relative errors incurred in\ndifferent reliability measures due to such assumptions when actually they\nfollow some bivariate exponential distributions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2019 16:32:36 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Nanda", "Asok K.", ""], ["Gayen", "Sanjib", ""], ["Chowdhury", "Shovan", ""]]}, {"id": "1902.09293", "submitter": "Hugo Kussaba Ph.D.", "authors": "Hugo T. M. Kussaba and Jo\\~ao Y. Ishihara and Leonardo R. A. X.\n  Menezes", "title": "A Robust Unscented Transformation for Uncertain Moments", "comments": "26 pages, 3 figures, accepted by Journal of the Franklin Institute", "journal-ref": null, "doi": "10.1016/j.jfranklin.2019.02.018", "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a robust version of the unscented transform (UT) for\none-dimensional random variables. It is assumed that the moments are not\nexactly known, but are known to lie in intervals. In this scenario, the moment\nmatching equations are reformulated as a system of polynomial equations and\ninequalities, and it is proposed to use the Chebychev center of the solution\nset as a robust UT. This method yields a parametrized polynomial optimization\nproblem, which in spite of being NP-Hard, can be relaxed by some algorithms\nthat are proposed in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 14:34:25 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Kussaba", "Hugo T. M.", ""], ["Ishihara", "Jo\u00e3o Y.", ""], ["Menezes", "Leonardo R. A. X.", ""]]}, {"id": "1902.09474", "submitter": "William Leeb", "authors": "William Leeb", "title": "Matrix denoising for weighted loss functions and heterogeneous signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a low-rank matrix from a noisy observed\nmatrix. Previous work has shown that the optimal method depends crucially on\nthe choice of loss function. In this paper, we use a family of weighted loss\nfunctions, which arise naturally for problems such as submatrix denoising,\ndenoising with heteroscedastic noise, and denoising with missing data. However,\nweighted loss functions are challenging to analyze because they are not\northogonally-invariant. We derive optimal spectral denoisers for these weighted\nloss functions. By combining different weights, we then use these optimal\ndenoisers to construct a new denoiser that exploits heterogeneity in the signal\nmatrix to boost estimation with unweighted loss.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 17:48:47 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 04:52:26 GMT"}, {"version": "v3", "created": "Sat, 26 Sep 2020 19:33:00 GMT"}, {"version": "v4", "created": "Wed, 7 Apr 2021 02:52:22 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Leeb", "William", ""]]}, {"id": "1902.09614", "submitter": "Guilherme Pumi", "authors": "Guilherme Pumi, Taiane Schaedler Prass and Rafael Rig\\~ao Souza", "title": "A Dynamic Model for Double Bounded Time Series With Chaotic Driven\n  Conditional Averages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DS stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a class of dynamic models for time series taking\nvalues on the unit interval. The proposed model follows a generalized linear\nmodel approach where the random component, conditioned on the past information,\nfollows a beta distribution, while the conditional mean specification may\ninclude covariates and also an extra additive term given by the iteration of a\nmap that can present chaotic behavior. The resulting model is very flexible and\nits systematic component can accommodate short and long range dependence,\nperiodic behavior, laminar phases, etc. We derive easily verifiable conditions\nfor the stationarity of the proposed model, as well as conditions for the law\nof large numbers and a Birkhoff-type theorem to hold. A Monte Carlo simulation\nstudy is performed to assess the finite sample behavior of the partial maximum\nlikelihood approach for parameter estimation in the proposed model. Finally, an\napplication to the proportion of stored hydroelectrical energy in Southern\nBrazil is presented.\n", "versions": [{"version": "v1", "created": "Mon, 25 Feb 2019 20:58:51 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 20:50:58 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Pumi", "Guilherme", ""], ["Prass", "Taiane Schaedler", ""], ["Souza", "Rafael Rig\u00e3o", ""]]}, {"id": "1902.09803", "submitter": "Joseph De Vilmarest", "authors": "Joseph De Vilmarest (LPSM UMR 8001), Olivier Wintenberger (LPSM UMR\n  8001)", "title": "Logarithmic Regret for parameter-free Online Logistic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider online optimization procedures in the context of logistic\nregression, focusing on the Extended Kalman Filter (EKF). We introduce a\nsecond-order algorithm close to the EKF, named Semi-Online Step (SOS), for\nwhich we prove a O(log(n)) regret in the adversarial setting, paving the way to\nsimilar results for the EKF. This regret bound on SOS is the first for such\nparameter-free algorithm in the adversarial logistic regression. We prove for\nthe EKF in constant dynamics a O(log(n)) regret in expectation and in the\nwell-specified logistic regression model.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 08:51:45 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["De Vilmarest", "Joseph", "", "LPSM UMR 8001"], ["Wintenberger", "Olivier", "", "LPSM UMR\n  8001"]]}, {"id": "1902.09905", "submitter": "Bernd Sturmfels", "authors": "Bernd Sturmfels, Caroline Uhler and Piotr Zwiernik", "title": "Brownian motion tree models are toric", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Felsenstein's classical model for Gaussian distributions on a phylogenetic\ntree is shown to be a toric variety in the space of concentration matrices. We\npresent an exact semialgebraic characterization of this model, and we\ndemonstrate how the toric structure leads to exact methods for maximum\nlikelihood estimation. Our results also give new insights into the geometry of\nultrametric matrices.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 13:01:53 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Sturmfels", "Bernd", ""], ["Uhler", "Caroline", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "1902.09917", "submitter": "Remi Jezequel", "authors": "R\\'emi J\\'ez\\'equel (SIERRA), Pierre Gaillard (SIERRA), Alessandro\n  Rudi (SIERRA)", "title": "Efficient online learning with kernels for adversarial large scale\n  problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in a framework of online learning with kernels for\nlow-dimensional but large-scale and potentially adversarial datasets. We study\nthe computational and theoretical performance of online variations of kernel\nRidge regression. Despite its simplicity, the algorithm we study is the first\nto achieve the optimal regret for a wide range of kernels with a per-round\ncomplexity of order $n^\\alpha$ with $\\alpha < 2$. The algorithm we consider is\nbased on approximating the kernel with the linear span of basis functions. Our\ncontributions is two-fold: 1) For the Gaussian kernel, we propose to build the\nbasis beforehand (independently of the data) through Taylor expansion. For\n$d$-dimensional inputs, we provide a (close to) optimal regret of order\n$O((\\log n)^{d+1})$ with per-round time complexity and space complexity\n$O((\\log n)^{2d})$. This makes the algorithm a suitable choice as soon as $n\n\\gg e^d$ which is likely to happen in a scenario with small dimensional and\nlarge-scale dataset; 2) For general kernels with low effective dimension, the\nbasis functions are updated sequentially in a data-adaptive fashion by sampling\nNystr{\\\"o}m points. In this case, our algorithm improves the computational\ntrade-off known for online kernel regression.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 13:17:11 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 12:57:43 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["J\u00e9z\u00e9quel", "R\u00e9mi", "", "SIERRA"], ["Gaillard", "Pierre", "", "SIERRA"], ["Rudi", "Alessandro", "", "SIERRA"]]}, {"id": "1902.09923", "submitter": "Dave Zachariah", "authors": "Dave Zachariah and Petre Stoica", "title": "Effect Inference from Two-Group Data with Sampling Bias", "comments": null, "journal-ref": "IEEE Signal Processing Letters, vol. 26, no. 8, Aug. 2019", "doi": "10.1109/LSP.2019.2921255", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, different populations are compared using data that are\nsampled in a biased manner. Under sampling biases, standard methods that\nestimate the difference between the population means yield unreliable\ninferences. Here we develop an inference method that is resilient to sampling\nbiases and is able to control the false positive errors under moderate bias\nlevels in contrast to the standard approach. We demonstrate the method using\nsynthetic and real biomarker data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 13:37:18 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 09:47:45 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Zachariah", "Dave", ""], ["Stoica", "Petre", ""]]}, {"id": "1902.10100", "submitter": "Demian Pouzo", "authors": "Xiaohong Chen, Demian Pouzo and James L. Powell", "title": "Penalized Sieve GEL for Weighted Average Derivatives of Nonparametric\n  Quantile IV Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers estimation and inference for a weighted average\nderivative (WAD) of a nonparametric quantile instrumental variables regression\n(NPQIV). NPQIV is a non-separable and nonlinear ill-posed inverse problem,\nwhich might be why there is no published work on the asymptotic properties of\nany estimator of its WAD. We first characterize the semiparametric efficiency\nbound for a WAD of a NPQIV, which, unfortunately, depends on an unknown\nconditional derivative operator and hence an unknown degree of ill-posedness,\nmaking it difficult to know if the information bound is singular or not. In\neither case, we propose a penalized sieve generalized empirical likelihood\n(GEL) estimation and inference procedure, which is based on the unconditional\nWAD moment restriction and an increasing number of unconditional moments that\nare implied by the conditional NPQIV restriction, where the unknown quantile\nfunction is approximated by a penalized sieve. Under some regularity\nconditions, we show that the self-normalized penalized sieve GEL estimator of\nthe WAD of a NPQIV is asymptotically standard normal. We also show that the\nquasi likelihood ratio statistic based on the penalized sieve GEL criterion is\nasymptotically chi-square distributed regardless of whether or not the\ninformation bound is singular.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 18:29:53 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Chen", "Xiaohong", ""], ["Pouzo", "Demian", ""], ["Powell", "James L.", ""]]}, {"id": "1902.10142", "submitter": "Feras Saad", "authors": "Feras A. Saad, Cameron E. Freer, Nathanael L. Ackerman, Vikash K.\n  Mansinghka", "title": "A Family of Exact Goodness-of-Fit Tests for High-Dimensional Discrete\n  Distributions", "comments": "20 pages, 6 figures. Appearing in AISTATS 2019", "journal-ref": "Proceedings of the 22nd International Conference on Artificial\n  Intelligence and Statistics, PMLR 89:1640-1649, 2019", "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of goodness-of-fit testing is to assess whether a dataset of\nobservations is likely to have been drawn from a candidate probability\ndistribution. This paper presents a rank-based family of goodness-of-fit tests\nthat is specialized to discrete distributions on high-dimensional domains. The\ntest is readily implemented using a simulation-based, linear-time procedure.\nThe testing procedure can be customized by the practitioner using knowledge of\nthe underlying data domain. Unlike most existing test statistics, the proposed\ntest statistic is distribution-free and its exact (non-asymptotic) sampling\ndistribution is known in closed form. We establish consistency of the test\nagainst all alternatives by showing that the test statistic is distributed as a\ndiscrete uniform if and only if the samples were drawn from the candidate\ndistribution. We illustrate its efficacy for assessing the sample quality of\napproximate sampling algorithms over combinatorially large spaces with\nintractable probabilities, including random partitions in Dirichlet process\nmixture models and random lattices in Ising models.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 15:45:34 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Saad", "Feras A.", ""], ["Freer", "Cameron E.", ""], ["Ackerman", "Nathanael L.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1902.10257", "submitter": "Jonas Latz", "authors": "Jonas Latz", "title": "On the well-posedness of Bayesian inverse problems", "comments": "30 pages, 7 figures", "journal-ref": "SIAM/ASA J. Uncertain. Quantif. 8(1), p. 451-482, 2020", "doi": "10.1137/19M1247176", "report-no": null, "categories": "math.ST cs.NA math.NA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subject of this article is the introduction of a new concept of\nwell-posedness of Bayesian inverse problems. The conventional concept of\n(Lipschitz, Hellinger) well-posedness in [Stuart 2010, Acta Numerica 19, pp.\n451-559] is difficult to verify in practice and may be inappropriate in some\ncontexts. Our concept simply replaces the Lipschitz continuity of the posterior\nmeasure in the Hellinger distance by continuity in an appropriate distance\nbetween probability measures. Aside from the Hellinger distance, we investigate\nwell-posedness with respect to weak convergence, the total variation distance,\nthe Wasserstein distance, and also the Kullback--Leibler divergence. We\ndemonstrate that the weakening to continuity is tolerable and that the\ngeneralisation to other distances is important. The main results of this\narticle are proofs of well-posedness with respect to some of the aforementioned\ndistances for large classes of Bayesian inverse problems. Here, little or no\ninformation about the underlying model is necessary; making these results\nparticularly interesting for practitioners using black-box models. We\nillustrate our findings with numerical examples motivated from machine learning\nand image processing.\n", "versions": [{"version": "v1", "created": "Tue, 26 Feb 2019 22:52:33 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 14:03:15 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 14:57:54 GMT"}, {"version": "v4", "created": "Wed, 11 Sep 2019 17:40:36 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["Latz", "Jonas", ""]]}, {"id": "1902.10288", "submitter": "Hongkang Yang", "authors": "Hongkang Yang, Esteban G. Tabak", "title": "Clustering, factor discovery and optimal transport", "comments": "Improved clarity of presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clustering problem, and more generally, latent factor discovery --or\nlatent space inference-- is formulated in terms of the Wasserstein barycenter\nproblem from optimal transport. The objective proposed is the maximization of\nthe variability attributable to class, further characterized as the\nminimization of the variance of the Wasserstein barycenter. Existing theory,\nwhich constrains the transport maps to rigid translations, is extended to\naffine transformations. The resulting non-parametric clustering algorithms\ninclude k-means as a special case and exhibit more robust performance. A\ncontinuous version of these algorithms discovers continuous latent variables\nand generalizes principal curves. The strength of these algorithms is\ndemonstrated by tests on both artificial and real-world data sets.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 01:04:12 GMT"}, {"version": "v2", "created": "Wed, 30 Oct 2019 20:14:42 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 13:13:40 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Yang", "Hongkang", ""], ["Tabak", "Esteban G.", ""]]}, {"id": "1902.10381", "submitter": "Stefan Richter", "authors": "Rainer Dahlhaus and Stefan Richter", "title": "Adaptation for nonparametric estimators of locally stationary processes", "comments": "38 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two adaptive bandwidth selection methods for nonparametric estimators in\nlocally stationary processes are proposed. We investigate a cross validation\napproach and a method based on contrast minimization and derive asymptotic\nproperties of both methods. The results are applicable for different statistics\nunder a broad setting of locally stationarity including nonlinear processes. At\nthe same time we deepen the general framework for local stationarity based on\nstationary approximations. For example a general Bernstein inequality is\nderived for such processes. A simulation study performed on the covariance\nfunction and more complicated functionals shows that both adaptation methods\nwork well.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 08:17:26 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Dahlhaus", "Rainer", ""], ["Richter", "Stefan", ""]]}, {"id": "1902.10490", "submitter": "Fadhel Ayed", "authors": "Fadhel Ayed, Marco Battiston, Federico Camerlenghi and Stefano Favaro", "title": "A Good-Turing estimator for feature allocation models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature allocation models generalize species sampling models by allowing\nevery observation to belong to more than one species, now called features.\nUnder the popular Bernoulli product model for feature allocation, given $n$\nsamples, we study the problem of estimating the missing mass $M_{n}$, namely\nthe expected number hitherto unseen features that would be observed if one\nadditional individual was sampled. This is motivated by numerous applied\nproblems where the sampling procedure is expensive, in terms of time and/or\nfinancial resources allocated, and further samples can be only motivated by the\npossibility of recording new unobserved features. We introduce a simple, robust\nand theoretically sound nonparametric estimator $\\hat{M}_{n}$ of $M_{n}$.\n$\\hat{M}_{n}$ turns out to have the same analytic form of the popular\nGood-Turing estimator of the missing mass in species sampling models, with the\ndifference that the two estimators have different ranges. We show that\n$\\hat{M}_{n}$ admits a natural interpretation both as a jackknife estimator and\nas a nonparametric empirical Bayes estimator, we give provable guarantees for\nthe performance of $\\hat{M}_{n}$ in terms of minimax rate optimality, and we\nprovide with an interesting connection between $\\hat{M}_{n}$ and the\nGood-Turing estimator for species sampling. Finally, we derive non-asymptotic\nconfidence intervals for $\\hat{M}_{n}$, which are easily computable and do not\nrely on any asymptotic approximation. Our approach is illustrated with\nsynthetic data and SNP data from the ENCODE sequencing genome project.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 12:47:07 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 14:57:30 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Ayed", "Fadhel", ""], ["Battiston", "Marco", ""], ["Camerlenghi", "Federico", ""], ["Favaro", "Stefano", ""]]}, {"id": "1902.10530", "submitter": "Fadhel Ayed", "authors": "Fadhel Ayed, Marco Battiston, Federico Camerlenghi and Stefano Favaro", "title": "Consistent estimation of the missing mass for feature models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature models are popular in machine learning and they have been recently\nused to solve many unsupervised learning problems. In these models every\nobservation is endowed with a finite set of features, usually selected from an\ninfinite collection $(F_{j})_{j\\geq 1}$. Every observation can display feature\n$F_{j}$ with an unknown probability $p_{j}$. A statistical problem inherent to\nthese models is how to estimate, given an initial sample, the conditional\nexpected number of hitherto unseen features that will be displayed in a future\nobservation. This problem is usually referred to as the missing mass problem.\nIn this work we prove that, using a suitable multiplicative loss function and\nwithout imposing any assumptions on the parameters $p_{j}$, there does not\nexist any universally consistent estimator for the missing mass. In the second\npart of the paper, we focus on a special class of heavy-tailed probabilities\n$(p_{j})_{j\\geq 1}$, which are common in many real applications, and we show\nthat, within this restricted class of probabilities, the nonparametric\nestimator of the missing mass suggested by Ayed et al. (2017) is strongly\nconsistent. As a byproduct result, we will derive concentration inequalities\nfor the missing mass and the number of features observed with a specified\nfrequency in a sample of size $n$.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 13:51:35 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Ayed", "Fadhel", ""], ["Battiston", "Marco", ""], ["Camerlenghi", "Federico", ""], ["Favaro", "Stefano", ""]]}, {"id": "1902.10605", "submitter": "Solenne Gaucher", "authors": "Solenne Gaucher, Olga Klopp (MODAL'X)", "title": "Maximum Likelihood Estimation of Sparse Networks with Missing\n  Observations", "comments": "We derive a variational approximation to the maximum likelihood\n  estimator of the connection probabilities. We bound the risk of this\n  tractable estimator", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the matrix of connections probabilities is one of the key\nquestions when studying sparse networks. In this work, we consider networks\ngenerated under the sparse graphon model and the in-homogeneous random graph\nmodel with missing observations. Using the Stochastic Block Model as a\nparametric proxy, we bound the risk of the maximum likelihood estimator of\nnetwork connections probabilities , and show that it is minimax optimal. When\nrisk is measured in Frobenius norm, no estimator running in polynomial time has\nbeen shown to attain the minimax optimal rate of convergence for this problem.\nThus, maximum likelihood estimation is of particular interest as\ncomputationally efficient approximations to it have been proposed in the\nliterature and are often used in practice.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 15:55:47 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 14:05:35 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Gaucher", "Solenne", "", "MODAL'X"], ["Klopp", "Olga", "", "MODAL'X"]]}, {"id": "1902.10708", "submitter": "Sandra Fortini", "authors": "Sandra Fortini, Sonia Petrone", "title": "Quasi-Bayes properties of a recursive procedure for mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods are attractive and often optimal, yet nowadays pressure for\nfast computations, especially with streaming data and online learning, brings\nrenewed interest in faster, although possibly sub-optimal, solutions. To what\nextent these algorithms may approximate a Bayesian solution is a problem of\ninterest, not always solved. On this background, in this paper we revisit a\nsequential procedure proposed by Smith and Makov (1978) for unsupervised\nlearning and classification in finite mixtures, and developed by M. Newton and\nZhang (1999), for nonparametric mixtures. Newton's algorithm is simple and\nfast, and theoretically intriguing. Although originally proposed as an\napproximation of the Bayesian solution, its quasi-Bayes properties remain\nunclear. We propose a novel methodological approach. We regard the algorithm as\na probabilistic learning rule, that implicitly defines an underlying\nprobabilistic model; and we find this model. We can then prove that it is,\nasymptotically, a Bayesian, exchangeable mixture model. Moreover, while the\nalgorithm only offers a point estimate, our approach allows us to obtain an\nasymptotic posterior distribution and asymptotic credible intervals for the\nmixing distribution. Our results also provide practical hints for tuning the\nalgorithm and obtaining desirable properties, as we illustrate in a simulation\nstudy. Beyond mixture models, our study suggests a theoretical framework that\nmay be of interest for recursive quasi-Bayes methods in other settings.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 17:13:31 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Fortini", "Sandra", ""], ["Petrone", "Sonia", ""]]}, {"id": "1902.10709", "submitter": "L.A. Prashanth", "authors": "Prashanth L.A. and Sanjay P. Bhat", "title": "Concentration of risk measures: A Wasserstein distance approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a unified approach based on Wasserstein distance to\nderive concentration bounds for empirical estimates for a broad class of risk\nmeasures. The results cover two broad classes of risk measures which are\ndefined in the paper. The classes of risk measures introduced include include\nas special cases well known risk measures from the finance literature such as\nconditional value at risk (CVaR), spectral risk measures, utility-based\nshortfall risk, cumulative prospect theory (CPT) value, and rank dependent\nexpected utility. Two estimation schemes are considered, one for each class of\nrisk measures. One estimation scheme involves applying the risk measure to the\nempirical distribution function formed from a collection of i.i.d. samples of\nthe random variable (r.v.), while the second scheme involves applying the same\nprocedure to a truncated sample. The bounds provided apply to three popular\nclasses of distributions, namely sub-Gaussian, sub-exponential and heavy-tailed\ndistributions. The bounds are derived by first relating the estimation error to\nthe Wasserstein distance between the true and empirical distributions, and then\nusing recent concentration bounds for the latter. Previous concentration bounds\nare available only for specific risk measures such as CVaR and CPT-value. The\nbounds derived in this paper are shown to either match or improve upon previous\nbounds in cases where they are available. The usefulness of the bounds is\nillustrated through an algorithm and the corresponding regret bound for a\nstochastic bandit problem involving a general risk measure from either of the\ntwo classes introduced in the paper.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 17:41:35 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 05:52:06 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2020 11:37:31 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["A.", "Prashanth L.", ""], ["Bhat", "Sanjay P.", ""]]}, {"id": "1902.10822", "submitter": "Fang Han", "authors": "Yandi Shen, Chao Gao, Daniela Witten, and Fang Han", "title": "Optimal estimation of variance in nonparametric regression with random\n  design", "comments": "to appear in the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the heteroscedastic nonparametric regression model with random\ndesign \\begin{align*} Y_i = f(X_i) + V^{1/2}(X_i)\\varepsilon_i, \\quad\ni=1,2,\\ldots,n, \\end{align*} with $f(\\cdot)$ and $V(\\cdot)$ $\\alpha$- and\n$\\beta$-H\\\"older smooth, respectively. We show that the minimax rate of\nestimating $V(\\cdot)$ under both local and global squared risks is of the order\n\\begin{align*} n^{-\\frac{8\\alpha\\beta}{4\\alpha\\beta + 2\\alpha + \\beta}} \\vee\nn^{-\\frac{2\\beta}{2\\beta+1}}, \\end{align*} where $a\\vee b := \\max\\{a,b\\}$ for\nany two real numbers $a,b$. This result extends the fixed design rate\n$n^{-4\\alpha} \\vee n^{-2\\beta/(2\\beta+1)}$ derived in Wang et al. [2008] in a\nnon-trivial manner, as indicated by the appearances of both $\\alpha$ and\n$\\beta$ in the first term. In the special case of constant variance, we show\nthat the minimax rate is $n^{-8\\alpha/(4\\alpha+1)}\\vee n^{-1}$ for variance\nestimation, which further implies the same rate for quadratic functional\nestimation and thus unifies the minimax rate under the nonparametric regression\nmodel with those under the density model and the white noise model. To achieve\nthe minimax rate, we develop a U-statistic-based local polynomial estimator and\na lower bound that is constructed over a specified distribution family of\nrandomness designed for both $\\varepsilon_i$ and $X_i$.\n", "versions": [{"version": "v1", "created": "Wed, 27 Feb 2019 23:04:37 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 18:02:34 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Shen", "Yandi", ""], ["Gao", "Chao", ""], ["Witten", "Daniela", ""], ["Han", "Fang", ""]]}, {"id": "1902.11176", "submitter": "Victor-Emmanuel Brunel", "authors": "Victor-Emmanuel Brunel", "title": "Learning rates for Gaussian mixtures under group invariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the pointwise maximum likelihood estimation rates for a class of\nGaussian mixtures that are invariant under the action of some isometry group.\nThis model is also known as multi-reference alignment, where random isometries\nof a given vector are observed, up to Gaussian noise. We completely\ncharacterize the speed of the maximum likelihood estimator, by giving a\ncomprehensive description of the likelihood geometry of the model. We show that\nthe unknown parameter can always be decomposed into two components, one of\nwhich can be estimated at the fast rate $n^{-1/2}$, the other one being\nestimated at the slower rate $n^{-1/4}$. We provide an algebraic description\nand a geometric interpretation of these facts.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 15:55:30 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Brunel", "Victor-Emmanuel", ""]]}, {"id": "1902.11192", "submitter": "Francesco Ortelli", "authors": "Francesco Ortelli, Sara van de Geer", "title": "Oracle inequalities for square root analysis estimators with application\n  to total variation penalties", "comments": null, "journal-ref": "Information and Inference: A Journal of the IMA, iaaa002, 2020", "doi": "10.1093/imaiai/iaaa002", "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Through the direct study of the analysis estimator we derive oracle\ninequalities with fast and slow rates by adapting the arguments involving\nprojections by Dalalyan, Hebiri and Lederer (2017). We then extend the theory\nto the square root analysis estimator. Finally, we focus on (square root) total\nvariation regularized estimators on graphs and obtain constant-friendly rates,\nwhich, up to log-terms, match previous results obtained by entropy\ncalculations. We also obtain an oracle inequality for the (square root) total\nvariation regularized estimator over the cycle graph.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 16:19:57 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 13:30:35 GMT"}], "update_date": "2021-02-12", "authors_parsed": [["Ortelli", "Francesco", ""], ["van de Geer", "Sara", ""]]}, {"id": "1902.11260", "submitter": "Tobias Boege", "authors": "Tobias Boege and Thomas Kahle", "title": "Construction Methods for Gaussoids", "comments": "18 pages, 3 figures, 1 table; corrected Remark 3.13 and minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of $n$-gaussoids is shown to be a double exponential function in\n$n$. The necessary bounds are achieved by studying construction methods for\ngaussoids that rely on prescribing $3$-minors and encoding the resulting\ncombinatorial constraints in a suitable transitive graph. Various special\nclasses of gaussoids arise from restricting the allowed $3$-minors.\n", "versions": [{"version": "v1", "created": "Thu, 28 Feb 2019 18:07:38 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2019 16:28:21 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Boege", "Tobias", ""], ["Kahle", "Thomas", ""]]}]