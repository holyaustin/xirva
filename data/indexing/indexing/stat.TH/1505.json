[{"id": "1505.00003", "submitter": "Dimitris Kugiumtzis", "authors": "G. Papadopoulos and D. Kugiumtzis", "title": "Estimation of connectivity measures in gappy time series", "comments": "20 pages, 10 figures, submitted to Physica A", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.data-an q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method is proposed to compute connectivity measures on multivariate\ntime series with gaps. Rather than removing or filling the gaps, the rows of\nthe joint data matrix containing empty entries are removed and the calculations\nare done on the remainder matrix. The method, called measure adapted gap\nremoval (MAGR), can be applied to any connectivity measure that uses a joint\ndata matrix, such as cross correlation, cross mutual information and transfer\nentropy. MAGR is favorably compared using these three measures to a number of\nknown gap-filling techniques, as well as the gap closure. The superiority of\nMAGR is illustrated on time series from synthetic systems and financial time\nseries.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 22:35:10 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Papadopoulos", "G.", ""], ["Kugiumtzis", "D.", ""]]}, {"id": "1505.00261", "submitter": "Evan Warren Sangaline", "authors": "Evan Sangaline", "title": "Strongly Intensive Cumulants: Fluctuation Measures for Systems With\n  Incompletely Constrained Volumes", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "nucl-th math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cumulants of thermal variables are of general interest in physics due to\ntheir extensivity and their correspondence with susceptibilities. They become\nespecially significant near critical points of phase transitions where they\ndiverge along with the correlation length. Cumulant measurements have been used\nextensively within the field of heavy-ion physics, principally as tools in the\nsearch for a hypothetical QCD critical point along the transition between\nhadronic matter and QGP. The volume of individual heavy-ion collisions can be\nonly partially constrained and, as a result, cumulant measurements are\nsignificantly biased by the limited volume resolution. We propose a class of\nmoments called strongly intensive cumulants which can be accurately measured in\nthe presence of unconstrained volume fluctuations. Additionally, they share the\nsame direct relationship with susceptibilities as cumulants in many cases.\n", "versions": [{"version": "v1", "created": "Fri, 1 May 2015 19:58:48 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2015 15:38:17 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Sangaline", "Evan", ""]]}, {"id": "1505.00369", "submitter": "Vianney Perchet", "authors": "Vianney Perchet, Philippe Rigollet, Sylvain Chassang, Erik Snowberg", "title": "Batched bandit problems", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1381 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 2, 660-681", "doi": "10.1214/15-AOS1381", "report-no": "IMS-AOS-AOS1381", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by practical applications, chiefly clinical trials, we study the\nregret achievable for stochastic bandits under the constraint that the employed\npolicy must split trials into a small number of batches. We propose a simple\npolicy, and show that a very small number of batches gives close to minimax\noptimal regret bounds. As a byproduct, we derive optimal policies with low\nswitching cost for stochastic bandits.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 20:22:00 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2015 13:55:27 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2016 08:00:15 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Perchet", "Vianney", ""], ["Rigollet", "Philippe", ""], ["Chassang", "Sylvain", ""], ["Snowberg", "Erik", ""]]}, {"id": "1505.00379", "submitter": "Jon A. Wellner", "authors": "Qiyang Han and Jon A. Wellner", "title": "Approximation and Estimation of s-Concave Densities via R\\'enyi\n  Divergences", "comments": "65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the approximation and estimation of $s$-concave\ndensities via R\\'enyi divergence. We first show that the approximation of a\nprobability measure $Q$ by an $s$-concave densities exists and is unique via\nthe procedure of minimizing a divergence functional proposed by Koenker and\nMizera (2010) if and only if $Q$ admits full-dimensional support and a first\nmoment. We also show continuity of the divergence functional in $Q$: if $Q_n\n\\to Q$ in the Wasserstein metric, then the projected densities converge in\nweighted $L_1$ metrics and uniformly on closed subsets of the continuity set of\nthe limit. Moreover, directional derivatives of the projected densities also\nenjoy local uniform convergence. This contains both on-the-model and\noff-the-model situations, and entails strong consistency of the divergence\nestimator of an $s$-concave density under mild conditions. One interesting and\nimportant feature for the R\\'enyi divergence estimator of an $s$-concave\ndensity is that the estimator is intrinsically related with the estimation of\nlog-concave densities via maximum likelihood methods. In fact, we show that for\n$d=1$ at least, the R\\'enyi divergence estimators for $s$-concave densities\nconverge to the maximum likelihood estimator of a log-concave density as $s\n\\nearrow 0$. The R\\'enyi divergence estimator shares similar characterizations\nas the MLE for log-concave distributions, which allows us to develop pointwise\nasymptotic distribution theory assuming that the underlying density is\n$s$-concave.\n", "versions": [{"version": "v1", "created": "Sat, 2 May 2015 23:09:24 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 18:49:20 GMT"}, {"version": "v3", "created": "Sat, 13 Jun 2015 15:51:48 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2015 19:36:33 GMT"}], "update_date": "2015-10-23", "authors_parsed": [["Han", "Qiyang", ""], ["Wellner", "Jon A.", ""]]}, {"id": "1505.00482", "submitter": "Larry Wasserman", "authors": "Martin Azizyan, Yen-Chi Chen, Aarti Singh and Larry Wasserman", "title": "Risk Bounds For Mode Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density mode clustering is a nonparametric clustering method. The clusters\nare the basins of attraction of the modes of a density estimator. We study the\nrisk of mode-based clustering. We show that the clustering risk over the\ncluster cores --- the regions where the density is high --- is very small even\nin high dimensions. And under a low noise condition, the overall cluster risk\nis small even beyond the cores, in high dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 3 May 2015 21:46:42 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Azizyan", "Martin", ""], ["Chen", "Yen-Chi", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1505.00579", "submitter": "Daniel Rudolf", "authors": "Daniel Rudolf and Mario Ullrich", "title": "Comparison of hit-and-run, slice sampling and random walk Metropolis", "comments": "18 pages, Accepted for publication by the Applied Probability Trust\n  (http://www.appliedprobability.org) in J. Appl. Prob", "journal-ref": null, "doi": "10.1017/jpr.2018.78", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different Markov chains can be used for approximate sampling of a\ndistribution given by an unnormalized density function with respect to the\nLebesgue measure. The hit-and-run, (hybrid) slice sampler and random walk\nMetropolis algorithm are popular tools to simulate such Markov chains. We\ndevelop a general approach to compare the efficiency of these sampling\nprocedures by the use of a partial ordering of their Markov operators, the\ncovariance ordering. In particular, we show that the hit-and-run and the simple\nslice sampler are more efficient than a hybrid slice sampler based on\nhit-and-run which, itself, is more efficient than a (lazy) random walk\nMetropolis algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 10:28:50 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 06:50:31 GMT"}, {"version": "v3", "created": "Sat, 18 Aug 2018 08:34:16 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Rudolf", "Daniel", ""], ["Ullrich", "Mario", ""]]}, {"id": "1505.00662", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Optimal Learning via the Fourier Transform for Sums of Independent\n  Integer Random Variables", "comments": "Main differences from v1: Changed title and restructured\n  introduction. Added new sample optimal algorithm. Generalized sample lower\n  bound for any value of k", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the structure and learnability of sums of independent integer random\nvariables (SIIRVs). For $k \\in \\mathbb{Z}_{+}$, a $k$-SIIRV of order $n \\in\n\\mathbb{Z}_{+}$ is the probability distribution of the sum of $n$ independent\nrandom variables each supported on $\\{0, 1, \\dots, k-1\\}$. We denote by ${\\cal\nS}_{n,k}$ the set of all $k$-SIIRVs of order $n$.\n  In this paper, we tightly characterize the sample and computational\ncomplexity of learning $k$-SIIRVs. More precisely, we design a computationally\nefficient algorithm that uses $\\widetilde{O}(k/\\epsilon^2)$ samples, and learns\nan arbitrary $k$-SIIRV within error $\\epsilon,$ in total variation distance.\nMoreover, we show that the {\\em optimal} sample complexity of this learning\nproblem is $\\Theta((k/\\epsilon^2)\\sqrt{\\log(1/\\epsilon)}).$ Our algorithm\nproceeds by learning the Fourier transform of the target $k$-SIIRV in its\neffective support. Its correctness relies on the {\\em approximate sparsity} of\nthe Fourier transform of $k$-SIIRVs -- a structural property that we establish,\nroughly stating that the Fourier transform of $k$-SIIRVs has small magnitude\noutside a small set.\n  Along the way we prove several new structural results about $k$-SIIRVs. As\none of our main structural contributions, we give an efficient algorithm to\nconstruct a sparse {\\em proper} $\\epsilon$-cover for ${\\cal S}_{n,k},$ in total\nvariation distance. We also obtain a novel geometric characterization of the\nspace of $k$-SIIRVs. Our characterization allows us to prove a tight lower\nbound on the size of $\\epsilon$-covers for ${\\cal S}_{n,k}$, and is the key\ningredient in our tight sample complexity lower bound.\n  Our approach of exploiting the sparsity of the Fourier transform in\ndistribution learning is general, and has recently found additional\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 14:48:01 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2015 07:03:28 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1505.00675", "submitter": "Daniel Waltner", "authors": "Tim Wirtz, Daniel Waltner, Mario Kieburg, Santosh Kumar", "title": "The Correlated Jacobi and the Correlated Cauchy-Lorentz ensembles", "comments": "29 pages, 3 figures", "journal-ref": "J. Stat Phys 162 (2016) 495", "doi": "10.1007/s10955-015-1416-5", "report-no": null, "categories": "math.ST math-ph math.MP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We calculate the $k$-point generating function of the correlated Jacobi\nensemble using supersymmetric methods. We use the result for complex matrices\nfor $k=1$ to derive a closed-form expression for eigenvalue density. For real\nmatrices we obtain the density in terms of a twofold integral that we evaluate\nnumerically. For both expressions we find agreement when comparing with Monte\nCarlo simulations. Relations between these quantities for the Jacobi and the\nCauchy-Lorentz ensemble are derived.\n", "versions": [{"version": "v1", "created": "Mon, 4 May 2015 15:17:23 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Wirtz", "Tim", ""], ["Waltner", "Daniel", ""], ["Kieburg", "Mario", ""], ["Kumar", "Santosh", ""]]}, {"id": "1505.00990", "submitter": "Goetz Pfander", "authors": "G\\\"otz E. Pfander, Pavel Zheltov", "title": "Identification of stochastic operators", "comments": null, "journal-ref": "Applied and Computational Harmonic Analysis 36, 256-279, 2014", "doi": "10.1016/j.acha.2013.05.001", "report-no": null, "categories": "math.FA cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the here developed functional analytic machinery we extend the\ntheory of operator sampling and identification to apply to operators with\nstochastic spreading functions. We prove that identification with a delta train\nsignal is possible for a large class of stochastic operators that have the\nproperty that the autocorrelation of the spreading function is supported on a\nset of 4D volume less than one and this support set does not have a defective\nstructure. In fact, unlike in the case of deterministic operator\nidentification, the geometry of the support set has a significant impact on the\nidentifiability of the considered operator class. Also, we prove that,\nanalogous to the deterministic case, the restriction of the 4D volume of a\nsupport set to be less or equal to one is necessary for identifiability of a\nstochastic operator class.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 12:51:48 GMT"}], "update_date": "2015-05-06", "authors_parsed": [["Pfander", "G\u00f6tz E.", ""], ["Zheltov", "Pavel", ""]]}, {"id": "1505.00991", "submitter": "Yair Goldberg", "authors": "Yael Travis-Lumer and Yair Goldberg", "title": "Kernel Machines for Current Status Data", "comments": "37 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In survival analysis, estimating the failure time distribution is an\nimportant and difficult task, since usually the data is subject to censoring.\nSpecifically, in this paper we consider current status data, a type of data\nwhere all of the observations are censored. The format of the data is such that\nthe failure time is restricted to knowledge of whether or not the failure time\nexceeds a random monitoring time. We propose a flexible kernel machine approach\nfor estimation of the failure time expectation as a function of the covariates,\nwith current status data. In order to obtain the kernel machine decision\nfunction, we minimize a regularized version of the empirical risk with respect\nto a new loss function. Using finite sample bounds and novel oracle\ninequalities, we prove that the obtained estimator converges to the true\nconditional expectation for a large family of probability measures. Finally, we\npresent a simulation study and an analysis of real-world data that compares the\nperformance of the proposed approach to existing methods. We show empirically\nthat our approach is comparable to current state of the art, and in some cases\nis even better.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 12:53:50 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 06:29:41 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Travis-Lumer", "Yael", ""], ["Goldberg", "Yair", ""]]}, {"id": "1505.01163", "submitter": "Yi Shen", "authors": "Yi Shen and Tony S. Wirjanto", "title": "Stationarity as a Path Property with Applications in Time Series\n  Analysis", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally stationarity refers to shift invariance of the distribution of\na stochastic process. In this paper, we rediscover stationarity as a path\nproperty instead of a distributional property. More precisely, we characterize\na set of paths denoted as $A$, which corresponds to the notion of stationarity.\nOn one hand, the set $A$ is shown to be large enough, so that for any\nstationary process, almost all of its paths are in $A$. On the other hand, we\nprove that any path in $A$ will behave in the optimal way under any\nstationarity test satisfying some mild conditions. The results provide a\nunified framework to understand and assess the existing time series tests for\nstationarity, and can potentially lead to new families of stationarity tests.\n", "versions": [{"version": "v1", "created": "Tue, 5 May 2015 20:01:01 GMT"}, {"version": "v2", "created": "Sat, 15 Oct 2016 16:29:59 GMT"}], "update_date": "2016-10-18", "authors_parsed": [["Shen", "Yi", ""], ["Wirjanto", "Tony S.", ""]]}, {"id": "1505.01247", "submitter": "Meng Wang", "authors": "Ery Arias-Castro and Meng Wang", "title": "The Sparse Poisson Means Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting a sparse Poisson mixture. Our results\nparallel those for the detection of a sparse normal mixture, pioneered by\nIngster (1997) and Donoho and Jin (2004), when the Poisson means are larger\nthan logarithmic in the sample size. In particular, a form of higher criticism\nachieves the detection boundary in the whole sparse regime. When the Poisson\nmeans are smaller than logarithmic in the sample size, a different regime\narises in which simple multiple testing with Bonferroni correction is enough in\nthe sparse regime. We present some numerical experiments that confirm our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 03:58:24 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Wang", "Meng", ""]]}, {"id": "1505.01394", "submitter": "William Kleiber", "authors": "William Kleiber", "title": "Coherence for Random Fields", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate spatial field data are increasingly common and whose modeling\ntypically relies on building cross-covariance functions to describe\ncross-process relationships. An alternative viewpoint is to model the matrix of\nspectral measures. We develop the notions of coherence, phase and gain for\nmultidimensional stationary processes. Coherence, as a function of frequency,\ncan be seen to be a measure of linear relationship between two spatial\nprocesses at that frequency band. We use the coherence function to illustrate\nfundamental limitations on a number of previously proposed constructions for\nmultivariate processes, suggesting these options are not viable for real data.\nWe also give natural interpretations to cross-covariance parameters of the\nMatern class, where the smoothness indexes dependence at low frequencies while\nthe range parameter can imply dependence at low or high frequencies. Estimation\nfollows from smoothed multivariate periodogram matrices. We illustrate the\nestimation and interpretation of these functions on two datasets, forecast and\nreanalysis sea level pressure and geopotential heights over the equatorial\nregion. Examining these functions lends insight that would otherwise be\ndifficult to detect and model using standard cross-covariance formulations.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 15:19:19 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Kleiber", "William", ""]]}, {"id": "1505.01461", "submitter": "Dave Zachariah", "authors": "Dave Zachariah, Petre Stoica", "title": "Online Hyperparameter-Free Sparse Estimation Method", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, 2015, Vol. 63, No. 13,\n  pages 3348 - 3359", "doi": "10.1109/TSP.2015.2421472", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive an online estimator for sparse parameter vectors\nwhich, unlike the LASSO approach, does not require the tuning of any\nhyperparameters. The algorithm is based on a covariance matching approach and\nis equivalent to a weighted version of the square-root LASSO. The computational\ncomplexity of the estimator is of the same order as that of the online versions\nof regularized least-squares (RLS) and LASSO. We provide a numerical comparison\nwith feasible and infeasible implementations of the LASSO and RLS to illustrate\nthe advantage of the proposed online hyperparameter-free estimator.\n", "versions": [{"version": "v1", "created": "Wed, 6 May 2015 19:03:25 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Zachariah", "Dave", ""], ["Stoica", "Petre", ""]]}, {"id": "1505.01583", "submitter": "Dennis Leung", "authors": "Dennis Leung, Mathias Drton, Hisayuki Hara", "title": "Identifiability of directed Gaussian graphical models with one latent\n  source", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study parameter identifiability of directed Gaussian graphical models with\none latent variable. In the scenario we consider, the latent variable is a\nconfounder that forms a source node of the graph and is a parent to all other\nnodes, which correspond to the observed variables. We give a graphical\ncondition that is sufficient for the Jacobian matrix of the parametrization map\nto be full rank, which entails that the parametrization is generically\nfinite-to-one, a fact that is sometimes also referred to as local\nidentifiability. We also derive a graphical condition that is necessary for\nsuch identifiability. Finally, we give a condition under which generic\nparameter identifiability can be determined from identifiability of a model\nassociated with a subgraph. The power of these criteria is assessed via an\nexhaustive algebraic computational study on models with 4, 5, and 6 observable\nvariables.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 04:36:52 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Leung", "Dennis", ""], ["Drton", "Mathias", ""], ["Hara", "Hisayuki", ""]]}, {"id": "1505.01585", "submitter": "Xin Lu Tan", "authors": "T. Tony Cai and Xin Lu Tan", "title": "Optimal Estimation of A Quadratic Functional and Detection of\n  Simultaneous Signals", "comments": "41 pages including appendix, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in genomics, this paper studies the problem of\noptimal estimation of a quadratic functional of two normal mean vectors,\n$Q(\\mu, \\theta) = \\frac{1}{n}\\sum_{i=1}^n\\mu_i^2\\theta_i^2$, with a particular\nfocus on the case where both mean vectors are sparse. We propose optimal\nestimators of $Q(\\mu, \\theta)$ for different regimes and establish the minimax\nrates of convergence over a family of parameter spaces. The optimal rates\nexhibit interesting phase transitions in this family. The simultaneous signal\ndetection problem is also considered under the minimax framework. It is shown\nthat the proposed estimators for $Q(\\mu, \\theta)$ naturally lead to optimal\ntesting procedures.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 04:41:15 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Cai", "T. Tony", ""], ["Tan", "Xin Lu", ""]]}, {"id": "1505.01665", "submitter": "Stanley I. M. Ko", "authors": "Stanley I. M. Ko, Terence T. L. Chong, Pulak Ghosh", "title": "Dirichlet Process Hidden Markov Multiple Change-point Model", "comments": "Published at http://dx.doi.org/10.1214/14-BA910 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 275-296", "doi": "10.1214/14-BA910", "report-no": "VTeX-BA-BA910", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new Bayesian multiple change-point model which is based\non the hidden Markov approach. The Dirichlet process hidden Markov model does\nnot require the specification of the number of change-points a priori. Hence\nour model is robust to model specification in contrast to the fully parametric\nBayesian model. We propose a general Markov chain Monte Carlo algorithm which\nonly needs to sample the states around change-points. Simulations for a normal\nmean-shift model with known and unknown variance demonstrate advantages of our\napproach. Two applications, namely the coal-mining disaster data and the real\nUnited States Gross Domestic Product growth, are provided. We detect a single\nchange-point for both the disaster data and US GDP growth. All the change-point\nlocations and posterior inferences of the two applications are in line with\nexisting methods.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 11:14:49 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Ko", "Stanley I. M.", ""], ["Chong", "Terence T. L.", ""], ["Ghosh", "Pulak", ""]]}, {"id": "1505.01687", "submitter": "Hao Wang", "authors": "Hao Wang", "title": "Scaling It Up: Stochastic Search Structure Learning in Graphical Models", "comments": "Published at http://dx.doi.org/10.1214/14-BA916 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 351-377", "doi": "10.1214/14-BA916", "report-no": "VTeX-BA-BA916", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian concentration graph models and covariance graph models are two\nclasses of graphical models that are useful for uncovering latent dependence\nstructures among multivariate variables. In the Bayesian literature, graphs are\noften determined through the use of priors over the space of positive definite\nmatrices with fixed zeros, but these methods present daunting computational\nburdens in large problems. Motivated by the superior computational efficiency\nof continuous shrinkage priors for regression analysis, we propose a new\nframework for structure learning that is based on continuous spike and slab\npriors and uses latent variables to identify graphs. We discuss model\nspecification, computation, and inference for both concentration and covariance\ngraph models. The new approach produces reliable estimates of graphs and\nefficiently handles problems with hundreds of variables.\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 12:53:40 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Wang", "Hao", ""]]}, {"id": "1505.01787", "submitter": "James Duffy", "authors": "James A. Duffy", "title": "Uniform convergence rates over maximal domains in structural\n  nonparametric cointegrating regression", "comments": "34 pp. + 5 pp. supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents uniform convergence rates for kernel regression\nestimators, in the setting of a structural nonlinear cointegrating regression\nmodel. We generalise the existing literature in three ways. First, the domain\nto which these rates apply is much wider than has been previously considered,\nand can be chosen so as to contain as large a fraction of the sample as desired\nin the limit. Second, our results allow the regression disturbance to be\nserially correlated, and cross-correlated with the regressor; previous work on\nthis problem (of obtaining uniform rates) having been confined entirely to the\nsetting of an exogenous regressor. Third, we permit the bandwidth to be\ndata-dependent, requiring only that it satisfy certain weak asymptotic\nshrinkage conditions. Our assumptions on the regressor process are consistent\nwith a very broad range of departures from the standard unit root\nautoregressive model, allowing the regressor to be fractionally integrated, and\nto have an infinite variance (and even infinite lower-order moments).\n", "versions": [{"version": "v1", "created": "Thu, 7 May 2015 17:42:07 GMT"}], "update_date": "2015-05-08", "authors_parsed": [["Duffy", "James A.", ""]]}, {"id": "1505.01922", "submitter": "Yuma Uehara", "authors": "Hiroki Masuda, Yuma Uehara", "title": "Two-step estimation of ergodic L\\'evy driven SDE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high frequency samples from ergodic L\\'evy driven stochastic\ndifferential equation (SDE) with drift coefficient $a(x,\\alpha)$ and scale\ncoefficient $c(x,\\gamma)$ involving unknown parameters $\\alpha$ and $\\gamma$.\nWe suppose that the L\\'evy measure $\\nu_{0}$, has all order moments but is not\nfully specified. We will prove the joint asymptotic normality of some\nestimators of $\\alpha$, $\\gamma$ and a class of functional parameter\n$\\int\\varphi(z)\\nu_0(dz)$, which are constructed in a two-step manner: first,\nwe use the Gaussian quasi-likelihood for estimation of $(\\alpha,\\gamma)$, and\nthen, for estimating $\\int\\varphi(z)\\nu_0(dz)$ we makes use of the method of\nmoments based on the Euler-type residual with the the previously obtained\nquasi-likelihood estimator.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 03:55:17 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 05:36:35 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2015 08:12:26 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2015 09:07:50 GMT"}, {"version": "v5", "created": "Thu, 20 Aug 2015 02:32:34 GMT"}, {"version": "v6", "created": "Mon, 11 Jan 2016 03:35:04 GMT"}], "update_date": "2016-01-12", "authors_parsed": [["Masuda", "Hiroki", ""], ["Uehara", "Yuma", ""]]}, {"id": "1505.01957", "submitter": "Natesh Pillai", "authors": "Natesh S. Pillai, Xiao-Li Meng", "title": "An unexpected encounter with Cauchy and L\\'evy", "comments": "We present an elementary proof of a recent conjecture about a Cauchy\n  random variable obtained as the ratio of dependent Gaussians; A minor\n  correction from the previous version; a figure added. This is the final\n  version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cauchy distribution is usually presented as a mathematical curiosity, an\nexception to the Law of Large Numbers, or even as an \"Evil\" distribution in\nsome introductory courses. It therefore surprised us when Drton and Xiao (2016)\nproved the following result for $m=2$ and conjectured it for $m\\ge 3$. Let $X=\n(X_1,..., X_m)$ and $Y = (Y_1, ...,Y_m)$ be i.i.d $N(0,\\Sigma)$, where\n$\\Sigma=\\{\\sigma_{ij}\\}\\ge 0$ is an $m\\times m$ and \\textit{arbitrary}\ncovariance matrix with $\\sigma_{jj}>0$ for all $1\\leq j\\leq m$. Then $$Z =\n\\sum_{j=1}^m w_j \\frac{X_j}{Y_j} \\ \\sim \\mathrm{Cauchy}(0,1),$$ as long as\n$w=(w_1,..., w_m) $ is independent of $(X, Y)$, $w_j\\ge 0, j=1,..., m$, and\n$\\sum_{j=1}^m w_j=1$. In this note, we present an elementary proof of this\nconjecture for any $m \\geq 2$ by linking $Z$ to a geometric characterization of\nCauchy(0,1) given in Willams (1969). This general result is essential to the\nlarge sample behavior of Wald tests in many applications such as factor models\nand contingency tables. It also leads to other unexpected results such as $$\n\\sum_{i=1}^m\\sum_{j=1}^m \\frac{w_iw_j\\sigma_{ij}}{X_iX_j} \\sim\n{\\text{L\\'{e}vy}}(0, 1). $$ This generalizes the \"super Cauchy phenomenon\" that\nthe average of $m$ i.i.d. standard L\\'evy variables (i.e., inverse chi-squared\nvariables with one degree of freedom) has the same distribution as that of a\nsingle standard L\\'evy variable multiplied by $m$ (which is obtained by taking\n$w_j=1/m$ and $\\Sigma$ to be the identity matrix).\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 08:52:09 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2015 02:56:32 GMT"}], "update_date": "2015-10-20", "authors_parsed": [["Pillai", "Natesh S.", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "1505.02023", "submitter": "Shahin Tavakoli", "authors": "John A. D. Aston, Davide Pigoli and Shahin Tavakoli", "title": "Tests for separability in nonparametric covariance operators of random\n  surfaces", "comments": "47 pages, 10 figures, 4 tables", "journal-ref": "Annals of Statistics, Vol. 45, No. 4, 1431-1461 (2017)", "doi": "10.1214/16-AOS1495", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assumption of separability of the covariance operator for a random image\nor hypersurface can be of substantial use in applications, especially in\nsituations where the accurate estimation of the full covariance structure is\nunfeasible, either for computational reasons, or due to a small sample size.\nHowever, inferential tools to verify this assumption are somewhat lacking in\nhigh-dimensional or functional {data analysis} settings, where this assumption\nis most relevant. We propose here to test separability by focusing on\n$K$-dimensional projections of the difference between the covariance operator\nand a nonparametric separable approximation. The subspace we project onto is\none generated by the eigenfunctions of the covariance operator estimated under\nthe separability hypothesis, negating the need to ever estimate the full\nnon-separable covariance. We show that the rescaled difference of the sample\ncovariance operator with its separable approximation is asymptotically\nGaussian. As a by-product of this result, we derive asymptotically pivotal\ntests under Gaussian assumptions, and propose bootstrap methods for\napproximating the distribution of the test statistics. We probe the finite\nsample performance through simulations studies, and present an application to\nlog-spectrogram images from a phonetic linguistics dataset.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 13:01:37 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 17:04:45 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2016 15:36:04 GMT"}, {"version": "v4", "created": "Fri, 3 Jun 2016 12:24:36 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Aston", "John A. D.", ""], ["Pigoli", "Davide", ""], ["Tavakoli", "Shahin", ""]]}, {"id": "1505.02077", "submitter": "Marta Ferreira", "authors": "Helena Ferreira and Marta Ferreira", "title": "Estimating the extremal index through local dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extremal index is an important parameter in the characterization of\nextreme values of a stationary sequence. Our new estimation approach for this\nparameter is based on the extremal behavior under the local dependence\ncondition D$^{(k)}$($u_n$). We compare a process satisfying one of this\nhierarchy of increasingly weaker local mixing conditions with a process of\ncycles satisfying the D$^{(2)}$($u_n$) condition. We also analyze local\ndependence within moving maxima processes and derive a necessary and sufficient\ncondition for D$^{(k)}$($u_n$). In order to evaluate the performance of the\nproposed estimators, we apply an empirical diagnostic for local dependence\nconditions, we conduct a simulation study and compare with existing methods. An\napplication to a financial time series is also presented.\n", "versions": [{"version": "v1", "created": "Fri, 8 May 2015 16:02:06 GMT"}], "update_date": "2015-05-11", "authors_parsed": [["Ferreira", "Helena", ""], ["Ferreira", "Marta", ""]]}, {"id": "1505.02212", "submitter": "Yakir Reshef", "authors": "Yakir A. Reshef, David N. Reshef, Pardis C. Sabeti, Michael M.\n  Mitzenmacher", "title": "Equitability, interval estimation, and statistical power", "comments": "Yakir A. Reshef and David N. Reshef are co-first authors, Pardis C.\n  Sabeti and Michael M. Mitzenmacher are co-last authors. This paper, together\n  with arXiv:1505.02212, subsumes arXiv:1408.4908", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG q-bio.QM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For analysis of a high-dimensional dataset, a common approach is to test a\nnull hypothesis of statistical independence on all variable pairs using a\nnon-parametric measure of dependence. However, because this approach attempts\nto identify any non-trivial relationship no matter how weak, it often\nidentifies too many relationships to be useful. What is needed is a way of\nidentifying a smaller set of relationships that merit detailed further\nanalysis.\n  Here we formally present and characterize equitability, a property of\nmeasures of dependence that aims to overcome this challenge. Notionally, an\nequitable statistic is a statistic that, given some measure of noise, assigns\nsimilar scores to equally noisy relationships of different types [Reshef et al.\n2011]. We begin by formalizing this idea via a new object called the\ninterpretable interval, which functions as an interval estimate of the amount\nof noise in a relationship of unknown type. We define an equitable statistic as\none with small interpretable intervals.\n  We then draw on the equivalence of interval estimation and hypothesis testing\nto show that under moderate assumptions an equitable statistic is one that\nyields well powered tests for distinguishing not only between trivial and\nnon-trivial relationships of all kinds but also between non-trivial\nrelationships of different strengths. This means that equitability allows us to\nspecify a threshold relationship strength $x_0$ and to search for relationships\nof all kinds with strength greater than $x_0$. Thus, equitability can be\nthought of as a strengthening of power against independence that enables\nfruitful analysis of data sets with a small number of strong, interesting\nrelationships and a large number of weaker ones. We conclude with a\ndemonstration of how our two equivalent characterizations of equitability can\nbe used to evaluate the equitability of a statistic in practice.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 00:31:23 GMT"}, {"version": "v2", "created": "Tue, 12 May 2015 20:05:17 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Reshef", "Yakir A.", ""], ["Reshef", "David N.", ""], ["Sabeti", "Pardis C.", ""], ["Mitzenmacher", "Michael M.", ""]]}, {"id": "1505.02288", "submitter": "Alessio Benavoli", "authors": "Alessio Benavoli and Giorgio Corani and Francesca Mangili", "title": "Should we really use post-hoc tests based on mean-ranks?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST physics.data-an q-bio.QM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical comparison of multiple algorithms over multiple data sets is\nfundamental in machine learning. This is typically carried out by the Friedman\ntest. When the Friedman test rejects the null hypothesis, multiple comparisons\nare carried out to establish which are the significant differences among\nalgorithms. The multiple comparisons are usually performed using the mean-ranks\ntest. The aim of this technical note is to discuss the inconsistencies of the\nmean-ranks post-hoc test with the goal of discouraging its use in machine\nlearning as well as in medicine, psychology, etc.. We show that the outcome of\nthe mean-ranks test depends on the pool of algorithms originally included in\nthe experiment. In other words, the outcome of the comparison between\nalgorithms A and B depends also on the performance of the other algorithms\nincluded in the original experiment. This can lead to paradoxical situations.\nFor instance the difference between A and B could be declared significant if\nthe pool comprises algorithms C, D, E and not significant if the pool comprises\nalgorithms F, G, H. To overcome these issues, we suggest instead to perform the\nmultiple comparison using a test whose outcome only depends on the two\nalgorithms being compared, such as the sign-test or the Wilcoxon signed-rank\ntest.\n", "versions": [{"version": "v1", "created": "Sat, 9 May 2015 15:54:56 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Benavoli", "Alessio", ""], ["Corani", "Giorgio", ""], ["Mangili", "Francesca", ""]]}, {"id": "1505.02475", "submitter": "Bala Rajaratnam", "authors": "Alfred O. Hero and Bala Rajaratnam", "title": "Foundational principles for large scale inference: Illustrations through\n  correlation mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When can reliable inference be drawn in the \"Big Data\" context? This paper\npresents a framework for answering this fundamental question in the context of\ncorrelation mining, with implications for general large scale inference. In\nlarge scale data applications like genomics, connectomics, and eco-informatics\nthe dataset is often variable-rich but sample-starved: a regime where the\nnumber $n$ of acquired samples (statistical replicates) is far fewer than the\nnumber $p$ of observed variables (genes, neurons, voxels, or chemical\nconstituents). Much of recent work has focused on understanding the\ncomputational complexity of proposed methods for \"Big Data.\" Sample complexity\nhowever has received relatively less attention, especially in the setting when\nthe sample size $n$ is fixed, and the dimension $p$ grows without bound. To\naddress this gap, we develop a unified statistical framework that explicitly\nquantifies the sample complexity of various inferential tasks. Sampling regimes\ncan be divided into several categories: 1) the classical asymptotic regime\nwhere the variable dimension is fixed and the sample size goes to infinity; 2)\nthe mixed asymptotic regime where both variable dimension and sample size go to\ninfinity at comparable rates; 3) the purely high dimensional asymptotic regime\nwhere the variable dimension goes to infinity and the sample size is fixed.\nEach regime has its niche but only the latter regime applies to exa-scale data\ndimension. We illustrate this high dimensional framework for the problem of\ncorrelation mining, where it is the matrix of pairwise and partial correlations\namong the variables that are of interest. We demonstrate various regimes of\ncorrelation mining based on the unifying perspective of high dimensional\nlearning rates and sample complexity for different structured covariance models\nand different inference tasks.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 03:35:33 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 18:46:41 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Hero", "Alfred O.", ""], ["Rajaratnam", "Bala", ""]]}, {"id": "1505.02570", "submitter": "Gabriela Nane", "authors": "Hendrik P. Lopuhaa and Gabriela F. Nane", "title": "An Asymptotic Linear Representation for the Breslow Estimator", "comments": null, "journal-ref": null, "doi": "10.1080/03610926.2012.679762", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an asymptotic linear representation for the Breslow estimator of\nthe baseline cumulative hazard function in the Cox model. Our representation\nconsists of an average of independent random variables and a term involving the\ndifference between the maximum partial likelihood estimator and the underlying\nregression parameter. The order of the remainder term is arbitrarily close to\n1/n.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 11:27:53 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Lopuhaa", "Hendrik P.", ""], ["Nane", "Gabriela F.", ""]]}, {"id": "1505.02589", "submitter": "Garritt L. Page", "authors": "Garritt L. Page, Fernando A. Quintana", "title": "Predictions Based on the Clustering of Heterogeneous Functions via Shape\n  and Subject-Specific Covariates", "comments": "Published at http://dx.doi.org/10.1214/14-BA919 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 379-410", "doi": "10.1214/14-BA919", "report-no": "VTeX-BA-BA919", "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a study of players employed by teams who are members of the\nNational Basketball Association where units of observation are functional\ncurves that are realizations of production measurements taken through the\ncourse of one's career. The observed functional output displays large amounts\nof between player heterogeneity in the sense that some individuals produce\ncurves that are fairly smooth while others are (much) more erratic. We argue\nthat this variability in curve shape is a feature that can be exploited to\nguide decision making, learn about processes under study and improve\nprediction. In this paper we develop a methodology that takes advantage of this\nfeature when clustering functional curves. Individual curves are flexibly\nmodeled using Bayesian penalized B-splines while a hierarchical structure\nallows the clustering to be guided by the smoothness of individual curves. In a\nsense, the hierarchical structure balances the desire to fit individual curves\nwell while still producing meaningful clusters that are used to guide\nprediction. We seamlessly incorporate available covariate information to guide\nthe clustering of curves non-parametrically through the use of a product\npartition model prior for a random partition of individuals. Clustering based\non curve smoothness and subject-specific covariate information is particularly\nimportant in carrying out the two types of predictions that are of interest,\nthose that complete a partially observed curve from an active player, and those\nthat predict the entire career curve for a player yet to play in the National\nBasketball Association.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 12:47:08 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Page", "Garritt L.", ""], ["Quintana", "Fernando A.", ""]]}, {"id": "1505.02605", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss, Anirban Bhattacharya", "title": "Comment on Article by Dawid and Musio", "comments": "Published at http://dx.doi.org/10.1214/15-BA942A in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 501-504", "doi": "10.1214/15-BA942A", "report-no": "VTeX-BA-BA942A", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Bayesian Model Selection Based on Proper Scoring Rules\" by\nDawid and Musio [arXiv:1409.5291].\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 13:30:58 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "1505.02607", "submitter": "Christopher M. Hans", "authors": "Christopher M. Hans, Mario Peruggia", "title": "Comment on Article by Dawid and Musio", "comments": "Published at http://dx.doi.org/10.1214/15-BA942B in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 505-509", "doi": "10.1214/15-BA942B", "report-no": "VTeX-BA-BA942B", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Bayesian Model Selection Based on Proper Scoring Rules\" by\nDawid and Musio [arXiv:1409.5291].\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 13:36:30 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Hans", "Christopher M.", ""], ["Peruggia", "Mario", ""]]}, {"id": "1505.02611", "submitter": "A. Philip Dawid", "authors": "A. Philip Dawid, Monica Musio", "title": "Rejoinder to \"Bayesian Model Selection Based on Proper Scoring Rules\"", "comments": "Published at http://dx.doi.org/10.1214/15-BA942REJ in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 517-521", "doi": "10.1214/15-BA942REJ", "report-no": "VTeX-BA-BA942REJ", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are deeply appreciative of the initiative of the editor, Marina Vanucci,\nin commissioning a discussion of our paper, and extremely grateful to all the\ndiscussants for their insightful and thought-provoking comments. We respond to\nthe discussions in alphabetical order [arXiv:1409.5291].\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 13:41:33 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Dawid", "A. Philip", ""], ["Musio", "Monica", ""]]}, {"id": "1505.02613", "submitter": "Joni Virta", "authors": "Joni Virta, Klaus Nordhausen, Hannu Oja", "title": "Joint Use of Third and Fourth Cumulants in Independent Component\n  Analysis", "comments": "34 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The independent component model is a latent variable model where the\ncomponents of the observed random vector are linear combinations of latent\nindependent variables. The aim is to find an estimate for a transformation\nmatrix back to independent components. In moment-based approaches third\ncumulants are often neglected in favor of fourth cumulants, even though both\napproaches have similar appealing properties. This paper considers the joint\nuse of third and fourth cumulants in finding independent components. First,\nunivariate cumulants are used as projection indices in search for independent\ncomponents (projection pursuit). Second, multivariate cumulant matrices are\njointly used to solve the problem. The properties of the estimates are\nconsidered in detail through corresponding optimization problems, estimating\nequations, algorithms and asymptotic statistical properties. Comparisons of the\nasymptotic variances of different estimates in wide independent component\nmodels show that in most cases symmetric projection pursuit approach using both\nthird and fourth squared cumulants is a safe choice.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 13:47:58 GMT"}], "update_date": "2015-05-12", "authors_parsed": [["Virta", "Joni", ""], ["Nordhausen", "Klaus", ""], ["Oja", "Hannu", ""]]}, {"id": "1505.02725", "submitter": "Yuliana Linke Yu", "authors": "Yu. Yu. Linke", "title": "Asymptotic properties of one-step weighted $M$-estimators and\n  applications to some regression problems", "comments": "in Russian. arXiv admin note: substantial text overlap with\n  arXiv:1503.03393", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study asymptotic behavior of one-step weighted $M$-estimators based on\nsamples from arrays of not necessarily identically distributed random variables\nand representing explicit approximations to the corresponding consistent\nweighted $M$-estimators. Sufficient conditions are presented for asymptotic\nnormality of the one-step weighted $M$-estimators under consideration. As a\nconsequence, we consider some well-known nonlinear regression models where the\nprocedure mentioned allow us to construct explicit asymptotically optimal\nestimators.\n", "versions": [{"version": "v1", "created": "Mon, 11 May 2015 18:43:45 GMT"}, {"version": "v2", "created": "Sat, 4 Jul 2015 17:27:46 GMT"}], "update_date": "2015-07-07", "authors_parsed": [["Linke", "Yu. Yu.", ""]]}, {"id": "1505.02913", "submitter": "Mohammad Arashi", "authors": "M. Norouzirad, M. Arashi and A.K.Md.Ehsanes Saleh", "title": "Restricted LASSO and Double Shrinking", "comments": "20 pages; 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of multiple regression model, suppose that the vector\nparameter of interest \\beta is subjected to lie in the subspace hypothesis\nH\\beta = h, where this restriction is based on either additional information or\nprior knowledge. Then, the restricted estimator performs fairly well than the\nordinary least squares one. In addition, when the number of variables is\nrelatively large with respect to observations, the use of least absolute\nshrinkage and selection operator (LASSO) estimator is suggested for variable\nselection purposes. In this paper, we deffine a restricted LASSO estimator and\nconfigure three classes of LASSO-type estimators to fulfill both variable\nselection and restricted estimation. Asymptotic performance of the proposed\nestimators are studied and a simulation is conducted to analyze asymptotic\nrelative efficiencies. The application of our result is considered for the\nprostate dataset where the expected prediction errors and risks are compared.\nIt has been shown that the proposed shrunken LASSO estimators, resulted from\ndouble shrinking methodology, perform better than the classical LASSO.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 08:55:00 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Norouzirad", "M.", ""], ["Arashi", "M.", ""], ["Saleh", "A. K. Md. Ehsanes", ""]]}, {"id": "1505.02948", "submitter": "Alexander Engstrom", "authors": "Alexander Engstrom", "title": "Expanders from Markov bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.AC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diaconis and Sturmfels introduced an influential method to construct Markov\nchains using commutative algebra. One major point of their method is that\ninfinite families of graphs are simultaneously proved to be connected by a\nsingle algebraic calculation. For large state spaces in the infinite families\nthese Markov chains are not rapidly mixing and only ad hoc methods have been\navailable to improve their mixing times. We provide a method to get rapid\nmixing by constructing expanders for the Diaconis-Sturmfels type Markov chains.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 10:28:56 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Engstrom", "Alexander", ""]]}, {"id": "1505.02965", "submitter": "Mark Ebden", "authors": "Mark Ebden", "title": "Gaussian Processes: A Quick Introduction", "comments": "13 pages: 6 for regression, 5 for classification, 2 for\n  dimensionality reduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A gentle introduction to Gaussian processes (GPs). The three parts of the\ndocument consider GPs for regression, classification, and dimensionality\nreduction.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 11:28:57 GMT"}, {"version": "v2", "created": "Sat, 29 Aug 2015 12:16:19 GMT"}], "update_date": "2015-09-01", "authors_parsed": [["Ebden", "Mark", ""]]}, {"id": "1505.03018", "submitter": "Tobias Windisch", "authors": "Tobias Windisch", "title": "Rapid mixing and Markov bases", "comments": "19 pages, 2 figures, v3: improved version, final version as in SIAM\n  Journal on Discrete Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mixing behaviour of random walks on lattice points of polytopes using\nMarkov bases is examined. It is shown that under a dilation of the underlying\npolytope, these random walks do not mix rapidly when a fixed Markov basis is\nused. We also show that this phenomenon does not disappear after adding more\nmoves to the Markov basis. Avoiding rejections by sampling applicable moves\ndoes also not lead to an asymptotic improvement. As a way out, a method of how\nto adapt Markov bases in order to achieve the fastest mixing behaviour is\nintroduced.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 14:19:21 GMT"}, {"version": "v2", "created": "Mon, 18 May 2015 20:18:03 GMT"}, {"version": "v3", "created": "Sat, 20 Aug 2016 15:37:09 GMT"}], "update_date": "2016-08-23", "authors_parsed": [["Windisch", "Tobias", ""]]}, {"id": "1505.03118", "submitter": "Richard Kennaway", "authors": "Richard Kennaway", "title": "When causation does not imply correlation: robust violations of the\n  Faithfulness axiom", "comments": "18 pages. Matlab code for running the reported simulations and\n  generating the figures is included", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate that the Faithfulness property that is assumed in much causal\nanalysis is robustly violated for a large class of systems of a type that\noccurs throughout the life and social sciences: control systems. These systems\nexhibit correlations indistinguishable from zero between variables that are\nstrongly causally connected, and can show very high correlations between\nvariables that have no direct causal connection, only a connection via causal\nlinks between uncorrelated variables. Their patterns of correlation are robust,\nin that they remain unchanged when their parameters are varied. The violation\nof Faithfulness is fundamental to what a control system does: hold some\nvariable constant despite the disturbing influences on it. No method of causal\nanalysis that requires Faithfulness is applicable to such systems.\n", "versions": [{"version": "v1", "created": "Tue, 12 May 2015 18:53:50 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Kennaway", "Richard", ""]]}, {"id": "1505.03350", "submitter": "Stefano Cabras", "authors": "Stefano Cabras, Maria Eugenia Castellanos Nueda, Erlis Ruli", "title": "Approximate Bayesian Computation by Modelling Summary Statistics in a\n  Quasi-likelihood Framework", "comments": "Published at http://dx.doi.org/10.1214/14-BA921 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 411-439", "doi": "10.1214/14-BA921", "report-no": "VTeX-BA-BA921", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) is a useful class of methods for\nBayesian inference when the likelihood function is computationally intractable.\nIn practice, the basic ABC algorithm may be inefficient in the presence of\ndiscrepancy between prior and posterior. Therefore, more elaborate methods,\nsuch as ABC with the Markov chain Monte Carlo algorithm (ABC-MCMC), should be\nused. However, the elaboration of a proposal density for MCMC is a sensitive\nissue and very difficult in the ABC setting, where the likelihood is\nintractable. We discuss an automatic proposal distribution useful for ABC-MCMC\nalgorithms. This proposal is inspired by the theory of quasi-likelihood (QL)\nfunctions and is obtained by modelling the distribution of the summary\nstatistics as a function of the parameters. Essentially, given a real-valued\nvector of summary statistics, we reparametrize the model by means of a\nregression function of the statistics on parameters, obtained by sampling from\nthe original model in a pilot-run simulation study. The QL theory is well\nestablished for a scalar parameter, and it is shown that when the conditional\nvariance of the summary statistic is assumed constant, the QL has a closed-form\nnormal density. This idea of constructing proposal distributions is extended to\nnon constant variance and to real-valued parameter vectors. The method is\nillustrated by several examples and by an application to a real problem in\npopulation genetics.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 12:11:11 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Cabras", "Stefano", ""], ["Nueda", "Maria Eugenia Castellanos", ""], ["Ruli", "Erlis", ""]]}, {"id": "1505.03451", "submitter": "Victor Blanco", "authors": "V\\'ictor Blanco, Justo Puerto and Rom\\'an Salmer\\'on", "title": "A general framework for locating hyperplanes to fitting set of points", "comments": "26 pages; 7 Figures; 12 Tables", "journal-ref": "Computers and Operations Research 95 (2018) 172-193", "doi": "10.1016/j.cor.2018.03.009", "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a family of new methods for locating/fitting hyperplanes\nwith respect to a given set of points. We introduce a general framework for a\nfamily of aggregation criteria of different distance-based errors. The most\npopular methods found in the specialized literature can be cast within this\nfamily as particular choices of the errors and the aggregation criteria.\nMathematical programming formulations for these methods are stated and some\ninteresting cases are analyzed. It is also proposed a new goodness of fitting\nindex which extends the classical coefficient of determination. A series of\nillustrative examples and extensive computational experiments implemented in R\nare provided to show the performances of some of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 13 May 2015 16:33:41 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 10:20:33 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Blanco", "V\u00edctor", ""], ["Puerto", "Justo", ""], ["Salmer\u00f3n", "Rom\u00e1n", ""]]}, {"id": "1505.03772", "submitter": "Chao Gao", "authors": "Chao Gao, Zongming Ma, Anderson Y. Zhang, Harrison H. Zhou", "title": "Achieving Optimal Misclassification Proportion in Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection is a fundamental statistical problem in network data\nanalysis. Many algorithms have been proposed to tackle this problem. Most of\nthese algorithms are not guaranteed to achieve the statistical optimality of\nthe problem, while procedures that achieve information theoretic limits for\ngeneral parameter spaces are not computationally tractable. In this paper, we\npresent a computationally feasible two-stage method that achieves optimal\nstatistical performance in misclassification proportion for stochastic block\nmodel under weak regularity conditions. Our two-stage procedure consists of a\ngeneric refinement step that can take a wide range of weakly consistent\ncommunity detection procedures as initializer, to which the refinement stage\napplies and outputs a community assignment achieving optimal misclassification\nproportion with high probability. The practical effectiveness of the new\nalgorithm is demonstrated by competitive numerical results.\n", "versions": [{"version": "v1", "created": "Thu, 14 May 2015 15:59:00 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 00:28:26 GMT"}, {"version": "v3", "created": "Thu, 21 May 2015 19:18:20 GMT"}, {"version": "v4", "created": "Fri, 29 May 2015 21:03:56 GMT"}, {"version": "v5", "created": "Sat, 3 Oct 2015 03:52:33 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Gao", "Chao", ""], ["Ma", "Zongming", ""], ["Zhang", "Anderson Y.", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1505.04195", "submitter": "Zachary Kilpatrick PhD", "authors": "Alan Veliz-Cuba, Zachary P. Kilpatrick, and Kresimir Josic", "title": "Stochastic models of evidence accumulation in changing environments", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Organisms and ecological groups accumulate evidence to make decisions.\nClassic experiments and theoretical studies have explored this process when the\ncorrect choice is fixed during each trial. However, we live in a constantly\nchanging world. What effect does such impermanence have on classical results\nabout decision making? To address this question we use sequential analysis to\nderive a tractable model of evidence accumulation when the correct option\nchanges in time. Our analysis shows that ideal observers discount prior\nevidence at a rate determined by the volatility of the environment, and the\ndynamics of evidence accumulation is governed by the information gained over an\naverage environmental epoch. A plausible neural implementation of an optimal\nobserver in a changing environment shows that, in contrast to previous models,\nneural populations representing alternate choices are coupled through\nexcitation. Our work builds a bridge between statistical decision making in\nvolatile environments and stochastic nonlinear dynamics.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 20:07:28 GMT"}, {"version": "v2", "created": "Sat, 13 Jun 2015 17:38:03 GMT"}, {"version": "v3", "created": "Tue, 8 Sep 2015 18:41:42 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2015 14:40:45 GMT"}], "update_date": "2015-10-01", "authors_parsed": [["Veliz-Cuba", "Alan", ""], ["Kilpatrick", "Zachary P.", ""], ["Josic", "Kresimir", ""]]}, {"id": "1505.04215", "submitter": "Aaditya Ramdas", "authors": "Aaditya Ramdas, Barnabas Poczos, Aarti Singh, Larry Wasserman", "title": "An Analysis of Active Learning With Uniform Feature Noise", "comments": "24 pages, 2 figures, published in the proceedings of the 17th\n  International Conference on Artificial Intelligence and Statistics (AISTATS),\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In active learning, the user sequentially chooses values for feature $X$ and\nan oracle returns the corresponding label $Y$. In this paper, we consider the\neffect of feature noise in active learning, which could arise either because\n$X$ itself is being measured, or it is corrupted in transmission to the oracle,\nor the oracle returns the label of a noisy version of the query point. In\nstatistics, feature noise is known as \"errors in variables\" and has been\nstudied extensively in non-active settings. However, the effect of feature\nnoise in active learning has not been studied before. We consider the\nwell-known Berkson errors-in-variables model with additive uniform noise of\nwidth $\\sigma$.\n  Our simple but revealing setting is that of one-dimensional binary\nclassification setting where the goal is to learn a threshold (point where the\nprobability of a $+$ label crosses half). We deal with regression functions\nthat are antisymmetric in a region of size $\\sigma$ around the threshold and\nalso satisfy Tsybakov's margin condition around the threshold. We prove minimax\nlower and upper bounds which demonstrate that when $\\sigma$ is smaller than the\nminimiax active/passive noiseless error derived in \\cite{CN07}, then noise has\nno effect on the rates and one achieves the same noiseless rates. For larger\n$\\sigma$, the \\textit{unflattening} of the regression function on convolution\nwith uniform noise, along with its local antisymmetry around the threshold,\ntogether yield a behaviour where noise \\textit{appears} to be beneficial. Our\nkey result is that active learning can buy significant improvement over a\npassive strategy even in the presence of feature noise.\n", "versions": [{"version": "v1", "created": "Fri, 15 May 2015 22:54:01 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Ramdas", "Aaditya", ""], ["Poczos", "Barnabas", ""], ["Singh", "Aarti", ""], ["Wasserman", "Larry", ""]]}, {"id": "1505.04228", "submitter": "Yun S. Song", "authors": "Jonathan Terhorst and Yun S. Song", "title": "Fundamental limits on the accuracy of demographic inference based on the\n  sample frequency spectrum", "comments": "17 pages, 1 figure", "journal-ref": "Proc. Natl. Acad. Sci. U.S.A., Vol. 112, No. 25 (2015) 7677-7682", "doi": "10.1073/pnas.1503717112", "report-no": null, "categories": "q-bio.PE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sample frequency spectrum (SFS) of DNA sequences from a collection of\nindividuals is a summary statistic which is commonly used for parametric\ninference in population genetics. Despite the popularity of SFS-based inference\nmethods, currently little is known about the information-theoretic limit on the\nestimation accuracy as a function of sample size. Here, we show that using the\nSFS to estimate the size history of a population has a minimax error of at\nleast $O(1/\\log s)$, where $s$ is the number of independent segregating sites\nused in the analysis. This rate is exponentially worse than known convergence\nrates for many classical estimation problems in statistics. Another surprising\naspect of our theoretical bound is that it does not depend on the dimension of\nthe SFS, which is related to the number of sampled individuals. This means\nthat, for a fixed number $s$ of segregating sites considered, using more\nindividuals does not help to reduce the minimax error bound. Our result\npertains to populations that have experienced a bottleneck, and we argue that\nit can be expected to apply to many populations in nature.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 01:25:19 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Terhorst", "Jonathan", ""], ["Song", "Yun S.", ""]]}, {"id": "1505.04242", "submitter": "Prithwish Bhaumik", "authors": "Prithwish Bhaumik and Subhashis Ghosal", "title": "Bayesian inference for higher order ordinary differential equation\n  models", "comments": "arXiv admin note: substantial text overlap with arXiv:1411.1166", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often the regression function appearing in fields like economics,\nengineering, biomedical sciences obeys a system of higher order ordinary\ndifferential equations (ODEs). The equations are usually not analytically\nsolvable. We are interested in inferring on the unknown parameters appearing in\nthe equations. Significant amount of work has been done on parameter estimation\nin first order ODE models. Bhaumik and Ghosal (2014a) considered a two-step\nBayesian approach by putting a finite random series prior on the regression\nfunction using B-spline basis. The posterior distribution of the parameter\nvector is induced from that of the regression function. Although this approach\nis computationally fast, the Bayes estimator is not asymptotically efficient.\nBhaumik and Ghosal (2014b) remedied this by directly considering the distance\nbetween the function in the nonparametric model and a Runge-Kutta (RK$4$)\napproximate solution of the ODE while inducing the posterior distribution on\nthe parameter. They also studied the direct Bayesian method obtained from the\napproximate likelihood obtained by the RK4 method. In this paper we extend\nthese ideas for the higher order ODE model and establish Bernstein-von Mises\ntheorems for the posterior distribution of the parameter vector for each method\nwith $n^{-1/2}$ contraction rate.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 04:21:08 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Bhaumik", "Prithwish", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1505.04243", "submitter": "Paul Grigas", "authors": "Robert M. Freund, Paul Grigas, Rahul Mazumder", "title": "A New Perspective on Boosting in Linear Regression via Subgradient\n  Optimization and Relatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze boosting algorithms in linear regression from a new\nperspective: that of modern first-order methods in convex optimization. We show\nthat classic boosting algorithms in linear regression, namely the incremental\nforward stagewise algorithm (FS$_\\varepsilon$) and least squares boosting\n(LS-Boost($\\varepsilon$)), can be viewed as subgradient descent to minimize the\nloss function defined as the maximum absolute correlation between the features\nand residuals. We also propose a modification of FS$_\\varepsilon$ that yields\nan algorithm for the Lasso, and that may be easily extended to an algorithm\nthat computes the Lasso path for different values of the regularization\nparameter. Furthermore, we show that these new algorithms for the Lasso may\nalso be interpreted as the same master algorithm (subgradient descent), applied\nto a regularized version of the maximum absolute correlation loss function. We\nderive novel, comprehensive computational guarantees for several boosting\nalgorithms in linear regression (including LS-Boost($\\varepsilon$) and\nFS$_\\varepsilon$) by using techniques of modern first-order methods in convex\noptimization. Our computational guarantees inform us about the statistical\nproperties of boosting algorithms. In particular they provide, for the first\ntime, a precise theoretical description of the amount of data-fidelity and\nregularization imparted by running a boosting algorithm with a prespecified\nlearning rate for a fixed but arbitrary number of iterations, for any dataset.\n", "versions": [{"version": "v1", "created": "Sat, 16 May 2015 04:23:08 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Freund", "Robert M.", ""], ["Grigas", "Paul", ""], ["Mazumder", "Rahul", ""]]}, {"id": "1505.04379", "submitter": "Maria D. Ruiz-Medina MDRM", "authors": "M.D. Ruiz-Medina", "title": "Functional Analysis of Variance for Hilbert-Valued Multivariate Fixed\n  Effect Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new results on Functional Analysis of Variance for fixed\neffect models with correlated Hilbert-valued Gaussian error components. The\ngeometry of the Reproducing Kernel Hilbert Space (RKHS) of the error term is\nconsidered in the computation of the total sum of squares, the residual sum of\nsquares, and the sum of squares due to the regression. Under suitable linear\ntransformation of the correlated functional data, the distributional\ncharacteristics of these statistics, their moment generating and characteristic\nfunctions, are derived. Fixed effect linear hypothesis testing is finally\nformulated in the Hilbert-valued multivariate Gaussian context considered.\n", "versions": [{"version": "v1", "created": "Sun, 17 May 2015 10:55:09 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2015 16:03:22 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Ruiz-Medina", "M. D.", ""]]}, {"id": "1505.04647", "submitter": "Yaroslav Nikitenko", "authors": "Yaroslav Nikitenko", "title": "Estimation of the directional parameter of the offset exponential and\n  normal distributions in three-dimensional space using the sample mean", "comments": "30 pages, 1 figure. Diploma thesis submitted at the Independent\n  University of Moscow. Keywords: exponential distribution, normal\n  distribution, gaussian distribution, directional statistics, angular\n  precision, direction estimator, offset distribution, 3d, three-dimensional,\n  multidimensional", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The directional precision of the sample mean estimator was calculated\nanalytically for the offset exponential and normal distributions in\nthree-dimensional space both for a finite sample and for limiting cases. It was\nshown that the spherical projection of the sample mean of the shifted\nexponential distribution has connections with modified Bessel functions and\nwith hypergeometric functions. It was shown explicitly how the distribution of\nthe sample mean of the exponential pdf converges near the mode to the normal\ndistribution. Approximation formulae for the distribution of the sample mean of\nthe shifted exponential distribution and for its directional precision and for\nthe precision of the estimation of the direction of shift of the normal\ndistribution were obtained.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2015 18:10:37 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Nikitenko", "Yaroslav", ""]]}, {"id": "1505.04778", "submitter": "Soledad Villar", "authors": "Takayuki Iguchi, Dustin G. Mixon, Jesse Peterson, Soledad Villar", "title": "On the tightness of an SDP relaxation of k-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Recently, Awasthi et al. introduced an SDP relaxation of the $k$-means\nproblem in $\\mathbb R^m$. In this work, we consider a random model for the data\npoints in which $k$ balls of unit radius are deterministically distributed\nthroughout $\\mathbb R^m$, and then in each ball, $n$ points are drawn according\nto a common rotationally invariant probability distribution. For any fixed ball\nconfiguration and probability distribution, we prove that the SDP relaxation of\nthe $k$-means problem exactly recovers these planted clusters with probability\n$1-e^{-\\Omega(n)}$ provided the distance between any two of the ball centers is\n$>2+\\epsilon$, where $\\epsilon$ is an explicit function of the configuration of\nthe ball centers, and can be arbitrarily small when $m$ is large.\n", "versions": [{"version": "v1", "created": "Mon, 18 May 2015 19:50:00 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Iguchi", "Takayuki", ""], ["Mixon", "Dustin G.", ""], ["Peterson", "Jesse", ""], ["Villar", "Soledad", ""]]}, {"id": "1505.04898", "submitter": "Florian Pein", "authors": "Florian Pein, Hannes Sieling, Axel Munk", "title": "Heterogeneous Change Point Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose HSMUCE (heterogeneous simultaneous multiscale change-point\nestimator) for the detection of multiple change-points of the signal in a\nheterogeneous gaussian regression model. A piecewise constant function is\nestimated by minimizing the number of change-points over the acceptance region\nof a multiscale test which locally adapts to changes in the variance. The\nmultiscale test is a combination of local likelihood ratio tests which are\nproperly calibrated by scale dependent critical values in order to keep a\nglobal nominal level alpha, even for finite samples. We show that HSMUCE\ncontrols the error of over- and underestimation of the number of change-points.\nTo this end, new deviation bounds for F-type statistics are derived. Moreover,\nwe obtain confidence sets for the whole signal. All results are non-asymptotic\nand uniform over a large class of heterogeneous change-point models. HSMUCE is\nfast to compute, achieves the optimal detection rate and estimates the number\nof change-points at almost optimal accuracy for vanishing signals, while still\nbeing robust. We compare HSMUCE with several state of the art methods in\nsimulations and analyse current recordings of a transmembrane protein in the\nbacterial outer membrane with pronounced heterogeneity for its states. An\nR-package is available online.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 07:56:20 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 14:19:41 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Pein", "Florian", ""], ["Sieling", "Hannes", ""], ["Munk", "Axel", ""]]}, {"id": "1505.05068", "submitter": "Patrick Rubin-Delanchy Dr", "authors": "Patrick Rubin-Delanchy and Nicholas A. Heard and Daniel John Lawson", "title": "Meta-analysis of mid-p-values: some new results based on the convex\n  order", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mid-p-value is a proposed improvement on the ordinary p-value for the\ncase where the test statistic is partially or completely discrete. In this\ncase, the ordinary p-value is conservative, meaning that its null distribution\nis larger than a uniform distribution on the unit interval, in the usual\nstochastic order. The mid-p-value is not conservative. However, its null\ndistribution is dominated by the uniform distribution in a different stochastic\norder, called the convex order. The property leads us to discover some new\nfinite-sample and asymptotic bounds on functions of mid-p-values, which can be\nused to combine results from different hypothesis tests conservatively, yet\nmore powerfully, using mid-p-values rather than p-values. Our methodology is\ndemonstrated on real data from a cyber-security application.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 16:23:50 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 13:17:10 GMT"}, {"version": "v3", "created": "Fri, 2 Sep 2016 20:40:01 GMT"}, {"version": "v4", "created": "Wed, 31 May 2017 23:49:59 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Rubin-Delanchy", "Patrick", ""], ["Heard", "Nicholas A.", ""], ["Lawson", "Daniel John", ""]]}, {"id": "1505.05114", "submitter": "Yuxin Chen", "authors": "Yuxin Chen, Emmanuel J. Candes", "title": "Solving Random Quadratic Systems of Equations Is Nearly as Easy as\n  Solving Linear Systems", "comments": "accepted to Communications on Pure and Applied Mathematics (CPAM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.NA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fundamental problem of solving quadratic systems of equations\nin $n$ variables, where $y_i = |\\langle \\boldsymbol{a}_i, \\boldsymbol{x}\n\\rangle|^2$, $i = 1, \\ldots, m$ and $\\boldsymbol{x} \\in \\mathbb{R}^n$ is\nunknown. We propose a novel method, which starting with an initial guess\ncomputed by means of a spectral method, proceeds by minimizing a nonconvex\nfunctional as in the Wirtinger flow approach. There are several key\ndistinguishing features, most notably, a distinct objective functional and\nnovel update rules, which operate in an adaptive fashion and drop terms bearing\ntoo much influence on the search direction. These careful selection rules\nprovide a tighter initial guess, better descent directions, and thus enhanced\npractical performance. On the theoretical side, we prove that for certain\nunstructured models of quadratic systems, our algorithms return the correct\nsolution in linear time, i.e. in time proportional to reading the data\n$\\{\\boldsymbol{a}_i\\}$ and $\\{y_i\\}$ as soon as the ratio $m/n$ between the\nnumber of equations and unknowns exceeds a fixed numerical constant. We extend\nthe theory to deal with noisy systems in which we only have $y_i \\approx\n|\\langle \\boldsymbol{a}_i, \\boldsymbol{x} \\rangle|^2$ and prove that our\nalgorithms achieve a statistical accuracy, which is nearly un-improvable. We\ncomplement our theoretical study with numerical examples showing that solving\nrandom quadratic systems is both computationally and statistically not much\nharder than solving linear systems of the same size---hence the title of this\npaper. For instance, we demonstrate empirically that the computational cost of\nour algorithm is about four times that of solving a least-squares problem of\nthe same size.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 18:37:07 GMT"}, {"version": "v2", "created": "Tue, 22 Mar 2016 17:05:16 GMT"}], "update_date": "2016-03-23", "authors_parsed": [["Chen", "Yuxin", ""], ["Candes", "Emmanuel J.", ""]]}, {"id": "1505.05189", "submitter": "Isabel Fraga Alves", "authors": "Jan Beirlant, Isabel Fraga Alves, Ivette Gomes", "title": "Tail fitting for truncated and non-truncated Pareto-type distributions", "comments": "arXiv admin note: text overlap with arXiv:1410.4097", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently some papers, such as Aban, Meerschaert and Panorska (2006), Nuyts\n(2010) and Clark (2013), have drawn attention to possible truncation in Pareto\ntail modelling. Sometimes natural upper bounds exist that truncate the\nprobability tail, such as the Maximum Possible Loss in insurance treaties. At\nother instances ultimately at the largest data, deviations from a Pareto tail\nbehaviour become apparent. This matter is especially important when\nextrapolation outside the sample is required. Given that in practice one does\nnot always know whether the distribution is truncated or not, we consider\nestimators for extreme quantiles both under truncated and non-truncated\nPareto-type distributions. Hereby we make use of the estimator of the tail\nindex for the truncated Pareto distribution first proposed in Aban {\\it et al.}\n(2006). We also propose a truncated Pareto QQ-plot and a formal test for\ntruncation in order to help deciding between a truncated and a non-truncated\ncase. In this way we enlarge the possibilities of extreme value modelling using\nPareto tails, offering an alternative scenario by adding a truncation point $T$\nthat is large with respect to the available data. In the mathematical modelling\nwe hence let $T \\to \\infty$ at different speeds compared to the limiting\nfraction ($k/n \\to 0$) of data used in the extreme value estimation. This work\nis motivated using practical examples from different fields of applications,\nsimulation results, and some asymptotic results.\n", "versions": [{"version": "v1", "created": "Tue, 19 May 2015 21:09:14 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Beirlant", "Jan", ""], ["Alves", "Isabel Fraga", ""], ["Gomes", "Ivette", ""]]}, {"id": "1505.05235", "submitter": "Alex Ely Kossovsky", "authors": "Alex Ely Kossovsky", "title": "Random Consolidations and Fragmentations Cycles Lead to Benford' Law", "comments": "arXiv admin note: substantial text overlap with arXiv:1410.2174", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Benford's Law predicts that the first significant digit on the leftmost side\nof numbers in real-life data is proportioned between all possible 1 to 9 digits\napproximately as in LOG(1 + 1/digit), so that low digits occur much more\nfrequently than high digits in the first place. For example, digit 1 occurs\napproximately 30.1% in the first place in random numbers, while digit 9 occurs\nonly approximately 4.6%. In this article it is shown that a process where a\nlarge enough set of identical quantities constantly alternates between\nminuscule random consolidations (summing two randomly chosen values into a\nsingular value) and tiny random fragmentations (division of one randomly chosen\nvalue into two new values) converges digit-wise to the Benford proportions\nafter sufficiently many such cycles. The statistical tendency of the system\nafter numerous cycles is to have approximately 2/3 multiplicative expressions\nwhich are conducive to Benford behavior as they tend to the Lognormal\nDistribution, and 1/3 additive expressions which are detrimental to Benford\nbehavior as they tend to the Normal Distribution, hence the process represents\nin essence a tug of war between addition and multiplication. Since the process\nencounters the so-called Achilles' heel of the Central Limit Theorem, namely\nadditions of skewed distributions with high order of magnitude, additions are\nnot very effective, and the war is decisively won by multiplication, leading to\nBenford behavior. Randomness in selecting the particular quantity to be\nfragmented, as well as randomness in selecting the two particular quantities to\nbe consolidated, is essential for convergence. Not surprisingly then,\nfragmentation itself could be performed either randomly say via a realization\nfrom the continuous Uniform on (0, 1), or deterministically via any fixed split\nratio such as say 25% - 75%, and Benford's Law emerges in either case.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 03:25:00 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 14:08:23 GMT"}, {"version": "v3", "created": "Wed, 2 Jan 2019 21:06:31 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Kossovsky", "Alex Ely", ""]]}, {"id": "1505.05257", "submitter": "Shota Katayama", "authors": "Shota Katayama and Hironori Fujisawa", "title": "Sparse and Robust Linear Regression: An Optimization Algorithm and Its\n  Statistical Properties", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies sparse linear regression analysis with outliers in the\nresponses. A parameter vector for modeling outliers is added to the standard\nlinear regression model and then the sparse estimation problem for both\ncoefficients and outliers is considered. The $\\ell_{1}$ penalty is imposed for\nthe coefficients, while various penalties including redescending type penalties\nare for the outliers. To solve the sparse estimation problem, we introduce an\noptimization algorithm. Under some conditions, we show the algorithmic and\nstatistical convergence property for the coefficients obtained by the\nalgorithm. Moreover, it is shown that the algorithm can recover the true\nsupport of the coefficients with probability going to one.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 06:46:32 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Katayama", "Shota", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "1505.05314", "submitter": "Johanna F. Ziegel", "authors": "Christof Str\\\"ahl and Johanna F. Ziegel", "title": "Cross-calibration of probabilistic forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When providing probabilistic forecasts for uncertain future events, it is\ncommon to strive for calibrated forecasts, that is, the predictive distribution\nshould be compatible with the observed outcomes. Several notions of calibration\nare available in the case of a single forecaster alongside with diagnostic\ntools and statistical tests to assess calibration in practice. Often, there is\nmore than one forecaster providing predictions, and these forecasters may use\ninformation of the others and therefore influence one another. We extend common\nnotions of calibration, where each forecaster is analysed individually, to\nnotions of cross-calibration where each forecaster is analysed with respect to\nthe other forecasters in a natural way. It is shown theoretically and in\nsimulation studies that cross-calibration is a stronger requirement on a\nforecaster than calibration. Analogously to calibration for individual\nforecasters, we provide diagnostic tools and statistical tests to assess\nforecasters in terms of cross-calibration. The methods are illustrated in\nsimulation examples and applied to probabilistic forecasts for inflation rates\nby the Bank of England.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 10:54:43 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Str\u00e4hl", "Christof", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "1505.05385", "submitter": "Muneya Matsui", "authors": "Muneya Matsui and Thomas Mikosch", "title": "The extremogram and the cross-extremogram for a bivariate GARCH(1,1)\n  process", "comments": "21 pages, 31 figures,", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive some asymptotic theory for the extremogram and\ncross-extremogram of a bivariate GARCH(1,1) process. We show that the tails of\nthe components of a bivariate GARCH(1,1) process may exhibit power law behavior\nbut, depending on the choice of the parameters, the tail indices of the\ncomponents may differ. We apply the theory to 5-minute return data of stock\nprices and foreign exchange rates. We judge the fit of a bivariate GARCH(1,1)\nmodel by considering the sample extremogram and cross-extremogram of the\nresiduals. The results are in agreement with the iid hypothesis of the\ntwo-dimensional innovations sequence. The cross-extremograms at lag zero have a\nvalue significantly distinct from zero. This fact points at some strong\nextremal dependence of the components of the innovations.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 13:49:54 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Matsui", "Muneya", ""], ["Mikosch", "Thomas", ""]]}, {"id": "1505.05461", "submitter": "Karl Rohe", "authors": "Karl Rohe", "title": "Network driven sampling; a critical threshold for design effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web crawling, snowball sampling, and respondent-driven sampling (RDS) are\nthree types of network sampling techniques used to contact individuals in\nhard-to-reach populations. This paper studies these procedures as a Markov\nprocess on the social network that is indexed by a tree. Each node in this tree\ncorresponds to an observation and each edge in the tree corresponds to a\nreferral. Indexing with a tree (instead of a chain) allows for the sampled\nunits to refer multiple future units into the sample. In survey sampling, the\ndesign effect characterizes the additional variance induced by a novel sampling\nstrategy. If the design effect is some value $DE$, then constructing an\nestimator from the novel design makes the variance of the estimator $DE$ times\ngreater than it would be under a simple random sample with the same sample size\n$n$. Under certain assumptions on the referral tree, the design effect of\nnetwork sampling has a critical threshold that is a function of the referral\nrate $m$ and the clustering structure in the social network, represented by the\nsecond eigenvalue of the Markov transition matrix, $\\lambda_2$. If $m <\n1/\\lambda_2^2$, then the design effect is finite (i.e. the standard estimator\nis $\\sqrt{n}$-consistent). However, if $m > 1/\\lambda_2^2$, then the design\neffect grows with $n$ (i.e. the standard estimator is no longer\n$\\sqrt{n}$-consistent). Past this critical threshold, the standard error of the\nestimator converges at the slower rate of $n^{\\log_m \\lambda_2}$. The Markov\nmodel allows for nodes to be resampled; computational results show that the\nfindings hold in without-replacement sampling. To estimate confidence intervals\nthat adapt to the correct level of uncertainty, a novel resampling procedure is\nproposed. Computational experiments compare this procedure to previous\ntechniques.\n", "versions": [{"version": "v1", "created": "Wed, 20 May 2015 17:36:03 GMT"}, {"version": "v2", "created": "Tue, 26 May 2015 13:01:27 GMT"}, {"version": "v3", "created": "Fri, 28 Aug 2015 22:43:28 GMT"}, {"version": "v4", "created": "Tue, 8 Sep 2015 21:53:47 GMT"}, {"version": "v5", "created": "Thu, 1 Jun 2017 11:27:37 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Rohe", "Karl", ""]]}, {"id": "1505.05654", "submitter": "Fabien Navarro", "authors": "Fabien Navarro and Adrien Saumard", "title": "Slope heuristics and V-Fold model selection in heteroscedastic\n  regression using strongly localized bases", "comments": null, "journal-ref": null, "doi": "10.1051/ps/2017005", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the optimality for model selection of the so-called slope\nheuristics, $V$-fold cross-validation and $V$-fold penalization in a\nheteroscedastic with random design regression context. We consider a new class\nof linear models that we call strongly localized bases and that generalize\nhistograms, piecewise polynomials and compactly supported wavelets. We derive\nsharp oracle inequalities that prove the asymptotic optimality of the slope\nheuristics---when the optimal penalty shape is known---and $V$ -fold\npenalization. Furthermore, $V$-fold cross-validation seems to be suboptimal for\na fixed value of $V$ since it recovers asymptotically the oracle learned from a\nsample size equal to $1-V^{-1}$ of the original amount of data. Our results are\nbased on genuine concentration inequalities for the true and empirical excess\nrisks that are of independent interest. We show in our experiments the good\nbehavior of the slope heuristics for the selection of linear wavelet models.\nFurthermore, $V$-fold cross-validation and $V$-fold penalization have\ncomparable efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 09:29:53 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 09:57:40 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 09:32:36 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Navarro", "Fabien", ""], ["Saumard", "Adrien", ""]]}, {"id": "1505.05691", "submitter": "Anirvan Chakraborty Mr.", "authors": "Anirvan Chakraborty and Probal Chaudhuri", "title": "Tests for high dimensional data based on means, spatial signs and\n  spatial ranks", "comments": "31 pages, 5 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tests based on sample mean vectors and sample spatial signs have been studied\nin the recent literature for high dimensional data with the dimension larger\nthan the sample size. For suitable sequences of alternatives, we show that the\npowers of the mean based tests and the tests based on spatial signs and ranks\ntend to be same as the data dimension grows to infinity for any sample size,\nwhen the coordinate variables satisfy appropriate mixing conditions. Further,\ntheir limiting powers do not depend on the heaviness of the tails of the\ndistributions. This is in striking contrast to the asymptotic results obtained\nin the classical multivariate setup. On the other hand, we show that in the\npresence of stronger dependence among the coordinate variables, the spatial\nsign and rank based tests for high dimensional data can be asymptotically more\npowerful than the mean based tests if in addition to the data dimension, the\nsample size also grows to infinity. The sizes of some mean based tests for high\ndimensional data studied in the recent literature are observed to be\nsignificantly different from their nominal levels. This is due to the\ninadequacy of the asymptotic approximations used for the distributions of those\ntest statistics. However, our asymptotic approximations for the tests based on\nspatial signs and ranks are observed to work well when the tests are applied on\na variety of simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 21 May 2015 12:11:39 GMT"}], "update_date": "2015-05-22", "authors_parsed": [["Chakraborty", "Anirvan", ""], ["Chaudhuri", "Probal", ""]]}, {"id": "1505.06101", "submitter": "Paul Rochet", "authors": "Flavia Barsotti, Anne Philippe (LMJL), Paul Rochet (LMJL)", "title": "Hypothesis testing for markovian models with random time observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to propose a methodology for testing general\nhypothesis in a Markovian setting with random sampling. A discrete Markov chain\nX is observed at random time intervals $\\tau$ k, assumed to be iid with unknown\ndistribution $\\mu$. Two test procedures are investigated. The first one is\ndevoted to testing if the transition matrix P of the Markov chain X satisfies\nspecific affine constraints, covering a wide range of situations such as\nsymmetry or sparsity. The second procedure is a goodness-of-fit test on the\ndistribution $\\mu$, which reveals to be consistent under mild assumptions even\nthough the time gaps are not observed. The theoretical results are supported by\na Monte Carlo simulation study to show the performance and robustness of the\nproposed methodologies on specific numerical examples.\n", "versions": [{"version": "v1", "created": "Fri, 22 May 2015 14:39:12 GMT"}], "update_date": "2015-05-25", "authors_parsed": [["Barsotti", "Flavia", "", "LMJL"], ["Philippe", "Anne", "", "LMJL"], ["Rochet", "Paul", "", "LMJL"]]}, {"id": "1505.06292", "submitter": "Or Zuk", "authors": "Avishai Wagner and Or Zuk", "title": "Low-Rank Matrix Recovery from Row-and-Column Affine Measurements", "comments": "ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and study a row-and-column affine measurement scheme for low-rank\nmatrix recovery. Each measurement is a linear combination of elements in one\nrow or one column of a matrix $X$. This setting arises naturally in\napplications from different domains. However, current algorithms developed for\nstandard matrix recovery problems do not perform well in our case, hence the\nneed for developing new algorithms and theory for our problem. We propose a\nsimple algorithm for the problem based on Singular Value Decomposition ($SVD$)\nand least-squares ($LS$), which we term \\alg. We prove that (a simplified\nversion of) our algorithm can recover $X$ exactly with the minimum possible\nnumber of measurements in the noiseless case. In the general noisy case, we\nprove performance guarantees on the reconstruction accuracy under the Frobenius\nnorm. In simulations, our row-and-column design and \\alg algorithm show\nimproved speed, and comparable and in some cases better accuracy compared to\nstandard measurements designs and algorithms. Our theoretical and experimental\nresults suggest that the proposed row-and-column affine measurements scheme,\ntogether with our recovery algorithm, may provide a powerful framework for\naffine matrix reconstruction.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 08:45:20 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Wagner", "Avishai", ""], ["Zuk", "Or", ""]]}, {"id": "1505.06298", "submitter": "Nicolas Goix", "authors": "Nicolas Goix (LTCI), Anne Sabourin (LTCI), St\\'ephan Cl\\'emen\\c{c}on\n  (LTCI)", "title": "Learning the dependence structure of rare events: a non-asymptotic study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the probability of occurrence of extreme events is a crucial issue\nin various fields like finance, insurance, telecommunication or environmental\nsciences. In a multivariate framework, the tail dependence is characterized by\nthe so-called stable tail dependence function (STDF). Learning this structure\nis the keystone of multivariate extremes. Although extensive studies have\nproved consistency and asymptotic normality for the empirical version of the\nSTDF, non-asymptotic bounds are still missing. The main purpose of this paper\nis to fill this gap. Taking advantage of adapted VC-type concentration\ninequalities, upper bounds are derived with expected rate of convergence in\nO(k^-1/2). The concentration tools involved in this analysis rely on a more\ngeneral study of maximal deviations in low probability regions, and thus\ndirectly apply to the classification of extreme data.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 10:00:33 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Goix", "Nicolas", "", "LTCI"], ["Sabourin", "Anne", "", "LTCI"], ["Cl\u00e9men\u00e7on", "St\u00e9phan", "", "LTCI"]]}, {"id": "1505.06417", "submitter": "Morteza Amini", "authors": "S. M. T. K. MirMostafaee, Morteza Amini and A. Asgharzadeh", "title": "Bayesian prediction of minimal repair times of a series system based on\n  hybrid censored sample of components' lifetimes under Rayleigh distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop Bayesian predictive inferential procedures for\nprediction of repair times of a series system, applying a minimal repair\nstrategy, using the information contained in an independent observed hybrid\ncensored sample of the lifetimes of the components of the system, assuming the\nunderlying distribution of the lifetimes to be Rayleigh distribution. An\nillustrative real data example and a simulation study are presented for the\npurpose of illustration and comparison of the proposed predictors.\n", "versions": [{"version": "v1", "created": "Sun, 24 May 2015 08:17:46 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["MirMostafaee", "S. M. T. K.", ""], ["Amini", "Morteza", ""], ["Asgharzadeh", "A.", ""]]}, {"id": "1505.06536", "submitter": "Jose Rodriguez", "authors": "Jose Israel Rodriguez and Botong Wang", "title": "The maximum likelihood degree of mixtures of independence models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.AT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum likelihood degree (ML degree) measures the algebraic complexity\nof a fundamental optimization problem in statistics: maximum likelihood\nestimation. In this problem, one maximizes the likelihood function over a\nstatistical model. The ML degree of a model is an upper bound to the number of\nlocal extrema of the likelihood function and can be expressed as a weighted sum\nof Euler characteristics. The independence model (i.e. rank one matrices over\nthe probability simplex) is well known to have an ML degree of one, meaning\ntheir is a unique local maxima of the likelihood function. However, for\nmixtures of independence models (i.e. rank two matrices over the probability\nsimplex), it was an open question as to how the ML degree behaved. In this\npaper, we use Euler characteristics to prove an outstanding conjecture by\nHauenstein, the first author, and Sturmfels; we give recursions and closed form\nexpressions for the ML degree of mixtures of independence models.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 04:06:46 GMT"}, {"version": "v2", "created": "Tue, 9 Aug 2016 05:11:43 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 05:29:41 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Rodriguez", "Jose Israel", ""], ["Wang", "Botong", ""]]}, {"id": "1505.06644", "submitter": "Marcelo Moreira", "authors": "Humberto Moreira, Marcelo J. Moreira", "title": "Optimal Two-Sided Tests for Instrumental Variables Regression with\n  Heteroskedastic and Autocorrelated Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers two-sided tests for the parameter of an endogenous\nvariable in an instrumental variable (IV) model with heteroskedastic and\nautocorrelated errors. We develop the finite-sample theory of weighted-average\npower (WAP) tests with normal errors and a known long-run variance. We\nintroduce two weights which are invariant to orthogonal transformations of the\ninstruments; e.g., changing the order in which the instruments appear. While\ntests using the MM1 weight can be severely biased, optimal tests based on the\nMM2 weight are naturally two-sided when errors are homoskedastic.\n  We propose two boundary conditions that yield two-sided tests whether errors\nare homoskedastic or not. The locally unbiased (LU) condition is related to the\npower around the null hypothesis and is a weaker requirement than unbiasedness.\nThe strongly unbiased (SU) condition is more restrictive than LU, but the\nassociated WAP tests are easier to implement. Several tests are SU in finite\nsamples or asymptotically, including tests robust to weak IV (such as the\nAnderson-Rubin, score, conditional quasi-likelihood ratio, and I. Andrews'\n(2015) PI-CLC tests) and two-sided tests which are optimal when the sample size\nis large and instruments are strong.\n  We refer to the WAP-SU tests based on our weights as MM1-SU and MM2-SU tests.\nDropping the restrictive assumptions of normality and known variance, the\ntheory is shown to remain valid at the cost of asymptotic approximations. The\nMM2-SU test is optimal under the strong IV asymptotics, and outperforms other\nexisting tests under the weak IV asymptotics.\n", "versions": [{"version": "v1", "created": "Mon, 25 May 2015 14:31:19 GMT"}], "update_date": "2015-05-26", "authors_parsed": [["Moreira", "Humberto", ""], ["Moreira", "Marcelo J.", ""]]}, {"id": "1505.06794", "submitter": "Debdeep Pati", "authors": "Debdeep Pati and Anirban Bhattacharya", "title": "Optimal Bayesian estimation in stochastic block models", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of structured data in the form of social networks, genetic\ncircuits and protein interaction networks, statistical analysis of networks has\ngained popularity over recent years. Stochastic block model constitutes a\nclassical cluster-exhibiting random graph model for networks. There is a\nsubstantial amount of literature devoted to proposing strategies for estimating\nand inferring parameters of the model, both from classical and Bayesian\nviewpoints. Unlike the classical counterpart, there is however a dearth of\ntheoretical results on the accuracy of estimation in the Bayesian setting. In\nthis article, we undertake a theoretical investigation of the posterior\ndistribution of the parameters in a stochastic block model. In particular, we\nshow that one obtains optimal rates of posterior convergence with routinely\nused multinomial-Dirichlet priors on cluster indicators and uniform priors on\nthe probabilities of the random edge indicators. En route, we develop geometric\nembedding techniques to exploit the lower dimensional structure of the\nparameter space which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 03:03:50 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Pati", "Debdeep", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "1505.06832", "submitter": "Lilia Costa", "authors": "Lilia Costa, Jim Smith, Thomas Nichols, James Cussens, Eugene P. Duff,\n  Tamar R. Makin", "title": "Searching Multiregression Dynamic Models of Resting-State fMRI Networks\n  Using Integer Programming", "comments": "Published at http://dx.doi.org/10.1214/14-BA913 in the Bayesian\n  Analysis (http://projecteuclid.org/euclid.ba) by the International Society of\n  Bayesian Analysis (http://bayesian.org/)", "journal-ref": "Bayesian Analysis 2015, Vol. 10, No. 2, 441-478", "doi": "10.1214/14-BA913", "report-no": "VTeX-BA-BA913", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Multiregression Dynamic Model (MDM) is a class of multivariate time series\nthat represents various dynamic causal processes in a graphical way. One of the\nadvantages of this class is that, in contrast to many other Dynamic Bayesian\nNetworks, the hypothesised relationships accommodate conditional conjugate\ninference. We demonstrate for the first time how straightforward it is to\nsearch over all possible connectivity networks with dynamically changing\nintensity of transmission to find the Maximum a Posteriori Probability (MAP)\nmodel within this class. This search method is made feasible by using a novel\napplication of an Integer Programming algorithm. The efficacy of applying this\nparticular class of dynamic models to this domain is shown and more\nspecifically the computational efficiency of a corresponding search of 11-node\nDirected Acyclic Graph (DAG) model space. We proceed to show how diagnostic\nmethods, analogous to those defined for static Bayesian Networks, can be used\nto suggest embellishment of the model class to extend the process of model\nselection. All methods are illustrated using simulated and real resting-state\nfunctional Magnetic Resonance Imaging (fMRI) data.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 07:34:35 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Costa", "Lilia", ""], ["Smith", "Jim", ""], ["Nichols", "Thomas", ""], ["Cussens", "James", ""], ["Duff", "Eugene P.", ""], ["Makin", "Tamar R.", ""]]}, {"id": "1505.06925", "submitter": "Chanchal Kundu", "authors": "Chanchal Kundu", "title": "On weighted measure of inaccuracy for doubly truncated random variables", "comments": "arXiv admin note: text overlap with arXiv:math/0703489 by other\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, authors have studied weighted version of Kerridge inaccuracy\nmeasure for truncated distributions. In the present communication we introduce\nthe notion of weighted interval inaccuracy measure for two-sided truncated\nrandom variables. In reliability theory and survival analysis, this measure may\nhelp to study the various characteristics of a system/component when it fails\nbetween two time points. Various aspects of weighted interval inaccuracy\nmeasure have been discussed and some characterization results have been\nprovided. This new measure is a generalization of recent dynamic weighted\ninaccuracy measure.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 12:34:34 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Kundu", "Chanchal", ""]]}, {"id": "1505.07072", "submitter": "Yves Atchade F", "authors": "Yves F. Atchad\\'e", "title": "A Moreau-Yosida approximation scheme for a class of high-dimensional\n  posterior distributions", "comments": "35 pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact-sparsity inducing prior distributions in Bayesian analysis typically\nlead to posterior distributions that are very challenging to handle by standard\nMarkov Chain Monte Carlo (MCMC) methods, particular in high-dimensional models\nwith large number of parameters. We propose a methodology to derive smooth\napproximations of such posterior distributions that are, in some cases, easier\nto handle by standard MCMC methods. The approximation is obtained from the\nforward-backward approximation of the Moreau-Yosida regularization of the\nnegative log-density. We show that the derived approximation is within\n$O(\\sqrt{\\gamma})$ of the true posterior distribution in the $\\beta$-metric,\nwhere $\\gamma>0$ is a user-controlled parameter that defines the approximation.\nWe illustrate the method with a high-dimensional linear regression model.\n", "versions": [{"version": "v1", "created": "Tue, 26 May 2015 18:34:23 GMT"}, {"version": "v2", "created": "Sun, 26 Jun 2016 12:59:35 GMT"}], "update_date": "2016-06-28", "authors_parsed": [["Atchad\u00e9", "Yves F.", ""]]}, {"id": "1505.07082", "submitter": "Christopher Hammond", "authors": "Christopher N. B. Hammond, Warren P. Johnson", "title": "Multi-Opponent James Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The James function, also known as the \"log5 method,\" assigns a probability to\nthe result of a competition between two teams based on their respective winning\npercentages. This paper, which builds on earlier work of the authors and Steven\nJ. Miller, explores the analogous situation where a single team or player\ncompetes simultaneously against multiple opponents.\n", "versions": [{"version": "v1", "created": "Sat, 23 May 2015 00:42:51 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 13:10:55 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Hammond", "Christopher N. B.", ""], ["Johnson", "Warren P.", ""]]}, {"id": "1505.07178", "submitter": "Xinghui Wang", "authors": "Xinghui Wang and Shuhe Hu", "title": "A note on the strong consistency of M-estimates in linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We improve a known result on the strong consistency of M-estimates of the\nregression parameters in a linear model for independent and identically\ndistributed random errors under some mild conditions.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 02:25:42 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Wang", "Xinghui", ""], ["Hu", "Shuhe", ""]]}, {"id": "1505.07260", "submitter": "Martin Wendler", "authors": "Olimjon Sh. Sharipov, Johannes Tewes, Martin Wendler", "title": "Bootstrap for U-Statistics: A new approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrap for nonlinear statistics like U-statistics of dependent data has\nbeen studied by several authors. This is typically done by producing a\nbootstrap version of the sample and plugging it into the statistic. We suggest\nan alternative approach of getting a bootstrap version of U-statistics, which\ncan be described as a compromise between bootstrap and subsampling. We will\nshow the consistency of the new method and compare its finite sample properties\nin a simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 10:38:48 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Sharipov", "Olimjon Sh.", ""], ["Tewes", "Johannes", ""], ["Wendler", "Martin", ""]]}, {"id": "1505.07345", "submitter": "Aim\\'e Lachal", "authors": "Sergio Alvarez-Andrade, Salim Bouzebda and Aim\\'e Lachal", "title": "Some asymptotic results for the integrated empirical process with\n  applications to statistical tests", "comments": "28 pages", "journal-ref": "Communications in Statistics - Theory and Methods 46(7) (2017),\n  3365-3392", "doi": "10.1080/03610926.2015.1060346", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this paper is to investigate the strong approximation of\nthe integrated empirical process. More precisely, we obtain the exact rate of\nthe approximations by a sequence of weighted Brownian bridges and a weighted\nKiefer process. Our arguments are based in part on the Koml\\'os, Major and\nTusn\\'ady's results. Applications include the two-sample testing procedures\ntogether with the change-point problems. We also consider the strong\napproximation of the integrated empirical process when the parameters are\nestimated. Finally, we study the behavior of the self-intersection local time\nof the partial sum process representation of the integrated empirical process.\n  Reference: Koml\\'os, J., Major, P. and Tusn\\'ady, G. (1975). An approximation\nof partial sums of independent RV's and the sample DF. I. Z.\nWahrscheinlichkeitstheorie und Verw. Gebiete, 32, 111-131.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 14:36:51 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Alvarez-Andrade", "Sergio", ""], ["Bouzebda", "Salim", ""], ["Lachal", "Aim\u00e9", ""]]}, {"id": "1505.07414", "submitter": "Lingzhou Xue", "authors": "Jianqing Fan, Lingzhou Xue and Jiawei Yao", "title": "Sufficient Forecasting Using Factor Models", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider forecasting a single time series when there is a large number of\npredictors and a possible nonlinear effect. The dimensionality was first\nreduced via a high-dimensional (approximate) factor model implemented by the\nprincipal component analysis. Using the extracted factors, we develop a novel\nforecasting method called the sufficient forecasting, which provides a set of\nsufficient predictive indices, inferred from high-dimensional predictors, to\ndeliver additional predictive power. The projected principal component analysis\nwill be employed to enhance the accuracy of inferred factors when a\nsemi-parametric (approximate) factor model is assumed. Our method is also\napplicable to cross-sectional sufficient regression using extracted factors.\nThe connection between the sufficient forecasting and the deep learning\narchitecture is explicitly stated. The sufficient forecasting correctly\nestimates projection indices of the underlying factors even in the presence of\na nonparametric forecasting function. The proposed method extends the\nsufficient dimension reduction to high-dimensional regimes by condensing the\ncross-sectional information through factor models. We derive asymptotic\nproperties for the estimate of the central subspace spanned by these projection\ndirections as well as the estimates of the sufficient predictive indices. We\nfurther show that the natural method of running multiple regression of target\non estimated factors yields a linear estimate that actually falls into this\ncentral subspace. Our method and theory allow the number of predictors to be\nlarger than the number of observations. We finally demonstrate that the\nsufficient forecasting improves upon the linear forecasting in both simulation\nstudies and an empirical study of forecasting macroeconomic variables.\n", "versions": [{"version": "v1", "created": "Wed, 27 May 2015 17:46:37 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 23:25:53 GMT"}], "update_date": "2015-12-29", "authors_parsed": [["Fan", "Jianqing", ""], ["Xue", "Lingzhou", ""], ["Yao", "Jiawei", ""]]}, {"id": "1505.07602", "submitter": "Bertrand Michel", "authors": "Fr\\'ed\\'eric Chazal, Pascal Massart and Bertrand Michel", "title": "Rates of convergence for robust geometric inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distances to compact sets are widely used in the field of Topological Data\nAnalysis for inferring geometric and topological features from point clouds. In\nthis context, the distance to a probability measure (DTM) has been introduced\nby Chazal et al. (2011) as a robust alternative to the distance a compact set.\nIn practice, the DTM can be estimated by its empirical counterpart, that is the\ndistance to the empirical measure (DTEM). In this paper we give a tight control\nof the deviation of the DTEM. Our analysis relies on a local analysis of\nempirical processes. In particular, we show that the rates of convergence of\nthe DTEM directly depends on the regularity at zero of a particular quantile\nfonction which contains some local information about the geometry of the\nsupport. This quantile function is the relevant quantity to describe precisely\nhow difficult is a geometric inference problem. Several numerical experiments\nillustrate the convergence of the DTEM and also confirm that our bounds are\ntight.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 08:57:59 GMT"}, {"version": "v2", "created": "Tue, 15 Mar 2016 20:01:43 GMT"}], "update_date": "2016-03-17", "authors_parsed": [["Chazal", "Fr\u00e9d\u00e9ric", ""], ["Massart", "Pascal", ""], ["Michel", "Bertrand", ""]]}, {"id": "1505.07607", "submitter": "Zhiqiang Tan", "authors": "Zhiqiang Tan", "title": "Improved minimax estimation of a multivariate normal mean under\n  heteroscedasticity", "comments": "Published at http://dx.doi.org/10.3150/13-BEJ580 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2015, Vol. 21, No. 1, 574-603", "doi": "10.3150/13-BEJ580", "report-no": "IMS-BEJ-BEJ580", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of estimating a multivariate normal mean with a known\nvariance matrix, which is not necessarily proportional to the identity matrix.\nThe coordinates are shrunk directly in proportion to their variances in Efron\nand Morris' (J. Amer. Statist. Assoc. 68 (1973) 117-130) empirical Bayes\napproach, whereas inversely in proportion to their variances in Berger's (Ann.\nStatist. 4 (1976) 223-226) minimax estimators. We propose a new minimax\nestimator, by approximately minimizing the Bayes risk with a normal prior among\na class of minimax estimators where the shrinkage direction is open to\nspecification and the shrinkage magnitude is determined to achieve minimaxity.\nThe proposed estimator has an interesting simple form such that one group of\ncoordinates are shrunk in the direction of Berger's estimator and the remaining\ncoordinates are shrunk in the direction of the Bayes rule. Moreover, the\nproposed estimator is scale adaptive: it can achieve close to the minimum Bayes\nrisk simultaneously over a scale class of normal priors (including the\nspecified prior) and achieve close to the minimax linear risk over a\ncorresponding scale class of hyper-rectangles. For various scenarios in our\nnumerical study, the proposed estimators with extreme priors yield more\nsubstantial risk reduction than existing minimax estimators.\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 09:16:49 GMT"}], "update_date": "2015-05-29", "authors_parsed": [["Tan", "Zhiqiang", ""]]}, {"id": "1505.07915", "submitter": "Morteza Amini", "authors": "Morteza Amini and Nader Nematollahi", "title": "Estimation of the parameter of a dynamically selected population for two\n  subclasses of the exponential family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of estimation of the parameters of a dynamically\nselected population in an infinite sequence of random variables and provide its\napplication in the statistical inference based on record values from a\nnon-stationary scheme. We develop unbiased estimation of the parameters of the\ndynamically selected population and evaluate the risk of the estimators. We\nprovide comparisons with natural estimators and obtain asymptotic results.\nFinally, we illustrate the applicability of the results using real data.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 04:00:48 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Amini", "Morteza", ""], ["Nematollahi", "Nader", ""]]}, {"id": "1505.07925", "submitter": "Yun Yang", "authors": "Yun Yang, Martin J. Wainwright, Michael I. Jordan", "title": "On the Computational Complexity of High-Dimensional Bayesian Variable\n  Selection", "comments": "42 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of Markov chain Monte Carlo (MCMC)\nmethods for high-dimensional Bayesian linear regression under sparsity\nconstraints. We first show that a Bayesian approach can achieve\nvariable-selection consistency under relatively mild conditions on the design\nmatrix. We then demonstrate that the statistical criterion of posterior\nconcentration need not imply the computational desideratum of rapid mixing of\nthe MCMC algorithm. By introducing a truncated sparsity prior for variable\nselection, we provide a set of conditions that guarantee both\nvariable-selection consistency and rapid mixing of a particular\nMetropolis-Hastings algorithm. The mixing time is linear in the number of\ncovariates up to a logarithmic factor. Our proof controls the spectral gap of\nthe Markov chain by constructing a canonical path ensemble that is inspired by\nthe steps taken by greedy algorithms for variable selection.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 05:33:22 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Yang", "Yun", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1505.07981", "submitter": "Pavel Mozgunov", "authors": "Valentin Konakov and Pavel Mozgunov", "title": "Limits of Kalman Filter application in heavy tailed problems", "comments": "23 pages, 6 figures, in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the behavior of Kalman Filter state estimates in\nthe case of distribution with heavy tails .The simulated linear state space\nmodels with Gaussian measurement noises were used. Gaussian noises in state\nequation are replaced by components with alpha-stable distribution with di\nerent parameters alpha and beta. We consider the case when \"all parameters are\nknown\" and two methods of parameters estimation are compared: the maximum\nlikelihood estimator (MLE) and the expectation- maximization algorithm (EM). It\nwas shown that in cases of large deviation from Gaussian distribution the total\nerror of states estimation rises dramatically. We conjecture that it can be\nexplained by underestimation of the state equation noises covariance matrix\nthat can be taken into account through the EM parameters estimation and ignored\nin the case of ML estimation.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 10:17:17 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2015 15:15:37 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Konakov", "Valentin", ""], ["Mozgunov", "Pavel", ""]]}, {"id": "1505.08049", "submitter": "Stephane Chretien", "authors": "St\\'ephane Chr\\'etien and Tianwen Wei", "title": "Sensing tensors with Gaussian filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse recovery from linear Gaussian measurements has been the subject of\nmuch investigation since the breaktrough papers \\cite{CRT:IEEEIT06} and\n\\cite{donoho2006compressed} on Compressed Sensing. Application to sparse\nvectors and sparse matrices via least squares penalized with sparsity promoting\nnorms is now well understood using tools such as Gaussian mean width,\nstatistical dimension and the notion of descent cones \\cite{tropp2014convex}\n\\cite{Vershynin:ArXivEstimation14}. Extention of these ideas to low rank tensor\nrecovery is starting to enjoy considerable interest due to its many potential\napplications to Independent Component Analysis, Hidden Markov Models and\nGaussian Mixture Models \\cite{AnandkumarEtAl:JMLR14}, hyperspectral image\nanalysis \\cite{zhang2008tensor}, to name a few. In this paper, we demonstrate\nthat the recent approach of \\cite{Vershynin:ArXivEstimation14} provides very\nuseful error bounds in the tensor setting using the nuclear norm or the\nRomera-Paredes--Pontil \\cite{RomeraParedesPontil:NIPS13} penalization.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 14:04:58 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Wei", "Tianwen", ""]]}, {"id": "1505.08120", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Michael Jansson and Whitney K. Newey", "title": "Alternative Asymptotics and the Partially Linear Model with Many\n  Regressors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-standard distributional approximations have received considerable\nattention in recent years. They often provide more accurate approximations in\nsmall samples, and theoretical improvements in some cases. This paper shows\nthat the seemingly unrelated \"many instruments asymptotics\" and \"small\nbandwidth asymptotics\" share a common structure, where the object determining\nthe limiting distribution is a V-statistic with a remainder that is an\nasymptotically normal degenerate U-statistic. We illustrate how this general\nstructure can be used to derive new results by obtaining a new asymptotic\ndistribution of a series estimator of the partially linear model when the\nnumber of terms in the series approximation possibly grows as fast as the\nsample size, which we call \"many terms asymptotics\".\n", "versions": [{"version": "v1", "created": "Thu, 28 May 2015 19:44:22 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Jansson", "Michael", ""], ["Newey", "Whitney K.", ""]]}, {"id": "1505.08134", "submitter": "Salvador Flores M.", "authors": "Salvador Flores", "title": "SOCP relaxation bounds for the optimal subset selection problem applied\n  to robust linear regression", "comments": "12 pages, 3 figures, 2 tables", "journal-ref": "European Journal of Operational Research 246 (2015) pp. 44-50", "doi": "10.1016/j.ejor.2015.04.024", "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of finding the globally optimal subset of h\nelements from a larger set of n elements in d space dimensions so as to\nminimize a quadratic criterion, with an special emphasis on applications to\ncomputing the Least Trimmed Squares Estimator (LTSE) for robust regression. The\ncomputation of the LTSE is a challenging subset selection problem involving a\nnonlinear program with continuous and binary variables, linked in a highly\nnonlinear fashion. The selection of a globally optimal subset using the branch\nand bound (BB) algorithm is limited to problems in very low dimension,\ntipically d<5, as the complexity of the problem increases exponentially with d.\nWe introduce a bold pruning strategy in the BB algorithm that results in a\nsignificant reduction in computing time, at the price of a negligeable accuracy\nlost. The novelty of our algorithm is that the bounds at nodes of the BB tree\ncome from pseudo-convexifications derived using a linearization technique with\napproximate bounds for the nonlinear terms. The approximate bounds are computed\nsolving an auxiliary semidefinite optimization problem. We show through a\ncomputational study that our algorithm performs well in a wide set of the most\ndifficult instances of the LTSE problem.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2015 18:24:12 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Flores", "Salvador", ""]]}]