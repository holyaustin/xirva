[{"id": "1611.00297", "submitter": "Kostas N. Oikonomou", "authors": "Kostas N. Oikonomou", "title": "Generalized Entropy Concentration for Counts", "comments": "Fixed the proof of Proposition 2.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phenomenon of entropy concentration provides strong support for the\nmaximum entropy method, MaxEnt, for inferring a probability vector from\ninformation in the form of constraints. Here we extend this phenomenon, in a\ndiscrete setting, to non-negative integral vectors not necessarily summing to\n1. We show that linear constraints that simply bound the allowable sums suffice\nfor concentration to occur even in this setting. This requires a new,\n`generalized' entropy measure in which the sum of the vector plays a role. We\nmeasure the concentration in terms of deviation from the maximum generalized\nentropy value, or in terms of the distance from the maximum generalized entropy\nvector. We provide non-asymptotic bounds on the concentration in terms of\nvarious parameters, including a tolerance on the constraints which ensures that\nthey are always satisfied by an integral vector. Generalized entropy\nmaximization is not only compatible with ordinary MaxEnt, but can also be\nconsidered an extension of it, as it allows us to address problems that cannot\nbe formulated as MaxEnt problems.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 17:06:59 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 13:56:44 GMT"}, {"version": "v3", "created": "Sat, 8 Jul 2017 12:48:00 GMT"}, {"version": "v4", "created": "Sun, 3 Dec 2017 01:28:36 GMT"}, {"version": "v5", "created": "Sun, 5 May 2019 23:12:52 GMT"}, {"version": "v6", "created": "Thu, 5 Sep 2019 15:14:58 GMT"}, {"version": "v7", "created": "Fri, 8 Jan 2021 03:36:21 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Oikonomou", "Kostas N.", ""]]}, {"id": "1611.00442", "submitter": "Angus Ian McLeod", "authors": "Esam Mahdi and A. Ian McLeod", "title": "Improved multivariate portmanteau test", "comments": "25 pages, 1 figure, 5 tables", "journal-ref": "Journal of Time Series Analysis 33/2, 211-222 (2012)", "doi": "10.1111/j.1467-9892.2011.00752.x", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new portmanteau diagnostic test for vector autoregressive moving average\n(VARMA) models that is based on the determinant of the standardized\nmultivariate residual autocorrelations is derived. The new test statistic may\nbe considered an extension of the univariate portmanteau test statistic\nsuggested by Pena and Rodriguez (2002, A Powerful Portmanteau Test of Lack of\nTest for Time Series, Journal of American Statistical Association) The\nasymptotic distribution of the test statistic is derived as well as a\nchi-square approximation. However, the Monte-Carlo test is recommended unless\nthe series is very long. Extensive simulation experiments demonstrate the\nusefulness of this test as well as its improved power performance compared to\nwidely used previous multivariate portmanteau diagnostic check. Two\nillustrative applications are given.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 01:53:20 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Mahdi", "Esam", ""], ["McLeod", "A. Ian", ""]]}, {"id": "1611.00519", "submitter": "Chong Wu", "authors": "Chong Wu, Can Yang, Hongyu Zhao and Ji Zhu", "title": "On the Convergence of the EM Algorithm: A Data-Adaptive Analysis", "comments": "55 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Expectation-Maximization (EM) algorithm is an iterative method to\nmaximize the log-likelihood function for parameter estimation. Previous works\non the convergence analysis of the EM algorithm have established results on the\nasymptotic (population level) convergence rate of the algorithm. In this paper,\nwe give a data-adaptive analysis of the sample level local convergence rate of\nthe EM algorithm. In particular, we show that the local convergence rate of the\nEM algorithm is a random variable $\\overline{K}_{n}$ derived from the data\ngenerating distribution, which adaptively yields the convergence rate of the EM\nalgorithm on each finite sample data set from the same population distribution.\nWe then give a non-asymptotic concentration bound of $\\overline{K}_{n}$ on the\npopulation level optimal convergence rate $\\overline{\\kappa}$ of the EM\nalgorithm, which implies that $\\overline{K}_{n}\\to\\overline{\\kappa}$ in\nprobability as the sample size $n\\to\\infty$. Our theory identifies the effect\nof sample size on the convergence behavior of sample EM sequence, and explains\na surprising phenomenon in applications of the EM algorithm, i.e. the finite\nsample version of the algorithm sometimes converges faster even than the\npopulation version. We apply our theory to the EM algorithm on three canonical\nmodels and obtain specific forms of the adaptive convergence theorem for each\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 09:35:30 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 14:30:30 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Wu", "Chong", ""], ["Yang", "Can", ""], ["Zhao", "Hongyu", ""], ["Zhu", "Ji", ""]]}, {"id": "1611.00630", "submitter": "Christophe Biscio", "authors": "Christophe Biscio and Jesper M{\\o}ller", "title": "The accumulated persistence function, a new useful functional summary\n  statistic for topological data analysis, with a view to brain artery trees\n  and spatial point process applications", "comments": "45 pages, 15 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We start with a simple introduction to topological data analysis where the\nmost popular tool is called a persistent diagram. Briefly, a persistent diagram\nis a multiset of points in the plane describing the persistence of topological\nfeatures of a compact set when a scale parameter varies. Since statistical\nmethods are difficult to apply directly on persistence diagrams, various\nalternative functional summary statistics have been suggested, but either they\ndo not contain the full information of the persistence diagram or they are\ntwo-dimensional functions. We suggest a new functional summary statistic that\nis one-dimensional and hence easier to handle, and which under mild conditions\ncontains the full information of the persistence diagram. Its usefulness is\nillustrated in statistical settings concerned with point clouds and brain\nartery trees. The appendix includes additional methods and examples, together\nwith technical details. The R-code used for all examples is available at\nhttp://people.math.aau.dk/~christophe/Rcode.zip.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 14:27:30 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 11:19:22 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Biscio", "Christophe", ""], ["M\u00f8ller", "Jesper", ""]]}, {"id": "1611.00819", "submitter": "Angus Ian McLeod", "authors": "Ying Zhang, H. Yu, and A. I. McLeod", "title": "Developments in Maximum Likelihood Unit Root Tests", "comments": "23 page, 4 figures, 1 table", "journal-ref": "Communications in Statistics - Simulation and Computation. 42:5,\n  1088-1103 (2013)", "doi": "10.1080/03610918.2012.655828", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exact maximum likelihood estimate (MLE) provides a test statistic for the\nunit root test that is more powerful \\citep[p. 577]{Fuller96} than the usual\nleast squares approach. In this paper a new derivation is given for the\nasymptotic distribution of this test statistic that is simpler and more direct\nthan the previous method. The response surface regression method is used to\nobtain a fast algorithm that computes accurate finite-sample critical values.\nThis algorithm is available in the R package {\\tt mleur} that is available on\nCRAN. The empirical power of the new test is shown to be much better than the\nusual test not only in the normal case but also for innovations generated from\nan infinite variance stable distribution as well as for innovations generated\nfrom a GARCH$(1,1)$ process.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 21:29:07 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Zhang", "Ying", ""], ["Yu", "H.", ""], ["McLeod", "A. I.", ""]]}, {"id": "1611.00826", "submitter": "Angus Ian McLeod", "authors": "A. Ian McLeod", "title": "Hyperbolic decay time series", "comments": "16 pages, 1 table", "journal-ref": "The Journal of Time Series Analysis 19, 473-484 (1998)", "doi": "10.1111/1467-9892.00104", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperbolic decay time series such as, fractional Gaussian noise (FGN) or\nfractional autoregressive moving-average (FARMA) process, each exhibit two\ndistinct types of behaviour: strong persistence or antipersistence. Beran\n(1994) characterized the family of strongly persistent time series. A more\ngeneral family of hyperbolic decay time series is introduced and its basic\nproperties are characterized in terms of the autocovariance and spectral\ndensity functions. The random shock and inverted form representations are\nderived. It is shown that every strongly persistent series is the dual of an\nantipersistent series and vice versa. The asymptotic generalized variance of\nhyperbolic decay time series with unit innovation variance is shown to be\ninfinite which implies that the variance of the minimum mean-square error\none-step linear predictor using the last $k$ observations decays slowly to the\ninnovation variance as $k$ gets large.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 22:03:48 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["McLeod", "A. Ian", ""]]}, {"id": "1611.00843", "submitter": "Victor Veitch", "authors": "Victor Veitch and Daniel M. Roy", "title": "Sampling and Estimation for (Sparse) Exchangeable Graphs", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI math.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse exchangeable graphs on $\\mathbb{R}_+$, and the associated graphex\nframework for sparse graphs, generalize exchangeable graphs on $\\mathbb{N}$,\nand the associated graphon framework for dense graphs. We develop the graphex\nframework as a tool for statistical network analysis by identifying the\nsampling scheme that is naturally associated with the models of the framework,\nand by introducing a general consistent estimator for the parameter (the\ngraphex) underlying these models. The sampling scheme is a modification of\nindependent vertex sampling that throws away vertices that are isolated in the\nsampled subgraph. The estimator is a dilation of the empirical graphon\nestimator, which is known to be a consistent estimator for dense exchangeable\ngraphs; both can be understood as graph analogues to the empirical distribution\nin the i.i.d. sequence setting. Our results may be viewed as a generalization\nof consistent estimation via the empirical graphon from the dense graph regime\nto also include sparse graphs.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 23:24:15 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Veitch", "Victor", ""], ["Roy", "Daniel M.", ""]]}, {"id": "1611.00868", "submitter": "Nicholas Kiefer", "authors": "Nicholas M. Kiefer", "title": "Incentive-Compatible Elicitation of Quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporation of expert information in inference or decision settings is\noften important, especially in cases where data are unavailable, costly or\nunreliable. One approach is to elicit prior quantiles from an expert and then\nto fit these to a statistical distribution and proceed according to Bayes rule.\nQuantiles are often thought to be easier to elicit than moments. An\nincentive-compatible elicitation method using an external randomization is\navailable. Such a mechanism will encourage the expert to exert the care\nnecessary to report accurate information. A second application might be called\nposterior elicitation. Here an analysis has been done and the results must be\nreported to a decision maker. For a variety of reasons (possibly including the\nreward system in the corporate hierarchy) the modeler might need the right\nincentive system to report results accurately. Again, eliciting posterior\nquantiles can be done with an incentive compatible mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 03:10:09 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Kiefer", "Nicholas M.", ""]]}, {"id": "1611.00884", "submitter": "Angus Ian McLeod", "authors": "Ian McLeod and Benoit Quenneville", "title": "Mean likelihood estimators", "comments": "29 page, 10 figures", "journal-ref": "Statistics and Computing 11, 57-65 (2001)", "doi": "10.1023/A:1026509916251", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of {\\it Mathematica} in deriving mean likelihood estimators is\ndiscussed. Comparisons between the maximum likelihood estimator, the mean\nlikelihood estimator and the Bayes estimate based on a Jeffrey's noninformative\nprior using the criteria mean-square error and Pitman measure of closeness.\nBased on these criteria we find that for the first-order moving-average time\nseries model, the mean likelihood estimator outperforms the maximum likelihood\nestimator and the Bayes estimator with a Jeffrey's noninformative prior.\n  {\\it Mathematica} was used for symbolic and numeric computations as well as\nfor the graphical display of results. A {\\it Mathematica} notebook is available\nwhich provides supplementary derivations and code from\nhttp://www.stats.uwo.ca/mcleod/epubs/mele The interested reader can easily\nreproduce or extend any of the results in this paper using this supplement.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 05:03:09 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["McLeod", "Ian", ""], ["Quenneville", "Benoit", ""]]}, {"id": "1611.00965", "submitter": "Angus Ian McLeod", "authors": "A. Ian McLeod and Ying Zhang", "title": "Faster ARMA maximum likelihood estimation", "comments": "27 pages, 3 Figures, 3 Tables", "journal-ref": "Computational Statistics & Data Analysis, 52-4, 2166-2176 (2008)", "doi": "10.1016/j.csda.2007.07.020", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new likelihood based AR approximation is given for ARMA models. The usual\nalgorithms for the computation of the likelihood of an ARMA model require\n$O(n)$ flops per function evaluation. Using our new approximation, an algorithm\nis developed which requires only $O(1)$ flops in repeated likelihood\nevaluations. In most cases, the new algorithm gives results identical to or\nvery close to the exact maximum likelihood estimate (MLE). This algorithm is\neasily implemented in high level Quantitative Programming Environments (QPEs)\nsuch as {\\it Mathematica\\/}, MatLab and R. In order to obtain reasonable speed,\nprevious ARMA maximum likelihood algorithms are usually implemented in C or\nsome other machine efficient language. With our algorithm it is easy to do\nmaximum likelihood estimation for long time series directly in the QPE of your\nchoice. The new algorithm is extended to obtain the MLE for the mean parameter.\nSimulation experiments which illustrate the effectiveness of the new algorithm\nare discussed. {\\it Mathematica\\/} and R packages which implement the algorithm\ndiscussed in this paper are available (McLeod and Zhang, 2007). Based on these\npackage implementations, it is expected that the interested researcher would be\nable to implement this algorithm in other QPE's.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 12:06:23 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["McLeod", "A. Ian", ""], ["Zhang", "Ying", ""]]}, {"id": "1611.01006", "submitter": "Mohammad Amin Rahimian", "authors": "M. Amin Rahimian and Ali Jadbabaie", "title": "Bayesian Heuristics for Group Decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.SI cs.SY math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model of inference and heuristic decision-making in groups that\nis rooted in the Bayes rule but avoids the complexities of rational inference\nin partially observed environments with incomplete information, which are\ncharacteristic of group interactions. Our model is also consistent with a\ndual-process psychological theory of thinking: the group members behave\nrationally at the initiation of their interactions with each other (the slow\nand deliberative mode); however, in the ensuing decision epochs, they rely on a\nheuristic that replicates their experiences from the first stage (the fast\nautomatic mode). We specialize this model to a group decision scenario where\nprivate observations are received at the beginning, and agents aim to take the\nbest action given the aggregate observations of all group members. We study the\nimplications of the information structure together with the properties of the\nprobability distributions which determine the structure of the so-called\n\"Bayesian heuristics\" that the agents follow in our model. We also analyze the\ngroup decision outcomes in two classes of linear action updates and log-linear\nbelief updates and show that many inefficiencies arise in group decisions as a\nresult of repeated interactions between individuals, leading to overconfident\nbeliefs as well as choice-shifts toward extremes. Nevertheless, balanced\nregular structures demonstrate a measure of efficiency in terms of aggregating\nthe initial information of individuals. These results not only verify some\nwell-known insights about group decision-making but also complement these\ninsights by revealing additional mechanistic interpretations for the group\ndeclension-process, as well as psychological and cognitive intuitions about the\ngroup interaction model.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 16:57:42 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Rahimian", "M. Amin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1611.01043", "submitter": "Fran\\c{c}ois Bachoc", "authors": "Fran\\c{c}ois Bachoc, David Preinerstorfer, Lukas Steinberger", "title": "Uniformly valid confidence intervals post-model-selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest general methods to construct asymptotically uniformly valid\nconfidence intervals post-model-selection. The constructions are based on\nprinciples recently proposed by Berk et al. (2013). In particular the candidate\nmodels used can be misspecified, the target of inference is model-specific, and\ncoverage is guaranteed for any data-driven model selection procedure. After\ndeveloping a general theory we apply our methods to practically important\nsituations where the candidate set of models, from which a working model is\nselected, consists of fixed design homoskedastic or heteroskedastic linear\nmodels, or of binary regression models with general link functions. In an\nextensive simulation study, we find that the proposed confidence intervals\nperform remarkably well, even when compared to existing methods that are\ntailored only for specific model selection procedures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 14:34:18 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 14:47:04 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 10:57:02 GMT"}, {"version": "v4", "created": "Mon, 13 Nov 2017 16:57:29 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Bachoc", "Fran\u00e7ois", ""], ["Preinerstorfer", "David", ""], ["Steinberger", "Lukas", ""]]}, {"id": "1611.01099", "submitter": "James P. Crutchfield", "authors": "Sarah E. Marzen and James P. Crutchfield", "title": "Informational and Causal Architecture of Continuous-time Renewal and\n  Hidden Semi-Markov Processes", "comments": "16 pages, 7 figures;\n  http://csc.ucdavis.edu/~cmg/compmech/pubs/ctrp.htm", "journal-ref": null, "doi": "10.1007/s10955-017-1793-z", "report-no": null, "categories": "cond-mat.stat-mech cs.IT math.IT math.ST nlin.CD stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the minimal maximally predictive models ({\\epsilon}-machines) of\nprocesses generated by certain hidden semi-Markov models. Their causal states\nare either hybrid discrete-continuous or continuous random variables and\ncausal-state transitions are described by partial differential equations.\nClosed-form expressions are given for statistical complexities, excess\nentropies, and differential information anatomy rates. We present a complete\nanalysis of the {\\epsilon}-machines of continuous-time renewal processes and,\nthen, extend this to processes generated by unifilar hidden semi-Markov models\nand semi-Markov models. Our information-theoretic analysis leads to new\nexpressions for the entropy rate and the rates of related information measures\nfor these very general continuous-time process classes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 17:24:11 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Marzen", "Sarah E.", ""], ["Crutchfield", "James P.", ""]]}, {"id": "1611.01119", "submitter": "Gogi Pantsulaia", "authors": "Levan Labadze, Gimzer Saatashvili and Gogi Pantsulaia", "title": "Infinite-sample consistent estimations of parameters of the Wiener\n  process with drift", "comments": "12 pages, 1 figures, 4 tables. arXiv admin note: substantial text\n  overlap with arXiv:1608.04507", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Wiener process with drift $$ dX_t=\\mu dt +\\sigma d W_t $$\nwith initial value problem $X_0=x_0$, where $x_0 \\in R$, $ \\mu \\in R$ and\n$\\sigma > 0$ are parameters. By use values $(z_k)_{k \\in N}$ of corresponding\ntrajectories at a fixed positive moment $t$, the infinite-sample consistent\nestimates of each unknown parameter of the Wiener process with drift are\nconstructed under assumption that all another parameters are known. Further, we\npropose a certain approach for estimation of unknown parameters\n$x_0,\\mu,\\sigma$ of the Wiener process with drift by use the values\n$(z^{(1)}_k)_{k \\in N}$ and $(z^{(2)}_k)_{k \\in N}$ being the results of\nobservations on the $2k$-th and $2k+1$-th trajectories of the Wiener process\nwith drift at moments $t_1$ and $t_2$ , respectively.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 18:20:56 GMT"}, {"version": "v2", "created": "Sat, 5 Nov 2016 08:16:34 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Labadze", "Levan", ""], ["Saatashvili", "Gimzer", ""], ["Pantsulaia", "Gogi", ""]]}, {"id": "1611.01125", "submitter": "Debdeep Pati", "authors": "Anirban Bhattacharya, Debdeep Pati and Yun Yang", "title": "Bayesian fractional posteriors", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the fractional posterior distribution that is obtained by\nupdating a prior distribution via Bayes theorem with a fractional likelihood\nfunction, a usual likelihood function raised to a fractional power. First, we\nanalyze the contraction property of the fractional posterior in a general\nmisspecified framework. Our contraction results only require a prior mass\ncondition on certain Kullback-Leibler (KL) neighborhood of the true parameter\n(or the KL divergence minimizer in the misspecified case), and obviate\nconstructions of test functions and sieves commonly used in the literature for\nanalyzing the contraction property of a regular posterior. We show through a\ncounterexample that some condition controlling the complexity of the parameter\nspace is necessary for the regular posterior to contract, rendering additional\nflexibility on the choice of the prior for the fractional posterior. Second, we\nderive a novel Bayesian oracle inequality based on a PAC-Bayes inequality in\nmisspecified models. Our derivation reveals several advantages of averaging\nbased Bayesian procedures over optimization based frequentist procedures. As an\napplication of the Bayesian oracle inequality, we derive a sharp oracle\ninequality in the convex regression problem under an arbitrary dimension. We\nalso illustrate the theory in Gaussian process regression and density\nestimation problems.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 18:43:40 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 18:32:21 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""], ["Yang", "Yun", ""]]}, {"id": "1611.01129", "submitter": "Anru Zhang", "authors": "Anru Zhang", "title": "Cross: Efficient Low-rank Tensor Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The completion of tensors, or high-order arrays, attracts significant\nattention in recent research. Current literature on tensor completion primarily\nfocuses on recovery from a set of uniformly randomly measured entries, and the\nrequired number of measurements to achieve recovery is not guaranteed to be\noptimal. In addition, the implementation of some previous methods is NP-hard.\nIn this article, we propose a framework for low-rank tensor completion via a\nnovel tensor measurement scheme we name Cross. The proposed procedure is\nefficient and easy to implement. In particular, we show that a third order\ntensor of Tucker rank-$(r_1, r_2, r_3)$ in $p_1$-by-$p_2$-by-$p_3$ dimensional\nspace can be recovered from as few as $r_1r_2r_3 + r_1(p_1-r_1) + r_2(p_2-r_2)\n+ r_3(p_3-r_3)$ noiseless measurements, which matches the sample complexity\nlower-bound. In the case of noisy measurements, we also develop a theoretical\nupper bound and the matching minimax lower bound for recovery error over\ncertain classes of low-rank tensors for the proposed procedure. The results can\nbe further extended to fourth or higher-order tensors. Simulation studies show\nthat the method performs well under a variety of settings. Finally, the\nprocedure is illustrated through a real dataset in neuroimaging.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 19:02:02 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 18:08:38 GMT"}], "update_date": "2018-11-28", "authors_parsed": [["Zhang", "Anru", ""]]}, {"id": "1611.01179", "submitter": "Wenjing Liao", "authors": "Wenjing Liao and Mauro Maggioni", "title": "Adaptive Geometric Multiscale Approximations for Intrinsically\n  Low-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of efficiently approximating and encoding\nhigh-dimensional data sampled from a probability distribution $\\rho$ in\n$\\mathbb{R}^D$, that is nearly supported on a $d$-dimensional set $\\mathcal{M}$\n- for example supported on a $d$-dimensional Riemannian manifold. Geometric\nMulti-Resolution Analysis (GMRA) provides a robust and computationally\nefficient procedure to construct low-dimensional geometric approximations of\n$\\mathcal{M}$ at varying resolutions. We introduce a thresholding algorithm on\nthe geometric wavelet coefficients, leading to what we call adaptive GMRA\napproximations. We show that these data-driven, empirical approximations\nperform well, when the threshold is chosen as a suitable universal function of\nthe number of samples $n$, on a wide variety of measures $\\rho$, that are\nallowed to exhibit different regularity at different scales and locations,\nthereby efficiently encoding data from more complex measures than those\nsupported on manifolds. These approximations yield a data-driven dictionary,\ntogether with a fast transform mapping data to coefficients, and an inverse of\nsuch a map. The algorithms for both the dictionary construction and the\ntransforms have complexity $C n \\log n$ with the constant linear in $D$ and\nexponential in $d$. Our work therefore establishes adaptive GMRA as a fast\ndictionary learning algorithm with approximation guarantees. We include several\nnumerical experiments on both synthetic and real data, confirming our\ntheoretical results and demonstrating the effectiveness of adaptive GMRA.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 20:26:08 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 12:38:50 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Liao", "Wenjing", ""], ["Maggioni", "Mauro", ""]]}, {"id": "1611.01205", "submitter": "Xuan Cao", "authors": "Xuan Cao, Kshitij Khare and Malay Ghosh", "title": "Posterior Graph Selection and Estimation Consistency for\n  High-dimensional Bayesian DAG Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance estimation and selection for high-dimensional multivariate\ndatasets is a fundamental problem in modern statistics. Gaussian directed\nacyclic graph (DAG) models are a popular class of models used for this purpose.\nGaussian DAG models introduce sparsity in the Cholesky factor of the inverse\ncovariance matrix, and the sparsity pattern in turn corresponds to specific\nconditional independence assumptions on the underlying variables. A variety of\npriors have been developed in recent years for Bayesian inference in DAG\nmodels, yet crucial convergence and sparsity selection properties for these\nmodels have not been thoroughly investigated. Most of these priors are\nadaptations or generalizations of the Wishart distribution in the DAG context.\nIn this paper, we consider a flexible and general class of these 'DAG-Wishart'\npriors with multiple shape parameters. Under mild regularity assumptions, we\nestablish strong graph selection consistency and establish posterior\nconvergence rates for estimation when the number of variables p is allowed to\ngrow at an appropriate sub-exponential rate with the sample size n.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 21:58:30 GMT"}, {"version": "v2", "created": "Wed, 11 Oct 2017 16:14:18 GMT"}], "update_date": "2017-10-12", "authors_parsed": [["Cao", "Xuan", ""], ["Khare", "Kshitij", ""], ["Ghosh", "Malay", ""]]}, {"id": "1611.01238", "submitter": "Ting Yan", "authors": "Jianwei Hu, Hong Qin, Ting Yan, Yunpeng Zhao", "title": "Corrected Bayesian information criterion for stochastic block models", "comments": "42 pages, major revision. Change the title to \"Corrected Bayesian\n  information criterion for stochastic block models\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the number of communities is one of the fundamental problems in\ncommunity detection. We re-examine the Bayesian paradigm for stochastic block\nmodels and propose a \"corrected Bayesian information criterion\",to determine\nthe number of communities and show that the proposed estimator is consistent\nunder mild conditions. The proposed criterion improves those used in Wang and\nBickel (2016) and Saldana et al. (2017) which tend to underestimate and\noverestimate the number of communities, respectively. Along the way, we\nestablish the Wilks theorem for stochastic block models. Moreover, we show\nthat, to obtain the consistency of model selection for stochastic block models,\nwe need a so-called \"consistency condition\". We also provide sufficient\nconditions for both homogenous networks and non-homogenous networks. The\nresults are further extended to degree corrected stochastic block models.\nNumerical studies demonstrate our theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 01:40:17 GMT"}, {"version": "v2", "created": "Fri, 3 Mar 2017 02:40:56 GMT"}, {"version": "v3", "created": "Tue, 7 Mar 2017 10:04:51 GMT"}, {"version": "v4", "created": "Fri, 19 May 2017 08:34:11 GMT"}, {"version": "v5", "created": "Tue, 17 Sep 2019 07:18:19 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Hu", "Jianwei", ""], ["Qin", "Hong", ""], ["Yan", "Ting", ""], ["Zhao", "Yunpeng", ""]]}, {"id": "1611.01240", "submitter": "Angus Ian McLeod", "authors": "Ying Zhang and A. Ian McLeod", "title": "Computer Algebra Derivation of the Bias of Burg Estimators", "comments": "11 pages", "journal-ref": "Journal of Time Series Analysis 27, 157-165 (2006)", "doi": "10.1111/j.1467-9892.2005.00459.x", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A symbolic method is discussed which can be used to obtain the asymptotic\nbias and variance to order $O(1/n)$ for estimators in stationary time series.\nUsing this method the bias to $O(1/n)$ of the Burg estimator in AR(1) and AR(2)\nmodels is shown to be equal to that of the least squares estimators in both the\nknown and unknown mean cases. Previous researchers have only been able to\nobtain simulation results for this bias because this problem is too intractable\nwithout using computer algebra.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 01:49:06 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Zhang", "Ying", ""], ["McLeod", "A. Ian", ""]]}, {"id": "1611.01351", "submitter": "Angus Ian McLeod", "authors": "Jen-Wen Lin and A. Ian McLeod", "title": "Improved Pena-Rodriguez Portmanteau Test", "comments": "19 pages, 2 figures, 4 tables", "journal-ref": "Computational Statistics and Data Analysis, 51/3, 1731-1738 (2006)", "doi": "10.1016/j.csda.2006.06.010", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several problems with the diagnostic check suggested by Pena and Rodriguez\n[2002. A powerful portmanteau test of lack of fit for time series. J. Amer.\nStatist. Assoc. 97, 601-610.] are noted and an improved Monte-Carlo version of\nthis test is suggested. It is shown that quite often the test statistic\nrecommended by Pena and Rodriguez [2002. A powerful portmanteau test of lack of\nfit for time series. J. Amer. Statist. Assoc. 97, 601-610.] may not exist and\ntheir asymptotic distribution of the test does not agree with the suggested\ngamma approximation very well if the number of lags used by the test is small.\nIt is shown that the convergence of this test statistic to its asymptotic\ndistribution may be quite slow when the series length is less than 1000 and so\na Monte-Carlo test is recommended. Simulation experiments suggest the\nMonte-Carlo test is usually more powerful than the test given by Pena and\nRodriguez [2002. A powerful portmanteau test of lack of fit for time series. J.\nAmer. Statist. Assoc. 97, 601-610.] and often much more powerful than the\nLjung-Box portmanteau test. Two illustrative examples of enhanced diagnostic\nchecking with the Monte-Carlo test are given.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 12:37:29 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Lin", "Jen-Wen", ""], ["McLeod", "A. Ian", ""]]}, {"id": "1611.01360", "submitter": "Angus Ian McLeod", "authors": "Jen-Wen Lin and A. Ian McLeod", "title": "Portmanteau Tests for ARMA Models with Infinite Variance", "comments": "36 pages, 3 figures, 5 tables", "journal-ref": "Journal of Time Series Analysis, 29, 600-617 (2008)", "doi": "10.1111/j.1467-9892.2007.00572.x", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive and moving-average (ARMA) models with stable Paretian errors\nis one of the most studied models for time series with infinite variance.\nEstimation methods for these models have been studied by many researchers but\nthe problem of diagnostic checking fitted models has not been addressed. In\nthis paper, we develop portmanteau tests for checking randomness of a time\nseries with infinite variance and as a diagnostic tool for checking model\nadequacy of fitted ARMA models. It is assumed that least-squares or an\nasymptotically equivalent estimation method, such as Gaussian maximum\nlikelihood in the case of AR models, is used. And it is assumed that the\ndistribution of the innovations is IID stable Paretian. It is seen via\nsimulation that the proposed portmanteau tests do not converge well to the\ncorresponding limiting distributions for practical series length so a\nMonte-Carlo test is suggested. Simulation experiments show that the proposed\ntest procedure works effectively. Two illustrative applications to actual data\nare provided to demonstrate that an incorrect conclusion may result if the\nusual portmanteau test based on the finite variance assumption is used.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 13:09:04 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Lin", "Jen-Wen", ""], ["McLeod", "A. Ian", ""]]}, {"id": "1611.01369", "submitter": "Sourabh Bhattacharya", "authors": "Noirrit K. Chandra and Sourabh Bhattacharya", "title": "Asymptotic Theory of Dependent Bayesian Multiple Testing Procedures\n  Under Possible Model Misspecification", "comments": "A significantly updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study asymptotic properties of Bayesian multiple testing procedures and\nprovide sufficient conditions for strong consistency under general dependence\nstructure. We also consider a novel Bayesian multiple testing procedure and\nassociated error measures that coherently accounts for the dependence structure\npresent in the model. We advocate posterior versions of FDR and FNR as\nappropriate error rates and show that their asymptotic convergence rates are\ndirectly associated with the Kullback-Leibler divergence from the true model.\nOur results hold even when the class of postulated models is misspecified. We\nillustrate our results in a variable selection problem with autoregressive\nresponse variables, and compare the new Bayesian procedure with some existing\nmethods through extensive simulation studies in the variable selection problem.\nSuperior performance of the new procedure compared to the others vindicate that\nproper exploitation of the dependence structure by multiple testing methods is\nindeed important. Moreover, we obtain encouraging results in a real, maize data\ncontext, where we select influential marker variables.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 13:45:35 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 17:04:52 GMT"}, {"version": "v3", "created": "Wed, 11 Apr 2018 18:08:46 GMT"}, {"version": "v4", "created": "Thu, 13 Sep 2018 15:50:56 GMT"}, {"version": "v5", "created": "Thu, 14 May 2020 16:32:00 GMT"}], "update_date": "2020-05-15", "authors_parsed": [["Chandra", "Noirrit K.", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1611.01370", "submitter": "Angus Ian McLeod", "authors": "A. Ian McLeod and Ying Zhang", "title": "Partial autocorrelation parameterization for subset autoregression", "comments": "28 pages, 3 figures, 3 tables", "journal-ref": "Journal of Time Series Analysis 27/4, 599-612 (2006)", "doi": "10.1111/j.1467-9892.2006.00481.x", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new version of the partial autocorrelation plot and a new family of subset\nautoregressive models are introduced. A comprehensive approach to model\nidentification, estimation and diagnostic checking is developed for these\nmodels. These models are better suited to efficient model building of\nhigh-order autoregressions with long time series. Several illustrative examples\nare given.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 13:45:48 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["McLeod", "A. Ian", ""], ["Zhang", "Ying", ""]]}, {"id": "1611.01373", "submitter": "Anna Heath", "authors": "Anna Heath, Ioanna Manolopoulou and Gianluca Baio", "title": "Efficient Monte Carlo Estimation of the Expected Value of Sample\n  Information using Moment Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Expected Value of Sample Information (EVSI) is used to calculate the\neconomic value of a new research strategy. While this value would be important\nto both researchers and funders, there are very few practical applications of\nthe EVSI. In the main, this is due to computational difficulties associated\nwith calculating the EVSI in practical health economic models using nested\nsimulations. We present an approximation method for the EVSI that is based on\nestimating the distribution of the posterior mean of the incremental net\nbenefit across all the possible future samples, known as the distribution of\nthe preposterior mean. Specifically, we suggest that this distribution is\nestimated using moment matching coupled with simulations that are available for\nprobabilistic sensitivity analysis, which is typically mandatory in health\neconomic evaluation. We demonstrate that this method is successful using an\nexample that has previously been applied to other EVSI approximation methods.\nWe then conclude by discussing how our method fits in with other recent\nadditions to the literature that detail approximation methods for the EVSI.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 13:53:59 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 15:42:53 GMT"}, {"version": "v3", "created": "Thu, 7 Sep 2017 15:37:04 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Heath", "Anna", ""], ["Manolopoulou", "Ioanna", ""], ["Baio", "Gianluca", ""]]}, {"id": "1611.01387", "submitter": "Angus Ian McLeod", "authors": "A. Ian McLeod", "title": "Necessary and Sufficient Condition for Nonsingular Fisher Information\n  Matrix in ARMA Models", "comments": "5 pages", "journal-ref": "The American Statistician 53 (1) June 1999", "doi": "10.1080/00031305.1999.10474433", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is demonstrated that a necessary and sufficient condition that the Fisher\ninformation matrix of an ARMA model be nonsingular is that the model not be\nredundant, that is, the autoregressive and moving-average polynomials do not\nshare common roots.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 14:14:54 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["McLeod", "A. Ian", ""]]}, {"id": "1611.01437", "submitter": "Joram Soch", "authors": "Joram Soch, Carsten Allefeld", "title": "Kullback-Leibler Divergence for the Normal-Gamma Distribution", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the Kullback-Leibler divergence for the normal-gamma distribution\nand show that it is identical to the Bayesian complexity penalty for the\nunivariate general linear model with conjugate priors. Based on this finding,\nwe provide two applications of the KL divergence, one in simulated and one in\nempirical data.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 16:16:24 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Soch", "Joram", ""], ["Allefeld", "Carsten", ""]]}, {"id": "1611.01439", "submitter": "Joram Soch", "authors": "Joram Soch, Carsten Allefeld", "title": "Exceedance Probabilities for the Dirichlet Distribution", "comments": "10 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an efficient method to calculate exceedance probabilities (EP) for\nthe Dirichlet distribution when the number of event types is larger than two.\nAlso, we present an intuitive application of Dirichlet EPs and compare our\nmethod to a sampling approach which is the current practice in neuroimaging\nmodel selection.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 16:20:10 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Soch", "Joram", ""], ["Allefeld", "Carsten", ""]]}, {"id": "1611.01506", "submitter": "Eni Musta", "authors": "Hendrik P. Lopuha\\\"a and Eni Musta", "title": "Isotonized smooth estimators of a monotone baseline hazard in the Cox\n  model", "comments": "arXiv admin note: text overlap with arXiv:1609.06617", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two isotonic smooth estimators for a monotone baseline hazard in\nthe Cox model, a maximum smooth likelihood estimator and a Grenander-type\nestimator based on the smoothed Breslow estimator for the cumulative baseline\nhazard. We show that they are both asymptotically normal at rate\n$n^{m/(2m+1)}$, where $m\\geq 2$ denotes the level of smoothness considered, and\nwe relate their limit behavior to kernel smoothed isotonic estimators studied\nin Lopuha\\\"a and Musta (2016). It turns out that the Grenander-type estimator\nis asymptotically equivalent to the kernel smoothed isotonic estimators, while\nthe maximum smoothed likelihood estimator exhibits the same asymptotic variance\nbut a different bias. Finally, we present numerical results on pointwise\nconfidence intervals that illustrate the comparable behavior of the two\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 19:42:48 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 13:03:55 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Lopuha\u00e4", "Hendrik P.", ""], ["Musta", "Eni", ""]]}, {"id": "1611.01535", "submitter": "Angus Ian McLeod", "authors": "A. Ian McLeod", "title": "Parsimony, model adequacy and periodic correlation in forecasting time\n  series", "comments": "19 pages, 2 tables", "journal-ref": "International Statistical Review 61/3, 387-393 (1993)", "doi": "10.2307/1403750", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The merits of the modelling philosophy of Box \\& Jenkins (1970) are\nillustrated with a summary of our recent work on seasonal river flow\nforecasting. Specifically, this work demonstrates that the principle of\nparsimony, which has been questioned by several authors recently, is helpful in\nselecting the best model for forecasting seasonal river flow. Our work also\ndemonstrates the importance of model adequacy. An adequate model for seasonal\nriver flow must incorporate seasonal periodic correlation. The usual\nautoregressive-moving average (ARMA) and seasonal ARMA models are not adequate\nin this respect for seasonal river flow time series. A new diagnostic check,\nfor detecting periodic correlation in fitted ARMA models is developed in this\npaper. This diagnostic check is recommended for routine use when fitting\nseasonal ARMA models. It is shown that this diagnostic check indicates that\nmany seasonal economic time series also exhibit periodic correlation. Since the\nstandard forecasting methods are inadequate on this account, it can be\nconcluded that in many cases, the forecasts produced are sub-optimal. Finally,\na limitation of the arbitrary combination of forecasts is also illustrated.\nCombining forecasts from an adequate parsimonious model with an inadequate\nmodel did not improve the forecasts whereas combining the two forecasts of two\ninadequate models did yield an improvement in forecasting performance. These\nfindings also support the model building philosophy of Box \\& Jenkins. The\nnon-intuitive findings of Newbold \\& Granger (1974) and Winkler \\& Makridakis\n(1983) that the apparent arbitrary combination of forecasts from similar models\nwill lead to forecasting performance is not supported by our case study with\nriver flow forecasting.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 21:00:48 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["McLeod", "A. Ian", ""]]}, {"id": "1611.01564", "submitter": "Angus Ian McLeod", "authors": "A. Ian McLeod", "title": "Improved spread-location visualization", "comments": "19 pages, 2 figures, 1 table", "journal-ref": "Journal of Graphical and Computational Statistics 8/1, 135-141\n  (1999)", "doi": "10.1080/10618600.1999.10474806", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread-location plot has often been used as a diagnostic plot suitable\nfor many types of fitted statistical models. The spread-location plot which\nplots the absolute residual or square-root absolute residual versus fitted\nvalue along with a robust loess smooth is a useful replacement for the\ncustomary practice of plotting residuals versus fitted values. In this note, we\nshow that neither absolute residual or square-root absolute residual is always\nappropriate for error distributions likely to be encountered in actual\napplications. Hence we recommend a multipanel display showing a suitable\ntransformation of the absolute residual versus fitted value along with a\nboxplot to judge the symmetry achieved by the transformation. We conclude with\nan illustrative example.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 23:04:52 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["McLeod", "A. Ian", ""]]}, {"id": "1611.01595", "submitter": "Angus Ian McLeod", "authors": "A. Ian McLeod and Evelyn R. Vingilis", "title": "Power Computations for Intervention Analysis", "comments": "28 pages, 3 figures, 8 tables", "journal-ref": "Technometrics 47/2, 174-180 (2005)", "doi": "10.1198/004017005000000094", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many intervention analysis applications time series data may be expensive\nor otherwise difficult to collect. In this case the power function is helpful\nsince it can be used to determine the probability that a proposed intervention\nanalysis application will detect a meaningful change. Assuming that an\nunderlying ARIMA or fractional ARIMA model is known or can be estimated from\nthe pre-intervention time series, the methodology for computing the required\npower function is developed for pulse, step and ramp interventions with ARIMA\nand fractional ARIMA errors. Convenient formulae for computing the power\nfunction for important special cases are given. Illustrative applications in\ntraffic safety and environmental impact assessment are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 03:42:59 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["McLeod", "A. Ian", ""], ["Vingilis", "Evelyn R.", ""]]}, {"id": "1611.01958", "submitter": "Nestor Parolya Jun.-Prof. Dr.", "authors": "Taras Bodnar, Yarema Okhrin and Nestor Parolya", "title": "Optimal shrinkage-based portfolio selection in high dimensions", "comments": "34 pages, UPDATE2: fully revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST q-fin.PM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we estimate the mean-variance (MV) portfolio in the\nhigh-dimensional case using the recent results from the theory of random\nmatrices. We construct a linear shrinkage estimator which is distribution-free\nand is optimal in the sense of maximizing with probability $1$ the asymptotic\nout-of-sample expected utility, i.e., mean-variance objective function for\nseveral values of risk aversion coefficient which in particular leads to the\nmaximization of the out-of sample expected utility, to the maximization of the\nout-of-sample Sharpe ratio, and to the minimization of the out-of-sample\nvariance. Its asymptotic properties are investigated when the number of assets\n$p$ together with the sample size $n$ tend to infinity such that $p/n\n\\rightarrow c\\in (0,+\\infty)$. The results are obtained under weak assumptions\nimposed on the distribution of the asset returns, namely the existence of the\nfourth moments is only required. Thereafter we perform numerical and empirical\nstudies where the small- and large-sample behavior of the derived estimator is\ninvestigated. The suggested estimator shows significant improvements over the\nnaive diversification and it is robust to the deviations from normality.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 09:42:11 GMT"}, {"version": "v2", "created": "Thu, 16 Feb 2017 20:39:55 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 09:07:56 GMT"}, {"version": "v4", "created": "Sat, 14 Jul 2018 11:40:51 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Bodnar", "Taras", ""], ["Okhrin", "Yarema", ""], ["Parolya", "Nestor", ""]]}, {"id": "1611.02304", "submitter": "Mevlana Gemici", "authors": "Mevlana C. Gemici, Danilo Rezende, Shakir Mohamed", "title": "Normalizing Flows on Riemannian Manifolds", "comments": "3 pages, 2 figures, Submitted to Workshop on Bayesian Deep Learning\n  at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of density estimation on Riemannian manifolds.\nDensity estimation on manifolds has many applications in fluid-mechanics,\noptics and plasma physics and it appears often when dealing with angular\nvariables (such as used in protein folding, robot limbs, gene-expression) and\nin general directional statistics. In spite of the multitude of algorithms\navailable for density estimation in the Euclidean spaces $\\mathbf{R}^n$ that\nscale to large n (e.g. normalizing flows, kernel methods and variational\napproximations), most of these methods are not immediately suitable for density\nestimation in more general Riemannian manifolds. We revisit techniques related\nto homeomorphisms from differential geometry for projecting densities to\nsub-manifolds and use it to generalize the idea of normalizing flows to more\ngeneral Riemannian manifolds. The resulting algorithm is scalable, simple to\nimplement and suitable for use with automatic differentiation. We demonstrate\nconcrete examples of this method on the n-sphere $\\mathbf{S}^n$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 21:21:34 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 07:16:26 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Gemici", "Mevlana C.", ""], ["Rezende", "Danilo", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1611.02315", "submitter": "Jacob Steinhardt", "authors": "Moses Charikar and Jacob Steinhardt and Gregory Valiant", "title": "Learning from Untrusted Data", "comments": "Updated based on STOC camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.CR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of theoretical results in machine learning and statistics\nassume that the available training data is a reasonably reliable reflection of\nthe phenomena to be learned or estimated. Similarly, the majority of machine\nlearning and statistical techniques used in practice are brittle to the\npresence of large amounts of biased or malicious data. In this work we consider\ntwo frameworks in which to study estimation, learning, and optimization in the\npresence of significant fractions of arbitrary data.\n  The first framework, list-decodable learning, asks whether it is possible to\nreturn a list of answers, with the guarantee that at least one of them is\naccurate. For example, given a dataset of $n$ points for which an unknown\nsubset of $\\alpha n$ points are drawn from a distribution of interest, and no\nassumptions are made about the remaining $(1-\\alpha)n$ points, is it possible\nto return a list of $\\operatorname{poly}(1/\\alpha)$ answers, one of which is\ncorrect? The second framework, which we term the semi-verified learning model,\nconsiders the extent to which a small dataset of trusted data (drawn from the\ndistribution in question) can be leveraged to enable the accurate extraction of\ninformation from a much larger but untrusted dataset (of which only an\n$\\alpha$-fraction is drawn from the distribution).\n  We show strong positive results in both settings, and provide an algorithm\nfor robust learning in a very general stochastic optimization setting. This\ngeneral result has immediate implications for robust estimation in a number of\nsettings, including for robustly estimating the mean of distributions with\nbounded second moments, robustly learning mixtures of such distributions, and\nrobustly finding planted partitions in random graphs in which significant\nportions of the graph have been perturbed by an adversary.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 21:43:39 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 17:48:31 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Charikar", "Moses", ""], ["Steinhardt", "Jacob", ""], ["Valiant", "Gregory", ""]]}, {"id": "1611.02335", "submitter": "Tamara Fernandez", "authors": "Tamara Fern\\'andez and Yee Whye Teh", "title": "Posterior Consistency for a Non-parametric Survival Model under a\n  Gaussian Process Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove almost surely consistency of a Survival Analysis\nmodel, which puts a Gaussian process, mapped to the unit interval, as a prior\non the so-called hazard function. We assume our data is given by survival\nlifetimes $T$ belonging to $\\mathbb{R}^{+}$, and covariates on $[0,1]^d$, where\n$d$ is an arbitrary dimension. We define an appropriate metric for survival\nfunctions and prove posterior consistency with respect to this metric. Our\nproof is based on an extension of the theorem of Schwartz (1965), which gives\ngeneral conditions for proving almost surely consistency in the setting of non\ni.i.d random variables. Due to the nature of our data, several results for\nGaussian processes on $\\mathbb{R}^+$ are proved which may be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 22:58:54 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Fern\u00e1ndez", "Tamara", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1611.02571", "submitter": "Alexander D\\\"urre", "authors": "Alexander D\\\"urre and Roland Fried", "title": "Robust change-point detection in panel data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In panel data we observe a usually high number N of individuals over a time\nperiod T. Even if T is large one often assumes stability of the model over\ntime. We propose a nonparametric and robust test for a change in location and\nderive its asymptotic distribution under short range dependence and for N, T\ntending to infinity. Some simulations show its usefulness under heavy tailed\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 15:55:56 GMT"}, {"version": "v2", "created": "Tue, 21 Mar 2017 14:55:06 GMT"}], "update_date": "2017-03-22", "authors_parsed": [["D\u00fcrre", "Alexander", ""], ["Fried", "Roland", ""]]}, {"id": "1611.02675", "submitter": "Rashad Eletreby", "authors": "Rashad Eletreby and Osman Ya\\u{g}an", "title": "$k$-connectivity of inhomogeneous random key graphs with unreliable\n  links", "comments": "Submitted to IEEE Transactions on Mobile Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.NI math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider secure and reliable connectivity in wireless sensor networks that\nutilize a heterogeneous random key predistribution scheme. We model the\nunreliability of wireless links by an on-off channel model that induces an\nErd\\H{o}s-R\\'enyi graph, while the heterogeneous scheme induces an\ninhomogeneous random key graph. The overall network can thus be modeled by the\nintersection of both graphs. We present conditions (in the form of zero-one\nlaws) on how to scale the parameters of the intersection model so that with\nhigh probability i) all of its nodes are connected to at least $k$ other nodes;\ni.e., the minimum node degree of the graph is no less than $k$ and ii) the\ngraph is $k$-connected, i.e., the graph remains connected even if any $k-1$\nnodes leave the network. We also present numerical results to support these\nconditions in the finite-node regime. Our results are shown to complement and\ngeneralize several previous work in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 20:27:19 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Eletreby", "Rashad", ""], ["Ya\u011fan", "Osman", ""]]}, {"id": "1611.02686", "submitter": "Mayya Zhilova", "authors": "Mayya Zhilova", "title": "Nonclassical Berry-Esseen inequalities and accuracy of the bootstrap", "comments": null, "journal-ref": "Annals of Statistics 2020, Vol. 48, No. 4, 1922-1939", "doi": "10.1214/18-AOS1802", "report-no": "IMS-AOS-AOS1802", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study accuracy of bootstrap procedures for estimation of quantiles of a\nsmooth function of a sum of independent sub-Gaussian random vectors. We\nestablish higher-order approximation bounds with error terms depending on a\nsample size and a dimension explicitly. These results lead to improvements of\naccuracy of a weighted bootstrap procedure for general log-likelihood ratio\nstatistics. The key element of our proofs of the bootstrap accuracy is a\nmultivariate higher-order Berry-Esseen inequality. We consider a problem of\napproximation of distributions of two sums of zero mean independent random\nvectors, such that summands with the same indices have equal moments up to at\nleast the second order. The derived approximation bound is uniform on the sets\nof all Euclidean balls. The presented approach extends classical Berry-Esseen\ntype inequalities to higher-order approximation bounds. The theoretical results\nare illustrated with numerical experiments.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 20:51:43 GMT"}, {"version": "v2", "created": "Mon, 1 Jun 2020 16:11:16 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Zhilova", "Mayya", ""]]}, {"id": "1611.02762", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen", "title": "Generalized Cluster Trees and Singular Measures", "comments": "51 pages, 6 figures; accepted to the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the $\\alpha$-cluster tree ($\\alpha$-tree) under both\nsingular and nonsingular measures. The $\\alpha$-tree uses probability contents\nwithin a level set to construct a cluster tree so that it is well-defined for\nsingular measures. We first derive the convergence rate for a density level set\naround critical points, which leads to the convergence rate for estimating an\n$\\alpha$-tree under nonsingular measures. For singular measures, we study how\nthe kernel density estimator (KDE) behaves and prove that the KDE is not\nuniformly consistent but pointwisely consistent after rescaling. We further\nprove that the estimated $\\alpha$-tree fails to converge in the $L_\\infty$\nmetric but is still consistent under the integrated distance. We also observe a\nnew type of critical points--the dimensional critical points (DCPs)--of a\nsingular measure. DCPs occur only at singular measures, and similar to the\nusual critical points, DCPs contribute to cluster tree topology as well.\nBuilding on the analysis of the KDE and DCPs, we prove the topological\nconsistency of an estimated $\\alpha$-tree.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 23:08:02 GMT"}, {"version": "v2", "created": "Fri, 16 Feb 2018 18:55:50 GMT"}, {"version": "v3", "created": "Tue, 10 Jul 2018 18:11:18 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Chen", "Yen-Chi", ""]]}, {"id": "1611.02851", "submitter": "Jorge Clarke de La Cerda", "authors": "Jorge Clarke de La Cerda (CEREMADE), Alfredo Alegr\\'ia, Emilio Porcu,\n  Jorge De La Cerda", "title": "Regularity properties and simulations of Gaussian random fields on the\n  Sphere cross Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the regularity properties of Gaussian fields defined over spheres\ncross time. In particular, we consider two alternative spectral decompositions\nfor a Gaussian field on $\\mathbb{S}^d \\times \\mathbb{R}$. For each\ndecomposition, we establish regularity properties through Sobolev and\ninterpolation spaces. We then propose a simulation method and study its level\nof accuracy in the $L^2$ sense. The method turns to be both fast and efficient.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 08:44:22 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 08:15:05 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["de La Cerda", "Jorge Clarke", "", "CEREMADE"], ["Alegr\u00eda", "Alfredo", ""], ["Porcu", "Emilio", ""], ["De La Cerda", "Jorge", ""]]}, {"id": "1611.02874", "submitter": "Alexey Miroshnikov", "authors": "Alexey Miroshnikov, Evgeny Savelev", "title": "Asymptotic properties of parallel Bayesian kernel density estimators", "comments": "31 pages, 8 pictures, submitted", "journal-ref": "Annals of the Institute of Statistical Mathematics, volume 71,\n  pages 771-810 (2019)", "doi": "10.1007/s10463-018-0662-0", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we perform an asymptotic analysis of Bayesian parallel kernel\ndensity estimators introduced by Neiswanger, Wang and Xing (2014). We derive\nthe asymptotic expansion of the mean integrated squared error for the full data\nposterior estimator and investigate the properties of asymptotically optimal\nbandwidth parameters. Our analysis demonstrates that partitioning data into\nsubsets requires a non-trivial choice of bandwidth parameters that optimizes\nthe estimation error.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 10:10:03 GMT"}, {"version": "v2", "created": "Sun, 26 Mar 2017 06:05:24 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Miroshnikov", "Alexey", ""], ["Savelev", "Evgeny", ""]]}, {"id": "1611.02910", "submitter": "Anna Bonnet", "authors": "Anna Bonnet", "title": "Heritability estimation of diseases in case-control studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of genetics, the concept of heritability refers to the\nproportion of variations of a biological trait or disease that can be explained\nby genetic factors. Quantifying the heritability of a disease is a fundamental\nchallenge in human genetics, especially when the causes are plural and not\nclearly identified. Although the literature regarding heritability estimation\nfor binary traits is less rich than for quantitative traits, several methods\nhave been proposed to estimate the heritability of complex diseases. However,\nto the best of our knowledge, the existing methods are not supported by\ntheoretical grounds. Moreover, most of the methodologies do not take into\naccount a major specificity of the data coming from medical studies, which is\nthe oversampling of the number of patients compared to controls. We propose in\nthis paper to investigate the theoretical properties of the method developed by\nGolan et al. (2014), which is very efficient in practice, despite the\noversampling of patients. Our main result is the proof of the consistency of\nthis estimator. We also provide a numerical study to compare two approximations\nleading to two heritability estimators.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 13:04:46 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Bonnet", "Anna", ""]]}, {"id": "1611.03015", "submitter": "Andrii Babii", "authors": "Andrii Babii", "title": "Honest Confidence Sets in Nonparametric IV Regression and Other\n  Ill-Posed Models", "comments": null, "journal-ref": "Econometric Theory , 36(4), 2020, pp. 658-706", "doi": "10.1017/S0266466619000380", "report-no": null, "categories": "math.ST econ.EM stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops inferential methods for a very general class of ill-posed\nmodels in econometrics encompassing the nonparametric instrumental variable\nregression, various functional regressions, and the density deconvolution. We\nfocus on uniform confidence sets for the parameter of interest estimated with\nTikhonov regularization, as in Darolles, Fan, Florens, and Renault (2011).\nSince it is impossible to have inferential methods based on the central limit\ntheorem, we develop two alternative approaches relying on the concentration\ninequality and bootstrap approximations. We show that expected diameters and\ncoverage properties of resulting sets have uniform validity over a large class\nof models, i.e., constructed confidence sets are honest. Monte Carlo\nexperiments illustrate that introduced confidence sets have reasonable width\nand coverage properties. Using U.S. data, we provide uniform confidence sets\nfor Engel curves for various commodities.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 17:10:31 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 22:50:56 GMT"}, {"version": "v3", "created": "Sun, 13 Oct 2019 03:28:41 GMT"}, {"version": "v4", "created": "Sat, 19 Dec 2020 19:59:59 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Babii", "Andrii", ""]]}, {"id": "1611.03042", "submitter": "Nestor Parolya Jun.-Prof. Dr.", "authors": "Taras Bodnar, Stepan Mazur, Stanislas Muhinyuza and Nestor Parolya", "title": "On the product of a singular Wishart matrix and a singular Gaussian\n  vector in high dimension", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the product of a singular Wishart random matrix and\na singular normal random vector. A very useful stochastic representation is\nderived for this product, using which the characteristic function of the\nproduct and its asymptotic distribution under the double asymptotic regime are\nestablished. The application of obtained stochastic representation speeds up\nthe simulation studies where the product of a singular Wishart random matrix\nand a singular normal random vector is present. We further document a good\nperformance of the derived asymptotic distribution within a numerical\nillustration. Finally, several important properties of the singular Wishart\ndistribution are provided.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 18:36:20 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Bodnar", "Taras", ""], ["Mazur", "Stepan", ""], ["Muhinyuza", "Stanislas", ""], ["Parolya", "Nestor", ""]]}, {"id": "1611.03063", "submitter": "Gang Li", "authors": "Gang Li and Xiaoyan Wang", "title": "Prediction Accuracy Measures for a Nonlinear Model and for\n  Right-Censored Time-to-Event Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies prediction summary measures for a prediction function\nunder a general setting in which the model is allowed to be misspecified and\nthe prediction function is not required to be the conditional mean response. We\nshow that the R2 measure based on a variance decomposition is insufficient to\nsummarize the predictive power of a nonlinear prediction function. By deriving\na prediction error decompo- sition, we introduce an additional measure, L2, to\naugment the R2 measure. When used together, the two measures provide a complete\nsummary of the predictive power of a prediction function. Furthermore, we\nextend these measures to right-censored time-to-event data by establishing\nright-censored data analogs of the variance and prediction error\ndecompositions. We illustrate the usefulness of the proposed mea- sures with\nsimulations and real data examples. Supplementary materials for this article\nare available online.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 20:01:06 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Li", "Gang", ""], ["Wang", "Xiaoyan", ""]]}, {"id": "1611.03146", "submitter": "Wenge Guo", "authors": "Gavin Lynch, Wenge Guo, Sanat K. Sarkar, Helmut Finner", "title": "The Control of the False Discovery Rate in Fixed Sequence Multiple\n  Testing", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling the false discovery rate (FDR) is a powerful approach to multiple\ntesting. In many applications, the tested hypotheses have an inherent\nhierarchical structure. In this paper, we focus on the fixed sequence structure\nwhere the testing order of the hypotheses has been strictly specified in\nadvance. We are motivated to study such a structure, since it is the most basic\nof hierarchical structures, yet it is often seen in real applications such as\nstatistical process control and streaming data analysis. We first consider a\nconventional fixed sequence method that stops testing once an acceptance\noccurs, and develop such a method controlling the FDR under both arbitrary and\nnegative dependencies. The method under arbitrary dependency is shown to be\nunimprovable without losing control of the FDR and unlike existing FDR methods;\nit cannot be improved even by restricting to the usual positive regression\ndependence on subset (PRDS) condition. To account for any potential mistakes in\nthe ordering of the tests, we extend the conventional fixed sequence method to\none that allows more but a given number of acceptances. Simulation studies show\nthat the proposed procedures can be powerful alternatives to existing FDR\ncontrolling procedures. The proposed procedures are illustrated through a real\ndata set from a microarray experiment.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 01:17:23 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Lynch", "Gavin", ""], ["Guo", "Wenge", ""], ["Sarkar", "Sanat K.", ""], ["Finner", "Helmut", ""]]}, {"id": "1611.03328", "submitter": "Mohammad Amin Rahimian", "authors": "M. Amin Rahimian and Ali Jadbabaie", "title": "Distributed Estimation and Learning over Heterogeneous Networks", "comments": "In Proceedings of the 53rd Annual Allerton Conference on\n  Communication, Control, and Computing, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI cs.SY math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider several estimation and learning problems that networked agents\nface when making decisions given their uncertainty about an unknown variable.\nOur methods are designed to efficiently deal with heterogeneity in both size\nand quality of the observed data, as well as heterogeneity over time\n(intermittence). The goal of the studied aggregation schemes is to efficiently\ncombine the observed data that is spread over time and across several network\nnodes, accounting for all the network heterogeneities. Moreover, we require no\nform of coordination beyond the local neighborhood of every network agent or\nsensor node. The three problems that we consider are (i) maximum likelihood\nestimation of the unknown given initial data sets, (ii) learning the true model\nparameter from streams of data that the agents receive intermittently over\ntime, and (iii) minimum variance estimation of a complete sufficient statistic\nfrom several data points that the networked agents collect over time. In each\ncase we rely on an aggregation scheme to combine the observations of all\nagents; moreover, when the agents receive streams of data over time, we modify\nthe update rules to accommodate the most recent observations. In every case, we\ndemonstrate the efficiency of our algorithms by proving convergence to the\nglobally efficient estimators given the observations of all agents. We\nsupplement these results by investigating the rate of convergence and providing\nfinite-time performance guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 15:05:34 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Rahimian", "M. Amin", ""], ["Jadbabaie", "Ali", ""]]}, {"id": "1611.03473", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Daniel M. Kane, Alistair Stewart", "title": "Statistical Query Lower Bounds for Robust Estimation of High-dimensional\n  Gaussians and Gaussian Mixtures", "comments": "Changes from v1: Revised presentation. Added more applications of the\n  technique (SQ lower bounds for robust sparse mean estimation and robust\n  covariance estimation in spectral norm). Sharpened testing lower bound to\n  linear in the dimension (compared to nearly-linear in first version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CC cs.DS cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general technique that yields the first {\\em Statistical Query\nlower bounds} for a range of fundamental high-dimensional learning problems\ninvolving Gaussian distributions. Our main results are for the problems of (1)\nlearning Gaussian mixture models (GMMs), and (2) robust (agnostic) learning of\na single unknown Gaussian distribution. For each of these problems, we show a\n{\\em super-polynomial gap} between the (information-theoretic) sample\ncomplexity and the computational complexity of {\\em any} Statistical Query\nalgorithm for the problem. Our SQ lower bound for Problem (1) is qualitatively\nmatched by known learning algorithms for GMMs. Our lower bound for Problem (2)\nimplies that the accuracy of the robust learning algorithm\nin~\\cite{DiakonikolasKKLMS16} is essentially best possible among all\npolynomial-time SQ algorithms.\n  Our SQ lower bounds are attained via a unified moment-matching technique that\nis useful in other contexts and may be of broader interest. Our technique\nyields nearly-tight lower bounds for a number of related unsupervised\nestimation problems. Specifically, for the problems of (3) robust covariance\nestimation in spectral norm, and (4) robust sparse mean estimation, we\nestablish a quadratic {\\em statistical--computational tradeoff} for SQ\nalgorithms, matching known upper bounds. Finally, our technique can be used to\nobtain tight sample complexity lower bounds for high-dimensional {\\em testing}\nproblems. Specifically, for the classical problem of robustly {\\em testing} an\nunknown mean (known covariance) Gaussian, our technique implies an\ninformation-theoretic sample lower bound that scales {\\em linearly} in the\ndimension. Our sample lower bound matches the sample complexity of the\ncorresponding robust {\\em learning} problem and separates the sample complexity\nof robust testing from standard (non-robust) testing.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 20:32:48 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 15:48:34 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1611.03579", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas, Themis Gouleakis, John Peebles, Eric Price", "title": "Collision-based Testers are Optimal for Uniformity and Closeness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problems of (i) uniformity testing of a discrete\ndistribution, and (ii) closeness testing between two discrete distributions\nwith bounded $\\ell_2$-norm. These problems have been extensively studied in\ndistribution testing and sample-optimal estimators are known for\nthem~\\cite{Paninski:08, CDVV14, VV14, DKN:15}.\n  In this work, we show that the original collision-based testers proposed for\nthese problems ~\\cite{GRdist:00, BFR+:00} are sample-optimal, up to constant\nfactors. Previous analyses showed sample complexity upper bounds for these\ntesters that are optimal as a function of the domain size $n$, but suboptimal\nby polynomial factors in the error parameter $\\epsilon$. Our main contribution\nis a new tight analysis establishing that these collision-based testers are\ninformation-theoretically optimal, up to constant factors, both in the\ndependence on $n$ and in the dependence on $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 03:59:24 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Gouleakis", "Themis", ""], ["Peebles", "John", ""], ["Price", "Eric", ""]]}, {"id": "1611.03974", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen, Geoffrey J. McLachlan", "title": "On approximations via convolution-defined mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An often-cited fact regarding mixing or mixture distributions is that their\ndensity functions are able to approximate the density function of any unknown\ndistribution to arbitrary degrees of accuracy, provided that the mixing or\nmixture distribution is sufficiently complex. This fact is often not made\nconcrete. We investigate and review theorems that provide approximation bounds\nfor mixing distributions. Connections between the approximation bounds of\nmixing distributions and estimation bounds for the maximum likelihood estimator\nof finite mixtures of location- scale distributions are reviewed.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 09:13:12 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 22:29:54 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 04:53:04 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Nguyen", "Hien D.", ""], ["McLachlan", "Geoffrey J.", ""]]}, {"id": "1611.04248", "submitter": "Jianfei Shen", "authors": "Jianfei Shen and Tianxiao Pang", "title": "Asymptotic Inference for AR(1) Penal Data", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general asymptotic theory is given for the panel data AR(1) model with time\nseries independent in different cross sections. The theory covers the cases of\nstationary process, nearly non-stationary process, unit root process, mildly\nintegrated, mildly explosive and explosive processes. It is assumed that the\ncross-sectional dimension and time-series dimension are respectively $N$ and\n$T$. The results in this paper illustrate that whichever the process is, with\nan appropriate regularization, the least squares estimator of the\nautoregressive coefficient converges to a normal distribution with rate at\nleast $O(N^{-1/3})$. Since the variance is the key to characterize the normal\ndistribution, it is important to discuss the variance of the least squares\nestimator. We will show that when the autoregressive coefficient $\\rho$\nsatisfies $|\\rho|<1$, the variance declines at the rate $O((NT)^{-1/2})$, while\nthe rate changes to $O(N^{-1/2}T^{-1})$ when $\\rho=1$ and\n$O(N^{-1/2}\\rho^{-T+2})$ when $|\\rho|>1$. $\\rho=1$ is the critical point where\nthe convergence rate changes radically. The transition process is studied by\nassuming $\\rho$ depending on $T$ and going to $1$. An interesting phenomenon\ndiscovered in this paper is that, in the explosive case, the least squares\nestimator of the autoregressive coefficient has a standard normal limiting\ndistribution in panel data case while it may not has a limiting distribution in\nunivariate time series case.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 04:26:56 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Shen", "Jianfei", ""], ["Pang", "Tianxiao", ""]]}, {"id": "1611.04323", "submitter": "Paula Gordaliza Pastor", "authors": "Eustasio Del Barrio, Paula Gordaliza, H\\'el\\`ene Lescornel (IMT),\n  Jean-Michel Loubes (IMT)", "title": "Central limit theorem and bootstrap procedure for Wasserstein's\n  variations with an application to structural relationships between\n  distributions", "comments": "arXiv admin note: text overlap with arXiv:1508.06465", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein barycenters and variance-like criteria based on the Wasserstein\ndistance are used in many problems to analyze the homogeneity of collections of\ndistributions and structural relationships between the observations. We propose\nthe estimation of the quantiles of the empirical process of Wasserstein's\nvariation using a bootstrap procedure. We then use these results for\nstatistical inference on a distribution registration model for general\ndeformation functions. The tests are based on the variance of the distributions\nwith respect to their Wasserstein's barycenters for which we prove central\nlimit theorems, including bootstrap versions.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 10:26:22 GMT"}, {"version": "v2", "created": "Thu, 16 Nov 2017 13:31:55 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 08:03:47 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Del Barrio", "Eustasio", "", "IMT"], ["Gordaliza", "Paula", "", "IMT"], ["Lescornel", "H\u00e9l\u00e8ne", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"]]}, {"id": "1611.04365", "submitter": "Christophe Culan M.", "authors": "Christophe Culan, Claude Adnet", "title": "Maximum likelihood estimation of covariances of elliptically symmetric\n  distributions", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elliptically symmetric distributions are widely used in portfolio modeling,\nas well as in signal processing applications for modeling impulsive background\nnoises. Of particular interest are algorithms for covariance estimation and\nsubspace detection in such backgrounds.\n  This article tackles the issue of correctly estimating the covariance matrix\nassociated to such models and detecting additional signal superimposed on such\ndistributions. A particular attention is given to the proper accounting of the\ncircular symmetry for the subclass of complex elliptical distributions in the\ncase of complex signals.\n  In particular Tyler's estimator is shown to be a maximum likelihood estimate\nover all elliptical models, and its extension to the complex case is shown to\nbe a maximum likelihood estimate for the subclass of complex elliptical models\n(CES); other M-estimators are also shown to be maximum likelihood estimates\nover some restricted classes of elliptical models. The extension of Tyler's and\nother M-estimators to constrained covariance estimation is also discussed, in\nparticular for toeplitz constrains.\n  Finally likelihood ratio signal detection tests associated to the various\nestimators introduced in this article are also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 12:51:24 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 17:01:00 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Culan", "Christophe", ""], ["Adnet", "Claude", ""]]}, {"id": "1611.04460", "submitter": "Tobias Kley", "authors": "Tobias Kley, Philip Preu{\\ss}, Piotr Fryzlewicz", "title": "Predictive, finite-sample model choice for time series under\n  stationarity and non-stationarity", "comments": "paper (42 pages, 9 figures, 7 tables), appendix (22 pages, 4\n  figures), and supplementary material (82 pages, 15 figures, 61 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical research there usually exists a choice between structurally\nsimpler or more complex models. We argue that, even if a more complex, locally\nstationary time series model were true, then a simple, stationary time series\nmodel may be advantageous to work with under parameter uncertainty. We present\na new model choice methodology, where one of two competing approaches is chosen\nbased on its empirical, finite-sample performance with respect to prediction,\nin a manner that ensures interpretability. A rigorous, theoretical analysis of\nthe procedure is provided. As an important side result we prove, for possibly\ndiverging model order, that the localised Yule-Walker estimator is strongly,\nuniformly consistent under local stationarity. An R package, forecastSNSTS, is\nprovided and used to apply the methodology to financial and meteorological data\nin empirical examples. We further provide an extensive simulation study and\ndiscuss when it is preferable to base forecasts on the more volatile\ntime-varying estimates and when it is advantageous to forecast as if the data\nwere from a stationary process, even though they might not be.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 16:54:27 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2019 19:46:42 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2019 11:11:18 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Kley", "Tobias", ""], ["Preu\u00df", "Philip", ""], ["Fryzlewicz", "Piotr", ""]]}, {"id": "1611.04505", "submitter": "Asad Lodhia", "authors": "Afonso S. Bandeira, Asad Lodhia, and Philippe Rigollet", "title": "Mar\\v{c}enko-Pastur Law for Kendall's Tau", "comments": "Fixed errors and typos in the section 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that Kendall's Rank correlation matrix converges to the\nMar\\v{c}enko-Pastur law, under the assumption that the observations are i.i.d\nrandom vectors $X_1$, $\\dots$, $X_n$ with components that are independent and\nabsolutely continuous with respect to the Lebesgue measure. This is the first\nresult on the empirical spectral distribution of a multivariate $U$-statistic.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 18:21:54 GMT"}, {"version": "v2", "created": "Sat, 21 Jan 2017 16:15:24 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Bandeira", "Afonso S.", ""], ["Lodhia", "Asad", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1611.04513", "submitter": "Aim\\'e Lachal", "authors": "Sergio Alvarez-Andrade, Salim Bouzebda, Aim\\'e Lachal", "title": "Strong approximations for the $p$-fold integrated empirical process with\n  applications to statistical tests", "comments": "48 pages. This second version is a considerable extension of the\n  first one. In particular, two new sections have been added (Sections 3 and 8)\n  together with the corresponding proofs (Sections 9.9 and 9.10) and the\n  corresponding references. An additional appendix (Appendix B) has been added\n  too. arXiv admin note: substantial text overlap with arXiv:1505.07345", "journal-ref": "TEST 27 (2018), no. 4, 826--849", "doi": "10.1007/s11749-017-0572-0", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this paper is to investigate the strong approximation of\nthe $p$-fold integrated empirical process, $p$ being a fixed positive integer.\nMore precisely, we obtain the exact rate of the approximations by a sequence of\nweighted Brownian bridges and a weighted Kiefer process. Our arguments are\nbased in part on results of Koml\\'os, Major and Tusn\\'ady (1975). We also\nobtain an exponential bound for the tail probability of the weighted\napproximation to the $p$-fold integrated empirical process. Applications\ninclude the two-sample testing procedures together with the change-point\nproblems. We also consider the strong approximation of integrated empirical\nprocesses when the parameters are estimated. We study the behavior of the\nself-intersection local time of the partial sum process representation of\nintegrated empirical processes. Finally, simulation results are provided to\nillustrate the finite sample performance of the proposed statistical tests\nbased on the integrated empirical processes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 06:54:50 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 12:50:55 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Alvarez-Andrade", "Sergio", ""], ["Bouzebda", "Salim", ""], ["Lachal", "Aim\u00e9", ""]]}, {"id": "1611.04537", "submitter": "Frank Werner", "authors": "Katharina Proksch, Frank Werner, Axel Munk", "title": "Multiscale scanning in inverse problems", "comments": "55 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA math.OC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a multiscale scanning method to determine active\ncomponents of a quantity $f$ w.r.t. a dictionary $\\mathcal{U}$ from\nobservations $Y$ in an inverse regression model $Y=Tf+\\xi$ with linear operator\n$T$ and general random error $\\xi$. To this end, we provide uniform confidence\nstatements for the coefficients $\\langle \\varphi, f\\rangle$, $\\varphi \\in\n\\mathcal U$, under the assumption that $(T^*)^{-1} \\left(\\mathcal U\\right)$ is\nof wavelet-type. Based on this we obtain a multiple test that allows to\nidentify the active components of $\\mathcal{U}$, i.e. $\\left\\langle f,\n\\varphi\\right\\rangle \\neq 0$, $\\varphi \\in \\mathcal U$, at controlled,\nfamily-wise error rate. Our results rely on a Gaussian approximation of the\nunderlying multiscale statistic with a novel scale penalty adapted to the\nill-posedness of the problem. The scale penalty furthermore ensures weak\nconvergence of the statistic's distribution towards a Gumbel limit under\nreasonable assumptions. The important special cases of tomography and\ndeconvolution are discussed in detail. Further, the regression case, when $T =\n\\text{id}$ and the dictionary consists of moving windows of various sizes\n(scales), is included, generalizing previous results for this setting. We show\nthat our method obeys an oracle optimality, i.e. it attains the same asymptotic\npower as a single-scale testing procedure at the correct scale. Simulations\nsupport our theory and we illustrate the potential of the method as an\ninferential tool for imaging. As a particular application we discuss\nsuper-resolution microscopy and analyze experimental STED data to locate single\nDNA origami.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 19:26:22 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 15:03:32 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Proksch", "Katharina", ""], ["Werner", "Frank", ""], ["Munk", "Axel", ""]]}, {"id": "1611.04538", "submitter": "Li Ma", "authors": "Li Ma", "title": "Recursive partitioning and multi-scale modeling on conditional densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a nonparametric prior on the conditional distribution of a\n(univariate or multivariate) response given a set of predictors. The prior is\nconstructed in the form of a two-stage generative procedure, which in the first\nstage recursively partitions the predictor space, and then in the second stage\ngenerates the conditional distribution by a multi-scale nonparametric density\nmodel on each predictor partition block generated in the first stage. This\ndesign allows adaptive smoothing on both the predictor space and the response\nspace, and it results in the full posterior conjugacy of the model, allowing\nexact Bayesian inference to be completed analytically through a\nforward-backward recursive algorithm without the need of MCMC, and thus\nenjoying high computational efficiency (scaling linearly with the sample size).\nWe show that this prior enjoys desirable theoretical properties such as full\n$L_1$ support and posterior consistency. We illustrate how to apply the model\nto a variety of inference problems such as conditional density estimation as\nwell as hypothesis testing and model selection in a manner similar to applying\na parametric conjugate prior, while attaining full nonparametricity. Also\nprovided is a comparison to two other state-of-the-art Bayesian nonparametric\nmodels for conditional densities in both model fit and computational time. A\nreal data example from flow cytometry containing 455,472 observations is given\nto illustrate the substantial computational efficiency of our method and its\napplication to multivariate problems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 19:28:25 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 23:08:54 GMT"}, {"version": "v3", "created": "Tue, 14 Mar 2017 18:38:41 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Ma", "Li", ""]]}, {"id": "1611.04701", "submitter": "Shuheng Zhou", "authors": "Mark Rudelson and Shuheng Zhou", "title": "Errors-in-variables models with dependent measurements", "comments": "8 Figures, one table, 99 pages. arXiv admin note: text overlap with\n  arXiv:1502.02355", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose that we observe $y \\in \\mathbb{R}^n$ and $X \\in \\mathbb{R}^{n \\times\nm}$ in the following errors-in-variables model: \\begin{eqnarray*} y & = & X_0\n\\beta^* +\\epsilon \\\\ X & = & X_0 + W, \\end{eqnarray*} where $X_0$ is an $n\n\\times m$ design matrix with independent subgaussian row vectors, $\\epsilon \\in\n\\mathbb{R}^n$ is a noise vector and $W$ is a mean zero $n \\times m$ random\nnoise matrix with independent subgaussian column vectors, independent of $X_0$\nand $\\epsilon$. This model is significantly different from those analyzed in\nthe literature in the sense that we allow the measurement error for each\ncovariate to be a dependent vector across its $n$ observations. Such error\nstructures appear in the science literature when modeling the trial-to-trial\nfluctuations in response strength shared across a set of neurons.\n  Under sparsity and restrictive eigenvalue type of conditions, we show that\none is able to recover a sparse vector $\\beta^* \\in \\mathbb{R}^m$ from the\nmodel given a single observation matrix $X$ and the response vector $y$. We\nestablish consistency in estimating $\\beta^*$ and obtain the rates of\nconvergence in the $\\ell_q$ norm, where $q = 1, 2$. We show error bounds which\napproach that of the regular Lasso and the Dantzig selector in case the errors\nin $W$ are tending to 0. We analyze the convergence rates of the gradient\ndescent methods for solving the nonconvex programs and show that the composite\ngradient descent algorithm is guaranteed to converge at a geometric rate to a\nneighborhood of the global minimizers: the size of the neighborhood is bounded\nby the statistical error in the $\\ell_2$ norm. Our analysis reveals interesting\nconnections between computational and statistical efficiency and the\nconcentration of measure phenomenon in random matrix theory. We provide\nsimulation evidence illuminating the theoretical predictions.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 04:12:08 GMT"}, {"version": "v2", "created": "Sat, 1 Apr 2017 03:25:39 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Rudelson", "Mark", ""], ["Zhou", "Shuheng", ""]]}, {"id": "1611.04765", "submitter": "Valeria Bignozzi", "authors": "Valeria Bignozzi, Claudio Macci and Lea Petrella", "title": "Large deviations for method-of-quantiles estimators of one-dimensional\n  parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider method-of-quantiles estimators of unknown parameters, namely the\nanalogue of method-of-moments estimators obtained by matching empirical and\ntheoretical quantiles at some probability level lambda in (0,1). The aim is to\npresent large deviation results for these estimators as the sample size tends\nto infinity. We study in detail several examples; for specific models we\ndiscuss the choice of the optimal value of lambda and we compare the\nconvergence of the method-of-quantiles and method-of-moments estimators.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 09:58:20 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 09:58:39 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 16:33:18 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Bignozzi", "Valeria", ""], ["Macci", "Claudio", ""], ["Petrella", "Lea", ""]]}, {"id": "1611.04907", "submitter": "Angus Ian McLeod", "authors": "Ying Zhang and A. Ian McLeod", "title": "Fitting MA(q) Models in the Closed Invertible Region", "comments": "8 pages, 1 table", "journal-ref": "Statistics and Probablity Letters, 76, 1331-1334", "doi": "10.1016/j.spl.2006.01.010", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of reparameterization in the maximization of the likelihood function\nof the MA(q) model is discussed. A general method for testing for the presence\nof a parameter estimate on the boundary of an MA(q) model is presented. This\ntest is illustrated with a brief simulation experiment for the MA(q) for\nq=1,2,3,4 in which it is shown that the probability of an estimate being on the\nboundary increases with q.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 16:05:31 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Zhang", "Ying", ""], ["McLeod", "A. Ian", ""]]}, {"id": "1611.04908", "submitter": "Klaus Nordhausen", "authors": "Klaus Nordhausen, Hannu Oja and David E. Tyler", "title": "Asymptotic and bootstrap tests for subspace dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most linear dimension reduction methods proposed in the literature can be\nformulated using an appropriate pair of scatter matrices, see e.g. Ye and Weiss\n(2003), Tyler et al. (2009), Bura and Yang (2011), Liski et al. (2014) and Luo\nand Li (2016). The eigen-decomposition of one scatter matrix with respect to\nanother is then often used to determine the dimension of the signal subspace\nand to separate signal and noise parts of the data. Three popular dimension\nreduction methods, namely principal component analysis (PCA), fourth order\nblind identification (FOBI) and sliced inverse regression (SIR) are considered\nin detail and the first two moments of subsets of the eigenvalues are used to\ntest for the dimension of the signal space. The limiting null distributions of\nthe test statistics are discussed and novel bootstrap strategies are suggested\nfor the small sample cases. In all three cases, consistent test-based estimates\nof the signal subspace dimension are introduced as well. The asymptotic and\nbootstrap tests are compared in simulations and illustrated in real data\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 16:06:08 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 20:12:49 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Nordhausen", "Klaus", ""], ["Oja", "Hannu", ""], ["Tyler", "David E.", ""]]}, {"id": "1611.05147", "submitter": "Abdelhakim Necir", "authors": "Nawel Haouas, Abdelhakim Necir, Djamel Meraghni, Brahim Brahimi", "title": "A Lynden-Bell integral estimator for the tail index of right-truncated\n  data with a random threshold", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By means of a Lynden-Bell integral with deterministic threshold, Worms and\nWorms [A Lynden-Bell integral estimator for extremes of randomly truncated\ndata. Statist. Probab. Lett. 2016; 109: 106-117] recently introduced an\nasymptotically normal estimator of the tail index for randomly right-truncated\nPareto-type data. In this context, we consider the random threshold case to\nderive a Hill-type estimator and establish its consistency and asymptotic\nnormality. A simulation study is carried out to evaluate the finite sample\nbehavior of the proposed estimator.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 05:11:50 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 10:35:27 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Haouas", "Nawel", ""], ["Necir", "Abdelhakim", ""], ["Meraghni", "Djamel", ""], ["Brahimi", "Brahim", ""]]}, {"id": "1611.05201", "submitter": "Konstantin Eckle", "authors": "Konstantin Eckle, Nicolai Bissantz, Holger Dette", "title": "Multiscale inference for multivariate deconvolution", "comments": "Keywords and Phrases: deconvolution, modes, multivariate density,\n  multiple tests, Gaussian approximation AMS Subject Classification: 62G07,\n  62G10, 62G20", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide new methodology for inference of the geometric\nfeatures of a multivariate density in deconvolution. Our approach is based on\nmultiscale tests to detect significant directional derivatives of the unknown\ndensity at arbitrary points in arbitrary directions. The multiscale method is\nused to identify regions of monotonicity and to construct a general procedure\nfor the detection of modes of the multivariate density. Moreover, as an\nimportant application a significance test for the presence of a local maximum\nat a pre-specified point is proposed. The performance of the new methods is\ninvestigated from a theoretical point of view and the finite sample properties\nare illustrated by means of a small simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 10:10:47 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Eckle", "Konstantin", ""], ["Bissantz", "Nicolai", ""], ["Dette", "Holger", ""]]}, {"id": "1611.05224", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh and Ayanendranath Basu", "title": "A New Family of Divergences Originating from Model Adequacy Tests and\n  Application to Robust Statistical Inference", "comments": "17 pages, ato appear in IEEE transactions on Information Theory", "journal-ref": null, "doi": "10.1109/TIT.2018.2794537", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum divergence methods are popular tools in a variety of statistical\napplications. We consider tubular model adequacy tests, and demonstrate that\nthe new divergences that are generated in the process are very useful in robust\nstatistical inference. In particular we show that the family of $S$-divergences\ncan be alternatively developed using the tubular model adequacy tests; a\nfurther application of the paradigm generates a larger superfamily of\ndivergences. We describe the properties of this larger class and its potential\napplications in robust inference. Along the way, the failure of the first order\ninfluence function analysis in capturing the robustness of these procedures is\nalso established.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 11:19:39 GMT"}, {"version": "v2", "created": "Sat, 21 Jan 2017 08:58:34 GMT"}, {"version": "v3", "created": "Mon, 15 Jan 2018 14:14:05 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1611.05302", "submitter": "Zeynep Baskurt", "authors": "Zeynep Baskurt and Lisa Strug", "title": "A composite likelihood ratio approach to the analysis of correlated\n  binary data in genetic association studies", "comments": "49 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood function represents statistical evidence in the context of\ndata and a probability model. Considerable theory has demonstrated that\nevidence strength for different parameter values can be interpreted from the\nratio of likelihoods at different points on the likelihood curve. The\nlikelihood function can, however, be unknown or difficult to compute; e.g. for\ngenetic association studies with a binary outcome in large multi-generational\nfamilies. Composite likelihood is a convenient alternative to using the real\nlikelihood and here we show composite likelihoods have valid evidential\ninterpretation. We show that composite likelihoods, with a robust adjustment,\nhave two large sample performance properties that enable reliable evaluation of\nrelative evidence for different values on the likelihood curve: (1) The\ncomposite likelihood function will support the true value over the false value\nby an arbitrarily large factor; and (2) the probability of favouring a false\nvalue over a true value with high probability is small and bounded. Using an\nextensive simulation study, and in a genetic association analysis of reading\ndisability in large complex pedigrees, we show that the composite approach\nyields valid statistical inference. Results are compared to analyses using\ngeneralized estimating equations and show similar inference is obtained,\nalthough the composite approach results in a full likelihood solution that\nprovides additional complementary information.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 15:00:16 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Baskurt", "Zeynep", ""], ["Strug", "Lisa", ""]]}, {"id": "1611.05401", "submitter": "Larry Wasserman", "authors": "Alessandro Rinaldo and Larry Wasserman and Max G'Sell and Jing Lei", "title": "Bootstrapping and Sample Splitting For High-Dimensional, Assumption-Free\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several new methods have been proposed for performing valid inference after\nmodel selection. An older method is sampling splitting: use part of the data\nfor model selection and part for inference. In this paper we revisit sample\nsplitting combined with the bootstrap (or the Normal approximation). We show\nthat this leads to a simple, assumption-free approach to inference and we\nestablish results on the accuracy of the method. In fact, we find new bounds on\nthe accuracy of the bootstrap and the Normal approximation for general\nnonlinear parameters with increasing dimension which we then use to assess the\naccuracy of regression inference. We show that an alternative, called the image\nbootstrap, has higher coverage accuracy at the cost of more computation. We\ndefine new parameters that measure variable importance and that can be inferred\nwith greater accuracy than the usual regression coefficients. There is a\ninference-prediction tradeoff: splitting increases the accuracy and robustness\nof inference but can decrease the accuracy of the predictions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 18:34:47 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 19:01:47 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Rinaldo", "Alessandro", ""], ["Wasserman", "Larry", ""], ["G'Sell", "Max", ""], ["Lei", "Jing", ""]]}, {"id": "1611.05407", "submitter": "David Choi", "authors": "David Choi", "title": "A Semidefinite Program for Structured Blockmodels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semidefinite programs have recently been developed for the problem of\ncommunity detection, which may be viewed as a special case of the stochastic\nblockmodel. Here, we develop a semidefinite program that can be tailored to\nother instances of the blockmodel, such as non-assortative networks and\noverlapping communities. We establish label recovery in sparse settings, with\nconditions that are analogous to recent results for community detection. In\nsettings where the data is not generated by a blockmodel, we give an oracle\ninequality that bounds excess risk relative to the best blockmodel\napproximation. Simulations are presented for community detection, for\noverlapping communities, and for latent space models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 19:00:47 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Choi", "David", ""]]}, {"id": "1611.05410", "submitter": "Lev B Klebanov", "authors": "Lev B. Klebanov", "title": "Big Outliers Versus Heavy Tails: what to use?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A possibility to give strong mathematical definitions of outliers and heavy\ntailed distributions or their modification is discussed. Some alternatives for\nthe notion of tail index are proposed. Key words: outliers, heavy tails, tail\nindex.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 19:14:44 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Klebanov", "Lev B.", ""]]}, {"id": "1611.05475", "submitter": "Daniel  Sanz-Alonso", "authors": "Nicolas Garcia Trillos, Daniel Sanz-Alonso", "title": "The Bayesian Formulation and Well-Posedness of Fractional Elliptic\n  Inverse Problems", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/aa711e", "report-no": null, "categories": "math.AP math.PR math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the inverse problem of recovering the order and the diffusion\ncoefficient of an elliptic fractional partial differential equation from a\nfinite number of noisy observations of the solution. We work in a Bayesian\nframework and show conditions under which the posterior distribution is given\nby a change of measure from the prior. Moreover, we show well-posedness of the\ninverse problem, in the sense that small perturbations of the observed solution\nlead to small Hellinger perturbations of the associated posterior measures. We\nthus provide a mathematical foundation to the Bayesian learning of the order\n---and other inputs--- of fractional models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 21:49:21 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Trillos", "Nicolas Garcia", ""], ["Sanz-Alonso", "Daniel", ""]]}, {"id": "1611.05545", "submitter": "Justin Sirignano", "authors": "Justin Sirignano and Konstantinos Spiliopoulos", "title": "Stochastic Gradient Descent in Continuous Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent in continuous time (SGDCT) provides a\ncomputationally efficient method for the statistical learning of\ncontinuous-time models, which are widely used in science, engineering, and\nfinance. The SGDCT algorithm follows a (noisy) descent direction along a\ncontinuous stream of data. SGDCT performs an online parameter update in\ncontinuous time, with the parameter updates $\\theta_t$ satisfying a stochastic\ndifferential equation. We prove that $\\lim_{t \\rightarrow \\infty} \\nabla \\bar\ng(\\theta_t) = 0$ where $\\bar g$ is a natural objective function for the\nestimation of the continuous-time dynamics. The convergence proof leverages\nergodicity by using an appropriate Poisson equation to help describe the\nevolution of the parameters for large times. SGDCT can also be used to solve\ncontinuous-time optimization problems, such as American options. For certain\ncontinuous-time problems, SGDCT has some promising advantages compared to a\ntraditional stochastic gradient descent algorithm. As an example application,\nSGDCT is combined with a deep neural network to price high-dimensional American\noptions (up to 100 dimensions).\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 03:02:01 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2016 15:53:10 GMT"}, {"version": "v3", "created": "Mon, 17 Apr 2017 19:11:30 GMT"}, {"version": "v4", "created": "Sun, 29 Oct 2017 13:14:07 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Sirignano", "Justin", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1611.05838", "submitter": "Miklos Z. Racz", "authors": "Miklos Z. Racz, Jacob Richey", "title": "A smooth transition from Wishart to GOE", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that an $n \\times n$ Wishart matrix with $d$ degrees of\nfreedom is close to the appropriately centered and scaled Gaussian Orthogonal\nEnsemble (GOE) if $d$ is large enough. Recent work of Bubeck, Ding, Eldan, and\nRacz, and independently Jiang and Li, shows that the transition happens when $d\n= \\Theta ( n^{3} )$. Here we consider this critical window and explicitly\ncompute the total variation distance between the Wishart and GOE matrices when\n$d / n^{3} \\to c \\in (0, \\infty)$. This shows, in particular, that the phase\ntransition from Wishart to GOE is smooth.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 20:02:13 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Racz", "Miklos Z.", ""], ["Richey", "Jacob", ""]]}, {"id": "1611.05843", "submitter": "William Weimin Yoo", "authors": "William Weimin Yoo", "title": "Contributed Discussion to Bayesian Solution Uncertainty Quantification\n  for Differential Equations", "comments": "2 pages", "journal-ref": "Bayesian Anal. 11, no 4 (2016), 1285-1293", "doi": "10.1214/16-BA1017A", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We begin by introducing the main ideas of the paper under discussion, and we\ngive a brief description of the method proposed. Next, we discuss an\nalternative approach based on B-spline expansion, and lastly we make some\ncomments on the method's convergence rate.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 20:12:59 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Yoo", "William Weimin", ""]]}, {"id": "1611.05909", "submitter": "Sean Chang", "authors": "Sean Chang, James O. Berger", "title": "Comparison of Bayesian and Frequentist Multiplicity Correction For\n  Testing Mutually Exclusive Hypotheses Under Data Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of testing mutually exclusive hypotheses with dependent test\nstatistics is considered. Bayesian and frequentist approaches to multiplicity\ncontrol are studied and compared to help gain understanding as to the effect of\ntest statistic dependence on each approach. The Bayesian approach is shown to\nhave excellent frequentist properties and is argued to be the most effective\nway of obtaining frequentist multiplicity control, without sacrificing power,\nwhen there is considerable test statistic dependence.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 21:45:23 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Chang", "Sean", ""], ["Berger", "James O.", ""]]}, {"id": "1611.05917", "submitter": "Robert Bassett", "authors": "Robert Bassett and Julio Deride", "title": "Maximum a Posteriori Estimators as a Limit of Bayes Estimators", "comments": null, "journal-ref": null, "doi": "10.1007/s10107-018-1241-0", "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum a posteriori and Bayes estimators are two common methods of point\nestimation in Bayesian Statistics. It is commonly accepted that maximum a\nposteriori estimators are a limiting case of Bayes estimators with 0-1 loss. In\nthis paper, we provide a counterexample which shows that in general this claim\nis false. We then correct the claim that by providing a level-set condition for\nposterior densities such that the result holds. Since both estimators are\ndefined in terms of optimization problems, the tools of variational analysis\nfind a natural application to Bayesian point estimation.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 22:07:55 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 16:44:18 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Bassett", "Robert", ""], ["Deride", "Julio", ""]]}, {"id": "1611.06034", "submitter": "Benjamin Poignard", "authors": "Benjamin Poignard", "title": "Asymptotic Theory of the Sparse Group LASSO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general framework for penalized convex empirical\ncriteria and a new version of the Sparse-Group LASSO (SGL, Simon and al.,\n2013), called the adaptive SGL, where both penalties of the SGL are weighted by\npreliminary random coefficients. We explore extensively its asymptotic\nproperties and prove that this estimator satisfies the so-called oracle\nproperty (Fan and Li, 2001), that is the sparsity based estimator recovers the\ntrue underlying sparse model and is asymptotically normally distributed. Then\nwe study its asymptotic properties in a double-asymptotic framework, where the\nnumber of parameters diverges with the sample size. We show by simulations that\nthe adaptive SGL outperforms other oracle-like methods in terms of estimation\nprecision and variable selection.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 10:33:46 GMT"}, {"version": "v2", "created": "Tue, 22 Nov 2016 17:06:19 GMT"}, {"version": "v3", "created": "Mon, 28 Nov 2016 13:46:56 GMT"}, {"version": "v4", "created": "Tue, 29 Nov 2016 21:07:07 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Poignard", "Benjamin", ""]]}, {"id": "1611.06129", "submitter": "Emanuele Taufer", "authors": "Emanuele Taufer", "title": "On a goodness of fit test for the Cauchy distribution", "comments": "10 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper discusses a test for the hypothesis that a random sample comes from\nthe Cauchy distribution. The test statistics is derived from a characterization\nand is based on the characteristic function. Properties of the test are\ndiscussed and its performance measured by simulations. The test presented turns\nout to be extremely powerful in a wide range of alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 15:43:38 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Taufer", "Emanuele", ""]]}, {"id": "1611.06173", "submitter": "Kevin McGoff", "authors": "Kevin McGoff and Andrew B. Nobel", "title": "Empirical risk minimization and complexity of dynamical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A dynamical model consists of a continuous self-map $T: \\mathcal{X} \\to\n\\mathcal{X}$ of a compact state space $\\mathcal{X}$ and a continuous\nobservation function $f: \\mathcal{X} \\to \\mathbb{R}$. This paper considers the\nfitting of a parametrized family of dynamical models to an observed real-valued\nstochastic process using empirical risk minimization. The limiting behavior of\nthe minimum risk parameters is studied in a general setting. We establish a\ngeneral convergence theorem for minimum risk estimators and ergodic\nobservations. We then study conditions under which empirical risk minimization\ncan effectively separate the signal from the noise in an additive observational\nnoise model. The key, necessary condition in the latter results is that the\nfamily of dynamical models has limited complexity, which is quantified through\na notion of entropy for families of infinite sequences. Close connections\nbetween entropy and limiting average mean widths for stationary processes are\nestablished.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 17:48:10 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 15:41:19 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["McGoff", "Kevin", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1611.06353", "submitter": "Andreas Hamel H", "authors": "Andreas H Hamel and Daniel Kostner", "title": "Cone distribution functions and quantiles for multivariate random\n  variables", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set-valued quantiles for multivariate distributions with respect to a general\nconvex cone are introduced which are based on a family of (univariate)\ndistribution functions rather than on the joint distribution function. It is\nshown that these quantiles enjoy basically all the properties of univariate\nquantile functions. Relationships to families of univariate quantile functions\nand to depth functions are discussed. Finally, a corresponding Value at Risk\nfor multivariate random variables as well as stochastic orders are introduced\nvia the set-valued approach.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 12:19:06 GMT"}, {"version": "v2", "created": "Sun, 12 Feb 2017 20:24:55 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Hamel", "Andreas H", ""], ["Kostner", "Daniel", ""]]}, {"id": "1611.06575", "submitter": "Michael Levine", "authors": "Zhou Shen and Michael Levine and Zuofeng Shang", "title": "A maximum smoothed likelihood based estimation for two component\n  semiparametric density mixtures with a known component", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a semiparametric mixture of two univariate density functions\nwhere one of them is known while the weight and the other function are unknown.\nSuch mixtures have a history of application to the problem of detecting\ndifferentially expressed genes under two or more conditions in microarray data.\nUntil now, some additional knowledge about the unknown component (e.g. the fact\nthat it belongs to a location family) has been assumed. As opposed to this\napproach, we do not assume any additional structure on the unknown density\nfunction. For this mixture model, we derive a new sufficient identifiability\ncondition and pinpoint a specific class of distributions describing the unknown\ncomponent for which this condition is mostly satisfied. Our approach to\nestimation of this model is based on an idea of applying a maximum smoothed\nlikelihood to what would otherwise have been an ill-posed problem. We introduce\nan iterative MM (Majorization-Minimization) algorithm that estimates all of the\nmodel parameters. We establish that the algorithm possesses a descent property\nwith respect to a log-likelihood objective functional and prove that the\nalgorithm converges to a minimizer of such an objective functional. Finally, we\nalso illustrate the performance of our algorithm in a simulation study and\nusing a real dataset.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 19:31:01 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 22:20:38 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Shen", "Zhou", ""], ["Levine", "Michael", ""], ["Shang", "Zuofeng", ""]]}, {"id": "1611.06655", "submitter": "Zhigen Zhao", "authors": "Qian Lin, Zhigen Zhao and Jun S. Liu", "title": "Sparse Sliced Inverse Regression Via Lasso", "comments": "41 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For multiple index models, it has recently been shown that the sliced inverse\nregression (SIR) is consistent for estimating the sufficient dimension\nreduction (SDR) space if and only if $\\rho=\\lim\\frac{p}{n}=0$, where $p$ is the\ndimension and $n$ is the sample size. Thus, when $p$ is of the same or a higher\norder of $n$, additional assumptions such as sparsity must be imposed in order\nto ensure consistency for SIR. By constructing artificial response variables\nmade up from top eigenvectors of the estimated conditional covariance matrix,\nwe introduce a simple Lasso regression method to obtain an estimate of the SDR\nspace. The resulting algorithm, Lasso-SIR, is shown to be consistent and\nachieve the optimal convergence rate under certain sparsity conditions when $p$\nis of order $o(n^2\\lambda^2)$, where $\\lambda$ is the generalized\nsignal-to-noise ratio. We also demonstrate the superior performance of\nLasso-SIR compared with existing approaches via extensive numerical studies and\nseveral real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 05:41:18 GMT"}, {"version": "v2", "created": "Sun, 17 Jun 2018 13:34:44 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Lin", "Qian", ""], ["Zhao", "Zhigen", ""], ["Liu", "Jun S.", ""]]}, {"id": "1611.06670", "submitter": "Biqin Song", "authors": "Yanfang Tao, Peipei Yuan, Biqin Song", "title": "Error analysis of regularized least-square regression with Fredholm\n  kernel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning with Fredholm kernel has attracted increasing attention recently\nsince it can effectively utilize the data information to improve the prediction\nperformance. Despite rapid progress on theoretical and experimental\nevaluations, its generalization analysis has not been explored in learning\ntheory literature. In this paper, we establish the generalization bound of\nleast square regularized regression with Fredholm kernel, which implies that\nthe fast learning rate O(l^{-1}) can be reached under mild capacity conditions.\nSimulated examples show that this Fredholm regression algorithm can achieve the\nsatisfactory prediction performance.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 07:03:46 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Tao", "Yanfang", ""], ["Yuan", "Peipei", ""], ["Song", "Biqin", ""]]}, {"id": "1611.06714", "submitter": "Yaniv Tenzer Yaniv Tenzer", "authors": "Yaniv Tenzer and Gal Elidan", "title": "On the Monotonicity of the Copula Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the way in which random entities interact is of key interest in\nnumerous scientific fields. This can range from a full characterization of the\njoint distribution to single scalar summary statistics. In this work we\nidentify a novel relationship between the ubiquitous Shannon's mutual\ninformation measure and the central tool for capturing real-valued non-Gaussian\ndistributions, namely the framework of copulas. Specifically, we establish a\nmonotonic relationship between the mutual information and the copula dependence\nparameter, for a wide range of copula families. In addition to the theoretical\nnovelty, our result gives rise to highly efficient proxy to the expected\nlikelihood, which in turn allows for scalable model selection (e.g. when\nlearning probabilistic graphical models).\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 10:36:37 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Tenzer", "Yaniv", ""], ["Elidan", "Gal", ""]]}, {"id": "1611.06727", "submitter": "Arindam Chatterjee", "authors": "Arindam Chatterjee, Tathagata Bandyopadhyay, Sumanta Adhya", "title": "Inference problems in binary regression model with misclassified\n  responses", "comments": "The numerical results on the full likelihood estimators in Section 3\n  are not correct. The paper will be modified suitably", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Misclassification of binary responses, if ignored, may severely bias the\nmaximum likelihood estimators (MLE) of regression parameters. For such data, a\nbinary regression model incorporating misclassification probabilities is\nextensively used by researchers in different application contexts. The model,\nhowever, suffers from a serious estimation problem because of confounding of\nthe unknown misclassification probabilities with the regression parameters. To\novercome this problem, in addition to the main sample, use of internal\nvalidation data is proposed. However, the maximum likelihood estimators (MLE)\nare found to be substantially biased. Investigating further, we propose a\nmaximum pseudo-likelihood method of estimation which leads to bias reduction.\nFor drawing inference on the regression parameters, we develop a rigorous\nasymptotic theory for the maximum pseudo-likelihood estimators under standard\nassumptions. To facilitate its easy implementation, a bootstrapped version of\nthe estimator is proposed, and its distributional consistency is proved.\nExtensions of these results are also provided for more general\nmisclassification models. The results of the simulation studies are\nencouraging. The methodology is illustrated with a survey data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 11:21:13 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 20:46:35 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2020 15:32:41 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Chatterjee", "Arindam", ""], ["Bandyopadhyay", "Tathagata", ""], ["Adhya", "Sumanta", ""]]}, {"id": "1611.06744", "submitter": "Ningning Xia", "authors": "Ningning Xia and Zhidong Bai", "title": "Convergence rate of eigenvector empirical spectral distribution of large\n  Wigner matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we adopt the eigenvector empirical spectral distribution\n(VESD) to investigate the limiting behavior of eigenvectors of a large\ndimensional Wigner matrix W_n. In particular, we derive the optimal bound for\nthe rate of convergence of the expected VESD of W_n to the semicircle law,\nwhich is of order O(n^{-1/2}) under the assumption of having finite 10th\nmoment. We further show that the convergence rates in probability and almost\nsurely of the VESD are O(n^{-1/4}) and O(n^{-1/6}), respectively, under finite\n8th moment condition. Numerical studies demonstrate that the convergence rate\ndoes not depend on the choice of unit vector involved in the VESD function, and\nthe best possible bound for the rate of convergence of the VESD is of order\nO(n^{-1/2}).\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 12:06:08 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Xia", "Ningning", ""], ["Bai", "Zhidong", ""]]}, {"id": "1611.06752", "submitter": "Teo Sharia", "authors": "Teo Sharia and Lei Zhong", "title": "Asymptotic Behaviour of Truncated Stochastic Approximation Procedures", "comments": "Portions of this text were previously a part of arXiv:1508.01902v1\n  which has been divided into two papers for publication at the request of the\n  journal. The first part is now arXiv:1508.01902v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study asymptotic behaviour of stochastic approximation procedures with\nthree main characteristics: truncations with random moving bounds, a matrix\nvalued random step-size sequence, and a dynamically changing random regression\nfunction. In particular, we show that under quite mild conditions, stochastic\napproximation procedures are asymptotically linear in the statistical sense,\nthat is, they can be represented as weighted sums of random variables.\nTherefore, a suitable form of the central limit theorem can be applied to\nderive asymptotic distribution of the corresponding processes. The theory is\nillustrated by various examples and special cases.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 12:16:43 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Sharia", "Teo", ""], ["Zhong", "Lei", ""]]}, {"id": "1611.06753", "submitter": "Ningning Xia", "authors": "Cheng Liu, Ningning Xia and Jun Yu", "title": "Shrinkage estimation of covariance matrix for portfolio choice with high\n  frequency data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines the usefulness of high frequency data in estimating the\ncovariance matrix for portfolio choice when the portfolio size is large. A\ncomputationally convenient nonlinear shrinkage estimator for the integrated\ncovariance (ICV) matrix of financial assets is developed in two steps. The\neigenvectors of the ICV are first constructed from a designed time variation\nadjusted realized covariance matrix of noise-free log-returns of relatively low\nfrequency data. Then the regularized eigenvalues of the ICV are estimated by\nquasi-maximum likelihood based on high frequency data. The estimator is always\npositive definite and its inverse is the estimator of the inverse of ICV. It\nminimizes the limit of the out-of-sample variance of portfolio returns within\nthe class of rotation-equivalent estimators. It works when the number of\nunderlying assets is larger than the number of time series observations in each\nasset and when the asset price follows a general stochastic process. Our\ntheoretical results are derived under the assumption that the number of assets\n(p) and the sample size (n) satisfy p/n \\to y >0 as n goes to infty . The\nadvantages of our proposed estimator are demonstrated using real data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 12:21:34 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Liu", "Cheng", ""], ["Xia", "Ningning", ""], ["Yu", "Jun", ""]]}, {"id": "1611.06828", "submitter": "Jerome Dedecker", "authors": "J\\'er\\^ome Dedecker (MAP5), Guillaume Sauli\\`ere (IRMES)", "title": "The Mann-Whitney U-statistic for $\\alpha$-dependent sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": "MAP5 2016-32", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give the asymptotic behavior of the Mann-Whitney U-statistic for two\nindependent stationary sequences. The result applies to a large class of\nshort-range dependent sequences, including many non-mixing processes in the\nsense of Rosenblatt. We also give some partial results in the long-range\ndependent case, and we investigate other related questions. Based on the\ntheoretical results, we propose some simple corrections of the usual tests for\nstochastic domination; next we simulate different (non-mixing) stationary\nprocesses to see that the corrected tests perform well.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 15:17:40 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Dedecker", "J\u00e9r\u00f4me", "", "MAP5"], ["Sauli\u00e8re", "Guillaume", "", "IRMES"]]}, {"id": "1611.06873", "submitter": "Antoine Barbieri", "authors": "Antoine Barbieri (IMAG), Jean-Michel Marin (IMAG), Karine Florin", "title": "A fully objective Bayesian approach for the Behrens-Fisher problem using\n  historical studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For in vivo research experiments with small sample sizes and available\nhistorical data, we propose a sequential Bayesian method for the Behrens-Fisher\nproblem. We consider it as a model choice question with two models in\ncompetition: one for which the two expectations are equal and one for which\nthey are different. The choice between the two models is performed through a\nBayesian analysis, based on a robust choice of combined objective and\nsubjective priors, set on the parameters space and on the models space. Three\nsteps are necessary to evaluate the posterior probability of each model using\ntwo historical datasets similar to the one of interest. Starting from the\nJeffreys prior, a posterior using a first historical dataset is deduced and\nallows to calibrate the Normal-Gamma informative priors for the second\nhistorical dataset analysis, in addition to a uniform prior on the model space.\nFrom this second step, a new posterior on the parameter space and the models\nspace can be used as the objective informative prior for the last Bayesian\nanalysis. Bayesian and frequentist methods have been compared on simulated and\nreal data. In accordance with FDA recommendations, control of type I and type\nII error rates has been evaluated. The proposed method controls them even if\nthe historical experiments are not completely similar to the one of interest.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 16:13:51 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Barbieri", "Antoine", "", "IMAG"], ["Marin", "Jean-Michel", "", "IMAG"], ["Florin", "Karine", ""]]}, {"id": "1611.07103", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti", "title": "Generation of discrete random variables in scalable frameworks", "comments": "The first sections of the paper have been almost completely\n  rewritten. A deep revision of the English has been made", "journal-ref": "Statistics & Probability Letters, Volume 132, January 2018, Pages\n  99-106", "doi": "10.1016/j.spl.2017.09.004", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we face the problem of simulating discrete random variables\nwith general and varying distributions in a scalable framework, where fully\nparallelizable operations should be preferred. The new paradigm is inspired by\nthe context of discrete choice models. Compared to classical algorithms, we add\nparallelized randomness, and we leave the final simulation of the random\nvariable to a single associative operation. We characterize the set of\nalgorithms that work in this way, and those algorithms that may have an\nadditive or multiplicative local noise. As a consequence, we could define a\nnatural way to solve some popular simulation problems.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 23:37:31 GMT"}, {"version": "v2", "created": "Sun, 1 Jan 2017 15:29:43 GMT"}, {"version": "v3", "created": "Mon, 10 Jul 2017 02:47:02 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Aletti", "Giacomo", ""]]}, {"id": "1611.07222", "submitter": "Tobias Zwingmann", "authors": "Tobias Zwingmann and Hajo Holzmann", "title": "Asymptotics for the expected shortfall", "comments": "Main Part and Supplementary Material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the joint asymptotic distribution of empirical quantiles and\nexpected shortfalls under general conditions on the distribution of the\nunderlying observations. In particular, we do not assume that the distribution\nfunction is differentiable at the quantile with strictly positive derivative.\nHence the rate of convergence and the asymptotic distribution for the quantile\ncan be non-standard, but our results show that the expected shortfall remains\nasymptotically normal with a $\\sqrt{n}$-rate, and we even give the joint\ndistribution in such non-standard cases. In the derivation we use the bivariate\nscoring functions for quantile and expected shortfall as recently introduced by\nFissler and Ziegel (2016). The main technical issue is to deal with the\ndistinct rates for quantile and expected shortfall when applying the\nargmax-continuity theorem. We also consider spectral risk measures with\nfinitely-supported spectral measures, and illustrate our results in a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 09:57:49 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 12:39:41 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Zwingmann", "Tobias", ""], ["Holzmann", "Hajo", ""]]}, {"id": "1611.07230", "submitter": "Viet Chi Tran", "authors": "Viet Chi Tran (LAMA), Gwena\\\"elle Castellan (LPP), Anthony Cousien\n  (IAME (UMR\\_S\\_1137 / U1137)), Chi Tran", "title": "Nonparametric adaptive estimation of order 1 Sobol indices in stochastic\n  models, with an application to Epidemiology", "comments": null, "journal-ref": "Electronic journal of statistics , Shaker Heights, OH : Institute\n  of Mathematical Statistics, 2020, 14 (1), pp.50-81.\n  \\&\\#x27E8;10.1214/19-EJS1627\\&\\#x27E9", "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis is a set of methods aiming at quantifying the\ncontribution of an uncertain input parameter of the model (or combination of\nparameters) on the variability of the response. We consider here the estimation\nof the Sobol indices of order 1 which are commonly-used indicators based on a\ndecomposition of the output's variance. In a deterministic framework, when the\nsame inputs always give the same outputs, these indices are usually estimated\nby replicated simulations of the model. In a stochastic framework, when the\nresponse given a set of input parameters is not unique due to randomness in the\nmodel, metamodels are often used to approximate the mean and dispersion of the\nresponse by deterministic functions. We propose a new non-parametric estimator\nwithout the need of defining a metamodel to estimate the Sobol indices of order\n1. The estimator is based on warped wavelets and is adaptive in the regularity\nof the model. The convergence of the mean square error to zero, when the number\nof simulations of the model tend to infinity, is computed and an elbow effect\nis shown, depending on the regularity of the model. Applications in\nEpidemiology are carried to illustrate the use of non-parametric estimators.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:08:15 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 07:35:39 GMT"}, {"version": "v3", "created": "Tue, 21 Jan 2020 10:01:34 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Tran", "Viet Chi", "", "LAMA"], ["Castellan", "Gwena\u00eblle", "", "LPP"], ["Cousien", "Anthony", "", "IAME"], ["Tran", "Chi", "", "LAMA"]]}, {"id": "1611.07237", "submitter": "Nathalie Akakpo", "authors": "Nathalie Akakpo (LPMA)", "title": "Multivariate Intensity Estimation via Hyperbolic Wavelet Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new statistical procedure able in some way to overcome the curse\nof dimensionality without structural assumptions on the function to estimate.\nIt relies on a least-squares type penalized criterion and a new collection of\nmodels built from hyperbolic biorthogonal wavelet bases. We study its\nproperties in a unifying intensity estimation framework, where an oracle-type\ninequality and adaptation to mixed smoothness are shown to hold. Besides, we\ndescribe an algorithm for implementing the estimator with a quite reasonable\ncomplexity.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:33:05 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 13:43:34 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Akakpo", "Nathalie", "", "LPMA"]]}, {"id": "1611.07242", "submitter": "Philippe Bernardoff", "authors": "Philippe Bernardoff (UPPA)", "title": "Laplace copulas of multifactor gamma distributions are new generalized\n  Farlie-Gumbel-Morgenstern copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides bifactor gamma distribution, trivariate gamma\ndistribution and two copula families on [0, 1] n obtained from the Laplace\ntransforms of the multivariate gamma distribution and the multi-factor gamma\ndistribution given by [P ($\\theta$)] --$\\lambda$ and [P ($\\theta$)] --$\\lambda$\nn i=1 (1 + pi$\\theta$i) --($\\lambda$ i --$\\lambda$) respectively, where P is an\naffine polynomial with respect to the n variables $\\theta$1,. .. , $\\theta$n.\nThese copulas are new generalized Farlie-Gumbel-Morgenstern copulas and allow\nin particular to obtain multivariate gamma distributions for which the\ncumulative distribution functions and the probability distribution functions\nare known.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:38:46 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Bernardoff", "Philippe", "", "UPPA"]]}, {"id": "1611.07253", "submitter": "Tobias Kley", "authors": "Stefan Birr, Holger Dette, Marc Hallin, Tobias Kley, Stanislav\n  Volgushev", "title": "On Wigner-Ville Spectra and the Unicity of Time-Varying Quantile-Based\n  Spectral Densities", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unicity of the time-varying quantile-based spectrum proposed in Birr et\nal. (2016) is established via an asymptotic representation result involving\nWigner-Ville spectra.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 11:30:18 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Birr", "Stefan", ""], ["Dette", "Holger", ""], ["Hallin", "Marc", ""], ["Kley", "Tobias", ""], ["Volgushev", "Stanislav", ""]]}, {"id": "1611.07256", "submitter": "Azzimonti Dario", "authors": "Dario Azzimonti (IDSIA), David Ginsbourger (Idiap, IMSV), Cl\\'ement\n  Chevalier (UNINE), Julien Bect (L2S, GdR MASCOT-NUM), Yann Richet (IRSN, GdR\n  MASCOT-NUM)", "title": "Adaptive Design of Experiments for Conservative Estimation of Excursion\n  Sets", "comments": null, "journal-ref": "Technometrics, 63(1):13-26, 2021", "doi": "10.1080/00401706.2019.1693427", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the set of all inputs that leads a\nsystem to some particular behavior. The system is modeled by an\nexpensive-to-evaluate function, such as a computer experiment, and we are\ninterested in its excursion set, i.e. the set of points where the function\ntakes values above or below some prescribed threshold. The objective function\nis emulated with a Gaussian Process (GP) model based on an initial design of\nexperiments enriched with evaluation results at (batch-)sequentially determined\ninput points. The GP model provides conservative estimates for the excursion\nset, which control false positives while minimizing false negatives. We\nintroduce adaptive strategies that sequentially select new evaluations of the\nfunction by reducing the uncertainty on conservative estimates. Following the\nStepwise Uncertainty Reduction approach we obtain new evaluations by minimizing\nadapted criteria. Tractable formulae for the conservative criteria are derived,\nwhich allow more convenient optimization. The method is benchmarked on random\nfunctions generated under the model assumptions in different scenarios of noise\nand batch size. We then apply it to a reliability engineering test case.\nOverall, the proposed strategy of minimizing false negatives in conservative\nestimation achieves competitive performance both in terms of model-based and\nmodel-free indicators.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 11:40:25 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 09:36:00 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 08:01:43 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2018 12:48:22 GMT"}, {"version": "v5", "created": "Fri, 25 Oct 2019 13:14:35 GMT"}, {"version": "v6", "created": "Tue, 4 Feb 2020 07:22:36 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Azzimonti", "Dario", "", "IDSIA"], ["Ginsbourger", "David", "", "Idiap, IMSV"], ["Chevalier", "Cl\u00e9ment", "", "UNINE"], ["Bect", "Julien", "", "L2S, GdR MASCOT-NUM"], ["Richet", "Yann", "", "IRSN, GdR\n  MASCOT-NUM"]]}, {"id": "1611.07276", "submitter": "Tetsuya Takabatake", "authors": "Masaaki Fukasawa and Tetsuya Takabatake", "title": "Asymptotically efficient estimators for self-similar stationary Gaussian\n  noises under high frequency observations", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes feasible asymptotically efficient estimators for a\ncertain class of Gaussian noises with self-similar and stationary properties,\nwhich includes the fractional Gaussian noise, under high frequency\nobservations. In this setting, the optimal rate of estimation depends on\nwhether either the Hurst or diffusion parameters is known or not. This is due\nto the singularity of the asymptotic Fisher information matrix for simultaneous\nestimation of the above two parameters. One of our key ideas is to extend the\nWhittle estimation method to the situation of high frequency observations. We\nshow that our estimators are asymptotically efficient in Fisher's sense.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 12:45:00 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Fukasawa", "Masaaki", ""], ["Takabatake", "Tetsuya", ""]]}, {"id": "1611.07347", "submitter": "Laurent Callot", "authors": "Laurent Callot, Mehmet Caner, Esra Ulasan, A. \\\"Ozlem \\\"Onder", "title": "A Nodewise Regression Approach to Estimating Large Portfolios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the large sample properties of the variance, weights,\nand risk of high-dimensional portfolios where the inverse of the covariance\nmatrix of excess asset returns is estimated using a technique called nodewise\nregression. Nodewise regression provides a direct estimator for the inverse\ncovariance matrix using the Least Absolute Shrinkage and Selection Operator\n(Lasso) of Tibshirani (1994) to estimate the entries of a sparse precision\nmatrix. We show that the variance, weights, and risk of the global minimum\nvariance portfolios and the Markowitz mean-variance portfolios are consistently\nestimated with more assets than observations. We show, empirically, that the\nnodewise regression-based approach performs well in comparison to factor models\nand shrinkage methods.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 15:12:32 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 11:33:21 GMT"}, {"version": "v3", "created": "Tue, 15 Oct 2019 09:55:17 GMT"}], "update_date": "2019-10-16", "authors_parsed": [["Callot", "Laurent", ""], ["Caner", "Mehmet", ""], ["Ulasan", "Esra", ""], ["\u00d6nder", "A. \u00d6zlem", ""]]}, {"id": "1611.07378", "submitter": "Serguei Pergamenshchikov", "authors": "Slim Beltaief, Oleg Chernoyarov and Serguei Pergamenchtchikov", "title": "Model selection for the robust efficient signal processing observed with\n  small L\\'evy noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new model selection method for the adaptive robust efficient\nnonparametric signal estimation observed with impulse noise which is defined by\nthe general non Gaussian L\\'evy processes. On the basis of the developed\nmethod, we construct the estimation procedures which are analyzed in two\nsettings: in non asymptotic and asymptotic ones. For the first time for such\nmodels we show non asymptotic sharp oracle inequalities for the quadratic and\nfor the robust risks, i.e. we show that the constructed procedures are optimal\nin the sharp oracle inequalities sense. Next, by making use of the obtained\noracle inequalities, we provide the asymptotic efficiency property for the\ndeveloped estimation methods in the adaptive setting when the signal/noise\nratio goes to infinity. We apply the developed model selection methods for the\nsignals number detection problem in multi-path information transmission.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 16:03:10 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 13:07:13 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 15:21:36 GMT"}, {"version": "v4", "created": "Sat, 24 Nov 2018 10:42:22 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Beltaief", "Slim", ""], ["Chernoyarov", "Oleg", ""], ["Pergamenchtchikov", "Serguei", ""]]}, {"id": "1611.07418", "submitter": "Christophe Culan M.", "authors": "Christophe Culan, Claude Adnet", "title": "Partial estimators and application to covariance estimation of gaussian\n  and elliptical distributions", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness to outliers is often a desirable property of statistical\nestimators. Indeed many well known estimators offer very good optimal\nperformance in theory but are unusable in applied contexts because of their\nsensitivity to outliers. Of particular interest to the authors is the case of\ncovariance estimators in adaptive matched filtering schemes in signal\nprocessing applications such as RADAR and SONAR detection, for which a\ncontamination by outliers of the estimated noise covariance can lead to a great\nimpact on performances, in particular when these outliers are similar to the\ntarget signal of the matched filter.\n  This paper presents a generic method for building partial estimators from\nknown estimators, which aim at avoiding these issues; the resulting algorithms\nare shown for a few chosen cases.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 09:12:45 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 17:01:12 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Culan", "Christophe", ""], ["Adnet", "Claude", ""]]}, {"id": "1611.07712", "submitter": "Dave Zachariah", "authors": "Dave Zachariah and Petre Stoica", "title": "Pearson information-based lower bound on Fisher information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher information matrix (FIM) plays an important role in the analysis\nof parameter inference and system design problems. In a number of cases,\nhowever, the statistical data distribution and its associated information\nmatrix are either unknown or intractable. For this reason, it is of interest to\ndevelop useful lower bounds on the FIM. In this lecture note, we derive such a\nbound based on moment constraints. We call this bound the Pearson information\nmatrix (PIM) and relate it to properties of a misspecified data distribution.\nFinally, we show that the inverse PIM coincides with the asymptotic covariance\nmatrix of the optimally weighted generalized method of moments.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 10:03:05 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Zachariah", "Dave", ""], ["Stoica", "Petre", ""]]}, {"id": "1611.07873", "submitter": "Paul Fearnhead", "authors": "Paul Fearnhead, Joris Bierkens, Murray Pollock and Gareth O Roberts", "title": "Piecewise Deterministic Markov Processes for Continuous-Time Monte Carlo", "comments": null, "journal-ref": "Statist. Sci., Volume 33, Number 3 (2018), 386-412", "doi": "10.1214/18-STS648", "report-no": null, "categories": "stat.CO math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there have been exciting developments in Monte Carlo methods, with\nthe development of new MCMC and sequential Monte Carlo (SMC) algorithms which\nare based on continuous-time, rather than discrete-time, Markov processes. This\nhas led to some fundamentally new Monte Carlo algorithms which can be used to\nsample from, say, a posterior distribution. Interestingly, continuous-time\nalgorithms seem particularly well suited to Bayesian analysis in big-data\nsettings as they need only access a small sub-set of data points at each\niteration, and yet are still guaranteed to target the true posterior\ndistribution. Whilst continuous-time MCMC and SMC methods have been developed\nindependently we show here that they are related by the fact that both involve\nsimulating a piecewise deterministic Markov process. Furthermore we show that\nthe methods developed to date are just specific cases of a potentially much\nwider class of continuous-time Monte Carlo algorithms. We give an informal\nintroduction to piecewise deterministic Markov processes, covering the aspects\nrelevant to these new Monte Carlo algorithms, with a view to making the\ndevelopment of new continuous-time Monte Carlo more accessible. We focus on how\nand why sub-sampling ideas can be used with these algorithms, and aim to give\ninsight into how these new algorithms can be implemented, and what are some of\nthe issues that affect their efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 16:42:29 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Fearnhead", "Paul", ""], ["Bierkens", "Joris", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth O", ""]]}, {"id": "1611.08049", "submitter": "Taku Moriyama", "authors": "Taku Moriyama, Yoshihiko Maesono", "title": "A new kernel estimator of hazard ratio and its asymptotic mean squared\n  error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hazard function is a ratio of a density and survival function, and it is\na basic tool of the survival analysis. In this paper we propose a kernel\nestimator of the hazard ratio function, which are based on a modification of\n\\'{C}wik and Mielniczuk's method. We study nonparametric estimators of the\nhazard function and compare those estimators by means of asymptotic mean\nsquared error ($AMSE$). We obtain asymptotic bias and variance of the new\nestimator, and compare them with a naive estimator. The asymptotic variance of\nthe new estimator is always smaller than the naive estimator's, so we also\ndiscuss an improvement of $AMSE$ using Terrell and Scott's bias reduction\nmethod. The new modified estimator ensures the non-negativity, and we\ndemonstrate the numerical improvement.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 01:57:56 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 05:04:33 GMT"}], "update_date": "2016-12-13", "authors_parsed": [["Moriyama", "Taku", ""], ["Maesono", "Yoshihiko", ""]]}, {"id": "1611.08261", "submitter": "Brian Bader", "authors": "Brian Bader", "title": "Automated, Efficient, and Practical Extreme Value Analysis with\n  Environmental Applications", "comments": "author's dissertation, 6 total chapters, 3 major chapters, Doctoral\n  Dissertations. Paper 1261. http://digitalcommons.uconn.edu/dissertations/1261", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the fundamental probabilistic theory of extremes has been well\ndeveloped, there are many practical considerations that must be addressed in\napplication. The contribution of this thesis is four-fold. The first concerns\nthe choice of r in the r largest order statistics modeling of extremes. The\nsecond contribution pertains to threshold selection in the peaks-over-threshold\napproach. The third combines a theoretical and methodological approach to\nimprove estimation within non-stationary regional frequency models of extremal\ndata\n  The methodology developed is demonstrated with climate based applications.\nLast, an overview of computational issues for extremes is provided, along with\na brief tutorial of the R package eva, which improves the functionality of\nexisting extreme value software, as well as providing new implementations.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 17:15:08 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Bader", "Brian", ""]]}, {"id": "1611.08293", "submitter": "Rajarshi Mukherjee", "authors": "Rajarshi Mukherjee, Sumit Mukherjee, Ming Yuan", "title": "Global Testing Against Sparse Alternatives under Ising Models", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the effect of dependence on detecting sparse signals.\nIn particular, we focus on global testing against sparse alternatives for the\nmeans of binary outcomes following an Ising model, and establish how the\ninterplay between the strength and sparsity of a signal determines its\ndetectability under various notions of dependence. The profound impact of\ndependence is best illustrated under the Curie-Weiss model where we observe the\neffect of a \"thermodynamic\" phase transition. In particular, the critical state\nexhibits a subtle \"blessing of dependence\" phenomenon in that one can detect\nmuch weaker signals at criticality than otherwise. Furthermore, we develop a\ntesting procedure that is broadly applicable to account for dependence and show\nthat it is asymptotically minimax optimal under fairly general regularity\nconditions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 19:34:29 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 05:26:13 GMT"}], "update_date": "2017-10-06", "authors_parsed": [["Mukherjee", "Rajarshi", ""], ["Mukherjee", "Sumit", ""], ["Yuan", "Ming", ""]]}, {"id": "1611.08299", "submitter": "Kim Hendrickx", "authors": "Piet Groeneboom and Kim Hendrickx", "title": "Confidence intervals for the current status model", "comments": "31 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a new way of constructing pointwise confidence intervals for the\ndistribution function in the current status model. The confidence intervals are\nbased on the smoothed maximum likelihood estimator (SMLE) and constructed using\nbootstrap methods. Other methods to construct confidence intervals, using the\nnon-standard limit distribution of the (restricted) MLE, are compared to our\napproach via simulations and real data applications.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 20:05:30 GMT"}, {"version": "v2", "created": "Mon, 28 Nov 2016 14:44:46 GMT"}, {"version": "v3", "created": "Thu, 1 Dec 2016 18:16:40 GMT"}, {"version": "v4", "created": "Fri, 24 Mar 2017 15:15:27 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Groeneboom", "Piet", ""], ["Hendrickx", "Kim", ""]]}, {"id": "1611.08444", "submitter": "Bas Kleijn", "authors": "B. J. K. Kleijn", "title": "On the frequentist validity of Bayesian limits", "comments": "journal article: main text 20pp., appendices 35 pp., 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To the frequentist who computes posteriors, not all priors are useful\nasymptotically: in this paper Schwartz's 1965 Kullback-Leibler condition is\ngeneralised to enable frequentist interpretation of convergence of posterior\ndistributions with the complex models and often dependent datasets in\npresent-day statistical applications. We prove four simple and fully general\nfrequentist theorems, for posterior consistency; for posterior rates of\nconvergence; for consistency of the Bayes factor in hypothesis testing or model\nselection; and a theorem to obtain confidence sets from credible sets. The\nlatter has a significant methodological consequence in frequentist uncertainty\nquantification: use of a suitable prior allows one to convert credible sets of\na calculated, simulated or approximated posterior into asymptotically\nconsistent confidence sets, in full generality. This extends the main\ninferential implication of the Bernstein-von Mises theorem to non-parametric\nmodels without smoothness conditions. Proofs require the existence of a\nBayesian type of test sequence and priors giving rise to local prior predictive\ndistributions that satisfy a weakened form of Le~Cam's contiguity with respect\nto the data distribution. Results are applied in a wide range of examples and\ncounterexamples.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 12:53:29 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 14:12:00 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 13:21:01 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Kleijn", "B. J. K.", ""]]}, {"id": "1611.08464", "submitter": "Maissa Tamraz", "authors": "Gildas Ratovomirija, Maissa Tamraz, Raluca Vernic", "title": "On some multivariate Sarmanov mixed Erlang reinsurance risks:\n  aggregation and capital allocation", "comments": "1 figure, 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following some recent works on risk aggregation and capital allocation for\nmixed Erlang risks joined by Sarmanov's multivariate distribution, in this\npaper we present some closed-form formulas for the same topic by considering,\nhowever, a different kernel function for Sarmanov's distribution, not\npreviously studied in this context. The risk aggregation and capital allocation\nformulas are derived and numerically illustrated in the general framework of\nstop-loss reinsurance, and then in the particular case with no stop-loss\nreinsurance. A discussion of the dependency structure of the considered\ndistribution, based on Pearson's correlation coefficient, is also presented for\ndifferent kernel functions and illustrated in the bivariate case.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 14:21:58 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Ratovomirija", "Gildas", ""], ["Tamraz", "Maissa", ""], ["Vernic", "Raluca", ""]]}, {"id": "1611.08483", "submitter": "Arnak Dalalyan S.", "authors": "Arnak S. Dalalyan, Edwin Grappin, Quentin Paris", "title": "On the Exponentially Weighted Aggregate with the Laplace Prior", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the statistical behaviour of the Exponentially\nWeighted Aggregate (EWA) in the problem of high-dimensional regression with\nfixed design. Under the assumption that the underlying regression vector is\nsparse, it is reasonable to use the Laplace distribution as a prior. The\nresulting estimator and, specifically, a particular instance of it referred to\nas the Bayesian lasso, was already used in the statistical literature because\nof its computational convenience, even though no thorough mathematical analysis\nof its statistical properties was carried out. The present work fills this gap\nby establishing sharp oracle inequalities for the EWA with the Laplace prior.\nThese inequalities show that if the temperature parameter is small, the EWA\nwith the Laplace prior satisfies the same type of oracle inequality as the\nlasso estimator does, as long as the quality of estimation is measured by the\nprediction loss. Extensions of the proposed methodology to the problem of\nprediction with low-rank matrices are considered.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 15:16:45 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Dalalyan", "Arnak S.", ""], ["Grappin", "Edwin", ""], ["Paris", "Quentin", ""]]}, {"id": "1611.08543", "submitter": "Ananya Lahiri", "authors": "Ananya Lahiri", "title": "Asymptotic properties of the volatility estimator from high frequency\n  data modeled by mixed fractional Brownian motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Properties of mixed fractional Brownian motion has been discussed by\nCheridito (2001) and Zili (2006). We have proposed an estimator of volatility\nparameter for a model driven by MFBM. In our article we have shown that the\nestimator has some desirable asymptotic properties.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 18:19:35 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 10:29:13 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Lahiri", "Ananya", ""]]}, {"id": "1611.08721", "submitter": "Rainer Dyckerhoff", "authors": "Rainer Dyckerhoff", "title": "Convergence of depths and depth-trimmed regions", "comments": "The cited supplement is included in the PDF file", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth is a concept that measures the `centrality' of a point in a given data\ncloud or in a given probability distribution. Every depth defines a family of\nso-called trimmed regions. For statistical applications it is desirable that\nwith increasing sample size the empirical depth as well as the empirical\ntrimmed regions converge almost surely to their population counterparts.\n  In this article the connections between different types of convergence are\ndiscussed. In particular, conditions are given under which the pointwise (resp.\nuniform) convergence of the data depth implies the pointwise (resp. compact)\nconvergence of the trimmed regions in the Hausdorff metric as well as\nconditions under which the reverse implications hold. Further, it is shown that\nunder relative weak conditions the pointwise convergence of the data depth\n(resp. trimmed regions) is equivalent to the uniform convergence of the data\ndepth (resp. compact convergence of the trimmed regions).\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 16:45:49 GMT"}, {"version": "v2", "created": "Wed, 12 Apr 2017 12:17:26 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Dyckerhoff", "Rainer", ""]]}, {"id": "1611.08761", "submitter": "David Kelly", "authors": "David Kelly and Andrew M Stuart", "title": "Ergodicity and Accuracy of Optimal Particle Filters for Bayesian Data\n  Assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For particle filters and ensemble Kalman filters it is of practical\nimportance to understand how and why data assimilation methods can be effective\nwhen used with a fixed small number of particles, since for many large-scale\napplications it is not practical to deploy algorithms close to the large\nparticle limit asymptotic. In this paper we address this question for particle\nfilters and, in particular, study their accuracy (in the small noise limit) and\nergodicity (for noisy signal and observation) without appealing to the large\nparticle number limit. We first overview the accuracy and minorization\nproperties for the true filtering distribution, working in the setting of\nconditional Gaussianity for the dynamics-observation model. We then show that\nthese properties are inherited by optimal particle filters for any fixed number\nof particles, and use the minorization to establish ergodicity of the filters.\nFor completeness we also prove large particle number consistency results for\nthe optimal particle filters, by writing the update equations for the\nunderlying distributions as recursions. In addition to looking at the optimal\nparticle filter with standard resampling, we derive all the above results for\n(what we term) the Gaussianized optimal particle filter and show that the\ntheoretical properties are favorable for this method, when compared to the\nstandard optimal particle filter.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 23:22:19 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 01:31:42 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Kelly", "David", ""], ["Stuart", "Andrew M", ""]]}, {"id": "1611.08874", "submitter": "Ayan Bhattacharya", "authors": "Ayan Bhattacharya and Parthanil Roy", "title": "A large sample test for the length of memory of stationary symmetric\n  stable random fields via nonsingular $\\mathbb{Z}^d$-actions", "comments": "Accepted for publication in Journal of Applied Probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the ratio of two block maxima, we propose a large sample test for\nthe length of memory of a stationary symmetric $\\alpha$-stable discrete\nparameter random field. We show that the power function converges to one as the\nsample-size increases to infinity under various classes of alternatives having\nlonger memory in the sense of Samorodnitsky(2004). Ergodic theory of\nnonsingular $\\mathbb{Z}^d$-actions play a very important role in the design and\nanalysis of our large sample test.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 17:22:35 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 23:07:49 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Bhattacharya", "Ayan", ""], ["Roy", "Parthanil", ""]]}, {"id": "1611.09005", "submitter": "Alfredo Alegr\\'ia", "authors": "Alfredo Alegria and Emilio Porcu", "title": "The dimple problem related to space-time modeling under the Lagrangian\n  framework", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmva.2017.09.001", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Space-time covariance modeling under the Lagrangian framework has been\nespecially popular to study atmospheric phenomena in the presence of transport\neffects, such as prevailing winds or ocean currents, which are incompatible\nwith the assumption of full symmetry. In this work, we assess the dimple\nproblem (Kent et al., 2011) for covariance functions coming from transport\nphenomena. We work under two important cases: the spatial domain can be either\nthe $d$-dimensional Euclidean space $\\mathbb{R}^d$ or the spherical shell of\n$\\mathbb{R}^d$. The choice is relevant for the type of metric chosen to\ndescribe spatial dependence. In particular, in Euclidean spaces, we work under\nvery general assumptions with the case of radial symmetry being deduced as a\ncorollary of a more general result. We illustrate through examples that, under\nthis framework, the dimple is a natural and physically interpretable property.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 07:17:03 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 21:44:23 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Alegria", "Alfredo", ""], ["Porcu", "Emilio", ""]]}, {"id": "1611.09008", "submitter": "Theofanis Sapatinas", "authors": "Clement Marteau and Theofanis Sapatinas", "title": "Minimax Signal Detection Under Weak Noise Assumptions", "comments": "20 pages, 1 Table (Final Version - To appear in: Mathematical Methods\n  of Statistics)", "journal-ref": "Mathematical Methods of Statistics, Vol. 26, 282-298 (2017)", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider minimax signal detection in the sequence model. Working with\ncertain ellipsoids in the space of square-summable sequences of real numbers,\nwith a ball of positive radius removed, we obtain upper and lower bounds for\nthe minimax separation radius in the non-asymptotic framework, i.e., for a\nfixed value of the involved noise level. We use very weak assumptions on the\nnoise (i.e., fourth moments are assumed to be uniformly bounded). In\nparticular, we do not use any kind of Gaussianity or independence assumption on\nthe noise. It is shown that the established minimax separation rates are not\nfaster than the ones obtained in the classical sequence model (i.e.,\nindependent standard Gaussian noise) but, surprisingly, are of the same order\nas the minimax estimation rates in the classical setting. Under an additional\ncondition on the noise, the classical minimax separation rates are also\nretrieved in benchmark well-posed and ill-posed inverse problems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 07:31:47 GMT"}, {"version": "v2", "created": "Fri, 9 Dec 2016 14:32:03 GMT"}, {"version": "v3", "created": "Wed, 1 Nov 2017 13:49:28 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Marteau", "Clement", ""], ["Sapatinas", "Theofanis", ""]]}, {"id": "1611.09391", "submitter": "Will Wei Sun", "authors": "Botao Hao, Will Wei Sun, Yufeng Liu, Guang Cheng", "title": "Simultaneous Clustering and Estimation of Heterogeneous Graphical Models", "comments": "61 pages. Accepted by Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider joint estimation of multiple graphical models arising from\nheterogeneous and high-dimensional observations. Unlike most previous\napproaches which assume that the cluster structure is given in advance, an\nappealing feature of our method is to learn cluster structure while estimating\nheterogeneous graphical models. This is achieved via a high dimensional version\nof Expectation Conditional Maximization (ECM) algorithm (Meng and Rubin, 1993).\nA joint graphical lasso penalty is imposed on the conditional maximization step\nto extract both homogeneity and heterogeneity components across all clusters.\nOur algorithm is computationally efficient due to fast sparse learning routines\nand can be implemented without unsupervised learning knowledge. The superior\nperformance of our method is demonstrated by extensive experiments and its\napplication to a Glioblastoma cancer dataset reveals some new insights in\nunderstanding the Glioblastoma cancer. In theory, a non-asymptotic error bound\nis established for the output directly from our high dimensional ECM algorithm,\nand it consists of two quantities: statistical error (statistical accuracy) and\noptimization error (computational complexity). Such a result gives a\ntheoretical guideline in terminating our ECM iterations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 21:28:13 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 23:00:23 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Hao", "Botao", ""], ["Sun", "Will Wei", ""], ["Liu", "Yufeng", ""], ["Cheng", "Guang", ""]]}, {"id": "1611.09408", "submitter": "Michelle Xia", "authors": "P. Richard Hahn and Michelle Xia", "title": "A finite mixture model approach to regression under covariate\n  misclassification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of mismeasured categorical covariates in the\ncontext of regression modeling; if unaccounted for, such misclassification is\nknown to result in misestimation of model parameters. Here, we exploit the fact\nthat explicitly modeling covariate misclassification leads to a mixture\nrepresentation. Assuming common parametric families for the mixture components,\nand assuming that the misclassification occurrence is independent of the\nresponse variable, the mixture representation permits model parameters to be\nidentified even when misclassification probabilities are unknown. Previous\napproaches to covariate misclassification use multiple surrogate covariates\nand/or validation data on the magnitude of errors. Based on this mixture\nstructure, we demonstrate that valid inference can be performed on all the\nparameters even when no such additional information is available. Using\nBayesian inference, the method allows for learning from data combined with\nexternal information on the magnitude of errors when such information does\nbecome available. The method is applied to adjust for misclassification on\nself-reported cocaine use in the Longitudinal Studies of HIV-Associated Lung\nInfections and Complications (Lung HIV). We find a substantial and\nstatistically significant effect of cocaine use on pulmonary complications\nmeasured by the relative area of emphysema, whereas a regression that does not\nadjust for misclassification yields a much smaller estimate.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 22:10:20 GMT"}, {"version": "v2", "created": "Sat, 24 Dec 2016 05:32:31 GMT"}, {"version": "v3", "created": "Wed, 26 Apr 2017 19:39:50 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Hahn", "P. Richard", ""], ["Xia", "Michelle", ""]]}, {"id": "1611.09588", "submitter": "Alejandro  Cholaquidis", "authors": "Alejandro Cholaquidis, Ricardo Fraiman, Ernesto Mordecki, Cecilia\n  Papalardo", "title": "Level sets and drift estimation for reflected Brownian motion with drift", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of the drift and the level sets of the stationary\ndistri- bution of a Brownian motion with drift, reflected in the boundary of a\ncompact set $S\\subset R^d$ , departing from the observation of a trajectory of\nthis process. We obtain the uniform consistency and rates of convergence for\nthe proposed kernel based estimators. This problem has relevant applications in\necology, in estimating the home-range and the core-area of an animal based on\ntracking data. Recently, the problem of estimat- ing the domain of a reflected\nBrownian motion was considered in Cholaquidis, et al. (2016), when the\nstationary distribution is uniform and the estimation of the core-area, defined\nas a level set of the stationary distribution, is meaningless. As a by-product\nof our results, we obtain an estimation of the drift function. In order to\nprove our re- sults, some new theoretical properties of the reflected Brownian\nmotion with drift are obtained, under fairly general assumptions. These\nproperties allow us to perform the estimation for flexible regions close to\nreality. The theoretical findings are illustrated on simulated and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 12:23:39 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 00:05:11 GMT"}, {"version": "v3", "created": "Fri, 18 Aug 2017 19:50:26 GMT"}, {"version": "v4", "created": "Sun, 17 Sep 2017 15:28:55 GMT"}, {"version": "v5", "created": "Sat, 10 Feb 2018 14:56:03 GMT"}, {"version": "v6", "created": "Sat, 27 Oct 2018 15:04:58 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Mordecki", "Ernesto", ""], ["Papalardo", "Cecilia", ""]]}, {"id": "1611.09744", "submitter": "Olivier Collier", "authors": "Olivier Collier, La\\\"etitia Comminges, Alexandre B. Tsybakov and\n  Nicolas Verz\\'elen", "title": "Optimal adaptive estimation of linear functionals under sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimation of a linear functional in the Gaussian\nsequence model where the unknown vector theta in R^d belongs to a class of\ns-sparse vectors with unknown s. We suggest an adaptive estimator achieving a\nnon-asymptotic rate of convergence that differs from the minimax rate at most\nby a logarithmic factor. We also show that this optimal adaptive rate cannot be\nimproved when s is unknown. Furthermore, we address the issue of simultaneous\nadaptation to s and to the variance sigma^2 of the noise. We suggest an\nestimator that achieves the optimal adaptive rate when both s and sigma^2 are\nunknown.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 17:38:25 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 13:59:21 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Collier", "Olivier", ""], ["Comminges", "La\u00ebtitia", ""], ["Tsybakov", "Alexandre B.", ""], ["Verz\u00e9len", "Nicolas", ""]]}, {"id": "1611.09804", "submitter": "Holger Dette", "authors": "Holger Dette, Andrey Pepelyshev, Anatoly Zhigljavsky", "title": "Best linear unbiased estimators in continuous time regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the problem of best linear unbiased estimation is investigated\nfor continuous-time regression models. We prove several general statements\nconcerning the explicit form of the best linear unbiased estimator (BLUE), in\nparticular when the error process is a smooth process with one or several\nderivatives of the response process available for construction of the\nestimators. We derive the explicit form of the BLUE for many specific models\nincluding the cases of continuous autoregressive errors of order two and\nintegrated error processes (such as integrated Brownian motion). The results\nare illustrated by several examples.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 19:40:09 GMT"}], "update_date": "2016-12-06", "authors_parsed": [["Dette", "Holger", ""], ["Pepelyshev", "Andrey", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "1611.09933", "submitter": "Wenyu Chen", "authors": "Wenyu Chen, Zhaokai Wang, Wooseok Ha, Rina Foygel Barber", "title": "Trimmed Conformal Prediction for High-Dimensional Models", "comments": "11 pages, 4 figures, Under review for AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression, conformal prediction is a general methodology to construct\nprediction intervals in a distribution-free manner. Although conformal\nprediction guarantees strong statistical property for predictive inference, its\ninherent computational challenge has attracted the attention of researchers in\nthe community. In this paper, we propose a new framework, called Trimmed\nConformal Prediction (TCP), based on two stage procedure, a trimming step and a\nprediction step. The idea is to use a preliminary trimming step to\nsubstantially reduce the range of possible values for the prediction interval,\nand then applying conformal prediction becomes far more efficient. As is the\ncase of conformal prediction, TCP can be applied to any regression method, and\nfurther offers both statistical accuracy and computational gains. For a\nspecific example, we also show how TCP can be implemented in the sparse\nregression setting. The experiments on both synthetic and real data validate\nthe empirical performance of TCP.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 23:03:19 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Chen", "Wenyu", ""], ["Wang", "Zhaokai", ""], ["Ha", "Wooseok", ""], ["Barber", "Rina Foygel", ""]]}, {"id": "1611.09972", "submitter": "Asad Haris", "authors": "Asad Haris, Ali Shojaie, Noah Simon", "title": "Nonparametric Regression with Adaptive Truncation via a Convex\n  Hierarchical Penalty", "comments": null, "journal-ref": "Biometrika 2018, Vol. 106, No. 1, 87-107", "doi": "10.1093/biomet/asy056", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of non-parametric regression with a potentially large\nnumber of covariates. We propose a convex, penalized estimation framework that\nis particularly well-suited for high-dimensional sparse additive models. The\nproposed approach combines appealing features of finite basis representation\nand smoothing penalties for non-parametric estimation. In particular, in the\ncase of additive models, a finite basis representation provides a parsimonious\nrepresentation for fitted functions but is not adaptive when component\nfunctions posses different levels of complexity. On the other hand, a smoothing\nspline type penalty on the component functions is adaptive but does not offer a\nparsimonious representation of the estimated function. The proposed approach\nsimultaneously achieves parsimony and adaptivity in a computationally efficient\nframework. We demonstrate these properties through empirical studies on both\nreal and simulated datasets. We show that our estimator converges at the\nminimax rate for functions within a hierarchical class. We further establish\nminimax rates for a large class of sparse additive models. The proposed method\nis implemented using an efficient algorithm that scales similarly to the Lasso\nwith the number of covariates and samples size.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 02:22:31 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2016 22:28:45 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 15:43:02 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2019 12:22:21 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Haris", "Asad", ""], ["Shojaie", "Ali", ""], ["Simon", "Noah", ""]]}, {"id": "1611.10011", "submitter": "Kou Fujimori", "authors": "Kou Fujimori and Yoichi Nishiyama", "title": "The Dantzig selector for diffusion processes with covariates", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dantzig selector for a special parametric model of diffusion processes is\nstudied in this paper. In our model, the diffusion coefficient is given as the\nexponential of the linear combination of other processes which are regarded as\ncovariates. We propose an estimation procedure which is an adaptation of the\nDantzig selector for linear regression models and prove the $l_q$ consistency\nof the estimator for all $q \\in [1,\\infty]$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 06:02:01 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Fujimori", "Kou", ""], ["Nishiyama", "Yoichi", ""]]}, {"id": "1611.10203", "submitter": "Carlos Martins-Filho", "authors": "Kairat Mynbaev and Carlos Martins-Filho", "title": "Reducing bias in nonparametric density estimation via bandwidth\n  dependent kernels: $L_1$ view", "comments": "9 pages", "journal-ref": "Statistics and Probability Letters 123, 17-22, 2017", "doi": "10.1016/j.spl.2016.11.019", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a new bandwidth-dependent kernel density estimator that improves\nexisting convergence rates for the bias, and preserves that of the variation,\nwhen the error is measured in $L_1$. No additional assumptions are imposed to\nthe extant literature.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 15:06:25 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Mynbaev", "Kairat", ""], ["Martins-Filho", "Carlos", ""]]}, {"id": "1611.10266", "submitter": "Christophe Culan M.", "authors": "Christophe Culan, Claude Adnet", "title": "Regularized maximum likelihood estimation of covariance matrices of\n  elliptical distributions", "comments": "9 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1611.04365, arXiv:1611.07418", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The maximum likelihood principle is widely used in statistics, and the\nassociated estimators often display good properties. indeed maximum likelihood\nestimators are guaranteed to be asymptotically efficient under mild conditions.\nHowever in some settings, one has too few samples to get a good estimation. It\nthen becomes desirable to take into account prior information about the\ndistribution one wants to estimate. One possible approach is to extend the\nmaximum likelihood principle in a bayesian context, which then becomes a\nmaximum a posteriori estimate; however this requires a distribution model on\nthe distribution parameters. We shall therefore concentrate on the alternative\napproach of regularized estimators in this paper; we will show how they can be\nnaturally introduced in the framework of maximum likelihood estimation, and how\nthey can be extended to form robust estimators which can reject outliers.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 17:10:52 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Culan", "Christophe", ""], ["Adnet", "Claude", ""]]}, {"id": "1611.10289", "submitter": "Han Cheng Lie", "authors": "Han Cheng Lie, T. J. Sullivan", "title": "Quasi-invariance of countable products of Cauchy measures under\n  non-unitary dilations", "comments": "5 pages, supersedes arXiv:1608.03784 \"Cameron-Martin theorems for\n  sequences of symmetric Cauchy-distributed random variables\"", "journal-ref": "Electron. Commun. Probab. (2018)", "doi": "10.1214/18-ECP113", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an infinite sequence $(U_n)_{n\\in\\mathbb{N}}$ of independent Cauchy\nrandom variables, defined by a sequence $(\\delta_n)_{n\\in\\mathbb{N}}$ of\nlocation parameters and a sequence $(\\gamma_n)_{n\\in\\mathbb{N}}$ of scale\nparameters. Let $(W_n)_{n\\in\\mathbb{N}}$ be another infinite sequence of\nindependent Cauchy random variables defined by the same sequence of location\nparameters and the sequence $(\\sigma_n\\gamma_n)_{n\\in\\mathbb{N}}$ of scale\nparameters, with $\\sigma_n\\neq 0$ for all $n\\in\\mathbb{N}$. Using a result of\nKakutani on equivalence of countably infinite product measures, we show that\nthe laws of $(U_n)_{n\\in\\mathbb{N}}$ and $(W_n)_{n\\in\\mathbb{N}}$ are\nequivalent if and only if the sequence $(\\vert\n\\sigma_n\\vert-1)_{n\\in\\mathbb{N}}$ is square-summable.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 17:50:25 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 17:16:17 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 14:45:41 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Lie", "Han Cheng", ""], ["Sullivan", "T. J.", ""]]}, {"id": "1611.10333", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban, William Leeb, Amit Singer", "title": "PCA from noisy, linearly reduced data: the diagonal case", "comments": "This technical report has been largely superseded by our later paper\n  arXiv:1709.03393. Please cite that one instead of this one. This paper has a\n  slightly different approach, so we want to keep it publicly available", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose we observe data of the form $Y_i = D_i (S_i + \\varepsilon_i) \\in\n\\mathbb{R}^p$ or $Y_i = D_i S_i + \\varepsilon_i \\in \\mathbb{R}^p$,\n$i=1,\\ldots,n$, where $D_i \\in \\mathbb{R}^{p\\times p}$ are known diagonal\nmatrices, $\\varepsilon_i$ are noise, and we wish to perform principal component\nanalysis (PCA) on the unobserved signals $S_i \\in \\mathbb{R}^p$. The first\nmodel arises in missing data problems, where the $D_i$ are binary. The second\nmodel captures noisy deconvolution problems, where the $D_i$ are the Fourier\ntransforms of the convolution kernels. It is often reasonable to assume the\n$S_i$ lie on an unknown low-dimensional linear space; however, because many\ncoordinates can be suppressed by the $D_i$, this low-dimensional structure can\nbe obscured.\n  We introduce diagonally reduced spiked covariance models to capture this\nsetting. We characterize the behavior of the singular vectors and singular\nvalues of the data matrix under high-dimensional asymptotics where\n$n,p\\to\\infty$ such that $p/n\\to\\gamma>0$. Our results have the most general\nassumptions to date even without diagonal reduction. Using them, we develop\noptimal eigenvalue shrinkage methods for covariance matrix estimation and\noptimal singular value shrinkage methods for data denoising.\n  Finally, we characterize the error rates of the empirical Best Linear\nPredictor (EBLP) denoisers. We show that, perhaps surprisingly, their optimal\ntuning depends on whether we denoise in-sample or out-of-sample, but the\noptimally tuned mean squared error is the same in the two cases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 20:01:21 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 20:21:39 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Dobriban", "Edgar", ""], ["Leeb", "William", ""], ["Singer", "Amit", ""]]}, {"id": "1611.10335", "submitter": "Charles R Doss", "authors": "Charles R. Doss and Jon A. Wellner", "title": "Univariate log-concave density estimation with symmetry or modal\n  constraints", "comments": "71 pages, 3 figures. This replacement is a revision of the previous\n  submission with a slightly modified title and some reorganizational changes\n  and clarifications made to the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonparametric maximum likelihood estimation of a log-concave density\nfunction $f_0$ which is known to satisfy further constraints, where either (a)\nthe mode $m$ of $f_0$ is known, or (b) $f_0$ is known to be symmetric about a\nfixed point $m$. We develop asymptotic theory for both constrained log-concave\nmaximum likelihood estimators (MLE's), including consistency, global rates of\nconvergence, and local limit distribution theory. In both cases, we find the\nMLE's pointwise limit distribution at $m$ (either the known mode or the known\ncenter of symmetry) and at a point $x_0 \\ne m$. Software to compute the\nconstrained estimators is available in the R package \\verb+logcondens.mode+.\n  The symmetry-constrained MLE is particularly useful in contexts of location\nestimation. The mode-constrained MLE is useful for mode-regression. The\nmode-constrained MLE can also be used to form a likelihood ratio test for the\nlocation of the mode of $f_0$. These problems are studied in separate papers.\nIn particular, in a separate paper we show that, under a curvature assumption,\nthe likelihood ratio statistic for the location of the mode can be used for\nhypothesis tests or confidence intervals that do not depend on either tuning\nparameters or nuisance parameters.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 20:04:15 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 18:12:56 GMT"}, {"version": "v3", "created": "Mon, 9 Apr 2018 23:58:02 GMT"}, {"version": "v4", "created": "Mon, 13 May 2019 21:29:00 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Doss", "Charles R.", ""], ["Wellner", "Jon A.", ""]]}, {"id": "1611.10348", "submitter": "Charles R Doss", "authors": "Charles R. Doss and Jon A. Wellner", "title": "Inference for the mode of a log-concave density", "comments": "61 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a likelihood ratio test for the location of the mode of a\nlog-concave density. Our test is based on comparison of the log-likelihoods\ncorresponding to the unconstrained maximum likelihood estimator of a\nlog-concave density and the constrained maximum likelihood estimator where the\nconstraint is that the mode of the density is fixed, say at $m$. The\nconstrained estimation problem is studied in detail in Doss and Wellner [2018].\nHere the results of that paper are used to show that, under the null hypothesis\n(and strict curvature of $-\\log f$ at the mode), the likelihood ratio statistic\nis asymptotically pivotal: that is, it converges in distribution to a limiting\ndistribution which is free of nuisance parameters, thus playing the role of the\n$\\chi_1^2$ distribution in classical parametric statistical problems. By\ninverting this family of tests we obtain new (likelihood ratio based)\nconfidence intervals for the mode of a log-concave density $f$. These new\nintervals do not depend on any smoothing parameters. We study the new\nconfidence intervals via Monte Carlo methods and illustrate them with two real\ndata sets. The new intervals seem to have several advantages over existing\nprocedures. Software implementing the test and confidence intervals is\navailable in the R package \\verb+logcondens.mode+.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 20:43:40 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 18:14:19 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2018 17:41:49 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Doss", "Charles R.", ""], ["Wellner", "Jon A.", ""]]}]