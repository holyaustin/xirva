[{"id": "1711.00070", "submitter": "Anna Korba", "authors": "Stephan Cl\\'emen\\c{c}on, Anna Korba, Eric Sibony", "title": "Ranking Median Regression: Learning to Order through Local Consensus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is devoted to the problem of predicting the value taken by a\nrandom permutation $\\Sigma$, describing the preferences of an individual over a\nset of numbered items $\\{1,\\; \\ldots,\\; n\\}$ say, based on the observation of\nan input/explanatory r.v. $X$ e.g. characteristics of the individual), when\nerror is measured by the Kendall $\\tau$ distance. In the probabilistic\nformulation of the 'Learning to Order' problem we propose, which extends the\nframework for statistical Kemeny ranking aggregation developped in\n\\citet{CKS17}, this boils down to recovering conditional Kemeny medians of\n$\\Sigma$ given $X$ from i.i.d. training examples $(X_1, \\Sigma_1),\\; \\ldots,\\;\n(X_N, \\Sigma_N)$. For this reason, this statistical learning problem is\nreferred to as \\textit{ranking median regression} here. Our contribution is\ntwofold. We first propose a probabilistic theory of ranking median regression:\nthe set of optimal elements is characterized, the performance of empirical risk\nminimizers is investigated in this context and situations where fast learning\nrates can be achieved are also exhibited. Next we introduce the concept of\nlocal consensus/median, in order to derive efficient methods for ranking median\nregression. The major advantage of this local learning approach lies in its\nclose connection with the widely studied Kemeny aggregation problem. From an\nalgorithmic perspective, this permits to build predictive rules for ranking\nmedian regression by implementing efficient techniques for (approximate) Kemeny\nmedian computations at a local level in a tractable manner. In particular,\nversions of $k$-nearest neighbor and tree-based methods, tailored to ranking\nmedian regression, are investigated. Accuracy of piecewise constant ranking\nmedian regression rules is studied under a specific smoothness assumption for\n$\\Sigma$'s conditional distribution given $X$.\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 19:40:40 GMT"}, {"version": "v2", "created": "Mon, 18 Dec 2017 21:36:32 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Cl\u00e9men\u00e7on", "Stephan", ""], ["Korba", "Anna", ""], ["Sibony", "Eric", ""]]}, {"id": "1711.00101", "submitter": "Anru R. Zhang", "authors": "Anru R. Zhang and Kehui Chen", "title": "Nonparametric covariance estimation for mixed longitudinal studies, with\n  applications in midlife women's health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In mixed longitudinal studies, a group of subjects enter the study at\ndifferent ages (cross-sectional) and are followed for successive years\n(longitudinal). In the context of such studies, we consider nonparametric\ncovariance estimation with samples of noisy and partially observed functional\ntrajectories. The proposed algorithm is based on a noniterative\nsequential-aggregation scheme with only basic matrix operations and closed-form\nsolutions in each step. The good performance of the proposed method is\nsupported by both theory and numerical experiments. We also apply the proposed\nprocedure to a study on the working memory of midlife women, based on data from\nthe Study of Women's Health Across the Nation (SWAN).\n", "versions": [{"version": "v1", "created": "Tue, 31 Oct 2017 20:42:02 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 14:30:28 GMT"}, {"version": "v3", "created": "Tue, 7 Jul 2020 22:17:09 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 15:07:54 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Zhang", "Anru R.", ""], ["Chen", "Kehui", ""]]}, {"id": "1711.00171", "submitter": "Indranil Ghosh", "authors": "Indranil Ghosh, Saralees Nadarajah", "title": "On some further properties and application of Weibull-R family of\n  distributions", "comments": "Short communication type paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide some new results for the Weibull-R family of\ndistributions (Alzaghal, Ghosh and Alzaatreh (2016)). We derive some new\nstructural properties of the Weibull-R family of distributions. We provide\nvarious characterizations of the family via conditional moments, some functions\nof order statistics and via record values.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 02:24:18 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Ghosh", "Indranil", ""], ["Nadarajah", "Saralees", ""]]}, {"id": "1711.00217", "submitter": "Guangming Pan", "authors": "Tony Cai, Xiao Han and Guangming Pan", "title": "Limiting Laws for Divergent Spiked Eigenvalues and Largest Non-spiked\n  Eigenvalue of Sample Covariance Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the asymptotic distributions of the spiked eigenvalues and the\nlargest nonspiked eigenvalue of the sample covariance matrix under a general\ncovariance matrix model with divergent spiked eigenvalues, while the other\neigenvalues are bounded but otherwise arbitrary. The limiting normal\ndistribution for the spiked sample eigenvalues is established. It has distinct\nfeatures that the asymptotic mean relies on not only the population spikes but\nalso the nonspikes and that the asymptotic variance in general depends on the\npopulation eigenvectors. In addition, the limiting Tracy-Widom law for the\nlargest nonspiked sample eigenvalue is obtained.\n  Estimation of the number of spikes and the convergence of the leading\neigenvectors are also considered. The results hold even when the number of the\nspikes diverges. As a key technical tool, we develop a Central Limit Theorem\nfor a type of random quadratic forms where the random vectors and random\nmatrices involved are dependent. This result can be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 06:22:45 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 03:22:32 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Cai", "Tony", ""], ["Han", "Xiao", ""], ["Pan", "Guangming", ""]]}, {"id": "1711.00303", "submitter": "Farkhondeh Sajadi Dr.", "authors": "Farkhondeh A. Sajadi", "title": "Assessing the reliability polynomial based on percolation theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST physics.soc-ph stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the robustness of network topologies. We use the\nconcept of percolation as measuring tool to assess the reliability polynomial\nof those systems which can be modeled as a general inhomogeneous random graph\nas well as scale-free random graph.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 12:05:46 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Sajadi", "Farkhondeh A.", ""]]}, {"id": "1711.00342", "submitter": "Lester Mackey", "authors": "Lester Mackey, Vasilis Syrgkanis, Ilias Zadik", "title": "Orthogonal Machine Learning: Power and Limitations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Double machine learning provides $\\sqrt{n}$-consistent estimates of\nparameters of interest even when high-dimensional or nonparametric nuisance\nparameters are estimated at an $n^{-1/4}$ rate. The key is to employ\nNeyman-orthogonal moment equations which are first-order insensitive to\nperturbations in the nuisance parameters. We show that the $n^{-1/4}$\nrequirement can be improved to $n^{-1/(2k+2)}$ by employing a $k$-th order\nnotion of orthogonality that grants robustness to more complex or\nhigher-dimensional nuisance parameters. In the partially linear regression\nsetting popular in causal inference, we show that we can construct second-order\northogonal moments if and only if the treatment residual is not normally\ndistributed. Our proof relies on Stein's lemma and may be of independent\ninterest. We conclude by demonstrating the robustness benefits of an explicit\ndoubly-orthogonal estimation procedure for treatment effect.\n", "versions": [{"version": "v1", "created": "Wed, 1 Nov 2017 13:42:54 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 22:49:28 GMT"}, {"version": "v3", "created": "Thu, 21 Dec 2017 03:16:15 GMT"}, {"version": "v4", "created": "Tue, 12 Jun 2018 02:25:49 GMT"}, {"version": "v5", "created": "Sat, 16 Jun 2018 05:07:05 GMT"}, {"version": "v6", "created": "Wed, 1 Aug 2018 18:40:19 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Mackey", "Lester", ""], ["Syrgkanis", "Vasilis", ""], ["Zadik", "Ilias", ""]]}, {"id": "1711.00608", "submitter": "Indranil Ghosh", "authors": "Indranil Ghosh, Saralees Nadarajah", "title": "An alternative approach for compatibility of two discrete conditional\n  distributions", "comments": null, "journal-ref": "Communications in Statistics-Theory and Methods, Volume 45, Pages\n  4416-4432, 2017", "doi": "10.1080/03610926.2014.921302", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional specification of distributions is a developing area with\nincreasing applications. In the finite discrete case, a variety of compatible\nconditions can be derived. In this paper, we propose an alternative approach to\nstudy the compatibility of two conditional probability distributions under the\nfinite discrete setup. A technique based on rank-based criterion is shown to be\nparticularly convenient for identifying compatible distributions corresponding\nto complete conditional specification including the case with zeros.The\nproposed methods are illustrated with several examples.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 03:47:46 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Ghosh", "Indranil", ""], ["Nadarajah", "Saralees", ""]]}, {"id": "1711.00668", "submitter": "Adrien Saumard", "authors": "Adrien Saumard and Jon A. Wellner", "title": "On the isoperimetric constant, covariance inequalities and\n  $L_p$-Poincar\\'{e} inequalities in dimension one", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Firstly, we derive in dimension one a new covariance inequality of\n$L_{1}-L_{\\infty}$ type that characterizes the isoperimetric constant as the\nbest constant achieving the inequality. Secondly, we generalize our result to\n$L_{p}-L_{q}$ bounds for the covariance. Consequently, we recover Cheeger's\ninequality without using the co-area formula. We also prove a generalized\nweighted Hardy type inequality that is needed to derive our covariance\ninequalities and that is of independent interest. Finally, we explore some\nconsequences of our covariance inequalities for $L_{p}$-Poincar\\'{e}\ninequalities and moment bounds. In particular, we obtain optimal constants in\ngeneral $L_{p}$-Poincar\\'{e} inequalities for measures with finite\nisoperimetric constant, thus generalizing in dimension one Cheeger's\ninequality, which is a $L_{p}$-Poincar\\'{e} inequality for $p=2$, to any real\n$p\\geq 1$.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 09:59:34 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 10:17:41 GMT"}, {"version": "v3", "created": "Wed, 7 Mar 2018 13:49:02 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Saumard", "Adrien", ""], ["Wellner", "Jon A.", ""]]}, {"id": "1711.00680", "submitter": "Sayan Ghosh", "authors": "S. Ghosh and P. Vellaisamy", "title": "Marginal Log-linear Parameters and their Collapsibility for Categorical\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider marginal log-linear models for parameterizing distributions on\nmultidimensional contingency tables. These models generalize ordinary\nlog-linear and multivariate logistic models, besides several others. First, we\nobtain some characteristic properties of marginal log-linear parameters. Then\nwe define collapsibility and strict collapsibility of these parameters in a\ngeneral sense. Several necessary and sufficient conditions for collapsibility\nand strict collapsibility are derived based on simple functions of only the\ncell probabilities, which are easily verifiable. These include results for an\narbitrary set of marginal log-linear parameters having some common effects. The\nconnections of strict collapsibility to various forms of independence of the\nvariables are explored. We analyze some real-life datasets to illustrate the\nabove results on collapsibility and strict collapsibility. Finally, we obtain a\nresult relating parameters with the same effect but different margins for an\narbitrary table, and demonstrate smoothness of marginal log-linear models under\ncollapsibility conditions.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 10:58:10 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 14:37:34 GMT"}, {"version": "v3", "created": "Fri, 23 Nov 2018 21:33:11 GMT"}, {"version": "v4", "created": "Thu, 24 Oct 2019 16:34:01 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Ghosh", "S.", ""], ["Vellaisamy", "P.", ""]]}, {"id": "1711.00708", "submitter": "Stefan Rass", "authors": "Stefan Rass", "title": "On Game-Theoretic Risk Management (Part Three) - Modeling and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The game-theoretic risk management framework put forth in the precursor\nreports \"Towards a Theory of Games with Payoffs that are\nProbability-Distributions\" (arXiv:1506.07368 [q-fin.EC]) and \"Algorithms to\nCompute Nash-Equilibria in Games with Distributions as Payoffs\"\n(arXiv:1511.08591v1 [q-fin.EC]) is herein concluded by discussing how to\nintegrate the previously developed theory into risk management processes. To\nthis end, we discuss how loss models (primarily but not exclusively\nnon-parametric) can be constructed from data. Furthermore, hints are given on\nhow a meaningful game theoretic model can be set up, and how it can be used in\nvarious stages of the ISO 27000 risk management process. Examples related to\nadvanced persistent threats and social engineering are given. We conclude by a\ndiscussion on the meaning and practical use of (mixed) Nash equilibria\nequilibria for risk management.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 12:44:35 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Rass", "Stefan", ""]]}, {"id": "1711.00748", "submitter": "Warren Lord", "authors": "Warren M. Lord, Jie Sun and Erik M. Bollt", "title": "Geometric k-nearest neighbor estimation of entropy and mutual\n  information", "comments": null, "journal-ref": null, "doi": "10.1063/1.5011683", "report-no": null, "categories": "math.ST cs.IT math.DS math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric estimation of mutual information is used in a wide range of\nscientific problems to quantify dependence between variables. The k-nearest\nneighbor (knn) methods are consistent, and therefore expected to work well for\nlarge sample size. These methods use geometrically regular local volume\nelements. This practice allows maximum localization of the volume elements, but\ncan also induce a bias due to a poor description of the local geometry of the\nunderlying probability measure. We introduce a new class of knn estimators that\nwe call geometric knn estimators (g-knn), which use more complex local volume\nelements to better model the local geometry of the probability measures. As an\nexample of this class of estimators, we develop a g-knn estimator of entropy\nand mutual information based on elliptical volume elements, capturing the local\nstretching and compression common to a wide range of dynamical systems\nattractors. A series of numerical examples in which the thickness of the\nunderlying distribution and the sample sizes are varied suggest that local\ngeometry is a source of problems for knn methods such as the\nKraskov-St\\\"{o}gbauer-Grassberger (KSG) estimator when local geometric effects\ncannot be removed by global preprocessing of the data. The g-knn method\nperforms well despite the manipulation of the local geometry. In addition, the\nexamples suggest that the g-knn estimators can be of particular relevance to\napplications in which the system is large, but data size is limited.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 14:03:37 GMT"}, {"version": "v2", "created": "Thu, 11 Jan 2018 19:50:44 GMT"}, {"version": "v3", "created": "Wed, 28 Feb 2018 22:11:36 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Lord", "Warren M.", ""], ["Sun", "Jie", ""], ["Bollt", "Erik M.", ""]]}, {"id": "1711.00912", "submitter": "Gunnar Taraldsen", "authors": "Gunnar Taraldsen and Bo H. Lindqvist", "title": "Conditional fiducial models", "comments": null, "journal-ref": null, "doi": "10.1016/j.jspi.2017.09.007", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fiducial is not unique in general, but we prove that in a restricted\nclass of models it is uniquely determined by the sampling distribution of the\ndata. It depends in particular not on the choice of a data generating model.\nThe arguments lead to a generalization of the classical formula found by Fisher\n(1930). The restricted class includes cases with discrete distributions, the\ncase of the shape parameter in the Gamma distribution, and also the case of the\ncorrelation coefficient in a bivariate Gaussian model. One of the examples can\nalso be used in a pedagogical context to demonstrate possible difficulties with\nlikelihood-, Bayesian-, and bootstrap-inference. Examples that demonstrate\nnon-uniqueness are also presented. It is explained that they can be seen as\ncases with restrictions on the parameter space. Motivated by this the concept\nof a conditional fiducial model is introduced. This class of models includes\nthe common case of iid samples from a one-parameter model investigated by\nHannig (2013), the structural group models investigated by Fraser (1968), and\nalso certain models discussed by Fisher (1973) in his final writing on the\nsubject.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 19:59:02 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Taraldsen", "Gunnar", ""], ["Lindqvist", "Bo H.", ""]]}, {"id": "1711.00949", "submitter": "Yoshikazu Terada", "authors": "Yoshikazu Terada and Hidetoshi Shimodaira", "title": "Selective inference for the problem of regions via multiscale bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general approach to selective inference is considered for hypothesis\ntesting of the null hypothesis represented as an arbitrary shaped region in the\nparameter space of multivariate normal model. This approach is useful for\nhierarchical clustering where confidence levels of clusters are calculated only\nfor those appeared in the dendrogram, thus subject to heavy selection bias. Our\ncomputation is based on a raw confidence measure, called bootstrap probability,\nwhich is easily obtained by counting how many times the same cluster appears in\nbootstrap replicates of the dendrogram. We adjust the bias of the bootstrap\nprobability by utilizing the scaling-law in terms of geometric quantities of\nthe region in the abstract parameter space, namely, signed distance and mean\ncurvature. Although this idea has been used for non-selective inference of\nhierarchical clustering, its selective inference version has not been discussed\nin the literature. Our bias-corrected $p$-values are asymptotically\nsecond-order accurate in the large sample theory of smooth boundary surfaces of\nregions, and they are also justified for nonsmooth surfaces such as polyhedral\ncones. The $p$-values are asymptotically equivalent to those of the iterated\nbootstrap but with less computation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Nov 2017 21:39:27 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 03:10:24 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Terada", "Yoshikazu", ""], ["Shimodaira", "Hidetoshi", ""]]}, {"id": "1711.00991", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini, Bryon Aragam, Qing Zhou", "title": "The neighborhood lattice for encoding partial correlations in a Hilbert\n  space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neighborhood regression has been a successful approach in graphical and\nstructural equation modeling, with applications to learning undirected and\ndirected graphical models. We extend these ideas by defining and studying an\nalgebraic structure called the neighborhood lattice based on a generalized\nnotion of neighborhood regression. We show that this algebraic structure has\nthe potential to provide an economic encoding of all conditional independence\nstatements in a Gaussian distribution (or conditional uncorrelatedness in\ngeneral), even in the cases where no graphical model exists that could\n\"perfectly\" encode all such statements. We study the computational complexity\nof computing these structures and show that under a sparsity assumption, they\ncan be computed in polynomial time, even in the absence of the assumption of\nperfectness to a graph. On the other hand, assuming perfectness, we show how\nthese neighborhood lattices may be \"graphically\" computed using the separation\nproperties of the so-called partial correlation graph. We also draw connections\nwith directed acyclic graphical models and Bayesian networks. We derive these\nresults using an abstract generalization of partial uncorrelatedness, called\npartial orthogonality, which allows us to use algebraic properties of\nprojection operators on Hilbert spaces to significantly simplify and extend\nexisting ideas and arguments. Consequently, our results apply to a wide range\nof random objects and data structures, such as random vectors, data matrices,\nand functions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 01:45:53 GMT"}, {"version": "v2", "created": "Wed, 6 Feb 2019 06:05:21 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Amini", "Arash A.", ""], ["Aragam", "Bryon", ""], ["Zhou", "Qing", ""]]}, {"id": "1711.01070", "submitter": "Theofanis Sapatinas", "authors": "Dimitrios Pilavakis, Efstathios Paparoditis, Theofanis Sapatinas", "title": "Moving Block and Tapered Block Bootstrap for Functional Time Series with\n  an Application to the K-Sample Mean Problem", "comments": "29 pages, 1 figure, 1 table (To appear in Bernoulli)", "journal-ref": "Bernoulli, Vol. 25, 3496-3526 (2019)", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider infinite-dimensional Hilbert space-valued random variables that\nare assumed to be temporal dependent in a broad sense. We prove a central limit\ntheorem for the moving block bootstrap and for the tapered block bootstrap, and\nshow that these block bootstrap procedures also provide consistent estimators\nof the long run covariance operator. Furthermore, we consider block\nbootstrap-based procedures for fully functional testing of the equality of mean\nfunctions between several independent functional time series. We establish\nvalidity of the block bootstrap methods in approximating the distribution of\nthe statistic of interest under the null and show consistency of the block\nbootstrap-based tests under the alternative. The finite sample behaviour of the\nprocedures is investigated by means of simulations. An application to a\nreal-life dataset is also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 09:20:22 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 12:38:34 GMT"}, {"version": "v3", "created": "Mon, 10 Dec 2018 09:00:30 GMT"}], "update_date": "2019-10-24", "authors_parsed": [["Pilavakis", "Dimitrios", ""], ["Paparoditis", "Efstathios", ""], ["Sapatinas", "Theofanis", ""]]}, {"id": "1711.01083", "submitter": "Artyom Kovalevskii", "authors": "Mikhail Chebunin and Artyom Kovalevskii", "title": "A statistical test for the Zipf's law by deviations from the Heaps' law", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a probabilistic model of an artistic text: words of the text are\nchosen independently of each other in accordance with a discrete probability\ndistribution on an infinite dictionary. The words are enumerated 1, 2,\n$\\ldots$, and the probability of appearing the $i$'th word is asymptotically a\npower function. Bahadur proved that in this case the number of different words\ndepends on the length of the text is asymptotically a power function, too. On\nthe other hand, in the applied statistics community, there exist statements\nsupported by empirical observations, the Zipf's and the Heaps' laws. We\nhighlight the links between Bahadur results and Zipf's/Heaps' laws, and\nintroduce and analyse a corresponding statistical test.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 09:56:22 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 07:31:20 GMT"}, {"version": "v3", "created": "Wed, 1 May 2019 15:27:50 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Chebunin", "Mikhail", ""], ["Kovalevskii", "Artyom", ""]]}, {"id": "1711.01280", "submitter": "Georgia Papadogeorgou", "authors": "Georgia Papadogeorgou, Fabrizia Mealli, Corwin M. Zigler", "title": "Causal inference for interfering units with cluster and population level\n  treatment allocation programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interference arises when an individual's potential outcome depends on the\nindividual treatment level, but also on the treatment level of others. A common\nassumption in the causal inference literature in the presence of interference\nis partial interference, implying that the population can be partitioned in\nclusters of individuals whose potential outcomes only depend on the treatment\nof units within the same cluster. Previous literature has defined average\npotential outcomes under counterfactual scenarios where treatments are randomly\nallocated to units within a cluster. However, within clusters there may be\nunits that are more or less likely to receive treatment based on covariates or\nneighbors' treatment. We define new estimands that describe average potential\noutcomes for realistic counterfactual treatment allocation programs, extending\nexisting estimands to take into consideration the units' covariates and\ndependence between units' treatment assignment. We further propose entirely new\nestimands for population-level interventions over the collection of clusters,\nwhich correspond in the motivating setting to regulations at the federal (vs.\ncluster or regional) level. We discuss these estimands, propose unbiased\nestimators and derive asymptotic results as the number of clusters grows.\nFinally, we estimate effects in a comparative effectiveness study of power\nplant emission reduction technologies on ambient ozone pollution.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 18:01:22 GMT"}, {"version": "v2", "created": "Mon, 14 May 2018 14:46:47 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Papadogeorgou", "Georgia", ""], ["Mealli", "Fabrizia", ""], ["Zigler", "Corwin M.", ""]]}, {"id": "1711.01366", "submitter": "Maxim Savelov", "authors": "M.P. Savelov", "title": "Sequential two-fold Pearson chi-squared test and tails of the Bessel\n  process distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find asymptotic formulas for error probabilities of two-fold Pearson\ngoodness-of-fit test as functions of two critical levels. These results may be\nreformulated in terms of tails of two-dimensional distributions of the Bessel\nprocess. Necessary properties of the Infeld function are obtained.\n", "versions": [{"version": "v1", "created": "Fri, 3 Nov 2017 23:58:57 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Savelov", "M. P.", ""]]}, {"id": "1711.01501", "submitter": "Luiz F. O. Chamon", "authors": "Luiz F. O. Chamon and Alejandro Ribeiro", "title": "Approximate Supermodularity Bounds for Experimental Design", "comments": "15 pages, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work provides performance guarantees for the greedy solution of\nexperimental design problems. In particular, it focuses on A- and E-optimal\ndesigns, for which typical guarantees do not apply since the mean-square error\nand the maximum eigenvalue of the estimation error covariance matrix are not\nsupermodular. To do so, it leverages the concept of approximate supermodularity\nto derive non-asymptotic worst-case suboptimality bounds for these greedy\nsolutions. These bounds reveal that as the SNR of the experiments decreases,\nthese cost functions behave increasingly as supermodular functions. As such,\ngreedy A- and E-optimal designs approach (1-1/e)-optimality. These results\nreconcile the empirical success of greedy experimental design with the\nnon-supermodularity of the A- and E-optimality criteria.\n", "versions": [{"version": "v1", "created": "Sat, 4 Nov 2017 22:28:01 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 21:14:32 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Chamon", "Luiz F. O.", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1711.01542", "submitter": "Saman Hosseini", "authors": "Saman Hosseini, Parviz Nasiri, Dler Hussein Kadir, and Sharad Damodar\n  Gore", "title": "Some Investigations about the Properties of Maximum Likelihood\n  Estimations Based on Lower Record Values for a Sub-Family of the Exponential\n  Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, in this paper it has been considered a sub family of exponential\nfamily. Maximum likelihood estimations (MLE) for the parameter of this family,\nprobability density function, and cumulative density function based on a sample\nand based on lower record values have been obtained. It has been considered\nMean Square Error (MSE) as a criterion for determining which is better in\ndifferent situations. Additionally, it has been proved some theories about the\nrelations between MLE based on lower record values and based on a random\nsample. Also, some interesting asymptotically properties for these estimations\nhave been shown during some theories.\n", "versions": [{"version": "v1", "created": "Sun, 5 Nov 2017 06:42:58 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 16:27:58 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Hosseini", "Saman", ""], ["Nasiri", "Parviz", ""], ["Kadir", "Dler Hussein", ""], ["Gore", "Sharad Damodar", ""]]}, {"id": "1711.01682", "submitter": "Andrea Montanari", "authors": "Andrea Montanari and Ramji Venkataramanan", "title": "Estimation of Low-Rank Matrices via Approximate Message Passing", "comments": "76 pages, 6 pdf figures; Version 4 expands the introductory material\n  and the applications to statistical inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of estimating a low-rank matrix when its entries are\nperturbed by Gaussian noise. If the empirical distribution of the entries of\nthe spikes is known, optimal estimators that exploit this knowledge can\nsubstantially outperform simple spectral approaches. Recent work characterizes\nthe asymptotic accuracy of Bayes-optimal estimators in the high-dimensional\nlimit. In this paper we present a practical algorithm that can achieve\nBayes-optimal accuracy above the spectral threshold. A bold conjecture from\nstatistical physics posits that no polynomial-time algorithm achieves optimal\nerror below the same threshold (unless the best estimator is trivial). Our\napproach uses Approximate Message Passing (AMP) in conjunction with a spectral\ninitialization. AMP algorithms have proved successful in a variety of\nstatistical estimation tasks, and are amenable to exact asymptotic analysis via\nstate evolution. Unfortunately, state evolution is uninformative when the\nalgorithm is initialized near an unstable fixed point, as often happens in\nlow-rank matrix estimation. We develop a new analysis of AMP that allows for\nspectral initializations. Our main theorem is general and applies beyond matrix\nestimation. However, we use it to derive detailed predictions for the problem\nof estimating a rank-one matrix in noise. Special cases of this problem are\nclosely related---via universality arguments---to the network community\ndetection problem for two asymmetric communities. For general rank-one models,\nwe show that AMP can be used to construct confidence intervals and control\nfalse discovery rate. We provide illustrations of the general methodology by\nconsidering the cases of sparse low-rank matrices and of block-constant\nlow-rank matrices with symmetric blocks (we refer to the latter as to the\n`Gaussian Block Model').\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 00:30:08 GMT"}, {"version": "v2", "created": "Tue, 2 Jan 2018 00:28:54 GMT"}, {"version": "v3", "created": "Sat, 19 May 2018 19:04:32 GMT"}, {"version": "v4", "created": "Wed, 7 Aug 2019 17:15:36 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Montanari", "Andrea", ""], ["Venkataramanan", "Ramji", ""]]}, {"id": "1711.01739", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Rheanna Mainzer", "title": "Two sources of poor coverage of confidence intervals after model\n  selection", "comments": null, "journal-ref": "Statistics and Probability Letters 2018", "doi": "10.1016/j.spl.2018.05.001", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare the following two sources of poor coverage of post-model-selection\nconfidence intervals: the preliminary data-based model selection sometimes\nchooses the wrong model and the data used to choose the model is re-used for\nthe construction of the confidence interval.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 06:19:20 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Kabaila", "Paul", ""], ["Mainzer", "Rheanna", ""]]}, {"id": "1711.01776", "submitter": "Reinhard  H\\\"opfner", "authors": "Reinhard H\\\"opfner and Carina Zeller", "title": "LAMN in a class of parametric models for null recurrent diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical models for one-dimensional diffusions which are\nrecurrent null. A first parameter in the drift is the principal one, and\ndetermines regular varying rates of convergence for the score and the\ninformation process. A finite number of other parameters, of secondary\nimportance, introduces additional flexibility for the modelization of the\ndrift, and does not perturb the null recurrent behaviour. Under time-continuous\nobservation we obtain local asymptotic mixed normality (LAMN), state a local\nasymptotic minimax bound, and specify asymptotically optimal estimators.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 08:18:29 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["H\u00f6pfner", "Reinhard", ""], ["Zeller", "Carina", ""]]}, {"id": "1711.01835", "submitter": "Ansgar Steland", "authors": "Ansgar Steland and Rainer von Sachs", "title": "Asymptotics for high-dimensional covariance matrices and quadratic forms\n  with applications to the trace functional and shrinkage", "comments": "42 pages", "journal-ref": "Stochastic Processes and Their Applications, Volume 128, Issue 8,\n  August 2018, Pages 2816-2855", "doi": "10.1016/j.spa.2017.10.007", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish large sample approximations for an arbitray number of bilinear\nforms of the sample variance-covariance matrix of a high-dimensional vector\ntime series using $ \\ell_1$-bounded and small $\\ell_2$-bounded weighting\nvectors. Estimation of the asymptotic covariance structure is also discussed.\nThe results hold true without any constraint on the dimension, the number of\nforms and the sample size or their ratios. Concrete and potential applications\nare widespread and cover high-dimensional data science problems such as tests\nfor large numbers of covariances, sparse portfolio optimization and projections\nonto sparse principal components or more general spanning sets as frequently\nconsidered, e.g. in classification and dictionary learning. As two specific\napplications of our results, we study in greater detail the asymptotics of the\ntrace functional and shrinkage estimation of covariance matrices. In shrinkage\nestimation, it turns out that the asymptotics differs for weighting vectors\nbounded away from orthogonaliy and nearly orthogonal ones in the sense that\ntheir inner product converges to 0.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 11:18:55 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Steland", "Ansgar", ""], ["von Sachs", "Rainer", ""]]}, {"id": "1711.02064", "submitter": "Bo Henry Lindqvist", "authors": "Bo H. Lindqvist and Gunnar Taraldsen", "title": "On the proper treatment of improper distributions", "comments": "Journal of Statistical Planning and Inference, 2017", "journal-ref": null, "doi": "10.1016/j.jspi.2017.09.008", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The axiomatic foundation of probability theory presented by Kolmogorov has\nbeen the basis of modern theory for probability and statistics. In certain\napplications it is, however, necessary or convenient to allow improper\n(unbounded) distributions, which is often done without a theoretical\nfoundation. The paper reviews a recent theory which includes improper\ndistributions, and which is related to Renyi's theory of conditional\nprobability spaces. It is in particular demonstrated how the theory leads to\nsimple explanations of apparent paradoxes known from the Bayesian literature.\nSeveral examples from statistical practice with improper distributions are\ndiscussed in light of the given theoretical results, which also include a\nrecent theory of convergence of proper distributions to improper ones.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 18:20:08 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Lindqvist", "Bo H.", ""], ["Taraldsen", "Gunnar", ""]]}, {"id": "1711.02123", "submitter": "Dena Asta", "authors": "Cosma Rohilla Shalizi and Dena Asta", "title": "Consistency of Maximum Likelihood for Continuous-Space Network Models", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network analysis needs tools to infer distributions over graphs of arbitrary\nsize from a single graph. Assuming the distribution is generated by a\ncontinuous latent space model which obeys certain natural symmetry and\nsmoothness properties, we establish three levels of consistency for\nnon-parametric maximum likelihood inference as the number of nodes grows: (i)\nthe estimated locations of all nodes converge in probability on their true\nlocations; (ii) the distribution over locations in the latent space converges\non the true distribution; and (iii) the distribution over graphs of arbitrary\nsize converges.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:15:52 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 19:42:11 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Shalizi", "Cosma Rohilla", ""], ["Asta", "Dena", ""]]}, {"id": "1711.02140", "submitter": "Matyas Barczy", "authors": "Matyas Barczy, Mohamed Ben Alaya, Ahmed Kebaier, Gyula Pap", "title": "Asymptotic properties of maximum likelihood estimator for the growth\n  rate of a stable CIR process based on continuous time observations", "comments": "47 pages. In Appendices we recall some notions and statements from\n  arXiv:1509.08869 and arXiv:1609.05865", "journal-ref": "Statistics 53(3), (2019), 533-568", "doi": null, "report-no": null, "categories": "math.ST math.PR q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stable Cox--Ingersoll--Ross process driven by a standard Wiener\nprocess and a spectrally positive strictly stable L\\'evy process, and we study\nasymptotic properties of the maximum likelihood estimator (MLE) for its growth\nrate based on continuous time observations. We distinguish three cases:\nsubcritical, critical and supercritical. In all cases we prove strong\nconsistency of the MLE in question, in the subcritical case asymptotic\nnormality, and in the supercritical case asymptotic mixed normality are shown\nas well. In the critical case the description of the asymptotic behavior of the\nMLE in question remains open.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:55:34 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 18:50:45 GMT"}, {"version": "v3", "created": "Tue, 12 Feb 2019 19:55:30 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Barczy", "Matyas", ""], ["Alaya", "Mohamed Ben", ""], ["Kebaier", "Ahmed", ""], ["Pap", "Gyula", ""]]}, {"id": "1711.02141", "submitter": "Yanjun Han", "authors": "Yanjun Han, Jiantao Jiao, Tsachy Weissman, Yihong Wu", "title": "Optimal rates of entropy estimation over Lipschitz balls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimax estimation of the entropy of a density\nover Lipschitz balls. Dropping the usual assumption that the density is bounded\naway from zero, we obtain the minimax rates $(n\\ln n)^{-s/(s+d)} + n^{-1/2}$\nfor $0<s\\leq 2$ for densities supported on $[0,1]^d$, where $s$ is the\nsmoothness parameter and $n$ is the number of independent samples. We\ngeneralize the results to densities with unbounded support: given an Orlicz\nfunctions $\\Psi$ of rapid growth (such as the sub-exponential and sub-Gaussian\nclasses), the minimax rates for densities with bounded $\\Psi$-Orlicz norm\nincrease to $(n\\ln n)^{-s/(s+d)} (\\Psi^{-1}(n))^{d(1-d/p(s+d))} + n^{-1/2}$,\nwhere $p$ is the norm parameter in the Lipschitz ball. We also show that the\nintegral-form plug-in estimators with kernel density estimates fail to achieve\nthe minimax rates, and characterize their worst case performances over the\nLipschitz ball.\n  One of the key steps in analyzing the bias relies on a novel application of\nthe Hardy-Littlewood maximal inequality, which also leads to a new inequality\non the Fisher information that may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 19:56:46 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 09:23:01 GMT"}, {"version": "v3", "created": "Sat, 26 Oct 2019 21:25:16 GMT"}, {"version": "v4", "created": "Mon, 11 Nov 2019 00:19:33 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Han", "Yanjun", ""], ["Jiao", "Jiantao", ""], ["Weissman", "Tsachy", ""], ["Wu", "Yihong", ""]]}, {"id": "1711.02186", "submitter": "Shaofeng Zou", "authors": "Shaofeng Zou and Georgios Fellouris and Venugopal V. Veeravalli", "title": "Quickest Change Detection under Transient Dynamics: Theory and\n  Asymptotic Analysis", "comments": "IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of quickest change detection (QCD) under transient dynamics is\nstudied, where the change from the initial distribution to the final persistent\ndistribution does not happen instantaneously, but after a series of transient\nphases. The observations within the different phases are generated by different\ndistributions. The objective is to detect the change as quickly as possible,\nwhile controlling the average run length (ARL) to false alarm, when the\ndurations of the transient phases are completely unknown. Two algorithms are\nconsidered, the dynamic Cumulative Sum (CuSum) algorithm, proposed in earlier\nwork, and a newly constructed weighted dynamic CuSum algorithm. Both algorithms\nadmit recursions that facilitate their practical implementation, and they are\nadaptive to the unknown transient durations. Specifically, their asymptotic\noptimality is established with respect to both Lorden's and Pollak's criteria\nas the ARL to false alarm and the durations of the transient phases go to\ninfinity at any relative rate. Numerical results are provided to demonstrate\nthe adaptivity of the proposed algorithms, and to validate the theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 6 Nov 2017 21:44:22 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 15:28:34 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Zou", "Shaofeng", ""], ["Fellouris", "Georgios", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "1711.02504", "submitter": "Davy Paindaveine", "authors": "Davy Paindaveine, Thomas Verdebout", "title": "Detecting the direction of a signal on high-dimensional spheres:\n  Non-null and Le Cam optimality results", "comments": "47 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider one of the most important problems in directional statistics,\nnamely the problem of testing the null hypothesis that the spike direction\n$\\theta$ of a Fisher-von Mises-Langevin distribution on the $p$-dimensional\nunit hypersphere is equal to a given direction $\\theta_0$. After a reduction\nthrough invariance arguments, we derive local asymptotic normality (LAN)\nresults in a general high-dimensional framework where the dimension $p_n$ goes\nto infinity at an arbitrary rate with the sample size $n$, and where the\nconcentration $\\kappa_n$ behaves in a completely free way with $n$, which\noffers a spectrum of problems ranging from arbitrarily easy to arbitrarily\nchallenging ones. We identify various asymptotic regimes, depending on the\nconvergence/divergence properties of $(\\kappa_n)$, that yield different\ncontiguity rates and different limiting experiments. In each regime, we derive\nLe Cam optimal tests under specified $\\kappa_n$ and we compute, from the Le Cam\nthird lemma, asymptotic powers of the classical Watson test under contiguous\nalternatives. We further establish LAN results with respect to both spike\ndirection and concentration, which allows us to discuss optimality also under\nunspecified $\\kappa_n$. To investigate the non-null behavior of the Watson test\noutside the parametric framework above, we derive its local asymptotic powers\nthrough martingale CLTs in the broader, semiparametric, model of rotationally\nsymmetric distributions. A Monte Carlo study shows that the finite-sample\nbehaviors of the various tests remarkably agree with our asymptotic results.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 14:49:46 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 09:50:45 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Paindaveine", "Davy", ""], ["Verdebout", "Thomas", ""]]}, {"id": "1711.02582", "submitter": "Alexander D'Amour", "authors": "Alexander D'Amour, Peng Ding, Avi Feller, Lihua Lei and Jasjeet Sekhon", "title": "Overlap in Observational Studies with High-Dimensional Covariates", "comments": "To appear in Journal of Econometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating causal effects under exogeneity hinges on two key assumptions:\nunconfoundedness and overlap. Researchers often argue that unconfoundedness is\nmore plausible when more covariates are included in the analysis. Less\ndiscussed is the fact that covariate overlap is more difficult to satisfy in\nthis setting. In this paper, we explore the implications of overlap in\nobservational studies with high-dimensional covariates and formalize\ncurse-of-dimensionality argument, suggesting that these assumptions are\nstronger than investigators likely realize. Our key innovation is to explore\nhow strict overlap restricts global discrepancies between the covariate\ndistributions in the treated and control populations. Exploiting results from\ninformation theory, we derive explicit bounds on the average imbalance in\ncovariate means under strict overlap and show that these bounds become more\nrestrictive as the dimension grows large. We discuss how these implications\ninteract with assumptions and procedures commonly deployed in observational\ncausal inference, including sparsity and trimming.\n", "versions": [{"version": "v1", "created": "Tue, 7 Nov 2017 16:11:23 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 00:52:32 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2018 04:49:23 GMT"}, {"version": "v4", "created": "Fri, 3 Jan 2020 16:43:18 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["D'Amour", "Alexander", ""], ["Ding", "Peng", ""], ["Feller", "Avi", ""], ["Lei", "Lihua", ""], ["Sekhon", "Jasjeet", ""]]}, {"id": "1711.02834", "submitter": "Robert Lunde", "authors": "Robert Lunde and Cosma Rohilla Shalizi", "title": "Bootstrapping Generalization Error Bounds for Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of finding confidence intervals for the risk of\nforecasting the future of a stationary, ergodic stochastic process, using a\nmodel estimated from the past of the process. We show that a bootstrap\nprocedure provides valid confidence intervals for the risk, when the data\nsource is sufficiently mixing, and the loss function and the estimator are\nsuitably smooth. Autoregressive (AR(d)) models estimated by least squares obey\nthe necessary regularity conditions, even when mis-specified, and simulations\nshow that the finite- sample coverage of our bounds quickly converges to the\ntheoretical, asymptotic level. As an intermediate step, we derive sufficient\nconditions for asymptotic independence between empirical distribution functions\nformed by splitting a realization of a stochastic process, of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 05:17:15 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 20:34:32 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Lunde", "Robert", ""], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1711.02876", "submitter": "Paulo Serra", "authors": "Paulo Serra and Michel Mandjes", "title": "Dimension Estimation Using Random Connection Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information about intrinsic dimension is crucial to perform dimensionality\nreduction, compress information, design efficient algorithms, and do\nstatistical adaptation. In this paper we propose an estimator for the intrinsic\ndimension of a data set. The estimator is based on binary neighbourhood\ninformation about the observations in the form of two adjacency matrices, and\ndoes not require any explicit distance information. The underlying graph is\nmodelled according to a subset of a specific random connection model, sometimes\nreferred to as the Poisson blob model. Computationally the estimator scales\nlike n log n, and we specify its asymptotic distribution and rate of\nconvergence. A simulation study on both real and simulated data shows that our\napproach compares favourably with some competing methods from the literature,\nincluding approaches that rely on distance information.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 09:17:35 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Serra", "Paulo", ""], ["Mandjes", "Michel", ""]]}, {"id": "1711.03149", "submitter": "Botond Szabo", "authors": "Botond Szabo and Harry van Zanten", "title": "An asymptotic analysis of distributed nonparametric methods", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate and compare the fundamental performance of several distributed\nlearning methods that have been proposed recently. We do this in the context of\na distributed version of the classical signal-in-Gaussian-white-noise model,\nwhich serves as a benchmark model for studying performance in this setting. The\nresults show how the design and tuning of a distributed method can have great\nimpact on convergence rates and validity of uncertainty quantification.\nMoreover, we highlight the difficulty of designing nonparametric distributed\nprocedures that automatically adapt to smoothness.\n", "versions": [{"version": "v1", "created": "Wed, 8 Nov 2017 20:31:58 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Szabo", "Botond", ""], ["van Zanten", "Harry", ""]]}, {"id": "1711.03342", "submitter": "Yuta Koike", "authors": "Yuta Koike, Yuta Tanoue", "title": "Oracle inequalities for sign constrained generalized linear models", "comments": "16 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data have recently been analyzed because of data collection\ntechnology evolution. Although many methods have been developed to gain sparse\nrecovery in the past two decades, most of these methods require selection of\ntuning parameters. As a consequence of this feature, results obtained with\nthese methods heavily depend on the tuning. In this paper we study the\ntheoretical properties of sign-constrained generalized linear models with\nconvex loss function, which is one of the sparse regression methods without\ntuning parameters. Recent studies on this topic have shown that, in the case of\nlinear regression, sign-constrains alone could be as efficient as the oracle\nmethod if the design matrix enjoys a suitable assumption in addition to a\ntraditional compatibility condition. We generalize this kind of result to a\nmuch more general model which encompasses the logistic and quantile\nregressions. We also perform some numerical experiments to confirm theoretical\nfindings obtained in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 12:05:36 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Koike", "Yuta", ""], ["Tanoue", "Yuta", ""]]}, {"id": "1711.03459", "submitter": "Jacek Grela", "authors": "Jacek Grela, Maciej A. Nowak", "title": "Extreme matrices or how an exponential map links classical and free\n  extreme laws", "comments": "18 pages, 4 figure; replaced version with new results", "journal-ref": "Phys. Rev. E 102, 022109 (2020)", "doi": "10.1103/PhysRevE.102.022109", "report-no": null, "categories": "math-ph cond-mat.stat-mech math.MP math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the proposed by us thinning approach to describe extreme matrices, we\nfind an explicit exponentiation formula linking classical extreme laws of\nFr\\'echet, Gumbel and Weibull given by Fisher-Tippet-Gnedenko classification\nand free extreme laws of free Fr\\'echet, free Gumbel and free Weibull by Ben\nArous and Voiculescu [1]. We also develop an extreme random matrix formalism,\nin which refined questions about extreme matrices can be answered. In\nparticular, we demonstrate explicit calculations for several more or less known\nrandom matrix ensembles, providing examples of all three free extreme laws.\nFinally, we present an exact mapping, showing the equivalence of free extreme\nlaws to the Peak-Over-Threshold method in classical probability.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 16:29:51 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 10:58:07 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 12:58:15 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Grela", "Jacek", ""], ["Nowak", "Maciej A.", ""]]}, {"id": "1711.03613", "submitter": "Sai Li", "authors": "Sai Li", "title": "Debiasing the Debiased Lasso with Bootstrap", "comments": "Accepted version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistical inference for a single coordinate of regression\ncoefficients in high-dimensional linear models. Recently, the debiased\nestimators are popularly used for constructing confidence intervals and\nhypothesis testing in high-dimensional models. However, some representative\nnumerical experiments show that they tend to be biased for large coefficients,\nespecially when the number of large coefficients dominates the number of small\ncoefficients. In this paper, we propose a modified debiased Lasso estimator\nbased on bootstrap. Let us denote the proposed estimator BS-DB for short. We\nshow that, under the irrepresentable condition and other mild technical\nconditions, the BS-DB has smaller order of bias than the debiased Lasso in\nexistence of a large proportion of strong signals. If the irrepresentable\ncondition does not hold, the BS-DB is guaranteed to perform no worse than the\ndebiased Lasso asymptotically. Confidence intervals based on the BS-DB are\nproposed and proved to be asymptotically valid under mild conditions. Our study\non the inference problems integrates the properties of the Lasso on variable\nselection and estimation novelly. The superior performance of the BS-DB over\nthe debiased Lasso is demonstrated via extensive numerical studies.\n", "versions": [{"version": "v1", "created": "Thu, 9 Nov 2017 21:43:13 GMT"}, {"version": "v2", "created": "Sat, 17 Oct 2020 19:50:40 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Li", "Sai", ""]]}, {"id": "1711.03740", "submitter": "Yury Kutoyants", "authors": "S. Dachian, N. Kordzakhia, Yu.A. Kutoyants and A. Novikov", "title": "Estimation of Cusp Location of Stochastic Processes: a Survey", "comments": "22 pages,3 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a review of some recent results on estimation of location\nparameter for several models of observations with cusp-type singularity at the\nchange point. We suppose that the cusp-type models fit better to the real\nphenomena described usually by change point models. The list of models includes\nGaussian, inhomogeneous Poisson, ergodic diffusion processes, time series and\nthe classical case of i.i.d. observations. We describe the properties of the\nmaximum likelihood and Bayes estimators under some asymptotic assumptions. The\nasymptotic efficiency of estimators are discussed as well and the results of\nsome numerical simulations are presented. We provide some heuristic arguments\nwhich demonstrate the convergence of log-likelihood ratios in the models under\nconsideration to the fractional Brownian motion.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 09:31:56 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Dachian", "S.", ""], ["Kordzakhia", "N.", ""], ["Kutoyants", "Yu. A.", ""], ["Novikov", "A.", ""]]}, {"id": "1711.03783", "submitter": "Yunbin Zhao Y", "authors": "Yun-Bin Zhao and Duan Li", "title": "A Theoretical Analysis of Sparse Recovery Stability of Dantzig Selector\n  and LASSO", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dantzig selector (DS) and LASSO problems have attracted plenty of attention\nin statistical learning, sparse data recovery and mathematical optimization. In\nthis paper, we provide a theoretical analysis of the sparse recovery stability\nof these optimization problems in more general settings and from a new\nperspective. We establish recovery error bounds for these optimization problems\nunder a mild assumption called weak range space property of a transposed design\nmatrix. This assumption is less restrictive than the well known sparse recovery\nconditions such as restricted isometry property (RIP), null space property\n(NSP) or mutual coherence. In fact, our analysis indicates that this assumption\nis tight and cannot be relaxed for the standard DS problems in order to\nmaintain their sparse recovery stability. As a result, a series of new\nstability results for DS and LASSO have been established under various matrix\nproperties, including the RIP with constant $\\delta_{2k}< 1/\\sqrt{2}$ and the\n(constant-free) standard NSP of order $k.$ We prove that these matrix\nproperties can yield an identical recovery error bound for DS and LASSO with\nstability coefficients being measured by the so-called Robinson's constant,\ninstead of the conventional RIP or NSP constant. To our knowledge, this is the\nfirst time that the stability results with such a unified feature are\nestablished for DS and LASSO problems. Different from the standard analysis in\nthis area of research, our analysis is carried out deterministically, and the\nkey analytic tools used in our analysis include the error bound of linear\nsystems due to Hoffman and Robinson and polytope approximation of symmetric\nconvex bodies due to Barvinok.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 11:53:27 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Zhao", "Yun-Bin", ""], ["Li", "Duan", ""]]}, {"id": "1711.03890", "submitter": "Filip Elvander", "authors": "Filip Elvander, Andreas Jakobsson, and Johan Karlsson", "title": "Interpolation and Extrapolation of Toeplitz Matrices via Optimal Mass\n  Transport", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, vol. 66, no. 20, (2018),\n  pp. 5285 - 5298", "doi": "10.1109/TSP.2018.2866432", "report-no": null, "categories": "eess.SP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel method for quantifying distances between\nToeplitz structured covariance matrices. By exploiting the spectral\nrepresentation of Toeplitz matrices, the proposed distance measure is defined\nbased on an optimal mass transport problem in the spectral domain. This may\nthen be interpreted in the covariance domain, suggesting a natural way of\ninterpolating and extrapolating Toeplitz matrices, such that the positive\nsemi-definiteness and the Toeplitz structure of these matrices are preserved.\nThe proposed distance measure is also shown to be contractive with respect to\nboth additive and multiplicative noise, and thereby allows for a quantification\nof the decreased distance between signals when these are corrupted by noise.\nFinally, we illustrate how this approach can be used for several applications\nin signal processing. In particular, we consider interpolation and\nextrapolation of Toeplitz matrices, as well as clustering problems and tracking\nof slowly varying stochastic processes.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 15:46:36 GMT"}, {"version": "v2", "created": "Wed, 29 May 2019 18:08:41 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Elvander", "Filip", ""], ["Jakobsson", "Andreas", ""], ["Karlsson", "Johan", ""]]}, {"id": "1711.03908", "submitter": "Vishesh Karwa", "authors": "Vishesh Karwa and Salil Vadhan", "title": "Finite Sample Differentially Private Confidence Intervals", "comments": "Presented at TPDP 2017 and a shorter version to appear at ITCS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating finite sample confidence intervals of the\nmean of a normal population under the constraint of differential privacy. We\nconsider both the known and unknown variance cases and construct differentially\nprivate algorithms to estimate confidence intervals. Crucially, our algorithms\nguarantee a finite sample coverage, as opposed to an asymptotic coverage.\nUnlike most previous differentially private algorithms, we do not require the\ndomain of the samples to be bounded. We also prove lower bounds on the expected\nsize of any differentially private confidence set showing that our the\nparameters are optimal up to polylogarithmic factors.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 16:30:36 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Karwa", "Vishesh", ""], ["Vadhan", "Salil", ""]]}, {"id": "1711.03959", "submitter": "Mika Meitz", "authors": "Mika Meitz and Pentti Saikkonen", "title": "Testing for observation-dependent regime switching in mixture\n  autoregressive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing for regime switching when the regime switching probabilities are\nspecified either as constants (`mixture models') or are governed by a\nfinite-state Markov chain (`Markov switching models') are long-standing\nproblems that have also attracted recent interest. This paper considers testing\nfor regime switching when the regime switching probabilities are time-varying\nand depend on observed data (`observation-dependent regime switching').\nSpecifically, we consider the likelihood ratio test for observation-dependent\nregime switching in mixture autoregressive models. The testing problem is\nhighly nonstandard, involving unidentified nuisance parameters under the null,\nparameters on the boundary, singular information matrices, and higher-order\napproximations of the log-likelihood. We derive the asymptotic null\ndistribution of the likelihood ratio test statistic in a general mixture\nautoregressive setting using high-level conditions that allow for various forms\nof dependence of the regime switching probabilities on past observations, and\nwe illustrate the theory using two particular mixture autoregressive models.\nThe likelihood ratio test has a nonstandard asymptotic distribution that can\neasily be simulated, and Monte Carlo studies show the test to have satisfactory\nfinite sample size and power properties.\n", "versions": [{"version": "v1", "created": "Fri, 10 Nov 2017 18:40:36 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Meitz", "Mika", ""], ["Saikkonen", "Pentti", ""]]}, {"id": "1711.04145", "submitter": "Merle Behr", "authors": "Merle Behr and Axel Munk", "title": "Minimax estimation in linear models with unknown design over finite\n  alphabets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a minimax optimal estimation procedure for F and W in matrix\nvalued linear models Y = F W + Z where the parameter matrix W and the design\nmatrix F are unknown but the latter takes values in a known finite set. The\nproposed finite alphabet linear model is justified in a variety of\napplications, ranging from signal processing to cancer genetics. We show that\nthis allows to separate F and W uniquely under weak identifiability conditions,\na task which is not doable, in general. To this end we quantify in the\nnoiseless case, that is, Z = 0, the perturbation range of Y in order to obtain\nstable recovery of F and W. Based on this, we derive an iterative Lloyd's type\nestimation procedure that attains minimax estimation rates for W and F for\nGaussian error matrix Z. In contrast to the least squares solution the\nestimation procedure can be computed efficiently and scales linearly with the\ntotal number of observations. We confirm our theoretical results in a\nsimulation study and illustrate it with a genetic sequencing data example.\n", "versions": [{"version": "v1", "created": "Sat, 11 Nov 2017 14:52:48 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 18:11:23 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 21:09:12 GMT"}, {"version": "v4", "created": "Thu, 18 Feb 2021 12:19:58 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Behr", "Merle", ""], ["Munk", "Axel", ""]]}, {"id": "1711.04454", "submitter": "Pierre Menard", "authors": "Aur\\'elien Garivier (IMT), Pierre M\\'enard (IMT), Laurent Rossi (IMT),\n  Pierre Menard (IMT)", "title": "Thresholding Bandit for Dose-ranging: The Impact of Monotonicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the sample complexity of the thresholding bandit problem, with and\nwithout the assumption that the mean values of the arms are increasing. In each\ncase, we provide a lower bound valid for any risk $\\delta$ and any\n$\\delta$-correct algorithm; in addition, we propose an algorithm whose sample\ncomplexity is of the same order of magnitude for small risks. This work is\nmotivated by phase 1 clinical trials, a practically important setting where the\narm means are increasing by nature, and where no satisfactory solution is\navailable so far.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 07:36:01 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 08:38:39 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Garivier", "Aur\u00e9lien", "", "IMT"], ["M\u00e9nard", "Pierre", "", "IMT"], ["Rossi", "Laurent", "", "IMT"], ["Menard", "Pierre", "", "IMT"]]}, {"id": "1711.04462", "submitter": "Shogo Nakakita", "authors": "Shogo H. Nakakita, Masayuki Uchida", "title": "Adaptive estimation and noise detection for an ergodic diffusion with\n  observation noises", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We research adaptive maximum likelihood-type estimation for an ergodic\ndiffusion process where the observation is contaminated by noise. This\nmethodology leads to the asymptotic independence of the estimators for the\nvariance of observation noise, the diffusion parameter and the drift one of the\nlatent diffusion process. Moreover, it can lessen the computational burden\ncompared to simultaneous maximum likelihood-type estimation. In addition to\nadaptive estimation, we propose a test to see if noise exists or not, and\nanalyse real data as the example such that data contains observation noise with\nstatistical significance.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 08:00:32 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 11:53:10 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Nakakita", "Shogo H.", ""], ["Uchida", "Masayuki", ""]]}, {"id": "1711.04466", "submitter": "Yue Wang", "authors": "Yue Wang and Linbo Wang", "title": "Causal inference in degenerate systems: An impossibility result", "comments": null, "journal-ref": "Proceedings of the Twenty Third International Conference on\n  Artificial Intelligence and Statistics, PMLR 108:3383-3392, 2020", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal relationships among variables are commonly represented via directed\nacyclic graphs. There are many methods in the literature to quantify the\nstrength of arrows in a causal acyclic graph. These methods, however, have\nundesirable properties when the causal system represented by a directed acyclic\ngraph is degenerate. In this paper, we characterize a degenerate causal system\nusing multiplicity of Markov boundaries. We show that in this case, it is\nimpossible to find an identifiable quantitative measure of causal effects that\nsatisfy a set of natural criteria. To supplement the impossibility result, we\nalso develop algorithms to identify degenerate causal systems from observed\ndata. Performance of our algorithms is investigated through synthetic data\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 13 Nov 2017 08:28:27 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 01:13:30 GMT"}, {"version": "v3", "created": "Fri, 28 Feb 2020 11:30:07 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Wang", "Yue", ""], ["Wang", "Linbo", ""]]}, {"id": "1711.04934", "submitter": "Dong Xia", "authors": "Dong Xia and Ming Yuan and Cun-Hui Zhang", "title": "Statistically Optimal and Computationally Efficient Low Rank Tensor\n  Completion from Noisy Entries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT math.IT math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we develop methods for estimating a low rank tensor from\nnoisy observations on a subset of its entries to achieve both statistical and\ncomputational efficiencies. There have been a lot of recent interests in this\nproblem of noisy tensor completion. Much of the attention has been focused on\nthe fundamental computational challenges often associated with problems\ninvolving higher order tensors, yet very little is known about their\nstatistical performance. To fill in this void, in this article, we characterize\nthe fundamental statistical limits of noisy tensor completion by establishing\nminimax optimal rates of convergence for estimating a $k$th order low rank\ntensor under the general $\\ell_p$ ($1\\le p\\le 2$) norm which suggest\nsignificant room for improvement over the existing approaches. Furthermore, we\npropose a polynomial-time computable estimating procedure based upon power\niteration and a second-order spectral initialization that achieves the optimal\nrates of convergence. Our method is fairly easy to implement and numerical\nexperiments are presented to further demonstrate the practical merits of our\nestimator.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 03:46:05 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 20:09:57 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Xia", "Dong", ""], ["Yuan", "Ming", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1711.04952", "submitter": "Ilias Zadik", "authors": "David Gamarnik, Ilias Zadik", "title": "Sparse High-Dimensional Linear Regression. Algorithmic Barriers and a\n  Local Search Algorithm", "comments": "Added a result on the failure of the LASSO recovery mechanism in the\n  conjectured algorithmically hard regime $n<c n_{alg}$ and minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a sparse high dimensional regression model where the goal is to\nrecover a $k$-sparse unknown vector $\\beta^*$ from $n$ noisy linear\nobservations of the form $Y=X\\beta^*+W \\in \\mathbb{R}^n$ where $X \\in\n\\mathbb{R}^{n \\times p}$ has iid $N(0,1)$ entries and $W \\in \\mathbb{R}^n$ has\niid $N(0,\\sigma^2)$ entries. Under certain assumptions on the parameters, an\nintriguing assymptotic gap appears between the minimum value of $n$, call it\n$n^*$, for which the recovery is information theoretically possible, and the\nminimum value of $n$, call it $n_{\\mathrm{alg}}$, for which an efficient\nalgorithm is known to provably recover $\\beta^*$. In \\cite{gamarnikzadik} it\nwas conjectured that the gap is not artificial, in the sense that for sample\nsizes $n \\in [n^*,n_{\\mathrm{alg}}]$ the problem is algorithmically hard.\n  We support this conjecture in two ways. Firstly, we show that the optimal\nsolution of the LASSO provably fails to $\\ell_2$-stably recover the unknown\nvector $\\beta^*$ when $n \\in [n^*,c n_{\\mathrm{alg}}]$, for some sufficiently\nsmall constant $c>0$. Secondly, we establish that $n_{\\mathrm{alg}}$, up to a\nmultiplicative constant factor, is a phase transition point for the appearance\nof a certain Overlap Gap Property (OGP) over the space of $k$-sparse vectors.\nThe presence of such an Overlap Gap Property phase transition, which originates\nin statistical physics, is known to provide evidence of an algorithmic\nhardness. Finally we show that if $n>C n_{\\mathrm{alg}}$ for some large enough\nconstant $C>0$, a very simple algorithm based on a local search improvement\nrule is able both to $\\ell_2$-stably recover the unknown vector $\\beta^*$ and\nto infer correctly its support, adding it to the list of provably successful\nalgorithms for the high dimensional linear regression problem.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 05:20:20 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 17:03:34 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Gamarnik", "David", ""], ["Zadik", "Ilias", ""]]}, {"id": "1711.04973", "submitter": "Shujaat Khan Engr", "authors": "Shujaat Khan, Muhammad Usman, Imran Naseem, Roberto Togneri, Mohammed\n  Bennamoun", "title": "A Robust Variable Step Size Fractional Least Mean Square (RVSS-FLMS)\n  Algorithm", "comments": "15 pages, 3 figures, 13th IEEE Colloquium on Signal Processing & its\n  Applications (CSPA 2017)", "journal-ref": "2017 IEEE 13th International Colloquium on Signal Processing & its\n  Applications (CSPA), Batu Ferringhi, 2017, pp. 1-6", "doi": "10.1109/CSPA.2017.8064914", "report-no": null, "categories": "math.OC cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an adaptive framework for the variable step size of\nthe fractional least mean square (FLMS) algorithm. The proposed algorithm named\nthe robust variable step size-FLMS (RVSS-FLMS), dynamically updates the step\nsize of the FLMS to achieve high convergence rate with low steady state error.\nFor the evaluation purpose, the problem of system identification is considered.\nThe experiments clearly show that the proposed approach achieves better\nconvergence rate compared to the FLMS and adaptive step-size modified FLMS\n(AMFLMS).\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 06:44:28 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Khan", "Shujaat", ""], ["Usman", "Muhammad", ""], ["Naseem", "Imran", ""], ["Togneri", "Roberto", ""], ["Bennamoun", "Mohammed", ""]]}, {"id": "1711.04990", "submitter": "Laura Dumitrescu", "authors": "Laura Dumitrescu and Ioana Schiopu-Kratina", "title": "Strong consistency and optimality for generalized estimating equations\n  with stochastic covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we study the existence and strong consistency of GEE\nestimators, when the generalized estimating functions are martingales with\nrandom coefficients. Furthermore, we characterize estimating functions which\nare asymptotically optimal.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 08:10:52 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Dumitrescu", "Laura", ""], ["Schiopu-Kratina", "Ioana", ""]]}, {"id": "1711.05085", "submitter": "Chuancun Yin", "authors": "Xiaoqian Zhang, Chuancun Yin", "title": "The mixability of elliptical distributions with supermodular functions", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of $\\phi$-complete mixability and $\\phi$-joint mixability was\nfirst introduced in Bignozzi and Puccetti (2015), which is a direct extension\nof complete and joint mixability. Following Bignozzi and Puccetti (2015), we\nconsider two cases of $\\phi$ and investigate the $\\phi$-joint mixability for\nelliptical distributions and logarithmic elliptical distributions. We obtain a\nnecessary and sufficient condition for the $\\phi$-joint mixability of some\ndistributions and a sufficient condition for uniqueness of the center of\n$\\phi$-joint mixability for some elliptical distributions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 12:49:00 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2018 00:23:44 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Zhang", "Xiaoqian", ""], ["Yin", "Chuancun", ""]]}, {"id": "1711.05134", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko and Servet Martinez and Jaime San Martin", "title": "A Note on the Quasi-Stationary Distribution of the Shiryaev Martingale\n  on the Positive Half-Line", "comments": "To appear in Theory of Probability and Its Applications, 16 pages, 15\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain a closed-form formula for the quasi-stationary distribution of the\nclassical Shiryaev martingale diffusion considered on the positive half-line\n$[A,+\\infty)$ with $A>0$ fixed; the state space's left endpoint is assumed to\nbe the killing boundary. The formula is obtained analytically as the solution\nof the appropriate singular Sturm-Liouville problem; the latter was first\nconsidered in Section 7.8.2 of Collet et al. (2013), but has heretofore\nremained unsolved.\n", "versions": [{"version": "v1", "created": "Tue, 14 Nov 2017 14:58:41 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Martinez", "Servet", ""], ["Martin", "Jaime San", ""]]}, {"id": "1711.05360", "submitter": "Alexander Shkolnik", "authors": "Lisa Goldberg, Alex Papanicolaou and Alex Shkolnik", "title": "The Dispersion Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation error has plagued quantitative finance since Harry Markowitz\nlaunched modern portfolio theory in 1952. Using random matrix theory, we\ncharacterize a source of bias in the sample eigenvectors of financial\ncovariance matrices. Unchecked, the bias distorts weights of minimum variance\nportfolios and leads to risk forecasts that are severely biased downward. To\naddress these issues, we develop an eigenvector bias correction. Our approach\nis distinct from the regularization and eigenvalue shrinkage methods found in\nthe literature. We provide theoretical guarantees on the improvement our\ncorrection provides as well as estimation methods for computing the optimal\ncorrection from data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 00:03:58 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 00:59:40 GMT"}, {"version": "v3", "created": "Fri, 15 Dec 2017 06:03:18 GMT"}, {"version": "v4", "created": "Thu, 15 Feb 2018 17:16:12 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Goldberg", "Lisa", ""], ["Papanicolaou", "Alex", ""], ["Shkolnik", "Alex", ""]]}, {"id": "1711.05381", "submitter": "Wen-Xin Zhou", "authors": "Wen-Xin Zhou, Koushiki Bose, Jianqing Fan and Han Liu", "title": "A New Perspective on Robust $M$-Estimation: Finite Sample Theory and\n  Applications to Dependence-Adjusted Multiple Testing", "comments": "Ann. Statist. (in press)", "journal-ref": null, "doi": "10.1214/17-AOS1606", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy-tailed errors impair the accuracy of the least squares estimate, which\ncan be spoiled by a single grossly outlying observation. As argued in the\nseminal work of Peter Huber in 1973 [{\\it Ann. Statist.} {\\bf 1} (1973)\n799--821], robust alternatives to the method of least squares are sorely\nneeded. To achieve robustness against heavy-tailed sampling distributions, we\nrevisit the Huber estimator from a new perspective by letting the tuning\nparameter involved diverge with the sample size. In this paper, we develop\nnonasymptotic concentration results for such an adaptive Huber estimator,\nnamely, the Huber estimator with the tuning parameter adapted to sample size,\ndimension, and the variance of the noise. Specifically, we obtain a\nsub-Gaussian-type deviation inequality and a nonasymptotic Bahadur\nrepresentation when noise variables only have finite second moments. The\nnonasymptotic results further yield two conventional normal approximation\nresults that are of independent interest, the Berry-Esseen inequality and\nCram\\'er-type moderate deviation. As an important application to large-scale\nsimultaneous inference, we apply these robust normal approximation results to\nanalyze a dependence-adjusted multiple testing procedure for moderately\nheavy-tailed data. It is shown that the robust dependence-adjusted procedure\nasymptotically controls the overall false discovery proportion at the nominal\nlevel under mild moment conditions. Thorough numerical results on both\nsimulated and real datasets are also provided to back up our theory.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 02:15:03 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Zhou", "Wen-Xin", ""], ["Bose", "Koushiki", ""], ["Fan", "Jianqing", ""], ["Liu", "Han", ""]]}, {"id": "1711.05424", "submitter": "Andrea Montanari", "authors": "Gerard Ben Arous, Song Mei, Andrea Montanari, Mihai Nica", "title": "The landscape of the spiked tensor model", "comments": "40 pages, 20 pdf figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a large rank-one tensor ${\\boldsymbol\nu}^{\\otimes k}\\in({\\mathbb R}^{n})^{\\otimes k}$, $k\\ge 3$ in Gaussian noise.\nEarlier work characterized a critical signal-to-noise ratio $\\lambda_{Bayes}=\nO(1)$ above which an ideal estimator achieves strictly positive correlation\nwith the unknown vector of interest. Remarkably no polynomial-time algorithm is\nknown that achieved this goal unless $\\lambda\\ge C n^{(k-2)/4}$ and even\npowerful semidefinite programming relaxations appear to fail for $1\\ll\n\\lambda\\ll n^{(k-2)/4}$.\n  In order to elucidate this behavior, we consider the maximum likelihood\nestimator, which requires maximizing a degree-$k$ homogeneous polynomial over\nthe unit sphere in $n$ dimensions. We compute the expected number of critical\npoints and local maxima of this objective function and show that it is\nexponential in the dimensions $n$, and give exact formulas for the exponential\ngrowth rate. We show that (for $\\lambda$ larger than a constant) critical\npoints are either very close to the unknown vector ${\\boldsymbol u}$, or are\nconfined in a band of width $\\Theta(\\lambda^{-1/(k-1)})$ around the maximum\ncircle that is orthogonal to ${\\boldsymbol u}$. For local maxima, this band\nshrinks to be of size $\\Theta(\\lambda^{-1/(k-2)})$. These `uninformative' local\nmaxima are likely to cause the failure of optimization algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 06:23:43 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 06:34:40 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Arous", "Gerard Ben", ""], ["Mei", "Song", ""], ["Montanari", "Andrea", ""], ["Nica", "Mihai", ""]]}, {"id": "1711.05483", "submitter": "Xu Gao", "authors": "Xu Gao, Hernando Ombao, Daniel Gillen", "title": "Fisher information matrix of binary time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common approach to analyzing categorical correlated time series data is to\nfit a generalized linear model (GLM) with past data as covariate inputs. There\nremain challenges to conducting inference for short time series length. By\ntreating the historical data as covariate inputs, standard errors of estimates\nof GLM parameters computed using the empirical Fisher information do not fully\naccount the auto-correlation in the data. To overcome this serious limitation,\nwe derive the exact conditional Fisher information matrix of a general logistic\nautoregressive model with endogenous covariates for any series length $T$.\nMoreover, we also develop an iterative computational formula that allows for\nrelatively easy implementation of the proposed estimator. Our simulation\nstudies show that confidence intervals derived using the exact Fisher\ninformation matrix tend to be narrower than those utilizing the empirical\nFisher information matrix while maintaining type I error rates at or below\nnominal levels. Further, we establish that the exact Fisher information matrix\napproaches, as T tends to infinity, the asymptotic Fisher information matrix\npreviously derived for binary time series data. The developed exact conditional\nFisher information matrix is applied to time-series data on respiratory rate\namong a cohort of expectant mothers where it is found to provide narrower\nconfidence intervals for functionals of scientific interest and lead to greater\nstatistical power when compared to the empirical Fisher information matrix.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 10:08:27 GMT"}, {"version": "v2", "created": "Sun, 21 Oct 2018 20:37:35 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Gao", "Xu", ""], ["Ombao", "Hernando", ""], ["Gillen", "Daniel", ""]]}, {"id": "1711.05524", "submitter": "Junyong Park", "authors": "Amanda Plunkett, Junyong Park", "title": "Two-Sample Test for Sparse High Dimensional Multinomial Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider testing the equality of probability vectors of two\nindependent multinomial distributions in high dimension. The classical\nchi-square test may have some drawbacks in this case since many of cell counts\nmay be zero or may not be large enough. We propose a new test and show its\nasymptotic normality and the asymptotic power function. Based on the asymptotic\npower function, we present an application of our result to neighborhood type\ntest which has been previously studied, especially for the case of fairly small\n$p$-values. To compare the proposed test with existing tests, we provide\nnumerical studies including simulations and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 12:28:23 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Plunkett", "Amanda", ""], ["Park", "Junyong", ""]]}, {"id": "1711.05704", "submitter": "Kirsten Schorning", "authors": "Kirsten Schorning, Maria Konstantinou", "title": "Bayesian optimal designs for dose-response curves with common parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of determining not only an adequate dose but also a dosing\nfrequency of a drug arises frequently in Phase II clinical trials. This results\nin the comparison of models which have some parameters in common. Planning such\nstudies based on Bayesian optimal designs offers robustness to our conclusions\nsince these designs, unlike locally optimal designs, are efficient even if the\nparameters are misspecified. In this paper we develop approximate design theory\nfor Bayesian $D$-optimality for nonlinear regression models with common\nparameters and investigate the cases of common location or common location and\nscale parameters separately. Analytical characterisations of saturated Bayesian\n$D$-optimal designs are derived for frequently used dose-response models and\nthe advantages of our results are illustrated via a numerical investigation.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 17:59:02 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Schorning", "Kirsten", ""], ["Konstantinou", "Maria", ""]]}, {"id": "1711.05724", "submitter": "James Johndrow", "authors": "James E. Johndrow and Julia A. Palacios", "title": "Exact Limits of Inference in Coalescent Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovery of population size history from molecular sequence data is an\nimportant problem in population genetics. Inference commonly relies on a\ncoalescent model linking the population size history to genealogies. The high\ncomputational cost of estimating parameters from these models usually compels\nresearchers to select a subset of the available data or to rely on\nnon-sufficient summary statistics for statistical inference. We consider the\nproblem of recovering the true population size history from two possible\nalternatives on the basis of coalescent time data. We give exact expressions\nfor the probability of selecting the correct alternative in a variety of\nbiologically interesting cases as a function of the separation between the\nalternative size histories, the number of individuals, loci, and the sampling\ntimes. The results are applied to human population history. This work has\nsignificant implications for optimal design when the inferential goal is to\ntest scientific hypotheses about population size trajectories in coalescent\nmodels with and without recombination.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 18:43:44 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 06:36:45 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Johndrow", "James E.", ""], ["Palacios", "Julia A.", ""]]}, {"id": "1711.05840", "submitter": "Mehmet Niyazi Cankaya mehmetn", "authors": "Mehmet Niyazi Cankaya, Jan Korbel", "title": "Least informative distributions in Maximum q-log-likelihood estimation", "comments": "16 pages; 12 Figures", "journal-ref": "Physica A 509 (2018), 140-150", "doi": "10.1016/j.physa.2018.06.004", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the Maximum $q$-log-likelihood estimation for Least informative\ndistributions (LID) in order to estimate the parameters in probability density\nfunctions (PDFs) efficiently and robustly when data include outlier(s). LIDs\nare derived by using convex combinations of two PDFs,\n$f_\\epsilon=(1-\\epsilon)f_0+\\epsilon f_1$. A convex combination of two PDFs is\nconsidered as a contamination $f_1$ as outlier(s) to underlying $f_0$\ndistributions and $f_\\epsilon$ is a contaminated distribution. The optimal\ncriterion is obtained by minimizing the change of Maximum q-log-likelihood\nfunction when the data have slightly more contamination. In this paper, we make\na comparison among ordinary Maximum likelihood, Maximum q-likelihood\nestimations, LIDs based on $\\log_q$ and Huber M-estimation. Akaike and Bayesian\ninformation criterions (AIC and BIC) based on $\\log_q$ and LID are proposed to\nassess the fitting performance of functions. Real data sets are applied to test\nthe fitting performance of estimating functions that include shape, scale and\nlocation parameters.\n", "versions": [{"version": "v1", "created": "Wed, 15 Nov 2017 22:59:15 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Cankaya", "Mehmet Niyazi", ""], ["Korbel", "Jan", ""]]}, {"id": "1711.05869", "submitter": "Franz J. Kir\\'aly", "authors": "Samuel Burkart and Franz J Kir\\'aly", "title": "Predictive Independence Testing, Predictive Conditional Independence\n  Testing, and Predictive Graphical Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing (conditional) independence of multivariate random variables is a task\ncentral to statistical inference and modelling in general - though\nunfortunately one for which to date there does not exist a practicable\nworkflow. State-of-art workflows suffer from the need for heuristic or\nsubjective manual choices, high computational complexity, or strong parametric\nassumptions.\n  We address these problems by establishing a theoretical link between\nmultivariate/conditional independence testing, and model comparison in the\nmultivariate predictive modelling aka supervised learning task. This link\nallows advances in the extensively studied supervised learning workflow to be\ndirectly transferred to independence testing workflows - including automated\ntuning of machine learning type which addresses the need for a heuristic\nchoice, the ability to quantitatively trade-off computational demand with\naccuracy, and the modern black-box philosophy for checking and interfacing.\n  As a practical implementation of this link between the two workflows, we\npresent a python package 'pcit', which implements our novel multivariate and\nconditional independence tests, interfacing the supervised learning API of the\nscikit-learn package. Theory and package also allow for straightforward\nindependence test based learning of graphical model structure.\n  We empirically show that our proposed predictive independence test outperform\nor are on par to current practice, and the derived graphical model structure\nlearning algorithms asymptotically recover the 'true' graph. This paper, and\nthe 'pcit' package accompanying it, thus provide powerful, scalable,\ngeneralizable, and easy-to-use methods for multivariate and conditional\nindependence testing, as well as for graphical model structure learning.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 00:37:34 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2018 20:32:52 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Burkart", "Samuel", ""], ["Kir\u00e1ly", "Franz J", ""]]}, {"id": "1711.06305", "submitter": "Nana Wang", "authors": "Nana Wang and Wolfgang Polonik", "title": "Neighborhood selection with application to social networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The topic of this paper is modeling and analyzing dependence in stochastic\nsocial networks. Using a latent variable block model allows the analysis of\ndependence between blocks via the analysis of a latent graphical model. Our\napproach to the analysis of the graphical model then is based on the idea\nunderlying the neighborhood selection scheme put forward by Meinshausen and\nB\\\"{u}hlmann (2006). However, because of the latent nature of our model,\nestimates have to be used in lieu of the unobserved variables. This leads to a\nnovel analysis of graphical models under uncertainty, in the spirit of\nRosenbaum et al. (2010), or Belloni et al. (2017). Lasso-based selectors, and a\nclass of Dantzig-type selectors are studied.\n", "versions": [{"version": "v1", "created": "Thu, 16 Nov 2017 20:10:06 GMT"}, {"version": "v2", "created": "Thu, 23 Nov 2017 08:31:19 GMT"}, {"version": "v3", "created": "Fri, 24 Aug 2018 02:08:42 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Wang", "Nana", ""], ["Polonik", "Wolfgang", ""]]}, {"id": "1711.06399", "submitter": "Fredrik S\\\"avje", "authors": "Fredrik S\\\"avje and Peter M. Aronow and Michael G. Hudgens", "title": "Average treatment effects in the presence of unknown interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate large-sample properties of treatment effect estimators under\nunknown interference in randomized experiments. The inferential target is a\ngeneralization of the average treatment effect estimand that marginalizes over\npotential spillover effects. We show that estimators commonly used to estimate\ntreatment effects under no interference are consistent for the generalized\nestimand for several common experimental designs under limited but otherwise\narbitrary and unknown interference. The rates of convergence depend on the rate\nat which the amount of interference grows and the degree to which it aligns\nwith dependencies in treatment assignment. Importantly for practitioners, the\nresults imply that if one erroneously assumes that units do not interfere in a\nsetting with limited, or even moderate, interference, standard estimators are\nnevertheless likely to be close to an average treatment effect if the sample is\nsufficiently large. Conventional confidence statements may, however, not be\naccurate.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 04:29:32 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 06:13:25 GMT"}, {"version": "v3", "created": "Mon, 17 Sep 2018 22:34:25 GMT"}, {"version": "v4", "created": "Wed, 23 Oct 2019 18:55:30 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["S\u00e4vje", "Fredrik", ""], ["Aronow", "Peter M.", ""], ["Hudgens", "Michael G.", ""]]}, {"id": "1711.06642", "submitter": "Richard Samworth", "authors": "Thomas B. Berrett and Richard J. Samworth", "title": "Nonparametric independence testing via mutual information", "comments": "46 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a test of independence of two multivariate random vectors, given a\nsample from the underlying population. Our approach, which we call MINT, is\nbased on the estimation of mutual information, whose decomposition into joint\nand marginal entropies facilitates the use of recently-developed efficient\nentropy estimators derived from nearest neighbour distances. The proposed\ncritical values, which may be obtained from simulation (in the case where one\nmarginal is known) or resampling, guarantee that the test has nominal size, and\nwe provide local power analyses, uniformly over classes of densities whose\nmutual information satisfies a lower bound. Our ideas may be extended to\nprovide a new goodness-of-fit tests of normal linear models based on assessing\nthe independence of our vector of covariates and an appropriately-defined\nnotion of an error vector. The theory is supported by numerical studies on both\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 17:38:50 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1711.06660", "submitter": "Matthew Reimherr", "authors": "Ardalan Mirshani, Matthew Reimherr, Aleksandra Slavkovic", "title": "Formal Privacy for Functional Data with Gaussian Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the rapid rise in statistical tools in Functional Data Analysis,\nwe consider the Gaussian mechanism for achieving differential privacy with\nparameter estimates taking values in a, potentially infinite-dimensional,\nseparable Banach space. Using classic results from probability theory, we show\nhow densities over function spaces can be utilized to achieve the desired\ndifferential privacy bounds. This extends prior results of Hall et al (2013) to\na much broader class of statistical estimates and summaries, including \"path\nlevel\" summaries, nonlinear functionals, and full function releases. By\nfocusing on Banach spaces, we provide a deeper picture of the challenges for\nprivacy with complex data, especially the role regularization plays in\nbalancing utility and privacy. Using an application to penalized smoothing, we\nexplicitly highlight this balance in the context of mean function estimation.\nSimulations and an application to diffusion tensor imaging are briefly\npresented, with extensive additions included in a supplement.\n", "versions": [{"version": "v1", "created": "Fri, 17 Nov 2017 18:34:01 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 06:19:05 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2019 01:20:44 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Mirshani", "Ardalan", ""], ["Reimherr", "Matthew", ""], ["Slavkovic", "Aleksandra", ""]]}, {"id": "1711.06808", "submitter": "Tavis Abrahamsen", "authors": "Tavis Abrahamsen and James P. Hobert", "title": "Fast Monte Carlo Markov chains for Bayesian shrinkage models with random\n  effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When performing Bayesian data analysis using a general linear mixed model,\nthe resulting posterior density is almost always analytically intractable.\nHowever, if proper conditionally conjugate priors are used, there is a simple\ntwo-block Gibbs sampler that is geometrically ergodic in nearly all practical\nsettings, including situations where $p > n$ (Abrahamsen and Hobert, 2017).\nUnfortunately, the (conditionally conjugate) multivariate normal prior on\n$\\beta$ does not perform well in the high-dimensional setting where $p \\gg n$.\nIn this paper, we consider an alternative model in which the multivariate\nnormal prior is replaced by the normal-gamma shrinkage prior developed by\nGriffin and Brown (2010). This change leads to a much more complex posterior\ndensity, and we develop a simple MCMC algorithm for exploring it. This\nalgorithm, which has both deterministic and random scan components, is easier\nto analyze than the more obvious three-step Gibbs sampler. Indeed, we prove\nthat the new algorithm is geometrically ergodic in most practical settings.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 03:57:28 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Abrahamsen", "Tavis", ""], ["Hobert", "James P.", ""]]}, {"id": "1711.06869", "submitter": "Inmo Jang", "authors": "Inmo Jang, Hyo-Sang Shin, Antonios Tsourdos", "title": "Bio-Inspired Local Information-Based Control for Probabilistic Swarm\n  Distribution Guidance", "comments": "Submitted to IEEE Transactions on Robotics", "journal-ref": "Published in Swarm Intelligence, 2018", "doi": "10.1007/s11721-018-0160-2", "report-no": null, "categories": "cs.MA math.OC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a task allocation problem for a large-scale robotic\nswarm, namely swarm distribution guidance problem. Unlike most of the existing\nframeworks handling this problem, the proposed framework suggests utilising\nlocal information available to generate its time-varying stochastic policies.\nAs each agent requires only local consistency on information with neighbouring\nagents, rather than the global consistency, the proposed framework offers\nvarious advantages, e.g., a shorter timescale for using new information and\npotential to incorporate an asynchronous decision-making process. We perform\ntheoretical analysis on the properties of the proposed framework. From the\nanalysis, it is proved that the framework can guarantee the convergence to the\ndesired density distribution even using local information while maintaining\nadvantages of global-information-based approaches. The design requirements for\nthese advantages are explicitly listed in this paper. This paper also provides\nspecific examples of how to implement the framework developed. The results of\nnumerical experiments confirm the effectiveness and comparability of the\nproposed framework, compared with the global-information-based framework.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 14:46:45 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Jang", "Inmo", ""], ["Shin", "Hyo-Sang", ""], ["Tsourdos", "Antonios", ""]]}, {"id": "1711.06926", "submitter": "William Weimin Yoo", "authors": "William Weimin Yoo and Aad W. van der Vaart", "title": "The Bayes Lepski's Method and Credible Bands through Volume of Tubular\n  Neighborhoods", "comments": "42 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a general class of priors based on random series basis expansion, we\ndevelop the Bayes Lepski's method to estimate unknown regression function. In\nthis approach, the series truncation point is determined based on a stopping\nrule that balances the posterior mean bias and the posterior standard\ndeviation. Equipped with this mechanism, we present a method to construct\nadaptive Bayesian credible bands, where this statistical task is reformulated\ninto a problem in geometry, and the band's radius is computed based on finding\nthe volume of certain tubular neighborhood embedded on a unit sphere. We\nconsider two special cases involving B-splines and wavelets, and discuss some\ninteresting consequences such as the uncertainty principle and self-similarity.\nLastly, we show how to program the Bayes Lepski stopping rule on a computer,\nand numerical simulations in conjunction with our theoretical investigations\nconcur that this is a promising Bayesian uncertainty quantification procedure.\n", "versions": [{"version": "v1", "created": "Sat, 18 Nov 2017 21:09:13 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Yoo", "William Weimin", ""], ["van der Vaart", "Aad W.", ""]]}, {"id": "1711.06952", "submitter": "Sunder Sethuraman", "authors": "Erik Davis, Sunder Sethuraman", "title": "Approximating geodesics via random points", "comments": "34 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a `cost' functional $F$ on paths $\\gamma$ in a domain\n$D\\subset\\mathbb{R}^d$, in the form $F(\\gamma) = \\int_0^1\nf(\\gamma(t),\\dot\\gamma(t))dt$, it is of interest to approximate its minimum\ncost and geodesic paths. Let $X_1,\\ldots, X_n$ be points drawn independently\nfrom $D$ according to a distribution with a density. Form a random geometric\ngraph on the points where $X_i$ and $X_j$ are connected when $0<|X_i -\nX_j|<\\epsilon$, and the length scale $\\epsilon=\\epsilon_n$ vanishes at a\nsuitable rate.\n  For a general class of functionals $F$, associated to Finsler and other\ndistances on $D$, using a probabilistic form of Gamma convergence, we show that\nthe minimum costs and geodesic paths, with respect to types of approximating\ndiscrete `cost' functionals, built from the random geometric graph, converge\nalmost surely in various senses to those corresponding to the continuum cost\n$F$, as the number of sample points diverges. In particular, the geodesic path\nconvergence shown appears to be among the first results of its kind.\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 01:54:54 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Davis", "Erik", ""], ["Sethuraman", "Sunder", ""]]}, {"id": "1711.06999", "submitter": "Daniele Durante", "authors": "Daniele Durante and Tommaso Rigon", "title": "Conditionally conjugate mean-field variational Bayes for logistic models", "comments": null, "journal-ref": "Statistical Science (2019). 34, 472-485", "doi": "10.1214/19-STS712", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) is a common strategy for approximate Bayesian\ninference, but simple methods are only available for specific classes of models\nincluding, in particular, representations having conditionally conjugate\nconstructions within an exponential family. Models with logit components are an\napparently notable exception to this class, due to the absence of conjugacy\nbetween the logistic likelihood and the Gaussian priors for the coefficients in\nthe linear predictor. To facilitate approximate inference within this widely\nused class of models, Jaakkola and Jordan (2000) proposed a simple variational\napproach which relies on a family of tangent quadratic lower bounds of logistic\nlog-likelihoods, thus restoring conjugacy between these approximate bounds and\nthe Gaussian priors. This strategy is still implemented successfully, but less\nattempts have been made to formally understand the reasons underlying its\nexcellent performance. To cover this key gap, we provide a formal connection\nbetween the above bound and a recent P\\'olya-gamma data augmentation for\nlogistic regression. Such a result places the computational methods associated\nwith the aforementioned bounds within the framework of variational inference\nfor conditionally conjugate exponential family models, thereby allowing recent\nadvances for this class to be inherited also by the methods relying on Jaakkola\nand Jordan (2000).\n", "versions": [{"version": "v1", "created": "Sun, 19 Nov 2017 10:50:24 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 06:59:24 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Durante", "Daniele", ""], ["Rigon", "Tommaso", ""]]}, {"id": "1711.07199", "submitter": "Norbert Henze", "authors": "Norbert Henze, Mar\\'ia Dolores Jim\\'enez-Gamero", "title": "A new class of tests for multinormality with i.i.d. and Garch data based\n  on the empirical moment generating function", "comments": "27 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1706.03029", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize a recent class of tests for univariate normality that are based\non the empirical moment generating function to the multivariate setting, thus\nobtaining a class of affine invariant, consistent and easy-to-use\ngoodness-of-fit tests for multinormality. The test statistics are suitably\nweighted $L^2$-statistics, and we provide their asymptotic behavior both for\ni.i.d. observations as well as in the context of testing that the innovation\ndistribution of a multivariate GARCH model is Gaussian. We study the\nfinite-sample behavior of the new tests, compare the criteria with alternative\nexisting procedures, and apply the new procedure to a data set of monthly log\nreturns.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 08:38:01 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Henze", "Norbert", ""], ["Jim\u00e9nez-Gamero", "Mar\u00eda Dolores", ""]]}, {"id": "1711.07211", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and Alistair Stewart", "title": "List-Decodable Robust Mean Estimation and Learning Mixtures of Spherical\n  Gaussians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of list-decodable Gaussian mean estimation and the\nrelated problem of learning mixtures of separated spherical Gaussians. We\ndevelop a set of techniques that yield new efficient algorithms with\nsignificantly improved guarantees for these problems.\n  {\\bf List-Decodable Mean Estimation.} Fix any $d \\in \\mathbb{Z}_+$ and $0<\n\\alpha <1/2$. We design an algorithm with runtime $O\n(\\mathrm{poly}(n/\\alpha)^{d})$ that outputs a list of $O(1/\\alpha)$ many\ncandidate vectors such that with high probability one of the candidates is\nwithin $\\ell_2$-distance $O(\\alpha^{-1/(2d)})$ from the true mean. The only\nprevious algorithm for this problem achieved error $\\tilde O(\\alpha^{-1/2})$\nunder second moment conditions. For $d = O(1/\\epsilon)$, our algorithm runs in\npolynomial time and achieves error $O(\\alpha^{\\epsilon})$. We also give a\nStatistical Query lower bound suggesting that the complexity of our algorithm\nis qualitatively close to best possible.\n  {\\bf Learning Mixtures of Spherical Gaussians.} We give a learning algorithm\nfor mixtures of spherical Gaussians that succeeds under significantly weaker\nseparation assumptions compared to prior work. For the prototypical case of a\nuniform mixture of $k$ identity covariance Gaussians we obtain: For any\n$\\epsilon>0$, if the pairwise separation between the means is at least\n$\\Omega(k^{\\epsilon}+\\sqrt{\\log(1/\\delta)})$, our algorithm learns the unknown\nparameters within accuracy $\\delta$ with sample complexity and running time\n$\\mathrm{poly} (n, 1/\\delta, (k/\\epsilon)^{1/\\epsilon})$. The previously best\nknown polynomial time algorithm required separation at least $k^{1/4}\n\\mathrm{polylog}(k/\\delta)$.\n  Our main technical contribution is a new technique, using degree-$d$\nmultivariate polynomials, to remove outliers from high-dimensional datasets\nwhere the majority of the points are corrupted.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 09:07:08 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}, {"id": "1711.07288", "submitter": "Chris Jennings-Shaffer", "authors": "Chris Jennings-Shaffer, Dane R. Skinner, and Edward C. Waymire", "title": "When Fourth Moments Are Enough", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note concerns a somewhat innocent question motivated by an observation\nconcerning the use of Chebyshev bounds on sample estimates of $p$ in the\nbinomial distribution with parameters $n,p$. Namely, what moment order produces\nthe best Chebyshev estimate of $p$? If $S_n(p)$ has a binomial distribution\nwith parameters $n,p$, there it is readily observed that ${\\rm argmax}_{0\\le\np\\le 1}{\\mathbb E}S_n^2(p) = {\\rm argmax}_{0\\le p\\le 1}np(1-p) = \\frac12,$ and\n${\\mathbb E}S_n^2(\\frac12) = \\frac{n}{4}$. Rabi Bhattacharya observed that\nwhile the second moment Chebyshev sample size for a $95\\%$ confidence estimate\nwithin $\\pm 5$ percentage points is $n = 2000$, the fourth moment yields the\nsubstantially reduced polling requirement of $n = 775$. Why stop at fourth\nmoment? Is the argmax achieved at $p = \\frac12$ for higher order moments and,\nif so, does it help, and compute $\\mathbb{E}S_n^{2m}(\\frac12)$? As captured by\nthe title of this note, answers to these questions lead to a simple rule of\nthumb for best choice of moments in terms of an effective sample size for\nChebyshev concentration inequalities.\n", "versions": [{"version": "v1", "created": "Mon, 20 Nov 2017 12:47:41 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Jennings-Shaffer", "Chris", ""], ["Skinner", "Dane R.", ""], ["Waymire", "Edward C.", ""]]}, {"id": "1711.07775", "submitter": "Bj\\\"orn B\\\"ottcher", "authors": "Bj\\\"orn B\\\"ottcher and Martin Keller-Ressel and Ren\\'e L. Schilling", "title": "Distance multivariance: New dependence measures for random vectors", "comments": "title changed; completely restructured; new content: comparison with\n  dHSIC and Example 5.2; accepted for publication in AoS", "journal-ref": "The Annals of Statistics, Vol. 47, No. 5 (2019) 2757-2789", "doi": "10.1214/18-AOS1764", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two new measures for the dependence of $n \\ge 2$ random\nvariables: distance multivariance and total distance multivariance. Both\nmeasures are based on the weighted $L^2$-distance of quantities related to the\ncharacteristic functions of the underlying random variables. These extend\ndistance covariance (introduced by Sz\\'ekely, Rizzo and Bakirov) from pairs of\nrandom variables to $n$-tuplets of random variables. We show that total\ndistance multivariance can be used to detect the independence of $n$ random\nvariables and has a simple finite-sample representation in terms of distance\nmatrices of the sample points, where distance is measured by a continuous\nnegative definite function. Under some mild moment conditions, this leads to a\ntest for independence of multiple random vectors which is consistent against\nall alternatives.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 13:36:13 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 14:50:39 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["B\u00f6ttcher", "Bj\u00f6rn", ""], ["Keller-Ressel", "Martin", ""], ["Schilling", "Ren\u00e9 L.", ""]]}, {"id": "1711.07778", "submitter": "Bj\\\"{o}rn B\\\"{o}ttcher", "authors": "Bj\\\"orn B\\\"ottcher, Martin Keller-Ressel, Ren\\'e L. Schilling", "title": "Detecting independence of random vectors: generalized distance\n  covariance and Gaussian covariance", "comments": "Published at https://doi.org/10.15559/18-VMSTA116 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)", "journal-ref": "Modern Stochastics: Theory and Applications 2018, Vol. 5, No. 3,\n  353-383", "doi": "10.15559/18-VMSTA116", "report-no": "VTeX-VMSTA-VMSTA116", "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance covariance is a quantity to measure the dependence of two random\nvectors. We show that the original concept introduced and developed by\nSz\\'{e}kely, Rizzo and Bakirov can be embedded into a more general framework\nbased on symmetric L\\'{e}vy measures and the corresponding real-valued\ncontinuous negative definite functions. The L\\'{e}vy measures replace the\nweight functions used in the original definition of distance covariance. All\nessential properties of distance covariance are preserved in this new\nframework. From a practical point of view this allows less restrictive moment\nconditions on the underlying random variables and one can use other distance\nfunctions than Euclidean distance, e.g. Minkowski distance. Most importantly,\nit serves as the basic building block for distance multivariance, a quantity to\nmeasure and estimate dependence of multiple random vectors, which is introduced\nin a follow-up paper [Distance Multivariance: New dependence measures for\nrandom vectors (submitted). Revised version of arXiv: 1711.07775v1] to the\npresent article.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 13:38:26 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 13:32:25 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 07:36:11 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["B\u00f6ttcher", "Bj\u00f6rn", ""], ["Keller-Ressel", "Martin", ""], ["Schilling", "Ren\u00e9 L.", ""]]}, {"id": "1711.08072", "submitter": "V\\'ictor Pe\\~na", "authors": "V\\'ictor Pe\\~na, James O. Berger", "title": "Restricted type II maximum likelihood priors on regression coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian hypothesis testing and model selection, prior distributions must\nbe chosen carefully. For example, setting arbitrarily large prior scales for\nlocation parameters, which is common practice in estimation problems, can lead\nto undesirable behavior in testing (Lindley's paradox). We study the properties\nof some restricted type II maximum likelihood (type II ML) priors on regression\ncoefficients. In type II ML, hyperparameters are \"estimated\" by maximizing the\nmarginal likelihood of a model. In this article, we define priors by estimating\ntheir variances or covariance matrices, adding restrictions which ensure that\nthe resulting priors are at least as vague as conventional proper priors for\nmodel uncertainty. We find that these type II ML priors typically yield results\nthat are close to answers obtained with the Bayesian Information Criterion\n(BIC).\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 22:22:35 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 15:09:28 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 18:27:20 GMT"}, {"version": "v4", "created": "Thu, 21 Nov 2019 23:44:22 GMT"}], "update_date": "2019-11-25", "authors_parsed": [["Pe\u00f1a", "V\u00edctor", ""], ["Berger", "James O.", ""]]}, {"id": "1711.08082", "submitter": "Jakub Mare\\v{c}ek", "authors": "Jing Xu, Jakub Marecek", "title": "Parameter Estimation in Gaussian Mixture Models with Malicious Noise,\n  without Balanced Mixing Coefficients", "comments": null, "journal-ref": null, "doi": "10.1109/ALLERTON.2018.8635825", "report-no": null, "categories": "math.ST cs.CC cs.DS stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating means of two Gaussians in a 2-Gaussian\nmixture, which is not balanced and is corrupted by noise of an arbitrary\ndistribution. We present a robust algorithm to estimate the parameters,\ntogether with upper bounds on the numbers of samples required for the estimate\nto be correct, where the bounds are parametrised by the dimension, ratio of the\nmixing coefficients, a measure of the separation of the two Gaussians, related\nto Mahalanobis distance, and a condition number of the covariance matrix. In\ntheory, this is the first sample-complexity result for imbalanced mixtures\ncorrupted by adversarial noise. In practice, our algorithm outperforms the\nvanilla Expectation-Maximisation (EM) algorithm in terms of estimation error.\n", "versions": [{"version": "v1", "created": "Tue, 21 Nov 2017 23:20:12 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Xu", "Jing", ""], ["Marecek", "Jakub", ""]]}, {"id": "1711.08093", "submitter": "V\\'ictor Pe\\~na", "authors": "V\\'ictor Pe\\~na, James O. Berger", "title": "A note on recent criticisms to Birnbaum's theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we provide critical commentary on two articles that cast doubt\non the validity and implications of Birnbaum's theorem: Evans (2013) and Mayo\n(2014). In our view, the proof is correct and the consequences of the theorem\nare alive and well.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 00:56:34 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Pe\u00f1a", "V\u00edctor", ""], ["Berger", "James O.", ""]]}, {"id": "1711.08181", "submitter": "Thi To Nhu Dang", "authors": "Thi To Nhu Dang", "title": "Estimation of the multifractional function and the stability index of\n  linear multifractional stable processes", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are interested in multifractional stable processes where the\nself-similarity index $H$ is a function of time, in other words $H$ becomes\ntime changing, and the stability index $\\alpha$ is a constant. Using $\\beta$-\nnegative power variations ($-1/2<\\beta<0$), we propose estimators for the value\nof the multifractional function $H$ at a fixed time $t_0$ and for $\\alpha$ for\ntwo cases: multifractional Brownian motion ($\\alpha=2$) and linear\nmultifractional stable motion ($0<\\alpha<2$). We get the consistency of our\nestimates for the underlying processes with the rate of convergence.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 09:10:55 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Dang", "Thi To Nhu", ""]]}, {"id": "1711.08328", "submitter": "Lucien Birg\\'e", "authors": "Yannick Baraud and Lucien Birg\\'e", "title": "Robust Bayes-Like Estimation: Rho-Bayes estimation", "comments": "68 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the joint distribution $P$ of $n$\nindependent random variables within the Bayes paradigm from a non-asymptotic\npoint of view. Assuming that $P$ admits some density $s$ with respect to a\ngiven reference measure, we consider a density model $\\overline S$ for $s$ that\nwe endow with a prior distribution $\\pi$ (with support $\\overline S$) and we\nbuild a robust alternative to the classical Bayes posterior distribution which\npossesses similar concentration properties around $s$ whenever it belongs to\nthe model $\\overline S$. Furthermore, in density estimation, the Hellinger\ndistance between the classical and the robust posterior distributions tends to\n0, as the number of observations tends to infinity, under suitable assumptions\non the model and the prior, provided that the model $\\overline S$ contains the\ntrue density $s$. However, unlike what happens with the classical Bayes\nposterior distribution, we show that the concentration properties of this new\nposterior distribution are still preserved in the case of a misspecification of\nthe model, that is when $s$ does not belong to $\\overline S$ but is close\nenough to it with respect to the Hellinger distance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 15:18:27 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 17:27:59 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Baraud", "Yannick", ""], ["Birg\u00e9", "Lucien", ""]]}, {"id": "1711.08411", "submitter": "Samprit Banerjee", "authors": "Samprit Banerjee and Stefano Monni", "title": "An Orthogonally Equivariant Estimator of the Covariance Matrix in High\n  Dimensions and for Small Sample Sizes", "comments": "Journal of Statistical Planning and Inference (2020)", "journal-ref": null, "doi": "10.1016/j.jspi.2020.10.006", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an estimation method of covariance matrices in a\nhigh-dimensional setting, i.e., when the dimension of the matrix, , is larger\nthan the sample size . Specifically, we propose an orthogonally equivariant\nestimator. The eigenvectors of such estimator are the same as those of the\nsample covariance matrix. The eigenvalue estimates are obtained from an\nadjusted profile likelihood function derived by approximating the integral of\nthe density function of the sample covariance matrix over its eigenvectors,\nwhich is a challenging problem in its own right. Exact solutions to the\napproximate likelihood equations are obtained and employed to construct\nestimates that involve a tuning parameter. Bootstrap and cross-validation based\nalgorithms are proposed to choose this tuning parameter under various loss\nfunctions. Finally, comparisons with two well-known orthogonally equivariant\nestimators of the covariance matrix are given, which are based on Monte-Carlo\nrisk estimates for simulated data and misclassification errors in real data\nanalyses. In addition, Monte-Carlo risk estimates are also provided to compare\nour estimates of eigenvalues to those of a consistent estimator of population\neigenvalues.\n", "versions": [{"version": "v1", "created": "Wed, 22 Nov 2017 17:38:36 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 18:46:56 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 07:17:54 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Banerjee", "Samprit", ""], ["Monni", "Stefano", ""]]}, {"id": "1711.08593", "submitter": "Oliver Lang", "authors": "Oliver Lang, Mario Huemer, Markus Steindl", "title": "Constrained Best Linear Unbiased Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The least squares (LS) estimator and the best linear unbiased estimator\n(BLUE) are two well-studied approaches for the estimation of a deterministic\nbut unknown parameter vector. In many applications it is known that the\nparameter vector fulfills some constraints, e.g., linear constraints. For such\nsituations the constrained LS estimator, which is a simple extension of the LS\nestimator, can be employed. In this paper we derive the constrained version of\nthe BLUE. It will turn out that the incorporation of the linear constraints\ninto the derivation of the BLUE is not straight forward as for the constrained\nLS estimator, but the final expression for the constrained BLUE is closely\nrelated to that of the constrained LS estimator.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 06:57:01 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Lang", "Oliver", ""], ["Huemer", "Mario", ""], ["Steindl", "Markus", ""]]}, {"id": "1711.08705", "submitter": "Jean-Bernard Salomond", "authors": "Jean-Bernard Salomond", "title": "Risk quantification for the thresholding rule for multiple testing using\n  Gaussian scale mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the asymptotic properties of Bayesian multiple testing\nprocedures for a large class of Gaussian scale mixture pri- ors. We study two\ntypes of multiple testing risks: a Bayesian risk proposed in Bogdan et al.\n(2011) where the data are assume to come from a mixture of normal, and a\nfrequentist risk similar to the one proposed by Arias-Castro and Chen (2017).\nFollowing the work of van der Pas et al. (2016), we give general conditions on\nthe prior such that both risks can be bounded. For the Bayesian risk, the bound\nis almost sharp. This result show that under these conditions, the considered\nclass of continuous prior can be competitive with the usual two-group model\n(e.g. spike and slab priors). We also show that if the non-zeros component of\nthe parameter are large enough, the minimax risk can be made asymptotically\nnull. The separation rates obtained are consistent with the one that could be\nguessed from the existing literature (see van der Pas et al., 2017b). For both\nproblems, we then give conditions under which an adaptive version of the result\ncan be obtained.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 14:25:43 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Salomond", "Jean-Bernard", ""]]}, {"id": "1711.08736", "submitter": "Mehmet Madensoy", "authors": "Markus Bibinger, Mehmet Madensoy", "title": "Change-point inference on volatility in noisy It\\^o semimartingales", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is concerned with tests on structural breaks in the spot volatility\nprocess of a general It\\^o semimartingale based on discrete observations\ncontaminated with i.i.d. microstructure noise. We construct a consistent test\nbuilding up on infill asymptotic results for certain functionals of spectral\nspot volatility estimates. A weak limit theorem is established under the null\nhypothesis relying on extreme value theory. We prove consistency of the test\nand of an associated estimator for the change point. A simulation study\nillustrates the finite-sample performance of the method and efficiency gains\ncompared to a skip-sampling approach.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 15:26:34 GMT"}, {"version": "v2", "created": "Sat, 22 Sep 2018 17:55:29 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Bibinger", "Markus", ""], ["Madensoy", "Mehmet", ""]]}, {"id": "1711.08747", "submitter": "Mengjia Yu", "authors": "Mengjia Yu, Xiaohui Chen", "title": "Finite sample change point inference and identification for\n  high-dimensional mean vectors", "comments": null, "journal-ref": null, "doi": "10.1111/rssb.12406", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cumulative sum (CUSUM) statistics are widely used in the change point\ninference and identification. For the problem of testing for existence of a\nchange point in an independent sample generated from the mean-shift model, we\nintroduce a Gaussian multiplier bootstrap to calibrate critical values of the\nCUSUM test statistics in high dimensions. The proposed bootstrap CUSUM test is\nfully data-dependent and it has strong theoretical guarantees under arbitrary\ndependence structures and mild moment conditions. Specifically, we show that\nwith a boundary removal parameter the bootstrap CUSUM test enjoys the uniform\nvalidity in size under the null and it achieves the minimax separation rate\nunder the sparse alternatives when the dimension $p$ can be larger than the\nsample size $n$.\n  Once a change point is detected, we estimate the change point location by\nmaximizing the $\\ell^{\\infty}$-norm of the generalized CUSUM statistics at two\ndifferent weighting scales corresponding to covariance stationary and\nnon-stationary CUSUM statistics. For both estimators, we derive their rates of\nconvergence and show that dimension impacts the rates only through logarithmic\nfactors, which implies that consistency of the CUSUM estimators is possible\nwhen $p$ is much larger than $n$. In the presence of multiple change points, we\npropose a principled bootstrap-assisted binary segmentation (BABS) algorithm to\ndynamically adjust the change point detection rule and recursively estimate\ntheir locations. We derive its rate of convergence under suitable signal\nseparation and strength conditions.\n  The results derived in this paper are non-asymptotic and we provide extensive\nsimulation studies to assess the finite sample performance. The empirical\nevidence shows an encouraging agreement with our theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 15:55:38 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 01:17:53 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 04:03:14 GMT"}, {"version": "v4", "created": "Sun, 6 Sep 2020 03:47:55 GMT"}, {"version": "v5", "created": "Sat, 2 Jan 2021 05:16:07 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Yu", "Mengjia", ""], ["Chen", "Xiaohui", ""]]}, {"id": "1711.08822", "submitter": "Kin Wai Chan", "authors": "Kin Wai Chan and Xiao-Li Meng", "title": "Multiple Improvements of Multiple Imputation Likelihood Ratio Tests", "comments": "45 pages, 9 tables, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation (MI) inference handles missing data by first properly\nimputing the missing values $m$ times, and then combining the $m$ analysis\nresults from applying a complete-data procedure to each of the completed\ndatasets. However, the existing method for combining likelihood ratio tests has\nmultiple defects: (i) the combined test statistic can be negative in practice\nwhen the reference null distribution is a standard $F$ distribution; (ii) it is\nnot invariant to re-parametrization; (iii) it fails to ensure monotonic power\ndue to its use of an inconsistent estimator of the fraction of missing\ninformation (FMI) under the alternative hypothesis; and (iv) it requires\nnon-trivial access to the likelihood ratio test statistic as a function of\nestimated parameters instead of datasets. This paper shows, via both\ntheoretical derivations and empirical investigations, that essentially all of\nthese problems can be straightforwardly addressed if we are willing to perform\nan additional likelihood ratio test by stacking the $m$ completed datasets as\none big completed dataset. A particularly intriguing finding is that the FMI\nitself can be estimated consistently by a likelihood ratio statistic for\ntesting whether the $m$ completed datasets produced by MI can be regarded\neffectively as samples coming from a common model. Practical guidelines are\nprovided based on an extensive comparison of existing MI tests.\n", "versions": [{"version": "v1", "created": "Thu, 23 Nov 2017 20:18:45 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 03:48:21 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Chan", "Kin Wai", ""], ["Meng", "Xiao-Li", ""]]}, {"id": "1711.08911", "submitter": "Guillaume Dehaene P.", "authors": "Guillaume P. Dehaene", "title": "Computing the quality of the Laplace approximation", "comments": "Advances in Approximate Bayesian Inference NIPS 2017 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference requires approximation methods to become computable, but\nfor most of them it is impossible to quantify how close the approximation is to\nthe true posterior. In this work, we present a theorem upper-bounding the KL\ndivergence between a log-concave target density\n$f\\left(\\boldsymbol{\\theta}\\right)$ and its Laplace approximation\n$g\\left(\\boldsymbol{\\theta}\\right)$. The bound we present is computable: on the\nclassical logistic regression model, we find our bound to be almost exact as\nlong as the dimensionality of the parameter space is high.\n  The approach we followed in this work can be extended to other Gaussian\napproximations, as we will do in an extended version of this work, to be\nsubmitted to the Annals of Statistics. It will then become a critical tool for\ncharacterizing whether, for a given problem, a given Gaussian approximation is\nsuitable, or whether a more precise alternative method should be used instead.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 10:10:15 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Dehaene", "Guillaume P.", ""]]}, {"id": "1711.08947", "submitter": "Elsa Cazelles", "authors": "J\\'er\\'emie Bigot, Elsa Cazelles and Nicolas Papadakis", "title": "Central limit theorems for entropy-regularized optimal transport on\n  finite spaces and statistical applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of entropy-regularized optimal transport, also known as Sinkhorn\ndivergence, has recently gained popularity in machine learning and statistics,\nas it makes feasible the use of smoothed optimal transportation distances for\ndata analysis. The Sinkhorn divergence allows the fast computation of an\nentropically regularized Wasserstein distance between two probability\ndistributions supported on a finite metric space of (possibly) high-dimension.\nFor data sampled from one or two unknown probability distributions, we derive\nthe distributional limits of the empirical Sinkhorn divergence and its centered\nversion (Sinkhorn loss). We also propose a bootstrap procedure which allows to\nobtain new test statistics for measuring the discrepancies between multivariate\nprobability distributions. Our work is inspired by the results of Sommerfeld\nand Munk (2016) on the asymptotic distribution of empirical Wasserstein\ndistance on finite space using unregularized transportation costs. Incidentally\nwe also analyze the asymptotic distribution of entropy-regularized Wasserstein\ndistances when the regularization parameter tends to zero. Simulated and real\ndatasets are used to illustrate our approach.\n", "versions": [{"version": "v1", "created": "Fri, 24 Nov 2017 12:46:58 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 18:18:26 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 21:21:58 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Bigot", "J\u00e9r\u00e9mie", ""], ["Cazelles", "Elsa", ""], ["Papadakis", "Nicolas", ""]]}, {"id": "1711.09200", "submitter": "Daniel Conn", "authors": "Daniel Conn and Gang Li", "title": "An Oracle Property of The Nadaraya-Watson Kernel Estimator for High\n  Dimensional Nonparametric Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The celebrated Nadaraya-Watson kernel estimator is among the most studied\nmethod for nonparametric regression. A classical result is that its rate of\nconvergence depends on the number of covariates and deteriorates quickly as the\ndimension grows, which underscores the \"curse of dimensionality\" and has\nlimited its use in high dimensional settings. In this article, we show that\nwhen the true regression function is single or multi-index, the effects of the\ncurse of dimensionality may be mitigated for the Nadaraya-Watson kernel\nestimator. Specifically, we prove that with $K$-fold cross-validation, the\nNadaraya-Watson kernel estimator indexed by a positive semidefinite bandwidth\nmatrix has an oracle property that its rate of convergence depends on the\nnumber of indices of the regression function rather than the number of\ncovariates. Intuitively, this oracle property is a consequence of allowing the\nbandwidths to diverge to infinity as opposed to restricting them all to\nconverge to zero at certain rates as done in previous theoretical studies. Our\nresult provides a theoretical perspective for the use of kernel estimation in\nhigh dimensional nonparametric regression and other applications such as metric\nlearning when a low rank structure is anticipated. Numerical illustrations are\ngiven through simulations and real data examples.\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 06:27:41 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Conn", "Daniel", ""], ["Li", "Gang", ""]]}, {"id": "1711.09208", "submitter": "Ekaterina Krymova", "authors": "Yuri Golubev, Ekaterina Krymova", "title": "On estimation of the noise variance in high-dimensional linear models", "comments": "in Russian", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering the unknown noise variance in the\nlinear regression model. To estimate the nuisance (a vector of regression\ncoefficients) we use a family of spectral regularisers of the maximum\nlikelihood estimator. The noise estimation is based on the adaptive\nnormalisation of the squared error. We derive the upper bound for the\nconcentration of the proposed method around the ideal estimator (the case of\nzero nuisance).\n", "versions": [{"version": "v1", "created": "Sat, 25 Nov 2017 07:37:45 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Golubev", "Yuri", ""], ["Krymova", "Ekaterina", ""]]}, {"id": "1711.09338", "submitter": "Pedro Ramos", "authors": "Pedro L. Ramos, Francisco Louzada, Taciana K.O. Shimizu, Aline O. Luiz", "title": "The Inverse Weighted Lindley Distribution: Properties, Estimation and an\n  Application on a Failure Time Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a new distribution is proposed. This new model provides more\nflexibility to modeling data with upside-down bathtub hazard rate function. A\nsignificant account of mathematical properties of the new distribution is\npresented. The maximum likelihood estimators for the parameters in the presence\nof complete and censored data are presented. Two corrective approaches are\nconsidered to derive modified estimators that are bias-free to second order. A\nnumerical simulation is carried out to examine the efficiency of the bias\ncorrection. Finally, an application using a real data set is presented in order\nto illustrate our proposed distribution.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 06:09:59 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Ramos", "Pedro L.", ""], ["Louzada", "Francisco", ""], ["Shimizu", "Taciana K. O.", ""], ["Luiz", "Aline O.", ""]]}, {"id": "1711.09388", "submitter": "Ingeborg Waernbaum PhD", "authors": "Ingeborg Waernbaum and Laura Pazzagli", "title": "Model misspecification and bias for inverse probability weighting and\n  doubly robust estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the causal inference literature an estimator belonging to a class of\nsemi-parametric estimators is called robust if it has desirable properties\nunder the assumption that at least one of the working models is correctly\nspecified. In this paper we propose a crude analytical approach to study the\nlarge sample bias of semi-parameteric estimators of the average causal effect\nwhen all working models are misspecified. We apply our approach to three\nprototypical estimators, two inverse probability weighting (IPW) estimators,\nusing a misspecified propensity score model, and a doubly robust (DR)\nestimator, using misspecified models for the outcome regression and the\npropensity score. To analyze the question of when the use of two misspecified\nmodels are better than one we derive necessary and sufficient conditions for\nwhen the DR estimator has a smaller bias than a simple IPW estimator and when\nit has a smaller bias than an IPW estimator with normalized weights. If the\nmisspecificiation of the outcome model is moderate the comparisons of the\nbiases of the IPW and DR estimators suggest that the DR estimator has a smaller\nbias than the IPW estimators. However, all biases include the PS-model error\nand we suggest that a researcher is careful when modeling the PS whenever such\na model is involved.\n", "versions": [{"version": "v1", "created": "Sun, 26 Nov 2017 13:49:14 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 14:54:48 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Waernbaum", "Ingeborg", ""], ["Pazzagli", "Laura", ""]]}, {"id": "1711.09628", "submitter": "Tobias Fissler", "authors": "Tobias Fissler, Johanna F. Ziegel", "title": "Order-Sensitivity and Equivariance of Scoring Functions", "comments": "45 pages", "journal-ref": "Electronic Journal of Statistics, Volume 13, Number 1 (2019),\n  1166-1211", "doi": "10.1214/19-EJS1552", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relative performance of competing point forecasts is usually measured in\nterms of loss or scoring functions. It is widely accepted that these scoring\nfunction should be strictly consistent in the sense that the expected score is\nminimized by the correctly specified forecast for a certain statistical\nfunctional such as the mean, median, or a certain risk measure. Thus, strict\nconsistency opens the way to meaningful forecast comparison, but is also\nimportant in regression and M-estimation. Usually strictly consistent scoring\nfunctions for an elicitable functional are not unique. To give guidance on the\nchoice of a scoring function, this paper introduces two additional quality\ncriteria. Order-sensitivity opens the possibility to compare two deliberately\nmisspecified forecasts given that the forecasts are ordered in a certain sense.\nOn the other hand, equivariant scoring functions obey similar equivariance\nproperties as the functional at hand - such as translation invariance or\npositive homogeneity. In our study, we consider scoring functions for popular\nfunctionals, putting special emphasis on vector-valued functionals, e.g. the\npair (mean, variance) or (Value at Risk, Expected Shortfall).\n", "versions": [{"version": "v1", "created": "Mon, 27 Nov 2017 11:40:03 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Fissler", "Tobias", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "1711.10156", "submitter": "Varathan Nagarajah", "authors": "Nagarajah Varathan and Pushpakanthie Wijekoon", "title": "More on the restricted almost unbiased Liu-estimator in Logistic\n  regression", "comments": "16 pages, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To address the problem of multicollinearity in the logistic regression model,\nin this paper we propose a new estimator called Stochastic restricted almost\nunbiased logistic Liu-estimator (SRAULLE) when the prior information is\navailable in the form of stochastic linear restrictions. A Monte Carlo\nsimulation study was carried out to compare the performance of the proposed\nestimator with some existing estimators in the scalar mean squared error (SMSE)\nsense. Finally, a real data example was given to appraise the performance of\nthe estimators.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 07:24:53 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Varathan", "Nagarajah", ""], ["Wijekoon", "Pushpakanthie", ""]]}, {"id": "1711.10265", "submitter": "Anita Lindmark PhD", "authors": "Anita Lindmark, Xavier de Luna and Marie Eriksson", "title": "Sensitivity analysis for unobserved confounding of direct and indirect\n  effects using uncertainty intervals", "comments": "22 pages, 5 figures", "journal-ref": "Statistics in Medicine, 2018", "doi": "10.1002/sim.7620", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To estimate direct and indirect effects of an exposure on an outcome from\nobserved data strong assumptions about unconfoundedness are required. Since\nthese assumptions cannot be tested using the observed data, a mediation\nanalysis should always be accompanied by a sensitivity analysis of the\nresulting estimates. In this article we propose a sensitivity analysis method\nfor parametric estimation of direct and indirect effects when the exposure,\nmediator and outcome are all binary. The sensitivity parameters consist of the\ncorrelation between the error terms of the mediator and outcome models, the\ncorrelation between the error terms of the mediator model and the model for the\nexposure assignment mechanism, and the correlation between the error terms of\nthe exposure assignment and outcome models. These correlations are incorporated\ninto the estimation of the model parameters and identification sets are then\nobtained for the direct and indirect effects for a range of plausible\ncorrelation values. We take the sampling variability into account through the\nconstruction of uncertainty intervals. The proposed method is able to assess\nsensitivity to both mediator-outcome confounding and confounding involving the\nexposure. To illustrate the method we apply it to a mediation study based on\ndata from the Swedish Stroke Register (Riksstroke).\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 12:58:40 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Lindmark", "Anita", ""], ["de Luna", "Xavier", ""], ["Eriksson", "Marie", ""]]}, {"id": "1711.10306", "submitter": "Guillaume Lecu\\'e", "authors": "Guillaume Lecu\\'e and Matthieu Lerasle", "title": "Robust machine learning by median-of-means : theory and practice", "comments": "48 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new estimators for robust machine learning based on\nmedian-of-means (MOM) estimators of the mean of real valued random variables.\nThese estimators achieve optimal rates of convergence under minimal assumptions\non the dataset. The dataset may also have been corrupted by outliers on which\nno assumption is granted. We also analyze these new estimators with standard\ntools from robust statistics. In particular, we revisit the concept of\nbreakdown point. We modify the original definition by studying the number of\noutliers that a dataset can contain without deteriorating the estimation\nproperties of a given estimator. This new notion of breakdown number, that\ntakes into account the statistical performances of the estimators, is\nnon-asymptotic in nature and adapted for machine learning purposes. We proved\nthat the breakdown number of our estimator is of the order of (number of\nobservations)*(rate of convergence). For instance, the breakdown number of our\nestimators for the problem of estimation of a d-dimensional vector with a noise\nvariance sigma^2 is sigma^2d and it becomes sigma^2 s log(d/s) when this vector\nhas only s non-zero component. Beyond this breakdown point, we proved that the\nrate of convergence achieved by our estimator is (number of outliers) divided\nby (number of observation).\n  Besides these theoretical guarantees, the major improvement brought by these\nnew estimators is that they are easily computable in practice. In fact,\nbasically any algorithm used to approximate the standard Empirical Risk\nMinimizer (or its regularized versions) has a robust version approximating our\nestimators. As a proof of concept, we study many algorithms for the classical\nLASSO estimator. A byproduct of the MOM algorithms is a measure of depth of\ndata that can be used to detect outliers.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 14:28:48 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 21:30:11 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Lecu\u00e9", "Guillaume", ""], ["Lerasle", "Matthieu", ""]]}, {"id": "1711.10467", "submitter": "Cong Ma", "authors": "Cong Ma, Kaizheng Wang, Yuejie Chi, Yuxin Chen", "title": "Implicit Regularization in Nonconvex Statistical Estimation: Gradient\n  Descent Converges Linearly for Phase Retrieval, Matrix Completion, and Blind\n  Deconvolution", "comments": "accepted to Foundations of Computational Mathematics (FOCM)", "journal-ref": "Foundations of Computational Mathematics, vol. 20, no. 3, pp.\n  451-632, June 2020", "doi": "10.1007/s10208-019-09429-9", "report-no": null, "categories": "cs.LG cs.IT math.IT math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a flurry of activities in designing provably efficient\nnonconvex procedures for solving statistical estimation problems. Due to the\nhighly nonconvex nature of the empirical loss, state-of-the-art procedures\noften require proper regularization (e.g. trimming, regularized cost,\nprojection) in order to guarantee fast convergence. For vanilla procedures such\nas gradient descent, however, prior theory either recommends highly\nconservative learning rates to avoid overshooting, or completely lacks\nperformance guarantees.\n  This paper uncovers a striking phenomenon in nonconvex optimization: even in\nthe absence of explicit regularization, gradient descent enforces proper\nregularization implicitly under various statistical models. In fact, gradient\ndescent follows a trajectory staying within a basin that enjoys nice geometry,\nconsisting of points incoherent with the sampling mechanism. This \"implicit\nregularization\" feature allows gradient descent to proceed in a far more\naggressive fashion without overshooting, which in turn results in substantial\ncomputational savings. Focusing on three fundamental statistical estimation\nproblems, i.e. phase retrieval, low-rank matrix completion, and blind\ndeconvolution, we establish that gradient descent achieves near-optimal\nstatistical and computational guarantees without explicit regularization. In\nparticular, by marrying statistical modeling with generic optimization theory,\nwe develop a general recipe for analyzing the trajectories of iterative\nalgorithms via a leave-one-out perturbation argument. As a byproduct, for noisy\nmatrix completion, we demonstrate that gradient descent achieves near-optimal\nerror control --- measured entrywise and by the spectral norm --- which might\nbe of independent interest.\n", "versions": [{"version": "v1", "created": "Tue, 28 Nov 2017 18:53:38 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 10:47:03 GMT"}, {"version": "v3", "created": "Tue, 30 Jul 2019 02:14:54 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Ma", "Cong", ""], ["Wang", "Kaizheng", ""], ["Chi", "Yuejie", ""], ["Chen", "Yuxin", ""]]}, {"id": "1711.10635", "submitter": "Shuxiao Chen", "authors": "Shuxiao Chen, Jacob Bien", "title": "Valid Inference Corrected for Outlier Removal", "comments": "21 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary least square (OLS) estimation of a linear regression model is\nwell-known to be highly sensitive to outliers. It is common practice to (1)\nidentify and remove outliers by looking at the data and (2) to fit OLS and form\nconfidence intervals and p-values on the remaining data as if this were the\noriginal data collected. This standard \"detect-and-forget\" approach has been\nshown to be problematic, and in this paper we highlight the fact that it can\nlead to invalid inference and show how recently developed tools in selective\ninference can be used to properly account for outlier detection and removal.\nOur inferential procedures apply to a general class of outlier removal\nprocedures that includes several of the most commonly used approaches. We\nconduct simulations to corroborate the theoretical results, and we apply our\nmethod to three real data sets to illustrate how our inferential results can\ndiffer from the traditional detect-and-forget strategy. A companion R package,\noutference, implements these new procedures with an interface that matches the\nfunctions commonly used for inference with lm in R.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 01:18:56 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 01:13:13 GMT"}, {"version": "v3", "created": "Sat, 10 Aug 2019 04:41:56 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Chen", "Shuxiao", ""], ["Bien", "Jacob", ""]]}, {"id": "1711.10646", "submitter": "Linjie Zhuang", "authors": "L. Zhuang and A. T. Walden", "title": "Intrinsic Analysis of the Sample Fr\\'echet Mean and Sample Mean of\n  Complex Wishart Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two types of averaging of complex covariance matrices, a sample\nmean (average) and the sample Fr\\'echet mean. We analyse the performance of\nthese quantities as estimators for the true covariance matrix via `intrinsic'\nversions of bias and mean square error, a methodology which takes account of\ngeometric structure. We derive simple expressions for the intrinsic bias in\nboth cases, and the simple average is seen to be preferable. The same is true\nfor the asymptotic Riemannian risk, and for the Riemannian risk itself in the\nscalar case. Combined with a similar preference for the simple average using\nnon-intrinsic analysis, we conclude that the simple average is preferred\noverall to the sample Fr\\'echet mean in this context.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 02:08:01 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 08:44:53 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Zhuang", "L.", ""], ["Walden", "A. T.", ""]]}, {"id": "1711.10696", "submitter": "Kengo Kato", "authors": "Victor Chernozhukov, Denis Chetverikov, Kengo Kato", "title": "Detailed proof of Nazarov's inequality", "comments": "This note is designated only for arXiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this note is to provide a detailed proof of Nazarov's\ninequality stated in Lemma A.1 in Chernozhukov, Chetverikov, and Kato (2017,\nAnnals of Probability).\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 06:28:41 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Chetverikov", "Denis", ""], ["Kato", "Kengo", ""]]}, {"id": "1711.10819", "submitter": "Erlis Ruli", "authors": "Federica Giummol\\`e, Valentina Mameli, Erlis Ruli and Laura Ventura", "title": "Objective Bayesian inference with proper scoring rules", "comments": "29 pages and 9 figures", "journal-ref": "Test 2019", "doi": "10.1007/s11749-018-0597-z", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard Bayesian analyses can be difficult to perform when the full\nlikelihood, and consequently the full posterior distribution, is too complex\nand difficult to specify or if robustness with respect to data or to model\nmisspecifications is required. In these situations, we suggest to resort to a\nposterior distribution for the parameter of interest based on proper scoring\nrules. Scoring rules are loss functions designed to measure the quality of a\nprobability distribution for a random variable, given its observed value.\nImportant examples are the Tsallis score and the Hyv\\\"arinen score, which allow\nus to deal with model misspecifications or with complex models. Also the full\nand the composite likelihoods are both special instances of scoring rules.\n  The aim of this paper is twofold. Firstly, we discuss the use of scoring\nrules in the Bayes formula in order to compute a posterior distribution, named\nSR-posterior distribution, and we derive its asymptotic normality. Secondly, we\npropose a procedure for building default priors for the unknown parameter of\ninterest that can be used to update the information provided by the scoring\nrule in the SR-posterior distribution. In particular, a reference prior is\nobtained by maximizing the average $\\alpha-$divergence from the SR-posterior\ndistribution. For $0 \\leq |\\alpha|<1$, the result is a Jeffreys-type prior that\nis proportional to the square root of the determinant of the Godambe\ninformation matrix associated to the scoring rule. Some examples are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 12:40:53 GMT"}, {"version": "v2", "created": "Sun, 6 Jan 2019 15:10:30 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Giummol\u00e8", "Federica", ""], ["Mameli", "Valentina", ""], ["Ruli", "Erlis", ""], ["Ventura", "Laura", ""]]}, {"id": "1711.10822", "submitter": "Ryo Imai", "authors": "Ryo Imai, Tatsuya Kubokawa and Malay Ghosh", "title": "Bayesian Simultaneous Estimation for Means in $k$ Sample Problems", "comments": "13 pages", "journal-ref": "Journal of Multivariate Analysis Volume 169, January 2019, Pages\n  49-60", "doi": "10.1016/j.jmva.2018.08.013", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the simultaneous estimation of $k$ population\nmeans when one suspects that the $k$ means are nearly equal. As an alternative\nto the preliminary test estimator based on the test statistics for testing\nhypothesis of equal means, we derive Bayesian and minimax estimators which\nshrink individual sample means toward a pooled mean estimator given under the\nhypothesis. It is shown that both the preliminary test estimator and the\nBayesian minimax shrinkage estimators are further improved by shrinking the\npooled mean estimator. The performance of the proposed shrinkage estimators is\ninvestigated by simulation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 12:44:44 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 06:18:22 GMT"}, {"version": "v3", "created": "Thu, 23 Aug 2018 16:24:52 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Imai", "Ryo", ""], ["Kubokawa", "Tatsuya", ""], ["Ghosh", "Malay", ""]]}, {"id": "1711.10900", "submitter": "Michael S{\\o}rensen", "authors": "Jean Jacod and Michael S{\\o}rensen", "title": "A review of asymptotic theory of estimating functions", "comments": null, "journal-ref": "Stat Inference Stoch Process 2018", "doi": "10.1007/s11203-018-9178-8", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymptotic statistical theory for estimating functions is reviewed in a\ngenerality suitable for stochastic processes. Conditions concerning existence\nof a consistent estimator, uniqueness, rate of convergence, and the asymptotic\ndistribution are treated separately. Our conditions are not minimal, but can be\nverified for many interesting stochastic process models. Several examples\nillustrate the wide applicability of the theory and why the generality is\nneeded.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 15:04:01 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Jacod", "Jean", ""], ["S\u00f8rensen", "Michael", ""]]}, {"id": "1711.10937", "submitter": "Maxime Taillardat", "authors": "Maxime Taillardat (1,2,3), Anne-Laure Foug\\`eres (3), Philippe Naveau\n  (2), Olivier Mestre (1) ((1) CNRM, (2) LSCE, (3) ICJ)", "title": "Forest-based methods and ensemble model output statistics for rainfall\n  ensemble forecasting", "comments": null, "journal-ref": null, "doi": "10.1175/WAF-D-18-0149.1", "report-no": null, "categories": "stat.ML math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rainfall ensemble forecasts have to be skillful for both low precipitation\nand extreme events. We present statistical post-processing methods based on\nQuantile Regression Forests (QRF) and Gradient Forests (GF) with a parametric\nextension for heavy-tailed distributions. Our goal is to improve ensemble\nquality for all types of precipitation events, heavy-tailed included, subject\nto a good overall performance. Our hybrid proposed methods are applied to daily\n51-h forecasts of 6-h accumulated precipitation from 2012 to 2015 over France\nusing the M{\\'e}t{\\'e}o-France ensemble prediction system called PEARP. They\nprovide calibrated pre-dictive distributions and compete favourably with\nstate-of-the-art methods like Analogs method or Ensemble Model Output\nStatistics. In particular, hybrid forest-based procedures appear to bring an\nadded value to the forecast of heavy rainfall.\n", "versions": [{"version": "v1", "created": "Wed, 29 Nov 2017 16:17:17 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Taillardat", "Maxime", "", "CNRM", "LSCE", "ICJ"], ["Foug\u00e8res", "Anne-Laure", "", "ICJ"], ["Naveau", "Philippe", "", "LSCE"], ["Mestre", "Olivier", "", "CNRM"]]}, {"id": "1711.11189", "submitter": "Chao Gao", "authors": "Chao Gao", "title": "Phase Transitions in Approximate Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of approximate ranking from observations of pairwise\ninteractions. The goal is to estimate the underlying ranks of $n$ objects from\ndata through interactions of comparison or collaboration. Under a general\nframework of approximate ranking models, we characterize the exact optimal\nstatistical error rates of estimating the underlying ranks. We discover\nimportant phase transition boundaries of the optimal error rates. Depending on\nthe value of the signal-to-noise ratio (SNR) parameter, the optimal rate, as a\nfunction of SNR, is either trivial, polynomial, exponential or zero. The four\ncorresponding regimes thus have completely different error behaviors. To the\nbest of our knowledge, this phenomenon, especially the phase transition between\nthe polynomial and the exponential rates, has not been discovered before.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 02:03:43 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 14:36:19 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Gao", "Chao", ""]]}, {"id": "1711.11218", "submitter": "Patrick J. Laub", "authors": "Patrick J. Laub, Robert Salomone, Zdravko I. Botev", "title": "Monte Carlo Estimation of the Density of the Sum of Dependent Random\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an unbiased estimator for the density of a sum of random variables\nthat are simulated from a computer model. A numerical study on examples with\ncopula dependence is conducted where the proposed estimator performs favourably\nin terms of variance compared to other unbiased estimators. We provide\napplications and extensions to the estimation of marginal densities in Bayesian\nstatistics and to the estimation of the density of sums of random variables\nunder Gaussian copula dependence.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 04:19:49 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 09:43:12 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Laub", "Patrick J.", ""], ["Salomone", "Robert", ""], ["Botev", "Zdravko I.", ""]]}, {"id": "1711.11220", "submitter": "Jue Wang", "authors": "Ery Arias-Castro and Jue Wang", "title": "RANSAC Algorithms for Subspace Recovery and Subspace Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the RANSAC algorithm in the context of subspace recovery and\nsubspace clustering. We derive some theory and perform some numerical\nexperiments. We also draw some correspondences with the methods of Hardt and\nMoitra (2013) and Chen and Lerman (2009b).\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 04:29:46 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Wang", "Jue", ""]]}, {"id": "1711.11280", "submitter": "Aretha Teckentrup", "authors": "Matthew M. Dunlop and Mark A. Girolami and Andrew M. Stuart and Aretha\n  L. Teckentrup", "title": "How Deep Are Deep Gaussian Processes?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has shown the potential utility of Deep Gaussian Processes.\nThese deep structures are probability distributions, designed through\nhierarchical construction, which are conditionally Gaussian. In this paper, the\ncurrent published body of work is placed in a common framework and, through\nrecursion, several classes of deep Gaussian processes are defined. The\nresulting samples generated from a deep Gaussian process have a Markovian\nstructure with respect to the depth parameter, and the effective depth of the\nresulting process is interpreted in terms of the ergodicity, or non-ergodicity,\nof the resulting Markov chain. For the classes of deep Gaussian processes\nintroduced, we provide results concerning their ergodicity and hence their\neffective depth. We also demonstrate how these processes may be used for\ninference; in particular we show how a Metropolis-within-Gibbs construction\nacross the levels of the hierarchy can be used to derive sampling tools which\nare robust to the level of resolution used to represent the functions on a\ncomputer. For illustration, we consider the effect of ergodicity in some simple\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 09:27:52 GMT"}, {"version": "v2", "created": "Fri, 17 Aug 2018 10:13:55 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Dunlop", "Matthew M.", ""], ["Girolami", "Mark A.", ""], ["Stuart", "Andrew M.", ""], ["Teckentrup", "Aretha L.", ""]]}, {"id": "1711.11286", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao and Dylan S. Small and Bhaswar B. Bhattacharya", "title": "Sensitivity analysis for inverse probability weighting estimators via\n  the percentile bootstrap", "comments": "32 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To identify the estimand in missing data problems and observational studies,\nit is common to base the statistical estimation on the \"missing at random\" and\n\"no unmeasured confounder\" assumptions. However, these assumptions are\nunverifiable using empirical data and pose serious threats to the validity of\nthe qualitative conclusions of the statistical inference. A sensitivity\nanalysis asks how the conclusions may change if the unverifiable assumptions\nare violated to a certain degree. In this paper we consider a marginal\nsensitivity model which is a natural extension of Rosenbaum's sensitivity model\nthat is widely used for matched observational studies. We aim to construct\nconfidence intervals based on inverse probability weighting estimators, such\nthat asymptotically the intervals have at least nominal coverage of the\nestimand whenever the data generating distribution is in the collection of\nmarginal sensitivity models. We use a percentile bootstrap and a generalized\nminimax/maximin inequality to transform this intractable problem to a linear\nfractional programming problem, which can be solved very efficiently. We\nillustrate our method using a real dataset to estimate the causal effect of\nfish consumption on blood mercury level.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 09:44:09 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 14:52:28 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Small", "Dylan S.", ""], ["Bhattacharya", "Bhaswar B.", ""]]}, {"id": "1711.11532", "submitter": "Igor Silin", "authors": "Igor Silin, Vladimir Spokoiny", "title": "Bayesian inference for spectral projectors of the covariance matrix", "comments": "40 pages, 2 figures, accepted version", "journal-ref": "Electronic Journal of Statistics, Vol. 12 (2018), 1948--1987", "doi": "10.1214/18-EJS1451", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_1, \\ldots, X_n$ be i.i.d. sample in $\\mathbb{R}^p$ with zero mean and\nthe covariance matrix $\\mathbf{\\Sigma^*}$. The classical PCA approach recovers\nthe projector $\\mathbf{P^*_{\\mathcal{J}}}$ onto the principal eigenspace of\n$\\mathbf{\\Sigma^*}$ by its empirical counterpart\n$\\mathbf{\\widehat{P}_{\\mathcal{J}}}$. Recent paper [Koltchinskii, Lounici\n(2017)] investigated the asymptotic distribution of the Frobenius distance\nbetween the projectors $\\| \\mathbf{\\widehat{P}_{\\mathcal{J}}} -\n\\mathbf{P^*_{\\mathcal{J}}} \\|_2$, while [Naumov et al. (2017)] offered a\nbootstrap procedure to measure uncertainty in recovering this subspace\n$\\mathbf{P^*_{\\mathcal{J}}}$ even in a finite sample setup. The present paper\nconsiders this problem from a Bayesian perspective and suggests to use the\ncredible sets of the pseudo-posterior distribution on the space of covariance\nmatrices induced by the conjugated Inverse Wishart prior as sharp confidence\nsets. This yields a numerically efficient procedure. Moreover, we theoretically\njustify this method and derive finite sample bounds on the corresponding\ncoverage probability. Contrary to [Koltchinskii, Lounici (2017), Naumov et al.\n(2017)], the obtained results are valid for non-Gaussian data: the main\nassumption that we impose is the concentration of the sample covariance\n$\\mathbf{\\widehat{\\Sigma}}$ in a vicinity of $\\mathbf{\\Sigma^*}$. Numerical\nsimulations illustrate good performance of the proposed procedure even on\nnon-Gaussian data in a rather challenging regime.\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 17:42:10 GMT"}, {"version": "v2", "created": "Sun, 10 Dec 2017 09:20:17 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 21:23:07 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Silin", "Igor", ""], ["Spokoiny", "Vladimir", ""]]}, {"id": "1711.11560", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne, Ilias Diakonikolas, Daniel M. Kane, Alistair\n  Stewart", "title": "Testing Conditional Independence of Discrete Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CC cs.DM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing \\emph{conditional independence} for discrete\ndistributions. Specifically, given samples from a discrete random variable $(X,\nY, Z)$ on domain $[\\ell_1]\\times[\\ell_2] \\times [n]$, we want to distinguish,\nwith probability at least $2/3$, between the case that $X$ and $Y$ are\nconditionally independent given $Z$ from the case that $(X, Y, Z)$ is\n$\\epsilon$-far, in $\\ell_1$-distance, from every distribution that has this\nproperty. Conditional independence is a concept of central importance in\nprobability and statistics with a range of applications in various scientific\ndomains. As such, the statistical task of testing conditional independence has\nbeen extensively studied in various forms within the statistics and\neconometrics communities for nearly a century. Perhaps surprisingly, this\nproblem has not been previously considered in the framework of distribution\nproperty testing and in particular no tester with sublinear sample complexity\nis known, even for the important special case that the domains of $X$ and $Y$\nare binary.\n  The main algorithmic result of this work is the first conditional\nindependence tester with {\\em sublinear} sample complexity for discrete\ndistributions over $[\\ell_1]\\times[\\ell_2] \\times [n]$. To complement our upper\nbounds, we prove information-theoretic lower bounds establishing that the\nsample complexity of our algorithm is optimal, up to constant factors, for a\nnumber of settings. Specifically, for the prototypical setting when $\\ell_1,\n\\ell_2 = O(1)$, we show that the sample complexity of testing conditional\nindependence (upper bound and matching lower bound) is\n  \\[\n  \\Theta\\left({\\max\\left(n^{1/2}/\\epsilon^2,\\min\\left(n^{7/8}/\\epsilon,n^{6/7}/\\epsilon^{8/7}\\right)\\right)}\\right)\\,.\n  \\]\n", "versions": [{"version": "v1", "created": "Thu, 30 Nov 2017 18:30:02 GMT"}, {"version": "v2", "created": "Sun, 1 Jul 2018 19:59:50 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""], ["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Stewart", "Alistair", ""]]}]