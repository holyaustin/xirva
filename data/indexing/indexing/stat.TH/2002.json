[{"id": "2002.00071", "submitter": "Cole Franks", "authors": "Cole Franks, Ankur Moitra", "title": "Rigorous Guarantees for Tyler's M-estimator via quantum expansion", "comments": "Added Frobenius guarantees", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the shape of an elliptical distribution is a fundamental problem\nin statistics. One estimator for the shape matrix, Tyler's M-estimator, has\nbeen shown to have many appealing asymptotic properties. It performs well in\nnumerical experiments and can be quickly computed in practice by a simple\niterative procedure. Despite the many years the estimator has been studied in\nthe statistics community, there was neither a non-asymptotic bound on the rate\nof the estimator nor a proof that the iterative procedure converges in\npolynomially many steps.\n  Here we observe a surprising connection between Tyler's M-estimator and\noperator scaling, which has been intensively studied in recent years in part\nbecause of its connections to the Brascamp-Lieb inequality in analysis. We use\nthis connection, together with novel results on quantum expanders, to show that\nTyler's M-estimator has the optimal rate up to factors logarithmic in the\ndimension, and that in the generative model the iterative procedure has a\nlinear convergence rate even without regularization.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 22:01:51 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 19:32:28 GMT"}, {"version": "v3", "created": "Wed, 19 Feb 2020 14:09:21 GMT"}, {"version": "v4", "created": "Wed, 1 Jul 2020 16:37:33 GMT"}], "update_date": "2020-07-02", "authors_parsed": [["Franks", "Cole", ""], ["Moitra", "Ankur", ""]]}, {"id": "2002.00178", "submitter": "Pierre Wolinski", "authors": "Pierre Wolinski, Guillaume Charpiat, Yann Ollivier", "title": "An Equivalence between Bayesian Priors and Penalties in Variational\n  Inference", "comments": "17 pages, 2 columns, including 2 pages of references and 7 pages of\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, it is common to optimize the parameters of a\nprobabilistic model, modulated by an ad hoc regularization term that penalizes\nsome values of the parameters. Regularization terms appear naturally in\nVariational Inference (VI), a tractable way to approximate Bayesian posteriors:\nthe loss to optimize contains a Kullback--Leibler divergence term between the\napproximate posterior and a Bayesian prior. We fully characterize which\nregularizers can arise this way, and provide a systematic way to compute the\ncorresponding prior. This viewpoint also provides a prediction for useful\nvalues of the regularization factor in neural networks. We apply this framework\nto regularizers such as L2, L1 or group-Lasso.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 09:48:51 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 10:11:57 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Wolinski", "Pierre", ""], ["Charpiat", "Guillaume", ""], ["Ollivier", "Yann", ""]]}, {"id": "2002.00274", "submitter": "Dheeraj Nagaraj", "authors": "Guy Bresler and Dheeraj Nagaraj", "title": "A Corrective View of Neural Networks: Representation, Memorization and\n  Learning", "comments": "Contains 2 figures (you heard that right!), V2 removes dimension\n  dependence in memorization bounds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a corrective mechanism for neural network approximation: the total\navailable non-linear units are divided into multiple groups and the first group\napproximates the function under consideration, the second group approximates\nthe error in approximation produced by the first group and corrects it, the\nthird group approximates the error produced by the first and second groups\ntogether and so on. This technique yields several new representation and\nlearning results for neural networks. First, we show that two-layer neural\nnetworks in the random features regime (RF) can memorize arbitrary labels for\narbitrary points under under Euclidean distance separation condition using\n$\\tilde{O}(n)$ ReLUs which is optimal in $n$ up to logarithmic factors. Next,\nwe give a powerful representation result for two-layer neural networks with\nReLUs and smoothed ReLUs which can achieve a squared error of at most\n$\\epsilon$ with $O(C(a,d)\\epsilon^{-1/(a+1)})$ for $a \\in \\mathbb{N}\\cup\\{0\\}$\nwhen the function is smooth enough (roughly when it has $\\Theta(ad)$ bounded\nderivatives). In certain cases $d$ can be replaced with effective dimension $q\n\\ll d$. Previous results of this type implement Taylor series approximation\nusing deep architectures. We also consider three-layer neural networks and show\nthat the corrective mechanism yields faster representation rates for smooth\nradial functions. Lastly, we obtain the first $O(\\mathrm{subpoly}(1/\\epsilon))$\nupper bound on the number of neurons required for a two layer network to learn\nlow degree polynomials up to squared error $\\epsilon$ via gradient descent.\nEven though deep networks can express these polynomials with\n$O(\\mathrm{polylog}(1/\\epsilon))$ neurons, the best learning bounds on this\nproblem require $\\mathrm{poly}(1/\\epsilon)$ neurons.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 20:51:09 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 02:37:48 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bresler", "Guy", ""], ["Nagaraj", "Dheeraj", ""]]}, {"id": "2002.00291", "submitter": "Niladri Chatterji", "authors": "Niladri S. Chatterji, Peter L. Bartlett, Philip M. Long", "title": "Oracle Lower Bounds for Stochastic Gradient Sampling Algorithms", "comments": "21 pages; accepted for publication at Bernoulli", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling from a strongly log-concave density in\n$\\mathbb{R}^d$, and prove an information theoretic lower bound on the number of\nstochastic gradient queries of the log density needed. Several popular sampling\nalgorithms (including many Markov chain Monte Carlo methods) operate by using\nstochastic gradients of the log density to generate a sample; our results\nestablish an information theoretic limit for all these algorithms.\n  We show that for every algorithm, there exists a well-conditioned strongly\nlog-concave target density for which the distribution of points generated by\nthe algorithm would be at least $\\varepsilon$ away from the target in total\nvariation distance if the number of gradient queries is less than\n$\\Omega(\\sigma^2 d/\\varepsilon^2)$, where $\\sigma^2 d$ is the variance of the\nstochastic gradient. Our lower bound follows by combining the ideas of Le Cam\ndeficiency routinely used in the comparison of statistical experiments along\nwith standard information theoretic tools used in lower bounding Bayes risk\nfunctions. To the best of our knowledge our results provide the first\nnontrivial dimension-dependent lower bound for this problem.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 23:46:35 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 20:35:47 GMT"}, {"version": "v3", "created": "Sat, 3 Jul 2021 04:12:14 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Chatterji", "Niladri S.", ""], ["Bartlett", "Peter L.", ""], ["Long", "Philip M.", ""]]}, {"id": "2002.00329", "submitter": "Jeongyeol Kwon", "authors": "Jeongyeol Kwon, Constantine Caramanis", "title": "The EM Algorithm gives Sample-Optimality for Learning Mixtures of\n  Well-Separated Gaussians", "comments": "Accepted to COLT 2020; Title changed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of spherical Gaussian Mixture models with $k \\geq 3$\ncomponents when the components are well separated. A fundamental previous\nresult established that separation of $\\Omega(\\sqrt{\\log k})$ is necessary and\nsufficient for identifiability of the parameters with polynomial sample\ncomplexity (Regev and Vijayaraghavan, 2017). In the same context, we show that\n$\\tilde{O} (kd/\\epsilon^2)$ samples suffice for any $\\epsilon \\lesssim 1/k$,\nclosing the gap from polynomial to linear, and thus giving the first optimal\nsample upper bound for the parameter estimation of well-separated Gaussian\nmixtures. We accomplish this by proving a new result for the\nExpectation-Maximization (EM) algorithm: we show that EM converges locally,\nunder separation $\\Omega(\\sqrt{\\log k})$. The previous best-known guarantee\nrequired $\\Omega(\\sqrt{k})$ separation (Yan, et al., 2017). Unlike prior work,\nour results do not assume or use prior knowledge of the (potentially different)\nmixing weights or variances of the Gaussian components. Furthermore, our\nresults show that the finite-sample error of EM does not depend on\nnon-universal quantities such as pairwise distances between means of Gaussian\ncomponents.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 05:09:26 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 17:36:40 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Kwon", "Jeongyeol", ""], ["Caramanis", "Constantine", ""]]}, {"id": "2002.00384", "submitter": "Krzysztof Szajowski", "authors": "Krzysztof J. Szajowski", "title": "A Model of Distributed Disorders Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.SY eess.SY math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with disorders detection in the multivariate stochastic\nprocess. We consider the multidimensional Poisson process or the multivariate\nrenewal process. This class of processes can be used as a description of the\ndistributed detection system. The multivariate renewal process can be seen as\nthe sequence of random vectors, where parts of its coordinates are holding\ntimes, others are the size of jumps and the index of stream, at which the new\nevent appears. It is assumed that at each stream two kinds of changes are\npossible: in the holding time or in the size of jumps distribution. The various\nspecific mutual relations between the change points are possible. The aim of\nthe research is to derive the detectors which realize the optimal value of the\nspecified criterion. The change point moment estimates have been obtained in\nsome cases. The difficulties have appeared for the dependent streams with\nunspecified order of change points. The presented results suggest further\nresearch on the construction of detectors in the general model.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 12:43:24 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Szajowski", "Krzysztof J.", ""]]}, {"id": "2002.00492", "submitter": "Peizhong Ju", "authors": "Peizhong Ju, Xiaojun Lin, Jia Liu", "title": "Overfitting Can Be Harmless for Basis Pursuit, But Only to a Degree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there have been significant interests in studying the so-called\n\"double-descent\" of the generalization error of linear regression models under\nthe overparameterized and overfitting regime, with the hope that such analysis\nmay provide the first step towards understanding why overparameterized deep\nneural networks (DNN) still generalize well. However, to date most of these\nstudies focused on the min $\\ell_2$-norm solution that overfits the data. In\ncontrast, in this paper we study the overfitting solution that minimizes the\n$\\ell_1$-norm, which is known as Basis Pursuit (BP) in the compressed sensing\nliterature. Under a sparse true linear regression model with $p$ i.i.d.\nGaussian features, we show that for a large range of $p$ up to a limit that\ngrows exponentially with the number of samples $n$, with high probability the\nmodel error of BP is upper bounded by a value that decreases with $p$. To the\nbest of our knowledge, this is the first analytical result in the literature\nestablishing the double-descent of overfitting BP for finite $n$ and $p$.\nFurther, our results reveal significant differences between the double-descent\nof BP and min $\\ell_2$-norm solutions. Specifically, the double-descent\nupper-bound of BP is independent of the signal strength, and for high SNR and\nsparse models the descent-floor of BP can be much lower and wider than that of\nmin $\\ell_2$-norm solutions.\n", "versions": [{"version": "v1", "created": "Sun, 2 Feb 2020 20:48:39 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 23:29:36 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Ju", "Peizhong", ""], ["Lin", "Xiaojun", ""], ["Liu", "Jia", ""]]}, {"id": "2002.00852", "submitter": "Quentin Paris", "authors": "Quentin Paris", "title": "The exponentially weighted average forecaster in geodesic spaces of\n  non-positive curvature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.GT cs.LG math.MG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of prediction with expert advice for\noutcomes in a geodesic space with non-positive curvature in the sense of\nAlexandrov. Via geometric considerations, and in particular the notion of\nbarycenters, we extend to this setting the definition and analysis of the\nclassical exponentially weighted average forecaster. We also adapt the\nprinciple of online to batch conversion to this setting. We shortly discuss the\napplication of these results in the context of aggregation and for the problem\nof barycenter estimation.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 15:59:42 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Paris", "Quentin", ""]]}, {"id": "2002.01012", "submitter": "Ziv Goldfeld", "authors": "Ziv Goldfeld, Kristjan Greenewald, and Kengo Kato", "title": "Asymptotic Guarantees for Generative Modeling Based on the Smooth\n  Wasserstein Distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Minimum distance estimation (MDE) gained recent attention as a formulation of\n(implicit) generative modeling. It considers minimizing, over model parameters,\na statistical distance between the empirical data distribution and the model.\nThis formulation lends itself well to theoretical analysis, but typical results\nare hindered by the curse of dimensionality. To overcome this and devise a\nscalable finite-sample statistical MDE theory, we adopt the framework of smooth\n1-Wasserstein distance (SWD) $\\mathsf{W}_1^{(\\sigma)}$. The SWD was recently\nshown to preserve the metric and topological structure of classic Wasserstein\ndistances, while enjoying dimension-free empirical convergence rates. In this\nwork, we conduct a thorough statistical study of the minimum smooth Wasserstein\nestimators (MSWEs), first proving the estimator's measurability and asymptotic\nconsistency. We then characterize the limit distribution of the optimal model\nparameters and their associated minimal SWD. These results imply an\n$O(n^{-1/2})$ generalization bound for generative modeling based on MSWE, which\nholds in arbitrary dimension. Our main technical tool is a novel\nhigh-dimensional limit distribution result for empirical\n$\\mathsf{W}_1^{(\\sigma)}$. The characterization of a nondegenerate limit stands\nin sharp contrast with the classic empirical 1-Wasserstein distance, for which\na similar result is known only in the one-dimensional case. The validity of our\ntheory is supported by empirical results, posing the SWD as a potent tool for\nlearning and inference in high dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 20:51:11 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 12:58:27 GMT"}, {"version": "v3", "created": "Fri, 2 Oct 2020 01:39:58 GMT"}, {"version": "v4", "created": "Tue, 20 Oct 2020 02:56:03 GMT"}], "update_date": "2020-10-21", "authors_parsed": [["Goldfeld", "Ziv", ""], ["Greenewald", "Kristjan", ""], ["Kato", "Kengo", ""]]}, {"id": "2002.01013", "submitter": "Ziv Goldfeld", "authors": "Ziv Goldfeld and Kengo Kato", "title": "Limit Distribution for Smooth Total Variation and $\\chi^2$-Divergence in\n  High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical divergences are ubiquitous in machine learning as tools for\nmeasuring discrepancy between probability distributions. As these applications\ninherently rely on approximating distributions from samples, we consider\nempirical approximation under two popular $f$-divergences: the total variation\n(TV) distance and the $\\chi^2$-divergence. To circumvent the sensitivity of\nthese divergences to support mismatch, the framework of Gaussian smoothing is\nadopted. We study the limit distributions of\n$\\sqrt{n}\\delta_{\\mathsf{TV}}(P_n\\ast\\mathcal{N},P\\ast\\mathcal{N})$ and\n$n\\chi^2(P_n\\ast\\mathcal{N}\\|P\\ast\\mathcal{N})$, where $P_n$ is the empirical\nmeasure based on $n$ independently and identically distributed (i.i.d.)\nobservations from $P$,\n$\\mathcal{N}_\\sigma:=\\mathcal{N}(0,\\sigma^2\\mathrm{I}_d)$, and $\\ast$ stands\nfor convolution. In arbitrary dimension, the limit distributions are\ncharacterized in terms of Gaussian process on $\\mathbb{R}^d$ with covariance\noperator that depends on $P$ and the isotropic Gaussian density of parameter\n$\\sigma$. This, in turn, implies optimality of the $n^{-1/2}$ expected value\nconvergence rates recently derived for\n$\\delta_{\\mathsf{TV}}(P_n\\ast\\mathcal{N},P\\ast\\mathcal{N})$ and\n$\\chi^2(P_n\\ast\\mathcal{N}\\|P\\ast\\mathcal{N})$. These strong statistical\nguarantees promote empirical approximation under Gaussian smoothing as a potent\nframework for learning and inference based on high-dimensional data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 20:55:39 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 14:37:07 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Goldfeld", "Ziv", ""], ["Kato", "Kengo", ""]]}, {"id": "2002.01040", "submitter": "Fernanda Lang Schumacher", "authors": "Fernanda L. Schumacher, Victor H. Lachos and Larissa A. Matos", "title": "Scale mixture of skew-normal linear mixed models with within-subject\n  serial dependence", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8870", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In longitudinal studies, repeated measures are collected over time and hence\nthey tend to be serially correlated. In this paper we consider an extension of\nskew-normal/independent linear mixed models introduced by Lachos et al. (2010),\nwhere the error term has a dependence structure, such as damped exponential\ncorrelation or autoregressive correlation of order p. The proposed model\nprovides flexibility in capturing the effects of skewness and heavy tails\nsimultaneously when continuous repeated measures are serially correlated. For\nthis robust model, we present an efficient EM-type algorithm for computation of\nmaximum likelihood estimation of parameters and the observed information matrix\nis derived analytically to account for standard errors. The methodology is\nillustrated through an application to schizophrenia data and several simulation\nstudies. The proposed algorithm and methods are implemented in the new R\npackage skewlmm.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 22:42:13 GMT"}, {"version": "v2", "created": "Thu, 14 May 2020 20:46:49 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 03:11:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Schumacher", "Fernanda L.", ""], ["Lachos", "Victor H.", ""], ["Matos", "Larissa A.", ""]]}, {"id": "2002.01044", "submitter": "Ardhendu Shekhar Tripathy", "authors": "Matthew L. Malloy, Ardhendu Tripathy, Robert D. Nowak", "title": "Optimal Confidence Regions for the Multinomial Parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Construction of tight confidence regions and intervals is central to\nstatistical inference and decision making. This paper develops new theory\nshowing minimum average volume confidence regions for categorical data. More\nprecisely, consider an empirical distribution $\\widehat{\\boldsymbol{p}}$\ngenerated from $n$ iid realizations of a random variable that takes one of $k$\npossible values according to an unknown distribution $\\boldsymbol{p}$. This is\nanalogous to a single draw from a multinomial distribution. A confidence region\nis a subset of the probability simplex that depends on\n$\\widehat{\\boldsymbol{p}}$ and contains the unknown $\\boldsymbol{p}$ with a\nspecified confidence. This paper shows how one can construct minimum average\nvolume confidence regions, answering a long standing question. We also show the\noptimality of the regions directly translates to optimal confidence intervals\nof linear functionals such as the mean, implying sample complexity and regret\nimprovements for adaptive machine learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 23:00:16 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 18:58:14 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Malloy", "Matthew L.", ""], ["Tripathy", "Ardhendu", ""], ["Nowak", "Robert D.", ""]]}, {"id": "2002.01052", "submitter": "Ryan Martin", "authors": "Indrabati Bhattacharya and Ryan Martin", "title": "Gibbs posterior inference on multivariate quantiles", "comments": "24 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian and other likelihood-based methods require specification of a\nstatistical model and may not be fully satisfactory for inference on\nquantities, such as quantiles, that are not naturally defined as model\nparameters. In this paper, we construct a direct and model-free Gibbs posterior\ndistribution for multivariate quantiles. Being model-free means that inferences\ndrawn from the Gibbs posterior are not subject to model misspecification bias,\nand being direct means that no priors for or marginalization over nuisance\nparameters are required. We show here that the Gibbs posterior enjoys a\nroot-$n$ convergence rate and a Bernstein--von Mises property, i.e., for large\nn, the Gibbs posterior distribution can be approximated by a Gaussian.\nMoreover, we present numerical results showing the validity and efficiency of\ncredible sets derived from a suitably scaled Gibbs posterior.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 23:40:36 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 11:56:23 GMT"}, {"version": "v3", "created": "Tue, 13 Apr 2021 09:56:12 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Bhattacharya", "Indrabati", ""], ["Martin", "Ryan", ""]]}, {"id": "2002.01182", "submitter": "Shahar Mendelson", "authors": "Shahar Mendelson", "title": "Learning bounded subsets of $L_p$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study learning problems in which the underlying class is a bounded subset\nof $L_p$ and the target $Y$ belongs to $L_p$. Previously, minimax sample\ncomplexity estimates were known under such boundedness assumptions only when\n$p=\\infty$. We present a sharp sample complexity estimate that holds for any $p\n> 4$. It is based on a learning procedure that is suited for heavy-tailed\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 09:25:34 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Mendelson", "Shahar", ""]]}, {"id": "2002.01216", "submitter": "Clement Levrard", "authors": "Fr\\'ed\\'eric Chazal (DATASHAPE), Cl\\'ement Levrard (DATASHAPE, LPSM\n  (UMR\\_8001)), Martin Royer (DATASHAPE)", "title": "Optimal quantization of the mean measure and applications to statistical\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the case where data come as point sets, or more\ngenerally as discrete measures. Our motivation is twofold: first we intend to\napproximate with a compactly supported measure the mean of the measure\ngenerating process, that coincides with the intensity measure in the point\nprocess framework, or with the expected persistence diagram in the framework of\npersistence-based topological data analysis. To this aim we provide two\nalgorithms that we prove almost minimax optimal. Second we build from the\nestimator of the mean measure a vectorization map, that sends every measure\ninto a finite-dimensional Euclidean space, and investigate its properties\nthrough a clustering-oriented lens. In a nutshell, we show that in a mixture of\nmeasure generating process, our technique yields a representation in\n$\\mathbb{R}^k$, for $k \\in \\mathbb{N}^*$ that guarantees a good clustering of\nthe data points with high probability. Interestingly, our results apply in the\nframework of persistence-based shape classification via the ATOL procedure\ndescribed in \\cite{Royer19}.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 10:45:51 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 14:16:56 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 07:47:52 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 09:08:01 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Chazal", "Fr\u00e9d\u00e9ric", "", "DATASHAPE"], ["Levrard", "Cl\u00e9ment", "", "DATASHAPE, LPSM"], ["Royer", "Martin", "", "DATASHAPE"]]}, {"id": "2002.01303", "submitter": "Abhishake Rastogi", "authors": "Abhishake Rastogi", "title": "Tikhonov regularization with oversmoothing penalty for nonlinear\n  statistical inverse problems", "comments": "arXiv admin note: text overlap with arXiv:1902.05404", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the nonlinear ill-posed inverse problem with noisy\ndata in the statistical learning setting. The Tikhonov regularization scheme in\nHilbert scales is considered to reconstruct the estimator from the random noisy\ndata. In this statistical learning setting, we derive the rates of convergence\nfor the regularized solution under certain assumptions on the nonlinear forward\noperator and the prior assumptions. We discuss estimates of the reconstruction\nerror using the approach of reproducing kernel Hilbert spaces.\n", "versions": [{"version": "v1", "created": "Sat, 1 Feb 2020 19:49:08 GMT"}], "update_date": "2020-02-05", "authors_parsed": [["Rastogi", "Abhishake", ""]]}, {"id": "2002.01381", "submitter": "Wenjia Wang", "authors": "Wenjia Wang", "title": "On the Inference of Applying Gaussian Process Modeling to a\n  Deterministic Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process modeling is a standard tool for building emulators for\ncomputer experiments, which are usually used to study deterministic functions,\nfor example, a solution to a given system of partial differential equations.\nThis work investigates applying Gaussian process modeling to a deterministic\nfunction from prediction and uncertainty quantification perspectives, where the\nGaussian process model is misspecified. Specifically, we consider the case\nwhere the underlying function is fixed and from a reproducing kernel Hilbert\nspace generated by some kernel function, and the same kernel function is used\nin the Gaussian process modeling as the correlation function for prediction and\nuncertainty quantification. While upper bounds and optimal convergence rate of\nprediction in the Gaussian process modeling have been extensively studied in\nthe literature, a thorough exploration of convergence rates and theoretical\nstudy of uncertainty quantification is lacking. We prove that, if one uses\nmaximum likelihood estimation to estimate the variance in Gaussian process\nmodeling, under different choices of the nugget parameter value, the predictor\nis not optimal and/or the confidence interval is not reliable. In particular,\nlower bounds of the prediction error under different choices of the nugget\nparameter value are obtained. The results indicate that, if one directly\napplies Gaussian process modeling to a fixed function, the reliability of the\nconfidence interval and the optimality of the predictor cannot be achieved at\nthe same time.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 15:59:23 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 04:24:44 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Wenjia", ""]]}, {"id": "2002.01432", "submitter": "Arshak Minasyan", "authors": "Arnak S. Dalalyan and Arshak Minasyan", "title": "All-In-One Robust Estimator of the Gaussian Mean", "comments": "41 pages, 5 figures; added sub-Gaussian case with unknown Sigma or\n  eps", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to show that a single robust estimator of the mean\nof a multivariate Gaussian distribution can enjoy five desirable properties.\nFirst, it is computationally tractable in the sense that it can be computed in\na time which is at most polynomial in dimension, sample size and the logarithm\nof the inverse of the contamination rate. Second, it is equivariant by\ntranslations, uniform scaling and orthogonal transformations. Third, it has a\nhigh breakdown point equal to $0.5$, and a nearly-minimax-rate-breakdown point\napproximately equal to $0.28$. Fourth, it is minimax rate optimal, up to a\nlogarithmic factor, when data consists of independent observations corrupted by\nadversarially chosen outliers. Fifth, it is asymptotically efficient when the\nrate of contamination tends to zero. The estimator is obtained by an iterative\nreweighting approach. Each sample point is assigned a weight that is\niteratively updated by solving a convex optimization problem. We also establish\na dimension-free non-asymptotic risk bound for the expected error of the\nproposed estimator. It is the first result of this kind in the literature and\ninvolves only the effective rank of the covariance matrix. Finally, we show\nthat the obtained results can be extended to sub-Gaussian distributions, as\nwell as to the cases of unknown rate of contamination or unknown covariance\nmatrix.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 17:52:34 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 11:38:16 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Dalalyan", "Arnak S.", ""], ["Minasyan", "Arshak", ""]]}, {"id": "2002.01526", "submitter": "Wenjia Wang", "authors": "Wenjia Wang, Xiaowei Yue, Benjamin Haaland, C. F. Jeff Wu", "title": "Gaussian Processes with Input Location Error and Applications to the\n  Composite Parts Assembly Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate Gaussian process modeling with input location\nerror, where the inputs are corrupted by noise. Here, the best linear unbiased\npredictor for two cases is considered, according to whether there is noise at\nthe target unobserved location or not. We show that the mean squared prediction\nerror converges to a non-zero constant if there is noise at the target\nunobserved location, and provide an upper bound of the mean squared prediction\nerror if there is no noise at the target unobserved location. We investigate\nthe use of stochastic Kriging in the prediction of Gaussian processes with\ninput location error, and show that stochastic Kriging is a good approximation\nwhen the sample size is large. Several numeric examples are given to illustrate\nthe results, and a case study on the assembly of composite parts is presented.\nTechnical proofs are provided in the Appendix.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 20:30:55 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 08:25:00 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Wenjia", ""], ["Yue", "Xiaowei", ""], ["Haaland", "Benjamin", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "2002.01569", "submitter": "Wenjia Wang", "authors": "Rui Tuo, Wenjia Wang", "title": "Uncertainty Quantification for Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization is a class of global optimization techniques. It\nregards the underlying objective function as a realization of a Gaussian\nprocess. Although the outputs of Bayesian optimization are random according to\nthe Gaussian process assumption, quantification of this uncertainty is rarely\nstudied in the literature. In this work, we propose a novel approach to assess\nthe output uncertainty of Bayesian optimization algorithms, in terms of\nconstructing confidence regions of the maximum point or value of the objective\nfunction. These regions can be computed efficiently, and their confidence\nlevels are guaranteed by newly developed uniform error bounds for sequential\nGaussian process regression. Our theory provides a unified uncertainty\nquantification framework for all existing sequential sampling policies and\nstopping criteria.\n", "versions": [{"version": "v1", "created": "Tue, 4 Feb 2020 22:48:07 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Tuo", "Rui", ""], ["Wang", "Wenjia", ""]]}, {"id": "2002.01586", "submitter": "Tengyuan Liang", "authors": "Tengyuan Liang and Pragya Sur", "title": "A Precise High-Dimensional Asymptotic Theory for Boosting and\n  Minimum-$\\ell_1$-Norm Interpolated Classifiers", "comments": "68 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes a precise high-dimensional asymptotic theory for\nboosting on separable data, taking statistical and computational perspectives.\nWe consider a high-dimensional setting where the number of features (weak\nlearners) $p$ scales with the sample size $n$, in an overparametrized regime.\nUnder a class of statistical models, we provide an exact analysis of the\ngeneralization error of boosting when the algorithm interpolates the training\ndata and maximizes the empirical $\\ell_1$-margin. Further, we explicitly pin\ndown the relation between the boosting test error and the optimal Bayes error,\nas well as the proportion of active features at interpolation (with zero\ninitialization). In turn, these precise characterizations answer certain\nquestions raised in \\cite{breiman1999prediction, schapire1998boosting}\nsurrounding boosting, under assumed data generating processes. At the heart of\nour theory lies an in-depth study of the maximum-$\\ell_1$-margin, which can be\naccurately described by a new system of non-linear equations; to analyze this\nmargin, we rely on Gaussian comparison techniques and develop a novel uniform\ndeviation argument. Our statistical and computational arguments can handle (1)\nany finite-rank spiked covariance model for the feature distribution and (2)\nvariants of boosting corresponding to general $\\ell_q$-geometry, $q \\in [1,\n2]$. As a final component, via the Lindeberg principle, we establish a\nuniversality result showcasing that the scaled $\\ell_1$-margin (asymptotically)\nremains the same, whether the covariates used for boosting arise from a\nnon-linear random feature model or an appropriately linearized model with\nmatching moments.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 00:24:53 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 20:49:20 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 20:55:22 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Liang", "Tengyuan", ""], ["Sur", "Pragya", ""]]}, {"id": "2002.01740", "submitter": "Davit Varron", "authors": "Benjamin Bobbia, Cl\\'ement Dombry, Davit Varron", "title": "Extreme quantile regression in a proportional tail framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the model of heteroscedastic extremes initially introduced by\nEinmahl et al. (JRSSB, 2016) to describe the evolution of a non stationary\nsequence whose extremes evolve over time and adapt it into a general extreme\nquantile regression framework. We provide estimates for the extreme value index\nand the integrated skedasis function and prove their asymptotic normality. Our\nresults are quite similar to those developed for heteroscedastic extremes but\nwith a different proof approach emphasizing coupling arguments. We also propose\na pointwise estimator of the skedasis function and a Weissman estimator of the\nconditional extreme quantile and prove the asymptotic normality of both\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 12:01:06 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Bobbia", "Benjamin", ""], ["Dombry", "Cl\u00e9ment", ""], ["Varron", "Davit", ""]]}, {"id": "2002.01800", "submitter": "Mehmet Caner", "authors": "Mehmet Caner, Marcelo Medeiros, and Gabriel Vasconcelos", "title": "Residual-Based Nodewise Regression in Factor Models with Ultra-High\n  Dimensions: Analysis of Mean-Variance Portfolio Efficiency and Estimation of\n  Out-of-Sample and Constrained Maximum Sharpe Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM econ.EM math.ST q-fin.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new theory for nodewise regression when the residuals from a\nfitted factor model are used to apply our results to the analysis of the\nmaximum Sharpe ratio when the number of assets in a portfolio is larger than\nits time span. We introduce a new hybrid model where factor models are combined\nwith feasible nodewise regression. Returns are generated from an increasing\nnumber of factors plus idiosyncratic components (errors). The precision matrix\nof the idiosyncratic terms is assumed to be sparse, but the respective\ncovariance matrix can be non-sparse. Since the nodewise regression is not\nfeasible due to the unknown nature of errors, we provide a\nfeasible-residual-based nodewise regression to estimate the precision matrix of\nerrors as a new method. Next, we show that the residual-based nodewise\nregression provides a consistent estimate for the precision matrix of errors.\nIn another new development, we also show that the precision matrix of returns\ncan be estimated consistently, even with an increasing number of factors.\nBenefiting from the consistency of the precision matrix estimate of returns, we\nshow that: (1) the portfolios in high dimensions are mean-variance efficient;\n(2) maximum out-of-sample Sharpe ratio estimator is consistent and the number\nof assets slows the convergence up to a logarithmic factor; (3) the maximum\nSharpe ratio estimator is consistent when the portfolio weights sum to one; and\n(4) the Sharpe ratio estimators are consistent in global minimum-variance and\nmean-variance portfolios.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 14:16:30 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 15:27:37 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 19:19:00 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Caner", "Mehmet", ""], ["Medeiros", "Marcelo", ""], ["Vasconcelos", "Gabriel", ""]]}, {"id": "2002.01884", "submitter": "Robert Gaunt", "authors": "Robert E. Gaunt and Milan Merkle", "title": "On bounds for the mode and median of the generalized hyperbolic and\n  related distributions", "comments": "22 pages", "journal-ref": "Journal of Mathematical Analysis and Applications 493 Article\n  124508 (2021), pp. 19", "doi": "10.1016/j.jmaa.2020.124508", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Except for certain parameter values, a closed form formula for the mode of\nthe generalized hyperbolic (GH) distribution is not available. In this paper,\nwe exploit results from the literature on modified Bessel functions and their\nratios to obtain simple but tight two-sided inequalities for the mode of the GH\ndistribution for general parameter values. As a special case, we deduce tight\ntwo-sided inequalities for the mode of the variance-gamma (VG) distribution,\nand through a similar approach we also obtain tight two-sided inequalities for\nthe mode of the McKay Type I distribution. The analogous problem for the median\nis more challenging, but we conjecture some monotonicity results for the median\nof the VG and McKay Type I distributions, from we which we conjecture some\ntight two-sided inequalities for their medians. Numerical experiments support\nthese conjectures and also lead us to a conjectured tight lower bound for the\nmedian of the GH distribution.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 17:47:51 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 21:08:36 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 12:59:48 GMT"}, {"version": "v4", "created": "Wed, 2 Sep 2020 05:38:19 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Gaunt", "Robert E.", ""], ["Merkle", "Milan", ""]]}, {"id": "2002.02024", "submitter": "Michelle Carey", "authors": "Michelle Carey and James O. Ramsay", "title": "Fast Stable Parameter Estimation for Linear Dynamical Systems", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2020.107124", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamical systems describe the changes in processes that arise naturally from\ntheir underlying physical principles, such as the laws of motion or the\nconservation of mass, energy or momentum. These models facilitate a causal\nexplanation for the drivers and impediments of the processes. But do they\ndescribe the behaviour of the observed data? And how can we quantify the\nmodels' parameters that cannot be measured directly? This paper addresses these\ntwo questions by providing a methodology for estimating the solution; and the\nparameters of linear dynamical systems from incomplete and noisy observations\nof the processes.\n  The proposed procedure builds on the parameter cascading approach, where a\nlinear combination of basis functions approximates the implicitly defined\nsolution of the dynamical system. The systems' parameters are then estimated so\nthat this approximating solution adheres to the data. By taking advantage of\nthe linearity of the system, we have simplified the parameter cascading\nestimation procedure, and by developing a new iterative scheme, we achieve fast\nand stable computation.\n  We illustrate our approach by obtaining a linear differential equation that\nrepresents real data from biomechanics. Comparing our approach with popular\nmethods for estimating the parameters of linear dynamical systems, namely, the\nnon-linear least-squares approach, simulated annealing, parameter cascading and\nsmooth functional tempering reveals a considerable reduction in computation and\nan improved bias and sampling variance.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 22:37:25 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Carey", "Michelle", ""], ["Ramsay", "James O.", ""]]}, {"id": "2002.02116", "submitter": "Snigdha Bhagat Ms", "authors": "Snigdha Bhagat, S.D. Joshi", "title": "Quantification of Differential Information using Matrix Pencil", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any traditional classification problem in general involves modelling\nindividual classes and in turn classification by evaluating the similarity of\nthe test set with the modelled classes. In this paper, we introduce another\napproach that would find the differential information between two classes\nrather than modelling individual classes separately. The classes are viewed on\na common frame of reference in which one class would have a constant variance,\nunlike the other class which would have unequal variance along its basis\nvectors which would capture the differential information of one class over the\nother.This, when mathematically formulated, leads to the solution of Matrix\nPencil equation.The theory of binary classification was extended to a\nmulti-class scenario.This is borne out by illustrative examples on the\nclassification of the MNIST database.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 06:30:03 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Bhagat", "Snigdha", ""], ["Joshi", "S. D.", ""]]}, {"id": "2002.02339", "submitter": "Nikita Zhivotovskiy", "authors": "Yegor Klochkov, Alexey Kroshnin, and Nikita Zhivotovskiy", "title": "Robust $k$-means Clustering for Distributions with Two Moments", "comments": "28 pages, to appear in Ann. of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the robust algorithms for the $k$-means clustering problem where\na quantizer is constructed based on $N$ independent observations. Our main\nresults are median of means based non-asymptotic excess distortion bounds that\nhold under the two bounded moments assumption in a general separable Hilbert\nspace. In particular, our results extend the renowned asymptotic result of\nPollard, 1981 who showed that the existence of two moments is sufficient for\nstrong consistency of an empirically optimal quantizer in $\\mathbb{R}^d$. In a\nspecial case of clustering in $\\mathbb{R}^d$, under two bounded moments, we\nprove matching (up to constant factors) non-asymptotic upper and lower bounds\non the excess distortion, which depend on the probability mass of the lightest\ncluster of an optimal quantizer. Our bounds have the sub-Gaussian form, and the\nproofs are based on the versions of uniform bounds for robust mean estimators.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 16:36:53 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 15:59:41 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Klochkov", "Yegor", ""], ["Kroshnin", "Alexey", ""], ["Zhivotovskiy", "Nikita", ""]]}, {"id": "2002.02434", "submitter": "John Bob Gali", "authors": "John Bob Gali, Priyadip Ray and Goutam Das", "title": "GLRT based Adaptive-Thresholding for CFAR-Detection of Pareto-Target in\n  Pareto-Distributed Clutter", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After Pareto distribution has been validated for sea clutter returns in\nvaried scenarios, some heuristics of adaptive-thresholding appeared in the\nliterature for constant false alarm rate (CFAR) criteria. These schemes used\nthe same adaptive-thresholding form that was originally derived for detecting\nSwerling-I (exponential) target in exponentially distributed clutter.\nStatistical procedures obtained under such idealistic assumptions would affect\nthe detection performance when applied to newer target and clutter models, esp.\nheavy tail distributions like Pareto. Further, in addition to the sea clutter\nreturns, it has also been reported that Generalized Pareto distribution fits\nbest for the measured Radar-cross-section (RCS) data of a SAAB aircraft.\nTherefore, in Radar application scenarios like Airborne Warning and Control\nSystem (AWACS), when both the target and clutter are Pareto distributed, we\npose the detection problem as a two-sample, Pareto vs. Pareto composite\nhypothesis testing problem. We address this problem by corroborating the binary\nhypothesis framework instead of the conventional way of tweaking the existing\nadaptive-thresholding CFAR detector. Whereby, for the composite case,\nconsidering no knowledge of both scale and shape parameters of Pareto\ndistributed clutter, we derive the new adaptive-thresholding detector based on\nthe generalized likelihood ratio test (GLRT) statistic. We further show that\nour proposed adaptive-thresholding detector has a CFAR property. We provide\nextensive simulation results to demonstrate the performance of the proposed\ndetector.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 18:36:15 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 06:35:16 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Gali", "John Bob", ""], ["Ray", "Priyadip", ""], ["Das", "Goutam", ""]]}, {"id": "2002.02525", "submitter": "Seth Strimas-Mackey", "authors": "Florentina Bunea, Seth Strimas-Mackey, Marten Wegkamp", "title": "Interpolating Predictors in High-Dimensional Factor Regression", "comments": "47 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies finite-sample properties of the risk of the minimum-norm\ninterpolating predictor in high-dimensional regression models. If the effective\nrank of the covariance matrix $\\Sigma$ of the $p$ regression features is much\nlarger than the sample size $n$, we show that the min-norm interpolating\npredictor is not desirable, as its risk approaches the risk of trivially\npredicting the response by 0. However, our detailed finite-sample analysis\nreveals, surprisingly, that this behavior is not present when the regression\nresponse and the features are {\\it jointly} low-dimensional, following a widely\nused factor regression model. Within this popular model class, and when the\neffective rank of $\\Sigma$ is smaller than $n$, while still allowing for $p \\gg\nn$, both the bias and the variance terms of the excess risk can be controlled,\nand the risk of the minimum-norm interpolating predictor approaches optimal\nbenchmarks. Moreover, through a detailed analysis of the bias term, we exhibit\nmodel classes under which our upper bound on the excess risk approaches zero,\nwhile the corresponding upper bound in the recent work arXiv:1906.11300\ndiverges. Furthermore, we show that the minimum-norm interpolating predictor\nanalyzed under the factor regression model, despite being model-agnostic and\ndevoid of tuning parameters, can have similar risk to predictors based on\nprincipal components regression and ridge regression, and can improve over\nLASSO based predictors, in the high-dimensional regime.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 22:08:36 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 16:52:54 GMT"}, {"version": "v3", "created": "Sat, 20 Mar 2021 22:48:52 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Bunea", "Florentina", ""], ["Strimas-Mackey", "Seth", ""], ["Wegkamp", "Marten", ""]]}, {"id": "2002.02564", "submitter": "F. Richard Guo", "authors": "F. Richard Guo, James McQueen and Thomas S. Richardson", "title": "Empirical Bayes for Large-scale Randomized Experiments: a Spectral\n  Approach", "comments": "Corrections and notational changes to Sec 4.4; added acknowledgments;\n  some contents of Sec 2.3 are moved to the Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale randomized experiments, sometimes called A/B tests, are\nincreasingly prevalent in many industries. Though such experiments are often\nanalyzed via frequentist $t$-tests, arguably such analyses are deficient:\n$p$-values are hard to interpret and not easily incorporated into\ndecision-making. As an alternative, we propose an empirical Bayes approach,\nwhich assumes that the treatment effects are realized from a \"true prior\". This\nrequires inferring the prior from previous experiments. Following Robbins, we\nestimate a family of marginal densities of empirical effects, indexed by the\nnoise scale. We show that this family is characterized by the heat equation. We\ndevelop a spectral maximum likelihood estimate based on a Fourier series\nrepresentation, which can be efficiently computed via convex optimization. In\norder to select hyperparameters and compare models, we describe two model\nselection criteria. We demonstrate our method on simulated and real data, and\ncompare posterior inference to that under a Gaussian mixture model of the\nprior.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 00:25:07 GMT"}, {"version": "v2", "created": "Wed, 12 Feb 2020 23:25:39 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 22:59:43 GMT"}], "update_date": "2020-03-27", "authors_parsed": [["Guo", "F. Richard", ""], ["McQueen", "James", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "2002.02584", "submitter": "Shuhang Chen", "authors": "Shuhang Chen, Adithya M. Devraj, Ana Bu\\v{s}i\\'c, Sean Meyn", "title": "Explicit Mean-Square Error Bounds for Monte-Carlo and Linear Stochastic\n  Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG cs.SY eess.SY math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns error bounds for recursive equations subject to Markovian\ndisturbances. Motivating examples abound within the fields of Markov chain\nMonte Carlo (MCMC) and Reinforcement Learning (RL), and many of these\nalgorithms can be interpreted as special cases of stochastic approximation\n(SA). It is argued that it is not possible in general to obtain a Hoeffding\nbound on the error sequence, even when the underlying Markov chain is\nreversible and geometrically ergodic, such as the M/M/1 queue. This is\nmotivation for the focus on mean square error bounds for parameter estimates.\nIt is shown that mean square error achieves the optimal rate of $O(1/n)$,\nsubject to conditions on the step-size sequence. Moreover, the exact constants\nin the rate are obtained, which is of great value in algorithm design.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 01:52:21 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Chen", "Shuhang", ""], ["Devraj", "Adithya M.", ""], ["Bu\u0161i\u0107", "Ana", ""], ["Meyn", "Sean", ""]]}, {"id": "2002.02592", "submitter": "Nicholas James", "authors": "Nick James, Max Menzies", "title": "Equivalence relations and $L^p$ distances between time series", "comments": "Equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a general framework for defining equivalence and measuring\ndistances between time series, and a first concrete method for doing so. We\nprove the existence of equivalence relations on the space of time series, such\nthat the quotient spaces can be equipped with a metrizable topology. We\nillustrate algorithmically how to calculate such distances among a collection\nof time series, and perform clustering analysis based on these distances. We\napply these insights to analyse the recent bushfires in NSW, Australia. There,\nwe introduce a new method to analyse time series in a cross-contextual setting.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 02:32:33 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["James", "Nick", ""], ["Menzies", "Max", ""]]}, {"id": "2002.02594", "submitter": "Estate Khmaladze", "authors": "Estate V. Khmaladze", "title": "Distribution free testing for linear regression. Extension to general\n  parametric regression", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a distribution free approach for testing parametric hypotheses based\non unitary transformations has been suggested in \\cite{Khm13, Khm16, Khm17} and\nfurther studied in \\cite{Ngu17} and \\cite{Rob19}. In this note we show that the\ntransformation takes extremely simple form in distribution free testing of\nlinear regression. Then we extend it to general parametric regression with\nvector-valued covariates.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 02:39:37 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Khmaladze", "Estate V.", ""]]}, {"id": "2002.02610", "submitter": "Marianna Pensky", "authors": "Majid Noroozi and Marianna Pensky", "title": "The Hierarchy of Block Models", "comments": "33 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist various types of network block models such as the Stochastic\nBlock Model (SBM), the Degree Corrected Block Model (DCBM), and the Popularity\nAdjusted Block Model (PABM). While this leads to a variety of choices, the\nblock models do not have a nested structure. In addition, there is a\nsubstantial jump in the number of parameters from the DCBM to the PABM. The\nobjective of this paper is formulation of a hierarchy of block model which does\nnot rely on arbitrary identifiability conditions. We propose a Nested Block\nModel (NBM) that treats the SBM, the DCBM and the PABM as its particular cases\nwith specific parameter values, and, in addition, allows a multitude of\nversions that are more complicated than DCBM but have fewer unknown parameters\nthan the PABM. The latter allows one to carry out clustering and estimation\nwithout preliminary testing, to see which block model is really true.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 03:47:15 GMT"}, {"version": "v2", "created": "Sat, 13 Mar 2021 22:11:59 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Noroozi", "Majid", ""], ["Pensky", "Marianna", ""]]}, {"id": "2002.02684", "submitter": "Bruno Torresani", "authors": "Adrien Meynard (I2M), Bruno Torr\\'esani (I2M)", "title": "Time-scale synthesis for locally stationary signals", "comments": null, "journal-ref": "ICASSP 2020, May 2020, Barcelona, Spain", "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a timescale synthesis-based probabilistic approach for the\nmodeling of locally stationary signals. Inspired by our previous work, the\nmodel involves zero-mean, complex Gaussian wavelet coefficients, whose\ndistribution varies as a function of time by time dependent translations on the\nscale axis. In a maximum a posteriori approach, we propose an estimator for the\nmodel parameters, namely the time-varying scale translation and an underlying\npower spectrum. The proposed approach is illustrated on a denoising example. It\nis also shown that the model can handle locally stationary signals with fast\nfrequency variations, and provide in this case very sharp timescale\nrepresentations more concentrated than synchrosqueezed or reassigned wavelet\ntransform.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 09:34:48 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Meynard", "Adrien", "", "I2M"], ["Torr\u00e9sani", "Bruno", "", "I2M"]]}, {"id": "2002.02717", "submitter": "Dmitry V. Dylov", "authors": "Nikolay Shvetsov and Nazar Buzun and Dmitry V. Dylov", "title": "Unsupervised non-parametric change point detection in quasi-periodic\n  signals", "comments": "8 pages, 7 figures, 1 table", "journal-ref": null, "doi": "10.1145/3400903.3400917", "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new unsupervised and non-parametric method to detect change\npoints in intricate quasi-periodic signals. The detection relies on optimal\ntransport theory combined with topological analysis and the bootstrap\nprocedure. The algorithm is designed to detect changes in virtually any\nharmonic or a partially harmonic signal and is verified on three different\nsources of physiological data streams. We successfully find abnormal or\nirregular cardiac cycles in the waveforms for the six of the most frequent\ntypes of clinical arrhythmias using a single algorithm. The validation and the\nefficiency of the method are shown both on synthetic and on real time series.\nOur unsupervised approach reaches the level of performance of the supervised\nstate-of-the-art techniques. We provide conceptual justification for the\nefficiency of the method and prove the convergence of the bootstrap procedure\ntheoretically.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 11:24:50 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shvetsov", "Nikolay", ""], ["Buzun", "Nazar", ""], ["Dylov", "Dmitry V.", ""]]}, {"id": "2002.02851", "submitter": "Georg Pichler", "authors": "Georg Pichler and Pablo Piantanida and G\\\"unther Koliander", "title": "On the Estimation of Information Measures of Continuous Distributions", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of information measures of continuous distributions based on\nsamples is a fundamental problem in statistics and machine learning. In this\npaper, we analyze estimates of differential entropy in $K$-dimensional\nEuclidean space, computed from a finite number of samples, when the probability\ndensity function belongs to a predetermined convex family $\\mathcal{P}$. First,\nestimating differential entropy to any accuracy is shown to be infeasible if\nthe differential entropy of densities in $\\mathcal{P}$ is unbounded, clearly\nshowing the necessity of additional assumptions. Subsequently, we investigate\nsufficient conditions that enable confidence bounds for the estimation of\ndifferential entropy. In particular, we provide confidence bounds for simple\nhistogram based estimation of differential entropy from a fixed number of\nsamples, assuming that the probability density function is Lipschitz continuous\nwith known Lipschitz constant and known, bounded support. Our focus is on\ndifferential entropy, but we provide examples that show that similar results\nhold for mutual information and relative entropy as well.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 15:36:10 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Pichler", "Georg", ""], ["Piantanida", "Pablo", ""], ["Koliander", "G\u00fcnther", ""]]}, {"id": "2002.02892", "submitter": "Nicolas Keriven", "authors": "Nicolas Keriven, Samuel Vaiter", "title": "Sparse and Smooth: improved guarantees for Spectral Clustering in the\n  Dynamic Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyse classical variants of the Spectral Clustering (SC)\nalgorithm in the Dynamic Stochastic Block Model (DSBM). Existing results show\nthat, in the relatively sparse case where the expected degree grows\nlogarithmically with the number of nodes, guarantees in the static case can be\nextended to the dynamic case and yield improved error bounds when the DSBM is\nsufficiently smooth in time, that is, the communities do not change too much\nbetween two time steps. We improve over these results by drawing a new link\nbetween the sparsity and the smoothness of the DSBM: the more regular the DSBM\nis, the more sparse it can be, while still guaranteeing consistent recovery. In\nparticular, a mild condition on the smoothness allows to treat the sparse case\nwith bounded degree. We also extend these guarantees to the normalized\nLaplacian, and as a by-product of our analysis, we obtain to our knowledge the\nbest spectral concentration bound available for the normalized Laplacian of\nmatrices with independent Bernoulli entries.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 16:49:25 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 15:46:24 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Keriven", "Nicolas", ""], ["Vaiter", "Samuel", ""]]}, {"id": "2002.02901", "submitter": "Azadeh Khaleghi", "authors": "Steffen Gr\\\"unew\\\"alder and Azadeh Khaleghi", "title": "Oblivious Data for Fairness with Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of algorithmic fairness in the case where\nsensitive and non-sensitive features are available and one aims to generate\nnew, `oblivious', features that closely approximate the non-sensitive features,\nand are only minimally dependent on the sensitive ones. We study this question\nin the context of kernel methods. We analyze a relaxed version of the Maximum\nMean Discrepancy criterion which does not guarantee full independence but makes\nthe optimization problem tractable. We derive a closed-form solution for this\nrelaxed optimization problem and complement the result with a study of the\ndependencies between the newly generated features and the sensitive ones. Our\nkey ingredient for generating such oblivious features is a Hilbert-space-valued\nconditional expectation, which needs to be estimated from data. We propose a\nplug-in approach and demonstrate how the estimation errors can be controlled.\nWhile our techniques help reduce the bias, we would like to point out that no\npost-processing of any dataset could possibly serve as an alternative to\nwell-designed experiments.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 16:59:24 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2020 19:44:18 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Gr\u00fcnew\u00e4lder", "Steffen", ""], ["Khaleghi", "Azadeh", ""]]}, {"id": "2002.02933", "submitter": "Francesco Morandin", "authors": "Silvia Giulia Galfre' and Francesco Morandin", "title": "A mathematical framework for raw counts of single-cell RNA-seq data\n  analysis", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single-cell RNA-seq data are challenging because of the sparseness of the\nread counts, the tiny expression of many relevant genes, and the variability in\nthe efficiency of RNA extraction for different cells. We consider a simple\nprobabilistic model for read counts, based on a negative binomial distribution\nfor each gene, modified by a cell-dependent coefficient interpreted as an\nextraction efficiency. We provide two alternative fast methods to estimate the\nmodel parameters, together with the probability that a cell results in zero\nread counts for a gene. This allows to measure genes co-expression and\ndifferential expression in a novel way.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 18:10:50 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Galfre'", "Silvia Giulia", ""], ["Morandin", "Francesco", ""]]}, {"id": "2002.03004", "submitter": "Morris Yau", "authors": "Prasad Raghavendra, Morris Yau", "title": "List Decodable Subspace Recovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning from data in the presence of outliers is a fundamental problem in\nstatistics. In this work, we study robust statistics in the presence of\noverwhelming outliers for the fundamental problem of subspace recovery. Given a\ndataset where an $\\alpha$ fraction (less than half) of the data is distributed\nuniformly in an unknown $k$ dimensional subspace in $d$ dimensions, and with no\nadditional assumptions on the remaining data, the goal is to recover a succinct\nlist of $O(\\frac{1}{\\alpha})$ subspaces one of which is nontrivially correlated\nwith the planted subspace. We provide the first polynomial time algorithm for\nthe 'list decodable subspace recovery' problem, and subsume it under a more\ngeneral framework of list decoding over distributions that are \"certifiably\nresilient\" capturing state of the art results for list decodable mean\nestimation and regression.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 20:18:33 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Raghavendra", "Prasad", ""], ["Yau", "Morris", ""]]}, {"id": "2002.03048", "submitter": "Marc Jaffrey Ph.D.", "authors": "Marc Jaffrey, Michael Dushkoff", "title": "Moments of the Distribution of Pearsons Correlation over Permutations of\n  Data", "comments": "5 Pages, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Presented is an inductive formula for computing the exact moments of the\ndistribution of Pearson's correlation over permutation of a data sample. These\nexact formulas for the moments open the door to the possibility of more precise\nand computationally efficient methods of evaluating the p-value for a\nhypothesis test of Pearson's correlation.\n", "versions": [{"version": "v1", "created": "Fri, 7 Feb 2020 23:54:22 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Jaffrey", "Marc", ""], ["Dushkoff", "Michael", ""]]}, {"id": "2002.03134", "submitter": "Fabio Gobbi", "authors": "Fabio Gobbi and Sabrina Mulinacci", "title": "State-Dependent Autoregressive Models: Properties, Estimation and\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies some temporal dependence properties and addresses the\nissue of parametric estimation for a class of state-dependent autoregressive\nmodels for nonlinear time series in which we assume a stochastic autoregressive\ncoefficient depending on the first lagged value of the process itself. We call\nsuch a model state-dependent first-order autoregressive process, (SDAR). We\nintroduce some assumptions under which this class of models is strictly\nstationary and uniformly ergodic and we establish consistency and asymptotic\nnormality of the quasi-maximum likelihood estimator of the parameters. In order\nto capture the potentiality of the model, we present an empirical application\nto nonlinear time series provided by the weekly realized volatility extracted\nfrom returns of some European financial indices. The comparison of forecasting\naccuracy is made considering an alternative approach provided by a two-regime\nSETAR model\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 10:30:07 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Gobbi", "Fabio", ""], ["Mulinacci", "Sabrina", ""]]}, {"id": "2002.03194", "submitter": "Christophe Chesneau", "authors": "Christophe Chesneau, Hassan S. Bakouch, Muhammad Nauman Khan", "title": "A weighted transmuted exponential distribution with environmental\n  applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new three-parameter distribution based on the\ncombination of re-parametrization of the so-called EGNB2 and transmuted\nexponential distributions. This combination aims to modify the transmuted\nexponential distribution via the incorporation of an additional parameter,\nmainly adding a high degree of flexibility on the mode and impacting the\nskewness and kurtosis of the tail. We explore some mathematical properties of\nthis distribution including the hazard rate function, moments, the moment\ngenerating function, the quantile function, various entropy measures and\n(reversed) residual life functions. A statistical study investigates estimation\nof the parameters using the method of maximum likelihood. The distribution\nalong with other existing distributions are fitted to two environmental data\nsets and its superior performance is assessed by using some goodness-of-fit\ntests. As a result, some environmental measures associated with these data are\nobtained such as the return level and mean deviation about this level.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 16:26:07 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Chesneau", "Christophe", ""], ["Bakouch", "Hassan S.", ""], ["Khan", "Muhammad Nauman", ""]]}, {"id": "2002.03237", "submitter": "Jos\\'e G. G\\'omez Garc\\'ia", "authors": "Jos\\'e G. G\\'omez Garc\\'ia, Jalal Fadili, Christophe Chesneau", "title": "Learning CHARME models with neural networks", "comments": "31 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a model called CHARME (Conditional Heteroscedastic\nAutoregressive Mixture of Experts), a class of generalized mixture of nonlinear\nnonparametric AR-ARCH time series. Under certain Lipschitz-type conditions on\nthe autoregressive and volatility functions, we prove that this model is\nstationary, ergodic and $\\tau$-weakly dependent. These conditions are much\nweaker than those presented in the literature that treats this model. Moreover,\nthis result forms the theoretical basis for deriving an asymptotic theory of\nthe underlying (non)parametric estimation, which we present for this model. As\nan application, from the universal approximation property of neural networks\n(NN), we develop a learning theory for the NN-based autoregressive functions of\nthe model, where the strong consistency and asymptotic normality of the\nconsidered estimator of the NN weights and biases are guaranteed under weak\nconditions.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 21:51:02 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2020 17:54:08 GMT"}], "update_date": "2020-11-18", "authors_parsed": [["Garc\u00eda", "Jos\u00e9 G. G\u00f3mez", ""], ["Fadili", "Jalal", ""], ["Chesneau", "Christophe", ""]]}, {"id": "2002.03307", "submitter": "Rodrigo Labouriau", "authors": "Rodrigo Labouriau", "title": "On the Bias of the Score Function of Finite Mixture Models", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize the unbiasedness of the score function, viewed as an\ninference function, for a class of finite mixture models. The models studied\nrepresent the situation where there is a stratification of the observations in\na finite number of groups. We show that if the observations belonging to the\nsame group follow the same distribution and the K distributions associated with\neach group are distinct elements of a sufficiently regular parametric family of\nprobability measures, then the score function for estimating the parameters\nidentifying the distribution of each group is unbiased. However, if one\nintroduces a mixture in the scenario described above, so that for some\nobservations it is only known that they belong to some of the groups with a\ngiven probability (not all in { 0, 1}), then the score function becomes biased.\nWe argue then that under further mild regularity conditions, the maximum\nlikelihood estimate is not consistent.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 07:10:31 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Labouriau", "Rodrigo", ""]]}, {"id": "2002.03414", "submitter": "Mohamed Laidi M", "authors": "Mohamed Laidi, Abdelaziz Rassoul, Hamid Ould Rouis", "title": "Improved Estimator of the Conditional Tail Expectation in the case of\n  heavy-tailed losses", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": "10.19139/soic-2310-5070-665", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the extreme-value methodology, to propose an\nimproved estimator of the conditional tail expectation ($CTE$) for a loss\ndistribution with a finite mean but infinite variance. The present work\nintroduces a new estimator of the $CTE$ based on the bias-reduced estimators of\nhigh quantile for heavy-tailed distributions. The asymptotic normality of the\nproposed estimator is established and checked, in a simulation study. Moreover,\nwe compare, in terms of bias and mean squared error, our estimator with the\nknown old estimator.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 18:13:24 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Laidi", "Mohamed", ""], ["Rassoul", "Abdelaziz", ""], ["Rouis", "Hamid Ould", ""]]}, {"id": "2002.03636", "submitter": "Joseph de Vilmarest", "authors": "Joseph de Vilmarest (LPSM (UMR\\_8001)), Olivier Wintenberger (LPSM\n  (UMR\\_8001))", "title": "Stochastic Online Optimization using Kalman Recursion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Extended Kalman Filter in constant dynamics, offering a bayesian\nperspective of stochastic optimization. We obtain high probability bounds on\nthe cumulative excess risk in an unconstrained setting. In order to avoid any\nprojection step we propose a two-phase analysis. First, for linear and logistic\nregressions, we prove that the algorithm enters a local phase where the\nestimate stays in a small region around the optimum. We provide explicit bounds\nwith high probability on this convergence time. Second, for generalized linear\nregressions, we provide a martingale analysis of the excess risk in the local\nphase, improving existing ones in bounded stochastic optimization. The EKF\nappears as a parameter-free online algorithm with O(d^2) cost per iteration\nthat optimally solves some unconstrained optimization problems.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 10:33:35 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 08:15:29 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["de Vilmarest", "Joseph", "", "LPSM"], ["Wintenberger", "Olivier", "", "LPSM"]]}, {"id": "2002.03658", "submitter": "Alessandra Salvan", "authors": "Luigi Pace and Alessandra Salvan", "title": "Likelihood, Replicability and Robbins' Confidence Sequences", "comments": null, "journal-ref": null, "doi": "10.1111/insr.12355", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely claimed replicability crisis in science may lead to revised\nstandards of significance. The customary frequentist confidence intervals,\ncalibrated through hypothetical repetitions of the experiment that is supposed\nto have produced the data at hand, rely on a feeble concept of replicability.\nIn particular, contradictory conclusions may be reached when a substantial\nenlargement of the study is undertaken. To redefine statistical confidence in\nsuch a way that inferential conclusions are non-contradictory, with large\nenough probability, under enlargements of the sample, we give a new reading of\na proposal dating back to the 60's, namely Robbins' confidence sequences.\nDirectly bounding the probability of reaching, in the future, conclusions that\ncontradict the current ones, Robbins' confidence sequences ensure a clear-cut\nform of replicability when inference is performed on accumulating data. Their\nmain frequentist property is easy to understand and to prove. We show that\nRobbins' confidence sequences may be justified under various views of\ninference: they are likelihood-based, can incorporate prior information, and\nobey the strong likelihood principle. They are easy to compute, even when\ninference is on a parameter of interest, especially using a closed-form\napproximation from normal asymptotic theory.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 11:10:01 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Pace", "Luigi", ""], ["Salvan", "Alessandra", ""]]}, {"id": "2002.03660", "submitter": "Agnieszka Goroncy", "authors": "Agnieszka Goroncy", "title": "On upper bounds on expectations of gOSs based on DFR and DFRA\n  distributions", "comments": "The final version of this paper will be published in Statistics", "journal-ref": null, "doi": "10.1080/02331888.2020.1725010", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of establishing the optimal upper bounds on\ngeneralized order statistics which are based on the underlying cdf belonging to\nthe family of distributions with decreasing failure rate and decreasing failure\nrate on the average. This issue has been previously considered by Bieniek\n[Projection bounds on expectations of generalized order statistics from DFR and\nDFRA families, Statistics, 2006; 40: 339--351], who established upper\nnonnegative mean-variance bounds with use of the projections of the\ncompositions of density functions of the uniform generalized order statistic\nand the exponential distribution function onto the properly chosen convex\ncones. In this paper we obtain possibly negative upper bounds, by improving the\nzero bounds obtained by Bieniek for some particular cases of gOSs. We express\nthe bounds in the scale units generated by the central absolute moments of\narbitrary orders. We also describe the attainability conditions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 11:19:26 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Goroncy", "Agnieszka", ""]]}, {"id": "2002.03674", "submitter": "Scott Ward", "authors": "Scott Ward, Edward A. K. Cohen, Niall Adams", "title": "Testing for complete spatial randomness on three dimensional bounded\n  convex shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is currently a gap in theory for point patterns that lie on the surface\nof objects, with researchers focusing on patterns that lie in a Euclidean\nspace, typically planar and spatial data. Methodology for planar and spatial\ndata thus relies on Euclidean geometry and is therefore inappropriate for\nanalysis of point patterns observed in non-Euclidean spaces. Recently, there\nhas been extensions to the analysis of point patterns on a sphere, however,\nmany other shapes are left unexplored. This is in part due to the challenge of\ndefining the notion of stationarity for a point process existing on such a\nspace due to the lack of rotational and translational isometries. Here, we\nconstruct functional summary statistics for Poisson processes defined on convex\nshapes in three dimensions. Using the Mapping Theorem, a Poisson process can be\ntransformed from any convex shape to a Poisson process on the unit sphere which\nhas rotational symmetries that allow for functional summary statistics to be\nconstructed. We present the first and second order properties of such summary\nstatistics and demonstrate how they can be used to test whether an observed\npattern exhibits complete spatial randomness or spatial preference on the\noriginal convex space. A study of the Type I and II errors of our test\nstatistics are explored through simulations on ellipsoids of varying\ndimensions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 11:57:38 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Ward", "Scott", ""], ["Cohen", "Edward A. K.", ""], ["Adams", "Niall", ""]]}, {"id": "2002.03959", "submitter": "Gecia Bravo-Hermsdorff", "authors": "Lee M. Gunderson and Gecia Bravo-Hermsdorff", "title": "Introducing Graph Cumulants: What is the Variance of Your Social\n  Network?", "comments": "Both authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM cs.SI physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an increasingly interconnected world, understanding and summarizing the\nstructure of these networks becomes increasingly relevant. However, this task\nis nontrivial; proposed summary statistics are as diverse as the networks they\ndescribe, and a standardized hierarchy has not yet been established. In\ncontrast, vector-valued random variables admit such a description in terms of\ntheir cumulants (e.g., mean, (co)variance, skew, kurtosis). Here, we introduce\nthe natural analogue of cumulants for networks, building a hierarchical\ndescription based on correlations between an increasing number of connections,\nseamlessly incorporating additional information, such as directed edges, node\nattributes, and edge weights. These graph cumulants provide a principled and\nunifying framework for quantifying the propensity of a network to display any\nsubstructure of interest (such as cliques to measure clustering). Moreover,\nthey give rise to a natural hierarchical family of maximum entropy models for\nnetworks (i.e., ERGMs) that do not suffer from the \"degeneracy problem\", a\ncommon practical pitfall of other ERGMs.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 17:17:14 GMT"}, {"version": "v2", "created": "Mon, 17 Feb 2020 03:20:32 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 17:22:13 GMT"}, {"version": "v4", "created": "Tue, 14 Apr 2020 14:58:50 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Gunderson", "Lee M.", ""], ["Bravo-Hermsdorff", "Gecia", ""]]}, {"id": "2002.04081", "submitter": "Andrea Arf\\'e", "authors": "Andrea Arf\\`e, Pietro Muliere", "title": "A general Bayesian bootstrap for censored data based on the beta-Stacy\n  process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel procedure to perform Bayesian non-parametric inference\nwith right-censored data, the \\emph{beta-Stacy bootstrap}. This approximates\nthe posterior law of summaries of the survival distribution (e.g. the mean\nsurvival time), which is often difficult in the non-parametric case. More\nprecisely, our procedure approximates the joint posterior law of functionals of\nthe beta-Stacy process, a non-parametric process prior widely used in survival\nanalysis. It also represents the missing link that unifies other common\nBayesian bootstraps for complete or censored data based on non-parametric\npriors. It is defined by an exact sampling algorithm that does not require\ntuning of Markov Chain Monte Carlo steps. We illustrate the beta-Stacy\nbootstrap by analyzing survival data from a real clinical trial.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 20:41:35 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Arf\u00e8", "Andrea", ""], ["Muliere", "Pietro", ""]]}, {"id": "2002.04115", "submitter": "Runmin Wang", "authors": "Runmin Wang and Xiaofeng Shao", "title": "Dating the Break in High-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with estimation and inference for the location of a\nchange point in the mean of independent high-dimensional data. Our change point\nlocation estimator maximizes a new U-statistic based objective function, and\nits convergence rate and asymptotic distribution after suitable centering and\nnormalization are obtained under mild assumptions. Our estimator turns out to\nhave better efficiency as compared to the least squares based counterpart in\nthe literature. Based on the asymptotic theory, we construct a confidence\ninterval by plugging in consistent estimates of several quantities in the\nnormalization. We also provide a bootstrap-based confidence interval and state\nits asymptotic validity under suitable conditions. Through simulation studies,\nwe demonstrate favorable finite sample performance of the new change point\nlocation estimator as compared to its least squares based counterpart, and our\nbootstrap-based confidence intervals, as compared to several existing\ncompetitors. The asymptotic theory based on high-dimensional U-statistic is\nsubstantially different from those developed in the literature and is of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 22:19:29 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Wang", "Runmin", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "2002.04254", "submitter": "Joseph Lam-Weil", "authors": "Joseph Lam-Weil, B\\'eatrice Laurent, Jean-Michel Loubes", "title": "Minimax optimal goodness-of-fit testing for densities and multinomials\n  under a local differential privacy constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding anonymization mechanisms to protect personal data is at the heart of\nrecent machine learning research. Here, we consider the consequences of local\ndifferential privacy constraints on goodness-of-fit testing, i.e. the\nstatistical problem assessing whether sample points are generated from a fixed\ndensity $f_0$, or not. The observations are kept hidden and replaced by a\nstochastic transformation satisfying the local differential privacy constraint.\nIn this setting, we propose a testing procedure which is based on an estimation\nof the quadratic distance between the density $f$ of the unobserved samples and\n$f_0$. We establish an upper bound on the separation distance associated with\nthis test, and a matching lower bound on the minimax separation rates of\ntesting under non-interactive privacy in the case that $f_0$ is uniform, in\ndiscrete and continuous settings. To the best of our knowledge, we provide the\nfirst minimax optimal test and associated private transformation under a local\ndifferential privacy constraint over Besov balls in the continuous setting,\nquantifying the price to pay for data privacy. We also present a test that is\nadaptive to the smoothness parameter of the unknown density and remains minimax\noptimal up to a logarithmic factor. Finally, we note that our results can be\ntranslated to the discrete case, where the treatment of probability vectors is\nshown to be equivalent to that of piecewise constant densities in our setting.\nThat is why we work with a unified setting for both the continuous and the\ndiscrete cases.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 08:41:05 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 08:53:58 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 07:12:21 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Lam-Weil", "Joseph", ""], ["Laurent", "B\u00e9atrice", ""], ["Loubes", "Jean-Michel", ""]]}, {"id": "2002.04271", "submitter": "Pradip Kundu", "authors": "Arindam Panja, Pradip Kundu, Biswabrata Pradhan", "title": "Stochastic comparisons of lifetimes of series and parallel systems with\n  dependent and heterogeneous components", "comments": "20 pages, 4 figures", "journal-ref": "Operations Research Letters, 2020", "doi": "10.1016/j.orl.2020.12.009", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers stochastic comparisons of lifetimes of series and\nparallel systems with dependent and heterogeneous components having lifetimes\nfollowing the proportional odds (PO) model. The joint distribution of component\nlifetimes is modeled by Archimedean survival copula. We discuss some potential\napplications of our findings in system reliability and actuarial science.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 09:21:12 GMT"}, {"version": "v2", "created": "Fri, 1 Jan 2021 11:40:25 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Panja", "Arindam", ""], ["Kundu", "Pradip", ""], ["Pradhan", "Biswabrata", ""]]}, {"id": "2002.04363", "submitter": "Kelvin Shuangjian Zhang", "authors": "Kelvin Shuangjian Zhang, Gabriel Peyr\\'e, Jalal Fadili, Marcelo\n  Pereyra", "title": "Wasserstein Control of Mirror Langevin Monte Carlo", "comments": "22 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discretized Langevin diffusions are efficient Monte Carlo methods for\nsampling from high dimensional target densities that are log-Lipschitz-smooth\nand (strongly) log-concave. In particular, the Euclidean Langevin Monte Carlo\nsampling algorithm has received much attention lately, leading to a detailed\nunderstanding of its non-asymptotic convergence properties and of the role that\nsmoothness and log-concavity play in the convergence rate. Distributions that\ndo not possess these regularity properties can be addressed by considering a\nRiemannian Langevin diffusion with a metric capturing the local geometry of the\nlog-density. However, the Monte Carlo algorithms derived from discretizations\nof such Riemannian Langevin diffusions are notoriously difficult to analyze. In\nthis paper, we consider Langevin diffusions on a Hessian-type manifold and\nstudy a discretization that is closely related to the mirror-descent scheme. We\nestablish for the first time a non-asymptotic upper-bound on the sampling error\nof the resulting Hessian Riemannian Langevin Monte Carlo algorithm. This bound\nis measured according to a Wasserstein distance induced by a Riemannian metric\nground cost capturing the Hessian structure and closely related to a\nself-concordance-like condition. The upper-bound implies, for instance, that\nthe iterates contract toward a Wasserstein ball around the target density whose\nradius is made explicit. Our theory recovers existing Euclidean results and can\ncope with a wide variety of Hessian metrics related to highly non-flat\ngeometries.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 13:16:31 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Zhang", "Kelvin Shuangjian", ""], ["Peyr\u00e9", "Gabriel", ""], ["Fadili", "Jalal", ""], ["Pereyra", "Marcelo", ""]]}, {"id": "2002.04383", "submitter": "Mikhail Moklyachuk", "authors": "Iryna Golichenko and Mikhail Moklyachuk", "title": "Interpolation problem for periodically correlated stochastic sequences\n  with missing observations", "comments": null, "journal-ref": null, "doi": "10.19139/soic-2310-5070-458", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of mean square optimal estimation of linear functionals which\ndepend on the unobserved values of a periodically correlated stochastic\nsequence is considered. The estimates are based on observations of the sequence\nwith a noise. Formulas for calculation the mean square errors and the spectral\ncharacteristics of the optimal estimates of functionals are derived in the case\nof spectral certainty, where the spectral densities of the sequences are\nexactly known. Formulas that determine the least favorable spectral densities\nand the minimax spectral characteristics are proposed in the case of spectral\nuncertainty, where the spectral densities of the sequences are not exactly\nknown while some classes of admissible spectral densities are specified.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 14:49:18 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Golichenko", "Iryna", ""], ["Moklyachuk", "Mikhail", ""]]}, {"id": "2002.04457", "submitter": "Dong Xia", "authors": "Bing-Yi Jing and Ting Li and Zhongyuan Lyu and Dong Xia", "title": "Community Detection on Mixture Multi-layer Networks via Regularized\n  Tensor Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.IT cs.LG math.IT math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of community detection in multi-layer networks, where\npairs of nodes can be related in multiple modalities. We introduce a general\nframework, i.e., mixture multi-layer stochastic block model (MMSBM), which\nincludes many earlier models as special cases. We propose a tensor-based\nalgorithm (TWIST) to reveal both global/local memberships of nodes, and\nmemberships of layers. We show that the TWIST procedure can accurately detect\nthe communities with small misclassification error as the number of nodes\nand/or the number of layers increases. Numerical studies confirm our\ntheoretical findings. To our best knowledge, this is the first systematic study\non the mixture multi-layer networks using tensor decomposition. The method is\napplied to two real datasets: worldwide trading networks and malaria parasite\ngenes networks, yielding new and interesting findings.\n", "versions": [{"version": "v1", "created": "Mon, 10 Feb 2020 06:19:50 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Jing", "Bing-Yi", ""], ["Li", "Ting", ""], ["Lyu", "Zhongyuan", ""], ["Xia", "Dong", ""]]}, {"id": "2002.04465", "submitter": "Agnes Lagnoux", "authors": "Fabrice Gamboa (IMT), Thierry Klein (ENAC), Agn\\`es Lagnoux (IMT),\n  Leonardo Moreno", "title": "Sensitivity analysis in general metric spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce new indices adapted to outputs valued in general\nmetric spaces. This new class of indices encompasses the classical ones; in\nparticular, the so-called Sobol indices and the Cram{\\'e}r-von-Mises indices.\nFurthermore, we provide asymptotically Gaussian estimators of these indices\nbased on U-statistics. Surprisingly, we prove the asymp-totic normality\nstraightforwardly. Finally, we illustrate this new procedure on a toy model and\non two real-data examples.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 15:16:11 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 13:38:41 GMT"}, {"version": "v3", "created": "Wed, 20 Jan 2021 10:44:38 GMT"}], "update_date": "2021-01-21", "authors_parsed": [["Gamboa", "Fabrice", "", "IMT"], ["Klein", "Thierry", "", "ENAC"], ["Lagnoux", "Agn\u00e8s", "", "IMT"], ["Moreno", "Leonardo", ""]]}, {"id": "2002.04732", "submitter": "Kumar Vijay Mishra", "authors": "Kumar Vijay Mishra and M. Ashok Kumar", "title": "Generalized Bayesian Cram\\'{e}r-Rao Inequality via Information Geometry\n  of Relative $\\alpha$-Entropy", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT eess.SP math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relative $\\alpha$-entropy is the R\\'enyi analog of relative entropy and\narises prominently in information-theoretic problems. Recent information\ngeometric investigations on this quantity have enabled the generalization of\nthe Cram\\'{e}r-Rao inequality, which provides a lower bound for the variance of\nan estimator of an escort of the underlying parametric probability\ndistribution. However, this framework remains unexamined in the Bayesian\nframework. In this paper, we propose a general Riemannian metric based on\nrelative $\\alpha$-entropy to obtain a generalized Bayesian Cram\\'{e}r-Rao\ninequality. This establishes a lower bound for the variance of an unbiased\nestimator for the $\\alpha$-escort distribution starting from an unbiased\nestimator for the underlying distribution. We show that in the limiting case\nwhen the entropy order approaches unity, this framework reduces to the\nconventional Bayesian Cram\\'{e}r-Rao inequality. Further, in the absence of\npriors, the same framework yields the deterministic Cram\\'{e}r-Rao inequality.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 23:38:01 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Mishra", "Kumar Vijay", ""], ["Kumar", "M. Ashok", ""]]}, {"id": "2002.04898", "submitter": "Ioannis Kalogridis Mr", "authors": "Ioannis Kalogridis", "title": "Asymptotics for M-type smoothing splines with non-smooth objective\n  functions", "comments": null, "journal-ref": "Test 2021", "doi": "10.1007/s11749-021-00782-y", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  M-type smoothing splines are a broad class of spline estimators that include\nthe popular least-squares smoothing spline but also spline estimators that are\nless susceptible to outlying observations and model-misspecification. However,\navailable asymptotic theory only covers smoothing spline estimators based on\nsmooth objective functions and consequently leaves out frequently used\nresistant estimators such as quantile and Huber-type smoothing splines. We\nprovide a general treatment in this paper and, assuming only the convexity of\nthe objective function, show that the least-squares (super-)convergence rates\ncan be extended to M-type estimators whose asymptotic properties have not been\nhitherto described. We further show that auxiliary scale estimates may be\nhandled under significantly weaker assumptions than those found in the\nliterature and we establish optimal rates of convergence for the derivatives,\nwhich have not been obtained outside the least-squares framework. A simulation\nstudy and a real-data example illustrate the competitive performance of\nnon-smooth M-type splines in relation to the least-squares spline on regular\ndata and their superior performance on data that contain anomalies.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 10:22:32 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2020 10:54:05 GMT"}, {"version": "v3", "created": "Mon, 3 Aug 2020 08:26:03 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Kalogridis", "Ioannis", ""]]}, {"id": "2002.04926", "submitter": "Dylan Foster", "authors": "Dylan J. Foster and Alexander Rakhlin", "title": "Beyond UCB: Optimal and Efficient Contextual Bandits with Regression\n  Oracles", "comments": "ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in contextual bandits is to develop flexible,\ngeneral-purpose algorithms with computational requirements no worse than\nclassical supervised learning tasks such as classification and regression.\nAlgorithms based on regression have shown promising empirical success, but\ntheoretical guarantees have remained elusive except in special cases. We\nprovide the first universal and optimal reduction from contextual bandits to\nonline regression. We show how to transform any oracle for online regression\nwith a given value function class into an algorithm for contextual bandits with\nthe induced policy class, with no overhead in runtime or memory requirements.\nWe characterize the minimax rates for contextual bandits with general,\npotentially nonparametric function classes, and show that our algorithm is\nminimax optimal whenever the oracle obtains the optimal rate for regression.\nCompared to previous results, our algorithm requires no distributional\nassumptions beyond realizability, and works even when contexts are chosen\nadversarially.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 11:33:46 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 10:44:25 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Foster", "Dylan J.", ""], ["Rakhlin", "Alexander", ""]]}, {"id": "2002.05105", "submitter": "Shisheng Cui", "authors": "Shisheng Cui and Chia-Jung Chang", "title": "Development of modeling and control strategies for an approximated\n  Gaussian process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian process (GP) model, which has been extensively applied as priors\nof functions, has demonstrated excellent performance. The specification of a\nlarge number of parameters affects the computational efficiency and the\nfeasibility of implementation of a control strategy. We propose a linear model\nto approximate GPs; this model expands the GP model by a series of basis\nfunctions. Several examples and simulation studies are presented to demonstrate\nthe advantages of the proposed method. A control strategy is provided with the\nproposed linear model.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 17:28:24 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Cui", "Shisheng", ""], ["Chang", "Chia-Jung", ""]]}, {"id": "2002.05153", "submitter": "Andrew Bennett", "authors": "Andrew Bennett and Nathan Kallus", "title": "Efficient Policy Learning from Surrogate-Loss Classification Reductions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work on policy learning from observational data has highlighted the\nimportance of efficient policy evaluation and has proposed reductions to\nweighted (cost-sensitive) classification. But, efficient policy evaluation need\nnot yield efficient estimation of policy parameters. We consider the estimation\nproblem given by a weighted surrogate-loss classification reduction of policy\nlearning with any score function, either direct, inverse-propensity weighted,\nor doubly robust. We show that, under a correct specification assumption, the\nweighted classification formulation need not be efficient for policy\nparameters. We draw a contrast to actual (possibly weighted) binary\nclassification, where correct specification implies a parametric model, while\nfor policy learning it only implies a semiparametric model. In light of this,\nwe instead propose an estimation approach based on generalized method of\nmoments, which is efficient for the policy parameters. We propose a particular\nmethod based on recent developments on solving moment problems using neural\nnetworks and demonstrate the efficiency and regret benefits of this method\nempirically.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 18:54:41 GMT"}], "update_date": "2020-02-13", "authors_parsed": [["Bennett", "Andrew", ""], ["Kallus", "Nathan", ""]]}, {"id": "2002.05193", "submitter": "Momin M. Malik", "authors": "Momin M. Malik", "title": "A Hierarchy of Limitations in Machine Learning", "comments": "68 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG econ.EM math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  \"All models are wrong, but some are useful\", wrote George E. P. Box (1979).\nMachine learning has focused on the usefulness of probability models for\nprediction in social systems, but is only now coming to grips with the ways in\nwhich these models are wrong---and the consequences of those shortcomings. This\npaper attempts a comprehensive, structured overview of the specific conceptual,\nprocedural, and statistical limitations of models in machine learning when\napplied to society. Machine learning modelers themselves can use the described\nhierarchy to identify possible failure points and think through how to address\nthem, and consumers of machine learning models can know what to question when\nconfronted with the decision about if, where, and how to apply machine\nlearning. The limitations go from commitments inherent in quantification\nitself, through to showing how unmodeled dependencies can lead to\ncross-validation being overly optimistic as a way of assessing model\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 19:39:29 GMT"}, {"version": "v2", "created": "Sat, 29 Feb 2020 21:04:27 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Malik", "Momin M.", ""]]}, {"id": "2002.05264", "submitter": "Abdul-Nasah Soale", "authors": "Abdul-Nasah Soale and Yuexiao Dong", "title": "On sufficient dimension reduction via principal asymmetric least squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce principal asymmetric least squares (PALS) as a\nunified framework for linear and nonlinear sufficient dimension reduction.\nClassical methods such as sliced inverse regression (Li, 1991) and principal\nsupport vector machines (Li, Artemiou and Li, 2011) may not perform well in the\npresence of heteroscedasticity, while our proposal addresses this limitation by\nsynthesizing different expectile levels. Through extensive numerical studies,\nwe demonstrate the superior performance of PALS in terms of both computation\ntime and estimation accuracy. For the asymptotic analysis of PALS for linear\nsufficient dimension reduction, we develop new tools to compute the derivative\nof an expectation of a non-Lipschitz function.\n", "versions": [{"version": "v1", "created": "Wed, 12 Feb 2020 22:23:28 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Soale", "Abdul-Nasah", ""], ["Dong", "Yuexiao", ""]]}, {"id": "2002.05297", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen", "title": "Solution manifold and Its Statistical Applications", "comments": "36 page, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CG stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A solution manifold is the collection of points in a $d$-dimensional space\nsatisfying a system of $s$ equations with $s<d$. Solution manifolds occur in\nseveral statistical problems including hypothesis testing, curved-exponential\nfamilies, constrained mixture models, partial identifications, and\nnonparametric set estimation. We analyze solution manifolds both theoretically\nand algorithmically. In terms of theory, we derive five useful results: the\nsmoothness theorem, the stability theorem (which implies the consistency of a\nplug-in estimator), the convergence of a gradient flow, the local center\nmanifold theorem and the convergence of the gradient descent algorithm. To\nnumerically approximate a solution manifold, we propose a Monte Carlo gradient\ndescent algorithm. In the case of likelihood inference, we design a manifold\nconstraint maximization procedure to find the maximum likelihood estimator on\nthe manifold. We also develop a method to approximate a posterior distribution\ndefined on a solution manifold.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 01:00:30 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Chen", "Yen-Chi", ""]]}, {"id": "2002.05319", "submitter": "Oscar Espinosa", "authors": "Oscar Espinosa, Fabio Nieto", "title": "A study on the leverage effect on financial series using a TAR model: a\n  Bayesian approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research shows that under certain mathematical conditions, a threshold\nautoregressive model (TAR) can represent the leverage effect based on its\nconditional variance function. Furthermore, the analytical expressions for the\nthird and fourth moment of the TAR model are obtained when it is weakly\nstationary.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 03:00:52 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 03:02:51 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Espinosa", "Oscar", ""], ["Nieto", "Fabio", ""]]}, {"id": "2002.05326", "submitter": "Victoria Rayskin", "authors": "Victoria Rayskin", "title": "Multivariate time series approximation by multiple trajectories of a\n  dynamical system. Applications to Internet traffic and COVID-19 data", "comments": "arXiv admin note: substantial text overlap with arXiv:1912.06939", "journal-ref": null, "doi": "10.1063/5.0033648", "report-no": null, "categories": "physics.soc-ph math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Utilization of multiple trajectories of a dynamical system model provides us\nwith several benefits in approximation of time series. For short term\npredictions a high accuracy can be achieved via switches to new trajectory at\nany time. Different long term trends (tendency to different stationary points)\nof the phase portrait characterize various scenarios of the process realization\ninfluenced by externalities. The dynamical system's phase portrait analysis\nhelps to see if the equations properly describe the reality. We also extend the\ndynamical systems approach (discussed in \\cite{R5}) to the dynamical systems\nwith external control.\n  We illustrate these ideas with the help of new examples of the rental\nproperties HOMES.mil platform data. We also compare the qualitative properties\nof HOMES.mil and Wikipedia.org platforms' phase portraits and the corresponding\ndifferences of the two platforms' users. In our last example with COVID-19 data\nwe discuss the high accuracy of the short term prediction of confirmed\ninfection cases, recovery cases and death cases in various countries.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 03:38:41 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 01:02:42 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Rayskin", "Victoria", ""]]}, {"id": "2002.05423", "submitter": "Frederic Lavancier", "authors": "Fr\\'ed\\'eric Lavancier, Ronan Le Gu\\'evel (IRMAR)", "title": "Spatial birth-death-move processes : basic properties and estimation of\n  their intensity functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many spatio-temporal data record the time of birth and death of individuals,\nalong with their spatial trajectories during their lifetime, whether through\ncontinuous-time observations or discrete-time observations. Natural\napplications include epidemiology, individual-based modelling in ecology,\nspatio-temporal dynamics observed in bio-imaging, and computer vision. The aim\nof this article is to estimate in this context the birth and death intensity\nfunctions, that depend in full generality on the current spatial configuration\nof all alive individuals. While the temporal evolution of the population size\nis a simple birth-death process, observing the lifetime and trajectories of all\nindividuals calls for a new paradigm. To formalise this framework, we introduce\nspatial birth-death-move processes, where the birth and death dynamics depends\non the current spatial configuration of the population and where individuals\ncan move during their lifetime according to a continuous Markov process with\npossible interactions.We consider non-parametric kernel estimators of their\nbirth and death intensity functions. The setting is original because each\nobservation in time belongs to a non-vectorial, infinite dimensional space and\nthe dependence between observations is barely tractable. We prove the\nconsistency of the estimators in presence of continuous-time and discrete-time\nobservations, under fairly simple conditions. We moreover discuss how we can\ntake advantage in practice of structural assumptions made on the intensity\nfunctions and we explain how data-driven bandwidth selection can be conducted,\ndespite the unknown (and sometimes undefined) second order moments of the\nestimators. We finally apply our statistical method to the analysis of the\nspatio-temporal dynamics of proteins involved in exocytosis in cells, providing\nnew insights on this complex mechanism.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 10:29:44 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 08:32:51 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Lavancier", "Fr\u00e9d\u00e9ric", "", "IRMAR"], ["Gu\u00e9vel", "Ronan Le", "", "IRMAR"]]}, {"id": "2002.05424", "submitter": "Carlo Ciliberto", "authors": "Carlo Ciliberto, Lorenzo Rosasco, Alessandro Rudi", "title": "A General Framework for Consistent Structured Prediction with Implicit\n  Loss Embeddings", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a novel theoretical and algorithmic framework for\nstructured prediction. While so far the term has referred to discrete output\nspaces, here we consider more general settings, such as manifolds or spaces of\nprobability measures. We define structured prediction as a problem where the\noutput space lacks a vectorial structure. We identify and study a large class\nof loss functions that implicitly defines a suitable geometry on the problem.\nThe latter is the key to develop an algorithmic framework amenable to a sharp\nstatistical analysis and yielding efficient computations. When dealing with\noutput spaces with infinite cardinality, a suitable implicit formulation of the\nestimator is shown to be crucial.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 10:30:04 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Ciliberto", "Carlo", ""], ["Rosasco", "Lorenzo", ""], ["Rudi", "Alessandro", ""]]}, {"id": "2002.05592", "submitter": "Manuel Lladser", "authors": "Antony Pearson and Manuel E. Lladser", "title": "On Contamination of Symbolic Datasets", "comments": "18 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST q-bio.GN stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data taking values on discrete sample spaces are the embodiment of modern\nbiological research. \"Omics\" experiments produce millions of symbolic outcomes\nin the form of reads (i.e., DNA sequences of a few dozens to a few hundred\nnucleotides). Unfortunately, these intrinsically non-numerical datasets are\noften highly contaminated, and the possible sources of contamination are\nusually poorly characterized. This contrasts with numerical datasets where\nGaussian-type noise is often well-justified. To overcome this hurdle, we\nintroduce the notion of latent weight, which measures the largest expected\nfraction of samples from a contaminated probabilistic source that conform to a\nmodel in a well-structured class of desired models. We examine various\nproperties of latent weights, which we specialize to the class of exchangeable\nprobability distributions. As proof of concept, we analyze DNA methylation data\nfrom the 22 human autosome pairs. Contrary to what it is usually assumed, we\nprovide strong evidence that highly specific methylation patterns are\noverrepresented at some genomic locations when contamination is taken into\naccount.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 16:13:09 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Pearson", "Antony", ""], ["Lladser", "Manuel E.", ""]]}, {"id": "2002.05632", "submitter": "Vasilis Kontonis", "authors": "Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, and Nikos\n  Zarifis", "title": "Learning Halfspaces with Massart Noise Under Structured Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning halfspaces with Massart noise in the\ndistribution-specific PAC model. We give the first computationally efficient\nalgorithm for this problem with respect to a broad family of distributions,\nincluding log-concave distributions. This resolves an open question posed in a\nnumber of prior works. Our approach is extremely simple: We identify a smooth\n{\\em non-convex} surrogate loss with the property that any approximate\nstationary point of this loss defines a halfspace that is close to the target\nhalfspace. Given this structural result, we can use SGD to solve the underlying\nlearning problem.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 17:02:37 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kontonis", "Vasilis", ""], ["Tzamos", "Christos", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2002.05792", "submitter": "Nadia Mezouar", "authors": "Abdenour Hamdaoui, Abdelkader Benkhaled, Nadia Mezouar", "title": "Minimaxity and Limits of Risks Ratios of Shrinkage Estimators of a\n  Multivariate Normal Mean in the Bayesian Case", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we consider two forms of shrinkage estimators of the mean\n$\\theta$ of a multivariate normal distribution $X\\sim N_{p}\\left(\\theta,\n\\sigma^{2}I_{p}\\right)$ where $\\sigma^{2}$ is unknown. We take the prior law\n$\\theta \\sim N_{p}\\left(\\upsilon, \\tau^{2}I_{p}\\right)$ and we constuct a\nModified Bayes estimator $\\delta_{B}^{\\ast}$ and an Empirical Modified Bayes\nestimator $\\delta_{EB}^{\\ast}$. We are interested in studying the minimaxity\nand the limits of risks ratios of these estimators, to the maximum likelihood\nestimator $X$, when $n$ and $p$ tend to infinity.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 21:54:19 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Hamdaoui", "Abdenour", ""], ["Benkhaled", "Abdelkader", ""], ["Mezouar", "Nadia", ""]]}, {"id": "2002.05818", "submitter": "Natalie Doss", "authors": "Natalie Doss and Yihong Wu and Pengkun Yang and Harrison H. Zhou", "title": "Optimal estimation of high-dimensional location Gaussian mixtures", "comments": "Adds Theorems 4.7 and 4.8 on computationally efficient density\n  estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the optimal rate of estimation in a finite Gaussian\nlocation mixture model in high dimensions without separation conditions. We\nassume that the number of components $k$ is bounded and that the centers lie in\na ball of bounded radius, while allowing the dimension $d$ to be as large as\nthe sample size $n$. Extending the one-dimensional result of Heinrich and Kahn\n\\cite{HK2015}, we show that the minimax rate of estimating the mixing\ndistribution in Wasserstein distance is $\\Theta((d/n)^{1/4} + n^{-1/(4k-2)})$,\nachieved by an estimator computable in time $O(nd^2+n^{5/4})$. Furthermore, we\nshow that the mixture density can be estimated at the optimal parametric rate\n$\\Theta(\\sqrt{d/n})$ in Hellinger distance and provide a computationally\nefficient algorithm to achieve this rate in the special case of $k=2$.\n  Both the theoretical and methodological development rely on a careful\napplication of the method of moments. Central to our results is the observation\nthat the information geometry of finite Gaussian mixtures is characterized by\nthe moment tensors of the mixing distribution, whose low-rank structure can be\nexploited to obtain a sharp local entropy bound.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 00:11:54 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 00:49:14 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Doss", "Natalie", ""], ["Wu", "Yihong", ""], ["Yang", "Pengkun", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "2002.05949", "submitter": "Saroja Kumar Singh", "authors": "Sarat Kumar Acharya, Saroja Kumar Singh", "title": "Upper and Lower Class Functions for Maximum Likelihood Estimator for\n  Single server Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Upper and lower class functions for the maximum likelihood estimator of the\narrival and the service rates in a $GI/G/1$ queue are studied and the results\nare verified for M/M/1 queue.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 10:15:16 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Acharya", "Sarat Kumar", ""], ["Singh", "Saroja Kumar", ""]]}, {"id": "2002.06041", "submitter": "Iavor Bojinov", "authors": "Guillaume Basse and Iavor Bojinov", "title": "A general theory of identification", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What does it mean to say that a quantity is identifiable from the data?\nStatisticians seem to agree on a definition in the context of parametric\nstatistical models --- roughly, a parameter $\\theta$ in a model $\\mathcal{P} =\n\\{P_\\theta: \\theta \\in \\Theta\\}$ is identifiable if the mapping $\\theta \\mapsto\nP_\\theta$ is injective. This definition raises important questions: Are\nparameters the only quantities that can be identified? Is the concept of\nidentification meaningful outside of parametric statistics? Does it even\nrequire the notion of a statistical model? Partial and idiosyncratic answers to\nthese questions have been discussed in econometrics, biological modeling, and\nin some subfields of statistics like causal inference. This paper proposes a\nunifying theory of identification that incorporates existing definitions for\nparametric and nonparametric models and formalizes the process of\nidentification analysis. The applicability of this framework is illustrated\nthrough a series of examples and two extended case studies.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 14:03:42 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Basse", "Guillaume", ""], ["Bojinov", "Iavor", ""]]}, {"id": "2002.06084", "submitter": "Xuewen Yu", "authors": "Xuewen Yu, Jim Q. Smith and Linda Nichols", "title": "Bayesian Learning of Causal Relationships for System Reliability", "comments": "8 pages, 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal theory is now widely developed with many applications to medicine and\npublic health. However within the discipline of reliability, although causation\nis a key concept in this field, there has been much less theoretical attention.\nIn this paper, we will demonstrate how some aspects of established causal\nmethodology can be translated via trees, and more specifically chain event\ngraphs, into domain of reliability theory to help the probability modeling of\nfailures. We further show how various domain specific concepts of causality\nparticular to reliability can be imported into more generic causal algebras and\nso demonstrate how these disciplines can inform each other. This paper is\ninformed by a detailed analysis of maintenance records associated with a large\nelectrical distribution company. Causal hypotheses embedded within these\nnatural language texts are extracted and analyzed using the new graphical\nframework we introduced here.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 15:40:10 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Yu", "Xuewen", ""], ["Smith", "Jim Q.", ""], ["Nichols", "Linda", ""]]}, {"id": "2002.06117", "submitter": "Rina Barber", "authors": "Rina Foygel Barber and Richard J. Samworth", "title": "Local continuity of log-concave projection, with applications to\n  estimation under model misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The log-concave projection is an operator that maps a d-dimensional\ndistribution P to an approximating log-concave density. Prior work by\nD{\\\"u}mbgen et al. (2011) establishes that, with suitable metrics on the\nunderlying spaces, this projection is continuous, but not uniformly continuous.\nIn this work we prove a local uniform continuity result for log-concave\nprojection -- in particular, establishing that this map is locally\nH{\\\"o}lder-(1/4) continuous. A matching lower bound verifies that this exponent\ncannot be improved. We also examine the implications of this continuity result\nfor the empirical setting -- given a sample drawn from a distribution P, we\nbound the squared Hellinger distance between the log-concave projection of the\nempirical distribution of the sample, and the log-concave projection of P. In\nparticular, this yields interesting statistical results for the misspecified\nsetting, where P is not itself log-concave.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 16:33:55 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 20:19:24 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2020 13:45:04 GMT"}, {"version": "v4", "created": "Fri, 18 Dec 2020 23:59:53 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2002.06118", "submitter": "Jack Noonan", "authors": "Anatoly Zhigljavsky and Jack Noonan", "title": "Covering of high-dimensional cubes and quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the main problem, we consider covering of a $d$-dimensional cube by $n$\nballs with reasonably large $d$ (10 or more) and reasonably small $n$, like\n$n=100$ or $n=1000$. We do not require the full coverage but only 90\\% or 95\\%\ncoverage. We establish that efficient covering schemes have several important\nproperties which are not seen in small dimensions and in asymptotical\nconsiderations, for very large $n$. One of these properties can be termed `do\nnot try to cover the vertices' as the vertices of the cube and their close\nneighbourhoods are very hard to cover and for large $d$ there are far too many\nof them. We clearly demonstrate that, contrary to a common belief, placing\nballs at points which form a low-discrepancy sequence in the cube, makes for a\nvery inefficient covering scheme. For a family of random coverings, we are able\nto provide very accurate approximations to the coverage probability. We then\nextend our results to the problems of coverage of a cube by smaller cubes and\nquantization, the latter being also referred to as facility location. Along\nwith theoretical considerations and derivation of approximations, we discuss\nresults of a large-scale numerical investigation.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 16:37:11 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Zhigljavsky", "Anatoly", ""], ["Noonan", "Jack", ""]]}, {"id": "2002.06143", "submitter": "Axel B\\\"ucher", "authors": "Axel B\\\"ucher, Holger Dette, Florian Heinrichs", "title": "Are deviations in a gradually varying mean relevant? A testing approach\n  based on sup-norm estimators", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical change point analysis aims at (1) detecting abrupt changes in the\nmean of a possibly non-stationary time series and at (2) identifying regions\nwhere the mean exhibits a piecewise constant behavior. In many applications\nhowever, it is more reasonable to assume that the mean changes gradually in a\nsmooth way. Those gradual changes may either be non-relevant (i.e., small), or\nrelevant for a specific problem at hand, and the present paper presents\nstatistical methodology to detect the latter. More precisely, we consider the\ncommon nonparametric regression model $X_{i} = \\mu (i/n) + \\varepsilon_{i}$\nwith possibly non-stationary errors and propose a test for the null hypothesis\nthat the maximum absolute deviation of the regression function $\\mu$ from a\nfunctional $g (\\mu )$ (such as the value $\\mu (0)$ or the integral\n$\\int_{0}^{1} \\mu (t) dt$) is smaller than a given threshold on a given\ninterval $[x_{0},x_{1}] \\subseteq [0,1]$. A test for this type of hypotheses is\ndeveloped using an appropriate estimator, say $\\hat d_{\\infty, n}$, for the\nmaximum deviation $ d_{\\infty}= \\sup_{t \\in [x_{0},x_{1}]} |\\mu (t) - g( \\mu)\n|$. We derive the limiting distribution of an appropriately standardized\nversion of $\\hat d_{\\infty,n}$, where the standardization depends on the\nLebesgue measure of the set of extremal points of the function\n$\\mu(\\cdot)-g(\\mu)$. A refined procedure based on an estimate of this set is\ndeveloped and its consistency is proved. The results are illustrated by means\nof a simulation study and a data example.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 17:53:58 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Dette", "Holger", ""], ["Heinrichs", "Florian", ""]]}, {"id": "2002.06159", "submitter": "Mohsen Ebadi", "authors": "Mohsen Ebadi, Shoja'eddin Chenouri, Dennis K. J. Lin, Stefan H.\n  Steiner", "title": "Statistical Monitoring of the Covariance Matrix in Multivariate\n  Processes: A Literature Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monitoring several correlated quality characteristics of a process is common\nin modern manufacturing and service industries. Although a lot of attention has\nbeen paid to monitoring the multivariate process mean, not many control charts\nare available for monitoring the covariance matrix. This paper presents a\ncomprehensive overview of the literature on control charts for monitoring the\ncovariance matrix in a multivariate statistical process monitoring (MSPM)\nframework. It classifies the research that has previously appeared in the\nliterature. We highlight the challenging areas for research and provide some\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 18:18:42 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 03:15:40 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ebadi", "Mohsen", ""], ["Chenouri", "Shoja'eddin", ""], ["Lin", "Dennis K. J.", ""], ["Steiner", "Stefan H.", ""]]}, {"id": "2002.06243", "submitter": "Kei Nakagawa", "authors": "Yusuke Uchiyama, Kei Nakagawa", "title": "TPLVM: Portfolio Construction by Student's $t$-process Latent Variable\n  Model", "comments": null, "journal-ref": null, "doi": "10.3390/math8030449", "report-no": null, "categories": "q-fin.PM cs.LG math.ST q-fin.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal asset allocation is a key topic in modern finance theory. To realize\nthe optimal asset allocation on investor's risk aversion, various portfolio\nconstruction methods have been proposed. Recently, the applications of machine\nlearning are rapidly growing in the area of finance. In this article, we\npropose the Student's $t$-process latent variable model (TPLVM) to describe\nnon-Gaussian fluctuations of financial timeseries by lower dimensional latent\nvariables. Subsequently, we apply the TPLVM to minimum-variance portfolio as an\nalternative of existing nonlinear factor models. To test the performance of the\nproposed portfolio, we construct minimum-variance portfolios of global stock\nmarket indices based on the TPLVM or Gaussian process latent variable model. By\ncomparing these portfolios, we confirm the proposed portfolio outperforms that\nof the existing Gaussian process latent variable model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 02:02:02 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Uchiyama", "Yusuke", ""], ["Nakagawa", "Kei", ""]]}, {"id": "2002.06439", "submitter": "Samyajoy Pal", "authors": "Samyajoy Pal, Prof. Dr. Christian Heumann, Dr. M. Subbiah", "title": "Further Inference on Categorical Data -- A Bayesian Approach", "comments": "Further improvement is going on", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three different inferential problems related to a two dimensional categorical\ndata from a Bayesian perspective have been discussed in this article. Conjugate\nprior distribution with symmetric and asymmetric hyper parameters are\nconsidered. Newly conceived asymmetric prior is based on perceived preferences\nof categories. An extension of test of independence by introducing a notion of\nmeasuring association between the parameters has been shown using correlation\nmatrix. Probabilities of different parametric combinations have been estimated\nfrom the posterior distribution using closed form integration, Monte-Carlo\nintegration and MCMC methods to draw further inference from categorical data.\nBayesian computation is done using R programming language and illustrated with\nappropriate data sets. Study has highlighted the application of Bayesian\ninference exploiting the distributional form of underlying parameters.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 20:01:17 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 22:42:17 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Pal", "Samyajoy", ""], ["Heumann", "Prof. Dr. Christian", ""], ["Subbiah", "Dr. M.", ""]]}, {"id": "2002.06467", "submitter": "Yuling Yao", "authors": "Andrew Gelman and Yuling Yao", "title": "Holes in Bayesian Statistics", "comments": "To appear in Journal of Physics G", "journal-ref": null, "doi": "10.1088/1361-6471/abc3a5", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every philosophy has holes, and it is the responsibility of proponents of a\nphilosophy to point out these problems. Here are a few holes in Bayesian data\nanalysis: (1) the usual rules of conditional probability fail in the quantum\nrealm, (2) flat or weak priors lead to terrible inferences about things we care\nabout, (3) subjective priors are incoherent, (4) Bayesian decision picks the\nwrong model, (5) Bayes factors fail in the presence of flat or weak priors, (6)\nfor Cantorian reasons we need to check our models, but this destroys the\ncoherence of Bayesian inference. Some of the problems of Bayesian statistics\narise from people trying to do things they shouldn't be trying to do, but other\nholes are not so easily patched. In particular, it may be a good idea to avoid\nflat, weak, or conventional priors, but such advice, if followed, would go\nagainst the vast majority of Bayesian practice and requires us to confront the\nfundamental incoherence of Bayesian inference. This does not mean that we think\nBayesian inference is a bad idea, but it does mean that there is a tension\nbetween Bayesian logic and Bayesian workflow which we believe can only be\nresolved by considering Bayesian logic as a tool, a way of revealing inevitable\nmisfits and incoherences in our model assumptions, rather than as an end in\nitself.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 23:02:46 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 23:20:21 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Gelman", "Andrew", ""], ["Yao", "Yuling", ""]]}, {"id": "2002.06471", "submitter": "Yanjun Han", "authors": "Zijun Gao and Yanjun Han", "title": "Minimax Optimal Nonparametric Estimation of Heterogeneous Treatment\n  Effects", "comments": "To appear at NeurIPS 2020 as a spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A central goal of causal inference is to detect and estimate the treatment\neffects of a given treatment or intervention on an outcome variable of\ninterest, where a member known as the heterogeneous treatment effect (HTE) is\nof growing popularity in recent practical applications such as the personalized\nmedicine. In this paper, we model the HTE as a smooth nonparametric difference\nbetween two less smooth baseline functions, and determine the tight statistical\nlimits of the nonparametric HTE estimation as a function of the covariate\ngeometry. In particular, a two-stage nearest-neighbor-based estimator throwing\naway observations with poor matching quality is near minimax optimal. We also\nestablish the tight dependence on the density ratio without the usual\nassumption that the covariate densities are bounded away from zero, where a key\nstep is to employ a novel maximal inequality which could be of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 23:42:48 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 11:33:24 GMT"}, {"version": "v3", "created": "Sat, 24 Oct 2020 22:14:24 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Gao", "Zijun", ""], ["Han", "Yanjun", ""]]}, {"id": "2002.06516", "submitter": "Amadou Diadie Ba", "authors": "Ba Amadou Diadie and Lo Gane Samb", "title": "Conditional Shannon, R\\'eyni, and Tsallis entropies estimation and\n  asymptotic limits: discrete case", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A method of estimating the joint probability mass function of a pair of\ndiscrete random variables is described. This estimator is used to construct the\nconditional Shannon-R\\'eyni-Tsallis entropies estimates. From there almost sure\nrates of convergence and asymptotic normality are established. The theorical\nresults are validated by simulations.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 06:15:48 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Diadie", "Ba Amadou", ""], ["Samb", "Lo Gane", ""]]}, {"id": "2002.06524", "submitter": "Chanwoo Lee", "authors": "Chanwoo Lee, Miaoyan Wang", "title": "Tensor denoising and completion based on ordinal observations", "comments": "35 pages, 6 figures", "journal-ref": "Proceedings of the 37th International Conference on Machine\n  Learning(ICML), 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-order tensors arise frequently in applications such as neuroimaging,\nrecommendation system, social network analysis, and psychological studies. We\nconsider the problem of low-rank tensor estimation from possibly incomplete,\nordinal-valued observations. Two related problems are studied, one on tensor\ndenoising and the other on tensor completion. We propose a multi-linear\ncumulative link model, develop a rank-constrained M-estimator, and obtain\ntheoretical accuracy guarantees. Our mean squared error bound enjoys a faster\nconvergence rate than previous results, and we show that the proposed estimator\nis minimax optimal under the class of low-rank models. Furthermore, the\nprocedure developed serves as an efficient completion method which guarantees\nconsistent recovery of an order-$K$ $(d,\\ldots,d)$-dimensional low-rank tensor\nusing only $\\tilde{\\mathcal{O}}(Kd)$ noisy, quantized observations. We\ndemonstrate the outperformance of our approach over previous methods on the\ntasks of clustering and collaborative filtering.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 07:09:56 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2020 19:01:19 GMT"}, {"version": "v3", "created": "Sun, 13 Dec 2020 00:04:56 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Lee", "Chanwoo", ""], ["Wang", "Miaoyan", ""]]}, {"id": "2002.06654", "submitter": "Peter Cohen", "authors": "Peter L. Cohen, Colin B. Fogarty", "title": "Gaussian Prepivoting for Finite Population Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In finite population causal inference exact randomization tests can be\nconstructed for sharp null hypotheses, i.e. hypotheses which fully impute the\nmissing potential outcomes. Oftentimes inference is instead desired for the\nweak null that the sample average of the treatment effects takes on a\nparticular value while leaving the subject-specific treatment effects\nunspecified. Without proper care, tests valid for sharp null hypotheses may be\nanti-conservative should only the weak null hold, creating the risk of\nmisinterpretation when randomization tests are deployed in practice. We develop\na general framework for unifying modes of inference for sharp and weak nulls,\nwherein a single procedure simultaneously delivers exact inference for sharp\nnulls and asymptotically valid inference for weak nulls. To do this, we employ\nrandomization tests based upon prepivoted test statistics, wherein a test\nstatistic is first transformed by a suitably constructed cumulative\ndistribution function and its randomization distribution assuming the sharp\nnull is then enumerated. For a large class of commonly employed test\nstatistics, we show that prepivoting may be accomplished by employing the\npush-forward of a sample-based Gaussian measure based upon a suitably\nconstructed covariance estimator. In essence, the approach enumerates the\nrandomization distribution (assuming the sharp null) of a P-value for a\nlarge-sample test known to be valid under the weak null, and uses the resulting\nrandomization distribution to perform inference. The versatility of the method\nis demonstrated through a host of examples, including rerandomized designs and\nregression-adjusted estimators in completely randomized designs.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 19:31:14 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 22:02:26 GMT"}, {"version": "v3", "created": "Wed, 9 Jun 2021 23:31:29 GMT"}, {"version": "v4", "created": "Sun, 13 Jun 2021 13:36:11 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Cohen", "Peter L.", ""], ["Fogarty", "Colin B.", ""]]}, {"id": "2002.06694", "submitter": "Yuqian Zhang", "authors": "Wei Qian, Yuqian Zhang, Yudong Chen", "title": "Structures of Spurious Local Minima in $k$-means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $k$-means clustering is a fundamental problem in unsupervised learning. The\nproblem concerns finding a partition of the data points into $k$ clusters such\nthat the within-cluster variation is minimized. Despite its importance and wide\napplicability, a theoretical understanding of the $k$-means problem has not\nbeen completely satisfactory. Existing algorithms with theoretical performance\nguarantees often rely on sophisticated (sometimes artificial) algorithmic\ntechniques and restricted assumptions on the data. The main challenge lies in\nthe non-convex nature of the problem; in particular, there exist additional\nlocal solutions other than the global optimum. Moreover, the simplest and most\npopular algorithm for $k$-means, namely Lloyd's algorithm, generally converges\nto such spurious local solutions both in theory and in practice.\n  In this paper, we approach the $k$-means problem from a new perspective, by\ninvestigating the structures of these spurious local solutions under a\nprobabilistic generative model with $k$ ground truth clusters. As soon as\n$k=3$, spurious local minima provably exist, even for well-separated and\nbalanced clusters. One such local minimum puts two centers at one true cluster,\nand the third center in the middle of the other two true clusters. For general\n$k$, one local minimum puts multiple centers at a true cluster, and one center\nin the middle of multiple true clusters. Perhaps surprisingly, we prove that\nthis is essentially the only type of spurious local minima under a separation\ncondition. Our results pertain to the $k$-means formulation for mixtures of\nGaussians or bounded distributions. Our theoretical results corroborate\nexisting empirical observations and provide justification for several improved\nalgorithms for $k$-means clustering.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 22:15:03 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 21:19:50 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Qian", "Wei", ""], ["Zhang", "Yuqian", ""], ["Chen", "Yudong", ""]]}, {"id": "2002.06708", "submitter": "Evan Rosenman", "authors": "Evan Rosenman, Guillaume Basse, Art Owen, Michael Baiocchi", "title": "Combining Observational and Experimental Datasets Using Shrinkage\n  Estimators", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of combining data from observational and experimental\nsources to make causal conclusions. This problem is increasingly relevant, as\nthe modern era has yielded passive collection of massive observational datasets\nin areas such as e-commerce and electronic health. These data may be used to\nsupplement experimental data, which is frequently expensive to obtain. In\nRosenman et al. (2018), we considered this problem under the assumption that\nall confounders were measured. Here, we relax the assumption of\nunconfoundedness. To derive combined estimators with desirable properties, we\nmake use of results from the Stein Shrinkage literature. Our contributions are\nthreefold. First, we propose a generic procedure for deriving shrinkage\nestimators in this setting, making use of a generalized unbiased risk estimate.\nSecond, we develop two new estimators, prove finite sample conditions under\nwhich they have lower risk than an estimator using only experimental data, and\nshow that each achieves a notion of asymptotic optimality. Third, we draw\nconnections between our approach and results in sensitivity analysis, including\nproposing a method for evaluating the feasibility of our estimators.\n", "versions": [{"version": "v1", "created": "Sun, 16 Feb 2020 23:30:42 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 04:52:22 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Rosenman", "Evan", ""], ["Basse", "Guillaume", ""], ["Owen", "Art", ""], ["Baiocchi", "Michael", ""]]}, {"id": "2002.06825", "submitter": "Janine Witte", "authors": "Janine Witte, Leonard Henckel, Marloes H. Maathuis, Vanessa Didelez", "title": "On efficient adjustment in causal graphs", "comments": "45 pages, 11 figures", "journal-ref": "Journal of Machine Learning Research 21(246):1-45 (2020)", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of a total causal effect from observational data via\ncovariate adjustment. Ideally, adjustment sets are selected based on a given\ncausal graph, reflecting knowledge of the underlying causal structure. Valid\nadjustment sets are, however, not unique. Recent research has introduced a\ngraphical criterion for an 'optimal' valid adjustment set (O-set). For a given\ngraph, adjustment by the O-set yields the smallest asymptotic variance compared\nto other adjustment sets in certain parametric and non-parametric models. In\nthis paper, we provide three new results on the O-set. First, we give a novel,\nmore intuitive graphical characterisation: We show that the O-set is the parent\nset of the outcome node(s) in a suitable latent projection graph, which we call\nthe forbidden projection. An important property is that the forbidden\nprojection preserves all information relevant to total causal effect estimation\nvia covariate adjustment, making it a useful methodological tool in its own\nright. Second, we extend the existing IDA algorithm to use the O-set, and argue\nthat the algorithm remains semi-local. This is implemented in the R-package\npcalg. Third, we present assumptions under which the O-set can be viewed as the\ntarget set of popular non-graphical variable selection algorithms such as\nstepwise backward selection.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 08:22:39 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 11:50:53 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Witte", "Janine", ""], ["Henckel", "Leonard", ""], ["Maathuis", "Marloes H.", ""], ["Didelez", "Vanessa", ""]]}, {"id": "2002.06861", "submitter": "Abdulaziz Alsenafi", "authors": "Abdulaziz Alsenafi, Mishari Al-Foraih, Khalifa Es-Sebaiy", "title": "Least squares estimation for non-ergodic weighted fractional\n  Ornstein-Uhlenbeck process of general parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $B^{a,b}:=\\{B_t^{a,b},t\\geq0\\}$ be a weighted fractional Brownian motion\nof parameters $a>-1$, $|b|<1$, $|b|<a+1$. We consider a least square-type\nmethod to estimate the drift parameter $\\theta>0$ of the weighted fractional\nOrnstein-Uhlenbeck process $X:=\\{X_t,t\\geq0\\}$ defined by $X_0=0; \\ dX_t=\\theta\nX_tdt+dB_t^{a,b}$.\n  In this work, we provide least squares-type estimators for $\\theta$ based\ncontinuous-time and discrete-time observations of $X$. The strong consistency\nand the asymptotic behavior in distribution of the estimators are studied for\nall $(a,b)$ such that $a>-1$, $|b|<1$, $|b|<a+1$. Here we extend the results of\n\\cite{SYY2,SYY} (resp. \\cite{CSC}), where the strong consistency and the\nasymptotic distribution of the estimators are proved for\n  $-\\frac12<a<0$, $-a<b<a+1$ (resp. $-1<a<0$, $-a<b<a+1$).\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 09:45:52 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 08:38:30 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Alsenafi", "Abdulaziz", ""], ["Al-Foraih", "Mishari", ""], ["Es-Sebaiy", "Khalifa", ""]]}, {"id": "2002.06870", "submitter": "Ayan Bhattacharya", "authors": "Ayan Bhattacharya, Bohan Chen, Remco van der Hofstad and Bert Zwart", "title": "Consistency of the PLFit estimator for power-law data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the consistency of the Power-Law Fit PLFit method proposed by\nClauset et al.(2009) to estimate the power-law exponent in data coming from a\ndistribution function with regularly-varying tail. In the complex systems\ncommunity, PLFit has emerged as the method of choice to estimate the power-law\nexponent. Yet, its mathematical properties are still poorly understood.\n  The difficulty in PLFit is that it is a minimum-distance estimator. It first\nchooses a threshold that minimizes the Kolmogorov-Smirnov distance between the\ndata points larger than the threshold and the Pareto tail, and then applies the\nHill estimator to this restricted data. Since the number of order statistics\nused is random, the general theory of consistency of power-law exponents from\nextreme value theory does not apply. Our proof consists in first showing that\nthe Hill estimator is consistent for general intermediate sequences for the\nnumber of order statistics used, even when that number is random. Here, we call\na sequence intermediate when it grows to infinity, while remaining much smaller\nthan the sample size. The second, and most involved, step is to prove that the\noptimizer in PLFit is with high probability an intermediate sequence, unless\nthe distribution has a Pareto tail above a certain value. For the latter\nspecial case, we give a separate proof.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 10:18:34 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Bhattacharya", "Ayan", ""], ["Chen", "Bohan", ""], ["van der Hofstad", "Remco", ""], ["Zwart", "Bert", ""]]}, {"id": "2002.06956", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Fr\\'ed\\'eric Ouimet", "title": "Density estimation using Dirichlet kernels", "comments": "39 pages, 1 figure v2: intro added, some proofs were fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study theoretically, for the first time, the Dirichlet kernel estimator\nintroduced by Aitchison & Lauder (1985) for the estimation of multivariate\ndensities supported on the $d$-dimensional simplex. The simplex is an important\ncase as it is the natural domain of compositional data and has been neglected\nin the literature on asymmetric kernels. Dirichlet kernel estimators, which\ngeneralize the unidimensional Beta kernel estimator from Chen (1999), are free\nof boundary bias and non-negative everywhere on the simplex. We show that they\nachieve the optimal convergence rate $O(n^{-4/(d+4)})$ for the mean squared\nerror and the mean integrated squared error, we prove their asymptotic\nnormality and uniform strong consistency, and we also find an asymptotic\nexpression for the mean absolute error.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 13:49:27 GMT"}, {"version": "v2", "created": "Sun, 2 Aug 2020 14:52:30 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2002.07261", "submitter": "Johannes Wiesel", "authors": "Julio Backhoff, Daniel Bartl, Mathias Beiglb\\\"ock, Johannes Wiesel", "title": "Estimating processes in adapted Wasserstein distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of researchers have independently introduced topologies on the set\nof laws of stochastic processes that extend the usual weak topology. Depending\non the respective scientific background this was motivated by applications and\nconnections to various areas (e.g. Plug-Pichler - stochastic programming,\nHellwig - game theory, Aldous - stability of optimal stopping, Hoover-Keisler -\nmodel theory). Remarkably, all these seemingly independent approaches define\nthe same adapted weak topology in finite discrete time. Our first main result\nis to construct an adapted variant of the empirical measure that consistently\nestimates the laws of stochastic processes in full generality. A natural\ncompatible metric for the weak adapted topology is the given by an adapted\nrefinement of the Wasserstein distance, as established in the seminal works of\nPflug-Pichler. Specifically, the adapted Wasserstein distance allows to control\nthe error in stochastic optimization problems, pricing and hedging problems,\noptimal stopping problems, etc. in a Lipschitz fashion. The second main result\nof this article yields quantitative bounds for the convergence of the adapted\nempirical measure with respect to adapted Wasserstein distance. Surprisingly,\nwe obtain virtually the same optimal rates and concentration results that are\nknown for the classical empirical measure wrt. Wasserstein distance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 21:35:47 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 13:17:14 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Backhoff", "Julio", ""], ["Bartl", "Daniel", ""], ["Beiglb\u00f6ck", "Mathias", ""], ["Wiesel", "Johannes", ""]]}, {"id": "2002.07284", "submitter": "Hossein Taheri", "authors": "Hossein Taheri, Ramtin Pedarsani, and Christos Thrampoulidis", "title": "Sharp Asymptotics and Optimal Performance for Inference in Binary Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT eess.SP math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study convex empirical risk minimization for high-dimensional inference in\nbinary models. Our first result sharply predicts the statistical performance of\nsuch estimators in the linear asymptotic regime under isotropic Gaussian\nfeatures. Importantly, the predictions hold for a wide class of convex loss\nfunctions, which we exploit in order to prove a bound on the best achievable\nperformance among them. Notably, we show that the proposed bound is tight for\npopular binary models (such as Signed, Logistic or Probit), by constructing\nappropriate loss functions that achieve it. More interestingly, for binary\nlinear classification under the Logistic and Probit models, we prove that the\nperformance of least-squares is no worse than 0.997 and 0.98 times the optimal\none. Numerical simulations corroborate our theoretical findings and suggest\nthey are accurate even for relatively small problem dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 22:32:14 GMT"}, {"version": "v2", "created": "Wed, 26 Feb 2020 06:14:29 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Taheri", "Hossein", ""], ["Pedarsani", "Ramtin", ""], ["Thrampoulidis", "Christos", ""]]}, {"id": "2002.07337", "submitter": "Hajime Kawakami", "authors": "Hajime Kawakami", "title": "Stabilities of Shape Identification Inverse Problems in a Bayesian\n  Framework", "comments": "Accepted by Journal of Mathematical Analysis and Applications", "journal-ref": null, "doi": "10.1016/j.jmaa.2020.123903", "report-no": null, "categories": "math.ST math.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general shape identification inverse problem is studied in a Bayesian\nframework. This problem requires the determination of the unknown shape of a\ndomain in the Euclidean space from finite-dimensional observation data with\nsome Gaussian random noise. Then, the stability of posterior is studied for\nobservation data. For each point of the space, the conditional probability that\nthe point is included in the unknown domain given the observation data is\nconsidered. The stability is also studied for this probability distribution. As\na model problem for our inverse problem, a heat inverse problem is considered.\nThis problem requires the determination of the unknown shape of cavities in a\nheat conductor from temperature data of some portion of the surface of the heat\nconductor. To apply the above stability results to this model problem, one\nneeds the measurability and some boundedness of the forward operator. These\nproperties are shown.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 02:19:16 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Kawakami", "Hajime", ""]]}, {"id": "2002.07384", "submitter": "Abhimanu Kumar", "authors": "Abhimanu Kumar, Aniket Anand Deshmukh, Urun Dogan, Denis Charles, Eren\n  Manavoglu", "title": "Data Transformation Insights in Self-supervision with Clustering Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervision is key to extending use of deep learning for label scarce\ndomains. For most of self-supervised approaches data transformations play an\nimportant role. However, up until now the impact of transformations have not\nbeen studied. Furthermore, different transformations may have different impact\non the system. We provide novel insights into the use of data transformation in\nself-supervised tasks, specially pertaining to clustering. We show\ntheoretically and empirically that certain set of transformations are helpful\nin convergence of self-supervised clustering. We also show the cases when the\ntransformations are not helpful or in some cases even harmful. We show faster\nconvergence rate with valid transformations for convex as well as certain\nfamily of non-convex objectives along with the proof of convergence to the\noriginal set of optima. We have synthetic as well as real world data\nexperiments. Empirically our results conform with the theoretical insights\nprovided.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 05:49:27 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Kumar", "Abhimanu", ""], ["Deshmukh", "Aniket Anand", ""], ["Dogan", "Urun", ""], ["Charles", "Denis", ""], ["Manavoglu", "Eren", ""]]}, {"id": "2002.07424", "submitter": "Patrick Michl", "authors": "Patrick Michl", "title": "Foundations of Structural Statistics: Statistical Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Upon a consistent topological statistical theory the application of\nstructural statistics requires a quantification of the proximity structure of\nmodel spaces. An important tool to study these structures are Pseudo-Riemannian\nmetrices, which in the category of statistical models are induced by\nstatistical divergences. The present article extends the notation of\ntopological statistical models by a differential structure to statistical\nmanifolds and introduces the differential geometric foundations to study\ndistribution families by their differential-, Riemannian- and symplectic\ngeometry.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 08:15:20 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 13:50:54 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Michl", "Patrick", ""]]}, {"id": "2002.07433", "submitter": "Fang Xie", "authors": "Fang Xie", "title": "Estimating the Penalty Level of $\\ell_{1}$-minimization via Two Gaussian\n  Approximation Methods", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to give a theoretical approximation for the penalty\nlevel of $\\ell_{1}$-regularization problems. This can save much time in\npractice compared with the traditional methods, such as cross-validation. To\nachieve this goal, we develop two Gaussian approximation methods, which are\nbased on a moderate deviation theorem and Stein's method respectively. Both of\nthem give efficient approximations and have good performances in simulations.\nWe apply the two Gaussian approximation methods into three types of ultra-high\ndimensional $\\ell_{1}$ penalized regressions: lasso, square-root lasso, and\nweighted $\\ell_{1}$ penalized Poisson regression. The numerical results\nindicate that our two ways to estimate the penalty levels achieve high\ncomputational efficiency. Besides, our prediction errors outperform that based\non the 10-fold cross-validation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 09:02:22 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Xie", "Fang", ""]]}, {"id": "2002.07441", "submitter": "Fang Xie", "authors": "Fang Xie and Zhijie Xiao", "title": "Consistency of $\\ell _{1}$ Penalized Negative Binomial Regressions", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the consistency of the $\\ell_1$ penalized negative binomial\nregression (NBR). A real data application about German health care demand shows\nthat the $\\ell_1$ penalized NBR produces a more concise but more accurate\nmodel, comparing to the classical NBR.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 09:22:20 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Xie", "Fang", ""], ["Xiao", "Zhijie", ""]]}, {"id": "2002.07623", "submitter": "Sandra Schluttenhofer", "authors": "Sandra Schluttenhofer and Jan Johannes", "title": "Adaptive minimax testing in inverse Gaussian sequence space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the inverse Gaussian sequence space model with additional noisy\nobservations of the operator, we derive nonasymptotic minimax radii of testing\nfor ellipsoid-type alternatives simultaneously for both the signal detection\nproblem (testing against zero) and the goodness-of-fit testing problem (testing\nagainst a prescribed sequence) without any regularity assumption on the null\nhypothesis. The radii are the maximum of two terms, each of which only depends\non one of the noise levels. Interestingly, the term involving the noise level\nof the operator explicitly depends on the null hypothesis and vanishes in the\nsignal detection case. The minimax radii are established by first showing a\nlower bound for arbitrary null hypotheses and noise levels. For the upper bound\nwe consider two testing procedures, a direct test based on estimating the\nenergy in the image space and an indirect test. Under mild assumptions, we\nprove that the testing radius of the indirect test achieves the lower bound,\nwhich shows the minimax optimality of the radius and the test. We highlight the\nassumptions under which the direct test also performs optimally. Furthermore,\nwe apply a classical Bonferroni method for making both the indirect and the\ndirect test adaptive with respect to the regularity of the alternative. The\nradii of the adaptive tests are deteriorated by an additional log-factor, which\nwe show to be unavoidable. The results are illustrated considering Sobolev\nspaces and mildly or severely ill-posed inverse problems.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 15:02:06 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Schluttenhofer", "Sandra", ""], ["Johannes", "Jan", ""]]}, {"id": "2002.07624", "submitter": "Rong Ma", "authors": "T. Tony Cai, Hongzhe Li and Rong Ma", "title": "Optimal Structured Principal Subspace Estimation: Metric Entropy and\n  Minimax Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Driven by a wide range of applications, many principal subspace estimation\nproblems have been studied individually under different structural constraints.\nThis paper presents a unified framework for the statistical analysis of a\ngeneral structured principal subspace estimation problem which includes as\nspecial cases non-negative PCA/SVD, sparse PCA/SVD, subspace constrained\nPCA/SVD, and spectral clustering. General minimax lower and upper bounds are\nestablished to characterize the interplay between the information-geometric\ncomplexity of the structural set for the principal subspaces, the\nsignal-to-noise ratio (SNR), and the dimensionality. The results yield\ninteresting phase transition phenomena concerning the rates of convergence as a\nfunction of the SNRs and the fundamental limit for consistent estimation.\nApplying the general results to the specific settings yields the minimax rates\nof convergence for those problems, including the previous unknown optimal rates\nfor non-negative PCA/SVD, sparse SVD and subspace constrained PCA/SVD.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 15:02:11 GMT"}, {"version": "v2", "created": "Sun, 23 Feb 2020 16:16:54 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 13:09:52 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""], ["Ma", "Rong", ""]]}, {"id": "2002.07673", "submitter": "Rajasekhar Anguluri", "authors": "Rajasekhar Anguluri, Vaibhav Katewa, Sandip Roy, Fabio Pasqualetti", "title": "Network Theoretic Analysis of Maximum a Posteriori Detectors for Optimal\n  Input Detection", "comments": "14 pages; 6 figures; submitted to Automatica", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers maximum-a-posteriori (MAP) and linear discriminant based\nMAP detectors to detect changes in the mean and covariance of a stochastic\ninput, driving specific network nodes, using noisy measurements from sensors\nnon-collocated with the input nodes. We explicitly characterize both detectors'\nperformance in terms of the network edge weights and input and sensor nodes'\nlocation. In the asymptotic measurement regime, when the input and measurement\nnoise are jointly Gaussian, we show that the detectors' performance can be\nstudied using the input to output gain of the system's transfer function\nmatrix. Using this result, we obtain conditions for which the detection\nperformance associated with the sensors on a given network cut is better (or\nworse) than that of the sensors associated with the subnetwork induced by the\ncut and not containing the input nodes. Our results also provide structural\ninsights into the sensor placement from a detection-theoretic viewpoint. We\nvalidate our theoretical findings via multiple numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:02:24 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 23:50:17 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Anguluri", "Rajasekhar", ""], ["Katewa", "Vaibhav", ""], ["Roy", "Sandip", ""], ["Pasqualetti", "Fabio", ""]]}, {"id": "2002.07729", "submitter": "Akshay Krishnamurthy", "authors": "Yi Su, Pavithra Srinath, Akshay Krishnamurthy", "title": "Adaptive Estimator Selection for Off-Policy Evaluation", "comments": "Fixed some typos. Published in ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a generic data-driven method for estimator selection in off-policy\npolicy evaluation settings. We establish a strong performance guarantee for the\nmethod, showing that it is competitive with the oracle estimator, up to a\nconstant factor. Via in-depth case studies in contextual bandits and\nreinforcement learning, we demonstrate the generality and applicability of the\nmethod. We also perform comprehensive experiments, demonstrating the empirical\nefficacy of our approach and comparing with related approaches. In both case\nstudies, our method compares favorably with existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 16:57:42 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 14:54:22 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Su", "Yi", ""], ["Srinath", "Pavithra", ""], ["Krishnamurthy", "Akshay", ""]]}, {"id": "2002.07758", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Fr\\'ed\\'eric Ouimet", "title": "Asymptotic properties of Bernstein estimators on the simplex", "comments": "22 pages, 1 figure", "journal-ref": "J. Multivar. Anal. (2021)", "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bernstein estimators are well-known to avoid the boundary bias problem of\ntraditional kernel estimators. The theoretical properties of these estimators\nhave been studied extensively on compact intervals and hypercubes, but never on\nthe simplex, except for the mean squared error of the density estimator in\nTenbusch (1994) when $d = 2$. The simplex is an important case as it is the\nnatural domain of compositional data. In this paper, we make an effort to prove\nseveral asymptotic results (bias, variance, mean squared error (MSE), mean\nintegrated squared error (MISE), asymptotic normality, uniform strong\nconsistency) for Bernstein estimators of cumulative distribution functions and\ndensity functions on the $d$-dimensional simplex. Our results generalize the\nones in Leblanc (2012) and Babu et al. (2002), who treated the case $d = 1$,\nand significantly extend those found in Tenbusch (1994). In particular, our\nrates of convergence for the MSE and MISE are optimal.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 17:48:54 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2020 22:52:46 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 21:55:15 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2002.07808", "submitter": "Kirstin Strokorb", "authors": "Kirstin Strokorb", "title": "Extremal independence old and new", "comments": "2 pages. Comment on arXiv:1812.01734", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On 12 February 2020 the Royal Statistical Society hosted a meeting to discuss\nthe forthcoming paper ``Graphical models for extremes'' by Sebastian Engelke\nand Adrien Hitz [arXiv:1812.01734]. This short note is a supplement to my\ndiscussion contribution. It contains the proofs. It is shown that the\ntraditional notion of extremal independence agrees with the newly introduced\nnotion of extremal independence, which subsequently allows for a meaningful\ninterpretation of disconnected graphs in the context of the discussion paper.\nThe notation and references used in this note are adopted from the discussion\npaper.\n", "versions": [{"version": "v1", "created": "Fri, 14 Feb 2020 20:56:24 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Strokorb", "Kirstin", ""]]}, {"id": "2002.07859", "submitter": "Art Owen", "authors": "Art B. Owen and Daniel Rudolf", "title": "A strong law of large numbers for scrambled net integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a strong law of large numbers for integration on\ndigital nets randomized by a nested uniform scramble. The motivating problem is\noptimization over some variables of an integral over others, arising in\nBayesian optimization. This strong law requires that the integrand have a\nfinite moment of order $p$ for some $p>1$. Previously known results implied a\nstrong law only for Riemann integrable functions. Previous general weak laws of\nlarge numbers for scrambled nets require a square integrable integrand. We\ngeneralize from $L^2$ to $L^p$ for $p>1$ via the Riesz-Thorin interpolation\ntheorem\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 20:07:55 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 00:12:38 GMT"}, {"version": "v3", "created": "Sat, 27 Jun 2020 22:41:47 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Owen", "Art B.", ""], ["Rudolf", "Daniel", ""]]}, {"id": "2002.07888", "submitter": "Weimao Ke", "authors": "Weimao Ke", "title": "DLITE: The Discounted Least Information Theory of Entropy", "comments": "12 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an entropy-based information measure, namely the Discounted Least\nInformation Theory of Entropy (DLITE), which not only exhibits important\ncharacteristics expected as an information measure but also satisfies\nconditions of a metric. Classic information measures such as Shannon Entropy,\nKL Divergence, and Jessen-Shannon Divergence have manifested some of these\nproperties while missing others. This work fills an important gap in the\nadvancement of information theory and its application, where related properties\nare desirable.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 21:41:21 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Ke", "Weimao", ""]]}, {"id": "2002.07928", "submitter": "Dimitrios Giannakis", "authors": "Tyrus Berry, Dimitrios Giannakis, John Harlim", "title": "Bridging data science and dynamical systems theory", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DS physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short review describes mathematical techniques for statistical analysis\nand prediction in dynamical systems. Two problems are discussed, namely (i) the\nsupervised learning problem of forecasting the time evolution of an observable\nunder potentially incomplete observations at forecast initialization; and (ii)\nthe unsupervised learning problem of identification of observables of the\nsystem with a coherent dynamical evolution. We discuss how ideas from from\noperator-theoretic ergodic theory combined with statistical learning theory\nprovide an effective route to address these problems, leading to methods\nwell-adapted to handle nonlinear dynamics, with convergence guarantees as the\namount of training data increases.\n", "versions": [{"version": "v1", "created": "Tue, 18 Feb 2020 23:53:33 GMT"}, {"version": "v2", "created": "Thu, 12 Mar 2020 02:13:45 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2020 02:59:14 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Berry", "Tyrus", ""], ["Giannakis", "Dimitrios", ""], ["Harlim", "John", ""]]}, {"id": "2002.08092", "submitter": "James Duffy", "authors": "James A. Duffy and Jerome R. Simons", "title": "The Cointegrated VAR without Unit Roots: Representation Theory and\n  Asymptotics", "comments": "ii + 37 pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been known since Elliott (1998) that efficient methods of inference on\ncointegrating relationships break down when autoregressive roots are near but\nnot exactly equal to unity. This paper addresses this problem within the\nframework of a VAR with non-unit roots. We develop a characterisation of\ncointegration, based on the impulse response function implied by the VAR, that\nremains meaningful even when roots are not exactly unity. Under this\ncharacterisation, the long-run equilibrium relationships between the series are\nidentified with a subspace associated to the largest characteristic roots of\nthe VAR. We analyse the asymptotics of maximum likelihood estimators of this\nsubspace, thereby generalising Johansen's (1995) treatment of the cointegrated\nVAR with exactly unit roots. Inference is complicated by nuisance parameter\nproblems similar to those encountered in the context of predictive regressions,\nand can be dealt with by approaches familiar from that setting.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 10:02:50 GMT"}], "update_date": "2020-02-20", "authors_parsed": [["Duffy", "James A.", ""], ["Simons", "Jerome R.", ""]]}, {"id": "2002.08100", "submitter": "Solym Manou-Abi Ph.D.", "authors": "Solym M. Manou-Abi", "title": "On bounded mild solutions for a class of semilinear stochastic evolution\n  equation driven by stable process", "comments": "20", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the existence and uniqueness of Lp-bounded mild solutions for a\nclass ofsemilinear stochastic evolutions equations driven by a real L\\'evy\nprocesses withoutGaussian component not square integrable for instance the\nstable process through atruncation method by separating the big and small jumps\ntogether with the classicaland simple Banach fixed point theorem ; under local\nLipschitz, Holder, linear growthconditions on the coefficients.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 10:47:34 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 22:13:39 GMT"}, {"version": "v3", "created": "Mon, 23 Mar 2020 07:23:53 GMT"}, {"version": "v4", "created": "Tue, 2 Feb 2021 06:38:11 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Manou-Abi", "Solym M.", ""]]}, {"id": "2002.08409", "submitter": "Michele Caprio", "authors": "Michele Caprio and Sayan Mukherjee", "title": "Finite mixture models: a bridge with stochastic geometry and Choquet\n  theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Bayesian density estimation, a question of interest is how the number of\ncomponents in a finite mixture model grows with the number of observations. We\nprovide a novel perspective on this question by using results from stochastic\ngeometry to find that the growth rate of the expected number of components of a\nfinite mixture model whose components belong to the unit simplex $\\Delta^{J-1}$\nof the Euclidean space $\\mathbb{R}^J$ is $(\\log n)^{J-1}$. We also provide a\ncentral limit theorem for the number of components. In addition, we relate our\nmodel to a classical non-parametric density estimator based on a P\\'olya tree.\nCombining this latter with techniques from Choquet theory, we are able to\nretrieve mixture weights. We also give the rate of convergence of the P\\'olya\ntree posterior to the Dirac measure on the weights. The analyses in this paper\napply to the well developed and popular latent Dirichlet allocation model.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 19:52:00 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 19:22:57 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 18:59:00 GMT"}, {"version": "v4", "created": "Sat, 11 Apr 2020 13:05:34 GMT"}, {"version": "v5", "created": "Thu, 27 Aug 2020 09:37:52 GMT"}, {"version": "v6", "created": "Thu, 25 Feb 2021 14:18:10 GMT"}], "update_date": "2021-02-26", "authors_parsed": [["Caprio", "Michele", ""], ["Mukherjee", "Sayan", ""]]}, {"id": "2002.08422", "submitter": "Jaehyeok Shin", "authors": "Jaehyeok Shin, Aaditya Ramdas, Alessandro Rinaldo", "title": "On conditional versus marginal bias in multi-armed bandits", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bias of the sample means of the arms in multi-armed bandits is an\nimportant issue in adaptive data analysis that has recently received\nconsiderable attention in the literature. Existing results relate in precise\nways the sign and magnitude of the bias to various sources of data adaptivity,\nbut do not apply to the conditional inference setting in which the sample means\nare computed only if some specific conditions are satisfied. In this paper, we\ncharacterize the sign of the conditional bias of monotone functions of the\nrewards, including the sample mean. Our results hold for arbitrary conditioning\nevents and leverage natural monotonicity properties of the data collection\npolicy. We further demonstrate, through several examples from sequential\ntesting and best arm identification, that the sign of the conditional and\nmarginal bias of the sample mean of an arm can be different, depending on the\nconditioning event. Our analysis offers new and interesting perspectives on the\nsubtleties of assessing the bias in data adaptive settings.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 20:16:10 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2020 04:02:37 GMT"}, {"version": "v3", "created": "Mon, 22 Feb 2021 21:10:15 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Shin", "Jaehyeok", ""], ["Ramdas", "Aaditya", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "2002.08521", "submitter": "Guanhua Fang", "authors": "Haochen Xu and Guanhua Fang and Xuening Zhu", "title": "Network Group Hawkes Process Model", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the event occurrences of user activities on online\nsocial network platforms. To characterize the social activity interactions\namong network users, we propose a network group Hawkes (NGH) process model.\nParticularly, the observed network structure information is employed to model\nthe users' dynamic posting behaviors. Furthermore, the users are clustered into\nlatent groups according to their dynamic behavior patterns. To estimate the\nmodel, a constraint maximum likelihood approach is proposed. Theoretically, we\nestablish the consistency and asymptotic normality of the estimators. In\naddition, we show that the group memberships can be identified consistently. To\nconduct estimation, a branching representation structure is firstly introduced,\nand a stochastic EM (StEM) algorithm is developed to tackle the computational\nproblem. Lastly, we apply the proposed method to a social network data\ncollected from Sina Weibo, and identify the infuential network users as an\ninteresting application.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 01:30:42 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Xu", "Haochen", ""], ["Fang", "Guanhua", ""], ["Zhu", "Xuening", ""]]}, {"id": "2002.08543", "submitter": "Marc Jaffrey Ph.D.", "authors": "Marc Jaffrey and Michael Dushkoff", "title": "Derivation of the Exact Moments of the Distribution of Pearsons\n  Correlation over Permutations of Data", "comments": "8 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Pearson's correlation is one of the most widely used measures of association\ntoday, the importance of which to modern science cannot be understated. Two of\nthe most common methods for computing the p-value for a hypothesis test of this\ncorrelation method are a t-statistic and permutation sampling. When a dataset\ncomes from a bivariate normal distribution under specific data transformations\na t-statistic is exact. However, for datasets which do not follow this\nstipulation, both approaches are merely estimations of the distribution of over\npermutations of data. In this paper we explicitly show the dependency of the\npermutation distribution of Pearson's correlation on the central moments of the\ndata and derive an inductive formula which allows the computation of these\nexact moments. This has direct implications for computing the p-value for\ngeneral datasets which could lead to more computationally accurate methods.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 02:55:49 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Jaffrey", "Marc", ""], ["Dushkoff", "Michael", ""]]}, {"id": "2002.08663", "submitter": "Jonathan Scarlett", "authors": "Anamay Chaturvedi and Jonathan Scarlett", "title": "Learning Gaussian Graphical Models via Multiplicative Weights", "comments": "AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical model selection in Markov random fields is a fundamental problem in\nstatistics and machine learning. Two particularly prominent models, the Ising\nmodel and Gaussian model, have largely developed in parallel using different\n(though often related) techniques, and several practical algorithms with\nrigorous sample complexity bounds have been established for each. In this\npaper, we adapt a recently proposed algorithm of Klivans and Meka (FOCS, 2017),\nbased on the method of multiplicative weight updates, from the Ising model to\nthe Gaussian model, via non-trivial modifications to both the algorithm and its\nanalysis. The algorithm enjoys a sample complexity bound that is qualitatively\nsimilar to others in the literature, has a low runtime $O(mp^2)$ in the case of\n$m$ samples and $p$ nodes, and can trivially be implemented in an online\nmanner.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 10:50:58 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 03:07:45 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Chaturvedi", "Anamay", ""], ["Scarlett", "Jonathan", ""]]}, {"id": "2002.08717", "submitter": "Marcel Nutz", "authors": "Marcel Nutz, Ruodu Wang", "title": "The Directional Optimal Transport", "comments": "Forthcoming in 'Annals of Applied Probability'", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a constrained optimal transport problem where origins $x$ can\nonly be transported to destinations $y\\geq x$. Our statistical motivation is to\ndescribe the sharp upper bound for the variance of the treatment effect $Y-X$\ngiven marginals when the effect is monotone, or $Y\\geq X$. We thus focus on\nsupermodular costs (or submodular rewards) and introduce a coupling $P_{*}$\nthat is optimal for all such costs and yields the sharp bound. This coupling\nadmits manifold characterizations -- geometric, order-theoretic, as optimal\ntransport, through the cdf, and via the transport kernel -- that explain its\nstructure and imply useful bounds. When the first marginal is atomless, $P_{*}$\nis concentrated on the graphs of two maps which can be described in terms of\nthe marginals, the second map arising due to the binding constraint.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 13:11:47 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 17:18:26 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Nutz", "Marcel", ""], ["Wang", "Ruodu", ""]]}, {"id": "2002.08724", "submitter": "Milana Gataric", "authors": "Milana Gataric", "title": "High-resolution signal recovery via generalized sampling and functional\n  principal component analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA eess.SP math.NA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a computational framework for recovering a\nhigh-resolution approximation of an unknown function from its low-resolution\nindirect measurements as well as high-resolution training observations by\nmerging the frameworks of generalized sampling and functional principal\ncomponent analysis. In particular, we increase the signal resolution via a data\ndriven approach, which models the function of interest as a realization of a\nrandom field and leverages a training set of observations generated via the\nsame underlying random process. We study the performance of the resulting\nestimation procedure and show that high-resolution recovery is indeed possible\nprovided appropriate low-rank and angle conditions hold and provided the\ntraining set is sufficiently large relative to the desired resolution.\nMoreover, we show that the size of the training set can be reduced by\nleveraging sparse representations of the functional principal components.\nFurthermore, the effectiveness of the proposed reconstruction procedure is\nillustrated by various numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 13:44:24 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 17:38:41 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Gataric", "Milana", ""]]}, {"id": "2002.08757", "submitter": "Mucyo Karemera", "authors": "St\\'ephane Guerrier, Mucyo Karemera, Samuel Orso, Maria-Pia\n  Victoria-Feser", "title": "Asymptotically Optimal Bias Reduction for Parametric Models", "comments": "arXiv admin note: substantial text overlap with arXiv:1907.11541", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important challenge in statistical analysis concerns the control of the\nfinite sample bias of estimators. This problem is magnified in high-dimensional\nsettings where the number of variables $p$ diverges with the sample size $n$,\nas well as for nonlinear models and/or models with discrete data. For these\ncomplex settings, we propose to use a general simulation-based approach and\nshow that the resulting estimator has a bias of order $\\mathcal{O}(0)$, hence\nproviding an asymptotically optimal bias reduction. It is based on an initial\nestimator that can be slightly asymptotically biased, making the approach very\ngenerally applicable. This is particularly relevant when classical estimators,\nsuch as the maximum likelihood estimator, can only be (numerically)\napproximated. We show that the iterative bootstrap of Kuk (1995) provides a\ncomputationally efficient approach to compute this bias reduced estimator. We\nillustrate our theoretical results in simulation studies for which we develop\nnew bias reduced estimators for the logistic regression, with and without\nrandom effects. These estimators enjoy additional properties such as robustness\nto data contamination and to the problem of separability.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 16:11:08 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Karemera", "Mucyo", ""], ["Orso", "Samuel", ""], ["Victoria-Feser", "Maria-Pia", ""]]}, {"id": "2002.08774", "submitter": "Victor-Emmanuel Brunel", "authors": "Victor-Emmanuel Brunel and Marco Avella-Medina", "title": "Propose, Test, Release: Differentially private estimation with high\n  probability", "comments": "arXiv admin note: text overlap with arXiv:1906.11923", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive concentration inequalities for differentially private median and\nmean estimators building on the \"Propose, Test, Release\" (PTR) mechanism\nintroduced by Dwork and Lei (2009). We introduce a new general version of the\nPTR mechanism that allows us to derive high probability error bounds for\ndifferentially private estimators. Our algorithms provide the first statistical\nguarantees for differentially private estimation of the median and mean without\nany boundedness assumptions on the data, and without assuming that the target\npopulation parameter lies in some known bounded interval. Our procedures do not\nrely on any truncation of the data and provide the first sub-Gaussian high\nprobability bounds for differentially private median and mean estimation, for\npossibly heavy tailed random variables.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 01:29:05 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Brunel", "Victor-Emmanuel", ""], ["Avella-Medina", "Marco", ""]]}, {"id": "2002.08789", "submitter": "Kengne William", "authors": "Mamadou Lamine Diop and William Kengne", "title": "Consistent model selection procedure for general integer-valued time\n  series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the problem of model selection for a general class of\ninteger-valued time series.\n  We propose a penalized criterion based on the Poisson quasi-likelihood of the\nmodel.\n  Under certain regularity conditions, the consistency of the procedure as well\nas the consistency and the asymptotic normality of the Poisson quasi-likelihood\nestimator of the selected model are established.\n  Simulation experiments are conducted for some classical models such as\nPoisson, binary INGARCH and negative binomial model with nonlinear dynamic.\nAlso, an application to a real dataset is provided.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 15:12:10 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Diop", "Mamadou Lamine", ""], ["Kengne", "William", ""]]}, {"id": "2002.08853", "submitter": "Yiming Xu", "authors": "Ruijian Han, Yiming Xu and Kani Chen", "title": "A General Pairwise Comparison Model for Extremely Sparse Networks", "comments": "25 pages, 4 figures, included more numerical simulations, changed\n  some phrasing in the statements and updated citation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference using pairwise comparison data has been an effective\napproach to analyzing complex and sparse networks. In this paper we propose a\ngeneral framework for modeling the mutual interaction in a network, which\nenjoys ample flexibility in terms of parametrization. Within this setup, we\nestablish that the maximum likelihood estimator (MLE) for the latent scores of\nthe subjects is uniformly consistent under a near-minimal condition on network\nsparsity. This condition is sharp in terms of the leading order asymptotics\ndescribing the sparsity. The proof utilizes a novel chaining technique based on\nthe error-induced metric as well as careful counting of comparison graph\nstructures. Our results guarantee that the MLE is a valid estimator for\ninference in large-scale comparison networks where data is asymptotically\ndeficient. Numerical simulations are provided to complement the theoretical\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 16:39:55 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 17:29:24 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Han", "Ruijian", ""], ["Xu", "Yiming", ""], ["Chen", "Kani", ""]]}, {"id": "2002.09225", "submitter": "Krikamol Muandet", "authors": "Krikamol Muandet, Wittawat Jitkrittum, Jonas K\\\"ubler", "title": "Kernel Conditional Moment Test via Maximum Moment Restriction", "comments": "In Proceedings of the 36th Conference on Uncertainty in Artificial\n  Intelligence (UAI2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG econ.EM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new family of specification tests called kernel conditional\nmoment (KCM) tests. Our tests are built on a novel representation of\nconditional moment restrictions in a reproducing kernel Hilbert space (RKHS)\ncalled conditional moment embedding (CMME). After transforming the conditional\nmoment restrictions into a continuum of unconditional counterparts, the test\nstatistic is defined as the maximum moment restriction (MMR) within the unit\nball of the RKHS. We show that the MMR not only fully characterizes the\noriginal conditional moment restrictions, leading to consistency in both\nhypothesis testing and parameter estimation, but also has an analytic\nexpression that is easy to compute as well as closed-form asymptotic\ndistributions. Our empirical studies show that the KCM test has a promising\nfinite-sample performance compared to existing tests.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 10:58:57 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 14:05:18 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2020 18:08:23 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Muandet", "Krikamol", ""], ["Jitkrittum", "Wittawat", ""], ["K\u00fcbler", "Jonas", ""]]}, {"id": "2002.09233", "submitter": "Steffen Lauritzen", "authors": "Carlos Am\\'endola and Claudia Kl\\\"uppelberg and Steffen Lauritzen and\n  Ngoc Tran", "title": "Conditional Independence in Max-linear Bayesian Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by extreme value theory, max-linear Bayesian networks have been\nrecently introduced and studied as an alternative to linear structural equation\nmodels. However, for max-linear systems the classical independence results for\nBayesian networks are far from exhausting valid conditional independence\nstatements. We use tropical linear algebra to derive a compact representation\nof the conditional distribution given a partial observation, and exploit this\nto obtain a complete description of all conditional independence relations. In\nthe context-specific case, where conditional independence is queried relative\nto a specific value of the conditioning variables, we introduce the notion of a\nsource DAG to disclose the valid conditional independence relations. In the\ncontext-free case we characterize conditional independence through a modified\nseparation concept, $\\ast$-separation, combined with a tropical eigenvalue\ncondition. We also introduce the notion of an impact graph which describes how\nextreme events spread deterministically through the network and we give a\ncomplete characterization of such impact graphs. Our analysis opens up several\ninteresting questions concerning conditional independence and tropical\ngeometry.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 11:31:39 GMT"}, {"version": "v2", "created": "Sun, 6 Sep 2020 09:46:09 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Kl\u00fcppelberg", "Claudia", ""], ["Lauritzen", "Steffen", ""], ["Tran", "Ngoc", ""]]}, {"id": "2002.09269", "submitter": "Binh T. Nguyen", "authors": "Tuan-Binh Nguyen, J\\'er\\^ome-Alexis Chevalier, Bertrand Thirion,\n  Sylvain Arlot", "title": "Aggregation of Multiple Knockoffs", "comments": "Accepted to ICML 2020 (Thirty-seventh International Conference on\n  Machine Learning). This version includes both the main text of the conference\n  paper and supplementary materials (as appendices). 35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an extension of the Knockoff Inference procedure, introduced by\nBarber and Candes (2015). This new method, called Aggregation of Multiple\nKnockoffs (AKO), addresses the instability inherent to the random nature of\nKnockoff-based inference. Specifically, AKO improves both the stability and\npower compared with the original Knockoff algorithm while still maintaining\nguarantees for False Discovery Rate control. We provide a new inference\nprocedure, prove its core properties, and demonstrate its benefits in a set of\nexperiments on synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 13:28:40 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 14:26:21 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Nguyen", "Tuan-Binh", ""], ["Chevalier", "J\u00e9r\u00f4me-Alexis", ""], ["Thirion", "Bertrand", ""], ["Arlot", "Sylvain", ""]]}, {"id": "2002.09338", "submitter": "Aude Sportisse", "authors": "Julie Josse (CMAP), Aude Sportisse (LPSM (UMR\\_8001)), Claire Boyer\n  (LPSM UMR 8001, DMA), Aymeric Dieuleveut (CMAP)", "title": "Debiasing Stochastic Gradient Descent to handle missing values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient algorithm is a key ingredient of many machine learning\nmethods, particularly appropriate for large-scale learning.However, a major\ncaveat of large data is their incompleteness.We propose an averaged stochastic\ngradient algorithm handling missing values in linear models. This approach has\nthe merit to be free from the need of any data distribution modeling and to\naccount for heterogeneous missing proportion.In both streaming and\nfinite-sample settings, we prove that this algorithm achieves convergence rate\nof $\\mathcal{O}(\\frac{1}{n})$ at the iteration $n$, the same as without missing\nvalues. We show the convergence behavior and the relevance of the algorithm not\nonly on synthetic data but also on real data sets, including those collected\nfrom medical register.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 14:49:24 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 15:09:18 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Josse", "Julie", "", "CMAP"], ["Sportisse", "Aude", "", "LPSM"], ["Boyer", "Claire", "", "LPSM UMR 8001, DMA"], ["Dieuleveut", "Aymeric", "", "CMAP"]]}, {"id": "2002.09339", "submitter": "Bruno Loureiro", "authors": "Federica Gerace, Bruno Loureiro, Florent Krzakala, Marc M\\'ezard and\n  Lenka Zdeborov\\'a", "title": "Generalisation error in learning with random features and the hidden\n  manifold model", "comments": "v2: ICML 2020 camera-ready", "journal-ref": "International Conference on Machine Learning, ICML 2020", "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study generalised linear regression and classification for a synthetically\ngenerated dataset encompassing different problems of interest, such as learning\nwith random features, neural networks in the lazy training regime, and the\nhidden manifold model. We consider the high-dimensional regime and using the\nreplica method from statistical physics, we provide a closed-form expression\nfor the asymptotic generalisation performance in these problems, valid in both\nthe under- and over-parametrised regimes and for a broad choice of generalised\nlinear model loss functions. In particular, we show how to obtain analytically\nthe so-called double descent behaviour for logistic regression with a peak at\nthe interpolation threshold, we illustrate the superiority of orthogonal\nagainst random Gaussian projections in learning with random features, and\ndiscuss the role played by correlations in the data generated by the hidden\nmanifold model. Beyond the interest in these particular problems, the\ntheoretical formalism introduced in this manuscript provides a path to further\nextensions to more complex tasks.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 14:49:41 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 08:32:53 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Gerace", "Federica", ""], ["Loureiro", "Bruno", ""], ["Krzakala", "Florent", ""], ["M\u00e9zard", "Marc", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "2002.09426", "submitter": "Celeste Mayer", "authors": "Vicky Fasen-Hartmann, Celeste Mayer", "title": "Whittle estimation for stationary state space models with finite second\n  moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the Whittle estimator for the parameters of a\nstationary solution of a continuous-time linear state space model sampled at\nlow frequencies. In our context the driving process is a L\\'evy process which\nallows flexible margins of the underlying model. The L\\'evy process is supposed\nto have finite second moments. It is well known that then the class of\nstationary solutions of linear state space models and the class of multivariate\nCARMA processes coincides. We prove that the Whittle estimator, which is based\non the periodogram, is strongly consistent and asymptotically normally\ndistributed. A comparison with the classical setting of discrete-time ARMA\nmodels shows that in the continuous-time setting the limit covariance matrix of\nthe Whittle estimator has an additional correction term for non-Gaussian\nmodels. For the proof, we investigate as well the asymptotic normality of the\nintegrated periodogram which is interesting for its own. It can be used to\nconstruct goodness of fit tests. Furthermore, for univariate state space\nprocesses, which are CARMA processes, we introduce an adjusted version of the\nWhittle estimator and derive as well the asymptotic properties of this\nestimator. The practical applicability of our estimators is demonstrated\nthrough a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 17:18:28 GMT"}], "update_date": "2020-02-24", "authors_parsed": [["Fasen-Hartmann", "Vicky", ""], ["Mayer", "Celeste", ""]]}, {"id": "2002.09427", "submitter": "Rui Jin", "authors": "Rui Jin and Aixin Tan", "title": "Central limit theorems for Markov chains based on their convergence\n  rates in Wasserstein distance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tools are available to bound the convergence rate of Markov chains in\ntotal variation (TV) distance. Such results can be used to establish central\nlimit theorems (CLT) that enable error evaluations of Monte Carlo estimates in\npractice. However, convergence analysis based on TV distance is often\nnon-scalable to high-dimensional Markov chains (Qin and Hobert (2018);\nRajaratnam and Sparks (2015)). Alternatively, robust bounds in Wasserstein\ndistance are often easier to obtain, thanks to a coupling argument. Our work is\nconcerned with the implication of such convergence results, in particular, do\nthey lead to CLTs of the corresponding Markov chains? One indirect and\ntypically non-trivial way is to first convert Wasserstein bounds into total\nvariation bounds. Alternatively, we provide two CLTs that directly depend on\n(sub-geometric) convergence rates in Wasserstein distance. Our CLTs hold for\nLipschitz functions under certain moment conditions. Finally, we apply these\nCLTs to four sets of Markov chain examples including a class of nonlinear\nautoregressive processes, an exponential integrator version of the metropolis\nadjusted Langevin algorithm (EI-MALA), an unadjusted Langevin algorithm (ULA),\nand a special autoregressive model that generates reducible chains.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 17:18:51 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 21:45:40 GMT"}, {"version": "v3", "created": "Fri, 7 Aug 2020 02:15:33 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Jin", "Rui", ""], ["Tan", "Aixin", ""]]}, {"id": "2002.09484", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen, Partha Dey", "title": "A note on Stein equation for weighted sums of independent $\\chi^{2}$\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note provides the Stein equation for weighted sums of independent\n$\\chi^{2}$ distributions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 15:25:20 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Chen", "Xiaohui", ""], ["Dey", "Partha", ""]]}, {"id": "2002.09516", "submitter": "Yaqi Duan", "authors": "Yaqi Duan, Mengdi Wang", "title": "Minimax-Optimal Off-Policy Evaluation with Linear Function Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the statistical theory of batch data reinforcement\nlearning with function approximation. Consider the off-policy evaluation\nproblem, which is to estimate the cumulative value of a new target policy from\nlogged history generated by unknown behavioral policies. We study a\nregression-based fitted Q iteration method, and show that it is equivalent to a\nmodel-based method that estimates a conditional mean embedding of the\ntransition operator. We prove that this method is information-theoretically\noptimal and has nearly minimal estimation error. In particular, by leveraging\ncontraction property of Markov processes and martingale concentration, we\nestablish a finite-sample instance-dependent error upper bound and a\nnearly-matching minimax lower bound. The policy evaluation error depends\nsharply on a restricted $\\chi^2$-divergence over the function class between the\nlong-term distribution of the target policy and the distribution of past data.\nThis restricted $\\chi^2$-divergence is both instance-dependent and\nfunction-class-dependent. It characterizes the statistical limit of off-policy\nevaluation. Further, we provide an easily computable confidence bound for the\npolicy evaluator, which may be useful for optimistic planning and safe policy\nimprovement.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 19:20:57 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Duan", "Yaqi", ""], ["Wang", "Mengdi", ""]]}, {"id": "2002.09578", "submitter": "Xiaochun Meng", "authors": "Xiaochun Meng, James W. Taylor, Souhaib Ben Taieb, Siran Li", "title": "Scoring Functions for Multivariate Distributions and Level Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in predicting multivariate probability distributions is growing due\nto the increasing availability of rich datasets and computational developments.\nScoring functions enable the comparison of forecast accuracy, and can\npotentially be used for estimation. A scoring function for multivariate\ndistributions that has gained some popularity is the energy score. This is a\ngeneralization of the continuous ranked probability score (CRPS), which is\nwidely used for univariate distributions. A little-known, alternative\ngeneralization is the multivariate CRPS (MCRPS). We propose a theoretical\nframework for scoring functions for multivariate distributions, which\nencompasses the energy score and MCRPS, as well as the quadratic score, which\nhas also received little attention. We demonstrate how this framework can be\nused to generate new scores. For univariate distributions, it is\nwell-established that the CRPS can be expressed as the integral over a quantile\nscore. We show that, in a similar way, scoring functions for multivariate\ndistributions can be \"disintegrated\" to obtain scoring functions for level\nsets. Using this, we present scoring functions for different types of level\nset, including those for densities and cumulative distributions. To compute the\nscoring functions, we propose a simple numerical algorithm. We illustrate our\nproposals using simulated and stock returns data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 23:56:14 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 23:46:22 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 00:43:56 GMT"}, {"version": "v4", "created": "Mon, 8 Jun 2020 15:06:46 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Meng", "Xiaochun", ""], ["Taylor", "James W.", ""], ["Taieb", "Souhaib Ben", ""], ["Li", "Siran", ""]]}, {"id": "2002.09589", "submitter": "Vaishakh Ravindrakumar", "authors": "Yi Hao, Ayush Jain, Alon Orlitsky, Vaishakh Ravindrakumar", "title": "SURF: A Simple, Universal, Robust, Fast Distribution Learning Algorithm", "comments": "27 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sample- and computationally-efficient distribution estimation is a\nfundamental tenet in statistics and machine learning. We present SURF, an\nalgorithm for approximating distributions by piecewise polynomials. SURF is:\nsimple, replacing prior complex optimization techniques by straight-forward\n{empirical probability} approximation of each potential polynomial piece\n{through simple empirical-probability interpolation}, and using plain\ndivide-and-conquer to merge the pieces; universal, as well-known\npolynomial-approximation results imply that it accurately approximates a large\nclass of common distributions; robust to distribution mis-specification as for\nany degree $d \\le 8$, it estimates any distribution to an $\\ell_1$ distance $<\n3$ times that of the nearest degree-$d$ piecewise polynomial, improving known\nfactor upper bounds of 3 for single polynomials and 15 for polynomials with\narbitrarily many pieces; fast, using optimal sample complexity, running in near\nsample-linear time, and if given sorted samples it may be parallelized to run\nin sub-linear time. In experiments, SURF outperforms state-of-the art\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 01:03:33 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 19:10:43 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Hao", "Yi", ""], ["Jain", "Ayush", ""], ["Orlitsky", "Alon", ""], ["Ravindrakumar", "Vaishakh", ""]]}, {"id": "2002.09643", "submitter": "Fan Yang", "authors": "Fan Yang", "title": "Sample canonical correlation coefficients of high-dimensional random\n  vectors: local law and Tracy-Widom limit", "comments": "56 pages", "journal-ref": "Random Matrices: Theory and Applications (2021)", "doi": "10.1142/S2010326322500071", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider two random vectors $\\mathbf C_1^{1/2}\\mathbf x \\in \\mathbb R^p$ and\n$\\mathbf C_2^{1/2}\\mathbf y\\in \\mathbb R^q$, where the entries of $\\mathbf x$\nand $\\mathbf y$ are i.i.d. random variables with mean zero and variance one,\nand $\\mathbf C_1$ and $\\mathbf C_2$ are $p \\times p$ and $q\\times q$\ndeterministic population covariance matrices. With $n$ independent samples of\n$(\\mathbf C_1^{1/2}\\mathbf x,\\mathbf C_2^{1/2}\\mathbf y)$, we study the sample\ncorrelation between these two vectors using canonical correlation analysis. We\ndenote by $S_{xx}$ and $S_{yy}$ the sample covariance matrices for $\\mathbf\nC_1^{1/2}\\mathbf x$ and $\\mathbf C_2^{1/2}\\mathbf y$, respectively, and\n$S_{xy}$ the sample cross-covariance matrix. Then the sample canonical\ncorrelation coefficients are the square roots of the eigenvalues of the sample\ncanonical correlation matrix $\\cal C_{XY}:=S_{xx}^{-1}S_{xy}S_{yy}^{-1}S_{yx}$.\nUnder the high-dimensional setting with ${p}/{n}\\to c_1 \\in (0, 1)$ and\n${q}/{n}\\to c_2 \\in (0, 1-c_1)$ as $n\\to \\infty$, we prove that the largest\neigenvalue of $\\mathcal C_{XY}$ converges to the Tracy-Widom distribution as\nlong as we have $\\lim_{s \\rightarrow \\infty}s^4 [\\mathbb{P}(\\vert x_{ij} \\vert\n\\geq s)+ \\mathbb{P}(\\vert y_{ij} \\vert \\geq s)]=0$. This extends the result in\n[16], which established the Tracy-Widom limit of the largest eigenvalue of\n$\\mathcal C_{XY}$ under the assumption that all moments are finite. Our proof\nis based on a linearization method, which reduces the problem to the study of a\n$(p+q+2n)\\times (p+q+2n)$ random matrix $H$. In particular, we shall prove an\noptimal local law on its inverse $G:=H^{-1}$, i.e the resolvent. This local law\nis the main tool for both the proof of the Tracy-Widom law in this paper, and\nthe study in [22,23] on the canonical correlation coefficients of\nhigh-dimensional random vectors with finite rank correlations.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 06:38:31 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 03:41:14 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Yang", "Fan", ""]]}, {"id": "2002.09735", "submitter": "Will Wei Sun", "authors": "Jie Zhou and Will Wei Sun and Jingfei Zhang and Lexin Li", "title": "Partially Observed Dynamic Tensor Response Regression", "comments": "Improved lower bound on observation probability (Assumptions 2,6);\n  Improved sample complexity conditions (Assumptions 5,10); Improved final\n  statistical error rate in Theorems 1-2; add a new initialization section;\n  extend to sub-Gaussian error tensor", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern data science, dynamic tensor data is prevailing in numerous\napplications. An important task is to characterize the relationship between\nsuch dynamic tensor and external covariates. However, the tensor data is often\nonly partially observed, rendering many existing methods inapplicable. In this\narticle, we develop a regression model with partially observed dynamic tensor\nas the response and external covariates as the predictor. We introduce the\nlow-rank, sparsity and fusion structures on the regression coefficient tensor,\nand consider a loss function projected over the observed entries. We develop an\nefficient non-convex alternating updating algorithm, and derive the\nfinite-sample error bound of the actual estimator from each step of our\noptimization algorithm. Unobserved entries in tensor response have imposed\nserious challenges. As a result, our proposal differs considerably in terms of\nestimation algorithm, regularity conditions, as well as theoretical properties,\ncompared to the existing tensor completion or tensor response regression\nsolutions. We illustrate the efficacy of our proposed method using simulations,\nand two real applications, a neuroimaging dementia study and a digital\nadvertising study.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 17:14:10 GMT"}, {"version": "v2", "created": "Sat, 28 Mar 2020 22:09:53 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 23:30:56 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhou", "Jie", ""], ["Sun", "Will Wei", ""], ["Zhang", "Jingfei", ""], ["Li", "Lexin", ""]]}, {"id": "2002.09753", "submitter": "Farzad Sabzikar", "authors": "Farzad Sabzikar, Kris De Brabanter", "title": "Asymptotic theory for regression models with fractional local to unity\n  root errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops the asymptotic theory for parametric and nonparametric\nregression models when the errors have a fractional local to unity root (FLUR)\nmodel structure. FLUR models are stationary time series with semi-long range\ndependence property in the sense that their covariance function resembles that\nof a long memory model for moderate lags but eventually diminishes\nexponentially fast according to the presence of a decay factor governed by a\nnoncentrality parameter. When this parameter is sample size dependent, the\nasymptotic normality for these regression models admit a wide range of\nstochastic processes with behavior that includes long, semi-long, and short\nmemory processes.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 19:22:03 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Sabzikar", "Farzad", ""], ["De Brabanter", "Kris", ""]]}, {"id": "2002.09764", "submitter": "Johannes Krebs", "authors": "Johannes Krebs", "title": "On the law of the iterated logarithm and strong invariance principles in\n  stochastic geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the law of the iterated logarithm (Khinchin (1924), Kolmogorov\n(1929)) and related strong invariance principles in stochastic geometry. As\npotential applications, we think of well-known functionals such as functionals\ndefined on the $k$-nearest neighbors graph and important functionals in\ntopological data analysis such as the Euler characteristic and persistent Betti\nnumbers.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 20:04:06 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 10:09:59 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2020 13:18:41 GMT"}, {"version": "v4", "created": "Fri, 25 Sep 2020 20:06:39 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Krebs", "Johannes", ""]]}, {"id": "2002.09810", "submitter": "Igor Silin", "authors": "Igor Silin and Jianqing Fan", "title": "Hypothesis testing for eigenspaces of covariance matrix", "comments": "78 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eigenspaces of covariance matrices play an important role in statistical\nmachine learning, arising in variety of modern algorithms. Quantitatively, it\nis convenient to describe the eigenspaces in terms of spectral projectors. This\nwork focuses on hypothesis testing for the spectral projectors, both in one-\nand two-sample scenario. We present new tests, based on a specific matrix norm\ndeveloped in order to utilize the structure of the spectral projectors. A new\nresampling technique of independent interest is introduced and analyzed: it\nserves as an alternative to the well-known multiplier bootstrap, significantly\nreducing computational complexity of bootstrap-based methods. We provide\ntheoretical guarantees for the type-I error of our procedures, which remarkably\nimprove the previously obtained results in the field. Moreover, we analyze\npower of our tests. Numerical experiments illustrate good performance of the\nproposed methods compared to previously developed ones.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 02:52:00 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Silin", "Igor", ""], ["Fan", "Jianqing", ""]]}, {"id": "2002.09968", "submitter": "Simone Giannerini", "authors": "Kung-Sik Chan, Simone Giannerini, Greta Goracci, Howell Tong", "title": "Unit-root test within a threshold ARMA framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new unit-root test based on Lagrange Multipliers, where we\nextend the null hypothesis to an integrated moving-average process (IMA(1,1))\nand the alternative to a first-order threshold autoregressive moving-average\nprocess (TARMA(1,1)). This new theoretical framework provides tests with good\nsize without pre-modelling steps. Moreover, leveraging on the versatile\ncapability of the TARMA(1,1), our test has power against a wide range of linear\nand nonlinear alternatives. We prove the consistency and asymptotic similarity\nof the test. The proof of tightness of the test is of independent and general\ntheoretical interest. Moreover, we propose a wild bootstrap version of the\nstatistic. Our proposals outperform most existing tests in many contexts. We\nsupport the view that rejection does not necessarily imply nonlinearity so that\nunit-root tests should not be used uncritically to select a model. Finally, we\npresent an application to real exchange rates.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 18:54:51 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Chan", "Kung-Sik", ""], ["Giannerini", "Simone", ""], ["Goracci", "Greta", ""], ["Tong", "Howell", ""]]}, {"id": "2002.09976", "submitter": "Donniell Fishkind", "authors": "Donniell E. Fishkind, Avanti Athreya, Lingyao Meng, Vince Lyzinski,\n  Carey E. Priebe", "title": "On a complete and sufficient statistic for the correlated Bernoulli\n  random graph model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Inference on vertex-aligned graphs is of wide theoretical and practical\nimportance.There are, however, few flexible and tractable statistical models\nfor correlated graphs, and even fewer comprehensive approaches to parametric\ninference on data arising from such graphs. In this paper, we consider the\ncorrelated Bernoulli random graph model (allowing different Bernoulli\ncoefficients and edge correlations for different pairs of vertices), and we\nintroduce a new variance-reducing technique -- called \\emph{balancing} -- that\ncan refine estimators for model parameters. Specifically, we construct a\ndisagreement statistic and show that it is complete and sufficient; balancing\ncan be interpreted as Rao-Blackwellization with this disagreement statistic. We\nshow that for unbiased estimators of functions of model parameters, balancing\ngenerates uniformly minimum variance unbiased estimators (UMVUEs). However,\neven when unbiased estimators for model parameters do {\\em not} exist -- which,\nas we prove, is the case with both the heterogeneity correlation and the total\ncorrelation parameters -- balancing is still useful, and lowers mean squared\nerror. In particular, we demonstrate how balancing can improve the efficiency\nof the alignment strength estimator for the total correlation, a parameter that\nplays a critical role in graph matchability and graph matching runtime\ncomplexity.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 19:37:22 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 18:48:16 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Fishkind", "Donniell E.", ""], ["Athreya", "Avanti", ""], ["Meng", "Lingyao", ""], ["Lyzinski", "Vince", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2002.10008", "submitter": "Stefano Vigogna", "authors": "Alessandro Lanteri, Mauro Maggioni and Stefano Vigogna", "title": "Conditional regression for single-index models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The single-index model is a statistical model for intrinsic regression where\nresponses are assumed to depend on a single yet unknown linear combination of\nthe predictors, allowing to express the regression function as $ \\mathbb{E} [ Y\n| X ] = f ( \\langle v , X \\rangle ) $ for some unknown \\emph{index} vector $v$\nand \\emph{link} function $f$. Conditional methods provide a simple and\neffective approach to estimate $v$ by averaging moments of $X$ conditioned on\n$Y$, but depend on parameters whose optimal choice is unknown and do not\nprovide generalization bounds on $f$. In this paper we propose a new\nconditional method converging at $\\sqrt{n}$ rate under an explicit parameter\ncharacterization. Moreover, we prove that polynomial partitioning estimates\nachieve the $1$-dimensional min-max rate for regression of H\\\"older functions\nwhen combined to any $\\sqrt{n}$-convergent index estimator. Overall this yields\nan estimator for dimension reduction and regression of single-index models that\nattains statistical optimality in quasilinear time.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 22:52:49 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2020 17:34:40 GMT"}], "update_date": "2020-12-23", "authors_parsed": [["Lanteri", "Alessandro", ""], ["Maggioni", "Mauro", ""], ["Vigogna", "Stefano", ""]]}, {"id": "2002.10046", "submitter": "Anderson Winkler", "authors": "Anderson M. Winkler, Olivier Renaud, Stephen M. Smith, Thomas E.\n  Nichols", "title": "Permutation Inference for Canonical Correlation Analysis", "comments": "49 pages, 2 figures, 10 tables, 3 algorithms, 119 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) has become a key tool for population\nneuroimaging, allowing investigation of associations between many imaging and\nnon-imaging measurements. As other variables are often a source of variability\nnot of direct interest, previous work has used CCA on residuals from a model\nthat removes these effects, then proceeded directly to permutation inference.\nWe show that such a simple permutation test leads to inflated error rates. The\nreason is that residualisation introduces dependencies among the observations\nthat violate the exchangeability assumption. Even in the absence of nuisance\nvariables, however, a simple permutation test for CCA also leads to excess\nerror rates for all canonical correlations other than the first. The reason is\nthat a simple permutation scheme does not ignore the variability already\nexplained by previous canonical variables. Here we propose solutions for both\nproblems: in the case of nuisance variables, we show that transforming the\nresiduals to a lower dimensional basis where exchangeability holds results in a\nvalid permutation test; for more general cases, with or without nuisance\nvariables, we propose estimating the canonical correlations in a stepwise\nmanner, removing at each iteration the variance already explained, while\ndealing with different number of variables in both sides. We also discuss how\nto address the multiplicity of tests, proposing an admissible test that is not\nconservative, and provide a complete algorithm for permutation inference for\nCCA.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 02:47:01 GMT"}, {"version": "v2", "created": "Sat, 14 Mar 2020 22:45:59 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 18:23:36 GMT"}, {"version": "v4", "created": "Thu, 18 Jun 2020 01:15:58 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Winkler", "Anderson M.", ""], ["Renaud", "Olivier", ""], ["Smith", "Stephen M.", ""], ["Nichols", "Thomas E.", ""]]}, {"id": "2002.10164", "submitter": "Arnaud Gloter", "authors": "Arnaud Gloter (LaMME), Nakahiro Yoshida", "title": "Adaptive and non-adaptive estimation for degenerate diffusion processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss parametric estimation of a degenerate diffusion system from\ntime-discrete observations. The first component of the degenerate diffusion\nsystem has a parameter $\\theta_1$ in a non-degenerate diffusion coefficient and\na parameter $\\theta_2$ in the drift term. The second component has a drift term\nparameterized by $\\theta_3$ and no diffusion term. Asymptotic normality is\nproved in three different situations for an adaptive estimator for $\\theta_3$\nwith some initial estimators for ($\\theta_1$ , $\\theta_2$), an adaptive\none-step estimator for ($\\theta_1$ , $\\theta_2$ , $\\theta_3$) with some initial\nestimators for them, and a joint quasi-maximum likelihood estimator for\n($\\theta_1$ , $\\theta_2$ , $\\theta_3$) without any initial estimator. Our\nestimators incorporate information of the increments of both components. Thanks\nto this construction, the asymptotic variance of the estimators for $\\theta_1$\nis smaller than the standard one based only on the first component. The\nconvergence of the estimators for $\\theta_3$ is much faster than the other\nparameters. The resulting asymptotic variance is smaller than that of an\nestimator only using the increments of the second component.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 10:58:34 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Gloter", "Arnaud", "", "LaMME"], ["Yoshida", "Nakahiro", ""]]}, {"id": "2002.10178", "submitter": "Sara Kristin Schmidt", "authors": "Sara Kristin Schmidt, Max Wornowizki, Roland Fried and Herold Dehling", "title": "An Asymptotic Test for Constancy of the Variance under Short-Range\n  Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to test for heteroscedasticity of a\nnon-stationary time series that is based on Gini's mean difference of\nlogarithmic local sample variances. In order to analyse the large sample\nbehaviour of our test statistic, we establish new limit theorems for\nU-statistics of dependent triangular arrays. We derive the asymptotic\ndistribution of the test statistic under the null hypothesis of a constant\nvariance and show that the test is consistent against a large class of\nalternatives, including multiple structural breaks in the variance. Our test is\napplicable even in the case of non-stationary processes, assuming a locally\nstationary mean function. The performance of the test and its comparatively low\ncomputation time are illustrated in an extensive simulation study. As an\napplication, we analyse Google Trends data, monitoring the relative search\ninterest for the topic \"global warming.\"\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 11:48:24 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 16:23:17 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Schmidt", "Sara Kristin", ""], ["Wornowizki", "Max", ""], ["Fried", "Roland", ""], ["Dehling", "Herold", ""]]}, {"id": "2002.10208", "submitter": "Abhishake Rastogi", "authors": "Abhishake Rastogi and Peter Math\\'e", "title": "Inverse learning in Hilbert scales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the linear ill-posed inverse problem with noisy data in the\nstatistical learning setting. Approximate reconstructions from random noisy\ndata are sought with general regularization schemes in Hilbert scale. We\ndiscuss the rates of convergence for the regularized solution under the prior\nassumptions and a certain link condition. We express the error in terms of\ncertain distance functions. For regression functions with smoothness given in\nterms of source conditions the error bound can then be explicitly established.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 12:49:54 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Rastogi", "Abhishake", ""], ["Math\u00e9", "Peter", ""]]}, {"id": "2002.10271", "submitter": "Wittawat Jitkrittum", "authors": "Wittawat Jitkrittum, Heishiro Kanagawa, Bernhard Sch\\\"olkopf", "title": "Testing Goodness of Fit of Conditional Density Models with Kernels", "comments": "In UAI 2020. http://auai.org/uai2020/accepted.php", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two nonparametric statistical tests of goodness of fit for\nconditional distributions: given a conditional probability density function\n$p(y|x)$ and a joint sample, decide whether the sample is drawn from\n$p(y|x)r_x(x)$ for some density $r_x$. Our tests, formulated with a Stein\noperator, can be applied to any differentiable conditional density model, and\nrequire no knowledge of the normalizing constant. We show that 1) our tests are\nconsistent against any fixed alternative conditional model; 2) the statistics\ncan be estimated easily, requiring no density estimation as an intermediate\nstep; and 3) our second test offers an interpretable test result providing\ninsight on where the conditional model does not fit well in the domain of the\ncovariate. We demonstrate the interpretability of our test on a task of\nmodeling the distribution of New York City's taxi drop-off location given a\npick-up point. To our knowledge, our work is the first to propose such\nconditional goodness-of-fit tests that simultaneously have all these desirable\nproperties.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 14:04:37 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 15:27:09 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Jitkrittum", "Wittawat", ""], ["Kanagawa", "Heishiro", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "2002.10412", "submitter": "Valentin Patilea", "authors": "Laurent Bordes, Maria Carmen Pardo, Christian Paroissin, Valentin\n  Patilea", "title": "Modified Cox regression with current status data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In survival analysis, the lifetime under study is not always observed. In\ncertain applications, for some individuals, the value of the lifetime is only\nknown to be smaller or larger than some random duration. This framework\nrepresent an extension of standard situations where the lifetime is only left\nor only right randomly censored. We consider the case where the independent\nobservation units include also some covariates, and we propose two\nsemiparametric regression models. The new models extend the standard Cox\nproportional hazard model to the situation of a more complex censoring\nmechanism. However, like in Cox's model, in both models the nonparametric\nbaseline hazard function still could be expressed as an explicit functional of\nthe distribution of the observations. This allows to define the estimator of\nthe finite-dimensional parameters as the maximum of a likelihood-type criterion\nwhich is an explicit function of the data. Given an estimate of the\nfinite-dimensional parameter, the estimation of the baseline cumulative hazard\nfunction is straightforward.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 17:57:38 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Bordes", "Laurent", ""], ["Pardo", "Maria Carmen", ""], ["Paroissin", "Christian", ""], ["Patilea", "Valentin", ""]]}, {"id": "2002.10519", "submitter": "Erin Gabriel", "authors": "Erin E. Gabriel and Michael C. Sachs and Arvid Sj\\\"olander", "title": "Causal bounds for outcome-dependent sampling in observational studies", "comments": "36 pages, 3 figures. Update to include revisions after peer review.\n  In Press at the Journal of the American Statistical Association, Theory and\n  methods", "journal-ref": null, "doi": "10.1080/01621459.2020.1832502", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outcome-dependent sampling designs are common in many different scientific\nfields including epidemiology, ecology, and economics. As with all\nobservational studies, such designs often suffer from unmeasured confounding,\nwhich generally precludes the nonparametric identification of causal effects.\nNonparametric bounds can provide a way to narrow the range of possible values\nfor a nonidentifiable causal effect without making additional untestable\nassumptions. The nonparametric bounds literature has almost exclusively focused\non settings with random sampling, and the bounds have often been derived with a\nparticular linear programming method. We derive novel bounds for the causal\nrisk difference, often referred to as the average treatment effect, in six\nsettings with outcome-dependent sampling and unmeasured confounding for a\nbinary outcome and exposure. Our derivations of the bounds illustrate two\napproaches that may be applicable in other settings where the bounding problem\ncannot be directly stated as a system of linear constraints. We illustrate our\nderived bounds in a real data example involving the effect of vitamin D\nconcentration on mortality.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 20:21:28 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 10:38:11 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Gabriel", "Erin E.", ""], ["Sachs", "Michael C.", ""], ["Sj\u00f6lander", "Arvid", ""]]}, {"id": "2002.10526", "submitter": "Michael Mahoney", "authors": "Ping Ma, Xinlian Zhang, Xin Xing, Jingyi Ma, and Michael W. Mahoney", "title": "Asymptotic Analysis of Sampling Estimators for Randomized Numerical\n  Linear Algebra Algorithms", "comments": "33 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of Randomized Numerical Linear Algebra (RandNLA)\nalgorithms within the past few years has mostly focused on their performance as\npoint estimators. However, this is insufficient for conducting statistical\ninference, e.g., constructing confidence intervals and hypothesis testing,\nsince the distribution of the estimator is lacking. In this article, we develop\nan asymptotic analysis to derive the distribution of RandNLA sampling\nestimators for the least-squares problem. In particular, we derive the\nasymptotic distribution of a general sampling estimator with arbitrary sampling\nprobabilities. The analysis is conducted in two complementary settings, i.e.,\nwhen the objective of interest is to approximate the full sample estimator or\nis to infer the underlying ground truth model parameters. For each setting, we\nshow that the sampling estimator is asymptotically normally distributed under\nmild regularity conditions. Moreover, the sampling estimator is asymptotically\nunbiased in both settings. Based on our asymptotic analysis, we use two\ncriteria, the Asymptotic Mean Squared Error (AMSE) and the Expected Asymptotic\nMean Squared Error (EAMSE), to identify optimal sampling probabilities. Several\nof these optimal sampling probability distributions are new to the literature,\ne.g., the root leverage sampling estimator and the predictor length sampling\nestimator. Our theoretical results clarify the role of leverage in the sampling\nprocess, and our empirical results demonstrate improvements over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Feb 2020 20:34:50 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Ma", "Ping", ""], ["Zhang", "Xinlian", ""], ["Xing", "Xin", ""], ["Ma", "Jingyi", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2002.10846", "submitter": "Dan Mikulincer", "authors": "Dan Mikulincer", "title": "A CLT in Stein's distance for generalized Wishart matrices and higher\n  order tensors", "comments": "22 pages. Added a section about non-homogenous sums and correlated\n  Wishart matrices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the central limit theorem for sums of independent tensor powers,\n$\\frac{1}{\\sqrt{d}}\\sum\\limits_{i=1}^d X_i^{\\otimes p}$. We focus on the\nhigh-dimensional regime where $X_i \\in \\mathbb{R}^n$ and $n$ may scale with\n$d$. Our main result is a proposed threshold for convergence. Specifically, we\nshow that, under some regularity assumption, if $n^{2p-1}\\gg d$, then the\nnormalized sum converges to a Gaussian. The results apply, among others, to\nsymmetric uniform log-concave measures and to product measures. This\ngeneralizes several results found in the literature.\n  Our main technique is a novel application of optimal transport to Stein's\nmethod which accounts for the low dimensional structure which is inherent in\n$X_i^{\\otimes p}$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 13:19:20 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 06:52:20 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Mikulincer", "Dan", ""]]}, {"id": "2002.10850", "submitter": "Oleg Lepski", "authors": "Lepski O.V. and Rebelles G", "title": "Structural adaptation in the density model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with non-parametric density estimation on $\\bR^2$ from i.i.d\nobservations. It is assumed that after unknown rotation of the coordinate\nsystem the coordinates of the observations are independent random variables\nwhose densities belong to a H\\\"older class with unknown parameters. The minimax\nand adaptive minimax theories for this structural statistical model are\ndeveloped.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 13:26:06 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["V.", "Lepski O.", ""], ["G", "Rebelles", ""]]}, {"id": "2002.11099", "submitter": "Ayush Jain", "authors": "Ayush Jain and Alon Orlitsky", "title": "A General Method for Robust Learning from Batches", "comments": "First Draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, data is collected in batches, some of which are corrupt\nor even adversarial. Recent work derived optimal robust algorithms for\nestimating discrete distributions in this setting. We consider a general\nframework of robust learning from batches, and determine the limits of both\nclassification and distribution estimation over arbitrary, including\ncontinuous, domains. Building on these results, we derive the first robust\nagnostic computationally-efficient learning algorithms for piecewise-interval\nclassification, and for piecewise-polynomial, monotone, log-concave, and\ngaussian-mixture distribution estimation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 18:53:25 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Jain", "Ayush", ""], ["Orlitsky", "Alon", ""]]}, {"id": "2002.11239", "submitter": "Sidney Resnick", "authors": "Ross A. Maller and Sidney I. Resnick", "title": "Extremes of Censored and Uncensored Lifetimes in Survival Data", "comments": "1 figure, 23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The i.i.d. censoring model for survival analysis assumes two independent\nsequences of i.i.d. positive random variables, $(T_i^*)_{1\\le i\\le n}$ and\n$(U_i)_{1\\le i\\le n}$. The data consists of observations on the random sequence\n$\\big(T_i=\\min(T_i^*,U_i)$ together with accompanying censor indicators. Values\nof $T_i$ with $T_i^*\\le U_i$ are said to be uncensored, those with $T_i^*> U_i$\nare censored. We assume that the distributions of the $T_i^*$ and $U_i$ are in\nthe domain of attraction of the Gumbel distribution and obtain the asymptotic\ndistributions, as sample size $n\\to\\infty$, of the maximum values of the\ncensored and uncensored lifetimes in the data, and of statistics related to\nthem. These enable us to examine questions concerning the possible existence of\ncured individuals in the population.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 00:51:35 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Maller", "Ross A.", ""], ["Resnick", "Sidney I.", ""]]}, {"id": "2002.11255", "submitter": "Rungang Han", "authors": "Rungang Han, Rebecca Willett and Anru R. Zhang", "title": "An Optimal Statistical and Computational Framework for Generalized\n  Tensor Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes a flexible framework for generalized low-rank tensor\nestimation problems that includes many important instances arising from\napplications in computational imaging, genomics, and network analysis. The\nproposed estimator consists of finding a low-rank tensor fit to the data under\ngeneralized parametric models. To overcome the difficulty of non-convexity in\nthese problems, we introduce a unified approach of projected gradient descent\nthat adapts to the underlying low-rank structure. Under mild conditions on the\nloss function, we establish both an upper bound on statistical error and the\nlinear rate of computational convergence through a general deterministic\nanalysis. Then we further consider a suite of generalized tensor estimation\nproblems, including sub-Gaussian tensor PCA, tensor regression, and Poisson and\nbinomial tensor PCA. We prove that the proposed algorithm achieves the minimax\noptimal rate of convergence in estimation error. Finally, we demonstrate the\nsuperiority of the proposed framework via extensive experiments on both\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 01:54:35 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 21:55:11 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Han", "Rungang", ""], ["Willett", "Rebecca", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2002.11259", "submitter": "Tae Yoon Lee", "authors": "Tae Yoon Lee, James V. Zidek", "title": "Scientific versus statistical modelling: a unifying approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses two fundamental features of quantities modeled and\nanalysed in statistical science, their dimensions (e.g. time) and measurement\nscales (units). Examples show that subtle issues can arise when dimensions and\nmeasurement scales are ignored. Special difficulties arise when the models\ninvolve transcendental functions. A transcendental function important in\nstatistics is the logarithm which is used in likelihood calculations and is a\nsingularity in the family of Box-Cox algebraic functions. Yet neither the\nargument of the logarithm nor its value can have units of measurement. Physical\nscientists have long recognized that dimension/scale difficulties can be\nside-stepped by nondimensionalizing the model; after all, models of natural\nphenomena cannot depend on the units by which they are measured, and the\ncelebrated Buckingham Pi theorem is a consequence. The paper reviews that\ntheorem, recognizing that the statistical invariance principle arose with\nsimilar aspirations. However, the potential relationship between the theorem\nand statistical invariance has not been investigated until very recently. The\nmain result of the paper is an exploration of that link, which leads to an\nextension of the Pi-theorem that puts it in a stochastic framework and thus\nquantifies uncertainties in deterministic physical models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 02:09:12 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Lee", "Tae Yoon", ""], ["Zidek", "James V.", ""]]}, {"id": "2002.11276", "submitter": "Guillaume Martinet", "authors": "Guillaume Martinet", "title": "A Balancing Weight Framework for Estimating the Causal Effect of General\n  Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, weighting methods that directly optimize the\nbalance between treatment and covariates have received much attention lately;\nhowever these have mainly focused on binary treatments. Inspired by domain\nadaptation, we show that such methods can be actually reformulated as specific\nimplementations of a discrepancy minimization problem aimed at tackling a shift\nof distribution from observational to interventional data. More precisely, we\nintroduce a new framework, Covariate Balance via Discrepancy Minimization\n(CBDM), that provably encompasses most of the existing balancing weight methods\nand formally extends them to treatments of arbitrary types (e.g., continuous or\nmultivariate). We establish theoretical guarantees for our framework that both\noffer generalizations of properties known when the treatment is binary, and\ngive a better grasp on what hyperparameters to choose in non-binary settings.\nBased on such insights, we propose a particular implementation of CBDM for\nestimating dose-response curves and demonstrate through experiments its\ncompetitive performance relative to other existing approaches for continuous\ntreatments.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 03:24:20 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Martinet", "Guillaume", ""]]}, {"id": "2002.11332", "submitter": "Vidyashankar Sivakumar", "authors": "Vidyashankar Sivakumar, Zhiwei Steven Wu, Arindam Banerjee", "title": "Structured Linear Contextual Bandits: A Sharp and Geometric Smoothed\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandit learning algorithms typically involve the balance of exploration and\nexploitation. However, in many practical applications, worst-case scenarios\nneeding systematic exploration are seldom encountered. In this work, we\nconsider a smoothed setting for structured linear contextual bandits where the\nadversarial contexts are perturbed by Gaussian noise and the unknown parameter\n$\\theta^*$ has structure, e.g., sparsity, group sparsity, low rank, etc. We\npropose simple greedy algorithms for both the single- and multi-parameter\n(i.e., different parameter for each context) settings and provide a unified\nregret analysis for $\\theta^*$ with any assumed structure. The regret bounds\nare expressed in terms of geometric quantities such as Gaussian widths\nassociated with the structure of $\\theta^*$. We also obtain sharper regret\nbounds compared to earlier work for the unstructured $\\theta^*$ setting as a\nconsequence of our improved analysis. We show there is implicit exploration in\nthe smoothed setting where a simple greedy algorithm works.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 07:29:24 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Sivakumar", "Vidyashankar", ""], ["Wu", "Zhiwei Steven", ""], ["Banerjee", "Arindam", ""]]}, {"id": "2002.11457", "submitter": "Cl\\'ement Canonne", "authors": "Cl\\'ement L. Canonne", "title": "A short note on learning discrete distributions", "comments": "This is a review article; its intent is not to provide new results,\n  but instead to gather known (and useful) ones, along with their proofs, in a\n  single convenient location", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The goal of this short note is to provide simple proofs for the \"folklore\nfacts\" on the sample complexity of learning a discrete probability distribution\nover a known domain of size $k$ to various distances $\\varepsilon$, with error\nprobability $\\delta$.\n", "versions": [{"version": "v1", "created": "Tue, 25 Feb 2020 06:14:59 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 23:08:13 GMT"}, {"version": "v3", "created": "Wed, 8 Apr 2020 22:39:42 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Canonne", "Cl\u00e9ment L.", ""]]}, {"id": "2002.11475", "submitter": "Bertrand Iooss", "authors": "Alejandro Ribes (EDF R&D PERICLES), Joachim Pouderoux, Bertrand Iooss\n  (EDF R&D PRISME, GdR MASCOT-NUM, IMT)", "title": "A Visual Sensitivity Analysis for Parameter-Augmented Ensembles of\n  Curves", "comments": null, "journal-ref": "The Journal of Verification, Validation and Uncertainty\n  Quantification (VVUQ), 2019, 4 (4)", "doi": "10.1115/1.4046020", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineers and computational scientists often study the behavior of their\nsimulations by repeated solutions with variations in their parameters, which\ncan be for instance boundary values or initial conditions. Through such\nsimulation ensembles, uncertainty in a solution is studied as a function of the\nvarious input parameters. Solutions of numerical simulations are often temporal\nfunctions, spatial maps or spatio-temporal outputs. The usual way to deal with\nsuch complex outputs is to limit the analysis to several probes in the\ntemporal/spatial domain. This leads to smaller and more tractable ensembles of\nfunctional outputs (curves) with their associated input parameters: augmented\nensembles of curves. This article describes a system for the interactive\nexploration and analysis of such augmented ensembles. Descriptive statistics on\nthe functional outputs are performed by Principal Component Analysis\nprojection, kernel density estimation and the computation of High Density\nRegions. This makes possible the calculation of functional quantiles and\noutliers. Brushing and linking the elements of the system allows in-depth\nanalysis of the ensemble. The system allows for functional descriptive\nstatistics, cluster detection and finally for the realization of a visual\nsensitivity analysis via cobweb plots. We present two synthetic examples and\nthen validate our approach in an industrial use-case concerning a marine\ncurrent study using a hydraulic solver.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 13:44:40 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Ribes", "Alejandro", "", "EDF R&D PERICLES"], ["Pouderoux", "Joachim", "", "EDF R&D PRISME, GdR MASCOT-NUM, IMT"], ["Iooss", "Bertrand", "", "EDF R&D PRISME, GdR MASCOT-NUM, IMT"]]}, {"id": "2002.11544", "submitter": "Francesca Mignacco", "authors": "Francesca Mignacco, Florent Krzakala, Yue M. Lu and Lenka Zdeborov\\'a", "title": "The role of regularization in classification of high-dimensional noisy\n  Gaussian mixture", "comments": "8 pages + appendix, 6 figures", "journal-ref": "International Conference on Machine Learning, ICML 2020", "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a high-dimensional mixture of two Gaussians in the noisy regime\nwhere even an oracle knowing the centers of the clusters misclassifies a small\nbut finite fraction of the points. We provide a rigorous analysis of the\ngeneralization error of regularized convex classifiers, including ridge, hinge\nand logistic regression, in the high-dimensional limit where the number $n$ of\nsamples and their dimension $d$ go to infinity while their ratio is fixed to\n$\\alpha= n/d$. We discuss surprising effects of the regularization that in some\ncases allows to reach the Bayes-optimal performances. We also illustrate the\ninterpolation peak at low regularization, and analyze the role of the\nrespective sizes of the two clusters.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 14:54:28 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Mignacco", "Francesca", ""], ["Krzakala", "Florent", ""], ["Lu", "Yue M.", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "2002.11553", "submitter": "Guillaume Maillard", "authors": "Guillaume Maillard (CELESTE, LM-Orsay)", "title": "Aggregated hold out for sparse linear regression with a robust loss\n  function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse linear regression methods generally have a free hyperparameter which\ncontrols the amount of sparsity, and is subject to a bias-variance tradeoff.\nThis article considers the use of Aggregated hold-out to aggregate over values\nof this hyperparameter, in the context of linear regression with the Huber loss\nfunction. Aggregated hold-out (Agghoo) is a procedure which averages estimators\nselected by hold-out (cross-validation with a single split). In the theoretical\npart of the article, it is proved that Agghoo satisfies a non-asymptotic oracle\ninequality when it is applied to sparse estimators which are parametrized by\ntheir zero-norm. In particular , this includes a variant of the Lasso\nintroduced by Zou, Hasti{\\'e} and Tibshirani. Simulations are used to compare\nAgghoo with cross-validation. They show that Agghoo performs better than CV\nwhen the intrinsic dimension is high and when there are confounders correlated\nwith the predictive covariates.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 15:08:37 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Maillard", "Guillaume", "", "CELESTE, LM-Orsay"]]}, {"id": "2002.11665", "submitter": "Yi Hao", "authors": "Yi Hao, Alon Orlitsky", "title": "Profile Entropy: A Fundamental Measure for the Learnability and\n  Compressibility of Discrete Distributions", "comments": "56 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The profile of a sample is the multiset of its symbol frequencies. We show\nthat for samples of discrete distributions, profile entropy is a fundamental\nmeasure unifying the concepts of estimation, inference, and compression.\nSpecifically, profile entropy a) determines the speed of estimating the\ndistribution relative to the best natural estimator; b) characterizes the rate\nof inferring all symmetric properties compared with the best estimator over any\nlabel-invariant distribution collection; c) serves as the limit of profile\ncompression, for which we derive optimal near-linear-time block and sequential\nalgorithms. To further our understanding of profile entropy, we investigate its\nattributes, provide algorithms for approximating its value, and determine its\nmagnitude for numerous structural distribution families.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 17:49:04 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Hao", "Yi", ""], ["Orlitsky", "Alon", ""]]}, {"id": "2002.11809", "submitter": "Joona Karjalainen", "authors": "Mindaugas Bloznelis, Joona Karjalainen, Lasse Leskel\\\"a", "title": "Assortativity and bidegree distributions on Bernoulli random graph\n  superpositions", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A probabilistic generative network model with $n$ nodes and $m$ overlapping\nlayers is obtained as a superposition of $m$ mutually independent Bernoulli\nrandom graphs of varying size and strength. When $n$ and $m$ are large and of\nthe same order of magnitude, the model admits a sparse limiting regime with a\ntunable power-law degree distribution and nonvanishing clustering coefficient.\nIn this article we prove an asymptotic formula for the joint degree\ndistribution of adjacent nodes. This yields a simple analytical formula for the\nmodel assortativity, and opens up ways to analyze rank correlation coefficients\nsuitable for random graphs with heavy-tailed degree distributions. We also\nstudy the effects of power laws on the asymptotic joint degree distributions.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 21:53:06 GMT"}, {"version": "v2", "created": "Sun, 24 Jan 2021 18:09:20 GMT"}, {"version": "v3", "created": "Mon, 31 May 2021 08:50:16 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Bloznelis", "Mindaugas", ""], ["Karjalainen", "Joona", ""], ["Leskel\u00e4", "Lasse", ""]]}, {"id": "2002.11815", "submitter": "Yuexi Wang", "authors": "Yuexi Wang, Veronika Ro\\v{c}kov\\'a", "title": "Uncertainty Quantification for Sparse Deep Learning", "comments": null, "journal-ref": "Proceedings of the Twenty Third International Conference on\n  Artificial Intelligence and Statistics, PMLR 108:298-308, 2020", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods continue to have a decided impact on machine learning,\nboth in theory and in practice. Statistical theoretical developments have been\nmostly concerned with approximability or rates of estimation when recovering\ninfinite dimensional objects (curves or densities). Despite the impressive\narray of available theoretical results, the literature has been largely silent\nabout uncertainty quantification for deep learning. This paper takes a step\nforward in this important direction by taking a Bayesian point of view. We\nstudy Gaussian approximability of certain aspects of posterior distributions of\nsparse deep ReLU architectures in non-parametric regression. Building on tools\nfrom Bayesian non-parametrics, we provide semi-parametric Bernstein-von Mises\ntheorems for linear and quadratic functionals, which guarantee that implied\nBayesian credible regions have valid frequentist coverage. Our results provide\nnew theoretical justifications for (Bayesian) deep learning with ReLU\nactivation functions, highlighting their inferential potential.\n", "versions": [{"version": "v1", "created": "Wed, 26 Feb 2020 22:00:16 GMT"}, {"version": "v2", "created": "Sun, 12 Jul 2020 23:57:22 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wang", "Yuexi", ""], ["Ro\u010dkov\u00e1", "Veronika", ""]]}, {"id": "2002.11846", "submitter": "Aaron Sarvet", "authors": "Aaron L. Sarvet, Kerollos N. Wanis, Jessica Young, Roberto\n  Hernandez-Alejandro, Miguel A. Hern\\'an, Mats J. Stensrud", "title": "Causal inference with limited resources: proportionally-representative\n  interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Investigators often evaluate treatment effects by considering settings in\nwhich all individuals are assigned a treatment of interest, assuming that an\nunlimited number of treatment units are available. However, many real-life\ntreatments are of limited supply and cannot be provided to all individuals in\nthe population. For example, patients on the liver transplant waiting list\ncannot be assigned a liver transplant immediately at the time they reach\nhighest priority because a suitable organ is not likely to be immediately\navailable. In these cases, investigators may still be interested in the effects\nof treatment strategies in which a finite number of organs are available at a\ngiven time, that is, treatment regimes that satisfy resource constraints. Here,\nwe describe an estimand that can be used to define causal effects of treatment\nstrategies that satisfy resource constraints: proportionally-representative\ninterventions for limited resources. We derive a simple class of inverse\nprobability weighted estimators, and apply one such estimator to evaluate the\neffect of restricting or expanding utilization of \"increased risk\" liver organs\nto treat patients with end-stage liver disease. Our method is designed to\nevaluate policy-relevant interventions in the setting of finite treatment\nresources.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 00:03:20 GMT"}, {"version": "v2", "created": "Fri, 28 Feb 2020 15:53:00 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Sarvet", "Aaron L.", ""], ["Wanis", "Kerollos N.", ""], ["Young", "Jessica", ""], ["Hernandez-Alejandro", "Roberto", ""], ["Hern\u00e1n", "Miguel A.", ""], ["Stensrud", "Mats J.", ""]]}, {"id": "2002.11992", "submitter": "Lilun Du", "authors": "Lilun Du, Xu Guo, Wenguang Sun, and Changliang Zou", "title": "False Discovery Rate Control Under General Dependence By Symmetrized\n  Data Aggregation", "comments": "33 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new class of distribution--free multiple testing rules for false\ndiscovery rate (FDR) control under general dependence. A key element in our\nproposal is a symmetrized data aggregation (SDA) approach to incorporating the\ndependence structure via sample splitting, data screening and information\npooling. The proposed SDA filter first constructs a sequence of ranking\nstatistics that fulfill global symmetry properties, and then chooses a\ndata--driven threshold along the ranking to control the FDR. The SDA filter\nsubstantially outperforms the knockoff method in power under moderate to strong\ndependence, and is more robust than existing methods based on asymptotic\n$p$-values. We first develop finite--sample theory to provide an upper bound\nfor the actual FDR under general dependence, and then establish the asymptotic\nvalidity of SDA for both the FDR and false discovery proportion (FDP) control\nunder mild regularity conditions. The procedure is implemented in the R package\n\\texttt{SDA}. Numerical results confirm the effectiveness and robustness of SDA\nin FDR control and show that it achieves substantial power gain over existing\nmethods in many settings.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 09:27:57 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:33:36 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Du", "Lilun", ""], ["Guo", "Xu", ""], ["Sun", "Wenguang", ""], ["Zou", "Changliang", ""]]}, {"id": "2002.12085", "submitter": "Bruno Ebner", "authors": "Bruno Ebner", "title": "On combining the zero bias transform and the empirical characteristic\n  function to test normality", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new powerful family of tests of univariate normality. These\ntests are based on an initial value problem in the space of characteristic\nfunctions originating from the fixed point property of the normal distribution\nin the zero bias transform. Limit distributions of the test statistics are\nprovided under the null hypothesis, as well as under contiguous and fixed\nalternatives. Using the covariance structure of the limiting Gaussian process\nfrom the null distribution, we derive explicit formulas for the first four\ncumulants of the limiting random element and apply the results by fitting a\ndistribution from the Pearson system. A comparative Monte Carlo power study\nshows that the new tests are serious competitors to the strongest well\nestablished tests.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 13:36:12 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Ebner", "Bruno", ""]]}, {"id": "2002.12321", "submitter": "Wanrong Zhang", "authors": "Wanrong Zhang, Gautam Kamath, Rachel Cummings", "title": "PAPRIKA: Private Online False Discovery Rate Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DS cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In hypothesis testing, a false discovery occurs when a hypothesis is\nincorrectly rejected due to noise in the sample. When adaptively testing\nmultiple hypotheses, the probability of a false discovery increases as more\ntests are performed. Thus the problem of False Discovery Rate (FDR) control is\nto find a procedure for testing multiple hypotheses that accounts for this\neffect in determining the set of hypotheses to reject. The goal is to minimize\nthe number (or fraction) of false discoveries, while maintaining a high true\npositive rate (i.e., correct discoveries).\n  In this work, we study False Discovery Rate (FDR) control in multiple\nhypothesis testing under the constraint of differential privacy for the sample.\nUnlike previous work in this direction, we focus on the online setting, meaning\nthat a decision about each hypothesis must be made immediately after the test\nis performed, rather than waiting for the output of all tests as in the offline\nsetting. We provide new private algorithms based on state-of-the-art results in\nnon-private online FDR control. Our algorithms have strong provable guarantees\nfor privacy and statistical performance as measured by FDR and power. We also\nprovide experimental results to demonstrate the efficacy of our algorithms in a\nvariety of data environments.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 18:42:23 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 03:06:54 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Zhang", "Wanrong", ""], ["Kamath", "Gautam", ""], ["Cummings", "Rachel", ""]]}, {"id": "2002.12436", "submitter": "Madhurima Datta", "authors": "Madhurima Datta and Nitin Gupta", "title": "Stochastic comparison study of the general family of PHR and PRHR\n  distributions", "comments": "No figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have obtained conditions on parameters that result in\ndispersive ordering and star ordering among two unequal sets of random\nvariables from Proportional hazard rate and Proportional reversed hazard rate\nfamily of distributions. The n-independent random variables under observation\nbelonging to a multiple-outlier model are given as corollary. The conditions\nobtained involve simple inequalities among the parameters and class of life\ndistribution corresponding to ageing. Some stochastic ordering results for\nsample minimum and maximum with dependency based on Archimedean copula has also\nbeen studied by varying the location parameters.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 20:58:46 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Datta", "Madhurima", ""], ["Gupta", "Nitin", ""]]}, {"id": "2002.12468", "submitter": "Madhurima Datta", "authors": "Madhurima Datta and Nitin Gupta", "title": "Usual stochastic ordering results for series and parallel systems with\n  components having Exponentiated Chen distribution", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have discussed the usual stochastic ordering relations\nbetween two systems. Each system consists of n mutually independent components.\nThe components follow Exponentiated (Extended) Chen distribution with three\nparameters $\\alpha, \\beta, \\lambda$. Two popular systems are taken into\nconsideration, one is the series system and another is the parallel system. The\nresults in this paper are obtained by varying one parameter and the other\nparameters are kept constant. The hazard rate ordering or reversed hazard rate\nordering relations that are not possible for series or parallel systems have\nbeen demonstrated with the help of counterexamples.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 22:35:09 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Datta", "Madhurima", ""], ["Gupta", "Nitin", ""]]}, {"id": "2002.12474", "submitter": "Madhurima Datta", "authors": "Madhurima Datta and Nitin Gupta", "title": "A stochastic comparison study for the smallest and largest ordered\n  statistic from Weibull-G and Gompertz Makeham distribution", "comments": "16 pages, 2 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we have discussed the stochastic comparison of the smallest\nand largest ordered statistic from independent heterogeneous Weibull-G random\nvariables and Gompertz Makeham random variables. We compare systems arising\nfrom taking different model parameters and obtain stochastic ordering results\nunder the condition of multivariate chain majorization. Using the notion of\nvector majorization, we compare different systems and obtain stochastic\nordering results.\n", "versions": [{"version": "v1", "created": "Thu, 27 Feb 2020 23:14:24 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Datta", "Madhurima", ""], ["Gupta", "Nitin", ""]]}, {"id": "2002.12548", "submitter": "Haojie Ren", "authors": "Changliang Zou, Haojie Ren, Xu Guo and Runze Li", "title": "A New Procedure for Controlling False Discovery Rate in Large-Scale\n  t-tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with false discovery rate (FDR) control in\nlarge-scale multiple testing problems. We first propose a new data-driven\ntesting procedure for controlling the FDR in large-scale t-tests for one-sample\nmean problem. The proposed procedure achieves exact FDR control in finite\nsample settings when the populations are symmetric no matter the number of\ntests or sample sizes. Comparing with the existing bootstrap method for FDR\ncontrol, the proposed procedure is computationally efficient. We show that the\nproposed method can control the FDR asymptotically for asymmetric populations\neven when the test statistics are not independent. We further show that the\nproposed procedure with a simple correction is as accurate as the bootstrap\nmethod to the second-order degree, and could be much more effective than the\nexisting normal calibration. We extend the proposed procedure to two-sample\nmean problem. Empirical results show that the proposed procedures have better\nFDR control than existing ones when the proportion of true alternative\nhypotheses is not too low, while maintaining reasonably good detection ability.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 05:13:40 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Zou", "Changliang", ""], ["Ren", "Haojie", ""], ["Guo", "Xu", ""], ["Li", "Runze", ""]]}, {"id": "2002.12606", "submitter": "Benjamin Stokell", "authors": "Benjamin G. Stokell, Rajen D. Shah, Ryan J. Tibshirani", "title": "Modelling High-Dimensional Categorical Data Using Nonconvex Fusion\n  Penalties", "comments": "52 pages, 10 figures; to appear in JRSSB", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for estimation in high-dimensional linear models with\nnominal categorical data. Our estimator, called SCOPE, fuses levels together by\nmaking their corresponding coefficients exactly equal. This is achieved using\nthe minimax concave penalty on differences between the order statistics of the\ncoefficients for a categorical variable, thereby clustering the coefficients.\nWe provide an algorithm for exact and efficient computation of the global\nminimum of the resulting nonconvex objective in the case with a single variable\nwith potentially many levels, and use this within a block coordinate descent\nprocedure in the multivariate case. We show that an oracle least squares\nsolution that exploits the unknown level fusions is a limit point of the\ncoordinate descent with high probability, provided the true levels have a\ncertain minimum separation; these conditions are known to be minimal in the\nunivariate case. We demonstrate the favourable performance of SCOPE across a\nrange of real and simulated datasets. An R package CatReg implementing SCOPE\nfor linear models and also a version for logistic regression is available on\nCRAN.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 09:20:41 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 18:52:13 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 10:45:06 GMT"}, {"version": "v4", "created": "Mon, 28 Jun 2021 14:48:14 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Stokell", "Benjamin G.", ""], ["Shah", "Rajen D.", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "2002.12631", "submitter": "L\\'aszl\\'o Viharos", "authors": "Amenah AL-Najafi, L\\'aszl\\'o Viharos", "title": "Weighted least squares estimators for the Parzen tail index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the tail index of heavy-tailed distributions and its\napplications are essential in many research areas. We propose a class of\nweighted least squares (WLS) estimators for the Parzen tail index. Our approach\nis based on the method developed by \\cite{Holan2010}. We investigate\nconsistency and asymptotic normality of the WLS estimators. Through a\nsimulation study, we make a comparison with the Hill, Pickands, DEdH (Dekkers,\nEinmahl and de Haan) and ordinary least squares (OLS) estimators using the mean\nsquare error as criterion. The results show that in a restricted model some\nmembers of the WLS estimators are competitive with the Pickands, DEdH and OLS\nestimators.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 10:20:54 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["AL-Najafi", "Amenah", ""], ["Viharos", "L\u00e1szl\u00f3", ""]]}, {"id": "2002.12634", "submitter": "L\\'aszl\\'o Viharos", "authors": "Amenah AL-Najafi, L\\'aszl\\'o Viharos", "title": "Regression estimators for the tail index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of weighted least squares estimators for the tail index of\na distribution function with a regularly varying upper tail. Our approach is\nbased on the method developed by \\cite{Holan2010} for the Parzen tail index.\nAsymptotic normality of the estimators is proved. Through a simulation study,\nthese and earlier estimators are compared in the Pareto and Hall models using\nthe mean squared error as criterion. The results show that the weighted least\nsquares estimator is better than the other estimators investigated.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 10:26:02 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["AL-Najafi", "Amenah", ""], ["Viharos", "L\u00e1szl\u00f3", ""]]}, {"id": "2002.12653", "submitter": "Christian Soize", "authors": "Christian Soize and Roger Ghanem", "title": "Probabilistic Learning on Manifolds", "comments": "41 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents mathematical results in support of the methodology of the\nprobabilistic learning on manifolds (PLoM) recently introduced by the authors,\nwhich has been used with success for analyzing complex engineering systems. The\nPLoM considers a given initial dataset constituted of a small number of points\ngiven in an Euclidean space, which are interpreted as independent realizations\nof a vector-valued random variable for which its non-Gaussian probability\nmeasure is unknown but is, \\textit{a priori}, concentrated in an unknown subset\nof the Euclidean space. The objective is to construct a learned dataset\nconstituted of additional realizations that allow the evaluation of converged\nstatistics. A transport of the probability measure estimated with the initial\ndataset is done through a linear transformation constructed using a\nreduced-order diffusion-maps basis. In this paper, it is proven that this\ntransported measure is a marginal distribution of the invariant measure of a\nreduced-order It\\^o stochastic differential equation that corresponds to a\ndissipative Hamiltonian dynamical system. This construction allows for\npreserving the concentration of the probability measure. This property is shown\nby analyzing a distance between the random matrix constructed with the PLoM and\nthe matrix representing the initial dataset, as a function of the dimension of\nthe basis. It is further proven that this distance has a minimum for a\ndimension of the reduced-order diffusion-maps basis that is strictly smaller\nthan the number of points in the initial dataset. Finally, a brief numerical\napplication illustrates the mathematical results.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 11:15:59 GMT"}], "update_date": "2020-03-02", "authors_parsed": [["Soize", "Christian", ""], ["Ghanem", "Roger", ""]]}, {"id": "2002.12703", "submitter": "R\\'emy Mari\\'etan", "authors": "R\\'emy Mari\\'etan and Stephan Morgenthaler", "title": "Statistical applications of Random matrix theory: comparison of two\n  populations II", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a statistical procedure for testing the equality of\ntwo independent estimated covariance matrices when the number of potentially\ndependent data vectors is large and proportional to the size of the vectors,\nthat is, the number of variables. Inspired by the spike models used in random\nmatrix theory, we concentrate on the largest eigenvalues of the matrices in\norder to determine significance. To avoid false rejections we must guard\nagainst residual spikes and need a sufficiently precise description of the\nbehaviour of the largest eigenvalues under the null hypothesis.\n  In this paper we propose some \"invariant\" theorems that allows us to extend\nthe test of arXiv:2002.12741 for perturbation of order $1$ to some general\ntests for order $k$. The statistics introduced in this paper allow the user to\ntest the equality of two populations based on high-dimensional multivariate\ndata.\n  Simulations show that these tests have more power of detection than standard\nmultivariate approaches.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 13:28:52 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 10:08:38 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 12:15:34 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Mari\u00e9tan", "R\u00e9my", ""], ["Morgenthaler", "Stephan", ""]]}, {"id": "2002.12733", "submitter": "Huiming Zhang", "authors": "Yifan Fan, Huiming Zhang, Ting Yan", "title": "Asymptotic Theory for Differentially Private Generalized $\\beta$-models\n  with Parameters Increasing", "comments": "32 pages, 11 figures, to appear in Statistics and Its Interface", "journal-ref": "Statistics and Its Interface, 2020, 13(3): 385-398", "doi": "10.4310/SII.2020.v13.n3.a8", "report-no": null, "categories": "math.ST cs.SI stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling edge weights play a crucial role in the analysis of network data,\nwhich reveals the extent of relationships among individuals. Due to the\ndiversity of weight information, sharing these data has become a complicated\nchallenge in a privacy-preserving way. In this paper, we consider the case of\nthe non-denoising process to achieve the trade-off between privacy and weight\ninformation in the generalized $\\beta$-model. Under the edge differential\nprivacy with a discrete Laplace mechanism, the Z-estimators from estimating\nequations for the model parameters are shown to be consistent and\nasymptotically normally distributed. The simulations and a real data example\nare given to further support the theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 14:20:55 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Fan", "Yifan", ""], ["Zhang", "Huiming", ""], ["Yan", "Ting", ""]]}, {"id": "2002.12741", "submitter": "R\\'emy Mari\\'etan", "authors": "R\\'emy Mari\\'etan and Stephan Morgenthaler", "title": "Statistical applications of random matrix theory: comparison of two\n  populations I", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates a statistical procedure for testing the equality of\ntwo independent estimated covariance matrices when the number of potentially\ndependent data vectors is large and proportional to the size of the vectors,\nthat is, the number of variables. Inspired by the spike models used in random\nmatrix theory, we concentrate on the largest eigenvalues of the matrices in\norder to determine significance. To avoid false rejections we must guard\nagainst residual spikes and need a sufficiently precise description of the\nbehaviour of the largest eigenvalues under the null hypothesis. In this paper,\nwe lay a foundation by treating alternatives based on perturbations of order\n$1$, that is, a single large eigenvalue. Our statistic allows the user to test\nthe equality of two populations. Future work will extend the result to\nperturbations of order $k$ and demonstrate conservativeness of the procedure\nfor more general matrices.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 14:31:31 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2020 09:55:48 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Mari\u00e9tan", "R\u00e9my", ""], ["Morgenthaler", "Stephan", ""]]}, {"id": "2002.12871", "submitter": "Giovanni Pistone", "authors": "Giovanni Pistone", "title": "Information Geometry of smooth densities on the Gaussian space:\n  Poincar\\'e inequalities", "comments": "Accepted. Minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive bounds for the Orlicz norm of the deviation of a random variable\ndefined on $\\mathbb{R}^n$ from its Gaussian mean value. The random variables\nare assumed to be smooth and the bound itself depends on the Orlicz norm of the\ngradient. Applications to non-parametric Information Geometry are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 17:12:10 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 21:38:10 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Pistone", "Giovanni", ""]]}, {"id": "2002.12903", "submitter": "Michael Celentano", "authors": "Michael Celentano, Andrea Montanari, Yuchen Wu", "title": "The estimation error of general first order methods", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern large-scale statistical models require to estimate thousands to\nmillions of parameters. This is often accomplished by iterative algorithms such\nas gradient descent, projected gradient descent or their accelerated versions.\nWhat are the fundamental limits to these approaches? This question is well\nunderstood from an optimization viewpoint when the underlying objective is\nconvex. Work in this area characterizes the gap to global optimality as a\nfunction of the number of iterations. However, these results have only indirect\nimplications in terms of the gap to statistical optimality.\n  Here we consider two families of high-dimensional estimation problems:\nhigh-dimensional regression and low-rank matrix estimation, and introduce a\nclass of `general first order methods' that aim at efficiently estimating the\nunderlying parameters. This class of algorithms is broad enough to include\nclassical first order optimization (for convex and non-convex objectives), but\nalso other types of algorithms. Under a random design assumption, we derive\nlower bounds on the estimation error that hold in the high-dimensional\nasymptotics in which both the number of observations and the number of\nparameters diverge. These lower bounds are optimal in the sense that there\nexist algorithms whose estimation error matches the lower bounds up to\nasymptotically negligible terms. We illustrate our general results through\napplications to sparse phase retrieval and sparse principal component analysis.\n", "versions": [{"version": "v1", "created": "Fri, 28 Feb 2020 18:13:47 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2020 17:44:58 GMT"}], "update_date": "2020-03-04", "authors_parsed": [["Celentano", "Michael", ""], ["Montanari", "Andrea", ""], ["Wu", "Yuchen", ""]]}]