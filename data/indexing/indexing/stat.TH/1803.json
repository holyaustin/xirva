[{"id": "1803.00008", "submitter": "Gautam Kamath", "authors": "Jayadev Acharya, Gautam Kamath, Ziteng Sun, Huanyu Zhang", "title": "INSPECTRE: Privately Estimating the Unseen", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.CR cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop differentially private methods for estimating various\ndistributional properties. Given a sample from a discrete distribution $p$,\nsome functional $f$, and accuracy and privacy parameters $\\alpha$ and\n$\\varepsilon$, the goal is to estimate $f(p)$ up to accuracy $\\alpha$, while\nmaintaining $\\varepsilon$-differential privacy of the sample.\n  We prove almost-tight bounds on the sample size required for this problem for\nseveral functionals of interest, including support size, support coverage, and\nentropy. We show that the cost of privacy is negligible in a variety of\nsettings, both theoretically and experimentally. Our methods are based on a\nsensitivity analysis of several state-of-the-art methods for estimating these\nproperties with sublinear sample complexities.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 19:00:00 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Acharya", "Jayadev", ""], ["Kamath", "Gautam", ""], ["Sun", "Ziteng", ""], ["Zhang", "Huanyu", ""]]}, {"id": "1803.00098", "submitter": "Christophe Ley", "authors": "Fatemeh Ghaderinezhad and Christophe Ley", "title": "A general measure of the impact of priors in Bayesian statistics via\n  Stein's Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a measure of the impact of any two choices of prior distributions\nby quantifying the Wasserstein distance between the respective resulting\nposterior distributions at any fixed sample size. We illustrate this measure on\nthe normal, Binomial and Poisson models.\n", "versions": [{"version": "v1", "created": "Wed, 28 Feb 2018 21:37:21 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Ghaderinezhad", "Fatemeh", ""], ["Ley", "Christophe", ""]]}, {"id": "1803.00293", "submitter": "Jaakko Nevalainen", "authors": "Jaakko Nevalainen, Denis Larocque, Hannu Oja and Ilkka P\\\"orsti", "title": "Nonparametric Analysis of Clustered Multivariate Data", "comments": "Author produced version. Published version available at\n  https://doi.org/10.1198/jasa.2010.tm08545", "journal-ref": "Jaakko Nevalainen, Denis Larocque, Hannu Oja & Ilkka P\\\"orsti\n  (2012) Nonparametric Analysis of Clustered Multivariate Data, Journal of the\n  American Statistical Association, 105:490, 864-872, DOI:\n  10.1198/jasa.2010.tm08545", "doi": "10.1198/jasa.2010.tm08545", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a wide interest to extend univariate and multivariate\nnonparametric procedures to clustered and hierarchical data. Traditionally,\nparametric mixed models have been used to account for the correlation\nstructures among the dependent observational units. In this work we extend\nmultivariate nonparametric procedures for one-sample and several samples\nlocation problems to clustered data settings. The results are given for a\ngeneral score function, but with an emphasis on spatial sign and rank methods.\nMixed models notation involving design matrices for fixed and random effects is\nused throughout. The asymptotic variance formulas and limiting distributions of\nthe test statistics under the null hypothesis and under a sequence of\nalternatives are derived, as well as the limiting distributions for the\ncorresponding estimates. The approach based on a general score function also\nshows, for example, how $M$-estimates behave with clustered data. Efficiency\nstudies demonstrate practical advantages and disadvantages of the use of\nspatial sign and rank scores, and their weighted versions. Small sample\nprocedures based on sign change and permutation principles are discussed.\nFurther development of nonparametric methods for cluster correlated data would\nbenefit from the notation already familiar to statisticians working under\nnormality assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 10:40:55 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Nevalainen", "Jaakko", ""], ["Larocque", "Denis", ""], ["Oja", "Hannu", ""], ["P\u00f6rsti", "Ilkka", ""]]}, {"id": "1803.00508", "submitter": "Valeriy Avanesov", "authors": "Valeriy Avanesov", "title": "Structural break analysis in high-dimensional covariance structure", "comments": "arXiv admin note: text overlap with arXiv:1610.03783", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider detection and localization of an abrupt break in the covariance\nstructure of high-dimensional random data. The paper proposes a novel testing\nprocedure for this problem. Due to its nature, the approach requires a properly\nchosen critical level. In this regard we propose a purely data-driven\ncalibration scheme. The approach can be straightforwardly employed in online\nsetting and is essentially multiscale allowing for a trade-off between\nsensitivity and change-point localization (in online setting, the delay of\ndetection). The description of the algorithm is followed by a formal\ntheoretical study justifying the proposed calibration scheme under mild\nassumption and providing guaranties for break detection. All the theoretical\nresults are obtained in a high-dimensional setting (dimensionality $p >> n$).\nThe results are supported by a simulation study inspired by real-world\nfinancial data.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2018 17:14:48 GMT"}, {"version": "v2", "created": "Sun, 14 Jul 2019 19:48:32 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Avanesov", "Valeriy", ""]]}, {"id": "1803.00715", "submitter": "Ilmun Kim", "authors": "Ilmun Kim, Sivaraman Balakrishnan, Larry Wasserman", "title": "Robust Multivariate Nonparametric Tests via Projection-Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we generalize the Cram\\'er-von Mises statistic via\nprojection-averaging to obtain a robust test for the multivariate two-sample\nproblem. The proposed test is consistent against all fixed alternatives, robust\nto heavy-tailed data and minimax rate optimal against a certain class of\nalternatives. Our test statistic is completely free of tuning parameters and is\ncomputationally efficient even in high dimensions. When the dimension tends to\ninfinity, the proposed test is shown to have comparable power to the existing\nhigh-dimensional mean tests under certain location models. As a by-product of\nour approach, we introduce a new metric called the angular distance which can\nbe thought of as a robust alternative to the Euclidean distance. Using the\nangular distance, we connect the proposed method to the reproducing kernel\nHilbert space approach. In addition to the Cram\\'er-von Mises statistic, we\ndemonstrate that the projection-averaging technique can be used to define\nrobust, multivariate tests in many other problems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 04:19:33 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 15:58:58 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 16:18:29 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Kim", "Ilmun", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "1803.00733", "submitter": "Saeid Rezakhah", "authors": "M. Mohammadi, S. Rezakhah and N. Modarresi", "title": "Continuous-time GARCH process driven by semi-L\\'evy process", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the simple semi-L\\'evy driven continuous-time\ngeneralized autoregressive conditionally heteroscedastic (SS-COGARCH) process.\nThe statistical properties of this process are characterized. This process has\nthe potential to approximate any semi-L\\'evy driven COGARCH processes. We show\nthat the state representation of such SS-COGARCH process can be described by a\nrandom recurrence equation with periodic random coefficients. The almost sure\nabsolute convergence of the state process is proved. The periodically\nstationary solution of the state process is shown which cause the volatility to\nbe periodically stationary under some suitable conditions. Also it is shown\nthat the increments with constant length of such SS-COGARCH process is itself a\nperiodically correlated (PC) process. Finally, we apply some test to\ninvestigate the PC behavior of the increments (with constant length) of the\nsimulated samples of proposed SS-COGARCH process.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 06:56:01 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Mohammadi", "M.", ""], ["Rezakhah", "S.", ""], ["Modarresi", "N.", ""]]}, {"id": "1803.00739", "submitter": "Saeid Rezakhah", "authors": "Ferdous Mohammadi Basatini and Saeid Rezakhah", "title": "Markov Switch Smooth Transition HYGARCH Model: Stability and Estimation", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HYGARCH model is basically used to model long-range dependence in volatility.\nWe propose Markov switch smooth-transition HYGARCH model, where the volatility\nin each state is a time-dependent convex combination of GARCH and FIGARCH. This\nmodel provides a flexible structure to capture different levels of volatilities\nand also short and long memory effects. The necessary and sufficient condition\nfor the asymptotic stability is derived. Forecast of conditional variance is\nstudied by using all past information through a parsimonious way. Bayesian\nestimations based on Gibbs sampling are provided. A simulation study has been\ngiven to evaluate the estimations and model stability. The competitive\nperformance of the proposed model is shown by comparing it with the HYGARCH and\nsmooth-transition HYGARCH models for some period of the\n\\textit{S}\\&\\textit{P}500 indices based on volatility and value-at-risk\nforecasts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 07:22:33 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Basatini", "Ferdous Mohammadi", ""], ["Rezakhah", "Saeid", ""]]}, {"id": "1803.01150", "submitter": "Richard Samworth", "authors": "Yi Yu, Jelena Bradic and Richard J. Samworth", "title": "Confidence intervals for high-dimensional Cox models", "comments": "36 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to construct confidence intervals for the\nregression coefficients in high-dimensional Cox proportional hazards regression\nmodels where the number of covariates may be larger than the sample size. Our\ndebiased estimator construction is similar to those in Zhang and Zhang (2014)\nand van de Geer et al. (2014), but the time-dependent covariates and censored\nrisk sets introduce considerable additional challenges. Our theoretical\nresults, which provide conditions under which our confidence intervals are\nasymptotically valid, are supported by extensive numerical experiments.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 12:30:01 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Yu", "Yi", ""], ["Bradic", "Jelena", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1803.01156", "submitter": "Mohieddine Rahmouni", "authors": "Mohieddine Rahmouni, Ayman Orabi", "title": "A Generalization of the Exponential-Logarithmic Distribution for\n  Reliability and Life Data Analysis", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": "10.1007/s41872-018-0049-5", "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a new two-parameter lifetime distribution, called\nthe exponential-generalized truncated logarithmic (EGTL) distribution, by\ncompounding the exponential and generalized truncated logarithmic\ndistributions. Our procedure generalizes the exponential-logarithmic (EL)\ndistribution modelling the reliability of systems by the use of first-order\nconcepts, where the minimum lifetime is considered (Tahmasbi 2008). In our\napproach, we assume that a system fails if a given number k of the components\nfails and then, we consider the kth-smallest value of lifetime instead of the\nminimum lifetime. The reliability and failure rate functions as well as their\nproperties are presented for some special cases. The estimation of the\nparameters is attained by the maximum likelihood, the expectation maximization\nalgorithm, the method of moments and the Bayesian approach, with a simulation\nstudy performed to illustrate the different methods of estimation. The\napplication study is illustrated based on two real data sets used in many\napplications of reliability.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 12:53:19 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Rahmouni", "Mohieddine", ""], ["Orabi", "Ayman", ""]]}, {"id": "1803.01175", "submitter": "Jaakko Nevalainen", "authors": "Jaakko Nevalainen, Somnath Datta and Hannu Oja", "title": "Inference on the marginal distribution of clustered data with\n  informative cluster size", "comments": "Author produced version. Published version available at:\n  https://link.springer.com/article/10.1007/s00362-013-0504-3", "journal-ref": "Stat Papers (2014) 55: 71", "doi": "10.1007/s00362-013-0504-3", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of recent contributions to the literature, informative cluster size\nsettings are not well known and understood. In this paper, we give a formal\ndefinition of the problem and describe it from different viewpoints. Data\ngenerating mechanisms, parametric and nonparametric models are considered in\nlight of examples. Our emphasis is on nonparametric and robust approaches to\nthe inference on the marginal distribution. Descriptive statistics and\nparameters of interest are defined as functionals and they are accompanied with\na generally applicable testing procedure. The theory is illustrated with an\nexample on patients with incomplete spinal cord injuries.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 14:30:31 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Nevalainen", "Jaakko", ""], ["Datta", "Somnath", ""], ["Oja", "Hannu", ""]]}, {"id": "1803.01188", "submitter": "Xiucai Ding", "authors": "Xiucai Ding, and Zhou Zhou", "title": "Estimation and inference for precision matrices of non-stationary time\n  series", "comments": null, "journal-ref": "The Annals of Statistics (2019+)", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the estimation and inference of precision matrices\nof a rich class of locally stationary and nonlinear time series assuming that\nonly one realization of the time series is observed. Using a Cholesky\ndecomposition technique, we show that the precision matrices can be directly\nestimated via a series of least squares linear regressions with smoothly\ntime-varying coefficients. The method of sieves is utilized for the estimation\nand is shown to be efficient and optimally adaptive in terms of estimation\naccuracy and computational complexity. We establish an asymptotic theory for a\nclass of ${\\cal L}^2$ tests based on the nonparametric sieve estimators. The\nlatter are used for testing whether the precision matrices are diagonal or\nbanded. A high dimensional Gaussian approximation result is established for a\nwide class of quadratic form of non-stationary and nonlinear processes, which\nis of interest by itself.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 15:46:18 GMT"}, {"version": "v2", "created": "Sun, 15 Apr 2018 02:09:40 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 16:49:34 GMT"}, {"version": "v4", "created": "Mon, 25 Mar 2019 01:48:28 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Ding", "Xiucai", ""], ["Zhou", "Zhou", ""]]}, {"id": "1803.01209", "submitter": "Tommy Liu", "authors": "Tommy Liu", "title": "Stochastic Resonance for a Model with Two Pathways", "comments": "PhD thesis defended at the University of Reading", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST physics.data-an physics.geo-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this thesis we consider stochastic resonance for a diffusion with drift\ngiven by a potential, which has two metastable states and two pathways between\nthem. Depending on the direction of the forcing the height of the two barriers,\none for each path, will either oscillate alternating or in synchronisation.\n  We consider a simplified model given by discrete and continuous time Markov\nChains with two states. This was done for alternating and synchronised wells.\nThe invariant measures are derived for both cases and shown to be constant for\nthe synchronised case. A PDF for the escape time from an oscillatory potential\nis reviewed.\n  Methods of detecting stochastic resonance are presented, which are linear\nresponse, signal-to-noise ratio, energy, out-of-phase measures, relative\nentropy and entropy. A new statistical test called the conditional\nKolmogorov-Smirnov test is developed, which can be used to analyse stochastic\nresonance.\n  An explicit two dimensional potential is introduced, the critical point\nstructure derived and the dynamics, the invariant state and escape time studied\nnumerically.\n  The six measures are unable to detect the stochastic resonance in the case of\nsynchronised saddles. The distribution of escape times however not only shows a\nclear sign of stochastic resonance, but changing the direction of the forcing\nfrom alternating to synchronised saddles an additional resonance at double the\nforcing frequency starts to appear. The conditional KS test reliably detects\nthe stochastic resonance even for forcing quick enough and for data so sparse\nthat the stochastic resonance is not obvious directly from the histogram of\nescape times.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 17:50:10 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Liu", "Tommy", ""]]}, {"id": "1803.01231", "submitter": "Fangzheng Xie", "authors": "Fangzheng Xie, Yanxun Xu", "title": "Bayesian Projected Calibration of Computer Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian approach called Bayesian projected calibration to\naddress the problem of calibrating an imperfect computer model using\nobservational data from a complex physical system. The calibration parameter\nand the physical system are parametrized in an identifiable fashion via\n$L_2$-projection. The physical process is assigned a Gaussian process prior,\nwhich naturally induces a prior distribution on the calibration parameter\nthrough the $L_2$-projection constraint. The calibration parameter is estimated\nthrough its posterior distribution, which provides a natural and non-asymptotic\nway for the uncertainty quantification. We provide a rigorous large sample\njustification for the proposed approach by establishing the asymptotic\nnormality of the posterior of the calibration parameter with the efficient\ncovariance matrix. In addition, two efficient computational algorithms based on\nstochastic approximation are designed with theoretical guarantees. Through\nextensive simulation studies and two real-world datasets analyses, we show that\nthe Bayesian projected calibration can accurately estimate the calibration\nparameters, appropriately calibrate the computer models, and compare favorably\nto alternative approaches.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 20:32:29 GMT"}, {"version": "v2", "created": "Thu, 7 Feb 2019 17:31:41 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Xie", "Fangzheng", ""], ["Xu", "Yanxun", ""]]}, {"id": "1803.01240", "submitter": "Bo\\v{z}idar Popovi\\'c", "authors": "Filippo Domma, Bo\\v{z}idar V. Popovi\\'c, Saralees Nadarajah", "title": "An extension of Azzalini's method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to extend Azzalini's method. This extension is done\nin two stages: consider two dependent and non-identically distributed random\nvariables say $X_1$ and $X_2$; model the dependence between $X_1$ and $X_2$ by\na copula. To illustrate the new method, we assume $X_1$ and $X_2$ are\nexponential random variables. This assumption leads to a new distribution\ncalled the Generalized Weighted Exponential Distribution (GWED), a\ngeneralization of Gupta and Kundu (2009)'s Weighted Exponential Distribution\n(WED). Some mathematical properties of the GWED are derived, and its parameters\nestimated by maximum likelihood. The GWED is applied to biochemical data sets\nshowing its good performance compared to the WED.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2018 21:07:16 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Domma", "Filippo", ""], ["Popovi\u0107", "Bo\u017eidar V.", ""], ["Nadarajah", "Saralees", ""]]}, {"id": "1803.01302", "submitter": "Yuancheng Zhu", "authors": "Yuancheng Zhu and John Lafferty", "title": "Distributed Nonparametric Regression under Communication Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of nonparametric estimation of a smooth\nfunction with data distributed across multiple machines. We assume an\nindependent sample from a white noise model is collected at each machine, and\nan estimator of the underlying true function needs to be constructed at a\ncentral machine. We place limits on the number of bits that each machine can\nuse to transmit information to the central machine. Our results give both\nasymptotic lower bounds and matching upper bounds on the statistical risk under\nvarious settings. We identify three regimes, depending on the relationship\namong the number of machines, the size of the data available at each machine,\nand the communication budget. When the communication budget is small, the\nstatistical risk depends solely on this communication bottleneck, regardless of\nthe sample size. In the regime where the communication budget is large, the\nclassic minimax risk in the non-distributed estimation setting is recovered. In\nan intermediate regime, the statistical risk depends on both the sample size\nand the communication budget.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2018 05:15:10 GMT"}, {"version": "v2", "created": "Sat, 23 Jun 2018 15:27:50 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Zhu", "Yuancheng", ""], ["Lafferty", "John", ""]]}, {"id": "1803.01497", "submitter": "Kurnia Susvitasari", "authors": "Kurnia Susvitasari", "title": "Comparing the Behaviour of Deterministic and Stochastic Model of SIS\n  Epidemic", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Studies about epidemic modelling have been conducted since before 19th\ncentury. Both deterministic and stochastiic model were used to capture the\ndynamic of infection in the population. The purpose of this project is to\ninvestigate the behaviour of the models when we set the basic reproduction\nnumber, $R_0$. This quantity is defined as the expected number of contacts made\nby a typical infective to susceptibles in the population. According to the\nepidemic threshold theory, when $R_0 \\leq 1$, minor epidemic occurs with\nprobability one in both approaches, but when $R_0 > 1$, the deterministic and\nstochastic models have different interpretation. In the deterministic approach,\nmajor epidemic occurs with probability one when $R_0 > 1$ and predicts that the\ndisease will settle down to an endemic equilibrium. Stochastic models, on the\nother hand, identify that the minor epidemic can possibly occur. If it does,\nthen the epidemic will die out quickly. Moreover, if we let the population size\nbe large and the major epidemic occurs, then it will take off and then reach\nthe endemic level and move randomly around the deterministic's equilibrium.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 04:59:27 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Susvitasari", "Kurnia", ""]]}, {"id": "1803.01576", "submitter": "Simon Barthelm\\'e", "authors": "Simon Barthelm\\'e, Pierre-Olivier Amblard, Nicolas Tremblay", "title": "Asymptotic Equivalence of Fixed-size and Varying-size Determinantal\n  Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal Point Processes (DPPs) are popular models for point processes\nwith repulsion. They appear in numerous contexts, from physics to graph theory,\nand display appealing theoretical properties. On the more practical side of\nthings, since DPPs tend to select sets of points that are some distance apart\n(repulsion), they have been advocated as a way of producing random subsets with\nhigh diversity. DPPs come in two variants: fixed-size and varying-size. A\nsample from a varying-size DPP is a subset of random cardinality, while in\nfixed-size \"$k$-DPPs\" the cardinality is fixed. The latter makes more sense in\nmany applications, but unfortunately their computational properties are less\nattractive, since, among other things, inclusion probabilities are harder to\ncompute. In this work we show that as the size of the ground set grows,\n$k$-DPPs and DPPs become equivalent, meaning that their inclusion probabilities\nconverge. As a by-product, we obtain saddlepoint formulas for inclusion\nprobabilities in $k$-DPPs. These turn out to be extremely accurate, and suffer\nless from numerical difficulties than exact methods do. Our results also\nsuggest that $k$-DPPs and DPPs also have equivalent maximum likelihood\nestimators. Finally, we obtain results on asymptotic approximations of\nelementary symmetric polynomials which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 09:59:04 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 06:17:32 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Barthelm\u00e9", "Simon", ""], ["Amblard", "Pierre-Olivier", ""], ["Tremblay", "Nicolas", ""]]}, {"id": "1803.01665", "submitter": "Danijel Kivaranovic", "authors": "Danijel Kivaranovic, Hannes Leeb", "title": "On the length of post-model-selection confidence intervals conditional\n  on polyhedral constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Valid inference after model selection is currently a very active area of\nresearch. The polyhedral method, pioneered by Lee, et al. (2016), allows for\nvalid inference after model selection if the model selection event can be\ndescribed by polyhedral constraints. In that reference, the method is\nexemplified by constructing two valid confidence intervals when the Lasso\nestimator is used to select a model. We here study the length of these\nintervals. For one of these confidence intervals, which is easier to compute,\nwe find that its expected length is always infinite. For the other of these\nconfidence intervals, whose computation is more demanding, we give a necessary\nand sufficient condition for its expected length to be infinite. In\nsimulations, we find that this sufficient condition is typically satisfied,\nunless the selected model includes almost all or almost none of the available\nregressors. For the distribution of confidence interval length, we find that\nthe $\\kappa$-quantiles behave like $1/(1-\\kappa)$ for $\\kappa$ close to $1$.\nOur results can also be used to analyze other confidence intervals that are\nbased on the polyhedral method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2018 14:04:18 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 09:39:52 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Kivaranovic", "Danijel", ""], ["Leeb", "Hannes", ""]]}, {"id": "1803.02078", "submitter": "Adrien Saumard", "authors": "Adrien Saumard and Fabien Navarro", "title": "Finite sample improvement of Akaike's Information Criterion", "comments": "This is a further version of the preprint entitled \"Model Selection\n  as a Multiple Testing Procedure: Improving Akaike's Information Criterion\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We emphasize that it is possible to improve the principle of unbiased risk\nestimation for model selection by addressing excess risk deviations in the\ndesign of penalization procedures. Indeed, we propose a modification of\nAkaike's Information Criterion that avoids overfitting, even when the sample\nsize is small. We call this correction an over-penalization procedure. As proof\nof concept, we show the nonasymptotic optimality of our histogram selection\nprocedure in density estimation by establishing sharp oracle inequalities for\nthe Kullback-Leibler divergence. One of the main features of our theoretical\nresults is that they include the estimation of unbounded logdensities. To do\nso, we prove several analytical and probabilistic lemmas that are of\nindependent interest. In an experimental study, we also demonstrate\nstate-of-the-art performance of our over-penalization criterion for bin size\nselection, in particular outperforming AICc procedure.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 09:46:53 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 15:29:17 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 15:10:10 GMT"}, {"version": "v4", "created": "Fri, 20 Jul 2018 12:38:41 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Saumard", "Adrien", ""], ["Navarro", "Fabien", ""]]}, {"id": "1803.02260", "submitter": "Romeo Mestrovic mester", "authors": "Romeo Me\\v{s}trovi\\'c", "title": "On some discrete random variables arising from recent study on\n  statistical analysis of compressive sensing", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent paper [27] provides a statistical analysis for efficient detection\nof signal components when missing data samples are present. Here we focus our\nattention to some complex-valued discrete random variables $X_l(m,N)$ ($0\\le\nl\\le N-1$, $1\\le M\\le N$), which are closely related to the random variables\ninvestigated by LJ. Stankovi\\'c, S. Stankovi\\'c and M. Amin in \\cite{ssa}. In\nparticular, by using a combinatorial approach, we prove that for $l\\not=0$ the\nexpected value of $X_l(m,N)$ is equal to zero, and we deduce the expression for\nthe variance of the random variables $X_l(m,N)$. The same results are also\ndeduced for the real part $U_l(m,N)$ and the imaginary part $V_l(m,N)$ of\n$X_l(m,N)$, as well as the facts that the $k$th moments of $U_l(m,N)$ and\n$V_l(m,N)$ are equal to zero for every positive integer $k$ which is not\ndivisible by $N/\\gcd(N,l)$. Moreover, some additional assertions and examples\nconcerning the random variables $X_l(m,N)$, $U_l(m,N)$ and $V_l(m,N)$ are also\npresented.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2018 21:44:56 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Me\u0161trovi\u0107", "Romeo", ""]]}, {"id": "1803.02402", "submitter": "Jonathan Yefenof", "authors": "Jonathan Yefenof, Yair Goldberg, Jennifer Wiler, Avishai Mandelbaum\n  and Ya'acov Ritov", "title": "Self-reporting and screening: Data with current-status and censored\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider survival data that combine three types of observations:\nuncensored, right-censored, and left-censored. Such data arises from screening\na medical condition, in situations where self-detection arises naturally. Our\ngoal is to estimate the failure-time distribution, based on these three\nobservation types. We propose a novel methodology for distribution estimation\nusing both parametric and nonparametric techniques. We then evaluate the\nperformance of these estimators via simulated data. Finally, as a case study,\nwe estimate the patience of patients who arrive at an emergency department and\nwait for treatment. Three categories of patients are observed: those who leave\nthe system and announce it, and thus their patience time is observed; those who\nget service and thus their patience time is right-censored by the waiting time;\nand those who leave the system without announcing it. For the third category,\nthe patients' absence is revealed only when they are called to service, which\nis after they have already left; formally, their patience time is\nleft-censored. Other applications of our proposed methodology are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 19:47:26 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Yefenof", "Jonathan", ""], ["Goldberg", "Yair", ""], ["Wiler", "Jennifer", ""], ["Mandelbaum", "Avishai", ""], ["Ritov", "Ya'acov", ""]]}, {"id": "1803.02415", "submitter": "Gregory Cox", "authors": "Gregory Cox", "title": "Almost Sure Uniqueness of a Global Minimum Without Convexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes the argmin of a random objective function to be unique\nalmost surely. This paper first formulates a general result that proves almost\nsure uniqueness without convexity of the objective function. The general result\nis then applied to a variety of applications in statistics. Four applications\nare discussed, including uniqueness of M-estimators, both classical likelihood\nand penalized likelihood estimators, and two applications of the argmin\ntheorem, threshold regression and weak identification.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2018 20:27:28 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 18:19:01 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 13:24:07 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Cox", "Gregory", ""]]}, {"id": "1803.02647", "submitter": "Mazza", "authors": "Corina Ciobotaru, Linard Hoessly, Christian Mazza, Xavier Richard", "title": "Mean field repulsive Kuramoto models: Phase locking and spatial signs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO math.DS math.PR math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The phenomenon of self-synchronization in populations of oscillatory units\nappears naturally in neurosciences. However, in some situations, the formation\nof a coherent state is damaging. In this article we study a repulsive\nmean-field Kuramoto model that describes the time evolution of n points on the\nunit circle, which are transformed into incoherent phase-locked states. It has\nbeen recently shown that such systems can be reduced to a three-dimensional\nsystem of ordinary differential equations, whose mathematical structure is\nstrongly related to hyperbolic geometry. The orbits of the Kuramoto dynamical\nsystem are then described by a ow of M\\\"obius transformations. We show this\nunderlying dynamic performs statistical inference by computing dynamically\nM-estimates of scatter matrices. We also describe the limiting phase-locked\nstates for random initial conditions using Tyler's transformation matrix.\nMoreover, we show the repulsive Kuramoto model performs dynamically not only\nrobust covariance matrix estimation, but also data processing: the initial\nconfiguration of the n points is transformed by the dynamic into a limiting\nphase-locked state that surprisingly equals the spatial signs from\nnonparametric statistics. That makes the sign empirical covariance matrix to\nequal 1 2 id2, the variance-covariance matrix of a random vector that is\nuniformly distributed on the unit circle.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 13:44:45 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Ciobotaru", "Corina", ""], ["Hoessly", "Linard", ""], ["Mazza", "Christian", ""], ["Richard", "Xavier", ""]]}, {"id": "1803.02739", "submitter": "Joshua Mike", "authors": "Joshua Lee Mike and Vasileios Maroulas", "title": "Nonparametric Estimation of Probability Density Functions of Random\n  Persistence Diagrams", "comments": "39 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a nonparametric way to estimate the global probability density\nfunction for a random persistence diagram. Precisely, a kernel density function\ncentered at a given persistence diagram and a given bandwidth is constructed.\nOur approach encapsulates the number of topological features and considers the\nappearance or disappearance of features near the diagonal in a stable fashion.\nIn particular, the structure of our kernel individually tracks long persistence\nfeatures, while considering features near the diagonal as a collective unit.\nThe choice to describe short persistence features as a group reduces\ncomputation time while simultaneously retaining accuracy. Indeed, we prove that\nthe associated kernel density estimate converges to the true distribution as\nthe number of persistence diagrams increases and the bandwidth shrinks\naccordingly. We also establish the convergence of the mean absolute deviation\nestimate, defined according to the bottleneck metric. Lastly, examples of\nkernel density estimation are presented for typical underlying datasets.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 16:16:58 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 00:56:16 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Mike", "Joshua Lee", ""], ["Maroulas", "Vasileios", ""]]}, {"id": "1803.02800", "submitter": "Sebastian Roch", "authors": "Sebastien Roch and Michael Nute and Tandy Warnow", "title": "Long-branch attraction in species tree estimation: inconsistency of\n  partitioned likelihood and topology-based summary methods", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.CE math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With advances in sequencing technologies, there are now massive amounts of\ngenomic data from across all life, leading to the possibility that a robust\nTree of Life can be constructed. However, \"gene tree heterogeneity\", which is\nwhen different genomic regions can evolve differently, is a common phenomenon\nin multi-locus datasets, and reduces the accuracy of standard methods for\nspecies tree estimation that do not take this heterogeneity into account. New\nmethods have been developed for species tree estimation that specifically\naddress gene tree heterogeneity, and that have been proven to converge to the\ntrue species tree when the number of loci and number of sites per locus both\nincrease (i.e., the methods are said to be \"statistically consistent\"). Yet,\nlittle is known about the biologically realistic condition where the number of\nsites per locus is bounded. We show that when the sequence length of each locus\nis bounded (by any arbitrarily chosen value), the most common approaches to\nspecies tree estimation that take heterogeneity into account (i.e., traditional\nfully partitioned concatenated maximum likelihood and newer approaches, called\nsummary methods, that estimate the species tree by combining gene trees) are\nnot statistically consistent, even when the heterogeneity is extremely\nconstrained. The main challenge is the presence of conditions such as long\nbranch attraction that create biased tree estimation when the number of sites\nis restricted. Hence, our study uncovers a fundamental challenge to species\ntree estimation using both traditional and new methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2018 18:19:21 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Roch", "Sebastien", ""], ["Nute", "Michael", ""], ["Warnow", "Tandy", ""]]}, {"id": "1803.02945", "submitter": "Francesco Buscemi", "authors": "Francesco Buscemi", "title": "Comparison of Noisy Channels and Reverse Data-Processing Theorems", "comments": "5 two-column pages, 3 figures; presented at 2017 IEEE Information\n  Theory Workshop, Kaohsiung", "journal-ref": "2017 IEEE Information Theory Workshop (ITW), pp. 489-493", "doi": "10.1109/ITW.2017.8278038", "report-no": null, "categories": "cs.IT math.IT math.ST quant-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the comparison of noisy channels from the viewpoint of\nstatistical decision theory. Various orderings are discussed, all formalizing\nthe idea that one channel is \"better\" than another for information\ntransmission. The main result is an equivalence relation that is proved for\nclassical channels, quantum channels with classical encoding, and quantum\nchannels with quantum encoding.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 02:43:24 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Buscemi", "Francesco", ""]]}, {"id": "1803.03051", "submitter": "Jesper M{\\o}ller", "authors": "Jesper M{\\o}ller and Francisco Cuevas-Pacheco", "title": "Log Gaussian Cox processes on the sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A log Gaussian Cox process (LGCP) is a doubly stochastic construction\nconsisting of a Poisson point process with a random log-intensity given by a\nGaussian random field. Statistical methodology have mainly been developed for\nLGCPs defined in the $d$-dimensional Euclidean space. This paper concerns the\ncase of LGCPs on the $d$-dimensional sphere, with $d=2$ of primary interest. We\ndiscuss the existence problem of such LGCPs, provide sufficient existence\nconditions, and establish further useful theoretical properties. The results\nare applied for the description of sky positions of galaxies, in comparison\nwith previous analysis based on a Thomas process, using simple estimation\nprocedures and making a careful model checking. We account for inhomogeneity in\nour models, and as the model checking is based on a thinning procedure which\nproduces homogeneous/isotropic LGCPs, we discuss its sensitivity.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 11:48:55 GMT"}, {"version": "v2", "created": "Sat, 5 May 2018 10:19:39 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["M\u00f8ller", "Jesper", ""], ["Cuevas-Pacheco", "Francisco", ""]]}, {"id": "1803.03154", "submitter": "Ahmed Bensalma", "authors": "Ahmed Bensalma (ENSSEA)", "title": "Two Distinct Seasonally Fractionally Differenced Periodic Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is devoted to study the effects of the S-periodical fractional\ndifferencing filter $(1-L^S)^{D_t}$. To put this effect in evidence, we have\nderived the periodic auto-covariance functions of two distinct univariate\nseasonally fractionally differenced periodic models. A multivariate\nrepresentation of periodically correlated process is exploited to provide the\nexact and approximated expression auto-covariance of each models. The\ndistinction between the models is clearly obvious through the expression of\nperiodic autocovariance function. Besides producing different autocovariance\nfunctions, the two models differ in their implications. In the first model, the\nseasons of the multivariate series are separately fractionally integrated. In\nthe second model, however, the seasons for the univariate series are\nfractionally co-integrated. On the simulated sample, for each models, with the\nsame parameters, the empirical periodic autocovariance are calculated and\ngraphically represented for illustrating the results and support the comparison\nbetween the two models.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2018 15:32:15 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Bensalma", "Ahmed", "", "ENSSEA"]]}, {"id": "1803.03348", "submitter": "Subhabrata Majumdar", "authors": "Subhabrata Majumdar, George Michailidis", "title": "Joint Estimation and Inference for Data Integration Problems based on\n  Multiple Multi-layered Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of high-throughput technologies has enabled the\ngeneration of data from biological or disease processes that span multiple\nlayers, like genomic, proteomic or metabolomic data, and further pertain to\nmultiple sources, like disease subtypes or experimental conditions. In this\nwork, we propose a general statistical framework based on Gaussian graphical\nmodels for horizontal (i.e. across conditions or subtypes) and vertical (i.e.\nacross different layers containing data on molecular compartments) integration\nof information in such datasets. We start with decomposing the multi-layer\nproblem into a series of two-layer problems. For each two-layer problem, we\nmodel the outcomes at a node in the lower layer as dependent on those of other\nnodes in that layer, as well as all nodes in the upper layer. We use a\ncombination of neighborhood selection and group-penalized regression to obtain\nsparse estimates of all model parameters. Following this, we develop a\ndebiasing technique and asymptotic distributions of inter-layer directed edge\nweights that utilize already computed neighborhood selection coefficients for\nnodes in the upper layer. Subsequently, we establish global and simultaneous\ntesting procedures for these edge weights. Performance of the proposed\nmethodology is evaluated on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 01:30:04 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 22:51:36 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Majumdar", "Subhabrata", ""], ["Michailidis", "George", ""]]}, {"id": "1803.03450", "submitter": "Keisuke Yano", "authors": "Keisuke Yano, Kengo Kato", "title": "On frequentist coverage errors of Bayesian credible sets in moderately\n  high dimensions", "comments": "53 pages; the supplement is included in pp. 32-53", "journal-ref": "Bernoulli 2020, Vol. 26, No. 1, 616-641", "doi": "10.3150/19-BEJ1142", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study frequentist coverage errors of Bayesian credible sets\nfor an approximately linear regression model with (moderately) high dimensional\nregressors, where the dimension of the regressors may increase with but is\nsmaller than the sample size. Specifically, we consider quasi-Bayesian\ninference on the slope vector under the quasi-likelihood with Gaussian error\ndistribution. Under this setup, we derive finite sample bounds on frequentist\ncoverage errors of Bayesian credible rectangles. Derivation of those bounds\nbuilds on a novel Berry--Esseen type bound on quasi-posterior distributions and\nrecent results on high-dimensional CLT on hyperrectangles. We use this general\nresult to quantify coverage errors of Castillo--Nickl and $L^{\\infty}$-credible\nbands for Gaussian white noise models, linear inverse problems, and (possibly\nnon-Gaussian) nonparametric regression models. In particular, we show that\nBayesian credible bands for those nonparametric models have coverage errors\ndecaying polynomially fast in the sample size, implying advantages of Bayesian\ncredible bands over confidence bands based on extreme value theory.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 10:11:50 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 00:47:46 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2019 02:04:08 GMT"}], "update_date": "2019-12-06", "authors_parsed": [["Yano", "Keisuke", ""], ["Kato", "Kengo", ""]]}, {"id": "1803.03469", "submitter": "Dogyoon Song", "authors": "Devavrat Shah and Dogyoon Song", "title": "Deconvolution with Unknown Error Distribution Interpreted as Blind\n  Isotonic Regression", "comments": "25 pages + 40 pages (appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconvolution is a statistical inverse problem to estimate the distribution\nof a random variable based on its noisy observations. Despite the extensive\nstudies on the topic, deconvolution with unknown noise distribution remains as\na notoriously hard problem. We propose a matrix-based viewpoint for collective\ndeconvolution that subsumes the setup with repeated measurements as a special\ncase. As the main result, we describe a simple algorithm that partially\nutilizes matrix structure to solve deconvolution problem and provide\nnon-asymptotic error analysis for the algorithm. We show that the proposed\nalgorithm achieves the minimax optimal rate for deconvolution in a restricted\nsense. We also remark the connection between the collective deconvolution and\nthe so-called statistical seriation as a byproduct or our matrix viewpoint. We\nconjecture that the link suggests that collective deconvolution, as well as\ndeconvolution with repeated measurements, is intrinsically much easier than\nusual deconvolution of a single distribution.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2018 11:19:53 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 22:36:52 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2020 21:17:43 GMT"}], "update_date": "2020-04-06", "authors_parsed": [["Shah", "Devavrat", ""], ["Song", "Dogyoon", ""]]}, {"id": "1803.03803", "submitter": "Thomas Deschatre", "authors": "Deschatre Thomas and F\\'eron Olivier and Hoffmann Marc", "title": "Estimating fast mean-reverting jumps in electricity market models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on empirical evidence of fast mean-reverting spikes, we model\nelectricity price processes $X+Z^\\beta$ as the sum of a continuous It\\^o\nsemimartingale $X$ and a a mean-reverting compound Poisson process $Z_t^\\beta =\n\\int_0^t \\int_{\\mathbb{R}} xe^{-\\beta(t-s)}\\underline{p}(ds,dt)$ where\n$\\underline{p}(ds,dt)$ is Poisson random measure with intensity $\\lambda\nds\\otimes dt$. In a first part, we investigate the estimation of\n$(\\lambda,\\beta)$ from discrete observations and establish asymptotic\nefficiency in various asymptotic settings. In a second part, we discuss the use\nof our inference results for correcting the value of forward contracts on\nelectricity markets in presence of spikes. We implement our method on real data\nin the French, Greman and Australian market over 2015 and 2016 and show in\nparticular the effect of spike modelling on the valuation of certain strip\noptions. In particular, we show that some out-of-the-money options have a\nsignificant value if we incorporate spikes in our modelling, while having a\nvalue close to $0$ otherwise.\n", "versions": [{"version": "v1", "created": "Sat, 10 Mar 2018 13:14:11 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 14:31:58 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Thomas", "Deschatre", ""], ["Olivier", "F\u00e9ron", ""], ["Marc", "Hoffmann", ""]]}, {"id": "1803.03895", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Reduction of Restricted Maximum Likelihood for Random Coefficient Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The restricted maximum likelihood (REML) estimator of the dispersion matrix\nfor random coefficient models is rewritten in terms of the sufficient\nstatistics of the individual regressions.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:01:17 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.03896", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Improved Asymptotics for Zeros of Kernel Estimates via a Reformulation\n  of the Leadbetter-Cryer Integral", "comments": null, "journal-ref": "Statistics & Probability Letters Volume 32, Issue 4, 1 April 1997,\n  Pages 351-356", "doi": null, "report-no": null, "categories": "stat.ME eess.SP math.PR math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The expected number of false inflection points of kernel smoothers is\nevaluated. To obtain the small noise limit, we use a reformulation of the\nLeadbetter-Cryer integral for the expected number of zero crossings of a\ndifferentiable Gaussian process.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:07:28 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.03898", "submitter": "Wei Li", "authors": "Wei Li and Subhashis Ghosal", "title": "Posterior Contraction and Credible Sets for Filaments of Regression\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A filament consists of local maximizers of a smooth function $f$ when moving\nin a certain direction. A filamentary structure is an important feature of the\nshape of an object and is also considered as an important lower dimensional\ncharacterization of multivariate data. There have been some recent theoretical\nstudies of filaments in the nonparametric kernel density estimation context.\nThis paper supplements the current literature in two ways. First, we provide a\nBayesian approach to the filament estimation in regression context and study\nthe posterior contraction rates using a finite random series of B-splines\nbasis. Compared with the kernel-estimation method, this has a theoretical\nadvantage as the bias can be better controlled when the function is smoother,\nwhich allows obtaining better rates. Assuming that $f: \\mathbb{R}^2 \\mapsto\n\\mathbb{R}$ belongs to an isotropic H\\\"{o}lder class of order $\\alpha \\geq 4$,\nwith the optimal choice of smoothing parameters, the posterior contraction\nrates for the filament points on some appropriately defined integral curves and\nfor the Hausdorff distance of the filament are both $(n/\\log\nn)^{(2-\\alpha)/(2(1+\\alpha))}$. Secondly, we provide a way to construct a\ncredible set with sufficient frequentist coverage for the filaments. We\ndemonstrate the success of our proposed method in simulations and one\napplication to earthquake data.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:24:21 GMT"}, {"version": "v2", "created": "Thu, 17 Jan 2019 18:53:58 GMT"}, {"version": "v3", "created": "Wed, 25 Mar 2020 03:45:22 GMT"}], "update_date": "2020-03-26", "authors_parsed": [["Li", "Wei", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1803.03899", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Piecewise Convex Function Estimation: Pilot Estimators", "comments": "PDF on https://projecteuclid.org/download/pdf_1/euclid.aos/1030741086", "journal-ref": "Annals of Statistics Volume 25, Number 6 (1997), 2592-2606", "doi": null, "report-no": null, "categories": "stat.ME eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given noisy data, function estimation is considered when the unknown function\nis known a priori to consist of a small number of regions where the function is\neither convex or concave. When the number of regions is unknown, the model\nselection problem is to determine the number of convexity change points. For\nkernel estimates in Gaussian noise, the number of false change points is\nevaluated as a function of the smoothing parameter. To ensure that the number\nof false convexity change points tends to zero, the smoothing level must be\nlarger than is generically optimal for minimizing the mean integrated square\nerror (MISE). A two-stage estimator is proposed and shown to achieve the\noptimal rate of convergence of the MISE. In the first-stage, the number and\nlocation of the change points is estimated using strong smoothing. In the\nsecond-stage, a constrained smoothing spline fit is performed with the\nsmoothing level chosen to minimize the MISE. The imposed constraint is that a\nsingle change point occur in a region about each empirical change point from\nthe first-stage estimate. This constraint is equivalent to the requirement that\nthe third derivative of the second-stage estimate have a single sign in a small\nneighborhood about each first-stage change point. The change points from the\nsecond-stage are in a neighborhood of the first-stage change points, but need\nnot be at the identical locations.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:28:48 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.03901", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Piecewise Convex Function Estimation: Representations, Duality and Model\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SP eess.SY math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider spline estimates which preserve prescribed piecewise convex\nproperties of the unknown function. A robust version of the penalized\nlikelihood is given and shown to correspond to a variable halfwidth kernel\nsmoother where the halfwidth adaptively decreases in regions of rapid change of\nthe unknown function. When the convexity change points are prescribed, we\nderive representation results and smoothness properties of the estimates. A\ndual formulation is given which reduces the estimate is reduced to a finite\ndimensional convex optimization in the dual space.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:38:39 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.03903", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Piecewise Convex Function Estimation and Model Selection", "comments": "arXiv admin note: text overlap with arXiv:1803.03901", "journal-ref": "Approximation Theory Viii - Volume 1: Approximation And\n  Interpolation edited by Chui Charles K, Schumaker Larry L 1995 by World\n  Scientific Publishing", "doi": null, "report-no": null, "categories": "stat.ME cs.LG eess.SP math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given noisy data, function estimation is considered when the unknown function\nis known apriori to consist of a small number of regions where the function is\neither convex or concave. When the regions are known apriori, the estimate is\nreduced to a finite dimensional convex optimization in the dual space. When the\nnumber of regions is unknown, the model selection problem is to determine the\nnumber of convexity change points. We use a pilot estimator based on the\nexpected number of false inflection points.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:45:57 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.03906", "submitter": "Kurt Riedel", "authors": "Alexander Sidorenko, Kurt S. Riedel", "title": "Adaptive Kernel Estimation of the Spectral Density with Boundary Kernel\n  Analysis", "comments": null, "journal-ref": "Approximation Theory VIII: Approximation And Interpolation, pg\n  519-528, edited by Chui, Schumaker, 1995 World Scientific", "doi": null, "report-no": null, "categories": "stat.ME cs.CV eess.AS eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hybrid estimator of the log-spectral density of a stationary time series is\nproposed. First, a multiple taper estimate is performed, followed by kernel\nsmoothing the log-multitaper estimate. This procedure reduces the expected mean\nsquare error by $({\\pi^2 \\over 4})^{.8}$ over simply smoothing the log tapered\nperiodogram. The optimal number of tapers is $O(N^{8/15})$. A data adaptive\nimplementation of a variable bandwidth kernel smoother is given. When the\nspectral density is discontinuous, one sided smoothing estimates are used.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 05:14:42 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Sidorenko", "Alexander", ""], ["Riedel", "Kurt S.", ""]]}, {"id": "1803.03908", "submitter": "Kurt Riedel", "authors": "Andrew P. Mullhaupt, Kurt S. Riedel", "title": "Fast Adaptive Identification of Stable Innovation Filters", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, Volume: 45, Issue: 10, Oct\n  1997, pgs. 2616 - 2619", "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SP eess.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The adaptive identification of the impulse response of an innovation filter\nis considered. The impulse response is a finite sum of known basis functions\nwith unknown coefficients. These unknown coefficients are estimated using a\npseudolinear regression. This estimate is implemented using a square root\nalgorithm based on a displacement rank structure. When the initial conditions\nhave low displacement rank, the filter update is $O(n)$. If the filter\narchitecture is chosen to be triangular input balanced, the estimation problem\nis well-conditioned and a simple, low rank initialization is available.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 05:21:20 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Mullhaupt", "Andrew P.", ""], ["Riedel", "Kurt S.", ""]]}, {"id": "1803.03995", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel, A. Sidorenko", "title": "Adaptive Smoothing of the Log-Spectrum with Multiple Tapering", "comments": null, "journal-ref": "IEEE Trans. Signal Process., vol. 44, no. 7, pp. 1794-1800, Jul.\n  1996", "doi": "10.1109/78.510625", "report-no": null, "categories": "stat.ME eess.AS eess.SP math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hybrid estimator of the log-spectral density of a stationary time series is\nproposed. First, a multiple taper estimate is performed, followed by kernel\nsmoothing the log-multiple taper estimate. This procedure reduces the expected\nmean square error by $(\\pi^2/ 4)^{4/5} $ over simply smoothing the log tapered\nperiodogram. A data adaptive implementation of a variable bandwidth kernel\nsmoother is given.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 17:45:08 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Riedel", "Kurt S.", ""], ["Sidorenko", "A.", ""]]}, {"id": "1803.03999", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel, A. Sidorenko", "title": "Function Estimation Using Data Adaptive Kernel Estimation - How Much\n  Smoothing?", "comments": "Available at https://aip.scitation.org/doi/pdf/10.1063/1.4823316", "journal-ref": "Computers in Physics Volume 8 Issue 4, July/Aug. 1994 Pages\n  402-409", "doi": "10.1063/1.4823316", "report-no": null, "categories": "stat.ME cs.AI eess.SP math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We determine the expected error by smoothing the data locally. Then we\noptimize the shape of the kernel smoother to minimize the error. Because the\noptimal estimator depends on the unknown function, our scheme automatically\nadjusts to the unknown function. By self-consistently adjusting the kernel\nsmoother, the total estimator adapts to the data.\n  Goodness of fit estimators select a kernel halfwidth by minimizing a function\nof the halfwidth which is based on the average square residual fit error:\n$ASR(h)$. A penalty term is included to adjust for using the same data to\nestimate the function and to evaluate the mean square error. Goodness of fit\nestimators are relatively simple to implement, but the minimum (of the goodness\nof fit functional) tends to be sensitive to small perturbations. To remedy this\nsensitivity problem, we fit the mean square error %goodness of fit functional\nto a two parameter model prior to determining the optimal halfwidth.\n  Plug-in derivative estimators estimate the second derivative of the unknown\nfunction in an initial step, and then substitute this estimate into the\nasymptotic formula.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 18:03:07 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Riedel", "Kurt S.", ""], ["Sidorenko", "A.", ""]]}, {"id": "1803.04046", "submitter": "Kurt Riedel", "authors": "Andrew Mullhaupt, Kurt Riedel", "title": "Exponential Condition Number of Solutions of the Discrete Lyapunov\n  Equation", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, Volume: 52, Issue: 5, May\n  2004, pgs. 1257 - 1265", "doi": "10.1109/TSP.2004.826177", "report-no": null, "categories": "stat.ME cs.NA cs.SY eess.SY math.NA math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The condition number of the $n\\ x\\ n$ matrix $P$ is examined, where $P$\nsolves %the discete Lyapunov equation, $P - A P A^* = BB^*$, and $B$ is a $n\\\nx\\ d$ matrix. Lower bounds on the condition number, $\\kappa$, of $P$ are given\nwhen $A$ is normal, a single Jordan block or in Frobenius form. The bounds show\nthat the ill-conditioning of $P$ grows as $\\exp(n/d) >> 1$. These bounds are\nrelated to the condition number of the transformation that takes $A$ to input\nnormal form. A simulation shows that $P$ is typically ill-conditioned in the\ncase of $n>>1$ and $d=1$. When $A_{ij}$ has an independent Gaussian\ndistribution (subject to restrictions), we observe that $\\kappa(P)^{1/n} ~=\n3.3$. The effect of auto-correlated forcing on the conditioning on state space\nsystems is examined\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 21:29:54 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Mullhaupt", "Andrew", ""], ["Riedel", "Kurt", ""]]}, {"id": "1803.04075", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel", "title": "Kernel estimation of the instantaneous frequency", "comments": null, "journal-ref": "I.E.E.E. Trans. Signal Processing 42, pp. 2644-2649 (1994)", "doi": "10.1109/78.324730", "report-no": null, "categories": "stat.ME eess.AS eess.SP math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider kernel estimators of the instantaneous frequency of a slowly\nevolving sinusoid in white noise. The expected estimation error consists of two\nterms. The systematic bias error grows as the kernel halfwidth increases while\nthe random error decreases. For a non-modulated signal, $g(t)$, the kernel\nhalfwidth which minimizes the expected error scales as$h \\sim \\left[{ \\sigma^2\n\\over\n  N| \\partial_t^2 g^{}|^2 } \\right]^{1/ 5}$, where %$A^{(\\ell)}$ is the\ncoherent signal at frequency, $f_{\\ell}$, $\\sigma^2$ is the noise variance and\n$N$ is the number of measurements per unit time. We show that estimating the\ninstantaneous frequency corresponds to estimating the first derivative of a\nmodulated signal, $A(t)\\exp(i\\phi(t))$. For instantaneous frequency estimation,\nthe halfwidth which minimizes the expected error is larger: $h_{1,3} \\sim\n\\left[{ \\sigma^2 \\over A^2N| \\partial_t^3 (e^{i \\tilde{\\phi}(t)} )|^2 }\n\\right]^{1/ 7}$. Since the optimal halfwidths depend on derivatives of the\nunknown function, we initially estimate these derivatives prior to estimating\nthe actual signal.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 00:43:32 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Riedel", "Kurt S.", ""]]}, {"id": "1803.04078", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel, Alexander Sidorenko", "title": "Minimum bias multiple taper spectral estimation", "comments": null, "journal-ref": "I.E.E.E. Trans. Signal Processing 43, pp. 188-195 (1995)", "doi": "10.1109/78.365298", "report-no": null, "categories": "stat.ME eess.AS eess.SP math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two families of orthonormal tapers are proposed for multi-taper spectral\nanalysis: minimum bias tapers, and sinusoidal tapers $\\{ \\bf{v}^{(k)}\\}$, where\n$v_n^{(k)}=\\sqrt{\\frac{2}{N+1}}\\sin\\frac{\\pi kn}{N+1}$, and $N$ is the number\nof points. The resulting sinusoidal multitaper spectral estimate is\n$\\hat{S}(f)=\\frac{1}{2K(N+1)} \\sum_{j=1}^K |y(f+\\frac{j}{2N+2})\n-y(f-\\frac{j}{2N+2})|^2$, where $y(f)$ is the Fourier transform of the\nstationary time series, $S(f)$ is the spectral density, and $K$ is the number\nof tapers. For fixed $j$, the sinusoidal tapers converge to the minimum bias\ntapers like $1/N$. Since the sinusoidal tapers have analytic expressions, no\nnumerical eigenvalue decomposition is necessary. Both the minimum bias and\nsinusoidal tapers have no additional parameter for the spectral bandwidth. The\nbandwidth of the $j$th taper is simply $\\frac{1}{N}$ centered about the\nfrequencies $\\frac{\\pm j}{2N+2}$. Thus the bandwidth of the multitaper spectral\nestimate can be adjusted locally by simply adding or deleting tapers. The band\nlimited spectral concentration, $\\int_{-w}^w |V(f)|^2 df$, of both the minimum\nbias and sinusoidal tapers is very close to the optimal concentration achieved\nby the Slepian tapers. In contrast, the Slepian tapers can have the local bias,\n$\\int_{-1/2}^{1/2} f^2 |V(f)|^2 df$, much larger than of the minimum bias\ntapers and the sinusoidal tapers.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 01:01:27 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 03:13:35 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Riedel", "Kurt S.", ""], ["Sidorenko", "Alexander", ""]]}, {"id": "1803.04221", "submitter": "Jennifer Wadsworth", "authors": "Sebastian Engelke, Thomas Opitz and Jennifer Wadsworth", "title": "Extremal dependence of random scale constructions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bivariate random vector can exhibit either asymptotic independence or\ndependence between the largest values of its components. When used as a\nstatistical model for risk assessment in fields such as finance, insurance or\nmeteorology, it is crucial to understand which of the two asymptotic regimes\noccurs. Motivated by their ubiquity and flexibility, we consider the extremal\ndependence properties of vectors with a random scale construction\n$(X_1,X_2)=R(W_1,W_2)$, with non-degenerate $R>0$ independent of $(W_1,W_2)$.\nFocusing on the presence and strength of asymptotic tail dependence, as\nexpressed through commonly-used summary parameters, broad factors that affect\nthe results are: the heaviness of the tails of $R$ and $(W_1,W_2)$, the shape\nof the support of $(W_1,W_2)$, and dependence between $(W_1,W_2)$. When $R$ is\ndistinctly lighter tailed than $(W_1,W_2)$, the extremal dependence of\n$(X_1,X_2)$ is typically the same as that of $(W_1,W_2)$, whereas similar or\nheavier tails for $R$ compared to $(W_1,W_2)$ typically result in increased\nextremal dependence. Similar tail heavinesses represent the most interesting\nand technical cases, and we find both asymptotic independence and dependence of\n$(X_1,X_2)$ possible in such cases when $(W_1,W_2)$ exhibit asymptotic\nindependence. The bivariate case often directly extends to higher-dimensional\nvectors and spatial processes, where the dependence is mainly analyzed in terms\nof summaries of bivariate sub-vectors. The results unify and extend many\nexisting examples, and we use them to propose new models that encompass both\ndependence classes.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 12:40:11 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2019 09:01:45 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Engelke", "Sebastian", ""], ["Opitz", "Thomas", ""], ["Wadsworth", "Jennifer", ""]]}, {"id": "1803.04353", "submitter": "Yuqi Gu", "authors": "Yuqi Gu and Gongjun Xu", "title": "Partial Identifiability of Restricted Latent Class Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent class models have wide applications in social and biological sciences.\nIn many applications, pre-specified restrictions are imposed on the parameter\nspace of latent class models, through a design matrix, to reflect\npractitioners' assumptions about how the observed responses depend on subjects'\nlatent traits. Though widely used in various fields, such restricted latent\nclass models suffer from non-identifiability due to their discreteness nature\nand complex structure of restrictions. This work addresses the fundamental\nidentifiability issue of restricted latent class models by developing a general\nframework for strict and partial identifiability of the model parameters. Under\ncorrect model specification, the developed identifiability conditions only\ndepend on the design matrix and are easily checkable, which provide useful\npractical guidelines for designing statistically valid diagnostic tests.\nFurthermore, the new theoretical framework is applied to establish, for the\nfirst time, identifiability of several designs from cognitive diagnosis\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 16:23:08 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 05:06:45 GMT"}, {"version": "v3", "created": "Fri, 31 May 2019 01:20:42 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Gu", "Yuqi", ""], ["Xu", "Gongjun", ""]]}, {"id": "1803.04362", "submitter": "Kai Wang", "authors": "Kai Wang and Yanling Zhu", "title": "M-estimation in high-dimensional linear model", "comments": "16 pages,3 tables", "journal-ref": "Journal of Inequalities and Applications (2018) 2018:225", "doi": "10.1186/s13660-018-1819-3", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We mainly study the M-estimation method for the high-dimensional linear\nregression model, and discuss the properties of M-estimator when the penalty\nterm is the local linear approximation. In fact, M-estimation method is a\nframework, which covers the methods of the least absolute deviation, the\nquantile regression, least squares regression and Huber regression. We show\nthat the proposed estimator possesses the good properties by applying certain\nassumptions. In the part of numerical simulation, we select the appropriate\nalgorithm to show the good robustness of this method\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 16:37:11 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Wang", "Kai", ""], ["Zhu", "Yanling", ""]]}, {"id": "1803.04464", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Hamid Javadi", "title": "False Discovery Rate Control via Debiased Lasso", "comments": "accepted for publication in the Electronic Journal of statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variable selection in high-dimensional statistical\nmodels where the goal is to report a set of variables, out of many predictors\n$X_1, \\dotsc, X_p$, that are relevant to a response of interest. For linear\nhigh-dimensional model, where the number of parameters exceeds the number of\nsamples $(p>n)$, we propose a procedure for variables selection and prove that\nit controls the \"directional\" false discovery rate (FDR) below a pre-assigned\nsignificance level $q\\in [0,1]$. We further analyze the statistical power of\nour framework and show that for designs with subgaussian rows and a common\nprecision matrix $\\Omega\\in\\mathbb{R}^{p\\times p}$, if the minimum nonzero\nparameter $\\theta_{\\min}$ satisfies $$\\sqrt{n} \\theta_{\\min} - \\sigma\n\\sqrt{2(\\max_{i\\in [p]}\\Omega_{ii})\\log\\left(\\frac{2p}{qs_0}\\right)} \\to\n\\infty\\,,$$ then this procedure achieves asymptotic power one. Our framework is\nbuilt upon the debiasing approach and assumes the standard condition $s_0 =\no(\\sqrt{n}/(\\log p)^2)$, where $s_0$ indicates the number of true positives\namong the $p$ features. Notably, this framework achieves exact directional FDR\ncontrol without any assumption on the amplitude of unknown regression\nparameters, and does not require any knowledge of the distribution of\ncovariates or the noise level. We test our method in synthetic and real data\nexperiments to assess its performance and to corroborate our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 19:03:33 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 06:14:48 GMT"}], "update_date": "2019-03-20", "authors_parsed": [["Javanmard", "Adel", ""], ["Javadi", "Hamid", ""]]}, {"id": "1803.04484", "submitter": "Bardia Panahbehagh Ph.D.", "authors": "Bardia Panahbehagh, Afshin Parvardeh and Babak Mohammadi", "title": "Adaptive two-stage sequential double sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many surveys inexpensive auxiliary variables are available that can help\nus to make more precise estimation about the main variable. Using auxiliary\nvariable has been extended by regression estimators for rare and cluster\npopulations. In conventional regression estimator it is assumed that the mean\nof auxiliary variable in the population is known. In many surveys we don't have\nsuch wide information about auxiliary variable. In this paper we present a\nmulti-phase variant of two-stage sequential sampling based on an inexpensive\nauxiliary variable associated with the survey variable in the form of double\nsampling. The auxiliary variable will be used in both design and estimation\nstage. The population mean is estimated by a modified regression-type estimator\nwith two different coefficient. Results will be investigated using some\nsimulations following Median and Thompson (2004).\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 19:38:52 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Panahbehagh", "Bardia", ""], ["Parvardeh", "Afshin", ""], ["Mohammadi", "Babak", ""]]}, {"id": "1803.04547", "submitter": "Zhixin Zhou", "authors": "Zhixin Zhou and Arash A. Amini", "title": "Analysis of spectral clustering algorithms for community detection: the\n  general bipartite setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider spectral clustering algorithms for community detection under a\ngeneral bipartite stochastic block model (SBM). A modern spectral clustering\nalgorithm consists of three steps: (1) regularization of an appropriate\nadjacency or Laplacian matrix (2) a form of spectral truncation and (3) a\nk-means type algorithm in the reduced spectral domain. We focus on the\nadjacency-based spectral clustering and for the first step, propose a new\ndata-driven regularization that can restore the concentration of the adjacency\nmatrix even for the sparse networks. This result is based on recent work on\nregularization of random binary matrices, but avoids using unknown population\nlevel parameters, and instead estimates the necessary quantities from the data.\nWe also propose and study a novel variation of the spectral truncation step and\nshow how this variation changes the nature of the misclassification rate in a\ngeneral SBM. We then show how the consistency results can be extended to models\nbeyond SBMs, such as inhomogeneous random graph models with approximate\nclusters, including a graphon clustering problem, as well as general\nsub-Gaussian biclustering. A theme of the paper is providing a better\nunderstanding of the analysis of spectral methods for community detection and\nestablishing consistency results, under fairly general clustering models and\nfor a wide regime of degree growths, including sparse cases where the average\nexpected degree grows arbitrarily slowly.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 21:50:58 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 23:24:58 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zhou", "Zhixin", ""], ["Amini", "Arash A.", ""]]}, {"id": "1803.04550", "submitter": "Fernando Gama", "authors": "Fernando Gama, Alejandro Ribeiro", "title": "Ergodicity in Stationary Graph Processes: A Weak Law of Large Numbers", "comments": "Submitted to IEEE Transactions on Signal Processing", "journal-ref": null, "doi": "10.1109/TSP.2019.2908909", "report-no": null, "categories": "eess.SP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For stationary signals in time the weak law of large numbers (WLLN) states\nthat ensemble and realization averages are within e of each other with a\nprobability of order O(1/Ne^2) when considering N signal components. The graph\nWLLN introduced in this paper shows that the same is essentially true for\nsignals supported on graphs. However, the notions of stationarity, ensemble\nmean, and realization mean are different. Recent papers have defined graph\nstationary signals as those that satisfy a form of invariance with respect to\ngraph diffusion. The ensemble mean of a graph stationary signal is not a\nconstant but a node-varying signal whose structure depends on the spectral\nproperties of the graph. The realization average of a graph signal is defined\nhere as an average of successive weighted averages of local signal values with\nsignal values of neighboring nodes. The graph WLLN shows that this two\nnode-varying signals are within e of each other with probability of order\nO(1/Ne^2) in at least some nodes. In stationary time signals, the realization\naverage is not only a consistent estimator of the ensemble mean but also\noptimal in terms of mean squared error (MSE). This is not true of graph\nsignals. Optimal MSE graph filter designs are also presented. An example\nproblem concerning the estimation of the mean of a Gaussian random field is\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2018 22:00:29 GMT"}, {"version": "v2", "created": "Fri, 29 Mar 2019 21:14:00 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Gama", "Fernando", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "1803.04839", "submitter": "Kayanan Manickavasagar", "authors": "Manickavasagar Kayanan and Pushpakanthie Wijekoon", "title": "Optimal estimators in misspecified linear regression model with an\n  application to real-world data", "comments": "18 pages, 8 figures", "journal-ref": "Kayanan M. & Wijekoon P. (2018). Optimal estimators in\n  misspecified linear regression model with an application to real-world data,\n  Communications in Statistics: Case Studies, Data Analysis and Applications,\n  4:3-4, 151-163", "doi": "10.1080/23737484.2018.1536863", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose the Sample Information Optimal Estimator (SIOE)\nand the Stochastic Restricted Optimal Estimator (SROE) for misspecified linear\nregression model when multicollinearity exists among explanatory variables.\nFurther, we obtain the superiority conditions of proposed estimators over some\nother existing estimators in the Mean Square Error Matrix (MSEM) criterion in a\nstandard form which can apply to all estimators considered in this study.\nFinally, a real world example and a Monte Carlo simulation study are presented\nfor the proposed estimators to illustrate the theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2018 04:12:04 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 10:43:18 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 11:10:43 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Kayanan", "Manickavasagar", ""], ["Wijekoon", "Pushpakanthie", ""]]}, {"id": "1803.04853", "submitter": "Vivien Goepp", "authors": "Vivien Goepp (MAP5 - UMR 8145, UPD5, UPD5 Math\\'ematiques\n  Informatique), Jean-Christophe Thalabard (MAP5 - UMR 8145, UPD5, USPC, UPD5\n  M\\'edecine), Gr\\'egory Nuel (LPMA), Olivier Bouaziz (MAP5 - UMR 8145, UPD5,\n  UPD5 Math\\'ematiques Informatique, IUT - Paris Descartes)", "title": "Regularized Bidimensional Estimation of the Hazard Rate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiological or demographic studies, with variable age at onset, a\ntypical quantity of interest is the incidence of a disease (for example the\ncancer incidence). In these studies, the individuals are usually highly\nheterogeneous in terms of dates of birth (the cohort) and with respect to the\ncalendar time (the period) and appropriate estimation methods are needed. In\nthis article a new estimation method is presented which extends classical\nage-period-cohort analysis by allowing interactions between age, period and\ncohort effects. This paper introduces a bidimensional regularized estimate of\nthe hazard rate where a penalty is introduced on the likelihood of the model.\nThis penalty can be designed either to smooth the hazard rate or to enforce\nconsecutive values of the hazard to be equal, leading to a parsimonious\nrepresentation of the hazard rate. In the latter case, we make use of an\niterative penalized likelihood scheme to approximate the L0 norm, which makes\nthe computation tractable. The method is evaluated on simulated data and\napplied on breast cancer survival data from the SEER program.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 14:49:54 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 16:53:17 GMT"}, {"version": "v3", "created": "Fri, 12 Jun 2020 11:58:38 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Goepp", "Vivien", "", "MAP5 - UMR 8145, UPD5, UPD5 Math\u00e9matiques\n  Informatique"], ["Thalabard", "Jean-Christophe", "", "MAP5 - UMR 8145, UPD5, USPC, UPD5\n  M\u00e9decine"], ["Nuel", "Gr\u00e9gory", "", "LPMA"], ["Bouaziz", "Olivier", "", "MAP5 - UMR 8145, UPD5,\n  UPD5 Math\u00e9matiques Informatique, IUT - Paris Descartes"]]}, {"id": "1803.04859", "submitter": "Lioudmila Vostrikova", "authors": "Paavo Salminen, Lioudmila Vostrikova (LAREMA)", "title": "On moments of integral exponential functionals of additive processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For real-valued additive process $(X\\_t)\\_{t\\geq 0}$ a recursive equation is\nderived for the entire positive moments of functionals $$I\\_{s,t}= \\int\n\\_s^t\\exp(-X\\_u)du, \\quad 0\\leq s<t\\leq\\infty, $$ in case the Laplace exponent\nof $X\\_t$ exists for positive values of the parameter. From the equation\nemergesan easy-to-apply sufficient condition for the finiteness of the moments.\nAs an application we study first hitprocesses of diffusions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2018 14:57:08 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 14:15:24 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 10:35:25 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Salminen", "Paavo", "", "LAREMA"], ["Vostrikova", "Lioudmila", "", "LAREMA"]]}, {"id": "1803.05087", "submitter": "Kurt Riedel", "authors": "Kurt S. Riedel, Kaya Imre", "title": "Smoothing Spline Growth Curves With Covariates", "comments": null, "journal-ref": "Comm. in Statistics 22, pp. 1795-1818 (1993)", "doi": null, "report-no": null, "categories": "stat.ME math.ST physics.data-an physics.plasm-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt the interactive spline model of Wahba to growth curves with\ncovariates. The smoothing spline formulation permits a non-parametric\nrepresentation of the growth curves. In the limit when the discretization error\nis small relative to the estimation error, the resulting growth curve estimates\noften depend only weakly on the number and locations of the knots. The\nsmoothness parameter is determined from the data by minimizing an empirical\nestimate of the expected error. We show that the risk estimate of Craven and\nWahba is a weighted goodness of fit estimate. A modified loss estimate is\ngiven, where $\\sigma^2$ is replaced by its unbiased estimate.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 00:32:19 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Riedel", "Kurt S.", ""], ["Imre", "Kaya", ""]]}, {"id": "1803.05130", "submitter": "Kurt Riedel", "authors": "Kurt Riedel", "title": "Signal Processing and Piecewise Convex Estimation", "comments": null, "journal-ref": "ICIAM Proceedings 1993", "doi": null, "report-no": null, "categories": "stat.ME eess.SP math.ST physics.data-an stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems on signal processing reduce to nonparametric function\nestimation. We propose a new methodology, piecewise convex fitting (PCF), and\ngive a two-stage adaptive estimate. In the first stage, the number and location\nof the change points is estimated using strong smoothing. In the second stage,\na constrained smoothing spline fit is performed with the smoothing level chosen\nto minimize the MSE. The imposed constraint is that a single change point\noccurs in a region about each empirical change point of the first-stage\nestimate. This constraint is equivalent to requiring that the third derivative\nof the second-stage estimate has a single sign in a small neighborhood about\neach first-stage change point. We sketch how PCF may be applied to signal\nrecovery, instantaneous frequency estimation, surface reconstruction, image\nsegmentation, spectral estimation and multivariate adaptive regression.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 04:17:21 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Riedel", "Kurt", ""]]}, {"id": "1803.05299", "submitter": "Fatma Zehra Do\\u{g}ru", "authors": "Fatma Zehra Do\\u{g}ru, Olcay Arslan", "title": "Joint Modelling of Location, Scale and Skewness Parameters of the Skew\n  Laplace Normal Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose joint location, scale and skewness models of the\nskew Laplace normal (SLN) distribution as an alternative model for joint\nmodelling location, scale and skewness models of the skew-t-normal (STN)\ndistribution when the data set contains both asymmetric and heavy-tailed\nobservations. We obtain the maximum likelihood (ML) estimators for the\nparameters of the joint location, scale and skewness models of the SLN\ndistribution using the expectation-maximization (EM) algorithm. The performance\nof the proposed model is demonstrated by a simulation study and a real data\nexample.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 14:19:00 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Do\u011fru", "Fatma Zehra", ""], ["Arslan", "Olcay", ""]]}, {"id": "1803.05408", "submitter": "Antoine Lejay", "authors": "Antoine Lejay (TOSCA, IECL), Paolo Pigato (WIAS)", "title": "Maximum likelihood drift estimation for a threshold diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum likelihood estimator of the drift parameters of a\nstochastic differential equation, with both drift and diffusion coefficients\nconstant on the positive and negative axis, yet discontinuous at zero. This\nthreshold diffusion is called drifted Oscillating Brownian motion.For this\ncontinuously observed diffusion, the maximum likelihood estimator coincide with\na quasi-likelihood estimator with constant diffusion term. We show that this\nestimator is the limit, as observations become dense in time, of the\n(quasi)-maximum likelihood estimator based on discrete observations. In long\ntime, the asymptotic behaviors of the positive and negative occupation times\nrule the ones of the estimators. Differently from most known results in the\nliterature, we do not restrict ourselves to the ergodic framework: indeed,\ndepending on the signs of the drift, the process may be ergodic, transient or\nnull recurrent. For each regime, we establish whether or not the estimators are\nconsistent; if they are, we prove the convergence in long time of the properly\nrescaled difference of the estimators towards a normal or mixed normal\ndistribution. These theoretical results are backed by numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2018 17:11:34 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 14:26:27 GMT"}, {"version": "v3", "created": "Wed, 21 Aug 2019 13:16:32 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Lejay", "Antoine", "", "TOSCA, IECL"], ["Pigato", "Paolo", "", "WIAS"]]}, {"id": "1803.05784", "submitter": "St\\'ephane Ga\\\"iffas", "authors": "Jaouad Mourtada, St\\'ephane Ga\\\"iffas, Erwan Scornet", "title": "Minimax optimal rates for Mondrian trees and forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduced by Breiman, Random Forests are widely used classification and\nregression algorithms. While being initially designed as batch algorithms,\nseveral variants have been proposed to handle online learning. One particular\ninstance of such forests is the \\emph{Mondrian Forest}, whose trees are built\nusing the so-called Mondrian process, therefore allowing to easily update their\nconstruction in a streaming fashion. In this paper, we provide a thorough\ntheoretical study of Mondrian Forests in a batch learning setting, based on new\nresults about Mondrian partitions. Our results include consistency and\nconvergence rates for Mondrian Trees and Forests, that turn out to be minimax\noptimal on the set of $s$-H\\\"older function with $s \\in (0,1]$ (for trees and\nforests) and $s \\in (1,2]$ (for forests only), assuming a proper tuning of\ntheir complexity parameter in both cases. Furthermore, we prove that an\nadaptive procedure (to the unknown $s \\in (0, 2]$) can be constructed by\ncombining Mondrian Forests with a standard model aggregation algorithm. These\nresults are the first demonstrating that some particular random forests achieve\nminimax rates \\textit{in arbitrary dimension}. Owing to their remarkably simple\ndistributional properties, which lead to minimax rates, Mondrian trees are a\npromising basis for more sophisticated yet theoretically sound random forests\nvariants.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 14:43:04 GMT"}, {"version": "v2", "created": "Tue, 9 Apr 2019 12:41:45 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Mourtada", "Jaouad", ""], ["Ga\u00efffas", "St\u00e9phane", ""], ["Scornet", "Erwan", ""]]}, {"id": "1803.05793", "submitter": "Luca Rossini", "authors": "Federico Bassetti, Roberto Casarin, Luca Rossini", "title": "Hierarchical Species Sampling Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a general class of hierarchical nonparametric prior\ndistributions. The random probability measures are constructed by a hierarchy\nof generalized species sampling processes with possibly non-diffuse base\nmeasures. The proposed framework provides a general probabilistic foundation\nfor hierarchical random measures with either atomic or mixed base measures and\nallows for studying their properties, such as the distribution of the marginal\nand total number of clusters. We show that hierarchical species sampling models\nhave a Chinese Restaurants Franchise representation and can be used as prior\ndistributions to undertake Bayesian nonparametric inference. We provide a\nmethod to sample from the posterior distribution together with some numerical\nillustrations. Our class of priors includes some new hierarchical mixture\npriors such as the hierarchical Gnedin measures, and other well-known prior\ndistributions such as the hierarchical Pitman-Yor and the hierarchical\nnormalized random measures.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 15:04:51 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Bassetti", "Federico", ""], ["Casarin", "Roberto", ""], ["Rossini", "Luca", ""]]}, {"id": "1803.05836", "submitter": "Raluca Balan", "authors": "Raluca M. Balan and Dina Jankovic", "title": "Asymptotic theory for longitudinal data with missing responses adjusted\n  by inverse probability weights", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a new method for analyzing longitudinal data\nwhich contain responses that are missing at random. This method consists in\nsolving the generalized estimating equation (GEE) of Liang and Zeger (1986) in\nwhich the incomplete responses are replaced by values adjusted using the\ninverse probability weights proposed in Yi, Ma and Carroll (2012). We show that\nthe root estimator is consistent and asymptotically normal, essentially under\nthe some conditions on the marginal distribution and the surrogate correlation\nmatrix as those presented in Xie and Yang (2003) in the case of complete data,\nand under minimal assumptions on the missingness probabilities. This method is\napplied to a real-life dataset taken from Sommer, Katz and Tarwotjo (1984),\nwhich examines the incidence of respiratory disease in a sample of 250\npre-school age Indonesian children which were examined every 3 months for 18\nmonths, using as covariates the age, gender, and vitamin A deficiency.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 16:15:04 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Balan", "Raluca M.", ""], ["Jankovic", "Dina", ""]]}, {"id": "1803.05875", "submitter": "Marianne Clausel", "authors": "Florent Autin and Marianne Clausel and Jean-Marc Freyermuth and\n  Cl\\'ement Marteau", "title": "Maxiset point of view for signal detection in inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the successful maxiset paradigm from function estimation\nto signal detection in inverse problems. In this context, the maxisets do not\nhave the same shape compared to the classical estimation framework.\nNevertheless, we introduce a robustified version of these maxisets, allowing to\nexhibit tail conditions on the signals of interest. Under this novel paradigm\nwe are able to compare direct and indirect testing procedures.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 17:23:09 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Autin", "Florent", ""], ["Clausel", "Marianne", ""], ["Freyermuth", "Jean-Marc", ""], ["Marteau", "Cl\u00e9ment", ""]]}, {"id": "1803.06010", "submitter": "Shannon McCurdy", "authors": "Shannon R. McCurdy", "title": "Ridge Regression and Provable Deterministic Ridge Leverage Score\n  Sampling", "comments": "24 pages, 15 figures. Minor changes such as typos fixed, some\n  background discussion added, references added", "journal-ref": "Advances in Neural Information Processing Systems (NeurIPS 2018),\n  Montreal, Canada", "doi": null, "report-no": null, "categories": "math.ST cs.DS stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ridge leverage scores provide a balance between low-rank approximation and\nregularization, and are ubiquitous in randomized linear algebra and machine\nlearning. Deterministic algorithms are also of interest in the moderately big\ndata regime, because deterministic algorithms provide interpretability to the\npractitioner by having no failure probability and always returning the same\nresults.\n  We provide provable guarantees for deterministic column sampling using ridge\nleverage scores. The matrix sketch returned by our algorithm is a column subset\nof the original matrix, yielding additional interpretability. Like the\nrandomized counterparts, the deterministic algorithm provides (1 + {\\epsilon})\nerror column subset selection, (1 + {\\epsilon}) error projection-cost\npreservation, and an additive-multiplicative spectral bound. We also show that\nunder the assumption of power-law decay of ridge leverage scores, this\ndeterministic algorithm is provably as accurate as randomized algorithms.\n  Lastly, ridge regression is frequently used to regularize ill-posed linear\nleast-squares problems. While ridge regression provides shrinkage for the\nregression coefficients, many of the coefficients remain small but non-zero.\nPerforming ridge regression with the matrix sketch returned by our algorithm\nand a particular regularization parameter forces coefficients to zero and has a\nprovable (1 + {\\epsilon}) bound on the statistical risk. As such, it is an\ninteresting alternative to elastic net regularization.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 21:35:55 GMT"}, {"version": "v2", "created": "Sun, 23 Dec 2018 21:06:18 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["McCurdy", "Shannon R.", ""]]}, {"id": "1803.06011", "submitter": "Joel Middleton", "authors": "Joel A. Middleton", "title": "A Unified Theory of Regression Adjustment for Design-based Inference", "comments": "Working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the Neyman causal model, it is well-known that OLS with\ntreatment-by-covariate interactions cannot harm asymptotic precision of\nestimated treatment effects in completely randomized experiments. But do such\nguarantees extend to experiments with more complex designs? This paper proposes\na general framework for addressing this question and defines a class of\ngeneralized regression estimators that are applicable to experiments of any\ndesign. The class subsumes common estimators (e.g., OLS). Within that class,\ntwo novel estimators are proposed that are applicable to arbitrary designs and\nasymptotically optimal. The first is composed of three Horvitz-Thompson\nestimators. The second recursively applies the principle of generalized\nregression estimation to obtain regression-adjusted regression adjustment.\nAdditionally, variance bounds are derived that are tighter than those existing\nin the literature for arbitrary designs. Finally, a simulation study\nillustrates the potential for MSE improvements.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 21:42:15 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Middleton", "Joel A.", ""]]}, {"id": "1803.06031", "submitter": "Zhixin Zhou", "authors": "Zhixin Zhou and Arash A. Amini", "title": "Optimal Bipartite Network Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study bipartite community detection in networks, or more generally the\nnetwork biclustering problem. We present a fast two-stage procedure based on\nspectral initialization followed by the application of a pseudo-likelihood\nclassifier twice. Under mild regularity conditions, we establish the weak\nconsistency of the procedure (i.e., the convergence of the misclassification\nrate to zero) under a general bipartite stochastic block model. We show that\nthe procedure is optimal in the sense that it achieves the optimal convergence\nrate that is achievable by a biclustering oracle, adaptively over the whole\nclass, up to constants. This is further formalized by deriving a minimax lower\nbound over a class of biclustering problems. The optimal rate we obtain\nsharpens some of the existing results and generalizes others to a wide regime\nof average degree growth, from sparse networks with average degrees growing\narbitrarily slowly to fairly dense networks with average degrees of order\n$\\sqrt{n}$. As a special case, we recover the known exact recovery threshold in\nthe $\\log n$ regime of sparsity. To obtain the consistency result, as part of\nthe provable version of the algorithm, we introduce a sub-block partitioning\nscheme that is also computationally attractive, allowing for distributed\nimplementation of the algorithm without sacrificing optimality. The provable\nalgorithm is derived from a general class of pseudo-likelihood biclustering\nalgorithms that employ simple EM type updates. We show the effectiveness of\nthis general class by numerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2018 23:19:30 GMT"}, {"version": "v2", "created": "Sat, 22 Dec 2018 22:53:26 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Zhou", "Zhixin", ""], ["Amini", "Arash A.", ""]]}, {"id": "1803.06044", "submitter": "Kurt Riedel", "authors": "Alexander Sidorenko, Kurt S. Riedel", "title": "Optimal Boundary Kernels and Weightings for Local Polynomial Regression", "comments": "Manuscript date: 1993", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel smoothers are considered near the boundary of the interval. Kernels\nwhich minimize the expected mean square error are derived. These kernels are\nequivalent to using a linear weighting function in the local polynomial\nregression. It is shown that any kernel estimator that satisfies the moment\nconditions up to order $m$ is equivalent to a local polynomial regression of\norder $m$ with some non-negative weight function if and only if the kernel has\nat most $m$ sign changes. A fast algorithm is proposed for computing the kernel\nestimate in the boundary region for an arbitrary placement of data points.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 00:52:06 GMT"}, {"version": "v2", "created": "Sun, 25 Mar 2018 17:42:59 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sidorenko", "Alexander", ""], ["Riedel", "Kurt S.", ""]]}, {"id": "1803.06050", "submitter": "Kurt Riedel", "authors": "Alexander Sidorenko, Kurt S. Riedel", "title": "Sufficient Conditions for a Linear Estimator to be a Local Polynomial\n  Regression", "comments": "Manuscript date 1993", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown that any linear estimator that satisfies the moment conditions up\nto order $p$ is equivalent to a local polynomial regression of order $p$ with\nsome non-negative weight function if and only if the kernel has at most $p$\nsign changes. If the data points are placed symmetrically about the estimation\npoint, a linear weighting function is equivalent to the standard quadratic\nweighting function.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 01:23:47 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Sidorenko", "Alexander", ""], ["Riedel", "Kurt S.", ""]]}, {"id": "1803.06096", "submitter": "Kurnia Susvitasari", "authors": "Kurnia Susvitasari", "title": "Expected Time to Extinction of SIS Epidemic Model Using Quasy Stationary\n  Distribution", "comments": "12 pages; 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR q-bio.PE stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study that the breakdown of epidemic depends on some parameters, that is\nexpressed in epidemic reproduction ratio number. It is noted that when $R_0 $\nexceeds 1, the stochastic model have two different results. But, eventually the\nextinction will be reached even though the major epidemic occurs. The question\nis how long this process will reach extinction. In this paper, we will focus on\nthe Markovian process of SIS model when major epidemic occurs. Using the\napproximation of quasi--stationary distribution, the expected mean time of\nextinction only occurs when the process is one step away from being extinct.\nCombining the theorm from Ethier and Kurtz, we use CLT to find the\napproximation of this quasi distribution and successfully determine the\nasymptotic mean time to extinction of SIS model without demography.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2018 07:15:30 GMT"}], "update_date": "2018-03-19", "authors_parsed": [["Susvitasari", "Kurnia", ""]]}, {"id": "1803.06446", "submitter": "Arkadi Nemirovski", "authors": "Anatoli Juditsky and Arkadi Nemirovski", "title": "On Polyhedral Estimation of Signals via Indirect Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering linear image of unknown signal\nbelonging to a given convex compact signal set from noisy observation of\nanother linear image of the signal. We develop a simple generic efficiently\ncomputable nonlinear in observations \"polyhedral\" estimate along with\ncomputation-friendly techniques for its design and risk analysis. We\ndemonstrate that under favorable circumstances the resulting estimate is\nprovably near-optimal in the minimax sense, the \"favorable circumstances\" being\nless restrictive than the weakest known so far assumptions ensuring\nnear-optimality of estimates which are linear in observations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 02:05:03 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 18:42:23 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Juditsky", "Anatoli", ""], ["Nemirovski", "Arkadi", ""]]}, {"id": "1803.06505", "submitter": "Radu Stoica", "authors": "R. S. Stoica, M. Deaconu, L. Hurtado", "title": "A simulated annealing procedure based on the ABC Shadow algorithm for\n  statistical inference of point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a new algorithm for sampling posteriors of unnormalised probability\ndensities, called ABC Shadow, was proposed in [8]. This talk introduces a\nglobal optimisation procedure based on the ABC Shadow simulation dynamics.\nFirst the general method is explained, and then results on simulated and real\ndata are presented. The method is rather general, in the sense that it applies\nfor probability densities that are continuously differentiable with respect to\ntheir parameters\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 13:45:29 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Stoica", "R. S.", ""], ["Deaconu", "M.", ""], ["Hurtado", "L.", ""]]}, {"id": "1803.06510", "submitter": "Yingjie Fei", "authors": "Yingjie Fei and Yudong Chen", "title": "Hidden Integrality of SDP Relaxation for Sub-Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the discrete clustering structures\nunder Sub-Gaussian Mixture Models. Our main results establish a hidden\nintegrality property of a semidefinite programming (SDP) relaxation for this\nproblem: while the optimal solutions to the SDP are not integer-valued in\ngeneral, their estimation errors can be upper bounded in terms of the error of\nan idealized integer program. The error of the integer program, and hence that\nof the SDP, are further shown to decay exponentially in the signal-to-noise\nratio. To the best of our knowledge, this is the first exponentially decaying\nerror bound for convex relaxations of mixture models, and our results reveal\nthe \"global-to-local\" mechanism that drives the performance of the SDP\nrelaxation.\n  A corollary of our results shows that in certain regimes the SDP solutions\nare in fact integral and exact, improving on existing exact recovery results\nfor convex relaxations. More generally, our results establish sufficient\nconditions for the SDP to correctly recover the cluster memberships of\n$(1-\\delta)$ fraction of the points for any $\\delta\\in(0,1)$. As a special\ncase, we show that under the $d$-dimensional Stochastic Ball Model, SDP\nachieves non-trivial (sometimes exact) recovery when the center separation is\nas small as $\\sqrt{1/d}$, which complements previous exact recovery results\nthat require constant separation.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 14:11:13 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Fei", "Yingjie", ""], ["Chen", "Yudong", ""]]}, {"id": "1803.06517", "submitter": "Paul-Christian B\\\"urkner", "authors": "Paul-Christian B\\\"urkner, Rainer Schwabe, Heinz Holling", "title": "Optimal Designs for the Generalized Partial Credit Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing ordinal data becomes increasingly important in psychology,\nespecially in the context of item response theory. The generalized partial\ncredit model (GPCM) is probably the most widely used ordinal model and finds\napplication in many large scale educational assessment studies such as PISA. In\nthe present paper, optimal test designs are investigated for estimating\npersons' abilities with the GPCM for calibrated tests when item parameters are\nknown from previous studies. We will derive that local optimality may be\nachieved by assigning non-zero probability only to the first and last category\nindependently of a person's ability. That is, when using such a design, the\nGPCM reduces to the dichotomous 2PL model. Since locally optimal designs\nrequire the true ability to be known, we consider alternative Bayesian design\ncriteria using weight distributions over the ability parameter space. For\nsymmetric weight distributions, we derive necessary conditions for the optimal\none-point design of two response categories to be Bayes optimal. Furthermore,\nwe discuss examples of common symmetric weight distributions and investigate,\nin which cases the necessary conditions are also sufficient. Since the 2PL\nmodel is a special case of the GPCM, all of these results hold for the 2PL\nmodel as well.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 15:07:52 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 06:49:51 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["B\u00fcrkner", "Paul-Christian", ""], ["Schwabe", "Rainer", ""], ["Holling", "Heinz", ""]]}, {"id": "1803.06519", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus", "title": "Signal detection via Phi-divergences for general mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we are interested in testing whether there are any signals\nhidden in high dimensional noise data. Therefore we study the family of\ngoodness-of-fit tests based on $\\Phi$-divergences including the test of Berk\nand Jones as well as Tukey's higher criticism test. The optimality of this\nfamily is already known for the heterogeneous normal mixture model. We now\npresent a technique to transfer this optimality to more general models. For\nillustration we apply our results to dense signal and sparse signal models\nincluding the exponential-$\\chi^2$ mixture model and general exponential\nfamilies as the normal, exponential and Gumbel distribution. Beside the\noptimality of the whole family we discuss the power behavior on the detection\nboundary and show that the whole family has no power there, whereas the\nlikelihood ratio test does.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 15:17:55 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Ditzhaus", "Marc", ""]]}, {"id": "1803.06571", "submitter": "Kurt Riedel", "authors": "Andrew Mullhaupt, Kurt Riedel", "title": "Orthogonal Representations for Output System Pairs", "comments": "Work done in 200. Minor Revision 2001", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of canonical forms is given proposed in which $(A, C)$ is in\nHessenberg observer or Schur form and output normal: $\\bf{I} - A^*A =C^*C$.\nHere, $C$ is the $d \\times n$ measurement matrix and $A$ is the advance matrix.\nThe $(C, A)$ stack is expressed as the product of $n$ orthogonal matrices, each\nof which depends on $d$ parameters. State updates require only ${\\cal O}(nd)$\noperations and derivatives of the system with respect to the parameters are\nfast and convenient to compute. Restrictions are given such that these models\nare generically identifiable. Since the observability Grammian is the identity\nmatrix, system identification is better conditioned than other classes of\nmodels with fast updates.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 21:19:27 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Mullhaupt", "Andrew", ""], ["Riedel", "Kurt", ""]]}, {"id": "1803.06620", "submitter": "Gwo Dong Lin", "authors": "Chin-Yuan Hu and Gwo Dong Lin", "title": "Characterizations of the Logistic and Related Distributions", "comments": "17 pages, Journal of Mathematical Analysis and Applications (2018)", "journal-ref": null, "doi": "10.1016/j.jmaa.2018.03.003", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that few characterization results of the logistic distribution\nwere available before, although it is similar in shape to the normal one whose\ncharacteristic properties have been well investigated. Fortunately, in the last\ndecade, several authors have made great progress in this topic. Some\ninteresting characterization results of the logistic distribution have been\ndeveloped recently. In this paper, we further provide some new results by the\ndistributional equalities in terms of order statistics of the underlying\ndistribution and the random exponential shifts. The characterization of the\nclosely related Pareto type II distribution is also investigated.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 07:35:29 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Hu", "Chin-Yuan", ""], ["Lin", "Gwo Dong", ""]]}, {"id": "1803.06675", "submitter": "Xiaohan Yan", "authors": "Xiaohan Yan, Jacob Bien", "title": "Rare Feature Selection in High Dimensions", "comments": "42 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common in modern prediction problems for many predictor variables to be\ncounts of rarely occurring events. This leads to design matrices in which many\ncolumns are highly sparse. The challenge posed by such \"rare features\" has\nreceived little attention despite its prevalence in diverse areas, ranging from\nnatural language processing (e.g., rare words) to biology (e.g., rare species).\nWe show, both theoretically and empirically, that not explicitly accounting for\nthe rareness of features can greatly reduce the effectiveness of an analysis.\nWe next propose a framework for aggregating rare features into denser features\nin a flexible manner that creates better predictors of the response. Our\nstrategy leverages side information in the form of a tree that encodes feature\nsimilarity.\n  We apply our method to data from TripAdvisor, in which we predict the\nnumerical rating of a hotel based on the text of the associated review. Our\nmethod achieves high accuracy by making effective use of rare words; by\ncontrast, the lasso is unable to identify highly predictive words if they are\ntoo rare. A companion R package, called rare, implements our new estimator,\nusing the alternating direction method of multipliers.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 15:15:49 GMT"}, {"version": "v2", "created": "Wed, 8 Jul 2020 19:30:27 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Yan", "Xiaohan", ""], ["Bien", "Jacob", ""]]}, {"id": "1803.06716", "submitter": "Ilias Zadik", "authors": "David Gamarnik, Ilias Zadik", "title": "High Dimensional Linear Regression using Lattice Basis Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a high dimensional linear regression problem where the goal is to\nefficiently recover an unknown vector $\\beta^*$ from $n$ noisy linear\nobservations $Y=X\\beta^*+W \\in \\mathbb{R}^n$, for known $X \\in \\mathbb{R}^{n\n\\times p}$ and unknown $W \\in \\mathbb{R}^n$. Unlike most of the literature on\nthis model we make no sparsity assumption on $\\beta^*$. Instead we adopt a\nregularization based on assuming that the underlying vectors $\\beta^*$ have\nrational entries with the same denominator $Q \\in \\mathbb{Z}_{>0}$. We call\nthis $Q$-rationality assumption.\n  We propose a new polynomial-time algorithm for this task which is based on\nthe seminal Lenstra-Lenstra-Lovasz (LLL) lattice basis reduction algorithm. We\nestablish that under the $Q$-rationality assumption, our algorithm recovers\nexactly the vector $\\beta^*$ for a large class of distributions for the iid\nentries of $X$ and non-zero noise $W$. We prove that it is successful under\nsmall noise, even when the learner has access to only one observation ($n=1$).\nFurthermore, we prove that in the case of the Gaussian white noise for $W$,\n$n=o\\left(p/\\log p\\right)$ and $Q$ sufficiently large, our algorithm tolerates\na nearly optimal information-theoretic level of the noise.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 19:02:22 GMT"}, {"version": "v2", "created": "Fri, 9 Nov 2018 00:30:03 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Gamarnik", "David", ""], ["Zadik", "Ilias", ""]]}, {"id": "1803.06727", "submitter": "Evgeny Burnaev", "authors": "Alexander Korotin and Vladimir V'yugin and Evgeny Burnaev", "title": "Aggregating Strategies for Long-term Forecasting", "comments": "20 pages, 4 figures", "journal-ref": "PMLR 91:63-82, 2018", "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article is devoted to investigating the application of aggregating\nalgorithms to the problem of the long-term forecasting. We examine the classic\naggregating algorithms based on the exponential reweighing. For the general\nVovk's aggregating algorithm we provide its generalization for the long-term\nforecasting. For the special basic case of Vovk's algorithm we provide its two\nmodifications for the long-term forecasting. The first one is theoretically\nclose to an optimal algorithm and is based on replication of independent\ncopies. It provides the time-independent regret bound with respect to the best\nexpert in the pool. The second one is not optimal but is more practical and has\n$O(\\sqrt{T})$ regret bound, where $T$ is the length of the game.\n", "versions": [{"version": "v1", "created": "Sun, 18 Mar 2018 20:04:07 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Korotin", "Alexander", ""], ["V'yugin", "Vladimir", ""], ["Burnaev", "Evgeny", ""]]}, {"id": "1803.06790", "submitter": "Eugene Katsevich", "authors": "Eugene Katsevich and Aaditya Ramdas", "title": "Simultaneous high-probability bounds on the false discovery proportion\n  in structured, regression, and online settings", "comments": null, "journal-ref": "Annals of Statistics 2020, Vol. 48, No. 6, 3465-3487", "doi": "10.1214/19-AOS1938", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While traditional multiple testing procedures prohibit adaptive analysis\nchoices made by users, Goeman and Solari (2011) proposed a simultaneous\ninference framework that allows users such flexibility while preserving\nhigh-probability bounds on the false discovery proportion (FDP) of the chosen\nset. In this paper, we propose a new class of such simultaneous FDP bounds,\ntailored for nested sequences of rejection sets. While most existing\nsimultaneous FDP bounds are based on closed testing using global null tests\nbased on sorted p-values, we additionally consider the setting where side\ninformation can be leveraged to boost power, the variable selection setting\nwhere knockoff statistics can be used to order variables, and the online\nsetting where decisions about rejections must be made as data arrives. Our\nfinite-sample, closed form bounds are based on repurposing the FDP estimates\nfrom false discovery rate (FDR) controlling procedures designed for each of the\nabove settings. These results establish a novel connection between the parallel\nliteratures of simultaneous FDP bounds and FDR control methods, and use proof\ntechniques employing martingales and filtrations that are new to both these\nliteratures. We demonstrate the utility of our results by augmenting a recent\nknockoffs analysis of the UK Biobank dataset.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 02:54:00 GMT"}, {"version": "v2", "created": "Wed, 28 Mar 2018 21:21:39 GMT"}, {"version": "v3", "created": "Sun, 4 Nov 2018 22:35:56 GMT"}, {"version": "v4", "created": "Sun, 25 Aug 2019 01:21:55 GMT"}, {"version": "v5", "created": "Sun, 1 Dec 2019 05:55:34 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Katsevich", "Eugene", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1803.06907", "submitter": "Mickael Albertus", "authors": "Mickael Albertus and Philippe Berthet", "title": "Auxiliary information : the raking-ratio empirical process", "comments": "46 pages", "journal-ref": null, "doi": "10.1214/18-EJS1526", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the empirical measure associated to a sample of size $n$ and\nmodified by $N$ iterations of the raking-ratio method. This empirical measure\nis adjusted to match the true probability of sets in a finite partition which\nchanges each step. We establish asymptotic properties of the raking-ratio\nempirical process indexed by functions as $n\\rightarrow +\\infty$, for $N$\nfixed. We study nonasymptotic properties by using a Gaussian approximation\nwhich yields uniform Berry-Esseen type bounds depending on $n, N$ and provides\nestimates of the uniform quadratic risk reduction. A closed-form expression of\nthe limiting covariance matrices is derived as $N\\rightarrow +\\infty$. In the\ntwo-way contingency table case the limiting process has a simple explicit\nformula.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 13:38:30 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 17:07:15 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Albertus", "Mickael", ""], ["Berthet", "Philippe", ""]]}, {"id": "1803.06964", "submitter": "Emmanuel Candes", "authors": "Pragya Sur and Emmanuel J. Candes", "title": "A modern maximum-likelihood theory for high-dimensional logistic\n  regression", "comments": "29 pages, 14 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every student in statistics or data science learns early on that when the\nsample size largely exceeds the number of variables, fitting a logistic model\nproduces estimates that are approximately unbiased. Every student also learns\nthat there are formulas to predict the variability of these estimates which are\nused for the purpose of statistical inference; for instance, to produce\np-values for testing the significance of regression coefficients. Although\nthese formulas come from large sample asymptotics, we are often told that we\nare on reasonably safe grounds when $n$ is large in such a way that $n \\ge 5p$\nor $n \\ge 10p$. This paper shows that this is far from the case, and\nconsequently, inferences routinely produced by common software packages are\noften unreliable.\n  Consider a logistic model with independent features in which $n$ and $p$\nbecome increasingly large in a fixed ratio. Then we show that (1) the MLE is\nbiased, (2) the variability of the MLE is far greater than classically\npredicted, and (3) the commonly used likelihood-ratio test (LRT) is not\ndistributed as a chi-square. The bias of the MLE is extremely problematic as it\nyields completely wrong predictions for the probability of a case based on\nobserved values of the covariates. We develop a new theory, which\nasymptotically predicts (1) the bias of the MLE, (2) the variability of the\nMLE, and (3) the distribution of the LRT. We empirically also demonstrate that\nthese predictions are extremely accurate in finite samples. Further, an\nappealing feature is that these novel predictions depend on the unknown\nsequence of regression coefficients only through a single scalar, the overall\nstrength of the signal. This suggests very concrete procedures to adjust\ninference; we describe one such procedure learning a single parameter from data\nand producing accurate inference\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 14:49:27 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 22:18:02 GMT"}, {"version": "v3", "created": "Wed, 25 Apr 2018 20:13:08 GMT"}, {"version": "v4", "created": "Sat, 16 Jun 2018 04:06:47 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Sur", "Pragya", ""], ["Candes", "Emmanuel J.", ""]]}, {"id": "1803.06971", "submitter": "Lilian Besson", "authors": "Lilian Besson (IETR), Emilie Kaufmann (SEQUEL, CNRS)", "title": "What Doubling Tricks Can and Can't Do for Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An online reinforcement learning algorithm is anytime if it does not need to\nknow in advance the horizon T of the experiment. A well-known technique to\nobtain an anytime algorithm from any non-anytime algorithm is the \"Doubling\nTrick\". In the context of adversarial or stochastic multi-armed bandits, the\nperformance of an algorithm is measured by its regret, and we study two\nfamilies of sequences of growing horizons (geometric and exponential) to\ngeneralize previously known results that certain doubling tricks can be used to\nconserve certain regret bounds. In a broad setting, we prove that a geometric\ndoubling trick can be used to conserve (minimax) bounds in $R\\_T = O(\\sqrt{T})$\nbut cannot conserve (distribution-dependent) bounds in $R\\_T = O(\\log T)$. We\ngive insights as to why exponential doubling tricks may be better, as they\nconserve bounds in $R\\_T = O(\\log T)$, and are close to conserving bounds in\n$R\\_T = O(\\sqrt{T})$.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 15:02:15 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Besson", "Lilian", "", "IETR"], ["Kaufmann", "Emilie", "", "SEQUEL, CNRS"]]}, {"id": "1803.06989", "submitter": "Stefan Steinerberger", "authors": "George C. Linderman, Stefan Steinerberger", "title": "Numerical Integration on Graphs: where to sample and how to weigh", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.NA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $G=(V,E,w)$ be a finite, connected graph with weighted edges. We are\ninterested in the problem of finding a subset $W \\subset V$ of vertices and\nweights $a_w$ such that $$ \\frac{1}{|V|}\\sum_{v \\in V}^{}{f(v)} \\sim \\sum_{w\n\\in W}{a_w f(w)}$$ for functions $f:V \\rightarrow \\mathbb{R}$ that are `smooth'\nwith respect to the geometry of the graph. The main application are problems\nwhere $f$ is known to somehow depend on the underlying graph but is expensive\nto evaluate on even a single vertex. We prove an inequality showing that the\nintegration problem can be rewritten as a geometric problem (`the optimal\npacking of heat balls'). We discuss how one would construct approximate\nsolutions of the heat ball packing problem; numerical examples demonstrate the\nefficiency of the method.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 15:27:59 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Linderman", "George C.", ""], ["Steinerberger", "Stefan", ""]]}, {"id": "1803.07054", "submitter": "Nicolai Baldin", "authors": "Nicolai Baldin, Quentin Berthet", "title": "Optimal link prediction with matrix logistic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of link prediction, based on partial observation of a\nlarge network, and on side information associated to its vertices. The\ngenerative model is formulated as a matrix logistic regression. The performance\nof the model is analysed in a high-dimensional regime under a structural\nassumption. The minimax rate for the Frobenius-norm risk is established and a\ncombinatorial estimator based on the penalised maximum likelihood approach is\nshown to achieve it. Furthermore, it is shown that this rate cannot be attained\nby any (randomised) algorithm computable in polynomial time under a\ncomputational complexity assumption.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 17:32:50 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Baldin", "Nicolai", ""], ["Berthet", "Quentin", ""]]}, {"id": "1803.07074", "submitter": "Saeid Rezakhah", "authors": "Ferdous Mohammadi Basatini and Saeid Rezakhah", "title": "On Time-Varying Amplitude HGARCH Mode", "comments": "15 pages. arXiv admin note: text overlap with arXiv:1803.00739", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The HGARCH model allows long-memory impact in volatilities. A new HGARCH\nmodel with time-varying amplitude is considered in this paper. We show the\nstability of the model as well. A score test is introduced to check the\ntime-varying behavior in amplitude. Some value-at-risk tests are applied to\nevaluate the forecastings. Simulations are provided which provide further\nsupport to the proposed model. We have also have shown the competative\nperformance of our model in forecasting, by compairing it with HGARH and\nFIGARCH models for some period of SP500 indices.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 13:32:09 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Basatini", "Ferdous Mohammadi", ""], ["Rezakhah", "Saeid", ""]]}, {"id": "1803.07164", "submitter": "Vasilis Syrgkanis", "authors": "Greg Lewis, Vasilis Syrgkanis", "title": "Adversarial Generalized Method of Moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.GT cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an approach for learning deep neural net representations of models\ndescribed via conditional moment restrictions. Conditional moment restrictions\nare widely used, as they are the language by which social scientists describe\nthe assumptions they make to enable causal inference. We formulate the problem\nof estimating the underling model as a zero-sum game between a modeler and an\nadversary and apply adversarial training. Our approach is similar in nature to\nGenerative Adversarial Networks (GAN), though here the modeler is learning a\nrepresentation of a function that satisfies a continuum of moment conditions\nand the adversary is identifying violating moments. We outline ways of\nconstructing effective adversaries in practice, including kernels centered by\nk-means clustering, and random forests. We examine the practical performance of\nour approach in the setting of non-parametric instrumental variable regression.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2018 21:02:51 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 13:27:54 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Lewis", "Greg", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "1803.07418", "submitter": "Yang Feng", "authors": "Emre Demirkaya, Yang Feng, Pallavi Basu, Jinchi Lv", "title": "Large-Scale Model Selection with Misspecification", "comments": "38 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1412.7468", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is crucial to high-dimensional learning and inference for\ncontemporary big data applications in pinpointing the best set of covariates\namong a sequence of candidate interpretable models. Most existing work assumes\nimplicitly that the models are correctly specified or have fixed\ndimensionality. Yet both features of model misspecification and high\ndimensionality are prevalent in practice. In this paper, we exploit the\nframework of model selection principles in misspecified models originated in Lv\nand Liu (2014) and investigate the asymptotic expansion of Bayesian principle\nof model selection in the setting of high-dimensional misspecified models. With\na natural choice of prior probabilities that encourages interpretability and\nincorporates Kullback-Leibler divergence, we suggest the high-dimensional\ngeneralized Bayesian information criterion with prior probability (HGBIC_p) for\nlarge-scale model selection with misspecification. Our new information\ncriterion characterizes the impacts of both model misspecification and high\ndimensionality on model selection. We further establish the consistency of\ncovariance contrast matrix estimation and the model selection consistency of\nHGBIC_p in ultra-high dimensions under some mild regularity conditions. The\nadvantages of our new method are supported by numerical studies.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2018 03:10:12 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Demirkaya", "Emre", ""], ["Feng", "Yang", ""], ["Basu", "Pallavi", ""], ["Lv", "Jinchi", ""]]}, {"id": "1803.07527", "submitter": "Anuran Makur", "authors": "Anuran Makur, Elchanan Mossel, Yury Polyanskiy", "title": "Broadcasting on Bounded Degree DAGs", "comments": "35 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following generalization of the well-known model of broadcasting\non trees. Consider an infinite directed acyclic graph (DAG) with a unique\nsource node $X$. Let the collection of nodes at distance $k$ from $X$ be called\nthe $k$th layer. At time zero, the source node is given a bit. At time $k\\geq\n1$, each node in the $(k-1)$th layer inspects its inputs and sends a bit to its\ndescendants in the $k$th layer. Each bit is flipped with a probability of error\n$\\delta \\in \\left(0,\\frac{1}{2}\\right)$ in the process of transmission. The\ngoal is to be able to recover the original bit with probability of error better\nthan $\\frac{1}{2}$ from the values of all nodes at an arbitrarily deep layer\n$k$.\n  Besides its natural broadcast interpretation, the DAG broadcast is a natural\nmodel of noisy computation. Some special cases of the model represent\ninformation flow in biological networks, and other cases represent noisy finite\nautomata models.\n  We show that there exist DAGs with bounded degree and layers of size\n$\\omega(\\log(k))$ that permit recovery provided $\\delta$ is sufficiently small\nand find the critical $\\delta$ for the DAGs constructed. Our result\ndemonstrates a doubly-exponential advantage for storing a bit in bounded degree\nDAGs compared to trees. On the negative side, we show that if the DAG is a\ntwo-dimensional regular grid, then recovery is impossible for any $\\delta \\in\n\\left(0,\\frac{1}{2}\\right)$ provided all nodes use either AND or XOR for their\nprocessing functions.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 17:10:08 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Makur", "Anuran", ""], ["Mossel", "Elchanan", ""], ["Polyanskiy", "Yury", ""]]}, {"id": "1803.07554", "submitter": "Lijun Ding", "authors": "Lijun Ding, Yudong Chen", "title": "Leave-one-out Approach for Matrix Completion: Primal and Dual Analysis", "comments": "45 pages. The sample complexity for nuclear norm minimization has\n  been reduced to $\\mathcal{O}(\\mu r \\log(\\mu r)d \\log d )$ from\n  $\\mathcal{O}(\\mu^2 r^3 d \\log d)$ in the early version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a powerful technique based on Leave-one-out\nanalysis to the study of low-rank matrix completion problems. Using this\ntechnique, we develop a general approach for obtaining fine-grained, entrywise\nbounds for iterative stochastic procedures in the presence of probabilistic\ndependency. We demonstrate the power of this approach in analyzing two of the\nmost important algorithms for matrix completion: (i) the non-convex approach\nbased on Projected Gradient Descent (PGD) for a rank-constrained formulation,\nalso known as the Singular Value Projection algorithm, and (ii) the convex\nrelaxation approach based on nuclear norm minimization (NNM).\n  Using this approach, we establish the first convergence guarantee for the\noriginal form of PGD without regularization or sample splitting}, and in\nparticular shows that it converges linearly in the infinity norm. For NNM, we\nuse this approach to study a fictitious iterative procedure that arises in the\ndual analysis. Our results show that \\NNM recovers an $ d $-by-$ d $ rank-$ r $\nmatrix with $\\mathcal{O}(\\mu r \\log(\\mu r) d \\log d )$ observed entries. This\nbound has optimal dependence on the matrix dimension and is independent of the\ncondition number. To the best of our knowledge, this is the first sample\ncomplexity result for a tractable matrix completion algorithm that satisfies\nthese two properties simultaneously.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 17:54:49 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 02:01:03 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 04:45:20 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Ding", "Lijun", ""], ["Chen", "Yudong", ""]]}, {"id": "1803.07645", "submitter": "Zhanglong Cao", "authors": "Zhanglong Cao, David Bryant, Matthew Parry", "title": "V-Splines and Bayes Estimate", "comments": "a draft. not peer-reviewed yet", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothing splines can be thought of as the posterior mean of a Gaussian\nprocess regression in a certain limit. By constructing a reproducing kernel\nHilbert space with an appropriate inner product, the Bayesian form of the\nV-spline is derived when the penalty term is a fixed constant instead of a\nfunction. An extension to the usual generalized cross-validation formula is\nutilized to find the optimal V-spline parameters.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2018 20:47:06 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 02:22:07 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 03:24:44 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Cao", "Zhanglong", ""], ["Bryant", "David", ""], ["Parry", "Matthew", ""]]}, {"id": "1803.07763", "submitter": "Yuting Wei", "authors": "Yuting Wei, Billy Fang, Martin J. Wainwright", "title": "From Gauss to Kolmogorov: Localized Measures of Complexity for Ellipses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian width is a fundamental quantity in probability, statistics and\ngeometry, known to underlie the intrinsic difficulty of estimation and\nhypothesis testing. In this work, we show how the Gaussian width, when\nlocalized to any given point of an ellipse, can be controlled by the Kolmogorov\nwidth of a set similarly localized. This connection leads to an explicit\ncharacterization of the estimation error of least-squares regression as a\nfunction of the true regression vector within the ellipse. The rate of error\ndecay varies substantially as a function of location: as a concrete example, in\nSobolev ellipses of smoothness $\\alpha$, we exhibit rates that vary from\n$(\\sigma^2)^{\\frac{2 \\alpha}{2 \\alpha + 1}}$, corresponding to the classical\nglobal rate, to the faster rate $(\\sigma^2)^{\\frac{4 \\alpha}{4 \\alpha + 1}}$.\nWe also show how the local Kolmogorov width can be related to local metric\nentropy.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 06:27:03 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Wei", "Yuting", ""], ["Fang", "Billy", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1803.07793", "submitter": "Jiang Hu Dr.", "authors": "Jiang Hu, Weiming Li, Zhi Liu and Wang Zhou", "title": "High-dimensional covariance matrices in elliptical distributions with\n  application to spherical test", "comments": "30 pages-to appear in The Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses fluctuations of linear spectral statistics of\nhigh-dimensional sample covariance matrices when the underlying population\nfollows an elliptical distribution. Such population often possesses high order\ncorrelations among their coordinates, which have great impact on the asymptotic\nbehaviors of linear spectral statistics. Taking such kind of dependency into\nconsideration, we establish a new central limit theorem for the linear spectral\nstatistics in this paper for a class of elliptical populations. This general\ntheoretical result has wide applications and, as an example, it is then applied\nto test the sphericity of elliptical populations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2018 08:25:02 GMT"}], "update_date": "2018-03-22", "authors_parsed": [["Hu", "Jiang", ""], ["Li", "Weiming", ""], ["Liu", "Zhi", ""], ["Zhou", "Wang", ""]]}, {"id": "1803.08269", "submitter": "Genki Kusano", "authors": "Genki Kusano", "title": "On the Expectation of a Persistence Diagram by the Persistence Weighted\n  Kernel", "comments": "23 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In topological data analysis, persistent homology characterizes robust\ntopological features in data and it has a summary representation, called a\npersistence diagram. Statistical research for persistence diagrams have been\nactively developed, and the persistence weighted kernel shows several\nadvantages over other statistical methods for persistence diagrams. If data is\ndrawn from some probability distribution, the corresponding persistence diagram\nhave randomness. Then, the expectation of the persistence diagram by the\npersistence weighted kernel is well-defined. In this paper, we study\nrelationships between a probability distribution and the persistence weighted\nkernel in the viewpoint of (1) the strong law of large numbers and the central\nlimit theorem, (2) a confidence interval to estimate the expectation of the\npersistence weighted kernel numerically, and (3) the stability theorem to\nensure the continuity of the map from a probability distribution to the\nexpectation. In numerical experiments, we demonstrate our method gives an\ninteresting counterexample to a common view in topological data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 08:55:36 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 01:37:06 GMT"}, {"version": "v3", "created": "Fri, 26 Oct 2018 08:07:26 GMT"}], "update_date": "2018-10-29", "authors_parsed": [["Kusano", "Genki", ""]]}, {"id": "1803.08586", "submitter": "Yining Wang", "authors": "Yining Wang, Sivaraman Balakrishnan, Aarti Singh", "title": "Optimization of Smooth Functions with Noisy Observations: Local Minimax\n  Rates", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of global optimization of an unknown non-convex\nsmooth function with zeroth-order feedback. In this setup, an algorithm is\nallowed to adaptively query the underlying function at different locations and\nreceives noisy evaluations of function values at the queried points (i.e. the\nalgorithm has access to zeroth-order information). Optimization performance is\nevaluated by the expected difference of function values at the estimated\noptimum and the true optimum. In contrast to the classical optimization setup,\nfirst-order information like gradients are not directly accessible to the\noptimization algorithm. We show that the classical minimax framework of\nanalysis, which roughly characterizes the worst-case query complexity of an\noptimization algorithm in this setting, leads to excessively pessimistic\nresults. We propose a local minimax framework to study the fundamental\ndifficulty of optimizing smooth functions with adaptive function evaluations,\nwhich provides a refined picture of the intrinsic difficulty of zeroth-order\noptimization. We show that for functions with fast level set growth around the\nglobal minimum, carefully designed optimization algorithms can identify a near\nglobal minimizer with many fewer queries. For the special case of strongly\nconvex and smooth functions, our implied convergence rates match the ones\ndeveloped for zeroth-order convex optimization problems. At the other end of\nthe spectrum, for worst-case smooth functions no algorithm can converge faster\nthan the minimax rate of estimating the entire unknown function in the\n$\\ell_\\infty$-norm. We provide an intuitive and efficient algorithm that\nattains the derived upper error bounds.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2018 21:21:02 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["Wang", "Yining", ""], ["Balakrishnan", "Sivaraman", ""], ["Singh", "Aarti", ""]]}, {"id": "1803.09015", "submitter": "Pedro H. C. Sant'Anna", "authors": "Brantly Callaway, Pedro H. C. Sant'Anna", "title": "Difference-in-Differences with Multiple Time Periods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider identification, estimation, and inference\nprocedures for treatment effect parameters using Difference-in-Differences\n(DiD) with (i) multiple time periods, (ii) variation in treatment timing, and\n(iii) when the \"parallel trends assumption\" holds potentially only after\nconditioning on observed covariates. We show that a family of causal effect\nparameters are identified in staggered DiD setups, even if differences in\nobserved characteristics create non-parallel outcome dynamics between groups.\nOur identification results allow one to use outcome regression, inverse\nprobability weighting, or doubly-robust estimands. We also propose different\naggregation schemes that can be used to highlight treatment effect\nheterogeneity across different dimensions as well as to summarize the overall\neffect of participating in the treatment. We establish the asymptotic\nproperties of the proposed estimators and prove the validity of a\ncomputationally convenient bootstrap procedure to conduct asymptotically valid\nsimultaneous (instead of pointwise) inference. Finally, we illustrate the\nrelevance of our proposed tools by analyzing the effect of the minimum wage on\nteen employment from 2001--2007. Open-source software is available for\nimplementing the proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2018 23:45:05 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2018 19:30:23 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 03:32:06 GMT"}, {"version": "v4", "created": "Tue, 1 Dec 2020 16:15:59 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Callaway", "Brantly", ""], ["Sant'Anna", "Pedro H. C.", ""]]}, {"id": "1803.09027", "submitter": "Bolin Ding", "authors": "Bolin Ding, Harsha Nori, Paul Li, Joshua Allen", "title": "Comparing Population Means under Local Differential Privacy: with\n  Significance and Power", "comments": "Full version of an AAAI 2018 conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A statistical hypothesis test determines whether a hypothesis should be\nrejected based on samples from populations. In particular, randomized\ncontrolled experiments (or A/B testing) that compare population means using,\ne.g., t-tests, have been widely deployed in technology companies to aid in\nmaking data-driven decisions. Samples used in these tests are collected from\nusers and may contain sensitive information. Both the data collection and the\ntesting process may compromise individuals' privacy. In this paper, we study\nhow to conduct hypothesis tests to compare population means while preserving\nprivacy. We use the notation of local differential privacy (LDP), which has\nrecently emerged as the main tool to ensure each individual's privacy without\nthe need of a trusted data collector. We propose LDP tests that inject noise\ninto every user's data in the samples before collecting them (so users do not\nneed to trust the data collector), and draw conclusions with bounded type-I\n(significance level) and type-II errors (1 - power). Our approaches can be\nextended to the scenario where some users require LDP while some are willing to\nprovide exact data. We report experimental results on real-world datasets to\nverify the effectiveness of our approaches.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2018 01:20:14 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Ding", "Bolin", ""], ["Nori", "Harsha", ""], ["Li", "Paul", ""], ["Allen", "Joshua", ""]]}, {"id": "1803.09321", "submitter": "Zi Ye", "authors": "Zi Ye, Giles Hooker", "title": "Local Quadratic Estimation of the Curvature in a Functional Single Index\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nonlinear effects of environmental variability on species abundance plays\nan important role in the maintenance of ecological diversity. Nonetheless, many\ncommon models use parametric nonlinear terms pre-determining ecological\nconclusions. Motivated by this concern, we study the estimate of the second\nderivative (curvature) of the link function g in a functional single index\nmodel. Since the coefficient function and the link function are both unknown,\nthe estimate is expressed as a nested optimization. For a fixed and unknown\ncoefficient function, the link function and its second derivative are estimated\nby local quadratic approximation, then the coefficient function is estimated by\nminimizing the MSE of the model. In this paper, we derive the rate of\nconvergence of the estimation. In addition, we prove that the argument of g,\ncan be estimated root-n consistently. However, practical implementation of the\nmethod requires solving a nonlinear optimization problem, and our results show\nthat the estimates of the link function and the coefficient function are quite\nsensitive to the choices of starting values.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2018 19:48:12 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Ye", "Zi", ""], ["Hooker", "Giles", ""]]}, {"id": "1803.09467", "submitter": "Shanyun Liu", "authors": "Shanyun Liu and Rui She and Shuo Wan and Pingyi Fan and Yunquan Dong", "title": "A Switch to the Concern of User: Importance Coefficient in Utility\n  Distribution and Message Importance Measure", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper mainly focuses on the utilization frequency in receiving end of\ncommunication systems, which shows the inclination of the user about different\nsymbols. When the average number of use is limited, a specific utility\ndistribution is proposed on the best effort in term of fairness, which is also\nthe closest one to occurring probability in the relative entropy. Similar to a\nswitch, its parameter can be selected to make it satisfy different users'\nrequirements: negative parameter means the user focus on high-probability\nevents and positive parameter means the user is interested in small-probability\nevents. In fact, the utility distribution is a measure of message importance in\nessence. It illustrates the meaning of message importance measure (MIM), and\nextend it to the general case by selecting the parameter. Numerical results\nshow that this utility distribution characterizes the message importance like\nMIM and its parameter determines the concern of users.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 08:33:11 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Liu", "Shanyun", ""], ["She", "Rui", ""], ["Wan", "Shuo", ""], ["Fan", "Pingyi", ""], ["Dong", "Yunquan", ""]]}, {"id": "1803.09479", "submitter": "Alexey Zaytsev", "authors": "A. Zaytsev, E. Romanenkova, D. Ermilov", "title": "Interpolation error of misspecified Gaussian process regression", "comments": "Submitted to COPA conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An interpolation error is an integral of the squared error of a regression\nmodel over a domain of interest. We consider the interpolation error for the\ncase of misspecified Gaussian process regression: used covariance function\ndiffers from the true one. We derive the interpolation error for an infinite\ngrid design of experiments. In particular, we show that for Matern 1/2\ncovariance function poor estimation of parameters only slightly affects the\nquality of interpolation. Then we proceed to numerical experiments that\nconsider the misspecification for the most common covariance functions\nincluding other Matern and squared exponential covariance functions. For them,\nthe quality of estimates of parameters affects the interpolation error.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 09:34:30 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Zaytsev", "A.", ""], ["Romanenkova", "E.", ""], ["Ermilov", "D.", ""]]}, {"id": "1803.09496", "submitter": "Jeremie Houssineau", "authors": "Jeremie Houssineau and Ajay Jasra and Sumeetpal S. Singh", "title": "On the loss of Fisher information in some multi-object tracking\n  observation models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of Fisher information can be useful even in cases where the\nprobability distributions of interest are not absolutely continuous with\nrespect to the natural reference measure on the underlying space. Practical\nexamples where this extension is useful are provided in the context of\nmulti-object tracking statistical models. Upon defining the Fisher information\nwithout introducing a reference measure, we provide remarkably concise proofs\nof the loss of Fisher information in some widely used multi-object tracking\nobservation models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 10:30:13 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Houssineau", "Jeremie", ""], ["Jasra", "Ajay", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1803.09501", "submitter": "Vaidotas Characiejus", "authors": "Vaidotas Characiejus and Gregory Rice", "title": "A general white noise test based on kernel lag-window estimates of the\n  spectral density operator", "comments": "32 pages, 4 figures", "journal-ref": "Econometrics and Statistics, Volume 13, January 2020, Pages\n  175-196", "doi": "10.1016/j.ecosta.2019.01.003", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general white noise test for functional time series based on\nestimating a distance between the spectral density operator of a weakly\nstationary time series and the constant spectral density operator of an\nuncorrelated time series. The estimator that we propose is based on a kernel\nlag-window type estimator of the spectral density operator. When the observed\ntime series is a strong white noise in a real separable Hilbert space, we show\nthat the asymptotic distribution of the test statistic is standard normal, and\nwe further show that the test statistic diverges for general serially\ncorrelated time series. These results recover as special cases those of Hong\n(1996) and Horv\\'ath et al. (2013). In order to implement the test, we propose\nand study a number of kernel and bandwidth choices, including a new data\nadaptive bandwidth, as well as data adaptive power transformations of the test\nstatistic that improve the normal approximation in finite samples. A simulation\nstudy demonstrated that the proposed method has good size and improved power\nwhen compared to other methods available in the literature, while also offering\na light computational burden.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2018 10:40:29 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 15:38:57 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Characiejus", "Vaidotas", ""], ["Rice", "Gregory", ""]]}, {"id": "1803.09849", "submitter": "Alberto J. Coca", "authors": "Alberto J. Coca", "title": "Adaptive nonparametric estimation for compound Poisson processes robust\n  to the discrete-observation scheme", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A compound Poisson process whose jump measure and intensity are unknown is\nobserved at finitely many equispaced times. We construct a purely data-driven\nestimator of the L\\'evy density $\\nu$ through the spectral approach using\ngeneral Calderon--Zygmund integral operators, which include convolution and\nprojection kernels. Assuming minimal tail assumptions, it is shown to estimate\n$\\nu$ at the minimax rate of estimation over Besov balls under the losses\n$L^p(\\mathbb{R})$, $p\\in[1,\\infty]$, and robustly to the observation regime\n(high- and low-frequency). To achieve adaptation in a minimax sense, we use\nLepski\\u{i}'s method as it is particularly well-suited for our generality.\nThus, novel exponential-concentration inequalities are proved including one for\nthe uniform fluctuations of the empirical characteristic function. These are of\nindependent interest, as are the proof-strategies employed to deal with general\nCalderon--Zygmund operators, to depart from the ubiquitous quadratic structure\nand to show robustness without polynomial-tail conditions. Part of the\nmotivation for such generality is a new insight we include here too that,\nfurthermore, allows us to unify the main two approaches to construct estimators\nused in related literature.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 02:27:13 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 19:36:30 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Coca", "Alberto J.", ""]]}, {"id": "1803.10282", "submitter": "Yves Atchade F", "authors": "Yves Atchade, Anwesha Bhattacharyya", "title": "An approach to large-scale Quasi-Bayesian inference with spike-and-slab\n  priors", "comments": "53 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework using spike-and-slab prior distributions to\naid with the development of high-dimensional Bayesian inference. Our framework\nallows inference with a general quasi-likelihood function. We show that highly\nefficient and scalable Markov Chain Monte Carlo (MCMC) algorithms can be easily\nconstructed to sample from the resulting quasi-posterior distributions.\n  We study the large scale behavior of the resulting quasi-posterior\ndistributions as the dimension of the parameter space grows, and we establish\nseveral convergence results. In large-scale applications where computational\nspeed is important, variational approximation methods are often used to\napproximate posterior distributions. We show that the contraction behaviors of\nthe quasi-posterior distributions can be exploited to provide theoretical\nguarantees for their variational approximations. We illustrate the theory with\nsome simulation results from Gaussian graphical models, and sparse principal\ncomponent analysis.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2018 19:25:14 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 12:33:49 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 20:27:31 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Atchade", "Yves", ""], ["Bhattacharyya", "Anwesha", ""]]}, {"id": "1803.10749", "submitter": "Erlis Ruli", "authors": "Erlis Ruli", "title": "On Model Selection with Summary Statistics", "comments": "6 pages, working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many authors have cast doubts on the validity of ABC model choice.\nIt has been shown that the use of sufficient statistic in ABC model selection\nleads, apart from few exceptional cases in which the sufficient statistic is\nalso cross-model sufficient, to unreliable results. In a single model context\nand given a sufficient summary statistic, we show that it is possible to fully\nrecover the posterior normalising constant, without using the likelihood\nfunction. The idea can be applied, in an approximate way, to more realistic\nscenarios in which the sufficient statistic is not unavailable but a \"good\"\nsummary statistic for estimation is available.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 17:36:49 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 07:06:31 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Ruli", "Erlis", ""]]}, {"id": "1803.10871", "submitter": "Alessandro Casini", "authors": "Alessandro Casini and Pierre Perron", "title": "Generalized Laplace Inference in Multiple Change-Points Models", "comments": null, "journal-ref": null, "doi": "10.1017/S0266466621000013", "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the classical long-span asymptotic framework we develop a class of\nGeneralized Laplace (GL) inference methods for the change-point dates in a\nlinear time series regression model with multiple structural changes analyzed\nin, e.g., Bai and Perron (1998). The GL estimator is defined by an integration\nrather than optimization-based method and relies on the least-squares criterion\nfunction. It is interpreted as a classical (non-Bayesian) estimator and the\ninference methods proposed retain a frequentist interpretation. This approach\nprovides a better approximation about the uncertainty in the data of the\nchange-points relative to existing methods. On the theoretical side, depending\non some input (smoothing) parameter, the class of GL estimators exhibits a dual\nlimiting distribution; namely, the classical shrinkage asymptotic distribution,\nor a Bayes-type asymptotic distribution. We propose an inference method based\non Highest Density Regions using the latter distribution. We show that it has\nattractive theoretical properties not shared by the other popular alternatives,\ni.e., it is bet-proof. Simulations confirm that these theoretical properties\ntranslate to better finite-sample performance.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 22:34:57 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 16:01:50 GMT"}, {"version": "v3", "created": "Mon, 2 Mar 2020 12:05:00 GMT"}, {"version": "v4", "created": "Fri, 15 Jan 2021 20:38:20 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Casini", "Alessandro", ""], ["Perron", "Pierre", ""]]}, {"id": "1803.10878", "submitter": "Rachel Ward", "authors": "Christopher Kennedy, Rachel Ward", "title": "Greedy Variance Estimation for the LASSO", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent results have proven the minimax optimality of LASSO and related\nalgorithms for noisy linear regression. However, these results tend to rely on\nvariance estimators that are inefficient or optimizations that are slower than\nLASSO itself. We propose an efficient estimator for the noise variance in high\ndimensional linear regression that is faster than LASSO, only requiring $p$\nmatrix-vector multiplications. We prove this estimator is consistent with a\ngood rate of convergence, under the condition that the design matrix satisfies\nthe Restricted Isometry Property (RIP). In practice, our estimator scales\nincredibly well into high dimensions, is highly parallelizable, and only incurs\na modest bias.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 23:43:39 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 12:42:31 GMT"}, {"version": "v3", "created": "Thu, 14 Mar 2019 21:38:15 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Kennedy", "Christopher", ""], ["Ward", "Rachel", ""]]}, {"id": "1803.10881", "submitter": "Alessandro Casini", "authors": "Alessandro Casini and Pierre Perron", "title": "Continuous Record Asymptotics for Structural Change Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a partial structural change in a linear regression model with a single\nbreak, we develop a continuous record asymptotic framework to build inference\nmethods for the break date. We have T observations with a sampling frequency h\nover a fixed time horizon [0, N] , and let T with h 0 while keeping the time\nspan N fixed. We impose very mild regularity conditions on an underlying\ncontinuous-time model assumed to generate the data. We consider the\nleast-squares estimate of the break date and establish consistency and\nconvergence rate. We provide a limit theory for shrinking magnitudes of shifts\nand locally increasing variances. The asymptotic distribution corresponds to\nthe location of the extremum of a function of the quadratic variation of the\nregressors and of a Gaussian centered martingale process over a certain time\ninterval. We can account for the asymmetric informational content provided by\nthe pre- and post-break regimes and show how the location of the break and\nshift magnitude are key ingredients in shaping the distribution. We consider a\nfeasible version based on plug-in estimates, which provides a very good\napproximation to the finite sample distribution. We use the concept of Highest\nDensity Region to construct confidence sets. Overall, our method is reliable\nand delivers accurate coverage probabilities and relatively short average\nlength of the confidence sets. Importantly, it does so irrespective of the size\nof the break.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2018 23:58:03 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 11:51:31 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Casini", "Alessandro", ""], ["Perron", "Pierre", ""]]}, {"id": "1803.10884", "submitter": "Adam Gustafson", "authors": "Adam Gustafson, Matthew Hirn, Kitty Mohammed, Hariharan Narayanan, and\n  Jason Xu", "title": "Structural Risk Minimization for $C^{1,1}(\\mathbb{R}^d)$ Regression", "comments": "32 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One means of fitting functions to high-dimensional data is by providing\nsmoothness constraints. Recently, the following smooth function approximation\nproblem was proposed: given a finite set $E \\subset \\mathbb{R}^d$ and a\nfunction $f: E \\rightarrow \\mathbb{R}$, interpolate the given information with\na function $\\widehat{f} \\in \\dot{C}^{1, 1}(\\mathbb{R}^d)$ (the class of\nfirst-order differentiable functions with Lipschitz gradients) such that\n$\\widehat{f}(a) = f(a)$ for all $a \\in E$, and the value of\n$\\mathrm{Lip}(\\nabla \\widehat{f})$ is minimal. An algorithm is provided that\nconstructs such an approximating function $\\widehat{f}$ and estimates the\noptimal Lipschitz constant $\\mathrm{Lip}(\\nabla \\widehat{f})$ in the noiseless\nsetting.\n  We address statistical aspects of reconstructing the approximating function\n$\\widehat{f}$ from a closely-related class $C^{1, 1}(\\mathbb{R}^d)$ given\nsamples from noisy data. We observe independent and identically distributed\nsamples $y(a) = f(a) + \\xi(a)$ for $a \\in E$, where $\\xi(a)$ is a noise term\nand the set $E \\subset \\mathbb{R}^d$ is fixed and known. We obtain uniform\nbounds relating the empirical risk and true risk over the class\n$\\mathcal{F}_{\\widetilde{M}} = \\{f \\in C^{1, 1}(\\mathbb{R}^d) \\mid\n\\mathrm{Lip}(\\nabla f) \\leq \\widetilde{M}\\}$, where the quantity\n$\\widetilde{M}$ grows with the number of samples at a rate governed by the\nmetric entropy of the class $C^{1, 1}(\\mathbb{R}^d)$. Finally, we provide an\nimplementation using Vaidya's algorithm, supporting our results via numerical\nexperiments on simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 00:19:45 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 00:49:35 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Gustafson", "Adam", ""], ["Hirn", "Matthew", ""], ["Mohammed", "Kitty", ""], ["Narayanan", "Hariharan", ""], ["Xu", "Jason", ""]]}, {"id": "1803.11039", "submitter": "Xingcheng Xu", "authors": "Zhongmin Qian and Xingcheng Xu", "title": "L\\'evy area of fractional Ornstein-Uhlenbeck process and parameter\n  estimation", "comments": "38 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the estimation problem of an unknown drift parameter\nmatrix for fractional Ornstein-Uhlenbeck process in multi-dimensional setting.\nBy using rough path theory, we propose pathwise rough path estimators based on\nboth continuous and discrete observations of a single path. The approach is\napplicable to the high-frequency data. To formulate the parameter estimators,\nwe define a theory of pathwise It\\^o integrals with respect to fractional\nBrownian motion. By showing the regularity of fractional Ornstein-Uhlenbeck\nprocesses and the long time asymptotic behaviour of the associated L\\'evy area\nprocesses, we prove that the estimators are strong consistent and pathwise\nstable. Numerical studies and simulations are also given in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 13:02:17 GMT"}, {"version": "v2", "created": "Tue, 3 Apr 2018 17:02:14 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Qian", "Zhongmin", ""], ["Xu", "Xingcheng", ""]]}, {"id": "1803.11202", "submitter": "Youssef Taleb", "authors": "Youssef Taleb, Edward A. K. Cohen", "title": "Multiresolution analysis of point processes and statistical thresholding\n  for wavelet-based intensity estimation", "comments": "48 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a wavelet based approach to the analysis of point processes and the\nestimation of the first order intensity under a continuous time setting. A\nmultiresolution analysis of a point process is formulated which motivates the\ndefinition of homogeneity at different scales of resolution, termed $J$-th\nlevel homogeneity. Further to this, the activity in a point processes' first\norder behavior at different scales of resolution is also defined and termed\n$L$-th level innovation. Likelihood ratio tests for both these properties are\nproposed with asymptotic distributions provided, even when only a single\nrealization of the point process is observed. The test for $L$-th level\ninnovation forms the basis for a collection of statistical strategies for\nthresholding coefficients in a wavelet based estimator of the intensity\nfunction. These thresholding strategies are shown to outperform the existing\nlocal hard thresholding strategy on a range of simulation scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 18:05:00 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Taleb", "Youssef", ""], ["Cohen", "Edward A. K.", ""]]}, {"id": "1803.11240", "submitter": "Daniel Eck", "authors": "Daniel J. Eck and Charles J. Geyer", "title": "Computationally efficient likelihood inference in exponential families\n  when the maximum likelihood estimator does not exist", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a regular full exponential family, the maximum likelihood estimator (MLE)\nneed not exist in the traditional sense. However, the MLE may exist in the\ncompletion of the exponential family. Existing algorithms for finding the MLE\nin the completion solve many linear programs; they are slow in small problems\nand too slow for large problems. We provide new, fast, and scalable methodology\nfor finding the MLE in the completion of the exponential family. This\nmethodology is based on conventional maximum likelihood computations which come\nclose, in a sense, to finding the MLE in the completion of the exponential\nfamily. These conventional computations construct a likelihood maximizing\nsequence of canonical parameter values which goes uphill on the likelihood\nfunction until they meet a convergence criteria. Nonexistence of the MLE in\nthis context results from a degeneracy of the canonical statistic of the\nexponential family, the canonical statistic is on the boundary of its support.\nThere is a correspondance between this boundary and the null eigenvectors of\nthe Fisher information matrix. Convergence of Fisher information along a\nlikelihood maximizing sequence follows from cumulant generating function (CGF)\nconvergence along a likelihood maximizing sequence, conditions for which are\ngiven. This allows for the construction of necessarily one-sided confidence\nintervals for mean value parameters when the MLE exists in the completion. We\ndemonstrate our methodology on three examples in the main text and three\nadditional examples in the Appendix. We show that when the MLE exists in the\ncompletion of the exponential family, our methodology provides statistical\ninference that is much faster than existing techniques.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 20:07:32 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 22:24:30 GMT"}, {"version": "v3", "created": "Wed, 25 Nov 2020 21:53:26 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Eck", "Daniel J.", ""], ["Geyer", "Charles J.", ""]]}, {"id": "1803.11249", "submitter": "Joseph Hart", "authors": "Joseph Hart and Pierre Gremaud", "title": "Robustness of the Sobol' indices to distributional uncertainty", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global sensitivity analysis (GSA) is used to quantify the influence of\nuncertain variables in a mathematical model. Prior to performing GSA, the user\nmust specify (or implicitly assume), a probability distribution to model the\nuncertainty, and possibly statistical dependencies, of the variables.\nDetermining this distribution is challenging in practice as the user has\nlimited and imprecise knowledge of the uncertain variables. This article\nanalyzes the robustness of the Sobol' indices, a commonly used tool in GSA, to\nchanges in the distribution of the uncertain variables. A method for assessing\nsuch robustness is developed which requires minimal user specification and no\nadditional evaluations of the model. Theoretical and computational aspects of\nthe method are considered and illustrated through examples.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 20:47:59 GMT"}, {"version": "v2", "created": "Thu, 31 May 2018 14:04:35 GMT"}, {"version": "v3", "created": "Tue, 20 Nov 2018 21:50:47 GMT"}], "update_date": "2018-11-22", "authors_parsed": [["Hart", "Joseph", ""], ["Gremaud", "Pierre", ""]]}, {"id": "1803.11262", "submitter": "Dmitrii Ostrovskii", "authors": "Dmitrii Ostrovskii, Zaid Harchaoui", "title": "Efficient First-Order Algorithms for Adaptive Signal Denoising", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of discrete-time signal denoising, focusing on a\nspecific family of non-linear convolution-type estimators. Each such estimator\nis associated with a time-invariant filter which is obtained adaptively, by\nsolving a certain convex optimization problem. Adaptive convolution-type\nestimators were demonstrated to have favorable statistical properties. However,\nthe question of their computational complexity remains largely unexplored, and\nin fact we are not aware of any publicly available implementation of these\nestimators. Our first contribution is an efficient implementation of these\nestimators via some known first-order proximal algorithms. Our second\ncontribution is a computational complexity analysis of the proposed procedures,\nwhich takes into account their statistical nature and the related notion of\nstatistical accuracy. The proposed procedures and their analysis are\nillustrated on a simulated data benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2018 21:11:48 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 14:42:58 GMT"}, {"version": "v3", "created": "Tue, 12 Jun 2018 13:48:40 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Ostrovskii", "Dmitrii", ""], ["Harchaoui", "Zaid", ""]]}, {"id": "1803.11332", "submitter": "Masahito Hayashi", "authors": "Masahito Hayashi", "title": "Local Equivalence Problem in Hidden Markov Model", "comments": "This paper has an overlap with the version 1 of arXiv:1705.06040.\n  However, this overlap will be removed in the second version of\n  arXiv:1705.06040", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the hidden Markov process, there is a possibility that two different\ntransition matrices for hidden and observed variables yield the same stochastic\nbehavior for the observed variables. Since such two transition matrices cannot\nbe distinguished, we need to identify them and consider that they are\nequivalent, in practice. We address the equivalence problem of hidden Markov\nprocess in a local neighborhood by using the geometrical structure of hidden\nMarkov process. For this aim, we introduce a mathematical concept to express\nMarkov process, and formulate its exponential family by using generators. Then,\nthe above equivalence problem is formulated as the equivalence problem of\ngenerators. Taking this equivalence problem into account, we derive several\nconcrete parametrizations in several natural cases.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 04:05:03 GMT"}, {"version": "v2", "created": "Wed, 8 May 2019 03:54:33 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Hayashi", "Masahito", ""]]}, {"id": "1803.11354", "submitter": "Natalie Karavarsamis", "authors": "N. Karavarsamis and R. M. Huggins", "title": "Two-stage approaches to the analysis of occupancy data II. The\n  heterogeneous model and conditional likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Occupancy models involve both the probability a site is occupied and the\nprobability occupancy is detected. The homogeneous occupancy model, where the\noccupancy and detection probabilities are the same at each site, admits an\northogonal parameter transformation that yields a two-stage process to\ncalculate the maximum likelihood estimates so that it is not necessary to\nsimultaneously estimate the occupancy and detection probabilities. The\ntwo-stage approach is examined here for the heterogeneous occupancy model where\nthe occupancy and detection probabilities now depend on covariates that may\nvary between sites and over time. There is no longer an orthogonal\ntransformation but this approach effectively reduces the parameter space and\nallows fuller use of the R functionality. This permits use of existing vector\ngeneralised linear models methods to fit models for detection and allows the\ndevelopment of an iterative weighted least squares approach to fit models for\noccupancy. Efficiency is examined in a simulation study and the full maximum\nlikelihood and two-stage approaches are compared on several data sets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 06:22:54 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 05:49:00 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Karavarsamis", "N.", ""], ["Huggins", "R. M.", ""]]}, {"id": "1803.11451", "submitter": "Shashank Singh", "authors": "Shashank Singh, Bharath K. Sriperumbudur, Barnab\\'as P\\'oczos", "title": "Minimax Estimation of Quadratic Fourier Functionals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study estimation of (semi-)inner products between two nonparametric\nprobability distributions, given IID samples from each distribution. These\nproducts include relatively well-studied classical $\\mathcal{L}^2$ and Sobolev\ninner products, as well as those induced by translation-invariant reproducing\nkernels, for which we believe our results are the first. We first propose\nestimators for these quantities, and the induced (semi)norms and\n(pseudo)metrics. We then prove non-asymptotic upper bounds on their mean\nsquared error, in terms of weights both of the inner product and of the two\ndistributions, in the Fourier basis. Finally, we prove minimax lower bounds\nthat imply rate-optimality of the proposed estimators over Fourier ellipsoids.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2018 13:41:42 GMT"}, {"version": "v2", "created": "Sat, 1 Sep 2018 10:48:41 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Singh", "Shashank", ""], ["Sriperumbudur", "Bharath K.", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}]