[{"id": "1905.00177", "submitter": "Jay Bartroff", "authors": "Xinrui He and Jay Bartroff", "title": "Asymptotically optimal sequential FDR and pFDR control with (or without)\n  prior information on the number of signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate asymptotically optimal multiple testing procedures for streams\nof sequential data in the context of prior information on the number of false\nnull hypotheses (\"signals\"). We show that the \"gap\" and \"gap-intersection\"\nprocedures, recently proposed and shown by Song and Fellouris (2017, Electron.\nJ. Statist.) to be asymptotically optimal for controlling type 1 and 2\nfamilywise error rates (FWEs), are also asymptotically optimal for controlling\nFDR/FNR when their critical values are appropriately adjusted. Generalizing\nthis result, we show that these procedures, again with appropriately adjusted\ncritical values, are asymptotically optimal for controlling any multiple\ntesting error metric that is bounded between multiples of FWE in a certain\nsense. This class of metrics includes FDR/FNR but also pFDR/pFNR, the\nper-comparison and per-family error rates, and the false positive rate. Our\nanalysis includes asymptotic regimes in which the number of null hypotheses\napproaches $\\infty$ as the type 1 and 2 error metrics approach $0$.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 04:01:17 GMT"}, {"version": "v2", "created": "Tue, 3 Dec 2019 18:30:56 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 03:28:47 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["He", "Xinrui", ""], ["Bartroff", "Jay", ""]]}, {"id": "1905.00206", "submitter": "Maurizia Rossi", "authors": "Elena Di Bernardino, Anne Estrade, Maurizia Rossi", "title": "On the excursion area of perturbed Gaussian fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate Lipschitz-Killing curvatures for excursion sets of random\nfields on $\\mathbb R^2$ under small spatial-invariant random perturbations. An\nexpansion formula for mean curvatures is derived when the magnitude of the\nperturbation vanishes, which recovers the Gaussian Kinematic Formula at the\nlimit by contiguity of the model. We develop an asymptotic study of the\nperturbed excursion area behaviour that leads to a quantitative non-Gaussian\nlimit theorem, in Wasserstein distance, for fixed small perturbations and\ngrowing domain. When letting both the perturbation vanish and the domain grow,\na standard Central Limit Theorem follows. Taking advantage of these results, we\npropose an estimator for the perturbation which turns out to be asymptotically\nnormal and unbiased, allowing to make inference through sparse information on\nthe field.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 07:09:50 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Di Bernardino", "Elena", ""], ["Estrade", "Anne", ""], ["Rossi", "Maurizia", ""]]}, {"id": "1905.00352", "submitter": "Bo-Qiang Ma", "authors": "Mingshu Cong, Congqiao Li, Bo-Qiang Ma", "title": "First digit law from Laplace transform", "comments": "10 latex pages, 8 figures, final version in journal publication", "journal-ref": "Phys. Lett. A 383 (2019) 1836-1844", "doi": "10.1016/j.physleta.2019.03.017", "report-no": null, "categories": "stat.OT math.ST physics.data-an stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The occurrence of digits 1 through 9 as the leftmost nonzero digit of numbers\nfrom real-world sources is distributed unevenly according to an empirical law,\nknown as Benford's law or the first digit law. It remains obscure why a variety\nof data sets generated from quite different dynamics obey this particular law.\nWe perform a study of Benford's law from the application of the Laplace\ntransform, and find that the logarithmic Laplace spectrum of the digital\nindicator function can be approximately taken as a constant. This particular\nconstant, being exactly the Benford term, explains the prevalence of Benford's\nlaw. The slight variation from the Benford term leads to deviations from\nBenford's law for distributions which oscillate violently in the inverse\nLaplace space. We prove that the whole family of completely monotonic\ndistributions can satisfy Benford's law within a small bound. Our study\nsuggests that Benford's law originates from the way that we write numbers, thus\nshould be taken as a basic mathematical knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 16:40:24 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Cong", "Mingshu", ""], ["Li", "Congqiao", ""], ["Ma", "Bo-Qiang", ""]]}, {"id": "1905.00425", "submitter": "Surojit Biswas", "authors": "Surojit Biswas and Nitin Gupta", "title": "Stochastic ordering results in parallel and series systems with Gumble\n  distributed random variables", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The stochastic comparisons of parallel and series system are worthy of study.\nIn this paper, we present some stochastic comparisons of parallel and series\nsystems having independent components from Gumble distribution with two\nparameters (one location and one shape). Here, we first put a condition for the\nlikelihood ratio ordering of the parallel systems and second we use the concept\nof vector majorization technique to compare the systems by the reversed hazard\nrate ordering, the hazard rate ordering, the dispersive ordering, and the less\nuncertainty ordering with respect to the location parameter.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 17:46:03 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Biswas", "Surojit", ""], ["Gupta", "Nitin", ""]]}, {"id": "1905.00516", "submitter": "Steffen Lauritzen", "authors": "Steffen Lauritzen, Caroline Uhler, Piotr Zwiernik", "title": "Total positivity in exponential families with application to binary\n  variables", "comments": null, "journal-ref": "Annals of Statistics 2021, Vol. 49, 1436-1459", "doi": "10.1214/20-AOS2007", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study exponential families of distributions that are multivariate totally\npositive of order 2 (MTP2), show that these are convex exponential families,\nand derive conditions for existence of the MLE. Quadratic exponential familes\nof MTP2 distributions contain attractive Gaussian graphical models and\nferromagnetic Ising models as special examples. We show that these are defined\nby intersecting the space of canonical parameters with a polyhedral cone whose\nfaces correspond to conditional independence relations. Hence MTP2 serves as an\nimplicit regularizer for quadratic exponential families and leads to sparsity\nin the estimated graphical model. We prove that the maximum likelihood\nestimator (MLE) in an MTP2 binary exponential family exists if and only if both\nof the sign patterns $(1,-1)$ and $(-1,1)$ are represented in the sample for\nevery pair of variables; in particular, this implies that the MLE may exist\nwith $n=d$ observations, in stark contrast to unrestricted binary exponential\nfamilies where $2^d$ observations are required. Finally, we provide a novel and\nglobally convergent algorithm for computing the MLE for MTP2 Ising models\nsimilar to iterative proportional scaling and apply it to the analysis of data\nfrom two psychological disorders.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2019 22:03:50 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 10:19:53 GMT"}, {"version": "v3", "created": "Sun, 26 Jul 2020 08:39:25 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Lauritzen", "Steffen", ""], ["Uhler", "Caroline", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "1905.00744", "submitter": "Yinchu Zhu", "authors": "Jelena Bradic, Stefan Wager, Yinchu Zhu", "title": "Sparsity Double Robust Inference of Average Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular methods for building confidence intervals on causal effects\nunder high-dimensional confounding require strong \"ultra-sparsity\" assumptions\nthat may be difficult to validate in practice. To alleviate this difficulty, we\nhere study a new method for average treatment effect estimation that yields\nasymptotically exact confidence intervals assuming that either the conditional\nresponse surface or the conditional probability of treatment allows for an\nultra-sparse representation (but not necessarily both). This guarantee allows\nus to provide valid inference for average treatment effect in high dimensions\nunder considerably more generality than available baselines. In addition, we\nshowcase that our results are semi-parametrically efficient.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 13:47:15 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Bradic", "Jelena", ""], ["Wager", "Stefan", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1905.00860", "submitter": "Fran\\c{c}ois Monard", "authors": "Fran\\c{c}ois Monard, Richard Nickl and Gabriel P. Paternain", "title": "Consistent Inversion of Noisy Non-Abelian X-Ray Transforms", "comments": "51 pages, 5 figures; to appear in Comm. Pure Appl. Math", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $M$ a simple surface, the non-linear statistical inverse problem of\nrecovering a matrix field $\\Phi: M \\to \\mathfrak{so}(n)$ from discrete, noisy\nmeasurements of the $SO(n)$-valued scattering data $C_\\Phi$ of a solution of a\nmatrix ODE is considered ($n\\geq 2$). Injectivity of the map $\\Phi \\mapsto\nC_\\Phi$ was established by [Paternain, Salo, Uhlmann; Geom.Funct.Anal. 2012].\n  A statistical algorithm for the solution of this inverse problem based on\nGaussian process priors is proposed, and it is shown how it can be implemented\nby infinite-dimensional MCMC methods. It is further shown that as the number\n$N$ of measurements of point-evaluations of $C_\\Phi$ increases, the statistical\nerror in the recovery of $\\Phi$ converges to zero in $L^2(M)$-distance at a\nrate that is algebraic in $1/N$, and approaches $1/\\sqrt N$ for smooth matrix\nfields $\\Phi$. The proof relies, among other things, on a new stability\nestimate for the inverse map $C_\\Phi \\to \\Phi$.\n  Key applications of our results are discussed in the case $n=3$ to\npolarimetric neutron tomography, see [Desai et al., Nature Sc.Rep. 2018] and\n[Hilger et al., Nature Comm. 2018]\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 17:13:53 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 23:57:56 GMT"}, {"version": "v3", "created": "Mon, 1 Jun 2020 05:53:35 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Monard", "Fran\u00e7ois", ""], ["Nickl", "Richard", ""], ["Paternain", "Gabriel P.", ""]]}, {"id": "1905.00959", "submitter": "R\\'emy Garnier", "authors": "Pierre Alquier, Karine Bertin, Paul Doukhan, R\\'emy Garnier", "title": "High dimensional VAR with low rank transition", "comments": null, "journal-ref": "Statistics and Computing, 2020, vol. 30, pp. 1139-1153", "doi": "10.1007/s11222-020-09929-7", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a vector auto-regressive (VAR) model with a low-rank constraint on\nthe transition matrix. This new model is well suited to predict\nhigh-dimensional series that are highly correlated, or that are driven by a\nsmall number of hidden factors. We study estimation, prediction, and rank\nselection for this model in a very general setting. Our method shows excellent\nperformances on a wide variety of simulated datasets. On macro-economic data\nfrom Giannone et al. (2015), our method is competitive with state-of-the-art\nmethods in small dimension, and even improves on them in high dimension.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 21:00:58 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 13:10:52 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Alquier", "Pierre", ""], ["Bertin", "Karine", ""], ["Doukhan", "Paul", ""], ["Garnier", "R\u00e9my", ""]]}, {"id": "1905.01021", "submitter": "Leo Pasquazzi", "authors": "Leo Pasquazzi", "title": "Functional central limit theorems for conditional Poisson sampling", "comments": "arXiv admin note: text overlap with arXiv:1902.09169", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides refined versions of some known functional central limit\ntheorems for conditional Poisson sampling which are more suitable for\napplications. The theorems presented in this paper are generalizations of some\nresults that have been recently published by \\citet*{Bertail_2017}. The\nasymptotic equicontinuity part of the proofs presented in this paper is based\non the same idea as in \\citep{Bertail_2017} but some of the missing details are\nprovided. On the way to the functional central limit theorems, this paper\nprovides a detailed discussion of what must be done in order to prove\nconditional and unconditional weak convergence in bounded function spaces in\nthe context of survey sampling. The results from this discussion can be useful\nto prove further weak convergence results.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2019 11:36:12 GMT"}, {"version": "v2", "created": "Mon, 17 Jun 2019 10:02:10 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Pasquazzi", "Leo", ""]]}, {"id": "1905.01096", "submitter": "Grigory Franguridi", "authors": "Grigory Franguridi, Hyungsik Roger Moon", "title": "A Uniform Bound on the Operator Norm of Sub-Gaussian Random Matrices and\n  Its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an $N \\times T$ random matrix $X(\\beta)$ with weakly dependent uniformly\nsub-Gaussian entries $x_{it}(\\beta)$ that may depend on a possibly\ninfinite-dimensional parameter $\\beta\\in \\mathbf{B}$, we obtain a uniform bound\non its operator norm of the form $\\mathbb{E} \\sup_{\\beta \\in \\mathbf{B}}\n||X(\\beta)|| \\leq CK \\left(\\sqrt{\\max(N,T)} +\n\\gamma_2(\\mathbf{B},d_\\mathbf{B})\\right)$, where $C$ is an absolute constant,\n$K$ controls the tail behavior of (the increments of) $x_{it}(\\cdot)$, and\n$\\gamma_2(\\mathbf{B},d_\\mathbf{B})$ is Talagrand's functional, a measure of\nmulti-scale complexity of the metric space $(\\mathbf{B},d_\\mathbf{B})$. We\nillustrate how this result may be used for estimation that seeks to minimize\nthe operator norm of moment conditions as well as for estimation of the maximal\nnumber of factors with functional data.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 09:58:07 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 19:34:06 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 06:53:07 GMT"}, {"version": "v4", "created": "Sat, 10 Apr 2021 22:44:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Franguridi", "Grigory", ""], ["Moon", "Hyungsik Roger", ""]]}, {"id": "1905.01282", "submitter": "Frederic Koehler", "authors": "Jonathan Kelner, Frederic Koehler, Raghu Meka, Ankur Moitra", "title": "Learning Some Popular Gaussian Graphical Models without Condition Number\n  Bounds", "comments": "V2: Updated version with some new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Graphical Models (GGMs) have wide-ranging applications in machine\nlearning and the natural and social sciences. In most of the settings in which\nthey are applied, the number of observed samples is much smaller than the\ndimension and they are assumed to be sparse. While there are a variety of\nalgorithms (e.g. Graphical Lasso, CLIME) that provably recover the graph\nstructure with a logarithmic number of samples, they assume various conditions\nthat require the precision matrix to be in some sense well-conditioned.\n  Here we give the first polynomial-time algorithms for learning attractive\nGGMs and walk-summable GGMs with a logarithmic number of samples without any\nsuch assumptions. In particular, our algorithms can tolerate strong\ndependencies among the variables. Our result for structure recovery in\nwalk-summable GGMs is derived from a more general result for efficient sparse\nlinear regression in walk-summable models without any norm dependencies. We\ncomplement our results with experiments showing that many existing algorithms\nfail even in some simple settings where there are long dependency chains,\nwhereas ours do not.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2019 17:26:18 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 03:11:45 GMT"}, {"version": "v3", "created": "Mon, 9 Mar 2020 00:41:15 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Kelner", "Jonathan", ""], ["Koehler", "Frederic", ""], ["Meka", "Raghu", ""], ["Moitra", "Ankur", ""]]}, {"id": "1905.01402", "submitter": "Pengfei Li", "authors": "Jiahua Chen, Pengfei Li, Jing Qin, and Tao Yu", "title": "Test for homogeneity with unordered paired observations", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In some applications, an experimental unit is composed of two distinct but\nrelated subunits. The response from such a unit is $(X_{1}, X_{2})$ but we\nobserve only $Y_1 = \\min\\{X_{1},X_{2}\\}$ and $Y_2 = \\max\\{X_{1},X_{2}\\}$, i.e.,\nthe subunit identities are not observed. We call $(Y_1, Y_2)$ unordered paired\nobservations. Based on unordered paired observations $\\{(Y_{1i},\nY_{2i})\\}_{i=1}^n$, we are interested in whether the marginal distributions for\n$X_1$ and $X_2$ are identical. Testing methods are available in the literature\nunder the assumptions that $Var(X_1) = Var(X_2)$ and $Cov(X_1, X_2) = 0$.\nHowever, by extensive simulation studies, we observe that when one or both\nassumptions are violated, these methods have inflated type I errors or much\nlower powers. In this paper, we study the likelihood ratio test statistics for\nvarious scenarios and explore their limiting distributions without these\nrestrictive assumptions. Furthermore, we develop Bartlett correction formulae\nfor these statistics to enhance their precision when the sample size is not\nlarge. Simulation studies and real-data examples are used to illustrate the\nefficacy of the proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 01:18:01 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Chen", "Jiahua", ""], ["Li", "Pengfei", ""], ["Qin", "Jing", ""], ["Yu", "Tao", ""]]}, {"id": "1905.01434", "submitter": "Atin Gayen", "authors": "Atin Gayen and M. Ashok Kumar", "title": "Projection Theorems and Estimating Equations for Power-Law Models", "comments": "New simulation results added stating the applicability of the\n  generalized estimators: (1) comparing the robust estimators for Student\n  distributions with maximum likelihood estimators, (2) Hellinger estimators\n  for Cauchy distributions using two kernel density estimates for the sample\n  empirical measure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend projection theorems concerning Hellinger and Jones et al.\ndivergences to the continuous case. These projection theorems reduce certain\nestimation problems on generalized exponential models to linear problems. We\nintroduce the notion of regularity for generalized exponential models and show\nthat the projection theorems in this case are similar to the ones in discrete\nand canonical case. We also apply these ideas to solve certain estimation\nproblems concerning Student and Cauchy distributions.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 05:42:48 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2019 14:06:03 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 13:54:42 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Gayen", "Atin", ""], ["Kumar", "M. Ashok", ""]]}, {"id": "1905.01494", "submitter": "Yuta Koike", "authors": "Yuta Koike", "title": "De-biased graphical Lasso for high-frequency data", "comments": "38 pages", "journal-ref": null, "doi": "10.3390/e22040456", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a new statistical inference theory for the precision\nmatrix of high-frequency data in a high-dimensional setting. The focus is not\nonly on point estimation but also on interval estimation and hypothesis testing\nfor entries of the precision matrix. To accomplish this purpose, we establish\nan abstract asymptotic theory for the weighted graphical Lasso and its\nde-biased version without specifying the form of the initial covariance\nestimator. We also extend the scope of the theory to the case that a known\nfactor structure is present in the data. The developed theory is applied to the\nconcrete situation where we can use the realized covariance matrix as the\ninitial covariance estimator, and we obtain a feasible asymptotic distribution\ntheory to construct (simultaneous) confidence intervals and (multiple) testing\nprocedures for entries of the precision matrix.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 13:55:03 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Koike", "Yuta", ""]]}, {"id": "1905.01502", "submitter": "Ingrid Blaschzyk", "authors": "Ingrid Blaschzyk and Ingo Steinwart", "title": "Improved Classification Rates for Localized SVMs", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localized support vector machines solve SVMs on many spatially defined small\nchunks and one of their main characteristics besides the computational benefit\ncompared to global SVMs is the freedom of choosing arbitrary kernel and\nregularization parameter on each cell. We take advantage of this observation to\nderive global learning rates for localized SVMs with Gaussian kernels and hinge\nloss. Under certain assumptions our rates outperform known classification rates\nfor localized SVMs, for global SVMs, and other learning algorithms based on\ne.g., plug-in rules, trees, or DNNs. These rates are achieved under a set of\nmargin conditions that describe the behavior of the data-generating\ndistribution, where no assumption on the existence of a density is made. We\nobserve that a margin condition that relates the distance to the decision\nboundary to the amount of noise is crucial to obtain rates. The statistical\nanalysis relies on a careful analysis of the excess risk which includes a\nseparation of the input space into a subset that is close to the decision\nboundary and into a subset that is sufficiently far away. Moreover, we show\nthat our rates are obtained adaptively, that is, without knowing the parameters\nresulting from the margin conditions.\n", "versions": [{"version": "v1", "created": "Sat, 4 May 2019 14:26:57 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 10:53:27 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Blaschzyk", "Ingrid", ""], ["Steinwart", "Ingo", ""]]}, {"id": "1905.01713", "submitter": "Hao Wu", "authors": "Hao Wu and Raj Rao Nadakuditi", "title": "Free Component Analysis: Theory, Algorithms & Applications", "comments": "68 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a method for unmixing mixtures of freely independent random\nvariables in a manner analogous to the independent component analysis (ICA)\nbased method for unmixing independent random variables from their additive\nmixtures. Random matrices play the role of free random variables in this\ncontext so the method we develop, which we call Free component analysis (FCA),\nunmixes matrices from additive mixtures of matrices. Thus, while the mixing\nmodel is standard, the novelty and difference in unmixing performance comes\nfrom the introduction of a new statistical criteria, derived from free\nprobability theory, that quantify freeness analogous to how kurtosis and\nentropy quantify independence. We describe the theory, the various algorithms,\nand compare FCA to vanilla ICA which does not account for spatial or temporal\nstructure. We highlight why the statistical criteria make FCA also vanilla\ndespite its matricial underpinnings and show that FCA performs comparably to,\nand often better than, (vanilla) ICA in every application, such as image and\nspeech unmixing, where ICA has been known to succeed. Our computational\nexperiments suggest that not-so-random matrices, such as images and\nspectrograms of waveforms are (closer to being) freer \"in the wild\" than we\nmight have theoretically expected.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2019 16:20:21 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 02:27:41 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wu", "Hao", ""], ["Nadakuditi", "Raj Rao", ""]]}, {"id": "1905.01774", "submitter": "Stepan Grinek", "authors": "Stepan Grinek", "title": "Exact Largest Eigenvalue Distribution for Doubly Singular Beta Ensemble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \\cite{Diaz} beta type I and II doubly singular distributions were\nintroduced and their densities and the joint densities of nonzero eigenvalues\nwere derived. In such matrix variate distributions $p$, the dimension of two\nsingular Wishart distributions defining beta distribution is larger than $m$\nand $q$, degrees of freedom of Wishart matrices. We found simple formula to\ncompute exact largest eigenvalue distribution for doubly singular beta ensemble\nin case of identity scale matrix, $\\Sigma=I$. Distribution is presented in\nterms of existing expression for CDF of Roy's statistic: $\\lambda_{max} \\sim\nmax \\ eig\\left\\{ W_q(m, I)W_q(p-m+q, I)^{-1}\\right\\}$, where $W_k(n, I)$ is\nWishart distribution with $k$ dimensions, $n$ degrees of freedom and identity\nscale matrix.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 00:37:42 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 19:19:28 GMT"}, {"version": "v3", "created": "Sat, 4 Jan 2020 17:10:51 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Grinek", "Stepan", ""]]}, {"id": "1905.01840", "submitter": "Kentaro Minami", "authors": "Kentaro Minami", "title": "Estimating Piecewise Monotone Signals", "comments": "Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating piecewise monotone vectors. This problem\ncan be seen as a generalization of the isotonic regression that allows a small\nnumber of order-violating changepoints. We focus mainly on the performance of\nthe nearly-isotonic regression proposed by Tibshirani et al. (2011). We derive\nrisk bounds for the nearly-isotonic regression estimators that are adaptive to\npiecewise monotone signals. The estimator achieves a near minimax convergence\nrate over certain classes of piecewise monotone signals under a weak\nassumption. Furthermore, we present an algorithm that can be applied to the\nnearly-isotonic type estimators on general weighted graphs. The simulation\nresults suggest that the nearly-isotonic regression performs as well as the\nideal estimator that knows the true positions of changepoints.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2019 06:36:24 GMT"}, {"version": "v2", "created": "Sat, 7 Mar 2020 05:42:55 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Minami", "Kentaro", ""]]}, {"id": "1905.02406", "submitter": "Francesca Fortunato", "authors": "Laura Anderlucci, Francesca Fortunato, Angela Montanari", "title": "One-class classification with application to forensic analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of broken glass is forensically important to reconstruct the\nevents of a criminal act. In particular, the comparison between the glass\nfragments found on a suspect (recovered cases) and those collected on the crime\nscene (control cases) may help the police to correctly identify the\noffender(s). The forensic issue can be framed as a one-class classification\nproblem. One-class classification is a recently emerging and special\nclassification task, where only one class is fully known (the so-called target\nclass), while information on the others is completely missing. We propose to\nconsider classic Gini's transvariation probability as a measure of typicality,\ni.e. a measure of resemblance between an observation and a set of well-known\nobjects (the control cases). The aim of the proposed Transvariation-based\nOne-Class Classifier (TOCC) is to identify the best boundary around the target\nclass, that is, to recognise as many target objects as possible while rejecting\nall those deviating from this class.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 08:41:37 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Anderlucci", "Laura", ""], ["Fortunato", "Francesca", ""], ["Montanari", "Angela", ""]]}, {"id": "1905.02508", "submitter": "Morten Overgaard", "authors": "Morten Overgaard, Stefan Nygaard Hansen", "title": "On the assumption of independent right censoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various assumptions on a right-censoring mechanism to ensure consistency of\nthe Kaplan--Meier and Aalen--Johansen estimators in a competing risks setting\nare studied. Specifically, eight different assumptions are seen to fall in two\ncategories: a weaker identifiability assumption, which is the weakest possible\nassumption in a precise sense, and a stronger representativity assumption which\nensures the existence of an independent censoring time. When a given censoring\ntime is considered, similar assumptions can be made on the censoring time. This\nallows for a characterization of so-called pointwise independence as well as\nfull independence of censoring time and event time and type. Examples\nillustrate how the various assumptions differ.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 12:49:19 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Overgaard", "Morten", ""], ["Hansen", "Stefan Nygaard", ""]]}, {"id": "1905.02511", "submitter": "Marta Ferreira", "authors": "Helena Ferreira and Marta Ferreira", "title": "Tail dependence and smoothness", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risk of catastrophes is related to the possibility of occurring extreme\nvalues. Several statistical methodologies have been developed in order to\nevaluate the propensity of a process for the occurrence of high values and the\npermanence of these in time. The extremal index $\\theta$ (Leadbetter 1983)\nallows to infer the tendency for clustering of high values, but does not allow\nto evaluate the greater or less amount of oscillations in a cluster. The\nestimation of $\\theta$ entails the validation of local dependence conditions\nregulating the distance between high levels oscillations of the process, which\nis difficult to implement in practice. In this work, we propose a smoothness\ncoefficient to evaluate the degree of smoothness/oscillation in the trajectory\nof a process, with an intuitive reading and simple estimation. Application in\nsome examples will be provided. We will see that, in a stationary process, it\ncoincides with the tail dependence coefficient $\\lambda$ (Sibuya 1960, Joe\n1997), providing a new interpretation of the latter. This relationship will\ninspire a new estimator for $\\lambda$ and its performance will be evaluated\nbased on a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 12:55:38 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Ferreira", "Helena", ""], ["Ferreira", "Marta", ""]]}, {"id": "1905.02618", "submitter": "Fr\\'ed\\'eric Pro\\\"ia", "authors": "Fr\\'ed\\'eric Pro\\\"ia", "title": "Moderate deviations in a class of stable but nearly unstable processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stable but nearly unstable autoregressive process of any order.\nThe bridge between stability and instability is expressed by a time-varying\ncompanion matrix $A_{n}$ with spectral radius $\\rho(A_{n}) < 1$ satisfying\n$\\rho(A_{n}) \\rightarrow 1$. In that framework, we establish a moderate\ndeviation principle for the empirical covariance only relying on the elements\nof $A_{n}$ through $1-\\rho(A_{n})$ and, as a by-product, we establish a\nmoderate deviation principle for the OLS estimator when $\\Gamma$, the\nrenormalized asymptotic variance of the process, is invertible. Finally, when\n$\\Gamma$ is singular, we also provide a compromise in the form of a moderate\ndeviation principle for a penalized version of the estimator. Our proofs\nessentially rely on truncations and deviations of $m_{n}$--dependent sequences,\nwith an unbounded rate $(m_{n})$.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 14:45:35 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 10:40:22 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Pro\u00efa", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1905.02656", "submitter": "Matthias Hammer", "authors": "Matthias Hammer, Reinhard H\\\"opfner, Tobias Berg", "title": "Ergodic branching diffusions with immigration: properties of invariant\n  occupation measure, identification of particles under high-frequency\n  observation, and estimation of the diffusion coefficient at nonparametric\n  rates", "comments": "60 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In branching diffusions with immigration (BDI), particles travel on\nindependent diffusion paths in $\\mathbb{R}^d$, branch at position-dependent\nrates and leave offspring -- randomly scattered around the parent's death\nposition -- according to position-dependent laws. We specify a set of\nconditions which grants ergodicity such that the invariant occupation measure\nis of finite total mass and admits a continuous Lebesgue density. Under\ndiscrete-time observation, BDI configurations being recorded at discrete times\n$i\\Delta$ only, $i\\in\\mathbb{N}_0$, we lose information about particle\nidentities between successive observation times. We present a reconstruction\nalgorithm which in a high-frequency setting (asymptotics $\\Delta\\downarrow 0$)\nallows to reconstruct correctly a sufficiently large proportion of particle\nidentities, and thus allows to recover $\\Delta$-increments of unobserved\ndiffusion paths on which particles are travelling. Picking some few well-chosen\nobservations we fill regression schemes which, on cubes $A$ where the invariant\noccupation density is strictly positive, allow to estimate the diffusion\ncoefficient of the one-particle motion at nonparametric rates.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 16:02:18 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Hammer", "Matthias", ""], ["H\u00f6pfner", "Reinhard", ""], ["Berg", "Tobias", ""]]}, {"id": "1905.02721", "submitter": "Jin Xu", "authors": "Jin Xu and Xu He and Xiaojun Duan and Zhengming Wang", "title": "Sliced Latin hypercube designs with arbitrary run sizes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latin hypercube designs achieve optimal univariate stratifications and are\nuseful for computer experiments. Sliced Latin hypercube designs are Latin\nhypercube designs that can be partitioned into smaller Latin hypercube designs.\nIn this work, we give, to the best of our knowledge, the first construction of\nsliced Latin hypercube designs that allow arbitrarily chosen run sizes for the\nslices. We also provide an algorithm to reduce correlations of our proposed\ndesigns.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 16:05:48 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Xu", "Jin", ""], ["He", "Xu", ""], ["Duan", "Xiaojun", ""], ["Wang", "Zhengming", ""]]}, {"id": "1905.02897", "submitter": "Paula Saavedra-Nieves", "authors": "Alberto Rodr\\'iguez-Casal and Paula Saavedra-Nieves", "title": "Minimax Hausdorff estimation of density level sets", "comments": "arXiv admin note: substantial text overlap with arXiv:1411.7687", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample of points from some unknown density, we propose a\ndata-driven method for estimating density level sets under the r-convexity\nassumption. This shape condition generalizes the convexity property. However,\nthe main problem in practice is that r is an unknown geometric characteristic\nof the set related to its curvature. A stochastic algorithm is proposed for\nselecting its optimal value from the data. The resulting reconstruction of the\nlevel set is able to achieve minimax rates for Hausdorff metric and distance in\nmeasure, up to log factors, uniformly on the level of the set.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2019 15:35:40 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Rodr\u00edguez-Casal", "Alberto", ""], ["Saavedra-Nieves", "Paula", ""]]}, {"id": "1905.03009", "submitter": "Lutz Duembgen", "authors": "Lutz Duembgen and Richard Samworth and Jon Wellner", "title": "Bounding distributional errors via density ratios", "comments": "Versions 3-5 contain better bounds for the log-gamma function and its\n  increments; Version 5 resulted from a major revision", "journal-ref": "Bernoulli 27(2), 2021, pp. 818-852", "doi": "10.3150/20-BEJ1256", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some new and explicit error bounds for the approximation of\ndistributions. The approximation error is quantified by the maximal density\nratio of the distribution $Q$ to be approximated and its proxy $P$. This\nnon-symmetric measure is more informative than and implies bounds for the total\nvariation distance. Explicit approximation problems include, among others,\nhypergeometric by binomial distributions, binomial by Poisson distributions,\nand beta by gamma distributions. In many cases we provide both upper and\n(matching) lower bounds.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 11:18:50 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 14:32:31 GMT"}, {"version": "v3", "created": "Wed, 3 Jul 2019 20:44:05 GMT"}, {"version": "v4", "created": "Wed, 24 Jul 2019 06:43:18 GMT"}, {"version": "v5", "created": "Mon, 3 Feb 2020 06:41:38 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Duembgen", "Lutz", ""], ["Samworth", "Richard", ""], ["Wellner", "Jon", ""]]}, {"id": "1905.03353", "submitter": "Ioannis Panageas", "authors": "Constantinos Daskalakis, Nishanth Dikkala, Ioannis Panageas", "title": "Regression from Dependent Observations", "comments": "33 pages, in proceedings of STOC 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard linear and logistic regression models assume that the response\nvariables are independent, but share the same linear relationship to their\ncorresponding vectors of covariates. The assumption that the response variables\nare independent is, however, too strong. In many applications, these responses\nare collected on nodes of a network, or some spatial or temporal domain, and\nare dependent. Examples abound in financial and meteorological applications,\nand dependencies naturally arise in social networks through peer effects.\nRegression with dependent responses has thus received a lot of attention in the\nStatistics and Economics literature, but there are no strong consistency\nresults unless multiple independent samples of the vectors of dependent\nresponses can be collected from these models. We present computationally and\nstatistically efficient methods for linear and logistic regression models when\nthe response variables are dependent on a network. Given one sample from a\nnetworked linear or logistic regression model and under mild assumptions, we\nprove strong consistency results for recovering the vector of coefficients and\nthe strength of the dependencies, recovering the rates of standard regression\nunder independent observations. We use projected gradient descent on the\nnegative log-likelihood, or negative log-pseudolikelihood, and establish their\nstrong convexity and consistency using concentration of measure for dependent\nrandom variables.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2019 21:11:50 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 10:26:36 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Dikkala", "Nishanth", ""], ["Panageas", "Ioannis", ""]]}, {"id": "1905.03495", "submitter": "Aurelien Garivier", "authors": "Aur\\'elien Garivier (UMPA-ENSL, MC2), Emilie Kaufmann (SEQUEL)", "title": "Non-Asymptotic Sequential Tests for Overlapping Hypotheses and\n  application to near optimal arm identification in bandit models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study sequential testing problems with \\emph{overlapping}\nhypotheses. We first focus on the simple problem of assessing if the mean $\\mu$\nof a Gaussian distribution is $\\geq -\\epsilon$ or $\\leq \\epsilon$; if\n$\\mu\\in(-\\epsilon,\\epsilon)$, both answers are considered to be correct. Then,\nwe consider PAC-best arm identification in a bandit model: given $K$\nprobability distributions on $\\mathbb{R}$ with means $\\mu_1,\\dots,\\mu_K$, we\nderive the asymptotic complexity of identifying, with risk at most $\\delta$, an\nindex $I\\in\\{1,\\dots,K\\}$ such that $\\mu_I\\geq \\max_i\\mu_i -\\epsilon$. We\nprovide non asymptotic bounds on the error of a parallel General Likelihood\nRatio Test, which can also be used for more general testing problems. We\nfurther propose lower bound on the number of observation needed to identify a\ncorrect hypothesis. Those lower bounds rely on information-theoretic arguments,\nand specifically on two versions of a change of measure lemma (a high-level\nform, and a low-level form) whose relative merits are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 09:04:03 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Garivier", "Aur\u00e9lien", "", "UMPA-ENSL, MC2"], ["Kaufmann", "Emilie", "", "SEQUEL"]]}, {"id": "1905.03530", "submitter": "Maria Michela Dickson", "authors": "Maria Michela Dickson, Giuseppe Espa, Lorenzo Fattorini", "title": "Double-calibration estimators accounting for under-coverage and\n  nonresponse in socio-economic surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under-coverage and nonresponse problems are jointly present in most\nsocio-economic surveys. The purpose of this paper is to propose a completely\ndesign-based estimation strategy that accounts for both problems without\nresorting to models but simply performing a two-step calibration. The first\ncalibration exploits a set of auxiliary variables only available for the units\nin the sampled population to account for nonresponse. The second calibration\nexploits a different set of auxiliary variables available for the whole\npopulation, to account for under-coverage. The two calibrations are then\nunified in a double-calibration estimator. Mean and variance of the estimator\nare derived up to the first order of approximation. Conditions ensuring\napproximate unbiasedness are derived and discussed. The strategy is empirically\nchecked by a simulation study performed on a set of artificial populations. A\ncase study is lead on Danish data coming from the European Union Statistics on\nIncome and Living Conditions survey. The strategy proposed is flexible and\nsuitable in most situations in which both under-coverage and nonresponse are\npresent.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 11:10:29 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Dickson", "Maria Michela", ""], ["Espa", "Giuseppe", ""], ["Fattorini", "Lorenzo", ""]]}, {"id": "1905.03657", "submitter": "Daniel Eck", "authors": "Daniel J. Eck and Forrest W. Crawford", "title": "Efficient and minimal length parametric conformal prediction regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformal prediction methods construct prediction regions for iid data that\nare valid in finite samples. We provide two parametric conformal prediction\nregions that are applicable for a wide class of continuous statistical models.\nThis class of statistical models includes generalized linear models (GLMs) with\ncontinuous outcomes. Our parametric conformal prediction regions possesses\nfinite sample validity, even when the model is misspecified, and are\nasymptotically of minimal length when the model is correctly specified. The\nfirst parametric conformal prediction region is constructed through binning of\nthe predictor space, guarantees finite-sample local validity and is\nasymptotically minimal at the $\\sqrt{\\log(n)/n}$ rate when the dimension $d$ of\nthe predictor space is one or two, and converges at the\n$O\\{(\\log(n)/n)^{1/d}\\}$ rate when $d > 2$. The second parametric conformal\nprediction region is constructed by transforming the outcome variable to a\ncommon distribution via the probability integral transform, guarantees\nfinite-sample marginal validity, and is asymptotically minimal at the\n$\\sqrt{\\log(n)/n}$ rate. We develop a novel concentration inequality for\nmaximum likelihood estimation that induces these convergence rates. We analyze\nprediction region coverage properties, large-sample efficiency, and robustness\nproperties of four methods for constructing conformal prediction intervals for\nGLMs: fully nonparametric kernel-based conformal, residual based conformal,\nnormalized residual based conformal, and parametric conformal which uses the\nassumed GLM density as a conformity measure. Extensive simulations compare\nthese approaches to standard asymptotic prediction regions. The utility of the\nparametric conformal prediction region is demonstrated in an application to\ninterval prediction of glycosylated hemoglobin levels, a blood measurement used\nto diagnose diabetes.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 14:31:29 GMT"}, {"version": "v2", "created": "Tue, 22 Oct 2019 15:52:10 GMT"}, {"version": "v3", "created": "Fri, 25 Oct 2019 18:08:51 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Eck", "Daniel J.", ""], ["Crawford", "Forrest W.", ""]]}, {"id": "1905.03673", "submitter": "Chris Oates", "authors": "Wilson Ye Chen, Alessandro Barp, Fran\\c{c}ois-Xavier Briol, Jackson\n  Gorham, Mark Girolami, Lester Mackey, Chris. J. Oates", "title": "Stein Point Markov Chain Monte Carlo", "comments": "Minor bug fixed in Theorem 4 (result unchanged)", "journal-ref": "ICML 2019", "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important task in machine learning and statistics is the approximation of\na probability measure by an empirical measure supported on a discrete point\nset. Stein Points are a class of algorithms for this task, which proceed by\nsequentially minimising a Stein discrepancy between the empirical measure and\nthe target and, hence, require the solution of a non-convex optimisation\nproblem to obtain each new point. This paper removes the need to solve this\noptimisation problem by, instead, selecting each new point based on a Markov\nchain sample path. This significantly reduces the computational cost of Stein\nPoints and leads to a suite of algorithms that are straightforward to\nimplement. The new algorithms are illustrated on a set of challenging Bayesian\ninference problems, and rigorous theoretical guarantees of consistency are\nestablished.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 14:57:02 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 08:16:59 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chen", "Wilson Ye", ""], ["Barp", "Alessandro", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Gorham", "Jackson", ""], ["Girolami", "Mark", ""], ["Mackey", "Lester", ""], ["Oates", "Chris. J.", ""]]}, {"id": "1905.03735", "submitter": "Veronika Rockova", "authors": "Veronika Rockova", "title": "On Semi-parametric Bernstein-von Mises Theorems for BART", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few methods in Bayesian non-parametric statistics/ machine learning have\nreceived as much attention as Bayesian Additive Regression Trees (BART). While\nBART is now routinely performed for prediction tasks, its theoretical\nproperties began to be understood only very recently. In this work, we continue\nthe theoretical investigation of BART initiated by Rockova and van der Pas\n(2017). In particular, we study the Bernstein-von Mises (BvM) phenomenon (i.e.\nasymptotic normality) for smooth linear functionals of the regression surface\nwithin the framework of non-parametric regression with fixed covariates. As\nwith other adaptive priors, the BvM phenomenon may fail when the regularities\nof the functional and the truth are not compatible. To overcome the curse of\nadaptivity under hierarchical priors, we induce a self-similarity assumption to\nensure convergence towards a single Gaussian distribution as opposed to a\nGaussian mixture. Similar qualitative restrictions on the functional parameter\nare known to be necessary for adaptive inference. Many machine learning methods\nlack coherent probabilistic mechanisms for gauging uncertainty. BART readily\nprovides such quantification via posterior credible sets. The BvM theorem\nimplies that the credible sets are also confidence regions with the same\nasymptotic coverage. This paper presents the first asymptotic normality result\nfor BART priors, providing another piece of evidence that BART is a valid tool\nfrom a frequentist point of view.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 16:26:40 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Rockova", "Veronika", ""]]}, {"id": "1905.03814", "submitter": "Max Simchowitz", "authors": "Max Simchowitz, Kevin Jamieson", "title": "Non-Asymptotic Gap-Dependent Regret Bounds for Tabular MDPs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper establishes that optimistic algorithms attain gap-dependent and\nnon-asymptotic logarithmic regret for episodic MDPs. In contrast to prior work,\nour bounds do not suffer a dependence on diameter-like quantities or\nergodicity, and smoothly interpolate between the gap dependent\nlogarithmic-regret, and the $\\widetilde{\\mathcal{O}}(\\sqrt{HSAT})$-minimax\nrate. The key technique in our analysis is a novel \"clipped\" regret\ndecomposition which applies to a broad family of recent optimistic algorithms\nfor episodic MDPs.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2019 19:00:31 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 21:53:40 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Simchowitz", "Max", ""], ["Jamieson", "Kevin", ""]]}, {"id": "1905.04022", "submitter": "Maxime Taillardat", "authors": "Maxime Taillardat (CNRM), Anne-Laure Foug\\`eres (PSPM), Philippe\n  Naveau (LSCE), Rapha\\\"el de Fondeville (EPFL)", "title": "Extreme events evaluation using CRPS distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verification of ensemble forecasts for extreme events remains a challenging\nquestion. The general public as well as the media naturely pay particular\nattention on extreme events and conclude about the global predictive\nperformance of ensembles, which are often unskillful when they are needed.\nAshing classical verification tools to focus on such events can lead to\nunexpected behaviors. To square up these effects, thresholded and weighted\nscoring rules have been developed. Most of them use derivations of the\nContinuous Ranked Probability Score (CRPS). However, some properties of the\nCRPS for extreme events generate undesirable effects on the quality of\nverification. Using theoretical arguments and simulation examples, we\nillustrate some pitfalls of conventional verification tools and propose a\ndifferent direction to assess ensemble forecasts using extreme value theory,\nconsidering proper scores as random variables.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 09:15:38 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Taillardat", "Maxime", "", "CNRM"], ["Foug\u00e8res", "Anne-Laure", "", "PSPM"], ["Naveau", "Philippe", "", "LSCE"], ["de Fondeville", "Rapha\u00ebl", "", "EPFL"]]}, {"id": "1905.04039", "submitter": "Evgenii Chzhen", "authors": "Evgenii Chzhen (LAMA)", "title": "Optimal rates for F-score binary classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the minimax settings of binary classification with F-score under the\n$\\beta$-smoothness assumptions on the regression function $\\eta(x) =\n\\mathbb{P}(Y = 1|X = x)$ for $x \\in \\mathbb{R}^d$. We propose a classification\nprocedure which under the $\\alpha$-margin assumption achieves the rate\n$O(n^{--(1+\\alpha)\\beta/(2\\beta+d)})$ for the excess F-score. In this context,\nthe Bayes optimal classifier for the F-score can be obtained by thresholding\nthe aforementioned regression function $\\eta$ on some level $\\theta^*$ to be\nestimated. The proposed procedure is performed in a semi-supervised manner,\nthat is, for the estimation of the regression function we use a labeled dataset\nof size $n \\in \\mathbb{N}$ and for the estimation of the optimal threshold\n$\\theta^*$ we use an unlabeled dataset of size $N \\in \\mathbb{N}$.\nInterestingly, the value of $N \\in \\mathbb{N}$ does not affect the rate of\nconvergence, which indicates that it is \"harder\" to estimate the regression\nfunction $\\eta$ than the optimal threshold $\\theta^*$. This further implies\nthat the binary classification with F-score behaves similarly to the standard\nsettings of binary classification. Finally, we show that the rates achieved by\nthe proposed procedure are optimal in the minimax sense up to a constant\nfactor.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 09:52:38 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Chzhen", "Evgenii", "", "LAMA"]]}, {"id": "1905.04045", "submitter": "Johannes Krebs", "authors": "Johannes Krebs", "title": "On limit theorems for persistent Betti numbers from dependent data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study persistent Betti numbers and persistence diagrams obtained from time\nseries and random fields. It is well known that the persistent Betti function\nis an efficient descriptor of the topology of a point cloud. So far,\nconvergence results for the $(r,s)$-persistent Betti number of the $q$th\nhomology group, $\\beta^{r,s}_q$, were mainly considered for finite-dimensional\npoint cloud data obtained from i.i.d. observations or stationary point\nprocesses such as a Poisson process. In this article, we extend these\nconsiderations. We derive limit theorems for the pointwise convergence of\npersistent Betti numbers $\\beta^{r,s}_q$ in the critical regime under quite\ngeneral dependence settings.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 10:04:06 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 10:20:31 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Krebs", "Johannes", ""]]}, {"id": "1905.04119", "submitter": "Stanislav Nagy", "authors": "Stanislav Nagy and Ji\\v{r}\\'i Dvo\\v{r}\\'ak", "title": "Illumination depth", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 30:1, 78-90\n  (2021)", "doi": "10.1080/10618600.2020.1776717", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of illumination bodies studied in convex geometry is used to\namend the halfspace depth for multivariate data. The proposed notion of\nillumination enables finer resolution of the sample points, naturally breaks\nties in the associated depth-based ordering, and introduces a depth-like\nfunction for points outside the convex hull of the support of the probability\nmeasure. The illumination is, in a certain sense, dual to the halfspace depth\nmapping, and shares the majority of its beneficial properties. It is affine\ninvariant, robust, uniformly consistent, and aligns well with common\nprobability distributions.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 12:36:31 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Nagy", "Stanislav", ""], ["Dvo\u0159\u00e1k", "Ji\u0159\u00ed", ""]]}, {"id": "1905.04180", "submitter": "Bertrand Iooss", "authors": "Alejandro Ribes (EDF), Th\\'eophile Terraz (DATAMOVE), Bertrand Iooss\n  (EDF R&D PRISME, IMT, GdR MASCOT-NUM), Yvan Fournier (EDF), Bruno Raffin\n  (UGA)", "title": "Large scale in transit computation of quantiles for ensemble runs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical approach for quantiles computation requires availability of the\nfull sample before ranking it. In uncertainty quantification of numerical\nsimulation models, this approach is not suitable at exascale as large ensembles\nof simulation runs would need to gather a prohibitively large amount of data.\nThis problem is solved thanks to an on-the-fly and iterative approach based on\nthe Robbins-Monro algorithm. This approach relies on Melissa, a file avoiding,\nadaptive, fault-tolerant and elastic framework. On a validation case producing\n11 TB of data, which consists in 3000 fluid dynamics parallel simulations on a\n6M cell mesh, it allows on-line computation of spatio-temporal maps of\npercentiles.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 14:08:01 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Ribes", "Alejandro", "", "EDF"], ["Terraz", "Th\u00e9ophile", "", "DATAMOVE"], ["Iooss", "Bertrand", "", "EDF R&D PRISME, IMT, GdR MASCOT-NUM"], ["Fournier", "Yvan", "", "EDF"], ["Raffin", "Bruno", "", "UGA"]]}, {"id": "1905.04233", "submitter": "Jonas Brehmer", "authors": "Jonas Brehmer and Kirstin Strokorb", "title": "Why scoring functions cannot assess tail properties", "comments": "18 pages", "journal-ref": "Electronic Journal of Statistics, Volume 13, Number 2 (2019),\n  4015-4034", "doi": "10.1214/19-EJS1622", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the growing interest in sound forecast evaluation techniques\nwith an emphasis on distribution tails rather than average behaviour, we\ninvestigate a fundamental question arising in this context: Can statistical\nfeatures of distribution tails be elicitable, i.e. be the unique minimizer of\nan expected score? We demonstrate that expected scores are not suitable to\ndistinguish genuine tail properties in a very strong sense. Specifically, we\nintroduce the class of max-functionals, which contains key characteristics from\nextreme value theory, for instance the extreme value index. We show that its\nmembers fail to be elicitable and that their elicitation complexity is in fact\ninfinite under mild regularity assumptions. Further we prove that, even if the\ninformation of a max-functional is reported via the entire distribution\nfunction, a proper scoring rule cannot separate max-functional values. These\nfindings highlight the caution needed in forecast evaluation and statistical\ninference if relevant information is encoded by such functionals.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 16:01:41 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 07:53:15 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Brehmer", "Jonas", ""], ["Strokorb", "Kirstin", ""]]}, {"id": "1905.04281", "submitter": "Geoffrey Chinot", "authors": "Geoffrey Chinot, Guillaume Lecu\\'e, Matthieu Lerasle", "title": "Robust high dimensional learning for Lipschitz and convex losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish risk bounds for Regularized Empirical Risk Minimizers (RERM)\nwhen the loss is Lipschitz and convex and the regularization function is a\nnorm. In a first part, we obtain these results in the i.i.d. setup under\nsubgaussian assumptions on the design. In a second part, a more general\nframework where the design might have heavier tails and data may be corrupted\nby outliers both in the design and the response variables is considered. In\nthis situation, RERM performs poorly in general. We analyse an alternative\nprocedure based on median-of-means principles and called minmax MOM. We show\noptimal subgaussian deviation rates for these estimators in the relaxed\nsetting. The main results are meta-theorems allowing a wide-range of\napplications to various problems in learning theory. To show a non-exhaustive\nsample of these potential applications, it is applied to classification\nproblems with logistic loss functions regularized by LASSO and SLOPE, to\nregression problems with Huber loss regularized by Group LASSO and Total\nVariation. Another advantage of the minmax MOM formulation is that it suggests\na systematic way to slightly modify descent based algorithms used in\nhigh-dimensional statistics to make them robust to outliers. We illustrate this\nprinciple in a Simulations section where a minmax MOM version of classical\nproximal descent algorithms are turned into robust to outliers algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 17:41:13 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 02:53:22 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 17:27:39 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Chinot", "Geoffrey", ""], ["Lecu\u00e9", "Guillaume", ""], ["Lerasle", "Matthieu", ""]]}, {"id": "1905.04365", "submitter": "Tapio Helin", "authors": "Matthew M. Dunlop and Tapio Helin and Andrew M. Stuart", "title": "Hyperparameter Estimation in Bayesian MAP Estimation: Parameterizations\n  and Consistency", "comments": "36 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian formulation of inverse problems is attractive for three primary\nreasons: it provides a clear modelling framework; means for uncertainty\nquantification; and it allows for principled learning of hyperparameters. The\nposterior distribution may be explored by sampling methods, but for many\nproblems it is computationally infeasible to do so. In this situation maximum a\nposteriori (MAP) estimators are often sought. Whilst these are relatively cheap\nto compute, and have an attractive variational formulation, a key drawback is\ntheir lack of invariance under change of parameterization. This is a\nparticularly significant issue when hierarchical priors are employed to learn\nhyperparameters. In this paper we study the effect of the choice of\nparameterization on MAP estimators when a conditionally Gaussian hierarchical\nprior distribution is employed. Specifically we consider the centred\nparameterization, the natural parameterization in which the unknown state is\nsolved for directly, and the noncentred parameterization, which works with a\nwhitened Gaussian as the unknown state variable, and arises when considering\ndimension-robust MCMC algorithms; MAP estimation is well-defined in the\nnonparametric setting only for the noncentred parameterization. However, we\nshow that MAP estimates based on the noncentred parameterization are not\nconsistent as estimators of hyperparameters; conversely, we show that limits of\nfinite-dimensional centred MAP estimators are consistent as the dimension tends\nto infinity. We also consider empirical Bayesian hyperparameter estimation,\nshow consistency of these estimates, and demonstrate that they are more robust\nwith respect to noise than centred MAP estimates. An underpinning concept\nthroughout is that hyperparameters may only be recovered up to measure\nequivalence, a well-known phenomenon in the context of the Ornstein-Uhlenbeck\nprocess.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 20:03:03 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Dunlop", "Matthew M.", ""], ["Helin", "Tapio", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "1905.04396", "submitter": "Leying Guan", "authors": "Leying Guan, Rob Tibshirani", "title": "Prediction and outlier detection in classification problems", "comments": "22 pages; 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multi-class classification problem when the training data and\nthe out-of-sample test data may have different distributions and propose a\nmethod called BCOPS (balanced and conformal optimized prediction sets). BCOPS\nconstructs a prediction set $C(x)$ as a subset of class labels, possibly empty.\nIt tries to optimize the out-of-sample performance, aiming to include the\ncorrect class as often as possible, but also detecting outliers $x$, for which\nthe method returns no prediction (corresponding to $C(x)$ equal to the empty\nset). The proposed method combines supervised-learning algorithms with the\nmethod of conformal prediction to minimize a misclassification loss averaged\nover the out-of-sample distribution. The constructed prediction sets have a\nfinite-sample coverage guarantee without distributional assumptions.\n  We also propose a method to estimate the outlier detection rate of a given\nmethod. We prove asymptotic consistency and optimality of our proposals under\nsuitable assumptions and illustrate our methods on real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2019 22:56:39 GMT"}, {"version": "v2", "created": "Tue, 14 May 2019 05:00:56 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2019 16:23:07 GMT"}], "update_date": "2019-06-26", "authors_parsed": [["Guan", "Leying", ""], ["Tibshirani", "Rob", ""]]}, {"id": "1905.04578", "submitter": "Michael Levine", "authors": "Michael Levine, Inder Tecuapetla-Gomez", "title": "ACF estimation via difference schemes for a semiparametric model with\n  $m$-dependent errors", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this manuscript, we discuss a class of difference-based estimators of the\nautocovariance structure in a semiparametric regression model where the signal\nis discontinuous and the errors are serially correlated. The signal in this\nmodel consists of a sum of the function with jumps and an identifiable smooth\nfunction. A simpler form of this model has been considered earlier under the\nname of Nonparametric Jump Regression (NJRM). The estimators proposed allow us\nto bypass a complicated problem of prior estimation of the mean signal in such\na model. We provide finite-sample expressions for biases and variance of the\nproposed estimators when the errors are Gaussian. Gaussianity in the above is\nonly needed to provide explicit closed form expressions for biases and\nvariances of our estimators. Moreover, we observe that the mean squared error\nof the proposed variance estimator does not depend on either the unknown smooth\nfunction that is a part of the mean signal nor on the values of difference\nsequence coefficients. Our approach also suggests sufficient conditions for\n$\\sqrt{n}-$ consistency of the proposed estimators.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2019 19:18:47 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 21:06:18 GMT"}, {"version": "v3", "created": "Sun, 22 Mar 2020 17:59:32 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Levine", "Michael", ""], ["Tecuapetla-Gomez", "Inder", ""]]}, {"id": "1905.04667", "submitter": "Nadezhda Gribkova Dr.", "authors": "Nadezhda Gribkova and Ri\\v{c}ardas Zitikis", "title": "Functional Correlations in the Pursuit of Performance Assessment of\n  Classifiers", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical classification and machine learning, as well as in social and\nother sciences, a number of measures of association have been proposed for\nassessing and comparing individual classifiers, raters, as well as their\ngroups. In this paper, we introduce, justify, and explore several new measures\nof association, which we call CO-, ANTI- and COANTI-correlation coefficients,\nthat we demonstrate to be powerful tools for classifying confusion matrices. We\nillustrate the performance of these new coefficients using a number of\nexamples, from which we also conclude that the coefficients are new objects in\nthe sense that they differ from those already in the literature.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2019 08:43:06 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 17:18:24 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2020 08:18:20 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Gribkova", "Nadezhda", ""], ["Zitikis", "Ri\u010dardas", ""]]}, {"id": "1905.04852", "submitter": "Tetsuya Takabatake", "authors": "Masaaki Fukasawa, Tetsuya Takabatake, Rebecca Westphal", "title": "Is Volatility Rough ?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rough volatility models are continuous time stochastic volatility models\nwhere the volatility process is driven by a fractional Brownian motion with the\nHurst parameter smaller than half, and have attracted much attention since a\nseminal paper titled \"Volatility is rough\" was posted on SSRN in 2014 showing\nthat the log realized volatility time series of major stock indices have the\nsame scaling property as such a rough fractional Brownian motion has. We\nhowever find by simulations that the impressive approach tends to suggest the\nsame roughness irrespectively whether the volatility is actually rough or not;\nan overlooked estimation error of latent volatility often results in an\nillusive scaling property. Motivated by this preliminary finding, here we\ndevelop a statistical theory for a continuous time fractional stochastic\nvolatility model to examine whether the Hurst parameter is indeed estimated\nsmaller than half, that is, whether the volatility is really rough. We\nconstruct a quasi-likelihood estimator and apply it to realized volatility time\nseries. Our quasi-likelihood is based on the error distribution of the realized\nvolatility and a Whittle-type approximation to the auto-covariance of the\nlog-volatility process. We prove the consistency of our estimator under high\nfrequency asymptotics, and examine by simulations its finite sample\nperformance. Our empirical study suggests that the volatility is indeed rough;\nactually it is even rougher than considered in the literature.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 04:04:34 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 00:48:39 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Fukasawa", "Masaaki", ""], ["Takabatake", "Tetsuya", ""], ["Westphal", "Rebecca", ""]]}, {"id": "1905.04955", "submitter": "Mariia Vladimirova", "authors": "Mariia Vladimirova, Stephane Girard, Hien Nguyen, and Julyan Arbel", "title": "Sub-Weibull distributions: generalizing sub-Gaussian and sub-Exponential\n  properties to heavier-tailed distributions", "comments": "10 pages, 3 figures", "journal-ref": "Stat (2020)", "doi": "10.1002/sta4.318", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the notion of sub-Weibull distributions, which are characterised\nby tails lighter than (or equally light as) the right tail of a Weibull\ndistribution. This novel class generalises the sub-Gaussian and sub-Exponential\nfamilies to potentially heavier-tailed distributions. Sub-Weibull distributions\nare parameterized by a positive tail index $\\theta$ and reduce to sub-Gaussian\ndistributions for $\\theta=1/2$ and to sub-Exponential distributions for\n$\\theta=1$. A characterisation of the sub-Weibull property based on moments and\non the moment generating function is provided and properties of the class are\nstudied. An estimation procedure for the tail parameter is proposed and is\napplied to an example stemming from Bayesian deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 10:49:32 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2020 16:08:28 GMT"}, {"version": "v3", "created": "Thu, 16 Apr 2020 15:41:08 GMT"}, {"version": "v4", "created": "Thu, 3 Dec 2020 08:19:40 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Vladimirova", "Mariia", ""], ["Girard", "Stephane", ""], ["Nguyen", "Hien", ""], ["Arbel", "Julyan", ""]]}, {"id": "1905.05074", "submitter": "Wenqian Wang", "authors": "Wenqian Wang, Beth Andrews", "title": "Partially Specified Space Time Autoregressive Model with Artificial\n  Neural Network", "comments": "arXiv admin note: substantial text overlap with arXiv:1801.07822", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The space time autoregressive model has been widely applied in science, in\nareas such as economics, public finance, political science, agricultural\neconomics, environmental studies and transportation analyses. The classical\nspace time autoregressive model is a linear model for describing spatial\ncorrelation. In this work, we expand the classical model to include related\nexogenous variables, possibly non-Gaussian, high volatility errors, and a\nnonlinear neural network component. The nonlinear neural network component\nallows for more model flexibility, the ability to learn and model nonlinear and\ncomplex relationships. We use a maximum likelihood approach for model parameter\nestimation. We establish consistency and asymptotic normality for these\nestimators under some standard conditions on the space time model and neural\nnetwork component. We investigate the quality of the asymptotic approximations\nfor finite samples by means of numerical simulation studies. For illustration,\nwe include a real world application.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 15:05:08 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Wang", "Wenqian", ""], ["Andrews", "Beth", ""]]}, {"id": "1905.05125", "submitter": "Haoyang Liu", "authors": "Haoyang Liu", "title": "Exact high-dimensional asymptotics for Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Support Vector Machine (SVM) is one of the most widely used\nclassification methods. In this paper, we consider the soft-margin SVM used on\ndata points with independent features, where the sample size $n$ and the\nfeature dimension $p$ grows to $\\infty$ in a fixed ratio $p/n\\rightarrow\n\\delta$. We propose a set of equations that exactly characterizes the\nasymptotic behavior of support vector machine. In particular, we give exact\nformulas for (1) the variability of the optimal coefficients, (2) the\nproportion of data points lying on the margin boundary (i.e. number of support\nvectors), (3) the final objective function value, and (4) the expected\nmisclassification error on new data points, which in particular implies the\nexact formula for the optimal tuning parameter given a data generating\nmechanism. We first establish these formulas in the case where the label\n$y\\in\\{+1,-1\\}$ is independent of the feature $x$. Then the results are\ngeneralized to the case where the label $y\\in\\{+1,-1\\}$ is allowed to have a\ngeneral dependence on the feature $x$ through a linear combination $a_0^Tx$.\nThese formulas for the non-smooth hinge loss are analogous to the recent\nresults in \\citep{sur2018modern} for smooth logistic loss. Our approach is\nbased on heuristic leave-one-out calculations.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 16:25:44 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 21:54:03 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Liu", "Haoyang", ""]]}, {"id": "1905.05141", "submitter": "Carlos Am\\'endola", "authors": "Daniele Agostini, Carlos Am\\'endola, Kristian Ranestad", "title": "Moment Identifiability of Homoscedastic Gaussian Mixtures", "comments": "27 pages, 1 table, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying a mixture of Gaussian distributions\nwith same unknown covariance matrix by their sequence of moments up to certain\norder. Our approach rests on studying the moment varieties obtained by taking\nspecial secants to the Gaussian moment varieties, defined by their natural\npolynomial parametrization in terms of the model parameters. When the order of\nthe moments is at most three, we prove an analogue of the Alexander-Hirschowitz\ntheorem classifying all cases of homoscedastic Gaussian mixtures that produce\ndefective moment varieties. As a consequence, identifiability is determined\nwhen the number of mixed distributions is smaller than the dimension of the\nspace. In the two component setting we provide a closed form solution for\nparameter recovery based on moments up to order four, while in the one\ndimensional case we interpret the rank estimation problem in terms of secant\nvarieties of rational normal curves.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 16:51:34 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 16:51:28 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Agostini", "Daniele", ""], ["Am\u00e9ndola", "Carlos", ""], ["Ranestad", "Kristian", ""]]}, {"id": "1905.05145", "submitter": "Arrigo Coen", "authors": "Arrigo Coen, Luis Guti\\'errez and Rams\\'es H. Mena", "title": "Modeling failures times with dependent renewal type models via\n  exchangeability", "comments": "15 pages and 5 figures", "journal-ref": null, "doi": "10.1080/02331888.2019.1618858", "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Failure times of a machinery cannot always be assumed independent and\nidentically distributed, e.g. if after reparations the machinery is not\nrestored to a same-as-new condition. Framed within the renewal processes\napproach, a generalization that considers exchangeable inter-arrival times is\npresented. The resulting model provides a more realistic approach to capture\nthe dependence among events occurring at random times, while retaining much of\nthe tractability of the classical renewal process. Extensions of some classical\nresults and special cases of renewal functions are analyzed, in particular the\none corresponding to an exchangeable sequence driven by a Dirichlet process.\nThe proposal is tested through an estimation procedure using simulated data\nsets and with an application to the reliability of hydraulic subsystems in\nload-haul-dump machines.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2019 17:01:52 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Coen", "Arrigo", ""], ["Guti\u00e9rrez", "Luis", ""], ["Mena", "Rams\u00e9s H.", ""]]}, {"id": "1905.05340", "submitter": "Bodhisattva Sen", "authors": "Promit Ghosal and Bodhisattva Sen", "title": "Multivariate Ranks and Quantiles using Optimal Transport: Consistency,\n  Rates, and Nonparametric Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study multivariate ranks and quantiles, defined using the\ntheory of optimal transport, and build on the work of Chernozhukov et al.(2017)\nand Hallin et al.(2021). We study the characterization, computation and\nproperties of the multivariate rank and quantile functions and their empirical\ncounterparts. We derive the uniform consistency of these empirical estimates to\ntheir population versions, under certain assumptions. In fact, we prove a\nGlivenko-Cantelli type theorem that shows the asymptotic stability of the\nempirical rank map in any direction. Under mild structural assumptions, we\nprovide global and local rates of convergence of the empirical quantile and\nrank maps. We also provide a sub-Gaussian tail bound for the global L_2-loss of\nthe empirical quantile function. Further, we propose tuning parameter-free\nmultivariate nonparametric tests -- a two-sample test and a test for mutual\nindependence -- based on our notion of multivariate quantiles/ranks. Asymptotic\nconsistency of these tests are shown and the rates of convergence of the\nassociated test statistics are derived, both under the null and alternative\nhypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 01:45:54 GMT"}, {"version": "v2", "created": "Sat, 7 Sep 2019 23:59:58 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 17:27:31 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Ghosal", "Promit", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1905.05643", "submitter": "Cameron Musco", "authors": "Yonina C. Eldar, Jerry Li, Cameron Musco, Christopher Musco", "title": "Sample Efficient Toeplitz Covariance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.DS cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the sample complexity of estimating the covariance matrix $T$ of a\ndistribution $\\mathcal{D}$ over $d$-dimensional vectors, under the assumption\nthat $T$ is Toeplitz. This assumption arises in many signal processing\nproblems, where the covariance between any two measurements only depends on the\ntime or distance between those measurements. We are interested in estimation\nstrategies that may choose to view only a subset of entries in each vector\nsample $x \\sim \\mathcal{D}$, which often equates to reducing hardware and\ncommunication requirements in applications ranging from wireless signal\nprocessing to advanced imaging. Our goal is to minimize both 1) the number of\nvector samples drawn from $\\mathcal{D}$ and 2) the number of entries accessed\nin each sample.\n  We provide some of the first non-asymptotic bounds on these sample complexity\nmeasures that exploit $T$'s Toeplitz structure, and by doing so, significantly\nimprove on results for generic covariance matrices. Our bounds follow from a\nnovel analysis of classical and widely used estimation algorithms (along with\nsome new variants), including methods based on selecting entries from each\nvector sample according to a so-called sparse ruler. In many cases, we pair our\nupper bounds with matching or nearly matching lower bounds.\n  In addition to results that hold for any Toeplitz $T$, we further study the\nimportant setting when $T$ is close to low-rank, which is often the case in\npractice. We show that methods based on sparse rulers perform even better in\nthis setting, with sample complexity scaling sublinearly in $d$. Motivated by\nthis finding, we develop a new covariance estimation strategy that further\nimproves on all existing methods in the low-rank case: when $T$ is rank-$k$ or\nnearly rank-$k$, it achieves sample complexity depending polynomially on $k$\nand only logarithmically on $d$.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 14:34:58 GMT"}, {"version": "v2", "created": "Wed, 15 May 2019 20:37:26 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 15:11:34 GMT"}, {"version": "v4", "created": "Thu, 6 Jun 2019 12:43:44 GMT"}, {"version": "v5", "created": "Wed, 30 Oct 2019 05:47:07 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Eldar", "Yonina C.", ""], ["Li", "Jerry", ""], ["Musco", "Cameron", ""], ["Musco", "Christopher", ""]]}, {"id": "1905.05663", "submitter": "Aurelien Alfonsi", "authors": "Aur\\'elien Alfonsi and Rafa\\\"el Coyaud and Virginie Ehrlacher and\n  Damiano Lombardi", "title": "Approximation of Optimal Transport problems with marginal moments\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.NA math.ST q-fin.CP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal Transport (OT) problems arise in a wide range of applications, from\nphysics to economics. Getting numerical approximate solution of these problems\nis a challenging issue of practical importance. In this work, we investigate\nthe relaxation of the OT problem when the marginal constraints are replaced by\nsome moment constraints. Using Tchakaloff's theorem, we show that the Moment\nConstrained Optimal Transport problem (MCOT) is achieved by a finite discrete\nmeasure. Interestingly, for multimarginal OT problems, the number of points\nweighted by this measure scales linearly with the number of marginal laws,\nwhich is encouraging to bypass the curse of dimension. This approximation\nmethod is also relevant for Martingale OT problems. We show the convergence of\nthe MCOT problem toward the corresponding OT problem. In some fundamental\ncases, we obtain rates of convergence in $O(1/n)$ or $O(1/n^2)$ where $n$ is\nthe number of moments, which illustrates the role of the moment functions.\nLast, we present algorithms exploiting the fact that the MCOT is reached by a\nfinite discrete measure and provide numerical examples of approximations.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 15:17:36 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Alfonsi", "Aur\u00e9lien", ""], ["Coyaud", "Rafa\u00ebl", ""], ["Ehrlacher", "Virginie", ""], ["Lombardi", "Damiano", ""]]}, {"id": "1905.05828", "submitter": "Jan-Christian H\\\"utter", "authors": "Jan-Christian H\\\"utter, Philippe Rigollet", "title": "Minimax estimation of smooth optimal transport maps", "comments": "53 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brenier's theorem is a cornerstone of optimal transport that guarantees the\nexistence of an optimal transport map $T$ between two probability distributions\n$P$ and $Q$ over $\\mathbb{R}^d$ under certain regularity conditions. The main\ngoal of this work is to establish the minimax estimation rates for such a\ntransport map from data sampled from $P$ and $Q$ under additional smoothness\nassumptions on $T$. To achieve this goal, we develop an estimator based on the\nminimization of an empirical version of the semi-dual optimal transport\nproblem, restricted to truncated wavelet expansions. This estimator is shown to\nachieve near minimax optimality using new stability arguments for the semi-dual\nand a complementary minimax lower bound. Furthermore, we provide numerical\nexperiments on synthetic data supporting our theoretical findings and\nhighlighting the practical benefits of smoothness regularization. These are the\nfirst minimax estimation rates for transport maps in general dimension.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2019 20:24:31 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 18:36:00 GMT"}, {"version": "v3", "created": "Mon, 29 Jun 2020 23:24:09 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["H\u00fctter", "Jan-Christian", ""], ["Rigollet", "Philippe", ""]]}, {"id": "1905.05945", "submitter": "Luai Al-Labadi Dr.", "authors": "Luai Al-Labadi, Ce Wang", "title": "Measuring Bayesian Robustness Using R\\'enyi Divergence", "comments": "28", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with measuring the Bayesian robustness of classes of\ncontaminated priors. Two different classes of priors in the neighborhood of the\nelicited prior are considered. The first one is the well-known\n$\\epsilon$-contaminated class, while the second one is the geometric mixing\nclass. The proposed measure of robustness is based on computing the curvature\nof R\\'enyi divergence between posterior distributions. Examples are used to\nillustrate the results by using simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 04:51:00 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 15:36:58 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Al-Labadi", "Luai", ""], ["Wang", "Ce", ""]]}, {"id": "1905.05976", "submitter": "Takeru Matsuda", "authors": "Takeru Matsuda, Masatoshi Uehara, Aapo Hyvarinen", "title": "Information criteria for non-normalized models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical models are given in the form of non-normalized densities\nwith an intractable normalization constant. Since maximum likelihood estimation\nis computationally intensive for these models, several estimation methods have\nbeen developed which do not require explicit computation of the normalization\nconstant, such as noise contrastive estimation (NCE) and score matching.\nHowever, model selection methods for general non-normalized models have not\nbeen proposed so far. In this study, we develop information criteria for\nnon-normalized models estimated by NCE or score matching. They are\napproximately unbiased estimators of discrepancy measures for non-normalized\nmodels. Simulation results and applications to real data demonstrate that the\nproposed criteria enable selection of the appropriate non-normalized model in a\ndata-driven manner.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 06:57:47 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2020 08:33:30 GMT"}, {"version": "v3", "created": "Thu, 3 Dec 2020 09:08:17 GMT"}, {"version": "v4", "created": "Fri, 18 Jun 2021 07:31:56 GMT"}, {"version": "v5", "created": "Tue, 27 Jul 2021 06:00:47 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Matsuda", "Takeru", ""], ["Uehara", "Masatoshi", ""], ["Hyvarinen", "Aapo", ""]]}, {"id": "1905.06097", "submitter": "Zhou Fan", "authors": "Sheng Xu and Zhou Fan", "title": "Iterative Alpha Expansion for estimating gradient-sparse signals from\n  linear measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating a piecewise-constant image, or a gradient-sparse\nsignal on a general graph, from noisy linear measurements. We propose and study\nan iterative algorithm to minimize a penalized least-squares objective, with a\npenalty given by the \"l_0-norm\" of the signal's discrete graph gradient. The\nmethod proceeds by approximate proximal descent, applying the alpha-expansion\nprocedure to minimize a proximal gradient in each iteration, and using a\ngeometric decay of the penalty parameter across iterations. Under a\ncut-restricted isometry property for the measurement design, we prove global\nrecovery guarantees for the estimated signal. For standard Gaussian designs,\nthe required number of measurements is independent of the graph structure, and\nimproves upon worst-case guarantees for total-variation (TV) compressed sensing\non the 1-D and 2-D lattice graphs by polynomial and logarithmic factors,\nrespectively. The method empirically yields lower mean-squared recovery error\ncompared with TV regularization in regimes of moderate undersampling and\nmoderate to high signal-to-noise, for several examples of changepoint signals\nand gradient-sparse phantom images.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 11:40:35 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Xu", "Sheng", ""], ["Fan", "Zhou", ""]]}, {"id": "1905.06201", "submitter": "Alexander D\\\"urre", "authors": "Alexander D\\\"urre, Roland Fried", "title": "Robust change point tests by bounded transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical moment based change point tests like the cusum test are very\npowerful in case of Gaussian time series with one change point but behave\npoorly under heavy tailed distributions and corrupted data. A new class of\nrobust change point tests based on cusum statistics of robustly transformed\nobservations is proposed. This framework is quite flexible, depending on the\nused transformation one can detect for instance changes in the mean, scale or\ndependence of a possibly multivariate time series. Simulations indicate that\nthis approach is very powerful in detecting changes in the marginal variance of\nARCH processes and outperforms existing proposals for detecting structural\nbreaks in the dependence structure of heavy tailed multivariate time series.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 14:14:12 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["D\u00fcrre", "Alexander", ""], ["Fried", "Roland", ""]]}, {"id": "1905.06208", "submitter": "Philip Thomas", "authors": "Erik Learned-Miller and Philip S. Thomas", "title": "A New Confidence Interval for the Mean of a Bounded Random Variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for constructing a confidence interval for the mean\nof a bounded random variable from samples of the random variable. We conjecture\nthat the confidence interval has guaranteed coverage, i.e., that it contains\nthe mean with high probability for all distributions on a bounded interval, for\nall samples sizes, and for all confidence levels. This new method provides\nconfidence intervals that are competitive with those produced using Student's\nt-statistic, but does not rely on normality assumptions. In particular, its\nonly requirement is that the distribution be bounded on a known finite\ninterval.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 14:25:55 GMT"}, {"version": "v2", "created": "Wed, 4 Nov 2020 17:55:44 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Learned-Miller", "Erik", ""], ["Thomas", "Philip S.", ""]]}, {"id": "1905.06224", "submitter": "Zikun Yang", "authors": "Zikun Yang, Andrew Womack", "title": "Revisiting High Dimensional Bayesian Model Selection for Gaussian\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection for regression problems with an increasing number of\ncovariates continues to be an important problem both theoretically and in\napplications. Model selection consistency and mean structure reconstruction\ndepend on the interplay between the Bayes factor learning rate and the\npenalization on model complexity. In this work, we present results for the\nZellner-Siow prior for regression coefficients paired with a Poisson prior for\nmodel complexity. We show that model selection consistency restricts the\ndimension of the true model from increasing too quickly. Further, we show that\nthe additional contribution to the mean structure from new covariates must be\nlarge enough to overcome the complexity penalty. The average Bayes factors for\ndifferent sets of models involves random variables over the choices of columns\nfrom the design matrix. We show that a large class these random variables have\nno moments asymptotically and need to be analyzed using stable laws. We derive\nthe domain of attraction for these random variables and obtain conditions on\nthe design matrix that provide for the control of false discoveries.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 14:57:49 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Yang", "Zikun", ""], ["Womack", "Andrew", ""]]}, {"id": "1905.06318", "submitter": "Martin Tveten", "authors": "Martin Tveten", "title": "Which principal components are most sensitive to distributional changes?", "comments": "11 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.OT stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  PCA is often used in anomaly detection and statistical process control tasks.\nFor bivariate data, we prove that the minor projection (the least varying\nprojection) of the PCA-rotated data is the most sensitive to distributional\nchanges, where sensitivity is defined by the Hellinger distance between\ndistributions before and after a change. In particular, this is almost always\nthe case if only one parameter of the bivariate normal distribution changes,\ni.e., the change is sparse. Simulations indicate that the minor projections are\nthe most sensitive for a large range of changes and pre-change settings in\nhigher dimensions as well. This motivates using the minor projections for\ndetecting sparse distributional changes in high-dimensional data.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 17:50:22 GMT"}], "update_date": "2019-05-16", "authors_parsed": [["Tveten", "Martin", ""]]}, {"id": "1905.06406", "submitter": "Joshua N. Cooper", "authors": "Joshua N. Cooper and Christopher D. Edgar", "title": "A Development of Continuous-Time Transfer Entropy", "comments": "37 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer entropy (TE) was introduced by Schreiber in 2000 as a measurement of\nthe predictive capacity of one stochastic process with respect to another.\nOriginally stated for discrete time processes, we expand the theory in line\nwith recent work of Spinney, Prokopenko, and Lizier to define TE for stochastic\nprocesses indexed over a compact interval taking values in a Polish state\nspace. We provide a definition for continuous time TE using the Radon-Nikodym\nTheorem, random measures, and projective limits of probability spaces. As our\nmain result, we provide necessary and sufficient conditions to obtain this\ndefinition as a limit of discrete time TE, as well as illustrate its\napplication via an example involving Poisson point processes. As a derivative\nof continuous time TE, we also define the transfer entropy rate between two\nprocesses and show that (under mild assumptions) their stationarity implies a\nconstant rate. We also investigate TE between homogeneous Markov jump processes\nand discuss some open problems and possible future directions.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 19:26:19 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 02:10:29 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Cooper", "Joshua N.", ""], ["Edgar", "Christopher D.", ""]]}, {"id": "1905.06411", "submitter": "Arrigo Coen", "authors": "Arrigo Coen and Beatriz God\\'inez-Chaparro", "title": "Compound Dirichlet Processes", "comments": "11 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The compound Poisson process and the Dirichlet process are the pillar\nstructures of Renewal theory and Bayesian nonparametric theory, respectively.\nBoth processes have many useful extensions to fulfill the practitioners needs\nto model the particularities of data structures. Accordingly, in this\ncontribution, we joined their primal ideas to construct the compound Dirichlet\nprocess and the compound Dirichlet process mixture. As a consequence, these new\nprocesses had a fruitful structure to model the time occurrence among events,\nwith also a flexible structure on the arrival variables. These models have a\ndirect Bayesian interpretation of their posterior estimators and are easy to\nimplement. We obtain expressions of the posterior distribution, nonconditional\ndistribution and expected values. In particular to find these formulas we\nanalyze sums of random variables with Dirichlet process priors. We assessed our\napproach by applying our model on a real data example of a contagious zoonotic\ndisease.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 19:54:42 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Coen", "Arrigo", ""], ["God\u00ednez-Chaparro", "Beatriz", ""]]}, {"id": "1905.06467", "submitter": "Claus Ekstr{\\o}m", "authors": "Claus Thorn Ekstr{\\o}m and Christian Bressen Pipper (Section of\n  Biostatistics, Department of Public Health, University of Copenhagen)", "title": "Moment-based Estimation of Mixtures of Regression Models", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixtures of regression models provide a flexible modeling framework\nfor many phenomena. Using moment-based estimation of the regression parameters,\nwe develop unbiased estimators with a minimum of assumptions on the mixture\ncomponents. In particular, only the average regression model for one of the\ncomponents in the mixture model is needed and no requirements on the\ndistributions. The consistency and asymptotic distribution of the estimators is\nderived and the proposed method is validated through a series of simulation\nstudies and is shown to be highly accurate. We illustrate the use of the\nmoment-based mixture of regression models with an application to wine quality\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 15 May 2019 23:11:48 GMT"}], "update_date": "2019-05-17", "authors_parsed": [["Ekstr\u00f8m", "Claus Thorn", "", "Section of\n  Biostatistics, Department of Public Health, University of Copenhagen"], ["Pipper", "Christian Bressen", "", "Section of\n  Biostatistics, Department of Public Health, University of Copenhagen"]]}, {"id": "1905.06584", "submitter": "Eric Gautier", "authors": "Christophe Gaillac (TSE, CREST), Eric Gautier (TSE, UT1)", "title": "Adaptive estimation in the linear random coefficients model when\n  regressors have limited variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a linear model where the coefficients - intercept and slopes -\nare random with a law in a nonparametric class and independent from the\nregressors. Identification often requires the regressors to have a support\nwhich is the whole space. This is hardly ever the case in practice.\nAlternatively, the coefficients can have a compact support but this is not\ncompatible with unbounded error terms as usual in regression models. In this\npaper, the regressors can have a support which is a proper subset but the\nslopes (not the intercept) do not have heavy-tails. Lower bounds on the\nsupremum risk for the estimation of the joint density of the random\ncoefficients density are obtained for a wide range of smoothness, where some\nallow for polynomial and nearly parametric rates of convergence. We present a\nminimax optimal estimator, a data-driven rule for adaptive estimation, and made\navailable a R package.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 07:54:27 GMT"}, {"version": "v2", "created": "Fri, 26 Jul 2019 07:23:49 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 07:22:38 GMT"}, {"version": "v4", "created": "Fri, 19 Jun 2020 08:23:03 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Gaillac", "Christophe", "", "TSE, CREST"], ["Gautier", "Eric", "", "TSE, UT1"]]}, {"id": "1905.06661", "submitter": "Y. X. Rachel Wang", "authors": "Purnamrita Sarkar, Y. X. Rachel Wang, Soumendu Sundar Mukherjee", "title": "When random initializations help: a study of variational inference for\n  community detection", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational approximation has been widely used in large-scale Bayesian\ninference recently, the simplest kind of which involves imposing a mean field\nassumption to approximate complicated latent structures. Despite the\ncomputational scalability of mean field, theoretical studies of its loss\nfunction surface and the convergence behavior of iterative updates for\noptimizing the loss are far from complete. In this paper, we focus on the\nproblem of community detection for a simple two-class Stochastic Blockmodel\n(SBM) with equal class sizes. Using batch co-ordinate ascent (BCAVI) for\nupdates, we show different convergence behavior with respect to different\ninitializations. When the parameters are known or estimated within a reasonable\nrange and held fixed, we characterize conditions under which an initialization\ncan converge to the ground truth. On the other hand, when the parameters need\nto be estimated iteratively, a random initialization will converge to an\nuninformative local optimum.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 11:18:17 GMT"}, {"version": "v2", "created": "Sat, 18 May 2019 06:23:03 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Sarkar", "Purnamrita", ""], ["Wang", "Y. X. Rachel", ""], ["Mukherjee", "Soumendu Sundar", ""]]}, {"id": "1905.06829", "submitter": "Yaakov Malinovsky", "authors": "Emilio De Santis, Yaakov Malinovsky, Fabio Spizzichino", "title": "Stochastic precedence and minima among dependent variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of stochastic precedence between two random variables emerges as a\nrelevant concept in several fields of applied probability. When one consider a\nvector of random variables $X_1,...,X_n$, this notion has a preeminent role in\nthe analysis of minima of the type $\\min_{j \\in A} X_j$ for $A \\subset \\{1,\n\\ldots n\\}$. In such an analysis, however, several apparently controversial\naspects can arise (among which phenomena of \"non-transitivity\").\n  Here we concentrate attention on vectors of non-negative random variables\nwith absolutely continuous joint distributions, in which a case the set of the\nmultivariate conditional hazard rate (m.c.h.r.) functions can be employed as a\nconvenient method to describe different aspects of stochastic dependence.\n  In terms of the m.c.h.r. functions, we first obtain convenient formulas for\nthe probability distributions of the variables $\\min_{j \\in A} X_j$ and for the\nprobability of events $\\{X_i=\\min_{j \\in A} X_j\\}$. Then we detail several\naspects of the notion of stochastic precedence. On these bases, we explain some\ncontroversial behavior of such variables and give sufficient conditions under\nwhich paradoxical aspects can be excluded. On the purpose of stimulating active\ninterest of readers, we present several comments and pertinent examples.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 15:12:01 GMT"}, {"version": "v2", "created": "Wed, 23 Oct 2019 13:35:50 GMT"}, {"version": "v3", "created": "Sat, 11 Jan 2020 14:49:00 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["De Santis", "Emilio", ""], ["Malinovsky", "Yaakov", ""], ["Spizzichino", "Fabio", ""]]}, {"id": "1905.06858", "submitter": "Renata Talska", "authors": "Jitka Machalova, Renata Talska, Karel Hron, Ales Gaba", "title": "Compositional splines for representation of density functions", "comments": "28 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of functional data analysis, probability density functions as\nnon-negative functions are characterized by specific properties of scale\ninvariance and relative scale which enable to represent them with the unit\nintegral constraint without loss of information. On the other hand, all these\nproperties are a challenge when the densities need to be approximated with\nspline functions, including construction of the respective spline basis. The\nBayes space methodology of density functions enables to express them as real\nfunctions in the standard $L^2$ space using the centered log-ratio\ntransformation. The resulting functions satisfy the zero integral constraint.\nThis is a key to propose a new spline basis, holding the same property, and\nconsequently to build a new class of spline functions, called compositional\nsplines, which can approximate probability density functions in a consistent\nway. The paper provides also construction of smoothing compositional splines\nand possible orthonormalization of the spline basis which might be useful in\nsome applications. Finally, statistical processing of densities using the new\napproximation tool is demonstrated in case of simplicial functional principal\ncomponent analysis with anthropometric data.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 15:41:47 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 10:05:26 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Machalova", "Jitka", ""], ["Talska", "Renata", ""], ["Hron", "Karel", ""], ["Gaba", "Ales", ""]]}, {"id": "1905.06977", "submitter": "Fallaw Sowell", "authors": "Benjamin Holcblat and Fallaw Sowell", "title": "The Empirical Saddlepoint Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We define a moment-based estimator that maximizes the empirical saddlepoint\n(ESP) approximation of the distribution of solutions to empirical moment\nconditions. We call it the ESP estimator. We prove its existence, consistency\nand asymptotic normality, and we propose novel test statistics. We also show\nthat the ESP estimator corresponds to the MM (method of moments) estimator\nshrunk toward parameter values with lower estimated variance, so it reduces the\ndocumented instability of existing moment-based estimators. In the case of\njust-identified moment conditions, which is the case we focus on, the ESP\nestimator is different from the MM estimator, unlike the recently proposed\nalternatives, such as the empirical-likelihood-type estimators.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 18:04:52 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Holcblat", "Benjamin", ""], ["Sowell", "Fallaw", ""]]}, {"id": "1905.07050", "submitter": "Elizabeth Allman", "authors": "Elizabeth Allman and Hector Banos and John Rhodes", "title": "NANUQ: A method for inferring species networks from gene trees under the\n  coalescent model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Species networks generalize the notion of species trees to allow for\nhybridization or other lateral gene transfer. Under the Network Multispecies\nCoalescent Model, individual gene trees arising from a network can have any\ntopology, but arise with frequencies dependent on the network structure and\nnumerical parameters. We propose a new algorithm for statistical inference of a\nlevel-1 species network under this model, from data consisting of gene tree\ntopologies, and provide the theoretical justification for it. The algorithm is\nbased in an analysis of quartets displayed on gene trees, combining several\nstatistical hypothesis tests with combinatorial ideas such as a quartet-based\nintertaxon distance appropriate to networks, the NeighborNet algorithm for\ncircular split systems, and the Circular Network algorithm for constructing a\nsplits graph.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 22:11:52 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Allman", "Elizabeth", ""], ["Banos", "Hector", ""], ["Rhodes", "John", ""]]}, {"id": "1905.07067", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin, Jane-Ling Wang and Qixian Zhong", "title": "Basis Expansions for Functional Snippets", "comments": "51 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of mean and covariance functions is fundamental for functional\ndata analysis. While this topic has been studied extensively in the literature,\na key assumption is that there are enough data in the domain of interest to\nestimate both the mean and covariance functions. In this paper, we investigate\nmean and covariance estimation for functional snippets in which observations\nfrom a subject are available only in an interval of length strictly (and often\nmuch) shorter than the length of the whole interval of interest. For such a\nsampling plan, no data is available for direct estimation of the off-diagonal\nregion of the covariance function. We tackle this challenge via a basis\nrepresentation of the covariance function. The proposed approach allows one to\nconsistently estimate an infinite-rank covariance function from functional\nsnippets. We establish the convergence rates for the proposed estimators and\nillustrate their finite-sample performance via simulation studies and two data\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 16 May 2019 23:49:04 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 22:54:39 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 03:19:17 GMT"}, {"version": "v4", "created": "Sat, 29 Aug 2020 12:48:18 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lin", "Zhenhua", ""], ["Wang", "Jane-Ling", ""], ["Zhong", "Qixian", ""]]}, {"id": "1905.07157", "submitter": "Veronique Maume-Deschamps", "authors": "Weihong Ni, Corina Constantinescu, Alfredo Eg\\'idio dos Reis,\n  V\\'eronique Maume-Deschamps (ICJ, PSPM)", "title": "Estimation of foreseeable and unforeseeable risks in motor insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project works with the risk model developed by Li et al. (2015) and\nquests modelling, estimating and pricing insurance for risks brought in by\ninnovative technologies, or other emerging or latent risks. The model considers\ntwo different risk streams that arise together, however not clearly separated\nor observed. Specifically, we consider a risk surplus process where premia are\nadjusted according to past claim frequencies, like in a Bonus-Malus (BM)\nsystem, when we consider a classical or historical risk stream and an\nunforeseeable risk one. These are unknown risks which can be of high\nuncertainty that, when pricing insurance (ratemaking and experience rating),\nsuggest a sensitive premium adjustment strategy. It is not clear for the\nactuary to observe which claim comes from one or the other stream. When\nmodelling such risks it is crucial to estimate the behaviour of such claims,\noccurrence and their severity. Premium calculation must fairly reflect the\nnature of these two kinds of risk streams. We start proposing a model,\nseparating claim counts and severities, then propose a premium calculation\nmethod, and finally a parameter estimation procedure. In the modelling we\nassume a Bayesian approach as used in credibility theory, a credibility\napproach for premium calculation and the use of the Expectation-Maximization\n(EM) algorithm in the estimation procedure.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 08:04:32 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Ni", "Weihong", "", "ICJ, PSPM"], ["Constantinescu", "Corina", "", "ICJ, PSPM"], ["Reis", "Alfredo Eg\u00eddio dos", "", "ICJ, PSPM"], ["Maume-Deschamps", "V\u00e9ronique", "", "ICJ, PSPM"]]}, {"id": "1905.07342", "submitter": "Christophe Giraud", "authors": "Christophe Giraud and Yann Issartel and Luc Leh\\'ericy and Matthieu\n  Lerasle", "title": "Pair-Matching: Links Prediction with Adaptive Queries", "comments": "58 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pair-matching problem appears in many applications where one wants to\ndiscover good matches between pairs of entities or individuals. Formally, the\nset of individuals is represented by the nodes of a graph where the edges,\nunobserved at first, represent the good matches. The algorithm queries pairs of\nnodes and observes the presence/absence of edges. Its goal is to discover as\nmany edges as possible with a fixed budget of queries. Pair-matching is a\nparticular instance of multi-armed bandit problem in which the arms are pairs\nof individuals and the rewards are edges linking these pairs. This bandit\nproblem is non-standard though, as each arm can only be played once.\n  Given this last constraint, sublinear regret can be expected only if the\ngraph presents some underlying structure. This paper shows that sublinear\nregret is achievable in the case where the graph is generated according to a\nStochastic Block Model (SBM) with two communities. Optimal regret bounds are\ncomputed for this pair-matching problem. They exhibit a phase transition\nrelated to the Kesten-Stigum threshold for community detection in SBM. The\npair-matching problem is considered in the case where each node is constrained\nto be sampled less than a given amount of times. We show how optimal regret\nrates depend on this constraint. The paper is concluded by a conjecture\nregarding the optimal regret when the number of communities is larger than 2.\nContrary to the two communities case, we argue that a statistical-computational\ngap would appear in this problem.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 15:57:37 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 18:07:56 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Giraud", "Christophe", ""], ["Issartel", "Yann", ""], ["Leh\u00e9ricy", "Luc", ""], ["Lerasle", "Matthieu", ""]]}, {"id": "1905.07396", "submitter": "Carlos Am\\'endola", "authors": "Carlos Am\\'endola, Dimitra Kosta, Kaie Kubjas", "title": "Maximum Likelihood Estimation of Toric Fano Varieties", "comments": "28 pages, 4 figures, 4 tables, this article supersedes\n  arXiv:1602.08307", "journal-ref": "Alg. Stat. 11 (2020) 5-30", "doi": "10.2140/astat.2020.11.5", "report-no": null, "categories": "math.ST math.AG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum likelihood estimation problem for several classes of\ntoric Fano models. We start by exploring the maximum likelihood degree for all\n$2$-dimensional Gorenstein toric Fano varieties. We show that the ML degree is\nequal to the degree of the surface in every case except for the quintic del\nPezzo surface with two ordinary double points and provide explicit expressions\nthat allow one to compute the maximum likelihood estimate in closed form\nwhenever the ML degree is less than 5. We then explore the reasons for the ML\ndegree drop using $A$-discriminants and intersection theory. Finally, we show\nthat toric Fano varieties associated to 3-valent phylogenetic trees have ML\ndegree one and provide a formula for the maximum likelihood estimate. We prove\nit as a corollary to a more general result about the multiplicativity of ML\ndegrees of codimension zero toric fiber products, and it also follows from a\nconnection to a recent result about staged trees.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2019 17:49:50 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 16:12:10 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Kosta", "Dimitra", ""], ["Kubjas", "Kaie", ""]]}, {"id": "1905.07530", "submitter": "Dan Yang", "authors": "Rong Chen, Dan Yang and Cun-hui Zhang", "title": "Factor Models for High-Dimensional Tensor Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large tensor (multi-dimensional array) data are now routinely collected in a\nwide range of applications, due to modern data collection capabilities. Often\nsuch observations are taken over time, forming tensor time series. In this\npaper we present a factor model approach for analyzing high-dimensional dynamic\ntensor time series and multi-category dynamic transport networks. Two\nestimation procedures along with their theoretical properties and simulation\nresults are presented. Two applications are used to illustrate the model and\nits interpretations.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 03:40:48 GMT"}, {"version": "v2", "created": "Tue, 19 May 2020 02:00:31 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Chen", "Rong", ""], ["Yang", "Dan", ""], ["Zhang", "Cun-hui", ""]]}, {"id": "1905.07568", "submitter": "Rajesh Sharma", "authors": "R. Sharma, A. Sharma and R. Saini", "title": "A note on variance bounds and location of eigenvalues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss some extensions and refinements of the variance bounds for both\nreal and complex numbers. The related bounds for the eigenvalues and spread of\na matrix are also derived here.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 09:54:40 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Sharma", "R.", ""], ["Sharma", "A.", ""], ["Saini", "R.", ""]]}, {"id": "1905.07635", "submitter": "J\\\"urgen Franke", "authors": "J\\\"urgen Franke, Euna Gesare Nyarige", "title": "A residual-based bootstrap for functional autoregressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the residual-based or naive bootstrap for functional\nautoregressions of order 1 and prove that it is asymptotically valid for, e.g.,\nthe sample mean and for empirical covariance operator estimates. As a crucial\nauxiliary result, we also show that the empirical distribution of the centered\nsample innovations converges to the distribution of the innovations with\nrespect to the Mallows metric.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 19:57:50 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Franke", "J\u00fcrgen", ""], ["Nyarige", "Euna Gesare", ""]]}, {"id": "1905.07649", "submitter": "Franz Baumdicker", "authors": "Franz Baumdicker and Ulrich H\\\"olker", "title": "Method comparison with repeated measurements -- Passing-Bablok\n  regression for grouped data with errors in both variables", "comments": "16 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Passing-Bablok and Theil-Sen regression are closely related\nnon-parametric methods to estimate the regression coefficients and build tests\non the relationship between the dependent and independent variables. Both\nmethods rely on the slopes of the connecting lines between pairwise\nmeasurements. While Theil and Sen assume no measurement errors in the\nindependent variable, the method from Passing and Bablok accounts for errors in\nboth variables. Here we consider the case where multiple, e.g. repeated,\nmeasurements with errors in both variables are available for m samples. We show\nthat in this case the slopes between repeated measurements need to be excluded\nto obtain an unbiased estimate. We prove that the resulting\nBlock-Passing-Bablok estimate for grouped data is asymptotically normally\ndistributed. If measurements of the independent variable are without error the\nvariance of the estimate equals the variance of the Theil-Sen method with tied\nranks. If both variables are measured with imprecision the result depends on\nthe fraction of measurements between groups that fall within the range of each\nother. Only if no overlap between measurements of different groups occurs the\nvariance equals again the tied ranks version. Otherwise, the variance is\nsmaller. We explicitly compute this variance and provide a method comparison\ntest for data with repeated measurements based on the method from Passing and\nBablok for independent measurements. If repeated measurements are considered\nthis test has a higher power to detect the true relationship between two\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 21:54:39 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2020 11:12:21 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Baumdicker", "Franz", ""], ["H\u00f6lker", "Ulrich", ""]]}, {"id": "1905.07652", "submitter": "Samuel Justice", "authors": "Sam Justice and N. D. Shyamalkumar", "title": "On a Tail Bound for Root-Finding in Randomly Growing Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We re-examine a lower-tail upper bound for the random variable\n$$X=\\prod_{i=1}^{\\infty}\\min\\left\\{\\sum_{k=1}^iE_k,1\\right\\},$$ where\n$E_1,E_2,\\ldots\\stackrel{iid}\\sim\\text{Exp}(1)$. This bound has found use in\nroot-finding and seed-finding algorithms for randomly growing trees, and was\ninitially proved as a lemma in the context of the uniform attachment tree\nmodel. We first show that $X$ has a useful representation as a compound product\nof uniform random variables that allows us to determine its moments and refine\nthe existing nonasymptotic bound. Next we demonstrate that the lower-tail\nprobability for $X$ can equivalently be written as a probability involving two\nindependent Poisson random variables, an equivalence that yields a novel\ngeneral result regarding indpendent Poissons and that also enables us to obtain\ntight asymptotic bounds on the tail probability of interest.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2019 22:10:14 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Justice", "Sam", ""], ["Shyamalkumar", "N. D.", ""]]}, {"id": "1905.07670", "submitter": "Rafael Stern", "authors": "Luis G. Esteves, Rafael Izbicki and Rafael B. Stern", "title": "Teaching decision theory proof strategies using a crowdsourcing problem", "comments": "21 pages, 2 figures. This is an Accepted Manuscript of an article\n  published by Taylor & Francis Group in The American Statistician, available\n  online: https://amstat.tandfonline.com/doi/abs/10.1080/00031305.2016.1264316", "journal-ref": "The American Statistician, 71(4), 336-343 (2017)", "doi": "10.1080/00031305.2016.1264316", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching how to derive minimax decision rules can be challenging because of\nthe lack of examples that are simple enough to be used in the classroom.\nMotivated by this challenge, we provide a new example that illustrates the use\nof standard techniques in the derivation of optimal decision rules under the\nBayes and minimax approaches. We discuss how to predict the value of an unknown\nquantity, $\\theta \\! \\in \\! \\{0,1\\}$, given the opinions of $n$ experts. An\nimportant example of such crowdsourcing problem occurs in modern cosmology,\nwhere $\\theta$ indicates whether a given galaxy is merging or not, and $Y_1,\n\\ldots, Y_n$ are the opinions from $n$ astronomers regarding $\\theta$. We use\nthe obtained prediction rules to discuss advantages and disadvantages of the\nBayes and minimax approaches to decision theory. The material presented here is\nintended to be taught to first-year graduate students.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 01:40:52 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Esteves", "Luis G.", ""], ["Izbicki", "Rafael", ""], ["Stern", "Rafael B.", ""]]}, {"id": "1905.07765", "submitter": "Vladimir Ulyanov", "authors": "Gerd Christoph, Vladimir V. Ulyanov, Vladimir E. Bening", "title": "Second Order Expansions for Sample Median with Random Sample Size", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, we often encounter situations where a sample size is not defined\nin advance and can be a random value. The randomness of the sample size\ncrucially changes the asymptotic properties of the underlying statistic. In the\npresent paper second order Chebyshev--Edgeworth and Cornish--Fisher expansions\nbased of Student's $t$- and Laplace distributions and their quantiles are\nderived for sample median with random sample size of a special kind.\n", "versions": [{"version": "v1", "created": "Sun, 19 May 2019 16:16:15 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 16:35:40 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Christoph", "Gerd", ""], ["Ulyanov", "Vladimir V.", ""], ["Bening", "Vladimir E.", ""]]}, {"id": "1905.07865", "submitter": "Anil Damle", "authors": "Anil Damle and Yuekai Sun", "title": "Uniform bounds for invariant subspace perturbations", "comments": "29 pages, 3 figures; added new theorem for random E; corrected typos\n  and improved clarity; mild revisions to the way the main results are stated,\n  but no significant changes to the results themselves", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a fixed symmetric matrix A and symmetric perturbation E we develop purely\ndeterministic bounds on how invariant subspaces of A and A+E can differ when\nmeasured by a suitable \"row-wise\" metric rather than via traditional measures\nof subspace distance. Understanding perturbations of invariant subspaces with\nrespect to such metrics is becoming increasingly important across a wide\nvariety of applications and therefore necessitates new theoretical\ndevelopments. Under minimal assumptions we develop new bounds on subspace\nperturbations under the two-to-infinity matrix norm and show in what settings\nthese row-wise differences in the invariant subspaces can be significantly\nsmaller than the analogous two or Frobenius norm differences. We also\ndemonstrate that the constitutive pieces of our bounds are necessary absent\nadditional assumptions and, therefore, our results provide a natural starting\npoint for further analysis of specific problems. Lastly, we briefly discuss\nextensions of our bounds to scenarios where A and/or E are non-normal matrices.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 04:11:58 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 15:26:50 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Damle", "Anil", ""], ["Sun", "Yuekai", ""]]}, {"id": "1905.07881", "submitter": "Nikolai Dokuchaev", "authors": "Nikolai Dokuchaev", "title": "On approximation of the distribution for Pearson statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers the classical Goodness of Fit test.\n  It suggests to use the Gamma distribution for the approximation of the\ndistribution of the Pearson statistics with unknown parameters estimated from\nraw data. The parameters of these Gamma distribution can be estimated from the\nfirst two moments of the statistic after averaging over a distribution of the\nunknown parameter over its range. This allows to simplify calculation of the\nquantiles for the Pearson statistic, as is shown in some simulation experiments\nwith medium and small sample sizes.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 05:32:55 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Dokuchaev", "Nikolai", ""]]}, {"id": "1905.08011", "submitter": "Lixing Zhu", "authors": "Falong Tan and Lixing Zhu", "title": "Integrated conditional moment test and beyond: when the number of\n  covariates is divergent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic integrated conditional moment test is a promising method for\ntesting regression model misspecification. However, it severely suffers from\nthe curse of dimensionality. To extend it to handle the testing problem for\nparametric multi-index models with diverging number of covariates, we\ninvestigate three issues in inference in this paper. First, we study the\nconsistency and asymptotically linear representation of the least squares\nestimator of the parameter matrix at faster rates of divergence than those in\nthe literature for nonlinear models. Second, we propose, via sufficient\ndimension reduction techniques, an adaptive-to-model version of the integrated\nconditional moment test. We study the asymptotic properties of the new test\nunder both the null and alternative hypothesis to examine its ability of\nsignificance level maintenance and its sensitivity to the global and local\nalternatives that are distinct from the null at the fastest possible rate in\nhypothesis testing. Third, we derive the consistency of the bootstrap\napproximation for the new test in the diverging dimension setting. The\nnumerical studies show that the new test can very much enhance the performance\nof the original ICM test in high-dimensional scenarios. We also apply the test\nto a real data set for illustrations.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 11:56:33 GMT"}, {"version": "v2", "created": "Sun, 24 May 2020 01:17:00 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Tan", "Falong", ""], ["Zhu", "Lixing", ""]]}, {"id": "1905.08302", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya and Cl\\'ement L. Canonne and Himanshu Tyagi", "title": "Inference under Information Constraints II: Communication Constraints\n  and Shared Randomness", "comments": "To appear in IEEE Transactions on Information Theory. An abridged\n  version of this work appeared in the 2019 International Conference on Machine\n  Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A central server needs to perform statistical inference based on samples that\nare distributed over multiple users who can each send a message of limited\nlength to the center. We study problems of distribution learning and identity\ntesting in this distributed inference setting and examine the role of shared\nrandomness as a resource. We propose a general-purpose simulate-and-infer\nstrategy that uses only private-coin communication protocols and is\nsample-optimal for distribution learning. This general strategy turns out to be\nsample-optimal even for distribution testing among private-coin protocols.\nInterestingly, we propose a public-coin protocol that outperforms\nsimulate-and-infer for distribution testing and is, in fact, sample-optimal.\nUnderlying our public-coin protocol is a random hash that when applied to the\nsamples minimally contracts the chi-squared distance of their distribution to\nthe uniform distribution.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 19:18:47 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 04:56:00 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "1905.08308", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca, Matus Maciak, Francois Wahl", "title": "Detection of similar successive groups in a model with diverging number\n  of variable groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a linear model with grouped explanatory variables is\nconsidered. The idea is to perform an automatic detection of different\nsuccessive groups of the unknown coefficients under the assumption that the\nnumber of groups is of the same order as the sample size. The standard least\nsquares loss function and the quantile loss function are both used together\nwith the fused and adaptive fused penalty to simultaneously estimate and group\nthe unknown parameters. The proper convergence rate is given for the obtained\nestimators and the upper bound for the number of different successive group is\nderived. A simulation study is used to compare the empirical performance of the\nproposed fused and adaptive fused estimators and a real application on the air\nquality data demonstrates the practical applicability of the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2019 19:25:18 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Ciuperca", "Gabriela", ""], ["Maciak", "Matus", ""], ["Wahl", "Francois", ""]]}, {"id": "1905.08446", "submitter": "Runmin Wang", "authors": "Runmin Wang, Stanislav Volgushev, Xiaofeng Shao", "title": "Inference for Change Points in High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers change point testing and estimation for high\ndimensional data. In the case of testing for a mean shift, we propose a new\ntest which is based on U-statistics and utilizes the self-normalization\nprinciple. Our test targets dense alternatives in the high dimensional setting\nand involves no tuning parameters. The weak convergence of a sequential\nU-statistic based process is shown as an important theoretical contribution.\nExtensions to testing for multiple unknown change points in the mean, and\ntesting for changes in the covariance matrix are also presented with rigorous\nasymptotic theory and encouraging simulation results. Additionally, we\nillustrate how our approach can be used in combination with wild binary\nsegmentation to estimate the number and location of multiple unknown change\npoints.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 05:30:11 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Wang", "Runmin", ""], ["Volgushev", "Stanislav", ""], ["Shao", "Xiaofeng", ""]]}, {"id": "1905.08515", "submitter": "Miguel del \\'Alamo", "authors": "Miguel del \\'Alamo, Axel Munk", "title": "Total variation multiscale estimators for linear inverse problems", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Even though the statistical theory of linear inverse problems is a\nwell-studied topic, certain relevant cases remain open. Among these is the\nestimation of functions of bounded variation ($BV$), meaning $L^1$ functions on\na $d$-dimensional domain whose weak first derivatives are finite Radon\nmeasures. The estimation of $BV$ functions is relevant in many applications,\nsince it involves minimal smoothness assumptions and gives simplified,\ninterpretable cartoonized reconstructions. In this paper we propose a novel\ntechnique for estimating $BV$ functions in an inverse problem setting, and\nprovide theoretical guaranties by showing that the proposed estimator is\nminimax optimal up to logarithms with respect to the $L^q$-risk, for any\n$q\\in[1,\\infty)$. This is to the best of our knowledge the first convergence\nresult for $BV$ functions in inverse problems in dimension $d\\geq 2$, and it\nextends the results by Donoho (Appl. Comput. Harmon. Anal., 2(2):101--126,\n1995) in $d=1$. Furthermore, our analysis unravels a novel regime for large $q$\nin which the minimax rate is slower than $n^{-1/(d+2\\beta+2)}$, where $\\beta$\nis the degree of ill-posedness: our analysis shows that this slower rate arises\nfrom the low smoothness of $BV$ functions. The proposed estimator combines\nvariational regularization techniques with the wavelet-vaguelette decomposition\nof operators.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 09:34:40 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["del \u00c1lamo", "Miguel", ""], ["Munk", "Axel", ""]]}, {"id": "1905.08707", "submitter": "Michal Branicki", "authors": "Michal Branicki and Kenneth Uda", "title": "Lagrangian uncertainty quantification and information inequalities for\n  stochastic flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.DS math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a systematic information-theoretic framework for quantification\nand mitigation of error in probabilistic Lagrangian (i.e., trajectory-based)\npredictions which are obtained from (Eulerian) vector fields generating the\nunderlying dynamical system in a way which naturally applies in both\ndeterministic and stochastic settings. This work is motivated by the desire to\nimprove Lagrangian predictions in complex, multi-scale systems based on\nsimplified, data-driven models. Here, discrepancies between probability\nmeasures $\\mu$ and $\\nu$ associated with the true dynamics and its\napproximation are quantified via so-called $\\varphi$-divergencies,\n$\\mathcal{D}_\\varphi(\\mu\\|\\nu)$, which are premetrics defined by a class of\nstrictly convex functions $\\varphi$. We derive general information bounds on\nthe uncertainty in estimates, $\\mathbb{E}^{\\nu}[f]$, of `true' observables\n$\\mathbb{E}^{\\mu}[f]$ in terms of $\\varphi$-divergencies; we then derive two\ndistinct bounds on $\\mathcal{D}_\\varphi(\\mu\\|\\nu)$ itself. First, an\nanalytically tractable bound on $\\mathcal{D}_\\varphi(\\mu\\|\\nu)$ is derived from\ndifferences between vector fields generating the true dynamics and its\napproximations. The second bound on $\\mathcal{D}_\\varphi(\\mu\\|\\nu)$ is based on\na difference of so-called finite-time divergence rate (FTDR) fields and it can\nbe exploited within a computational framework to mitigate the error in\nLagrangian predictions by tuning the fields of expansion rates obtained from\nsimplified models. This new framework provides a systematic link between\nEulerian (field-based) model error and the resulting uncertainty in Lagrangian\n(trajectory-based) predictions.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 15:45:43 GMT"}, {"version": "v2", "created": "Tue, 18 May 2021 16:38:27 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Branicki", "Michal", ""], ["Uda", "Kenneth", ""]]}, {"id": "1905.08757", "submitter": "Xiaoou Li", "authors": "T. Tony Cai, Tiefeng Jiang, Xiaoou Li", "title": "Asymptotic Analysis for Extreme Eigenvalues of Principal Minors of\n  Random Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a standard white Wishart matrix with parameters $n$ and $p$.\nMotivated by applications in high-dimensional statistics and signal processing,\nwe perform asymptotic analysis on the maxima and minima of the eigenvalues of\nall the $m \\times m$ principal minors, under the asymptotic regime that $n,p,m$\ngo to infinity. Asymptotic results concerning extreme eigenvalues of principal\nminors of real Wigner matrices are also obtained. In addition, we discuss an\napplication of the theoretical results to the construction of compressed\nsensing matrices, which provides insights to compressed sensing in signal\nprocessing and high dimensional linear regression in statistics.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2019 17:18:08 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Cai", "T. Tony", ""], ["Jiang", "Tiefeng", ""], ["Li", "Xiaoou", ""]]}, {"id": "1905.08930", "submitter": "Alexander Kushkuley", "authors": "Alexander Kushkuley", "title": "Heavy Hitters and Bernoulli Convolutions", "comments": "1) fixed some typos and a reference 2) expanded section 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A very simple event frequency approximation algorithm that is sensitive to\nevent timeliness is suggested. The algorithm iteratively updates categorical\nclick-distribution, producing (path of) a random walk on a standard\n$n$-dimensional simplex. Under certain conditions, this random walk is\nself-similar and corresponds to a biased Bernoulli convolution. Algorithm\nevaluation naturally leads to estimation of moments of biased (finite and\ninfinite) Bernoulli convolutions.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 02:49:09 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 03:26:13 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Kushkuley", "Alexander", ""]]}, {"id": "1905.08975", "submitter": "Pedro Cisneros-Velarde", "authors": "Pedro Cisneros-Velarde, Sang-Yun Oh, Alexander Petersen", "title": "Distributionally Robust Formulation and Model Selection for the\n  Graphical Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building on a recent framework for distributionally robust optimization, we\nconsider estimation of the inverse covariance matrix for multivariate data. We\nprovide a novel notion of a Wasserstein ambiguity set specifically tailored to\nthis estimation problem, leading to a tractable class of regularized\nestimators. Special cases include penalized likelihood estimators for Gaussian\ndata, specifically the graphical lasso estimator. As a consequence of this\nformulation, the radius of the Wasserstein ambiguity set is directly related to\nthe regularization parameter in the estimation problem. Using this\nrelationship, the level of robustness of the estimation procedure can be shown\nto correspond to the level of confidence with which the ambiguity set contains\na distribution with the population covariance. Furthermore, a unique feature of\nour formulation is that the radius can be expressed in closed-form as a\nfunction of the ordinary sample covariance matrix. Taking advantage of this\nfinding, we develop a simple algorithm to determine a regularization parameter\nfor graphical lasso, using only the bootstrapped sample covariance matrices,\nmeaning that computationally expensive repeated evaluation of the graphical\nlasso algorithm is not necessary. Alternatively, the distributionally robust\nformulation can also quantify the robustness of the corresponding estimator if\none uses an off-the-shelf method such as cross-validation. Finally, we\nnumerically study the obtained regularization criterion and analyze the\nrobustness of other automated tuning procedures used in practice.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 06:24:32 GMT"}, {"version": "v2", "created": "Sat, 5 Oct 2019 10:07:02 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Cisneros-Velarde", "Pedro", ""], ["Oh", "Sang-Yun", ""], ["Petersen", "Alexander", ""]]}, {"id": "1905.09021", "submitter": "Dominik Liebl", "authors": "Dominik Po{\\ss}, Dominik Liebl, Alois Kneip, Hedwig Eisenbarth, Tor D.\n  Wager and Lisa Feldman Barrett", "title": "Super-Consistent Estimation of Points of Impact in Nonparametric\n  Regression with Functional Predictors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting scalar outcomes using functional predictors is a classic problem\nin functional data analysis. In many applications, however, only specific\nlocations or time-points of the functional predictors have an impact on the\noutcome. Such ``points of impact'' are typically unknown and have to be\nestimated in addition to estimating the usual model components. We show that\nour points of impact estimator enjoys a super-consistent convergence rate and\ndoes not require knowledge or pre-estimates of the unknown model components.\nThis remarkable result facilitates the subsequent estimation of the remaining\nmodel components as shown in the theoretical part, where we consider the case\nof nonparametric models and the practically relevant case of generalized linear\nmodels. The finite sample properties of our estimators are assessed by means of\na simulation study. Our methodology is motivated by data from a psychological\nexperiment in which the participants were asked to continuously rate their\nemotional state while watching an affective video eliciting a varying intensity\nof emotional reactions.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 08:45:08 GMT"}, {"version": "v2", "created": "Fri, 24 May 2019 13:25:06 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 07:39:23 GMT"}, {"version": "v4", "created": "Fri, 31 May 2019 21:58:30 GMT"}, {"version": "v5", "created": "Tue, 9 Jul 2019 16:55:00 GMT"}, {"version": "v6", "created": "Tue, 18 Feb 2020 16:41:13 GMT"}, {"version": "v7", "created": "Mon, 13 Jul 2020 12:36:38 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Po\u00df", "Dominik", ""], ["Liebl", "Dominik", ""], ["Kneip", "Alois", ""], ["Eisenbarth", "Hedwig", ""], ["Wager", "Tor D.", ""], ["Barrett", "Lisa Feldman", ""]]}, {"id": "1905.09044", "submitter": "Thomas Galtier", "authors": "H. Chraibi, A. Dutfoy, T. Galtier, J. Garnier", "title": "Application of the interacting particle system method to piecewise\n  deterministic Markov processes used in reliability", "comments": null, "journal-ref": null, "doi": "10.1063/1.5081446", "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance reduction methods are often needed for the reliability assessment of\ncomplex industrial systems, we focus on one variance reduction method in a\ngiven context, that is the interacting particle system method (IPS) used on\npiecewise deterministic Markov processes (PDMP) for reliability assessment .\nThe PDMPs are a very large class of processes which benefit from high modeling\ncapacities, they can model almost any Markovian phenomenon that does not\ninclude diffusion. In reliability assessment, the PDMPs modeling industrial\nsystems generally involve low jump rates and jump kernels favoring one safe\narrival, we call such model a \"concentrated PDMP\".\n  Used on such concentrated PDMPs, the IPS is inefficient and does not always\nprovide a variance reduction. Indeed, the efficiency of the IPS method relies\non simulating many different trajectories during its propagation steps, but\nunfortunately concentrated PDMPs are likely to generate the same deterministic\ntrajectories over and over. We propose an adaptation of the IPS method called\nIPS+M that reduces this phenomenon. The IPS+M consists in modifying the\npropagation steps of the IPS, by conditioning the propagation to avoid\ngenerating the same trajectories multiple times. We prove that, compared to the\nIPS, the IPS+M method always provides an estimator with a lower variance. We\nalso carry out a quick simulation study on a two-components system that\nconfirms this result.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 09:53:46 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Chraibi", "H.", ""], ["Dutfoy", "A.", ""], ["Galtier", "T.", ""], ["Garnier", "J.", ""]]}, {"id": "1905.09123", "submitter": "Andriy Olenko", "authors": "Vo Anh, Andriy Olenko, Volodymyr Vaskovych", "title": "On LSE in regression model for long-range dependent random fields on\n  spheres", "comments": "22 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the asymptotic behaviour of least squares estimators in regression\nmodels for long-range dependent random fields observed on spheres. The least\nsquares estimator can be given as a weighted functional of long-range dependent\nrandom fields. It is known that in this scenario the limits can be\nnon-Gaussian. We derive the limit distribution and the corresponding rate of\nconvergence for the estimators. The results were obtained under rather general\nassumptions on the random fields. Simulation studies were conducted to support\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 13:26:42 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Anh", "Vo", ""], ["Olenko", "Andriy", ""], ["Vaskovych", "Volodymyr", ""]]}, {"id": "1905.09195", "submitter": "Satoshi Hayakawa", "authors": "Satoshi Hayakawa, Taiji Suzuki", "title": "On the minimax optimality and superiority of deep neural network\n  learning over sparse parameter spaces", "comments": "33 pages", "journal-ref": null, "doi": "10.1016/j.neunet.2019.12.014", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has been applied to various tasks in the field of machine\nlearning and has shown superiority to other common procedures such as kernel\nmethods. To provide a better theoretical understanding of the reasons for its\nsuccess, we discuss the performance of deep learning and other methods on a\nnonparametric regression problem with a Gaussian noise. Whereas existing\ntheoretical studies of deep learning have been based mainly on mathematical\ntheories of well-known function classes such as H\\\"{o}lder and Besov classes,\nwe focus on function classes with discontinuity and sparsity, which are those\nnaturally assumed in practice. To highlight the effectiveness of deep learning,\nwe compare deep learning with a class of linear estimators representative of a\nclass of shallow estimators. It is shown that the minimax risk of a linear\nestimator on the convex hull of a target function class does not differ from\nthat of the original target function class. This results in the suboptimality\nof linear methods over a simple but non-convex function class, on which deep\nlearning can attain nearly the minimax-optimal rate. In addition to this\nextreme case, we consider function classes with sparse wavelet coefficients. On\nthese function classes, deep learning also attains the minimax rate up to log\nfactors of the sample size, and linear methods are still suboptimal if the\nassumed sparsity is strong. We also point out that the parameter sharing of\ndeep neural networks can remarkably reduce the complexity of the model in our\nsetting.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 15:35:40 GMT"}, {"version": "v2", "created": "Fri, 20 Sep 2019 14:43:41 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Hayakawa", "Satoshi", ""], ["Suzuki", "Taiji", ""]]}, {"id": "1905.09369", "submitter": "Arvind Prasadan", "authors": "Arvind Prasadan, Raj Rao Nadakuditi, Debashis Paul", "title": "Sparse Equisigned PCA: Algorithms and Performance Bounds in the Noisy\n  Rank-1 Setting", "comments": "To appear, Electronic Journal of Statistics, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG eess.SP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Singular value decomposition (SVD) based principal component analysis (PCA)\nbreaks down in the high-dimensional and limited sample size regime below a\ncertain critical eigen-SNR that depends on the dimensionality of the system and\nthe number of samples. Below this critical eigen-SNR, the estimates returned by\nthe SVD are asymptotically uncorrelated with the latent principal components.\nWe consider a setting where the left singular vector of the underlying rank one\nsignal matrix is assumed to be sparse and the right singular vector is assumed\nto be equisigned, that is, having either only nonnegative or only nonpositive\nentries. We consider six different algorithms for estimating the sparse\nprincipal component based on different statistical criteria and prove that by\nexploiting sparsity, we recover consistent estimates in the low eigen-SNR\nregime where the SVD fails. Our analysis reveals conditions under which a\ncoordinate selection scheme based on a \\textit{sum-type decision statistic}\noutperforms schemes that utilize the $\\ell_1$ and $\\ell_2$ norm-based\nstatistics. We derive lower bounds on the size of detectable coordinates of the\nprincipal left singular vector and utilize these lower bounds to derive lower\nbounds on the worst-case risk. Finally, we verify our findings with numerical\nsimulations and illustrate the performance with a video data example, where the\ninterest is in identifying objects.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 21:17:37 GMT"}, {"version": "v2", "created": "Mon, 16 Dec 2019 16:12:30 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Prasadan", "Arvind", ""], ["Nadakuditi", "Raj Rao", ""], ["Paul", "Debashis", ""]]}, {"id": "1905.09371", "submitter": "Kori Khan", "authors": "Kori Khan, Catherine A. Calder", "title": "Restricted Spatial Regression Methods: Implications for Inference", "comments": "Minor notation and typo edits. Primary change is to statement and\n  proof of Theorem 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of spatial confounding between the spatial random effect and the\nfixed effects in regression analyses has been identified as a concern in the\nstatistical literature. Multiple authors have offered perspectives and\npotential solutions. In this paper, for the areal spatial data setting, we show\nthat many of the methods designed to alleviate spatial confounding can be\nviewed as special cases of a general class of models. We refer to this class as\nRestricted Spatial Regression (RSR) models, extending terminology currently in\nuse. We offer a mathematically based exploration of the impact that RSR methods\nhave on inference for regression coefficients for the linear model. We then\nexplore whether these results hold in the generalized linear model setting for\ncount data using simulations. We show that the use of these methods have\ncounterintuitive consequences which defy the general expectations in the\nliterature. In particular, our results and the accompanying simulations suggest\nthat RSR methods will typically perform worse than non-spatial methods. These\nresults have important implications for dimension reduction strategies in\nspatial regression modeling. Specifically, we demonstrate that the problems\nwith RSR models cannot be fixed with a selection of \"better\" spatial basis\nvectors or dimension reduction techniques.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2019 21:23:45 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 15:43:28 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 15:35:56 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Khan", "Kori", ""], ["Calder", "Catherine A.", ""]]}, {"id": "1905.09420", "submitter": "Jordan Awan", "authors": "Matthew Reimherr and Jordan Awan", "title": "Elliptical Perturbations for Differential Privacy", "comments": "13 pages. Published in NeurIPS 2019\n  (https://proceedings.neurips.cc/paper/2019/hash/b3dd760eb02d2e669c604f6b2f1e803f-Abstract.html).\n  This Arxiv document corrects a few minor errors in the published version", "journal-ref": "NeurIPS 32 (2019)", "doi": null, "report-no": null, "categories": "cs.CR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study elliptical distributions in locally convex vector spaces, and\ndetermine conditions when they can or cannot be used to satisfy differential\nprivacy (DP). A requisite condition for a sanitized statistical summary to\nsatisfy DP is that the corresponding privacy mechanism must induce equivalent\nmeasures for all possible input databases. We show that elliptical\ndistributions with the same dispersion operator, $C$, are equivalent if the\ndifference of their means lies in the Cameron-Martin space of $C$. In the case\nof releasing finite-dimensional projections using elliptical perturbations, we\nshow that the privacy parameter $\\ep$ can be computed in terms of a\none-dimensional maximization problem. We apply this result to consider\nmultivariate Laplace, $t$, Gaussian, and $K$-norm noise. Surprisingly, we show\nthat the multivariate Laplace noise does not achieve $\\ep$-DP in any dimension\ngreater than one. Finally, we show that when the dimension of the space is\ninfinite, no elliptical distribution can be used to give $\\ep$-DP; only\n$(\\epsilon,\\delta)$-DP is possible.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 01:15:39 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 17:12:54 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Reimherr", "Matthew", ""], ["Awan", "Jordan", ""]]}, {"id": "1905.09451", "submitter": "Ujan Gangopadhyay", "authors": "Ujan Gangopadhyay, Gourab Mukherjee", "title": "Sparse Minimax Optimality of Bayes Predictive Density Estimates from\n  Clustered Discrete Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predictive density estimation under\nKullback-Leibler loss in a high-dimensional Gaussian model with exact sparsity\nconstraints on the location parameters. We study the first order asymptotic\nminimax risk of Bayes predictive density estimates based on product discrete\npriors where the proportion of non-zero coordinates converges to zero as\ndimension increases. Discrete priors that are product of clustered univariate\npriors provide a tractable configuration for diversification of the future risk\nand are used for constructing efficient predictive density estimates. We\nestablish that the Bayes predictive density estimate from an appropriately\ndesigned clustered discrete prior is asymptotically minimax optimal. The\nmarginals of our proposed prior have infinite clusters of identical sizes. The\nwithin cluster support points are equi-probable and the clusters are\nperiodically spaced with geometrically decaying probabilities as they move away\nfrom the origin. The cluster periodicity depends on the decay rate of the\ncluster probabilities. Under different sparsity regimes, through numerical\nexperiments, we compare the maximal risk of the Bayes predictive density\nestimates from the clustered prior with varied competing estimators including\nthose based on geometrically decaying non-clustered priors of Johnstone (1994)\nand Mukherjee & Johnstone (2017) and obtain encouraging results.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 03:55:06 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Gangopadhyay", "Ujan", ""], ["Mukherjee", "Gourab", ""]]}, {"id": "1905.09599", "submitter": "Hong Zhang", "authors": "Chengxiu Ling, Hong Zhang and Long Bai", "title": "On generalized Piterbarg-Berman function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to evaluate the Piterbarg-Berman function given by\n$$\\mathcal{P\\!B}_\\alpha^h(x, E) = \\int_\\mathbb{R}e^z\\mathbb{P} \\left\\{{\\int_E\n\\mathbb{I}\\left(\\sqrt2B_\\alpha(t) - |t|^\\alpha - h(t) - z>0 \\right) {\\text{d}}\nt > x} \\right\\} {\\text{d}} z,\\quad x\\in[0, {mes}(E)],$$ with $h$ a drift\nfunction and $B_\\alpha$ a fractional Brownian motion (fBm) with Hurst index\n$\\alpha/2\\in(0,1]$, i.e., a mean zero Gaussian process with continuous sample\npaths and covariance function \\begin{align*} {\\mathrm{Cov}}(B_\\alpha(s),\nB_\\alpha(t)) = \\frac12 (|s|^\\alpha + |t|^\\alpha - |s-t|^\\alpha). \\end{align*}\nThis note specifies its explicit expression for the fBms with $\\alpha=1$ and\n$2$ when the drift function $h(t)=ct^\\alpha, c>0$ and\n$E=\\mathbb{R}_+\\cup\\{0\\}$. For the Gaussian distribution $B_2$, we investigate\n$\\mathcal{P\\!B}_2^h(x, E)$ with general drift functions $h(t)$ such that\n$h(t)+t^2$ being convex or concave, and finite interval $E=[a,b]$. Typical\nexamples of $\\mathcal{P\\!B}_2^h(x, E)$ with $h(t)=c|t|^\\lambda-t^2$ and several\nbounds of $\\mathcal{P\\!B}_\\alpha^h(x, E)$ are discussed. Numerical studies are\ncarried out to illustrate all the findings.\n  Keywords: Piterbarg-Berman function; sojourn time; fractional Brownian\nmotion; drift function\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 11:51:37 GMT"}], "update_date": "2019-05-24", "authors_parsed": [["Ling", "Chengxiu", ""], ["Zhang", "Hong", ""], ["Bai", "Long", ""]]}, {"id": "1905.09813", "submitter": "Ian Langmore", "authors": "Ian Langmore, Michael Dikovsky, Scott Geraedts, Peter Norgaard, Rob\n  Von Behren", "title": "A Condition Number for Hamiltonian Monte Carlo", "comments": "Significant changes: (i) Added connection to inverse Wishart\n  ensemble, (ii) added estimation of kappa, (iii) checked and corrected proofs,\n  (iv) re-wrote everything for clarity, (v) added authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Hamiltonian Monte Carlo is a popular sampling technique for smooth target\ndensities. The scale lengths of the target have long been known to influence\nintegration error and sampling efficiency. However, quantitative measures\nintrinsic to the target have been lacking. In this paper, we restrict attention\nto the multivariate Gaussian and the leapfrog integrator, and obtain a\ncondition number corresponding to sampling efficiency. This number, based on\nthe spectral and Schatten norms, quantifies the number of leapfrog steps needed\nto efficiently sample. We demonstrate its utility by using this condition\nnumber to analyze HMC preconditioning techniques. We also find the condition\nnumber of large inverse Wishart matrices, from which we derive burn-in\nheuristics.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:59:31 GMT"}, {"version": "v2", "created": "Mon, 3 Jun 2019 16:00:24 GMT"}, {"version": "v3", "created": "Wed, 5 Feb 2020 00:16:31 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Langmore", "Ian", ""], ["Dikovsky", "Michael", ""], ["Geraedts", "Scott", ""], ["Norgaard", "Peter", ""], ["Von Behren", "Rob", ""]]}, {"id": "1905.09863", "submitter": "Yulong Lu", "authors": "Yulong Lu, Jianfeng Lu, James Nolen", "title": "Accelerating Langevin Sampling with Birth-death", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in Bayesian inference and statistical machine learning\nis to efficiently sample from multimodal distributions. Due to metastability,\nmultimodal distributions are difficult to sample using standard Markov chain\nMonte Carlo methods. We propose a new sampling algorithm based on a birth-death\nmechanism to accelerate the mixing of Langevin diffusion. Our algorithm is\nmotivated by its mean field partial differential equation (PDE), which is a\nFokker-Planck equation supplemented by a nonlocal birth-death term. This PDE\ncan be viewed as a gradient flow of the Kullback-Leibler divergence with\nrespect to the Wasserstein-Fisher-Rao metric. We prove that under some\nassumptions the asymptotic convergence rate of the nonlocal PDE is independent\nof the potential barrier, in contrast to the exponential dependence in the case\nof the Langevin diffusion. We illustrate the efficiency of the birth-death\naccelerated Langevin method through several analytical examples and numerical\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 18:39:26 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Lu", "Yulong", ""], ["Lu", "Jianfeng", ""], ["Nolen", "James", ""]]}, {"id": "1905.09881", "submitter": "Ardalan Mirshani", "authors": "Ardalan Mirshani, Matthew Reimherr", "title": "Adaptive Function-on-Scalar Regression with a Smoothing Elastic Net", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new methodology, called AFSSEN, to simultaneously\nselect significant predictors and produce smooth estimates in a\nhigh-dimensional function-on-scalar linear model with a sub-Gaussian errors.\nOutcomes are assumed to lie in a general real separable Hilbert space, H, while\nparameters lie in a subspace known as a Cameron Martin space, K, which are\nclosely related to Reproducing Kernel Hilbert Spaces, so that parameter\nestimates inherit particular properties, such as smoothness or periodicity,\nwithout enforcing such properties on the data. We propose a regularization\nmethod in the style of an adaptive Elastic Net penalty that involves mixing two\ntypes of functional norms, providing a fine tune control of both the smoothing\nand variable selection in the estimated model. Asymptotic theory is provided in\nthe form of a functional oracle property, and the paper concludes with a\nsimulation study demonstrating the advantage of using AFSSEN over existing\nmethods in terms of prediction error and variable selection.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 19:23:58 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Mirshani", "Ardalan", ""], ["Reimherr", "Matthew", ""]]}, {"id": "1905.09948", "submitter": "HaiYing Wang", "authors": "HaiYing Wang", "title": "Divide-and-Conquer Information-Based Optimal Subdata Selection Algorithm", "comments": "21 pages, 3 figures, 1 table", "journal-ref": null, "doi": "10.1007/s42519-019-0048-5", "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The information-based optimal subdata selection (IBOSS) is a computationally\nefficient method to select informative data points from large data sets through\nprocessing full data by columns. However, when the volume of a data set is too\nlarge to be processed in the available memory of a machine, it is infeasible to\nimplement the IBOSS procedure. This paper develops a divide-and-conquer IBOSS\napproach to solving this problem, in which the full data set is divided into\nsmaller partitions to be loaded into the memory and then subsets of data are\nselected from each partitions using the IBOSS algorithm. We derive both finite\nsample properties and asymptotic properties of the resulting estimator.\nAsymptotic results show that if the full data set is partitioned randomly and\nthe number of partitions is not very large, then the resultant estimator has\nthe same estimation efficiency as the original IBOSS estimator. We also carry\nout numerical experiments to evaluate the empirical performance of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 21:56:38 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Wang", "HaiYing", ""]]}, {"id": "1905.09959", "submitter": "Chiao-Yu Yang", "authors": "Chiao-Yu Yang, Eric Xia, Nhat Ho, Michael I. Jordan", "title": "Posterior Distribution for the Number of Clusters in Dirichlet Process\n  Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dirichlet process mixture models (DPMM) play a central role in Bayesian\nnonparametrics, with applications throughout statistics and machine learning.\nDPMMs are generally used in clustering problems where the number of clusters is\nnot known in advance, and the posterior distribution is treated as providing\ninference for this number. Recently, however, it has been shown that the DPMM\nis inconsistent in inferring the true number of components in certain cases.\nThis is an asymptotic result, and it would be desirable to understand whether\nit holds with finite samples, and to more fully understand the full posterior.\nIn this work, we provide a rigorous study for the posterior distribution of the\nnumber of clusters in DPMM under different prior distributions on the\nparameters and constraints on the distributions of the data. We provide novel\nlower bounds on the ratios of probabilities between $s+1$ clusters and $s$\nclusters when the prior distributions on parameters are chosen to be Gaussian\nor uniform distributions.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 22:51:15 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 00:17:53 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Yang", "Chiao-Yu", ""], ["Xia", "Eric", ""], ["Ho", "Nhat", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1905.09964", "submitter": "Alessandro Zocca", "authors": "John Moriarty, Jure Vogrinc, Alessandro Zocca", "title": "A Metropolis-class sampler for targets with non-convex support", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to improve upon the exploration of the general-purpose random walk\nMetropolis algorithm when the target has non-convex support $A \\subset\n\\mathbb{R}^d$, by reusing proposals in $A^c$ which would otherwise be rejected.\nThe algorithm is Metropolis-class and under standard conditions the chain\nsatisfies a strong law of large numbers and central limit theorem. Theoretical\nand numerical evidence of improved performance relative to random walk\nMetropolis are provided. Issues of implementation are discussed and numerical\nexamples, including applications to global optimisation and rare event\nsampling, are presented.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 23:12:47 GMT"}, {"version": "v2", "created": "Sat, 24 Oct 2020 10:52:05 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Moriarty", "John", ""], ["Vogrinc", "Jure", ""], ["Zocca", "Alessandro", ""]]}, {"id": "1905.10019", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla, Yi Yu, Daren Wang, and Alessandro Rinaldo", "title": "Optimal nonparametric change point detection and localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study change point detection and localization for univariate data in fully\nnonparametric settings in which, at each time point, we acquire an i.i.d.\nsample from an unknown distribution. We quantify the magnitude of the\ndistributional changes at the change points using the Kolmogorov--Smirnov\ndistance. We allow all the relevant parameters -- the minimal spacing between\ntwo consecutive change points, the minimal magnitude of the changes in the\nKolmogorov--Smirnov distance, and the number of sample points collected at each\ntime point -- to change with the length of time series. We generalize the\nrenowned binary segmentation (e.g. Scott and Knott, 1974) algorithm and its\nvariant, the wild binary segmentation of Fryzlewicz (2014), both originally\ndesigned for univariate mean change point detection problems, to our\nnonparametric settings and exhibit rates of consistency for both of them. In\nparticular, we prove that the procedure based on wild binary segmentation is\nnearly minimax rate-optimal. We further demonstrate a phase transition in the\nspace of model parameters that separates parameter combinations for which\nconsistent localization is possible from the ones for which this task is\nstatistical unfeasible. Finally, we provide extensive numerical experiments to\nsupport our theory. R code is available at https://github.com/hernanmp/NWBS.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 03:34:23 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Yu", "Yi", ""], ["Wang", "Daren", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1905.10030", "submitter": "Andriy Olenko", "authors": "Tareq Alodat, Andriy Olenko", "title": "Asymptotic Behaviour of Discretised Functionals of Long-Range Dependent\n  Functional Data", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper studies the asymptotic behaviour of weighted functionals of\nlong-range dependent data over increasing observation windows. Various\nimportant statistics, including sample means, high order moments, occupation\nmeasures can be given by these functionals. It is shown that in the discrete\nsampling case additive functionals have the same asymptotic distribution as the\ncorresponding integral functionals for the continuous functional data case.\nThese results are applied to obtain non-central limit theorems for weighted\nadditive functionals of random fields. As the majority of known results concern\nthe discrete sampling case the developed methodology helps in translating these\nresults to functional data without deriving them again. Numerical studies\nsuggest that the theoretical findings are valid for wider classes of long-range\ndependent data.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 04:55:54 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Alodat", "Tareq", ""], ["Olenko", "Andriy", ""]]}, {"id": "1905.10031", "submitter": "Jingbo Liu", "authors": "Vishesh Jain and Frederic Koehler and Jingbo Liu and Elchanan Mossel", "title": "Accuracy-Memory Tradeoffs and Phase Transitions in Belief Propagation", "comments": "To be presented on COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of Belief Propagation and other algorithms for the {\\em\nreconstruction problem} plays a key role in the analysis of community detection\nin inference on graphs, phylogenetic reconstruction in bioinformatics, and the\ncavity method in statistical physics.\n  We prove a conjecture of Evans, Kenyon, Peres, and Schulman (2000) which\nstates that any bounded memory message passing algorithm is statistically much\nweaker than Belief Propagation for the reconstruction problem. More formally,\nany recursive algorithm with bounded memory for the reconstruction problem on\nthe trees with the binary symmetric channel has a phase transition strictly\nbelow the Belief Propagation threshold, also known as the Kesten-Stigum bound.\nThe proof combines in novel fashion tools from recursive reconstruction,\ninformation theory, and optimal transport, and also establishes an asymptotic\nnormality result for BP and other message-passing algorithms near the critical\nthreshold.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 04:56:12 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Jain", "Vishesh", ""], ["Koehler", "Frederic", ""], ["Liu", "Jingbo", ""], ["Mossel", "Elchanan", ""]]}, {"id": "1905.10035", "submitter": "Vahid Partovi Nia", "authors": "Shaima Tilouche, Vahid Partovi Nia, Samuel Bassetto", "title": "Parallel Coordinate Order for High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.GR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization of high-dimensional data is counter-intuitive using\nconventional graphs. Parallel coordinates are proposed as an alternative to\nexplore multivariate data more effectively. However, it is difficult to extract\nrelevant information through the parallel coordinates when the data are\nhigh-dimensional with thousands of lines overlapping. The order of the axes\ndetermines the perception of information on parallel coordinates. Thus, the\ninformation between attributes remain hidden if coordinates are improperly\nordered. Here we propose a general framework to reorder the coordinates. This\nframework is general to cover a large range of data visualization objective. It\nis also flexible to contain many conventional ordering measures. Consequently,\nwe present the coordinate ordering binary optimization problem and enhance\ntowards a computationally efficient greedy approach that suites\nhigh-dimensional data. Our approach is applied on wine data and on genetic\ndata. The purpose of dimension reordering of wine data is highlighting\nattributes dependence. Genetic data are reordered to enhance cluster detection.\nThe presented framework shows that it is able to adapt the measures and\ncriteria tested.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 05:08:01 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Tilouche", "Shaima", ""], ["Nia", "Vahid Partovi", ""], ["Bassetto", "Samuel", ""]]}, {"id": "1905.10116", "submitter": "Mert Demirer", "authors": "Mert Demirer, Vasilis Syrgkanis, Greg Lewis, Victor Chernozhukov", "title": "Semi-Parametric Efficient Policy Learning with Continuous Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider off-policy evaluation and optimization with continuous action\nspaces. We focus on observational data where the data collection policy is\nunknown and needs to be estimated. We take a semi-parametric approach where the\nvalue function takes a known parametric form in the treatment, but we are\nagnostic on how it depends on the observed contexts. We propose a doubly robust\noff-policy estimate for this setting and show that off-policy optimization\nbased on this estimate is robust to estimation errors of the policy function or\nthe regression model. Our results also apply if the model does not satisfy our\nsemi-parametric form, but rather we measure regret in terms of the best\nprojection of the true value function to this functional space. Our work\nextends prior approaches of policy optimization from observational data that\nonly considered discrete actions. We provide an experimental evaluation of our\nmethod in a synthetic data example motivated by optimal personalized pricing\nand costly resource allocation.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 09:51:21 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 23:36:34 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Demirer", "Mert", ""], ["Syrgkanis", "Vasilis", ""], ["Lewis", "Greg", ""], ["Chernozhukov", "Victor", ""]]}, {"id": "1905.10155", "submitter": "Remi Flamary", "authors": "R\\'emi Flamary, Karim Lounici, Andr\\'e Ferrari", "title": "Concentration bounds for linear Monge mapping estimation and optimal\n  transport domain adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates the quality of the estimator of the linear Monge\nmapping between distributions. We provide the first concentration result on the\nlinear mapping operator and prove a sample complexity of $n^{-1/2}$ when using\nempirical estimates of first and second order moments. This result is then used\nto derive a generalization bound for domain adaptation with optimal transport.\nAs a consequence, this method approaches the performance of theoretical Bayes\npredictor under mild conditions on the covariance structure of the problem. We\nalso discuss the computational complexity of the linear mapping estimation and\nshow that when the source and target are stationary the mapping is a\nconvolution that can be estimated very efficiently using fast Fourier\ntransforms. Numerical experiments reproduce the behavior of the proven bounds\non simulated and real data for mapping estimation and domain adaptation on\nimages.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 11:31:34 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 10:27:58 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Flamary", "R\u00e9mi", ""], ["Lounici", "Karim", ""], ["Ferrari", "Andr\u00e9", ""]]}, {"id": "1905.10172", "submitter": "Dr. Alexander Paraskevov", "authors": "A.V. Paraskevov, A.S. Minkin", "title": "Damped oscillations of the probability of random events followed by\n  absolute refractory period", "comments": "additional section has been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC math.PR math.ST physics.data-an stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many events are followed by an absolute refractory state, when for some time\nafter the event a repetition of a similar event is impossible. If uniform\nevents, each of which is followed by the same period of absolute\nrefractoriness, occur randomly, as in the Bernoulli scheme, then the event\nprobability as a function of time can exhibit damped transient oscillations\ncaused by a specific initial condition. Here we give an exact analytical\ndescription of the oscillations, with a focus on application within\nneuroscience. The resulting formulas stand out for their relative simplicity,\nenabling analytical calculation of the damping coefficients for the second and\nthird peaks of the event probability.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 12:02:17 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 15:05:26 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Paraskevov", "A. V.", ""], ["Minkin", "A. S.", ""]]}, {"id": "1905.10221", "submitter": "Hedi Hadiji", "authors": "H\\'edi Hadiji (LMO)", "title": "Polynomial Cost of Adaptation for X -Armed Bandits", "comments": null, "journal-ref": "Thirty-third Conference on Neural Information Processing Systems,\n  Dec 2019, Vancouver, France", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of stochastic continuum-armed bandits, we present an algorithm\nthat adapts to the unknown smoothness of the objective function. We exhibit and\ncompute a polynomial cost of adaptation to the H{\\\"o}lder regularity for regret\nminimization. To do this, we first reconsider the recent lower bound of\nLocatelli and Carpentier [20], and define and characterize admissible rate\nfunctions. Our new algorithm matches any of these minimal rate functions. We\nprovide a finite-time analysis and a thorough discussion about asymptotic\noptimality.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 13:15:34 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 10:16:52 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Hadiji", "H\u00e9di", "", "LMO"]]}, {"id": "1905.10299", "submitter": "Weixin Cai", "authors": "Weixin Cai and Mark van der Laan", "title": "Nonparametric Bootstrap Inference for the Targeted Highly Adaptive LASSO\n  Estimator", "comments": "arXiv admin note: substantial text overlap with arXiv:1708.09502", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Highly-Adaptive-LASSO Targeted Minimum Loss Estimator (HAL-TMLE) is an\nefficient plug-in estimator of a pathwise differentiable parameter in a\nstatistical model that at minimal (and possibly only) assumes that the\nsectional variation norm of the true nuisance functional parameters (i.e., the\nrelevant part of data distribution) are finite. It relies on an initial\nestimator (HAL-MLE) of the nuisance functional parameters by minimizing the\nempirical risk over the parameter space under the constraint that the sectional\nvariation norm of the candidate functions are bounded by a constant, where this\nconstant can be selected with cross-validation. In this article, we establish\nthat the nonparametric bootstrap for the HAL-TMLE, fixing the value of the\nsectional variation norm at a value larger or equal than the cross-validation\nselector, provides a consistent method for estimating the normal limit\ndistribution of the HAL-TMLE.\n  In order to optimize the finite sample coverage of the nonparametric\nbootstrap confidence intervals, we propose a selection method for this\nsectional variation norm that is based on running the nonparametric bootstrap\nfor all values of the sectional variation norm larger than the one selected by\ncross-validation, and subsequently determining a value at which the width of\nthe resulting confidence intervals reaches a plateau.\n  We demonstrate our method for 1) nonparametric estimation of the average\ntreatment effect based on observing on each unit a covariate vector, binary\ntreatment, and outcome, and for 2) nonparametric estimation of the integral of\nthe square of the multivariate density of the data distribution. In addition,\nwe also present simulation results for these two examples demonstrating the\nexcellent finite sample coverage of bootstrap-based confidence intervals.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2019 17:55:38 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 23:21:22 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Cai", "Weixin", ""], ["van der Laan", "Mark", ""]]}, {"id": "1905.10325", "submitter": "Shahin Tavakoli", "authors": "Shahin Tavakoli and Gilles Nisol and Marc Hallin", "title": "Factor Models for High-Dimensional Functional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we set up the theoretical foundations for a high-dimensional\nfunctional factor model approach in the analysis of large cross-sections\n(panels) of functional time series (FTS). We first establish a representation\nresult stating that, under mild assumptions on the covariance operator of the\ncross-section, we can represent each FTS as the sum of a common component\ndriven by scalar factors loaded via functional loadings, and a mildly\ncross-correlated idiosyncratic component. Our model and theory are developed in\na general Hilbert space setting that allows for mixed panels of functional and\nscalar time series. We then turn to the identification of the number of\nfactors, and the estimation of the factors, their loadings, and the common\ncomponents. We provide a family of information criteria for identifying the\nnumber of factors, and prove their consistency. We provide average error bounds\nfor the estimators of the factors, loadings, and common component; our results\nencompass the scalar case, for which they reproduce and extend, under weaker\nconditions, well-established similar results. Under slightly stronger\nassumptions, we also provide uniform bounds for the estimators of factors,\nloadings, and common component, thus extending existing scalar results. Our\nconsistency results in the asymptotic regime where the number $N$ of series and\nthe number $T$ of time observations diverge thus extend to the functional\ncontext the \"blessing of dimensionality\" that explains the success of factor\nmodels in the analysis of high-dimensional (scalar) time series. We provide\nnumerical illustrations that corroborate the convergence rates predicted by the\ntheory, and provide finer understanding of the interplay between $N$ and $T$\nfor estimation purposes. We conclude with an application to forecasting\nmortality curves, where we demonstrate that our approach outperforms existing\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 16:33:14 GMT"}, {"version": "v2", "created": "Fri, 21 Feb 2020 18:56:20 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 17:30:16 GMT"}, {"version": "v4", "created": "Tue, 13 Apr 2021 16:34:43 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Tavakoli", "Shahin", ""], ["Nisol", "Gilles", ""], ["Hallin", "Marc", ""]]}, {"id": "1905.10326", "submitter": "Giovanna Nappo", "authors": "Giovanna Nappo, and Fabio L. Spizzichino", "title": "Revisiting Relations between Stochastic Ageing and Dependence for\n  Exchangeable Lifetimes with an Extension for the IFRA/DFRA Property", "comments": null, "journal-ref": null, "doi": null, "report-no": "Roma01.Math", "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first review an approach that had been developed in the past years to\nintroduce concepts of \"bivariate ageing\" for exchangeable lifetimes and to\nanalyze mutual relations among stochastic dependence, univariate ageing, and\nbivariate ageing. A specific feature of such an approach dwells on the concept\nof semi-copula and in the extension, from copulas to semi-copulas, of\nproperties of stochastic dependence. In this perspective, we aim to discuss\nsome intricate aspects of conceptual character and to provide the readers with\npertinent remarks from a Bayesian Statistics standpoint. In particular we will\ndiscuss the role of extensions of dependence properties. \"Archimedean\" models\nhave an important role in the present framework. In the second part of the\npaper, the definitions of Kendall distribution and of Kendall equivalence\nclasses will be extended to semi-copulas and related properties will be\nanalyzed. On such a basis, we will consider the notion of \"Pseudo-Archimedean\"\nmodels and extend to them the analysis of the relations between the ageing\nnotions of IFRA/DFRA-type and the dependence concepts of PKD/NKD.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 16:35:11 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Nappo", "Giovanna", ""], ["Spizzichino", "Fabio L.", ""]]}, {"id": "1905.10354", "submitter": "Holger Dette", "authors": "Holger Dette, Nina D\\\"ornemann", "title": "Likelihood ratio tests for many groups in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the asymptotic distribution of likelihood ratio\ntests in models with several groups, when the number of groups converges with\nthe dimension and sample size to infinity. We derive central limit theorems for\nthe logarithm of various test statistics and compare our results with the\napproximations obtained from a central limit theorem using a two step\nprocedure: first consider the number of groups fixed and assume that the sample\nsize and dimension converge to infinity, secondly investigating the resulting\ndistribution if the number of groups converges to infinity.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 17:40:16 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 19:45:28 GMT"}], "update_date": "2019-07-17", "authors_parsed": [["Dette", "Holger", ""], ["D\u00f6rnemann", "Nina", ""]]}, {"id": "1905.10358", "submitter": "Aleksandr Aravkin", "authors": "Aleksandr Aravkin, James Burke and Daiwei He", "title": "On the Global Minimizers of Real Robust Phase Retrieval with Sparse\n  Noise", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a class of real robust phase retrieval problems under a Gaussian\nassumption on the coding matrix when the received signal is sparsely corrupted\nby noise. The goal is to establish conditions on the sparsity under which the\ninput vector can be exactly recovered. The recovery problem is formulated as\nthe minimization of the $\\ell_1$ norm of the residual.\n  The main contribution is a robust phase retrieval counterpart to the seminal\npaper by Candes and Tao on compressed sensing ($\\ell_1$ regression) [Decoding\nby linear programming. IEEE Transactions on Information Theory,\n51(12):4203-4215, 2005]. Our analysis depends on a key new property on the\ncoding matrix which we call the {Absolute Range Property} (ARP). This property\nis an analogue to the Null Space Property (NSP) in compressed sensing. When the\nresiduals are computed using squared magnitudes, we show that ARP follows from\na standard Restricted Isometry Property (RIP). However, when the residuals are\ncomputed using absolute magnitudes, a new and very different kind of RIP or\ngrowth property is required. We conclude by showing that the robust phase\nretrieval objectives are sharp with respect to their minimizers with high\nprobability.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2019 17:46:03 GMT"}], "update_date": "2019-05-27", "authors_parsed": [["Aravkin", "Aleksandr", ""], ["Burke", "James", ""], ["He", "Daiwei", ""]]}, {"id": "1905.10638", "submitter": "Pierre Patie", "authors": "Pierre Patie and Anna Srapionyan", "title": "Spectral projections correlation structure for short-to-long range\n  dependent processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\mathbf{X}=(\\mathbf{X}_t)_{t \\geq 0}$ be a stochastic process issued\nfrom $x \\in \\mathbb R$ that admits a marginal stationary measure $\\nu$, i.e.\n$\\nu \\mathbf{P}_t f = \\nu f$ for all $t \\geq 0$, where $\\mathbf{P}_t f(x)=\n\\mathbb{E}_x[f(\\mathbf{X}_t)]$. In this paper, we introduce the (resp.\nbiorthogonal) spectral projections correlation functions which are expressed in\nterms of projections into the eigenspaces of $\\mathbf{P}_t$ (resp. and of its\nadjoint in the weighted Hilbert space $L^2(\\nu)$). We obtain closed-form\nexpressions involving eigenvalues, the condition number and/or the angle\nbetween the projections in the following different situations: when\n$\\mathbf{X}=X$ with $X=(X_t)_{t \\geq 0}$ is a Markov process, $\\mathbf{X}$ is\nthe subordination of $X$ in the sense of Bochner, and $\\mathbf X$ is a\nnon-Markovian process which is obtained by time-changing $X$ with an inverse of\na subordinator. It turns out that these spectral projections correlation\nfunctions have different expressions with respect to these classes of processes\nwhich enables to identify substantial and deep properties about their dynamics.\nThis interesting fact can be used to design original statistical tests to make\ninferences, for example, about the path properties of the process (presence of\njumps), distance from symmetry (self-adjoint or non-self-adjoint) and\nshort-to-long-range dependence. To reveal the usefulness of our results, we\napply them to a class of non-self-adjoint Markov semigroups studied in Patie\nand Savov [28], and then time-change by subordinators and their inverses.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 17:25:53 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Patie", "Pierre", ""], ["Srapionyan", "Anna", ""]]}, {"id": "1905.10651", "submitter": "Wei Peng", "authors": "Wei Peng, Tim Coleman, Lucas Mentch", "title": "Asymptotic Distributions and Rates of Convergence for Random Forests via\n  Generalized U-statistics", "comments": "62 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random forests remain among the most popular off-the-shelf supervised\nlearning algorithms. Despite their well-documented empirical success, however,\nuntil recently, few theoretical results were available to describe their\nperformance and behavior. In this work we push beyond recent work on\nconsistency and asymptotic normality by establishing rates of convergence for\nrandom forests and other supervised learning ensembles. We develop the notion\nof generalized U-statistics and show that within this framework, random forest\npredictions can potentially remain asymptotically normal for larger subsample\nsizes than previously established. We also provide Berry-Esseen bounds in order\nto quantify the rate at which this convergence occurs, making explicit the\nroles of the subsample size and the number of trees in determining the\ndistribution of random forest predictions.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 18:31:23 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2019 17:33:17 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Peng", "Wei", ""], ["Coleman", "Tim", ""], ["Mentch", "Lucas", ""]]}, {"id": "1905.10733", "submitter": "Juho Lee", "authors": "Juho Lee, Xenia Miscouridou, Fran\\c{c}ois Caron", "title": "A unified construction for series representations and finite\n  approximations of completely random measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infinite-activity completely random measures (CRMs) have become important\nbuilding blocks of complex Bayesian nonparametric models. They have been\nsuccessfully used in various applications such as clustering, density\nestimation, latent feature models, survival analysis or network science.\nPopular infinite-activity CRMs include the (generalized) gamma process and the\n(stable) beta process. However, except in some specific cases, exact simulation\nor scalable inference with these models is challenging and finite-dimensional\napproximations are often considered. In this work, we propose a general and\nunified framework to derive both series representations and finite-dimensional\napproximations of CRMs. Our framework can be seen as an extension of\nconstructions based on size-biased sampling of Poisson point process\n[Perman1992]. It includes as special cases several known series representations\nas well as novel ones. In particular, we show that one can get novel series\nrepresentations for the generalized gamma process and the stable beta process.\nWe also provide some analysis of the truncation error.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 05:50:25 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Lee", "Juho", ""], ["Miscouridou", "Xenia", ""], ["Caron", "Fran\u00e7ois", ""]]}, {"id": "1905.10764", "submitter": "Nicole M\\\"ucke", "authors": "Gilles Blanchard, Peter Math\\'e and Nicole M\\\"ucke", "title": "Lepskii Principle in Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the setting of supervised learning using reproducing kernel methods, we\npropose a data-dependent regularization parameter selection rule that is\nadaptive to the unknown regularity of the target function and is optimal both\nfor the least-square (prediction) error and for the reproducing kernel Hilbert\nspace (reconstruction) norm error. It is based on a modified Lepskii balancing\nprinciple using a varying family of norms.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 08:46:58 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Blanchard", "Gilles", ""], ["Math\u00e9", "Peter", ""], ["M\u00fccke", "Nicole", ""]]}, {"id": "1905.10859", "submitter": "Yixin Wang", "authors": "Yixin Wang, David M. Blei", "title": "Variational Bayes under Model Misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes (VB) is a scalable alternative to Markov chain Monte Carlo\n(MCMC) for Bayesian posterior inference. Though popular, VB comes with few\ntheoretical guarantees, most of which focus on well-specified models. However,\nmodels are rarely well-specified in practice. In this work, we study VB under\nmodel misspecification. We prove the VB posterior is asymptotically normal and\ncenters at the value that minimizes the Kullback-Leibler (KL) divergence to the\ntrue data-generating distribution. Moreover, the VB posterior mean centers at\nthe same value and is also asymptotically normal. These results generalize the\nvariational Bernstein--von Mises theorem [29] to misspecified models. As a\nconsequence of these results, we find that the model misspecification error\ndominates the variational approximation error in VB posterior predictive\ndistributions. It explains the widely observed phenomenon that VB achieves\ncomparable predictive accuracy with MCMC even though VB uses an approximating\nfamily. As illustrations, we study VB under three forms of model\nmisspecification, ranging from model over-/under-dispersion to latent\ndimensionality misspecification. We conduct two simulation studies that\ndemonstrate the theoretical results.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 19:04:21 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 22:24:55 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Wang", "Yixin", ""], ["Blei", "David M.", ""]]}, {"id": "1905.10888", "submitter": "Huijie Feng", "authors": "Huijie Feng, Yang Ning, Jiwei Zhao", "title": "Nonregular and Minimax Estimation of Individualized Thresholds in High\n  Dimension with Binary Responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a large number of covariates $Z$, we consider the estimation of a\nhigh-dimensional parameter $\\theta$ in an individualized linear threshold\n$\\theta^T Z$ for a continuous variable $X$, which minimizes the disagreement\nbetween $\\text{sign}(X-\\theta^TZ)$ and a binary response $Y$. While the problem\ncan be formulated into the M-estimation framework, minimizing the corresponding\nempirical risk function is computationally intractable due to discontinuity of\nthe sign function. Moreover, estimating $\\theta$ even in the fixed-dimensional\nsetting is known as a nonregular problem leading to nonstandard asymptotic\ntheory. To tackle the computational and theoretical challenges in the\nestimation of the high-dimensional parameter $\\theta$, we propose an empirical\nrisk minimization approach based on a regularized smoothed loss function. The\nstatistical and computational trade-off of the algorithm is investigated.\nStatistically, we show that the finite sample error bound for estimating\n$\\theta$ in $\\ell_2$ norm is $(s\\log d/n)^{\\beta/(2\\beta+1)}$, where $d$ is the\ndimension of $\\theta$, $s$ is the sparsity level, $n$ is the sample size and\n$\\beta$ is the smoothness of the conditional density of $X$ given the response\n$Y$ and the covariates $Z$. The convergence rate is nonstandard and slower than\nthat in the classical Lasso problems. Furthermore, we prove that the resulting\nestimator is minimax rate optimal up to a logarithmic factor. The Lepski's\nmethod is developed to achieve the adaption to the unknown sparsity $s$ and\nsmoothness $\\beta$. Computationally, an efficient path-following algorithm is\nproposed to compute the solution path. We show that this algorithm achieves\ngeometric rate of convergence for computing the whole path. Finally, we\nevaluate the finite sample performance of the proposed estimator in simulation\nstudies and a real data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 21:44:07 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Feng", "Huijie", ""], ["Ning", "Yang", ""], ["Zhao", "Jiwei", ""]]}, {"id": "1905.11014", "submitter": "Qiang Sun", "authors": "Qiang Sun", "title": "Gaussian Approximations for Maxima of Random Vectors under\n  $(2+\\iota)$-th Moments", "comments": "6 pages, short note", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a Gaussian approximation result for the maximum of a sum of random\nvectors under $(2+\\iota)$-th moments. Our main theorem is abstract and\nnonasymptotic, and can be applied to a variety of statistical learning\nproblems. The proof uses the Lindeberg telescopic sum device along with some\nother newly developed technical results.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 07:31:09 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Sun", "Qiang", ""]]}, {"id": "1905.11033", "submitter": "Alexander Schnurr", "authors": "Annika Betken, Jannis Buchsteiner, Herold Dehling, Ines M\\\"unker,\n  Alexander Schnurr and Jeannette H.C. Woerner", "title": "Ordinal Patterns in Long-Range Dependent Time Series", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the ordinal structure of long-range dependent time series. To this\nend, we use so called ordinal patterns which describe the relative position of\nconsecutive data points. We provide two estimators for the probabilities of\nordinal patterns and prove limit theorems in different settings, namely\nstationarity and (less restrictive) stationary increments. In the second\nsetting, we encounter a Rosenblatt distribution in the limit. We prove more\ngeneral limit theorems for functions with Hermite rank 1 and 2. We derive the\nlimit distribution for an estimation of the Hurst parameter $H$ if it is higher\nthan 3/4. Thus, our theorems complement results for lower values of $H$ which\ncan be found in the literature. Finally, we provide some simulations that\nillustrate our theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 08:19:12 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2020 08:35:52 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Betken", "Annika", ""], ["Buchsteiner", "Jannis", ""], ["Dehling", "Herold", ""], ["M\u00fcnker", "Ines", ""], ["Schnurr", "Alexander", ""], ["Woerner", "Jeannette H. C.", ""]]}, {"id": "1905.11053", "submitter": "Carl Graham", "authors": "Carl Graham (CMAP)", "title": "Regenerative properties of the linear hawkes process with unbounded\n  memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove regenerative properties for the linear Hawkes process under minimal\nassumptions on the transfer function, which may have unbounded support. These\nresults are applicable to sliding window statistical estimators. We exploit\nindependence in the Poisson cluster point process decomposition, and the\nregeneration times are not stopping times for the Hawkes process. The\nregeneration time is interpreted as the renewal time at zero of a M/G/infinity\nqueue, which yields a formula for its Laplace transform. When the transfer\nfunction admits some exponential moments, we stochastically dominate the\ncluster length by exponential random variables with parameters expressed in\nterms of these moments. This yields explicit bounds on the Laplace transform of\nthe regeneration time in terms of simple integrals or special functions\nyielding an explicit negative upper-bound on its abscissa of convergence. These\nregenerative results allow, e.g., to systematically derive long-time asymptotic\nresults in view of statistical applications. This is illustrated on a\nconcentration inequality previously obtained with coauthors.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 08:58:11 GMT"}, {"version": "v2", "created": "Thu, 6 Jun 2019 08:20:40 GMT"}], "update_date": "2019-06-07", "authors_parsed": [["Graham", "Carl", "", "CMAP"]]}, {"id": "1905.11067", "submitter": "Kazuto Fukuchi", "authors": "Kazuto Fukuchi, Chia-Mu Yu, Arashi Haishima, Jun Sakuma", "title": "Locally Differentially Private Minimum Finding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a problem of finding the minimum, in which each user has a\nreal value and we want to estimate the minimum of these values under the local\ndifferential privacy constraint. We reveal that this problem is fundamentally\ndifficult, and we cannot construct a mechanism that is consistent in the worst\ncase. Instead of considering the worst case, we aim to construct a private\nmechanism whose error rate is adaptive to the easiness of estimation of the\nminimum. As a measure of easiness, we introduce a parameter $\\alpha$ that\ncharacterizes the fatness of the minimum-side tail of the user data\ndistribution. As a result, we reveal that the mechanism can achieve\n$O((\\ln^6N/\\epsilon^2N)^{1/2\\alpha})$ error without knowledge of $\\alpha$ and\nthe error rate is near-optimal in the sense that any mechanism incurs\n$\\Omega((1/\\epsilon^2N)^{1/2\\alpha})$ error. Furthermore, we demonstrate that\nour mechanism outperforms a naive mechanism by empirical evaluations on\nsynthetic datasets. Also, we conducted experiments on the MovieLens dataset and\na purchase history dataset and demonstrate that our algorithm achieves\n$\\tilde{O}((1/N)^{1/2\\alpha})$ error adaptively to $\\alpha$.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 09:17:04 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Fukuchi", "Kazuto", ""], ["Yu", "Chia-Mu", ""], ["Haishima", "Arashi", ""], ["Sakuma", "Jun", ""]]}, {"id": "1905.11255", "submitter": "Ingmar Schuster", "authors": "Ingmar Schuster and Mattes Mollenhauer and Stefan Klus and Krikamol\n  Muandet", "title": "Kernel Conditional Density Operators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel conditional density estimation model termed the\nconditional density operator (CDO). It naturally captures multivariate,\nmultimodal output densities and shows performance that is competitive with\nrecent neural conditional density models and Gaussian processes. The proposed\nmodel is based on a novel approach to the reconstruction of probability\ndensities from their kernel mean embeddings by drawing connections to\nestimation of Radon-Nikodym derivatives in the reproducing kernel Hilbert space\n(RKHS). We prove finite sample bounds for the estimation error in a standard\ndensity reconstruction scenario, independent of problem dimensionality.\nInterestingly, when a kernel is used that is also a probability density, the\nCDO allows us to both evaluate and sample the output density efficiently. We\ndemonstrate the versatility and performance of the proposed model on both\nsynthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 14:16:55 GMT"}, {"version": "v2", "created": "Tue, 29 Oct 2019 12:39:41 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Schuster", "Ingmar", ""], ["Mollenhauer", "Mattes", ""], ["Klus", "Stefan", ""], ["Muandet", "Krikamol", ""]]}, {"id": "1905.11300", "submitter": "Jaffer Zaidi", "authors": "Jaffer M. Zaidi, Eric J. Tchetgen Tchetgen, and Tyler J. VanderWeele", "title": "Quantifying and Detecting Individual Level `Always Survivor' Causal\n  Effects Under `Truncation by Death' and Censoring Through Time", "comments": "Please email the first author if you want the online supplements. R\n  code is also available on request", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The analysis of causal effects when the outcome of interest is possibly\ntruncated by death has a long history in statistics and causal inference. The\nsurvivor average causal effect is commonly identified with more assumptions\nthan those guaranteed by the design of a randomized clinical trial or using\nsensitivity analysis. This paper demonstrates that individual level causal\neffects in the `always survivor' principal stratum can be identified with no\nstronger identification assumptions than randomization. We illustrate the\npractical utility of our methods using data from a clinical trial on patients\nwith prostate cancer. Our methodology is the first and, as of yet, only\nproposed procedure that enables detecting causal effects in the presence of\ntruncation by death using only the assumptions that are guaranteed by design of\nthe clinical trial. This methodology is applicable to all types of outcomes.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 15:36:37 GMT"}, {"version": "v2", "created": "Fri, 28 Jun 2019 22:26:45 GMT"}, {"version": "v3", "created": "Fri, 20 Mar 2020 21:38:33 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Zaidi", "Jaffer M.", ""], ["Tchetgen", "Eric J. Tchetgen", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "1905.11379", "submitter": "Souvik Roy", "authors": "Suvra Pal and Souvik Roy", "title": "A New Non-Linear Conjugate Gradient Algorithm for Destructive Cure Rate\n  Model and a Simulation Study: Illustration with Negative Binomial Competing\n  Risks", "comments": "arXiv admin note: text overlap with arXiv:1905.05963", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new estimation methodology based on a projected\nnon-linear conjugate gradient (PNCG) algorithm with an efficient line search\ntechnique. We develop a general PNCG algorithm for a survival model\nincorporating a proportion cure under a competing risks setup, where the\ninitial number of competing risks are exposed to elimination after an initial\ntreatment (known as destruction). In the literature, expectation maximization\n(EM) algorithm has been widely used for such a model to estimate the model\nparameters. Through an extensive Monte Carlo simulation study, we compare the\nperformance of our proposed PNCG with that of the EM algorithm and show the\nadvantages of our proposed method. Through simulation, we also show the\nadvantages of our proposed methodology over other optimization algorithms\n(including other conjugate gradient type methods) readily available as R\nsoftware packages. To show these we assume the initial number of competing\nrisks to follow a negative binomial distribution although our general algorithm\nallows one to work with any competing risks distribution. Finally, we apply our\nproposed algorithm to analyze a well-known melanoma data.\n", "versions": [{"version": "v1", "created": "Sat, 25 May 2019 07:32:51 GMT"}, {"version": "v2", "created": "Wed, 19 Feb 2020 19:42:06 GMT"}, {"version": "v3", "created": "Sat, 4 Jul 2020 20:41:14 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Pal", "Suvra", ""], ["Roy", "Souvik", ""]]}, {"id": "1905.11386", "submitter": "Yixin Wang", "authors": "Yixin Wang, Jos\\'e R. Zubizarreta", "title": "Large Sample Properties of Matching for Balance", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching methods are widely used for causal inference in observational\nstudies. Among them, nearest neighbor matching is arguably the most popular.\nHowever, nearest neighbor matching does not generally yield an average\ntreatment effect estimator that is $\\sqrt{n}$-consistent (Abadie and Imbens,\n2006). Are matching methods not $\\sqrt{n}$-consistent in general? In this\npaper, we study a recent class of matching methods that use integer programming\nto directly target aggregate covariate balance as opposed to finding close\nneighbor matches. We show that under standard conditions these methods can\nyield simple estimators that are $\\sqrt{n}$-consistent and asymptotically\noptimal provided that the integer program admits a solution.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2019 21:31:29 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Wang", "Yixin", ""], ["Zubizarreta", "Jos\u00e9 R.", ""]]}, {"id": "1905.11397", "submitter": "Jaehyeok Shin", "authors": "Jaehyeok Shin, Aaditya Ramdas, Alessandro Rinaldo", "title": "Are sample means in multi-armed bandits positively or negatively biased?", "comments": "21 pages. Advances in Neural Information Processing Systems 32\n  (NeurIPS 2019, Spotlight Presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that in stochastic multi-armed bandits (MAB), the sample\nmean of an arm is typically not an unbiased estimator of its true mean. In this\npaper, we decouple three different sources of this selection bias: adaptive\n\\emph{sampling} of arms, adaptive \\emph{stopping} of the experiment, and\nadaptively \\emph{choosing} which arm to study. Through a new notion called\n``optimism'' that captures certain natural monotonic behaviors of algorithms,\nwe provide a clean and unified analysis of how optimistic rules affect the sign\nof the bias. The main takeaway message is that optimistic sampling induces a\nnegative bias, but optimistic stopping and optimistic choosing both induce a\npositive bias. These results are derived in a general stochastic MAB setup that\nis entirely agnostic to the final aim of the experiment (regret minimization or\nbest-arm identification or anything else). We provide examples of optimistic\nrules of each type, demonstrate that simulations confirm our theoretical\npredictions, and pose some natural but hard open problems.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 14:50:16 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 23:02:19 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Shin", "Jaehyeok", ""], ["Ramdas", "Aaditya", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1905.11434", "submitter": "Jaffer Zaidi", "authors": "Jaffer M. Zaidi, Tyler J. VanderWeele", "title": "On the identification of individual principal stratum direct, natural\n  direct and pleiotropic effects without cross world independence assumptions", "comments": "Email the first author for the online supplement. Scandinavian\n  Journal of Statistics, 2020", "journal-ref": null, "doi": "10.1002/sjos.12464", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The analysis of natural direct and principal stratum direct effects has a\ncontroversial history in statistics and causal inference as these effects are\ncommonly identified with either untestable cross world independence or\ngraphical assumptions. This paper demonstrates that the presence of individual\nlevel natural direct and principal stratum direct effects can be identified\nwithout cross world independence assumptions. We also define a new type of\ncausal effect, called pleiotropy, that is of interest in genomics, and provide\nempirical conditions to detect such an effect as well. Our results are\napplicable for all types of distributions concerning the mediator and outcome.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 18:13:10 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 20:53:35 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 13:06:45 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Zaidi", "Jaffer M.", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "1905.11448", "submitter": "HongVan Le", "authors": "J\\\"urgen Jost, H\\^ong V\\^an L\\^e, and Tat Dat Tran", "title": "Probabilistic morphisms and Bayesian nonparametrics", "comments": "Final version: 22 p. Version 2: minor corrections, Proposition 2.10\n  added, improved presentation, 39 p., version 1: 38 p. comments welcome!", "journal-ref": "Eur. Phys. J. Plus (2021) 136:441", "doi": "10.1140/epjp/s13360-021-01427-7", "report-no": null, "categories": "math.ST math.CT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a functorial language of probabilistic morphisms and\napply it to some basic problems in Bayesian nonparametrics. First we extend and\nunify the Kleisli category of probabilistic morphisms proposed by Lawvere and\nGiry with the category of statistical models proposed by Chentsov and\nMorse-Sacksteder. Then we introduce the notion of a Bayesian statistical model\nthat formalizes the notion of a parameter space with a given prior distribution\nin Bayesian statistics. {We revisit the existence of a posterior distribution,\nusing probabilistic morphisms}. In particular, we give an explicit formula for\nposterior distributions of the Bayesian statistical model, assuming that the\nunderlying parameter space is a Souslin space and the sample space is a subset\nin a complete connected finite dimensional Riemannian manifold. Then we give a\nnew proof of the existence of Dirichlet measures over any measurable space\nusing a functorial property of the Dirichlet map constructed by Sethuraman.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 18:59:05 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 07:46:07 GMT"}, {"version": "v3", "created": "Sun, 21 Mar 2021 20:37:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jost", "J\u00fcrgen", ""], ["L\u00ea", "H\u00f4ng V\u00e2n", ""], ["Tran", "Tat Dat", ""]]}, {"id": "1905.11465", "submitter": "Jinjin Tian", "authors": "Jinjin Tian, Aaditya Ramdas", "title": "ADDIS: an adaptive discarding algorithm for online FDR control with\n  conservative nulls", "comments": "Accepted to Neurips 2019. Corrected some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major internet companies routinely perform tens of thousands of A/B tests\neach year. Such large-scale sequential experimentation has resulted in a recent\nspurt of new algorithms that can provably control the false discovery rate\n(FDR) in a fully online fashion. However, current state-of-the-art adaptive\nalgorithms can suffer from a significant loss in power if null p-values are\nconservative (stochastically larger than the uniform distribution), a situation\nthat occurs frequently in practice. In this work, we introduce a new adaptive\ndiscarding method called ADDIS that provably controls the FDR and achieves the\nbest of both worlds: it enjoys appreciable power increase over all existing\nmethods if nulls are conservative (the practical case), and rarely loses power\nif nulls are exactly uniformly distributed (the ideal case). We provide several\npractical insights on robust choices of tuning parameters, and extend the idea\nto asynchronous and offline settings as well.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2019 19:27:01 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 03:59:58 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 05:12:30 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Tian", "Jinjin", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1905.11549", "submitter": "Xin Zhang", "authors": "Xin Zhang, Jia Liu, Zhengyuan Zhu", "title": "Distributed Linear Model Clustering over Networks: A Tree-Based\n  Fused-Lasso ADMM Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC cs.LG cs.NI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider to improve the model estimation efficiency by\naggregating the neighbors' information as well as identify the subgroup\nmembership for each node in the network. A tree-based $l_1$ penalty is proposed\nto save the computation and communication cost. We design a decentralized\ngeneralized alternating direction method of multiplier algorithm for solving\nthe objective function in parallel. The theoretical properties are derived to\nguarantee both the model consistency and the algorithm convergence. Thorough\nnumerical experiments are also conducted to back up our theory, which also show\nthat our approach outperforms in the aspects of the estimation accuracy,\ncomputation speed and communication cost.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 00:40:01 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Zhang", "Xin", ""], ["Liu", "Jia", ""], ["Zhu", "Zhengyuan", ""]]}, {"id": "1905.11768", "submitter": "Adil Salim", "authors": "Adil Salim, Dmitry Kovalev, Peter Richt\\'arik", "title": "Stochastic Proximal Langevin Algorithm: Potential Splitting and\n  Nonasymptotic Rates", "comments": null, "journal-ref": "Neurips 2019 (Spotlight)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm---Stochastic Proximal Langevin Algorithm\n(SPLA)---for sampling from a log concave distribution. Our method is a\ngeneralization of the Langevin algorithm to potentials expressed as the sum of\none stochastic smooth term and multiple stochastic nonsmooth terms. In each\niteration, our splitting technique only requires access to a stochastic\ngradient of the smooth term and a stochastic proximal operator for each of the\nnonsmooth terms. We establish nonasymptotic sublinear and linear convergence\nrates under convexity and strong convexity of the smooth term, respectively,\nexpressed in terms of the KL divergence and Wasserstein distance. We illustrate\nthe efficiency of our sampling technique through numerical simulations on a\nBayesian learning task.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 12:28:31 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 14:09:42 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Salim", "Adil", ""], ["Kovalev", "Dmitry", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "1905.11882", "submitter": "Gonzalo Mena E", "authors": "Gonzalo Mena, Jonathan Weed", "title": "Statistical bounds for entropic optimal transport: sample complexity and\n  the central limit theorem", "comments": "Under review. 23 pages, 2 figures. Version 2 fixes minor typos and\n  errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove several fundamental statistical bounds for entropic OT with the\nsquared Euclidean cost between subgaussian probability measures in arbitrary\ndimension. First, through a new sample complexity result we establish the rate\nof convergence of entropic OT for empirical measures. Our analysis improves\nexponentially on the bound of Genevay et al. (2019) and extends their work to\nunbounded measures. Second, we establish a central limit theorem for entropic\nOT, based on techniques developed by Del Barrio and Loubes (2019). Previously,\nsuch a result was only known for finite metric spaces. As an application of our\nresults, we develop and analyze a new technique for estimating the entropy of a\nrandom variable corrupted by gaussian noise.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 15:23:37 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 17:42:36 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Mena", "Gonzalo", ""], ["Weed", "Jonathan", ""]]}, {"id": "1905.12062", "submitter": "Florian Puchhammer", "authors": "Amal Ben Abdellah, Pierre L'Ecuyer, Florian Puchhammer", "title": "Array-RQMC for option pricing under stochastic volatility models", "comments": "12 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Array-RQMC has been proposed as a way to effectively apply randomized\nquasi-Monte Carlo (RQMC) when simulating a Markov chain over a large number of\nsteps to estimate an expected cost or reward. The method can be very effective\nwhen the state of the chain has low dimension. For pricing an Asian option\nunder an ordinary geometric Brownian motion model, for example, Array-RQMC\nreduces the variance by huge factors. In this paper, we show how to apply this\nmethod and we study its effectiveness in case the underlying process has\nstochastic volatility. We show that Array-RQMC can also work very well for\nthese models, even if it requires RQMC points in larger dimension. We examine\nin particular the variance-gamma, Heston, and Ornstein-Uhlenbeck stochastic\nvolatility models, and we provide numerical results.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2019 20:05:59 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Abdellah", "Amal Ben", ""], ["L'Ecuyer", "Pierre", ""], ["Puchhammer", "Florian", ""]]}, {"id": "1905.12231", "submitter": "Jun Yan", "authors": "Jose Blanchet, Peter W. Glynn, Jun Yan, Zhengqing Zhou", "title": "Multivariate Distributionally Robust Convex Regression under Absolute\n  Error Loss", "comments": "v3. 17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel non-parametric multidimensional convex regression\nestimator which is designed to be robust to adversarial perturbations in the\nempirical measure. We minimize over convex functions the maximum (over\nWasserstein perturbations of the empirical measure) of the absolute regression\nerrors. The inner maximization is solved in closed form resulting in a\nregularization penalty involves the norm of the gradient. We show consistency\nof our estimator and a rate of convergence of order $ \\widetilde{O}\\left(\nn^{-1/d}\\right) $, matching the bounds of alternative estimators based on\nsquare-loss minimization. Contrary to all of the existing results, our\nconvergence rates hold without imposing compactness on the underlying domain\nand with no a priori bounds on the underlying convex function or its gradient\nnorm.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 05:50:55 GMT"}, {"version": "v2", "created": "Sat, 26 Oct 2019 07:24:48 GMT"}, {"version": "v3", "created": "Sun, 26 Jul 2020 02:53:48 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Blanchet", "Jose", ""], ["Glynn", "Peter W.", ""], ["Yan", "Jun", ""], ["Zhou", "Zhengqing", ""]]}, {"id": "1905.12347", "submitter": "Chen Amiraz", "authors": "Chen Amiraz, Robert Krauthgamer and Boaz Nadler", "title": "Tight Recovery Guarantees for Orthogonal Matching Pursuit Under Gaussian\n  Noise", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal Matching pursuit (OMP) is a popular algorithm to estimate an\nunknown sparse vector from multiple linear measurements of it. Assuming exact\nsparsity and that the measurements are corrupted by additive Gaussian noise,\nthe success of OMP is often formulated as exactly recovering the support of the\nsparse vector. Several authors derived a sufficient condition for exact support\nrecovery by OMP with high probability depending on the signal-to-noise ratio,\ndefined as the magnitude of the smallest non-zero coefficient of the vector\ndivided by the noise level. We make two contributions. First, we derive a\nslightly sharper sufficient condition for two variants of OMP, in which either\nthe sparsity level or the noise level is known. Next, we show that this sharper\nsufficient condition is tight, in the following sense: for a wide range of\nproblem parameters, there exist a dictionary of linear measurements and a\nsparse vector with a signal-to-noise ratio slightly below that of the\nsufficient condition, for which with high probability OMP fails to recover its\nsupport. Finally, we present simulations which illustrate that our condition is\ntight for a much broader range of dictionaries.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 11:52:28 GMT"}, {"version": "v2", "created": "Mon, 10 Jun 2019 07:23:01 GMT"}, {"version": "v3", "created": "Thu, 6 Aug 2020 14:06:37 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Amiraz", "Chen", ""], ["Krauthgamer", "Robert", ""], ["Nadler", "Boaz", ""]]}, {"id": "1905.12385", "submitter": "Benjamin Aubin", "authors": "Benjamin Aubin, Bruno Loureiro, Antoine Maillard, Florent Krzakala,\n  Lenka Zdeborov\\'a", "title": "The spiked matrix model with generative priors", "comments": "12 + 56, 8 figures, v2 lighter jpeg figures", "journal-ref": "Advances in Neural Information Processing Systems, pp. 8364-8375.\n  2019", "doi": null, "report-no": null, "categories": "math.ST cs.LG eess.SP math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a low-dimensional parametrization of signals is a generic and powerful\nway to enhance performance in signal processing and statistical inference. A\nvery popular and widely explored type of dimensionality reduction is sparsity;\nanother type is generative modelling of signal distributions. Generative models\nbased on neural networks, such as GANs or variational auto-encoders, are\nparticularly performant and are gaining on applicability. In this paper we\nstudy spiked matrix models, where a low-rank matrix is observed through a noisy\nchannel. This problem with sparse structure of the spikes has attracted broad\nattention in the past literature. Here, we replace the sparsity assumption by\ngenerative modelling, and investigate the consequences on statistical and\nalgorithmic properties. We analyze the Bayes-optimal performance under specific\ngenerative models for the spike. In contrast with the sparsity assumption, we\ndo not observe regions of parameters where statistical performance is superior\nto the best known algorithmic performance. We show that in the analyzed cases\nthe approximate message passing algorithm is able to reach optimal performance.\nWe also design enhanced spectral algorithms and analyze their performance and\nthresholds using random matrix theory, showing their superiority to the\nclassical principal component analysis. We complement our theoretical results\nby illustrating the performance of the spectral algorithms when the spikes come\nfrom real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 12:47:05 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 15:03:52 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Aubin", "Benjamin", ""], ["Loureiro", "Bruno", ""], ["Maillard", "Antoine", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1905.12442", "submitter": "Yariv Aizenbud", "authors": "Yariv Aizenbud and Boris Landa and Yoel Shkolnisky", "title": "Rank-one Multi-Reference Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there is a growing need for processing methods aimed at\nextracting useful information from large datasets. In many cases the challenge\nis to discover a low-dimensional structure in the data, often concealed by the\nexistence of nuisance parameters and noise. Motivated by such challenges, we\nconsider the problem of estimating a signal from its scaled, cyclically-shifted\nand noisy observations. We focus on the particularly challenging regime of low\nsignal-to-noise ratio (SNR), where different observations cannot be\nshift-aligned. We show that an accurate estimation of the signal from its noisy\nobservations is possible, and derive a procedure which is proved to\nconsistently estimate the signal. The asymptotic sample complexity (the number\nof observations required to recover the signal) of the procedure is\n$1/\\operatorname{SNR}^4$. Additionally, we propose a procedure which is\nexperimentally shown to improve the sample complexity by a factor equal to the\nsignal's length. Finally, we present numerical experiments which demonstrate\nthe performance of our algorithms, and corroborate our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 13:34:29 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 07:45:55 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Aizenbud", "Yariv", ""], ["Landa", "Boris", ""], ["Shkolnisky", "Yoel", ""]]}, {"id": "1905.12466", "submitter": "Hideatsu Tsukahara", "authors": "Anna Kiriliouk, Johan Segers and Hideatsu Tsukahara", "title": "Resampling Procedures with Empirical Beta Copulas", "comments": "22 pages, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The empirical beta copula is a simple but effective smoother of the empirical\ncopula. Because it is a genuine copula, from which, moreover, it is\nparticularly easy to sample, it is reasonable to expect that resampling\nprocedures based on the empirical beta copula are expedient and accurate. In\nthis paper, after reviewing the literature on some bootstrap approximations for\nthe empirical copula process, we first show the asymptotic equivalence of\nseveral bootstrapped processes related to the empirical copula and empirical\nbeta copula. Then we investigate the finite-sample properties of resampling\nschemes based on the empirical (beta) copula by Monte Carlo simulation. More\nspecifically, we consider interval estimation for some functionals such as rank\ncorrelation coefficients and dependence parameters of several well-known\nfamilies of copulas, constructing confidence intervals by several methods and\ncomparing their accuracy and efficiency. We also compute the actual size and\npower of symmetry tests based on several resampling schemes for the empirical\ncopula and empirical beta copula.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 14:02:12 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 11:49:44 GMT"}, {"version": "v3", "created": "Sat, 15 Feb 2020 13:35:01 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Kiriliouk", "Anna", ""], ["Segers", "Johan", ""], ["Tsukahara", "Hideatsu", ""]]}, {"id": "1905.12517", "submitter": "Pierre C. Bellec", "authors": "Pierre C Bellec and Dana Yang", "title": "The cost-free nature of optimally tuning Tikhonov regularizers and other\n  ordered smoothers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of selecting the best estimator among a family of\nTikhonov regularized estimators, or, alternatively, to select a linear\ncombination of these regularizers that is as good as the best regularizer in\nthe family. Our theory reveals that if the Tikhonov regularizers share the same\npenalty matrix with different tuning parameters, a convex procedure based on\n$Q$-aggregation achieves the mean square error of the best estimator, up to a\nsmall error term no larger than $C\\sigma^2$, where $\\sigma^2$ is the noise\nlevel and $C>0$ is an absolute constant. Remarkably, the error term does not\ndepend on the penalty matrix or the number of estimators as long as they share\nthe same penalty matrix, i.e., it applies to any grid of tuning parameters, no\nmatter how large the cardinality of the grid is. This reveals the surprising\n\"cost-free\" nature of optimally tuning Tikhonov regularizers, in striking\ncontrast with the existing literature on aggregation of estimators where one\ntypically has to pay a cost of $\\sigma^2\\log(M)$ where $M$ is the number of\nestimators in the family. The result holds, more generally, for any family of\nordered linear smoothers. This encompasses Ridge regression as well as\nPrincipal Component Regression. The result is extended to the problem of tuning\nTikhonov regularizers with different penalty matrices.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 15:15:52 GMT"}], "update_date": "2019-05-30", "authors_parsed": [["Bellec", "Pierre C", ""], ["Yang", "Dana", ""]]}, {"id": "1905.12600", "submitter": "Hanie Sedghi", "authors": "Philip M. Long and Hanie Sedghi", "title": "Generalization bounds for deep convolutional neural networks", "comments": "Published as a conference paper at ICLR 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove bounds on the generalization error of convolutional networks. The\nbounds are in terms of the training loss, the number of parameters, the\nLipschitz constant of the loss and the distance from the weights to the initial\nweights. They are independent of the number of pixels in the input, and the\nheight and width of hidden feature maps. We present experiments using CIFAR-10\nwith varying hyperparameters of a deep convolutional network, comparing our\nbounds with practical generalization gaps.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 17:20:36 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 16:54:56 GMT"}, {"version": "v3", "created": "Fri, 26 Jul 2019 16:39:20 GMT"}, {"version": "v4", "created": "Fri, 27 Sep 2019 01:55:42 GMT"}, {"version": "v5", "created": "Sun, 9 Feb 2020 20:00:07 GMT"}, {"version": "v6", "created": "Wed, 8 Apr 2020 05:10:39 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Long", "Philip M.", ""], ["Sedghi", "Hanie", ""]]}, {"id": "1905.12718", "submitter": "Davy Paindaveine", "authors": "Abdelaati Daouia and Davy Paindaveine", "title": "From Halfspace M-depth to Multiple-output Expectile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the renewed interest in the Newey and Powell (1987) concept of\nexpectiles in fields such as econometrics, risk management, and extreme value\ntheory, expectile regression---or, more generally, M-quantile\nregression---unfortunately remains limited to single-output problems. To\nimprove on this, we introduce hyperplane-valued multivariate M-quantiles that\nshow strong advantages, for instance in terms of equivariance, over the various\npoint-valued multivariate M-quantiles available in the literature. Like their\ncompetitors, our multivariate M-quantiles are directional in nature and provide\ncentrality regions when all directions are considered. These regions define a\nnew statistical depth, the halfspace M-depth, whose deepest point, in the\nexpectile case, is the mean vector. Remarkably, the halfspace M-depth can\nalternatively be obtained by substituting, in the celebrated Tukey (1975)\nhalfspace depth, M-quantile outlyingness for standard quantile outlyingness,\nwhich supports a posteriori the claim that our multivariate M-quantile concept\nis the natural one. We investigate thoroughly the properties of the proposed\nmultivariate M-quantiles, of halfspace M-depth, and of the corresponding\nregions. Since our original motivation was to define multiple-output expectile\nregression methods, we further focus on the expectile case. We show in\nparticular that expectile depth is smoother than the Tukey depth and enjoys\ninteresting monotonicity properties that are extremely promising for\ncomputational purposes. Unlike their quantile analogs, the proposed\nmultivariate expectiles also satisfy the coherency axioms of multivariate risk\nmeasures. Finally, we show that our multivariate expectiles indeed allow\nperforming multiple-output expectile regression, which is illustrated on\nsimulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 20:50:41 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Daouia", "Abdelaati", ""], ["Paindaveine", "Davy", ""]]}, {"id": "1905.12823", "submitter": "Qiyang Han", "authors": "Qiyang Han", "title": "Set structured global empirical risk minimizers are rate optimal in\n  general dimensions", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entropy integrals are widely used as a powerful empirical process tool to\nobtain upper bounds for the rates of convergence of global empirical risk\nminimizers (ERMs), in standard settings such as density estimation and\nregression. The upper bound for the convergence rates thus obtained typically\nmatches the minimax lower bound when the entropy integral converges, but admits\na strict gap compared to the lower bound when it diverges. Birg\\'e and Massart\n[BM93] provided a striking example showing that such a gap is real with the\nentropy structure alone: for a variant of the natural H\\\"older class with low\nregularity, the global ERM actually converges at the rate predicted by the\nentropy integral that substantially deviates from the lower bound. The\ncounter-example has spawned a long-standing negative position on the use of\nglobal ERMs in the regime where the entropy integral diverges, as they are\nheuristically believed to converge at a sub-optimal rate in a variety of\nmodels.\n  The present paper demonstrates that this gap can be closed if the models\nadmit certain degree of `set structures' in addition to the entropy structure.\nIn other words, the global ERMs in such set structured models will indeed be\nrate-optimal, matching the lower bound even when the entropy integral diverges.\nThe models with set structures we investigate include (i) image and edge\nestimation, (ii) binary classification, (iii) multiple isotonic regression,\n(iv) $s$-concave density estimation, all in general dimensions when the entropy\nintegral diverges. Here set structures are interpreted broadly in the sense\nthat the complexity of the underlying models can be essentially captured by the\nsize of the empirical process over certain class of measurable sets, for which\nmatching upper and lower bounds are obtained to facilitate the derivation of\nsharp convergence rates for the associated global ERMs.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 02:01:27 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 04:46:07 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Han", "Qiyang", ""]]}, {"id": "1905.12824", "submitter": "Qiyang Han", "authors": "Qiyang Han, Jon A. Wellner", "title": "Complex sampling designs: uniform limit theorems and applications", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a general approach to proving global and local\nuniform limit theorems for the Horvitz-Thompson empirical process arising from\ncomplex sampling designs. Global theorems such as Glivenko-Cantelli and Donsker\ntheorems, and local theorems such as local asymptotic modulus and related\nratio-type limit theorems are proved for both the Horvitz-Thompson empirical\nprocess, and its calibrated version. Limit theorems of other variants and their\nconditional versions are also established. Our approach reveals an interesting\nfeature: the problem of deriving uniform limit theorems for the\nHorvitz-Thompson empirical process is essentially no harder than the problem of\nestablishing the corresponding finite-dimensional limit theorems. These global\nand local uniform limit theorems are then applied to important statistical\nproblems including (i) $M$-estimation (ii) $Z$-estimation (iii) frequentist\ntheory of Bayes procedures, all with weighted likelihood, to illustrate their\nwide applicability.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 02:01:46 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Han", "Qiyang", ""], ["Wellner", "Jon A.", ""]]}, {"id": "1905.12825", "submitter": "Qiyang Han", "authors": "Qiyang Han, Cun-Hui Zhang", "title": "Limit distribution theory for block estimators in multiple isotonic\n  regression", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study limit distributions for the tuning-free max-min block estimator\noriginally proposed in [FLN17] in the problem of multiple isotonic regression,\nunder both fixed lattice design and random design settings. We show that, if\nthe regression function $f_0$ admits vanishing derivatives up to order\n$\\alpha_k$ along the $k$-th dimension ($k=1,\\ldots,d$) at a fixed point $x_0\n\\in (0,1)^d$, and the errors have variance $\\sigma^2$, then the max-min block\nestimator $\\hat{f}_n$ satisfies \\begin{align*}\n(n_\\ast/\\sigma^2)^{\\frac{1}{2+\\sum_{k \\in \\mathcal{D}_\\ast}\n\\alpha_k^{-1}}}\\big(\\hat{f}_n(x_0)-f_0(x_0)\\big)\\rightsquigarrow\n\\mathbb{C}(f_0,x_0). \\end{align*} Here $\\mathcal{D}_\\ast, n_\\ast$, depending on\n$\\{\\alpha_k\\}$ and the design points, are the set of all `effective dimensions'\nand the size of `effective samples' that drive the asymptotic limiting\ndistribution, respectively. If furthermore either $\\{\\alpha_k\\}$ are relative\nprimes to each other or all mixed derivatives of $f_0$ of certain critical\norder vanish at $x_0$, then the limiting distribution can be represented as\n$\\mathbb{C}(f_0,x_0) =_d K(f_0,x_0) \\cdot \\mathbb{D}_{\\alpha}$, where\n$K(f_0,x_0)$ is a constant depending on the local structure of the regression\nfunction $f_0$ at $x_0$, and $\\mathbb{D}_{\\alpha}$ is a non-standard limiting\ndistribution generalizing the well-known Chernoff distribution in univariate\nproblems. The above limit theorem is also shown to be optimal both in terms of\nthe local rate of convergence and the dependence on the unknown regression\nfunction whenever such dependence is explicit (i.e. $K(f_0,x_0)$), for the full\nrange of $\\{\\alpha_k\\}$ in a local asymptotic minimax sense.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 02:02:14 GMT"}, {"version": "v2", "created": "Sun, 17 Nov 2019 05:47:42 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Han", "Qiyang", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1905.12852", "submitter": "Ishfaq Shah Syed", "authors": "Anwar Hassan, Ishfaq Shah Ahmad, Peer Bilal Ahmad", "title": "A New Mixed Generalized Negative Binomial Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a generalized version of Negative binomial-beta exponential\ndistribution with five parameters have been introduced. Some interesting\nsubmodels have been derived from it. A comprehensive mathematical treatment of\nproposed distribution is being provided. Various expressions like that of\nmoment generating function, moments are derived. The model parameters are\nestimated by the maximum likelihood method. Finally, the application of\nproposed distribution is carried out on one sample of automobile insurance\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 04:45:29 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Hassan", "Anwar", ""], ["Ahmad", "Ishfaq Shah", ""], ["Ahmad", "Peer Bilal", ""]]}, {"id": "1905.12989", "submitter": "James Murphy", "authors": "Mauro Maggioni and James M. Murphy", "title": "Learning by Active Nonlinear Diffusion", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes an active learning method for high dimensional data,\nbased on intrinsic data geometries learned through diffusion processes on\ngraphs. Diffusion distances are used to parametrize low-dimensional structures\non the dataset, which allow for high-accuracy labelings of the dataset with\nonly a small number of carefully chosen labels. The geometric structure of the\ndata suggests regions that have homogeneous labels, as well as regions with\nhigh label complexity that should be queried for labels. The proposed method\nenjoys theoretical performance guarantees on a general geometric data model, in\nwhich clusters corresponding to semantically meaningful classes are permitted\nto have nonlinear geometries, high ambient dimensionality, and suffer from\nsignificant noise and outlier corruption. The proposed algorithm is implemented\nin a manner that is quasilinear in the number of unlabeled data points, and\nexhibits competitive empirical performance on synthetic datasets and real\nhyperspectral remote sensing images.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 12:05:33 GMT"}], "update_date": "2019-05-31", "authors_parsed": [["Maggioni", "Mauro", ""], ["Murphy", "James M.", ""]]}, {"id": "1905.13060", "submitter": "Xiucai Ding", "authors": "Xiucai Ding and Fan Yang", "title": "Spiked separable covariance matrices and principal components", "comments": "Annals of Statistics (to appear)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of separable sample covariance matrices of the form\n$\\widetilde{\\mathcal{Q}}_1:=\\widetilde A^{1/2} X \\widetilde B X^* \\widetilde\nA^{1/2}.$ Here $\\widetilde{A}$ and $\\widetilde{B}$ are positive definite\nmatrices whose spectrums consist of bulk spectrums plus several spikes, i.e.\nlarger eigenvalues that are separated from the bulks. Conceptually, we call\n$\\widetilde{\\mathcal{Q}}_1$ a \\emph{spiked separable covariance matrix model}.\nOn the one hand, this model includes the spiked covariance matrix as a special\ncase with $\\widetilde{B}=I$. On the other hand, it allows for more general\ncorrelations of datasets. In particular, for spatio-temporal dataset,\n$\\widetilde{A}$ and $\\widetilde{B}$ represent the spatial and temporal\ncorrelations, respectively.\n  In this paper, we study the outlier eigenvalues and eigenvectors, i.e. the\nprincipal components, of the spiked separable covariance model\n$\\widetilde{\\mathcal{Q}}_1$. We prove the convergence of the outlier\neigenvalues $\\widetilde \\lambda_i$ and the generalized components (i.e.\n$\\langle \\mathbf v, \\widetilde{\\mathbf{\\xi}}_i \\rangle$ for any deterministic\nvector $\\mathbf v$) of the outlier eigenvectors $\\widetilde{\\mathbf{\\xi}}_i$\nwith optimal convergence rates. Moreover, we also prove the delocalization of\nthe non-outlier eigenvectors. We state our results in full generality, in the\nsense that they also hold near the so-called BBP transition and for degenerate\noutliers. Our results highlight both the similarity and difference between the\nspiked separable covariance matrix model and the spiked covariance model. In\nparticular, we show that the spikes of both $\\widetilde{A}$ and $\\widetilde{B}$\nwill cause outliers of the eigenvalue spectrum, and the eigenvectors can help\nus to select the outliers that correspond to the spikes of $\\widetilde{A}$ (or\n$\\widetilde{B}$).\n", "versions": [{"version": "v1", "created": "Wed, 29 May 2019 17:28:55 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 03:54:21 GMT"}, {"version": "v3", "created": "Sun, 28 Jun 2020 21:02:47 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Ding", "Xiucai", ""], ["Yang", "Fan", ""]]}, {"id": "1905.13142", "submitter": "Sotirios Sabanis", "authors": "Ngoc Huy Chau, \\'Eric Moulines, Miklos R\\'asonyi, Sotirios Sabanis and\n  Ying Zhang", "title": "On stochastic gradient Langevin dynamics with dependent data streams:\n  the fully non-convex case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sampling from a target distribution, which is\n\\emph {not necessarily logconcave}, in the context of empirical risk\nminimization and stochastic optimization as presented in Raginsky et al.\n(2017). Non-asymptotic analysis results are established in the\n$L^1$-Wasserstein distance for the behaviour of Stochastic Gradient Langevin\nDynamics (SGLD) algorithms. We allow the estimation of gradients to be\nperformed even in the presence of \\emph{dependent} data streams. Our\nconvergence estimates are sharper and \\emph{uniform} in the number of\niterations, in contrast to those in previous studies.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 16:09:26 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 11:58:29 GMT"}, {"version": "v3", "created": "Mon, 22 Jun 2020 15:33:41 GMT"}, {"version": "v4", "created": "Tue, 2 Feb 2021 09:07:34 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Chau", "Ngoc Huy", ""], ["Moulines", "\u00c9ric", ""], ["R\u00e1sonyi", "Miklos", ""], ["Sabanis", "Sotirios", ""], ["Zhang", "Ying", ""]]}, {"id": "1905.13186", "submitter": "Anne van Delft Dr.", "authors": "Anne van Delft", "title": "A note on quadratic forms of stationary functional time series under\n  mild conditions", "comments": null, "journal-ref": null, "doi": "10.1016/j.spa.2019.12.002", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distributional properties of a quadratic form of a stationary\nfunctional time series under mild moment conditions. As an important\napplication, we obtain consistency rates of estimators of spectral density\noperators and prove joint weak convergence to a vector of complex Gaussian\nrandom operators. Weak convergence is established based on an approximation of\nthe form via transforms of Hilbert-valued martingale difference sequences. As a\nside-result, the distributional properties of the long-run covariance operator\nare established.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 17:13:03 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 15:00:30 GMT"}, {"version": "v3", "created": "Mon, 27 Jul 2020 22:06:07 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["van Delft", "Anne", ""]]}, {"id": "1905.13194", "submitter": "Giulia Luise", "authors": "Giulia Luise, Saverio Salzo, Massimiliano Pontil, Carlo Ciliberto", "title": "Sinkhorn Barycenters with Free Support via Frank-Wolfe Algorithm", "comments": "46 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel algorithm to estimate the barycenter of arbitrary\nprobability distributions with respect to the Sinkhorn divergence. Based on a\nFrank-Wolfe optimization strategy, our approach proceeds by populating the\nsupport of the barycenter incrementally, without requiring any pre-allocation.\nWe consider discrete as well as continuous distributions, proving convergence\nrates of the proposed algorithm in both settings. Key elements of our analysis\nare a new result showing that the Sinkhorn divergence on compact domains has\nLipschitz continuous gradient with respect to the Total Variation and a\ncharacterization of the sample complexity of Sinkhorn potentials. Experiments\nvalidate the effectiveness of our method in practice.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 17:27:03 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Luise", "Giulia", ""], ["Salzo", "Saverio", ""], ["Pontil", "Massimiliano", ""], ["Ciliberto", "Carlo", ""]]}, {"id": "1905.13283", "submitter": "Dylan Foster", "authors": "Dylan J. Foster and Andrej Risteski", "title": "Sum-of-squares meets square loss: Fast rates for agnostic tensor\n  completion", "comments": "To appear at COLT 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study tensor completion in the agnostic setting. In the classical tensor\ncompletion problem, we receive $n$ entries of an unknown rank-$r$ tensor and\nwish to exactly complete the remaining entries. In agnostic tensor completion,\nwe make no assumption on the rank of the unknown tensor, but attempt to predict\nunknown entries as well as the best rank-$r$ tensor.\n  For agnostic learning of third-order tensors with the square loss, we give\nthe first polynomial time algorithm that obtains a \"fast\" (i.e., $O(1/n)$-type)\nrate improving over the rate obtained by reduction to matrix completion. Our\nprediction error rate to compete with the best $d\\times{}d\\times{}d$ tensor of\nrank-$r$ is $\\tilde{O}(r^{2}d^{3/2}/n)$. We also obtain an exact oracle\ninequality that trades off estimation and approximation error.\n  Our algorithm is based on the degree-six sum-of-squares relaxation of the\ntensor nuclear norm. The key feature of our analysis is to show that a certain\ncharacterization for the subgradient of the tensor nuclear norm can be encoded\nin the sum-of-squares proof system. This unlocks the standard toolbox for\nlocalization of empirical processes under the square loss, and allows us to\nestablish restricted eigenvalue-type guarantees for various tensor regression\nmodels, with tensor completion as a special case. The new analysis of the\nrelaxation complements Barak and Moitra (2016), who gave slow rates for\nagnostic tensor completion, and Potechin and Steurer (2017), who gave exact\nrecovery guarantees for the noiseless setting. Our techniques are\nuser-friendly, and we anticipate that they will find use elsewhere.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 20:05:13 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Foster", "Dylan J.", ""], ["Risteski", "Andrej", ""]]}, {"id": "1905.13494", "submitter": "Judith Ter Schure", "authors": "Judith ter Schure and Peter D. Gr\\\"unwald", "title": "Accumulation Bias in Meta-Analysis: The Need to Consider Time in Error\n  Control", "comments": "Soon to be published at F1000 Research", "journal-ref": null, "doi": "10.12688/f1000research.19375.1", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studies accumulate over time and meta-analyses are mainly retrospective.\nThese two characteristics introduce dependencies between the analysis time, at\nwhich a series of studies is up for meta-analysis, and results within the\nseries. Dependencies introduce bias --- Accumulation Bias --- and invalidate\nthe sampling distribution assumed for p-value tests, thus inflating type-I\nerrors. But dependencies are also inevitable, since for science to accumulate\nefficiently, new research needs to be informed by past results. Here, we\ninvestigate various ways in which time influences error control in\nmeta-analysis testing. We introduce an Accumulation Bias Framework that allows\nus to model a wide variety of practically occurring dependencies, including\nstudy series accumulation, meta-analysis timing, and approaches to multiple\ntesting in living systematic reviews. The strength of this framework is that it\nshows how all dependencies affect p-value-based tests in a similar manner. This\nleads to two main conclusions. First, Accumulation Bias is inevitable, and even\nif it can be approximated and accounted for, no valid p-value tests can be\nconstructed. Second, tests based on likelihood ratios withstand Accumulation\nBias: they provide bounds on error probabilities that remain valid despite the\nbias. We leave the reader with a choice between two proposals to consider time\nin error control: either treat individual (primary) studies and meta-analyses\nas two separate worlds --- each with their own timing --- or integrate\nindividual studies in the meta-analysis world. Taking up likelihood ratios in\neither approach allows for valid tests that relate well to the accumulating\nnature of scientific knowledge. Likelihood ratios can be interpreted as betting\nprofits, earned in previous studies and invested in new ones, while the\nmeta-analyst is allowed to cash out at any time and advise against future\nstudies.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 10:12:01 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["ter Schure", "Judith", ""], ["Gr\u00fcnwald", "Peter D.", ""]]}, {"id": "1905.13499", "submitter": "Morten Overgaard", "authors": "Morten Overgaard", "title": "State occupation probabilities in non-Markov models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The consistency of the Aalen--Johansen-derived estimator of state occupation\nprobabilities in non-Markov multi-state settings is studied and established via\na new route. This new route is based on interval functions and relies on a\nclose connection between additive and multiplicative transforms of interval\nfunctions, which is established. Under certain assumptions, the consistency\nfollows from explicit expressions of the additive and multiplicative transforms\nrelated to the transition probabilities as interval functions, which are\nobtained, in combination with certain censoring and positivity assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 10:33:13 GMT"}], "update_date": "2019-06-03", "authors_parsed": [["Overgaard", "Morten", ""]]}, {"id": "1905.13522", "submitter": "Markus Bachmayr", "authors": "Markus Bachmayr, Ivan G. Graham, Van Kien Nguyen and Robert Scheichl", "title": "Unified Analysis of Periodization-Based Sampling Methods for Mat\\'ern\n  Covariances", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The periodization of a stationary Gaussian random field on a sufficiently\nlarge torus comprising the spatial domain of interest is the basis of various\nefficient computational methods, such as the classical circulant embedding\ntechnique using the fast Fourier transform for generating samples on uniform\ngrids. For the family of Mat\\'ern covariances with smoothness index $\\nu$ and\ncorrelation length $\\lambda$, we analyse the nonsmooth periodization\n(corresponding to classical circulant embedding) and an alternative procedure\nusing a smooth truncation of the covariance function. We solve two open\nproblems: the first concerning the $\\nu$-dependent asymptotic decay of\neigenvalues of the resulting circulant in the nonsmooth case, the second\nconcerning the required size in terms of $\\nu$, $\\lambda$ of the torus when\nusing a smooth periodization. In doing this we arrive at a complete\ncharacterisation of the performance of these two approaches. Both our\ntheoretical estimates and the numerical tests provided here show substantial\nadvantages of smooth truncation.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 11:40:02 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 09:14:41 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 14:40:01 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Bachmayr", "Markus", ""], ["Graham", "Ivan G.", ""], ["Nguyen", "Van Kien", ""], ["Scheichl", "Robert", ""]]}, {"id": "1905.13576", "submitter": "Ziv Goldfeld", "authors": "Ziv Goldfeld, Kristjan Greenewald, Yury Polyanskiy and Jonathan Weed", "title": "Convergence of Smoothed Empirical Measures with Applications to Entropy\n  Estimation", "comments": "arXiv admin note: substantial text overlap with arXiv:1810.11589", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies convergence of empirical measures smoothed by a Gaussian\nkernel. Specifically, consider approximating $P\\ast\\mathcal{N}_\\sigma$, for\n$\\mathcal{N}_\\sigma\\triangleq\\mathcal{N}(0,\\sigma^2 \\mathrm{I}_d)$, by\n$\\hat{P}_n\\ast\\mathcal{N}_\\sigma$, where $\\hat{P}_n$ is the empirical measure,\nunder different statistical distances. The convergence is examined in terms of\nthe Wasserstein distance, total variation (TV), Kullback-Leibler (KL)\ndivergence, and $\\chi^2$-divergence. We show that the approximation error under\nthe TV distance and 1-Wasserstein distance ($\\mathsf{W}_1$) converges at rate\n$e^{O(d)}n^{-\\frac{1}{2}}$ in remarkable contrast to a typical\n$n^{-\\frac{1}{d}}$ rate for unsmoothed $\\mathsf{W}_1$ (and $d\\ge 3$). For the\nKL divergence, squared 2-Wasserstein distance ($\\mathsf{W}_2^2$), and\n$\\chi^2$-divergence, the convergence rate is $e^{O(d)}n^{-1}$, but only if $P$\nachieves finite input-output $\\chi^2$ mutual information across the additive\nwhite Gaussian noise channel. If the latter condition is not met, the rate\nchanges to $\\omega(n^{-1})$ for the KL divergence and $\\mathsf{W}_2^2$, while\nthe $\\chi^2$-divergence becomes infinite - a curious dichotomy. As a main\napplication we consider estimating the differential entropy\n$h(P\\ast\\mathcal{N}_\\sigma)$ in the high-dimensional regime. The distribution\n$P$ is unknown but $n$ i.i.d samples from it are available. We first show that\nany good estimator of $h(P\\ast\\mathcal{N}_\\sigma)$ must have sample complexity\nthat is exponential in $d$. Using the empirical approximation results we then\nshow that the absolute-error risk of the plug-in estimator converges at the\nparametric rate $e^{O(d)}n^{-\\frac{1}{2}}$, thus establishing the minimax\nrate-optimality of the plug-in. Numerical results that demonstrate a\nsignificant empirical superiority of the plug-in approach to general-purpose\ndifferential entropy estimators are provided.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2019 14:54:50 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 13:41:29 GMT"}, {"version": "v3", "created": "Fri, 1 May 2020 14:24:07 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Goldfeld", "Ziv", ""], ["Greenewald", "Kristjan", ""], ["Polyanskiy", "Yury", ""], ["Weed", "Jonathan", ""]]}, {"id": "1905.13691", "submitter": "Evgeni Dimitrov", "authors": "Evgeni Dimitrov and Xuan Wu", "title": "KMT coupling for random walk bridges", "comments": "59 pages, 1 figure. Fixed a few typos in v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we prove an analogue of the Koml\\'os-Major-Tusn\\'ady (KMT)\nembedding theorem for random walk bridges. The random bridges we consider are\nconstructed through random walks with i.i.d jumps that are conditioned on the\nlocations of their endpoints. We prove that such bridges can be strongly\ncoupled to Brownian bridges of appropriate variance when the jumps are either\ncontinuous or integer valued under some mild technical assumptions on the jump\ndistributions. Our arguments follow a similar dyadic scheme to KMT's original\nproof, but they require more refined estimates and stronger assumptions\nnecessitated by the endpoint conditioning. In particular, our result does not\nfollow from the KMT embedding theorem, which we illustrate via a\ncounterexample.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2019 15:59:17 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2019 20:44:45 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Dimitrov", "Evgeni", ""], ["Wu", "Xuan", ""]]}]