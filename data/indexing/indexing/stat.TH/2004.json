[{"id": "2004.00041", "submitter": "Zhou Fan", "authors": "Zhou Fan, Yi Sun, Tianhao Wang, and Yihong Wu", "title": "Likelihood landscape and maximum likelihood estimation for the discrete\n  orbit recovery model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC math.PR stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the non-convex optimization landscape for maximum likelihood\nestimation in the discrete orbit recovery model with Gaussian noise. This model\nis motivated by applications in molecular microscopy and image processing,\nwhere each measurement of an unknown object is subject to an independent random\nrotation from a rotational group. Equivalently, it is a Gaussian mixture model\nwhere the mixture centers belong to a group orbit.\n  We show that fundamental properties of the likelihood landscape depend on the\nsignal-to-noise ratio and the group structure. At low noise, this landscape is\n\"benign\" for any discrete group, possessing no spurious local optima and only\nstrict saddle points. At high noise, this landscape may develop spurious local\noptima, depending on the specific group. We discuss several positive and\nnegative examples, and provide a general condition that ensures a globally\nbenign landscape. For cyclic permutations of coordinates on $\\mathbb{R}^d$\n(multi-reference alignment), there may be spurious local optima when $d \\geq\n6$, and we establish a correspondence between these local optima and those of a\nsurrogate function of the phase variables in the Fourier domain.\n  We show that the Fisher information matrix transitions from resembling that\nof a single Gaussian in low noise to having a graded eigenvalue structure in\nhigh noise, which is determined by the graded algebra of invariant polynomials\nunder the group action. In a local neighborhood of the true object, the\nlikelihood landscape is strongly convex in a reparametrized system of variables\ngiven by a transcendence basis of this polynomial algebra. We discuss\nimplications for optimization algorithms, including slow convergence of\nexpectation-maximization, and possible advantages of momentum-based\nacceleration and variable reparametrization for first- and second-order descent\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2020 18:13:18 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 01:11:32 GMT"}, {"version": "v3", "created": "Mon, 11 Jan 2021 20:37:00 GMT"}, {"version": "v4", "created": "Sun, 28 Feb 2021 00:30:43 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Fan", "Zhou", ""], ["Sun", "Yi", ""], ["Wang", "Tianhao", ""], ["Wu", "Yihong", ""]]}, {"id": "2004.00179", "submitter": "Jinshan Zeng", "authors": "Jinshan Zeng, Min Zhang and Shao-Bo Lin", "title": "Fully-Corrective Gradient Boosting with Squared Hinge: Fast Learning\n  Rates and Early Stopping", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting is a well-known method for improving the accuracy of weak learners\nin machine learning. However, its theoretical generalization guarantee is\nmissing in literature. In this paper, we propose an efficient boosting method\nwith theoretical generalization guarantees for binary classification. Three key\ningredients of the proposed boosting method are: a) the\n\\textit{fully-corrective greedy} (FCG) update in the boosting procedure, b) a\ndifferentiable \\textit{squared hinge} (also called \\textit{truncated\nquadratic}) function as the loss function, and c) an efficient alternating\ndirection method of multipliers (ADMM) algorithm for the associated FCG\noptimization. The used squared hinge loss not only inherits the robustness of\nthe well-known hinge loss for classification with outliers, but also brings\nsome benefits for computational implementation and theoretical justification.\nUnder some sparseness assumption, we derive a fast learning rate of the order\n${\\cal O}((m/\\log m)^{-1/4})$ for the proposed boosting method, which can be\nfurther improved to ${\\cal O}((m/\\log m)^{-1/2})$ if certain additional noise\nassumption is imposed, where $m$ is the size of sample set. Both derived\nlearning rates are the best ones among the existing generalization results of\nboosting-type methods for classification. Moreover, an efficient early stopping\nscheme is provided for the proposed method. A series of toy simulations and\nreal data experiments are conducted to verify the developed theories and\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 00:39:24 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Zeng", "Jinshan", ""], ["Zhang", "Min", ""], ["Lin", "Shao-Bo", ""]]}, {"id": "2004.00744", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen", "title": "Pattern graphs: a graphical approach to nonmonotone missing data", "comments": "Main paper: 25 pages. We added semi-parametric theory of pattern\n  graphs in Section 3.3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We introduce the concept of pattern graphs--directed acyclic graphs\nrepresenting how response patterns are associated. A pattern graph represents\nan identifying restriction that is nonparametrically identified/saturated and\nis often a missing not at random restriction. We introduce a selection model\nand a pattern mixture model formulations using the pattern graphs and show that\nthey are equivalent. A pattern graph leads to an inverse probability weighting\nestimator as well as an imputation-based estimator. We also study the\nsemi-parametric efficiency theory and derive a multiply-robust estimator using\npattern graphs.\n", "versions": [{"version": "v1", "created": "Wed, 1 Apr 2020 23:48:42 GMT"}, {"version": "v2", "created": "Thu, 3 Dec 2020 18:56:40 GMT"}], "update_date": "2020-12-04", "authors_parsed": [["Chen", "Yen-Chi", ""]]}, {"id": "2004.00775", "submitter": "Sreejith Sreekumar Dr", "authors": "Sreejith Sreekumar and Deniz G\\\"und\\\"uz", "title": "Strong Converse for Testing Against Independence over a Noisy channel", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distributed binary hypothesis testing (HT) problem over a noisy (discrete\nand memoryless) channel studied previously by the authors is investigated from\nthe perspective of the strong converse property. It was shown by Ahlswede and\nCsisz\\'{a}r that a strong converse holds in the above setting when the channel\nis rate-limited and noiseless. Motivated by this observation, we show that the\nstrong converse continues to hold in the noisy channel setting for a special\ncase of HT known as testing against independence (TAI). The proof utilizes the\nblowing up lemma and the recent change of measure technique of Tyagi and\nWatanabe as the key tools.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 02:06:59 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 03:06:50 GMT"}, {"version": "v3", "created": "Wed, 27 May 2020 21:04:12 GMT"}, {"version": "v4", "created": "Mon, 8 Jun 2020 05:42:54 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Sreekumar", "Sreejith", ""], ["G\u00fcnd\u00fcz", "Deniz", ""]]}, {"id": "2004.00792", "submitter": "HaiYing Wang", "authors": "Luc Pronzato and HaiYing Wang", "title": "Sequential online subsampling for thinning experimental designs", "comments": "35 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a design problem where experimental conditions (design points\n$X_i$) are presented in the form of a sequence of i.i.d.\\ random variables,\ngenerated with an unknown probability measure $\\mu$, and only a given\nproportion $\\alpha\\in(0,1)$ can be selected. The objective is to select good\ncandidates $X_i$ on the fly and maximize a concave function $\\Phi$ of the\ncorresponding information matrix. The optimal solution corresponds to the\nconstruction of an optimal bounded design measure $\\xi_\\alpha^*\\leq\n\\mu/\\alpha$, with the difficulty that $\\mu$ is unknown and $\\xi_\\alpha^*$ must\nbe constructed online. The construction proposed relies on the definition of a\nthreshold $\\tau$ on the directional derivative of $\\Phi$ at the current\ninformation matrix, the value of $\\tau$ being fixed by a certain quantile of\nthe distribution of this directional derivative. Combination with recursive\nquantile estimation yields a nonlinear two-time-scale stochastic approximation\nmethod. It can be applied to very long design sequences since only the current\ninformation matrix and estimated quantile need to be stored. Convergence to an\noptimum design is proved. Various illustrative examples are presented.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 03:19:11 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2020 12:58:58 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Pronzato", "Luc", ""], ["Wang", "HaiYing", ""]]}, {"id": "2004.01089", "submitter": "Anna Kirkpatrick", "authors": "Anna Kirkpatrick, Kalen Patton", "title": "Markov Chain-based Sampling for Exploring RNA Secondary Structure under\n  the Nearest Neighbor Thermodynamic Model", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study plane trees as a model for RNA secondary structure, assigning energy\nto each tree based on the Nearest Neighbor Thermodynamic Model, and defining a\ncorresponding Gibbs distribution on the trees. Through a bijection between\nplane trees and 2-Motzkin paths, we design a Markov chain converging to the\nGibbs distribution, and establish fast mixing time results by estimating the\nspectral gap of the chain. The spectral gap estimate is established through a\nseries of decompositions of the chain and also by building on known mixing time\nresults for other chains on Dyck paths. In addition to the mathematical aspects\nof the result, the resulting algorithm can be used as a tool for exploring the\nbranching structure of RNA and its dependence on energy model parameters. The\npseudocode implementing the Markov chain is provided in an appendix.\n", "versions": [{"version": "v1", "created": "Thu, 2 Apr 2020 15:45:40 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Kirkpatrick", "Anna", ""], ["Patton", "Kalen", ""]]}, {"id": "2004.01378", "submitter": "Larry Goldstein", "authors": "Max Fathi, Larry Goldstein, Gesine Reinert and Adrien Saumard", "title": "Relaxing the Gaussian assumption in Shrinkage and SURE in high dimension", "comments": "improvements and extensions made, such as non-diagonal covariance\n  cases and discrete distributions, additional examples including some\n  elliptical distributions and soft thresholding", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage estimation is a fundamental tool of modern statistics, pioneered by\nCharles Stein upon his discovery of the famous paradox involving the\nmultivariate Gaussian. A large portion of the subsequent literature only\nconsiders the efficiency of shrinkage, and that of an associated procedure\nknown as Stein's Unbiased Risk Estimate, or SURE, in the Gaussian setting of\nthat original work. We investigate what extensions to the domain of validity of\nshrinkage and SURE can be made away from the Gaussian through the use of tools\ndeveloped in the probabilistic area now known as Stein's method. We show that\nshrinkage is efficient away from the Gaussian under very mild conditions on the\ndistribution of the noise. SURE is also proved to be adaptive under similar\nassumptions, and in particular in a way that retains the classical asymptotics\nof Pinsker's theorem. Notably, shrinkage and SURE are shown to be efficient\nunder mild distributional assumptions, and particularly for general isotropic\nlog-concave measures.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 05:11:23 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 15:41:19 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Fathi", "Max", ""], ["Goldstein", "Larry", ""], ["Reinert", "Gesine", ""], ["Saumard", "Adrien", ""]]}, {"id": "2004.01571", "submitter": "Antoine Baker", "authors": "Antoine Baker, Benjamin Aubin, Florent Krzakala, Lenka Zdeborov\\'a", "title": "TRAMP: Compositional Inference with TRee Approximate Message Passing", "comments": "Source code available at https://github.com/sphinxteam/tramp. For\n  some examples, see https://github.com/benjaminaubin/tramp_examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG eess.SP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce tramp, standing for TRee Approximate Message Passing, a python\npackage for compositional inference in high-dimensional tree-structured models.\nThe package provides an unifying framework to study several approximate message\npassing algorithms previously derived for a variety of machine learning tasks\nsuch as generalized linear models, inference in multi-layer networks, matrix\nfactorization, and reconstruction using non-separable penalties. For some\nmodels, the asymptotic performance of the algorithm can be theoretically\npredicted by the state evolution, and the measurements entropy estimated by the\nfree entropy formalism. The implementation is modular by design: each module,\nwhich implements a factor, can be composed at will with other modules to solve\ncomplex inference tasks. The user only needs to declare the factor graph of the\nmodel: the inference algorithm, state evolution and entropy estimation are\nfully automated.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 13:51:10 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 11:27:59 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Baker", "Antoine", ""], ["Aubin", "Benjamin", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "2004.01744", "submitter": "Michael Bell", "authors": "Michael Bell and Yuval Kochman", "title": "On Universality and Training in Binary Hypothesis Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical binary hypothesis testing problem is revisited. We notice that\nwhen one of the hypotheses is composite, there is an inherent difficulty in\ndefining an optimality criterion that is both informative and well-justified.\nFor testing in the simple normal location problem (that is, testing for the\nmean of multivariate Gaussians), we overcome the difficulty as follows. In this\nproblem there exists a natural hardness order between parameters as for\ndifferent parameters the error-probailities curves (when the parameter is\nknown) are either identical, or one dominates the other. We can thus define\nminimax performance as the worst-case among parameters which are below some\nhardness level. Fortunately, there exists a universal minimax test, in the\nsense that it is minimax for all hardness levels simultaneously. Under this\ncriterion we also find the optimal test for composite hypothesis testing with\ntraining data. This criterion extends to the wide class of local asymptotic\nnormal models, in an asymptotic sense where the approximation of the error\nprobabilities is additive. Since we have the asymptotically optimal tests for\ncomposite hypothesis testing with and without training data, we quantify the\nloss of universality and gain of training data for these models.\n", "versions": [{"version": "v1", "created": "Fri, 3 Apr 2020 19:28:13 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 18:42:18 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bell", "Michael", ""], ["Kochman", "Yuval", ""]]}, {"id": "2004.01865", "submitter": "Jose Figueroa-Lopez", "authors": "Jos\\'e E. Figueroa-L\\'opez and Bei Wu", "title": "Kernel Estimation of Spot Volatility with Microstructure Noise Using\n  Pre-Averaging", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first revisit the problem of estimating the spot volatility of an It\\^o\nsemimartingale using a kernel estimator. We prove a Central Limit Theorem with\noptimal convergence rate for a general two-sided kernel. Next, we introduce a\nnew pre-averaging/kernel estimator for spot volatility to handle the\nmicrostructure noise of ultra high-frequency observations. We prove a Central\nLimit Theorem for the estimation error with an optimal rate and study the\noptimal selection of the bandwidth and kernel functions. We show that the\npre-averaging/kernel estimator's asymptotic variance is minimal for exponential\nkernels, hence, justifying the need of working with kernels of unbounded\nsupport as proposed in this work. We also develop a feasible implementation of\nthe proposed estimators with optimal bandwidth. Monte Carlo experiments confirm\nthe superior performance of the devised method.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 05:43:25 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2021 16:41:09 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Figueroa-L\u00f3pez", "Jos\u00e9 E.", ""], ["Wu", "Bei", ""]]}, {"id": "2004.01869", "submitter": "Guillaume Lecu\\'e", "authors": "St\\'ephane Chr\\'etien, Mihai Cucuringu, Guillaume Lecu\\'e and Lucie\n  Neirac", "title": "Learning with Semi-Definite Programming: new statistical bounds based on\n  fixed point analysis and excess risk curvature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical learning problems have recently been shown to be amenable to\nSemi-Definite Programming (SDP), with community detection and clustering in\nGaussian mixture models as the most striking instances [javanmard et al.,\n2016]. Given the growing range of applications of SDP-based techniques to\nmachine learning problems, and the rapid progress in the design of efficient\nalgorithms for solving SDPs, an intriguing question is to understand how the\nrecent advances from empirical process theory can be put to work in order to\nprovide a precise statistical analysis of SDP estimators.\n  In the present paper, we borrow cutting edge techniques and concepts from the\nlearning theory literature, such as fixed point equations and excess risk\ncurvature arguments, which yield general estimation and prediction results for\na wide class of SDP estimators. From this perspective, we revisit some\nclassical results in community detection from [gu\\'edon et al.,2016] and [chen\net al., 2016], and we obtain statistical guarantees for SDP estimators used in\nsigned clustering, group synchronization and MAXCUT.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 06:02:04 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Chr\u00e9tien", "St\u00e9phane", ""], ["Cucuringu", "Mihai", ""], ["Lecu\u00e9", "Guillaume", ""], ["Neirac", "Lucie", ""]]}, {"id": "2004.01923", "submitter": "Nick Kloodt", "authors": "Nick Kloodt", "title": "Estimation of the Transformation Function in Fully Nonparametric\n  Transformation Models with Heteroscedasticity", "comments": "51 Pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Completely nonparametric transformation models with heteroscedastic errors\nare considered. Despite their flexibility, such models have rarely been used so\nfar, since estimators of the model components have been missing and even\nidentification of such models has not been clear until very recently. The\nresults of Kloodt (2020) are used to construct the first two estimators of the\ntransformation function in these models. While the first estimator converges to\nthe true transformation function at a parametric rate, the second estimator can\nbe obtained by an explicit formula and is less computationally demanding.\nFinally, a simulation study is followed by some concluding remarks. Assumptions\nand proofs can be found in the appendix.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 12:50:43 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Kloodt", "Nick", ""]]}, {"id": "2004.01975", "submitter": "Wenshuo Wang", "authors": "Yichao Li, Wenshuo Wang, Ke Deng and Jun S Liu", "title": "Stratification and Optimal Resampling for Sequential Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo (SMC), also known as particle filters, has been widely\naccepted as a powerful computational tool for making inference with dynamical\nsystems. A key step in SMC is resampling, which plays the role of steering the\nalgorithm towards the future dynamics. Several strategies have been proposed\nand used in practice, including multinomial resampling, residual resampling\n(Liu and Chen 1998), optimal resampling (Fearnhead and Clifford 2003),\nstratified resampling (Kitagawa 1996), and optimal transport resampling (Reich\n2013). We show that, in the one dimensional case, optimal transport resampling\nis equivalent to stratified resampling on the sorted particles, and they both\nminimize the resampling variance as well as the expected squared energy\ndistance between the original and resampled empirical distributions; in the\nmultidimensional case, the variance of stratified resampling after sorting\nparticles using Hilbert curve (Gerber et al. 2019) in $\\mathbb{R}^d$ is\n$O(m^{-(1+2/d)})$, an improved rate compared to the original $O(m^{-(1+1/d)})$,\nwhere $m$ is the number of resampled particles. This improved rate is the\nlowest for ordered stratified resampling schemes, as conjectured in Gerber et\nal. (2019). We also present an almost sure bound on the Wasserstein distance\nbetween the original and Hilbert-curve-resampled empirical distributions. In\nlight of these theoretical results, we propose the stratified\nmultiple-descendant growth (SMG) algorithm, which allows us to explore the\nsample space more efficiently compared to the standard i.i.d.\nmultiple-descendant sampling-resampling approach as measured by the Wasserstein\nmetric. Numerical evidence is provided to demonstrate the effectiveness of our\nproposed method.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 17:12:09 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 08:41:07 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Li", "Yichao", ""], ["Wang", "Wenshuo", ""], ["Deng", "Ke", ""], ["Liu", "Jun S", ""]]}, {"id": "2004.02008", "submitter": "Brenda Betancourt", "authors": "Brenda Betancourt, Giacomo Zanella and Rebecca C. Steorts", "title": "Random Partition Models for Microclustering Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Bayesian random partition models assume that the size of each\ncluster grows linearly with the number of data points. While this is appealing\nfor some applications, this assumption is not appropriate for other tasks such\nas entity resolution, modeling of sparse networks, and DNA sequencing tasks.\nSuch applications require models that yield clusters whose sizes grow\nsublinearly with the total number of data points -- the microclustering\nproperty. Motivated by these issues, we propose a general class of random\npartition models that satisfy the microclustering property with\nwell-characterized theoretical properties. Our proposed models overcome major\nlimitations in the existing literature on microclustering models, namely a lack\nof interpretability, identifiability, and full characterization of model\nasymptotic properties. Crucially, we drop the classical assumption of having an\nexchangeable sequence of data points, and instead assume an exchangeable\nsequence of clusters. In addition, our framework provides flexibility in terms\nof the prior distribution of cluster sizes, computational tractability, and\napplicability to a large number of microclustering tasks. We establish\ntheoretical properties of the resulting class of priors, where we characterize\nthe asymptotic behavior of the number of clusters and of the proportion of\nclusters of a given size. Our framework allows a simple and efficient Markov\nchain Monte Carlo algorithm to perform statistical inference. We illustrate our\nproposed methodology on the microclustering task of entity resolution, where we\nprovide a simulation study and real experiments on survey panel data.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 20:06:51 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Betancourt", "Brenda", ""], ["Zanella", "Giacomo", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "2004.02287", "submitter": "Sohail Bahmani", "authors": "Sohail Bahmani", "title": "Nearly Optimal Robust Mean Estimation via Empirical Characteristic\n  Function", "comments": "some minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an estimator for the mean of random variables in separable real\nBanach spaces using the empirical characteristic function. Assuming that the\ncovariance operator of the random variable is bounded in a precise sense, we\nshow that the proposed estimator achieves the optimal sub-Gaussian rate, except\nfor a faster decaying mean-dependent term. Under a mild condition, an iterative\nrefinement procedure can essentially eliminate the mean-dependent term and\nprovide a shift-equivariant estimate. Our results particularly suggests that a\ncertain Gaussian width that appears in the best known rate in the literature\nmight not be necessary. Furthermore, using the boundedness of the\ncharacteristic functions, we also show that, except possibly at high\nsignal-to-noise ratios, the proposed estimator is gracefully robust against\nadversarial \"contamination\". Our analysis is overall concise and transparent,\nthanks to the tractability of the characteristic functions.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 19:29:42 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 16:38:18 GMT"}, {"version": "v3", "created": "Mon, 2 Nov 2020 21:32:15 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Bahmani", "Sohail", ""]]}, {"id": "2004.02328", "submitter": "Stanislav Minsker", "authors": "Stanislav Minsker", "title": "Asymptotic normality of robust risk minimizers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates asymptotic properties of a class of algorithms that\ncan be viewed as robust analogues of the classical empirical risk minimization.\nThese strategies are based on replacing the usual empirical average by a robust\nproxy of the mean, such as the (version of) the median-of-means estimator. It\nis well known by now that the excess risk of resulting estimators often\nconverges to 0 at optimal rates under much weaker assumptions than those\nrequired by their \"classical\" counterparts. However, much less is known about\nthe asymptotic properties of the estimators themselves, for instance, whether\nrobust analogues of the maximum likelihood estimators are asymptotically\nefficient. We make a step towards answering these questions and show that for a\nwide class of parametric problems, minimizers of the appropriately defined\nrobust proxy of the risk converge to the minimizers of the true risk at the\nsame rate, and often have the same asymptotic variance, as the estimators\nobtained by minimizing the usual empirical risk. Moreover, our results show\nthat robust algorithms based on the so-called \"min-max\" type procedures in many\ncases provably outperform, is the asymptotic sense, algorithms based on direct\nrisk minimization.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 22:03:03 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 05:37:18 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 21:26:39 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Minsker", "Stanislav", ""]]}, {"id": "2004.02389", "submitter": "Hidemasa Oda", "authors": "Hidemasa Oda and Fumiyasu Komaki", "title": "Shrinkage priors on complex-valued circular-symmetric autoregressive\n  processes", "comments": "revised; Figures are modified", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate shrinkage priors on power spectral densities for\ncomplex-valued circular-symmetric autoregressive processes. We construct\nshrinkage predictive power spectral densities, which asymptotically dominate\n(i) the Bayesian predictive power spectral density based on the Jeffreys prior\nand (ii) the estimative power spectral density with the maximal likelihood\nestimator, where the Kullback-Leibler divergence from the true power spectral\ndensity to a predictive power spectral density is adopted as a risk.\nFurthermore, we propose general constructions of objective priors for K\\\"ahler\nparameter spaces, utilizing a positive continuous eigenfunction of the\nLaplace-Beltrami operator with a negative eigenvalue. We present numerical\nexperiments on a complex-valued stationary autoregressive model of order $1$.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 03:27:02 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 23:31:43 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2021 07:08:31 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Oda", "Hidemasa", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "2004.02461", "submitter": "Feriel Bouhadjera", "authors": "Feriel Bouhadjera (LMPA), Elias Sa\\\"id (LMPA), Riad Remita", "title": "Strong consistency of the nonparametric local linear regression\n  estimation under censorship model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study a local linear nonparametric regression estimator for\ncensorship model. The main goal of this paper is, to establish the uniform\nalmost sure consistency result with rate over a compact set for the new\nestimate. To support our theoretical result, a simulation study has been done\nto make comparison with the classical regression estimator.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 08:07:18 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Bouhadjera", "Feriel", "", "LMPA"], ["Sa\u00efd", "Elias", "", "LMPA"], ["Remita", "Riad", ""]]}, {"id": "2004.02466", "submitter": "Feriel Bouhadjera", "authors": "Feriel Bouhadjera (LMPA), Elias Sa\\\"id (LMPA)", "title": "Nonparametric local linear estimation of the relative error regression\n  function for censorship model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we built a new nonparametric regression estimator with the\nlocal linear method by using the mean squared relative error as a loss function\nwhen the data are subject to random right censoring. We establish the uniform\nalmost sure consistency with rate over a compact set of the proposed estimator.\nSome simulations are given to show the asymptotic behavior of the estimate in\ndifferent cases.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 08:10:58 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Bouhadjera", "Feriel", "", "LMPA"], ["Sa\u00efd", "Elias", "", "LMPA"]]}, {"id": "2004.02718", "submitter": "Sohail Bahmani", "authors": "Sohail Bahmani and Kiryung Lee", "title": "Low-Rank Matrix Estimation From Rank-One Projections by Unlifted Convex\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an estimator with a convex formulation for recovery of low-rank\nmatrices from rank-one projections. Using initial estimates of the factors of\nthe target $d_1\\times d_2$ matrix of rank-$r$, the estimator admits a practical\nsubgradient method operating in a space of dimension $r(d_1+d_2)$. This\nproperty makes the estimator significantly more scalable than the convex\nestimators based on lifting and semidefinite programming. Furthermore, we\npresent a streamlined analysis for exact recovery under the real Gaussian\nmeasurement model, as well as the partially derandomized measurement model by\nusing the spherical $t$-design. We show that under both models the estimator\nsucceeds, with high probability, if the number of measurements exceeds $r^2\n(d_1+d_2)$ up to some logarithmic factors. This sample complexity improves on\nthe existing results for nonconvex iterative algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 14:57:54 GMT"}, {"version": "v2", "created": "Sun, 10 Jan 2021 19:23:16 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Bahmani", "Sohail", ""], ["Lee", "Kiryung", ""]]}, {"id": "2004.02799", "submitter": "Mike Pereira", "authors": "Mike Pereira, Nicolas Desassis, C\\'edric Magneron, Nathan Palmer", "title": "A matrix-free approach to geostatistical filtering", "comments": "25 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel approach to geostatistical filtering which\ntackles two challenges encountered when applying this method to complex spatial\ndatasets: modeling the non-stationarity of the data while still being able to\nwork with large datasets. The approach is based on a finite element\napproximation of Gaussian random fields expressed as an expansion of the\neigenfunctions of a Laplace--Beltrami operator defined to account for local\nanisotropies. The numerical approximation of the resulting random fields using\na finite element approach is then leveraged to solve the scalability issue\nthrough a matrix-free approach. Finally, two cases of application of this\napproach, on simulated and real seismic data are presented.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 16:44:25 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Pereira", "Mike", ""], ["Desassis", "Nicolas", ""], ["Magneron", "C\u00e9dric", ""], ["Palmer", "Nathan", ""]]}, {"id": "2004.02994", "submitter": "Feifang Hu Dr", "authors": "Feifang Hu and Li-Xin Zhang", "title": "On the Theory of Covariate-Adaptive Designs", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pocock and Simon's marginal procedure (Pocock and Simon, 1975) is often\nimplemented forbalancing treatment allocation over influential covariates in\nclinical trials. However, the theoretical properties of Pocock and Simion's\nprocedure have remained largely elusive for decades. In this paper, we propose\na general framework for covariate-adaptive designs and establish the\ncorresponding theory under widely satisfied conditions. As a special case, we\nobtain the theoretical properties of Pocock and Simon's marginal procedure: the\nmarginal imbalances and overall imbalance are bounded in probability, but the\nwithin-stratum imbalances increase with the rate of $\\sqrt{n}$ as the sample\nsize increases. The theoretical results provide new insights about balance\nproperties of covariate-adaptive randomization procedures and open a door to\nstudy the theoretical properties of statistical inference for clinical trials\nbased on covariate-adaptive randomization procedures.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 20:53:31 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Hu", "Feifang", ""], ["Zhang", "Li-Xin", ""]]}, {"id": "2004.03166", "submitter": "Yanjun Han", "authors": "Yanjun Han and Kirankumar Shiragur", "title": "On the Competitive Analysis and High Accuracy Optimality of Profile\n  Maximum Likelihood", "comments": "To appear at SODA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A striking result of [Acharya et al. 2017] showed that to estimate symmetric\nproperties of discrete distributions, plugging in the distribution that\nmaximizes the likelihood of observed multiset of frequencies, also known as the\nprofile maximum likelihood (PML) distribution, is competitive compared with any\nestimators regardless of the symmetric property. Specifically, given $n$\nobservations from the discrete distribution, if some estimator incurs an error\n$\\varepsilon$ with probability at most $\\delta$, then plugging in the PML\ndistribution incurs an error $2\\varepsilon$ with probability at most\n$\\delta\\cdot \\exp(3\\sqrt{n})$. In this paper, we strengthen the above result\nand show that using a careful chaining argument, the error probability can be\nreduced to $\\delta^{1-c}\\cdot \\exp(c'n^{1/3+c})$ for arbitrarily small\nconstants $c>0$ and some constant $c'>0$. In particular, we show that the PML\ndistribution is an optimal estimator of the sorted distribution: it is\n$\\varepsilon$-close in sorted $\\ell_1$ distance to the true distribution with\nsupport size $k$ for any $n=\\Omega(k/(\\varepsilon^2 \\log k))$ and $\\varepsilon\n\\gg n^{-1/3}$, which are the information-theoretically optimal sample\ncomplexity and the largest error regime where the classical empirical\ndistribution is sub-optimal, respectively.\n  In order to strengthen the analysis of the PML, a key ingredient is to employ\nnovel \"continuity\" properties of the PML distributions and construct a chain of\nsuitable quantized PMLs, or \"coverings\". We also construct a novel\napproximation-based estimator for the sorted distribution with a near-optimal\nconcentration property without any sample splitting, where as a byproduct we\nobtain better trade-offs between the polynomial approximation error and the\nmaximum magnitude of coefficients in the Poisson approximation of $1$-Lipschitz\nfunctions.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 07:28:11 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2020 04:16:11 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2020 10:01:01 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Han", "Yanjun", ""], ["Shiragur", "Kirankumar", ""]]}, {"id": "2004.03412", "submitter": "Theofanis Sapatinas", "authors": "Anne Leucht, Efstathios Paparoditis, Daniel Rademacher and Theofanis\n  Sapatinas", "title": "Testing Equality of Spectral Density Operators for Functional Processes", "comments": "24 pages, 3 figures 1 table. (Accepted for publication: Journal of\n  Multivariate Analysis). arXiv admin note: substantial text overlap with\n  arXiv:1804.03366", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of comparing the entire second order structure of two functional\nprocesses is considered and a $L^2$-type statistic for testing equality of the\ncorresponding spectral density operators is investigated. The test statistic\nevaluates, over all frequencies, the Hilbert-Schmidt distance between the two\nestimated spectral density operators. Under certain assumptions, the limiting\ndistribution under the null hypothesis is derived. A novel frequency domain\nbootstrap method is introduced, which leads to a more accurate approximation of\nthe distribution of the test statistic under the null than the large sample\nGaussian approximation derived. Under quite general conditions, asymptotic\nvalidity of the bootstrap procedure is established for estimating the\ndistribution of the test statistic under the null. Furthermore, consistency of\nthe bootstrap-based test under the alternative is proved. Numerical simulations\nshow that, even for small samples, the bootstrap-based test has a very good\nsize and power behavior. An application to a bivariate real-life functional\ntime series illustrates the methodology proposed.\n", "versions": [{"version": "v1", "created": "Sun, 5 Apr 2020 09:30:24 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 07:37:40 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Leucht", "Anne", ""], ["Paparoditis", "Efstathios", ""], ["Rademacher", "Daniel", ""], ["Sapatinas", "Theofanis", ""]]}, {"id": "2004.03417", "submitter": "Nicolas Marie", "authors": "Fabienne Comte and Nicolas Marie", "title": "Nonparametric Estimation for I.I.D. Paths of Fractional SDE", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with nonparametric estimators of the drift function $b$\ncomputed from independent continuous observations, on a compact time interval,\nof the solution of a stochastic differential equation driven by the fractional\nBrownian motion (fSDE). First, a risk bound is established on a Skorokhod's\nintegral based least squares oracle $\\widehat b$ of $b$. Thanks to the\nrelationship between the solution of the fSDE and its derivative with respect\nto the initial condition, a risk bound is deduced on a calculable approximation\nof $\\widehat b$. Another bound is directly established on an estimator of $b'$\nfor comparison. The consistency and rates of convergence are established for\nthese estimators in the case of the compactly supported trigonometric basis or\nthe $\\mathbb R$-supported Hermite basis.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 21:03:30 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 14:22:19 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Comte", "Fabienne", ""], ["Marie", "Nicolas", ""]]}, {"id": "2004.03480", "submitter": "Sharmodeep Bhattacharyya", "authors": "Sharmodeep Bhattacharyya and Shirshendu Chatterjee", "title": "General Community Detection with Optimal Recovery Conditions for\n  Multi-relational Sparse Networks with Dependent Layers", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilayer and multiplex networks are becoming common network data sets in\nrecent times. We consider the problem of identifying the common community\nstructure for a special type of multilayer networks called multi-relational\nnetworks. We consider extensions of the spectral clustering methods for\nmulti-relational networks and give theoretical guarantees that the spectral\nclustering methods recover community structure consistently for\nmulti-relational networks generated from multilayer versions of both stochastic\nand degree-corrected block models even with dependence between network layers.\nThe methods are shown to work under optimal conditions on the degree parameter\nof the networks to detect both assortative and disassortative community\nstructures with vanishing error proportions even if individual layers of the\nmulti-relational network has the network structures below community\ndetectability threshold. We reinforce the validity of the theoretical results\nvia simulations too.\n", "versions": [{"version": "v1", "created": "Mon, 6 Apr 2020 03:19:45 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bhattacharyya", "Sharmodeep", ""], ["Chatterjee", "Shirshendu", ""]]}, {"id": "2004.03503", "submitter": "Bartosz Ko{\\l}odziejek", "authors": "Piotr Graczyk, Hideyuki Ishi, Ko{\\l}odziejek Bartosz, H\\'el\\`ene\n  Massam", "title": "Model selection in the space of Gaussian models invariant by symmetry", "comments": "34 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math-ph math.MP math.RT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multivariate centred Gaussian models for the random variable\n$Z=(Z_1,\\ldots, Z_p)$, invariant under the action of a subgroup of the group of\npermutations on $\\{1,\\ldots, p\\}$. Using the representation theory of the\nsymmetric group on the field of reals, we derive the distribution of the\nmaximum likelihood estimate of the covariance parameter $\\Sigma$ and also the\nanalytic expression of the normalizing constant of the Diaconis-Ylvisaker\nconjugate prior for the precision parameter $K=\\Sigma^{-1}$. We can thus\nperform Bayesian model selection in the class of complete Gaussian models\ninvariant by the action of a subgroup of the symmetric group, which we could\nalso call complete RCOP models. We illustrate our results with a toy example of\ndimension $4$ and several examples for selection within cyclic groups,\nincluding a high dimensional example with $p=100$.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 15:58:12 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Graczyk", "Piotr", ""], ["Ishi", "Hideyuki", ""], ["Bartosz", "Ko\u0142odziejek", ""], ["Massam", "H\u00e9l\u00e8ne", ""]]}, {"id": "2004.03683", "submitter": "Brian Williamson", "authors": "Brian D. Williamson, Peter B. Gilbert, Noah R. Simon, Marco Carone", "title": "A unified approach for inference on algorithm-agnostic variable\n  importance", "comments": "55 total pages (31 in the main document, 24 supplementary), 14\n  figures (4 in the main document, 10 supplementary)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, it is of interest to assess the relative contribution\nof features (or subsets of features) toward the goal of predicting a response\n-- in other words, to gauge the variable importance of features. Most recent\nwork on variable importance assessment has focused on describing the importance\nof features within the confines of a given prediction algorithm. However, such\nassessment does not necessarily characterize the prediction potential of\nfeatures, and may provide a misleading reflection of the intrinsic value of\nthese features. To address this limitation, we propose a general framework for\nnonparametric inference on interpretable algorithm-agnostic variable\nimportance. We define variable importance as a population-level contrast\nbetween the oracle predictiveness of all available features versus all features\nexcept those under consideration. We propose a nonparametric efficient\nestimation procedure that allows the construction of valid confidence\nintervals, even when machine learning techniques are used. We also outline a\nvalid strategy for testing the null importance hypothesis. Through simulations,\nwe show that our proposal has good operating characteristics, and we illustrate\nits use with data from a study of an antibody against HIV-1 infection.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 20:09:21 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Williamson", "Brian D.", ""], ["Gilbert", "Peter B.", ""], ["Simon", "Noah R.", ""], ["Carone", "Marco", ""]]}, {"id": "2004.03730", "submitter": "Matthew Dunlop", "authors": "Matthew M. Dunlop, Yunan Yang", "title": "Stability of Gibbs Posteriors from the Wasserstein Loss for Bayesian\n  Full Waveform Inversion", "comments": "32 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the Wasserstein loss function has been proven to be effective when\napplied to deterministic full-waveform inversion (FWI) problems. We consider\nthe application of this loss function in Bayesian FWI so that the uncertainty\ncan be captured in the solution. Other loss functions that are commonly used in\npractice are also considered for comparison. Existence and stability of the\nresulting Gibbs posteriors are shown on function space under weak assumptions\non the prior and model. In particular, the distribution arising from the\nWasserstein loss is shown to be quite stable with respect to high-frequency\nnoise in the data. We then illustrate the difference between the resulting\ndistributions numerically, using Laplace approximations to estimate the unknown\nvelocity field and uncertainty associated with the estimates.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 22:17:30 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 23:17:53 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dunlop", "Matthew M.", ""], ["Yang", "Yunan", ""]]}, {"id": "2004.03758", "submitter": "Zijian Guo", "authors": "Zijian Guo, Domagoj \\'Cevid and Peter B\\\"uhlmann", "title": "Doubly Debiased Lasso: High-Dimensional Inference under Hidden\n  Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring causal relationships or related associations from observational\ndata can be invalidated by the existence of hidden confounding. We focus on a\nhigh-dimensional linear regression setting, where the measured covariates are\naffected by hidden confounding and propose the {\\em Doubly Debiased Lasso}\nestimator for individual components of the regression coefficient vector. Our\nadvocated method simultaneously corrects both the bias due to estimation of\nhigh-dimensional parameters as well as the bias caused by the hidden\nconfounding. We establish its asymptotic normality and also prove that it is\nefficient in the Gauss-Markov sense. The validity of our methodology relies on\na dense confounding assumption, i.e. that every confounding variable affects\nmany covariates. The finite sample performance is illustrated with an extensive\nsimulation study and a genomic application.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 00:30:19 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 03:12:35 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 20:15:45 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Guo", "Zijian", ""], ["\u0106evid", "Domagoj", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "2004.03853", "submitter": "Georgina Hall", "authors": "Mihaela Curmei and Georgina Hall", "title": "Shape-Constrained Regression using Sum of Squares Polynomials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fitting a polynomial function to a set of data\npoints, each data point consisting of a feature vector and a response variable.\nIn contrast to standard polynomial regression, we require that the polynomial\nregressor satisfy shape constraints, such as monotonicity,\nLipschitz-continuity, or convexity. We show how to use semidefinite programming\nto obtain polynomial regressors that have these properties. We then prove that,\nunder some assumptions on the generation of the data points, the regressor\nobtained is a consistent estimator of the underlying shape-constrained\nfunction. We follow up with a thorough empirical comparison of our regressor to\nthe convex least squares estimator introduced in [Hildreth 1954, Holloway 1979]\nand show that our regressor can be very valuable in settings where the number\nof data points is large and where new predictions need to be made quickly and\noften. We also propose a method that relies on linear and second-order cone\nprograms to quickly update our regressor when a new batch of data points is\nprovided. We conclude with two novel applications. The first application aims\nto approximate the function that maps a conic program's data to its optimal\nvalue. This enables us to obtain quick estimations of the optimal value without\nsolving the conic program, which can be useful for real-time decision-making.\nWe illustrate this on an example in inventory management contract negotiation.\nIn the second application, we compute optimal transport maps using shape\nconstraints as regularizers following [Paty 2020], and show, via a color\ntransfer example, that this is a setting in which our regressor significantly\noutperforms other methods.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 07:39:57 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 08:32:01 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Curmei", "Mihaela", ""], ["Hall", "Georgina", ""]]}, {"id": "2004.03933", "submitter": "Elvira Di Nardo Prof.", "authors": "Elvira Di Nardo, Marina Marena, Patrizia Semeraro", "title": "On non-linear dependence of multivariate subordinated L\\'evy processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate subordinated L\\'evy processes are widely employed in finance for\nmodeling multivariate asset returns. We propose to exploit non-linear\ndependence among financial assets through multivariate cumulants of these\nprocesses, for which we provide a closed form formula by using the multi-index\ngeneralized Bell polynomials. Using multivariate cumulants, we perform a\nsensitivity analysis, to investigate non-linear dependence as a function of the\nmodel parameters driving the dependence structure\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 11:04:54 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Di Nardo", "Elvira", ""], ["Marena", "Marina", ""], ["Semeraro", "Patrizia", ""]]}, {"id": "2004.03971", "submitter": "Efstathios Paparoditis", "authors": "Efstathios Paparoditis and Han Lin Shang", "title": "Bootstrap Prediction Bands for Functional Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bootstrap procedure for constructing prediction bands for a stationary\nfunctional time series is proposed. The procedure exploits a general vector\nautoregressive representation of the time-reversed series of Fourier\ncoefficients appearing in the Karhunen-Loeve representation of the functional\nprocess. It generates backward-in-time, functional replicates that adequately\nmimic the dependence structure of the underlying process in a model-free way\nand have the same conditionally fixed curves at the end of each functional\npseudo-time series. The bootstrap prediction error distribution is then\ncalculated as the difference between the model-free, bootstrap-generated future\nfunctional observations and the functional forecasts obtained from the model\nused for prediction. This allows the estimated prediction error distribution to\naccount for the innovation and estimation errors associated with prediction and\nthe possible errors due to model misspecification. We establish the asymptotic\nvalidity of the bootstrap procedure in estimating the conditional prediction\nerror distribution of interest, and we also show that the procedure enables the\nconstruction of prediction bands that achieve (asymptotically) the desired\ncoverage. Prediction bands based on a consistent estimation of the conditional\ndistribution of the studentized prediction error process also are introduced.\nSuch bands allow for taking more appropriately into account the local\nuncertainty of prediction. Through a simulation study and the analysis of two\ndata sets, we demonstrate the capabilities and the good finite-sample\nperformance of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 12:33:01 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 10:46:28 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Paparoditis", "Efstathios", ""], ["Shang", "Han Lin", ""]]}, {"id": "2004.04078", "submitter": "Simone Padoan PhD", "authors": "Simone A. Padoan and Gilles Stupfler", "title": "Extreme expectile estimation for heavy-tailed time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectiles are a least squares analogue of quantiles which have lately\nreceived substantial attention in actuarial and financial risk management\ncontexts. Unlike quantiles, expectiles define coherent risk measures and are\ndetermined by tail expectations rather than tail probabilities; unlike the\nExpected Shortfall, they define elicitable risk measures. This has motivated\nrecent studies of the behaviour and estimation of extreme expectile-based risk\nmeasures. The case of stationary but weakly dependent observations has,\nhowever, been left largely untouched, even though correctly accounting for the\nuncertainty present in typical financial applications requires the\nconsideration of dependent data. We investigate the estimation of, and\nconstruction of accurate confidence intervals for, extreme expectiles and\nexpectile-based Marginal Expected Shortfall in a general $\\beta-$mixing\ncontext, containing the classes of ARMA, ARCH and GARCH models with\nheavy-tailed innovations that are of interest in financial applications. The\nmethods are showcased in a numerical simulation study and on real financial\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 16:00:36 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 14:00:34 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Padoan", "Simone A.", ""], ["Stupfler", "Gilles", ""]]}, {"id": "2004.04254", "submitter": "Matthias Sachs", "authors": "Matthias Sachs, Deborshee Sen, Jianfeng Lu, and David Dunson", "title": "Posterior computation with the Gibbs zig-zag sampler", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intriguing new class of piecewise deterministic Markov processes (PDMPs)\nhas recently been proposed as an alternative to Markov chain Monte Carlo\n(MCMC). In order to facilitate the application to a larger class of problems,\nwe propose a new class of PDMPs termed Gibbs zig-zag samplers, which allow\nparameters to be updated in blocks with a zig-zag sampler applied to certain\nparameters and traditional MCMC-style updates to others. We demonstrate the\nflexibility of this framework on posterior sampling for logistic models with\nshrinkage priors for high-dimensional regression and random effects and provide\nconditions for geometric ergodicity and the validity of a central limit\ntheorem.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 21:05:10 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 04:59:44 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Sachs", "Matthias", ""], ["Sen", "Deborshee", ""], ["Lu", "Jianfeng", ""], ["Dunson", "David", ""]]}, {"id": "2004.04257", "submitter": "Martina Patone", "authors": "Martina Patone and Li-Chun Zhang", "title": "Incidence weighting estimation under bipartite incidence graph sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite incidence graph sampling provides a unified representation of many\nsampling situations for the purpose of estimation, including the existing\nunconventional sampling methods, such as indirect, network or adaptive cluster\nsampling, which are not originally described as graph problems. We develop a\nlarge class of linear estimators based on the edges in the sample bipartite\nincidence graph, subjected to a general condition of design unbiasedness. The\nclass contains as special cases the classic Horvitz-Thompson estimator, as well\nas the other unbiased estimators in the literature of unconventional sampling,\nwhich can be traced back to Birnbaum and Sirken (1965). Our generalisation\nallows one to devise other unbiased estimators, thereby providing a potential\nof efficiency gains in applications. Illustrations are given for adaptive\ncluster sampling, line-intercept sampling and simulated graphs.\n", "versions": [{"version": "v1", "created": "Wed, 8 Apr 2020 21:10:43 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Patone", "Martina", ""], ["Zhang", "Li-Chun", ""]]}, {"id": "2004.04267", "submitter": "Shivangi Singh", "authors": "Shivangi Singh and Chanchal Kundu", "title": "On Weighted Generalized Entropy for Double Truncated Distribution", "comments": "20 pages, 11 figures. arXiv admin note: text overlap with\n  arXiv:math/0703489 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of weighted Renyi's entropy for truncated random variables has\nrecently been proposed in the information-theoretic literature. In this paper,\nwe introduce a generalized measure of it for double truncated distribution,\nnamely weighted generalized interval entropy (WGIE), and study it in the\ncontext of reliability analysis. Several properties, including monotonicity,\nbounds and uniqueness of WGIE are investigated. Moreover, a simulation study is\ncarried out to demonstrate the performance of the estimates of the proposed\nmeasure using simulated and real data sets. The role of WGIE in reliability\nmodeling has also been investigated for a real-life problem.\n", "versions": [{"version": "v1", "created": "Tue, 7 Apr 2020 10:09:22 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Singh", "Shivangi", ""], ["Kundu", "Chanchal", ""]]}, {"id": "2004.04341", "submitter": "Marcos Prates O", "authors": "Jose A. Ordo\\~nez, Marcos O. Prates, Larissa A. Matos, and Victor H.\n  Lachos", "title": "Objective Bayesian analysis for spatial Student-t regression models", "comments": "21 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of the prior distribution is a key aspect of Bayesian analysis.\nFor the spatial regression setting a subjective prior choice for the parameters\nmay not be trivial, from this perspective, using the objective Bayesian\nanalysis framework a reference is introduced for the spatial Student-t\nregression model with unknown degrees of freedom. The spatial Student-t\nregression model poses two main challenges when eliciting priors: one for the\nspatial dependence parameter and the other one for the degrees of freedom. It\nis well-known that the propriety of the posterior distribution over objective\npriors is not always guaranteed, whereas the use of proper prior distributions\nmay dominate and bias the posterior analysis. In this paper, we show the\nconditions under which our proposed reference prior yield to a proper posterior\ndistribution. Simulation studies are used in order to evaluate the performance\nof the reference prior to a commonly used vague proper prior.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 02:57:01 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Ordo\u00f1ez", "Jose A.", ""], ["Prates", "Marcos O.", ""], ["Matos", "Larissa A.", ""], ["Lachos", "Victor H.", ""]]}, {"id": "2004.04402", "submitter": "Quentin Duchemin", "authors": "Quentin Duchemin (LAMA)", "title": "Inference in the Stochastic Block Model with a Markovian assignment of\n  the communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the community detection problem in the Stochastic Block Model (SBM)\nwhen the communities of the nodes of the graph are assigned with a Markovian\ndynamic. To recover the partition of the nodes, we adapt the relaxed K-means\nSDP program presented in [11]. We identify the relevant signal-to-noise ratio\n(SNR) in our framework and we prove that the misclassification error decays\nexponentially fast with respect to this SNR. We provide infinity norm\nconsistent estimation of the parameters of our model and we discuss our results\nthrough the prism of classical degree regimes of the SBMs' literature. MSC 2010\nsubject classifications: Primary 68Q32; secondary 68R10, 90C35.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 07:58:02 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Duchemin", "Quentin", "", "LAMA"]]}, {"id": "2004.04451", "submitter": "Frank Werner", "authors": "Shuai Lu, Pingping Niu, Frank Werner", "title": "On the asymptotical regularization for linear inverse problems in\n  presence of white noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We interpret steady linear statistical inverse problems as artificial dynamic\nsystems with white noise and introduce a stochastic differential equation (SDE)\nsystem where the inverse of the ending time $T$ naturally plays the role of the\nsquared noise level. The time-continuous framework then allows us to apply\nclassical methods from data assimilation, namely the Kalman-Bucy filter and\n3DVAR, and to analyze their behaviour as a regularization method for the\noriginal problem. Such treatment offers some connections to the famous\nasymptotical regularization method, which has not yet been analyzed in the\ncontext of random noise. We derive error bounds for both methods in terms of\nthe mean-squared error under standard assumptions and discuss commonalities and\ndifferences between both approaches. If an additional tuning parameter $\\alpha$\nfor the initial covariance is chosen appropriately in terms of the ending time\n$T$, one of the proposed methods gains order optimality. Our results extend\ntheoretical findings in the discrete setting given in the recent paper Iglesias\net al. (2017). Numerical examples confirm our theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 09:44:52 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Lu", "Shuai", ""], ["Niu", "Pingping", ""], ["Werner", "Frank", ""]]}, {"id": "2004.04636", "submitter": "Masoumeh Dashti", "authors": "Jean-Charles Croix, Masoumeh Dashti, and Istv\\`an Zolt\\`an Kiss", "title": "Nonparametric Bayesian inference of discretely observed diffusions", "comments": "25 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.AP math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of the Bayesian inference of drift and diffusion\ncoefficient functions in a stochastic differential equation given discrete\nobservations of a realisation of its solution. We give conditions for the\nwell-posedness and stable approximations of the posterior measure. These\nconditions in particular allow for priors with unbounded support. Our proof\nrelies on the explicit construction of transition probability densities using\nthe parametrix method for general parabolic equations. We then study an\napplication of these results in inferring the rates of Birth-and-Death\nprocesses.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 16:27:29 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Croix", "Jean-Charles", ""], ["Dashti", "Masoumeh", ""], ["Kiss", "Istv\u00e0n Zolt\u00e0n", ""]]}, {"id": "2004.04715", "submitter": "Justin Khim", "authors": "Justin Khim, Ziyu Xu and Shashank Singh", "title": "Multiclass Classification via Class-Weighted Nearest Neighbors", "comments": "62 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical properties of the k-nearest neighbors algorithm for\nmulticlass classification, with a focus on settings where the number of classes\nmay be large and/or classes may be highly imbalanced. In particular, we\nconsider a variant of the k-nearest neighbor classifier with non-uniform\nclass-weightings, for which we derive upper and minimax lower bounds on\naccuracy, class-weighted risk, and uniform error. Additionally, we show that\nuniform error bounds lead to bounds on the difference between empirical\nconfusion matrix quantities and their population counterparts across a set of\nweights. As a result, we may adjust the class weights to optimize\nclassification metrics such as F1 score or Matthew's Correlation Coefficient\nthat are commonly used in practice, particularly in settings with imbalanced\nclasses. We additionally provide a simple example to instantiate our bounds and\nnumerical experiments.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:50:16 GMT"}, {"version": "v2", "created": "Mon, 4 May 2020 00:40:57 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Khim", "Justin", ""], ["Xu", "Ziyu", ""], ["Singh", "Shashank", ""]]}, {"id": "2004.04719", "submitter": "Wenlong Mou", "authors": "Wenlong Mou, Chris Junchi Li, Martin J. Wainwright, Peter L. Bartlett,\n  Michael I. Jordan", "title": "On Linear Stochastic Approximation: Fine-grained Polyak-Ruppert and\n  Non-Asymptotic Concentration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We undertake a precise study of the asymptotic and non-asymptotic properties\nof stochastic approximation procedures with Polyak-Ruppert averaging for\nsolving a linear system $\\bar{A} \\theta = \\bar{b}$. When the matrix $\\bar{A}$\nis Hurwitz, we prove a central limit theorem (CLT) for the averaged iterates\nwith fixed step size and number of iterations going to infinity. The CLT\ncharacterizes the exact asymptotic covariance matrix, which is the sum of the\nclassical Polyak-Ruppert covariance and a correction term that scales with the\nstep size. Under assumptions on the tail of the noise distribution, we prove a\nnon-asymptotic concentration inequality whose main term matches the covariance\nin CLT in any direction, up to universal constants. When the matrix $\\bar{A}$\nis not Hurwitz but only has non-negative real parts in its eigenvalues, we\nprove that the averaged LSA procedure actually achieves an $O(1/T)$ rate in\nmean-squared error. Our results provide a more refined understanding of linear\nstochastic approximation in both the asymptotic and non-asymptotic settings. We\nalso show various applications of the main results, including the study of\nmomentum-based stochastic gradient methods as well as temporal difference\nalgorithms in reinforcement learning.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:54:18 GMT"}], "update_date": "2020-04-10", "authors_parsed": [["Mou", "Wenlong", ""], ["Li", "Chris Junchi", ""], ["Wainwright", "Martin J.", ""], ["Bartlett", "Peter L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2004.04724", "submitter": "Anne Van Delft Dr.", "authors": "Anne van Delft and Holger Dette", "title": "Pivotal tests for relevant differences in the second order dynamics of\n  functional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need to statistically quantify differences between modern\n(complex) data-sets which commonly result as high-resolution measurements of\nstochastic processes varying over a continuum, we propose novel testing\nprocedures to detect relevant differences between the second order dynamics of\ntwo functional time series. In order to take the between-function dynamics into\naccount that characterize this type of functional data, a frequency domain\napproach is taken. Test statistics are developed to compare differences in the\nspectral density operators and in the primary modes of variation as encoded in\nthe associated eigenelements. Under mild moment conditions, we show convergence\nof the underlying statistics to Brownian motions and construct pivotal test\nstatistics. The latter is essential because the nuisance parameters can be\nunwieldy and their robust estimation infeasible, especially if the two\nfunctional time series are dependent. Besides from these novel features, the\nproperties of the tests are robust to any choice of frequency band enabling\nalso to compare energy contents at a single frequency. The finite sample\nperformance of the tests are verified through a simulation study and are\nillustrated with an application to fMRI data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 17:56:54 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 22:49:28 GMT"}], "update_date": "2020-12-18", "authors_parsed": [["van Delft", "Anne", ""], ["Dette", "Holger", ""]]}, {"id": "2004.04767", "submitter": "Hai Tran-Bach", "authors": "Tengyuan Liang and Hai Tran-Bach", "title": "Mehler's Formula, Branching Process, and Compositional Kernels of Deep\n  Neural Networks", "comments": null, "journal-ref": "Journal of the American Statistical Association (2020)", "doi": "10.1080/01621459.2020.1853547", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We utilize a connection between compositional kernels and branching processes\nvia Mehler's formula to study deep neural networks. This new probabilistic\ninsight provides us a novel perspective on the mathematical role of activation\nfunctions in compositional neural networks. We study the unscaled and rescaled\nlimits of the compositional kernels and explore the different phases of the\nlimiting behavior, as the compositional depth increases. We investigate the\nmemorization capacity of the compositional kernels and neural networks by\ncharacterizing the interplay among compositional depth, sample size,\ndimensionality, and non-linearity of the activation. Explicit formulas on the\neigenvalues of the compositional kernel are provided, which quantify the\ncomplexity of the corresponding reproducing kernel Hilbert space. On the\nmethodological front, we propose a new random features algorithm, which\ncompresses the compositional layers by devising a new activation function.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 18:46:13 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 17:29:34 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Liang", "Tengyuan", ""], ["Tran-Bach", "Hai", ""]]}, {"id": "2004.04856", "submitter": "Rong Ma", "authors": "Rong Ma and Ian Barnett", "title": "The Asymptotic Distribution of Modularity in Weighted Signed Networks", "comments": null, "journal-ref": "Biometrika (2020)", "doi": "10.1093/biomet/asaa059", "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modularity is a popular metric for quantifying the degree of community\nstructure within a network. The distribution of the largest eigenvalue of a\nnetwork's edge weight or adjacency matrix is well studied and is frequently\nused as a substitute for modularity when performing statistical inference.\nHowever, we show that the largest eigenvalue and modularity are asymptotically\nuncorrelated, which suggests the need for inference directly on modularity\nitself when the network size is large. To this end, we derive the asymptotic\ndistributions of modularity in the case where the network's edge weight matrix\nbelongs to the Gaussian Orthogonal Ensemble, and study the statistical power of\nthe corresponding test for community structure under some alternative model. We\nempirically explore universality extensions of the limiting distribution and\ndemonstrate the accuracy of these asymptotic distributions through type I error\nsimulations. We also compare the empirical powers of the modularity based tests\nwith some existing methods. Our method is then used to test for the presence of\ncommunity structure in two real data applications.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 23:37:12 GMT"}], "update_date": "2020-07-15", "authors_parsed": [["Ma", "Rong", ""], ["Barnett", "Ian", ""]]}, {"id": "2004.04983", "submitter": "Jose Carlos Araujo Acuna", "authors": "Jose Carlos Araujo Acuna and Hansjoerg Albrecher and Jan Beirlant", "title": "Tempered Pareto-type modelling using Weibull distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various applications of heavy-tail modelling, the assumed Pareto behavior\nis tempered ultimately in the range of the largest data. In insurance\napplications, claim payments are influenced by claim management and claims may\nfor instance be subject to a higher level of inspection at highest damage\nlevels leading to weaker tails than apparent from modal claims. Generalizing\nearlier results of Meerschaert et al. (2012) and Raschke (2019), in this paper\nwe consider tempering of a Pareto-type distribution with a general Weibull\ndistribution in a peaks-over-threshold approach. This requires to modulate the\ntempering parameters as a function of the chosen threshold. Modelling such a\ntempering effect is important in order to avoid overestimation of risk measures\nsuch as the Value-at-Risk (VaR) at high quantiles. We use a pseudo maximum\nlikelihood approach to estimate the model parameters, and consider the\nestimation of extreme quantiles. We derive basic asymptotic results for the\nestimators, give illustrations with simulation experiments and apply the\ndeveloped techniques to fire and liability insurance data, providing insight\ninto the relevance of the tempering component in heavy-tail modelling.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 10:43:32 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 10:00:59 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Acuna", "Jose Carlos Araujo", ""], ["Albrecher", "Hansjoerg", ""], ["Beirlant", "Jan", ""]]}, {"id": "2004.05096", "submitter": "Yaozhong Hu", "authors": "El Mehdi Haress, Yaozhong Hu", "title": "Estimation Of all parameters in the Fractional Ornstein-Uhlenbeck model\n  under discrete observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let the Ornstein-Uhlenbeck process $(X_t)_{t\\ge0}$ driven by a fractional\nBrownian motion $B^{H }$, described by $dX_t = -\\theta X_t dt + \\sigma dB_t^{H\n}$ be observed at discrete time instants\n  $t_k=kh$, $k=0, 1, 2, \\cdots, 2n+2 $. We propose ergodic type statistical\nestimators $\\hat \\theta_n $, $\\hat H_n $ and $\\hat \\sigma_n $ to estimate all\nthe parameters $\\theta $, $H $ and $\\sigma $ in the above Ornstein-Uhlenbeck\nmodel simultaneously. We prove the strong consistence and the rate of\nconvergence of the estimators. The step size $h$ can be arbitrarily fixed and\nwill not be forced to go zero, which is usually a reality. The tools to use are\nthe generalized moment approach (via ergodic theorem) and the Malliavin\ncalculus.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 16:28:37 GMT"}], "update_date": "2020-04-13", "authors_parsed": [["Haress", "El Mehdi", ""], ["Hu", "Yaozhong", ""]]}, {"id": "2004.05192", "submitter": "Helena Ferreira", "authors": "Helena Ferreira, Marta Ferreira", "title": "Multivariate Medial Correlation with applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a multivariate medial correlation coefficient that extends the\nprobabilistic interpretation and properties of Blomqvist's $\\beta$ coefficient,\nincorporates multivariate marginal dependencies and it preserves a stronger\nmultivariate concordance relation. We determine the maximum and minimum values\nattainable and illustrate the results in some models. We end with an\napplication on real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Apr 2020 18:16:34 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2020 11:43:39 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 16:37:58 GMT"}], "update_date": "2020-10-15", "authors_parsed": [["Ferreira", "Helena", ""], ["Ferreira", "Marta", ""]]}, {"id": "2004.05307", "submitter": "Qifan Song", "authors": "Qifan Song", "title": "Bayesian Shrinkage towards Sharp Minimaxity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage prior are becoming more and more popular in Bayesian modeling for\nhigh dimensional sparse problems due to its computational efficiency. Recent\nworks show that a polynomially decaying prior leads to satisfactory posterior\nasymptotics under regression models. In the literature, statisticians have\ninvestigated how the global shrinkage parameter, i.e., the scale parameter, in\na heavy tail prior affects the posterior contraction. In this work, we explore\nhow the shape of the prior, or more specifically, the polynomial order of the\nprior tail affects the posterior. We discover that, under the sparse normal\nmeans models, the polynomial order does affect the multiplicative constant of\nthe posterior contraction rate. More importantly, if the polynomial order is\nsufficiently close to 1, it will induce the optimal Bayesian posterior\nconvergence, in the sense that the Bayesian contraction rate is sharply\nminimax, i.e., not only the order, but also the multiplicative constant of the\nposterior contraction rate are optimal. The above Bayesian sharp minimaxity\nholds when the global shrinkage parameter follows a deterministic choice which\ndepends on the unknown sparsity $s$. Therefore, a Beta-prior modeling is\nfurther proposed, such that our sharply minimax Bayesian procedure is adaptive\nto unknown $s$. Our theoretical discoveries are justified by simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 05:18:31 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Song", "Qifan", ""]]}, {"id": "2004.05308", "submitter": "Tanmay Sen", "authors": "Tanmay Sen, Ritwik Bhattacharya, Biswabrata Pradhan and Yogesh Mani\n  Tripathi", "title": "Statistical inference and Bayesian optimal life-testing plans under\n  Type-II unified hybrid censoring scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes the inferential procedures and Bayesian optimal\nlife-testing issues under Type-II unified hybrid censoring scheme. First, the\nexplicit expressions of expected number of failures, expected duration of\ntesting and Fisher information matrix for the unknown parameters of the\nunderlying lifetime model are derived. Then, using these quantities, the\nBayesian optimal life-testing plans are computed in subsequent section. A cost\nconstraint D-optimal optimization problem has been formulated and the\ncorresponding solution algorithm is provided to obtain optimal plans.\nComputational procedures are illustrated through numerical examples.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 05:26:47 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Sen", "Tanmay", ""], ["Bhattacharya", "Ritwik", ""], ["Pradhan", "Biswabrata", ""], ["Tripathi", "Yogesh Mani", ""]]}, {"id": "2004.05361", "submitter": "Martin Genzel", "authors": "Martin Genzel and Christian Kipp", "title": "Generic Error Bounds for the Generalized Lasso with Sub-Exponential Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work performs a non-asymptotic analysis of the generalized Lasso under\nthe assumption of sub-exponential data. Our main results continue recent\nresearch on the benchmark case of (sub-)Gaussian sample distributions and\nthereby explore what conclusions are still valid when going beyond. While many\nstatistical features of the generalized Lasso remain unaffected (e.g.,\nconsistency), the key difference becomes manifested in the way how the\ncomplexity of the hypothesis set is measured. It turns out that the estimation\nerror can be controlled by means of two complexity parameters that arise\nnaturally from a generic-chaining-based proof strategy. The output model can be\nnon-realizable, while the only requirement for the input vector is a generic\nconcentration inequality of Bernstein-type, which can be implemented for a\nvariety of sub-exponential distributions. This abstract approach allows us to\nreproduce, unify, and extend previously known guarantees for the generalized\nLasso. In particular, we present applications to semi-parametric output models\nand phase retrieval via the lifted Lasso. Moreover, our findings are discussed\nin the context of sparse recovery and high-dimensional estimation problems.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 10:39:48 GMT"}, {"version": "v2", "created": "Mon, 18 May 2020 17:23:49 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Genzel", "Martin", ""], ["Kipp", "Christian", ""]]}, {"id": "2004.05387", "submitter": "Karl Rohe", "authors": "Karl Rohe and Muzhe Zeng", "title": "Vintage Factor Analysis with Varimax Performs Statistical Inference", "comments": "All comments welcome <3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychologists developed Multiple Factor Analysis to decompose multivariate\ndata into a small number of interpretable factors without any a priori\nknowledge about those factors. In this form of factor analysis, the Varimax\n\"factor rotation\" is a key step to make the factors interpretable. Charles\nSpearman and many others objected to factor rotations because the factors seem\nto be rotationally invariant. These objections are still reported in all\ncontemporary multivariate statistics textbooks. This is an engima because this\nvintage form of factor analysis has survived and is widely popular because,\nempirically, the factor rotation often makes the factors easier to interpret.\nWe argue that the rotation makes the factors easier to interpret because, in\nfact, the Varimax factor rotation performs statistical inference. We show that\nPrincipal Components Analysis (PCA) with the Varimax rotation provides a\nunified spectral estimation strategy for a broad class of modern factor models,\nincluding the Stochastic Blockmodel and a natural variation of Latent Dirichlet\nAllocation (i.e., \"topic modeling\"). In addition, we show that Thurstone's\nwidely employed sparsity diagnostics implicitly assess a key \"leptokurtic\"\ncondition that makes the rotation statistically identifiable in these models.\nTaken together, this shows that the know-how of Vintage Factor Analysis\nperforms statistical inference, reversing nearly a century of statistical\nthinking on the topic. With a sparse eigensolver, PCA with Varimax is both fast\nand stable. Combined with Thurstone's straightforward diagnostics, this vintage\napproach is suitable for a wide array of modern applications.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 12:45:06 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 12:24:06 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Rohe", "Karl", ""], ["Zeng", "Muzhe", ""]]}, {"id": "2004.05470", "submitter": "Abhik Ghosh PhD", "authors": "Abhik Ghosh, Maria Jaenada, Leandro Pardo", "title": "Robust adaptive variable selection in ultra-high dimensional linear\n  regression models", "comments": "Pre-print, Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of simultaneous variable selection and estimation of\nthe corresponding regression coefficients in an ultra-high dimensional linear\nregression models, an extremely important problem in the recent era. The\nadaptive penalty functions are used in this regard to achieve the oracle\nvariable selection property along with easier computational burden. However,\nthe usual adaptive procedures (e.g., adaptive LASSO) based on the squared error\nloss function is extremely non-robust in the presence of data contamination\nwhich are quite common with large-scale data (e.g., noisy gene expression data,\nspectra and spectral data). In this paper, we present a regularization\nprocedure for the ultra-high dimensional data using a robust loss function\nbased on the popular density power divergence (DPD) measure along with the\nadaptive LASSO penalty. We theoretically study the robustness and the\nlarge-sample properties of the proposed adaptive robust estimators for a\ngeneral class of error distributions; in particular, we show that the proposed\nadaptive DPD-LASSO estimator is highly robust, satisfies the oracle variable\nselection property, and the corresponding estimators of the regression\ncoefficients are consistent and asymptotically normal under easily verifiable\nset of assumptions. Numerical illustrations are provided for the mostly used\nnormal error density. Finally, the proposal is applied to analyze an\ninteresting spectral dataset, in the field of chemometrics, regarding the\nelectron-probe X-ray microanalysis (EPXMA) of archaeological glass vessels from\nthe 16th and 17th centuries.\n", "versions": [{"version": "v1", "created": "Sat, 11 Apr 2020 19:39:42 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2020 09:06:24 GMT"}], "update_date": "2020-09-25", "authors_parsed": [["Ghosh", "Abhik", ""], ["Jaenada", "Maria", ""], ["Pardo", "Leandro", ""]]}, {"id": "2004.05542", "submitter": "Yun Wei", "authors": "Yun Wei and XuanLong Nguyen", "title": "Convergence of de Finetti's mixing measure in latent structure models\n  for observed exchangeable sequences", "comments": "101 pages, 0 figures; added Section 9, expanded references,\n  strengthen Lemma 4.10, removed some material (e.g. Sec. 7.2 and Lemma 4.7 in\n  the previous version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of product distributions are a powerful device for learning about\nheterogeneity within data populations. In this class of latent structure\nmodels, de Finetti's mixing measure plays the central role for describing the\nuncertainty about the latent parameters representing heterogeneity. In this\npaper posterior contraction theorems for de Finetti's mixing measure arising\nfrom finite mixtures of product distributions will be established, under the\nsetting the number of exchangeable sequences of observed variables increases\nwhile sequence length(s) may be either fixed or varied. The role of both the\nnumber of sequences and the sequence lengths will be carefully examined. In\norder to obtain concrete rates of convergence, a first-order identifiability\ntheory for finite mixture models and a family of sharp inverse bounds for\nmixtures of product distributions will be developed via a harmonic analysis of\nsuch latent structure models. This theory is applicable to broad classes of\nprobability kernels composing the mixture model of product distributions for\nboth continuous and discrete domain $\\mathfrak{X}$. Examples of interest\ninclude the case the probability kernel is only weakly identifiable in the\nsense of Ho and Nguyen (2016), the case where the kernel is itself a mixture\ndistribution as in hierarchical models, and the case the kernel may not have a\ndensity with respect to a dominating measure on an abstract domain\n$\\mathfrak{X}$ such as Dirichlet processes.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 04:10:14 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 20:16:57 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Wei", "Yun", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "2004.05692", "submitter": "Panagiotis Sidiropoulos", "authors": "Panagiotis Sidiropoulos", "title": "Measuring spatial uniformity with the hypersphere chord length\n  distribution", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data uniformity is a concept associated with several semantic data\ncharacteristics such as lack of features, correlation and sample bias. This\narticle introduces a novel measure to assess data uniformity and detect uniform\npointsets on high-dimensional Euclidean spaces. Spatial uniformity measure\nbuilds upon the isomorphism between hyperspherical chords and L2-normalised\ndata Euclidean distances, which is implied by the fact that, in Euclidean\nspaces, L2-normalised data can be geometrically defined as points on a\nhypersphere. The imposed connection between the distance distribution of\nuniformly selected points and the hyperspherical chord length distribution is\nemployed to quantify uniformity. More specifically,, the closed-form expression\nof hypersphere chord length distribution is revisited extended, before\nexamining a few qualitative and quantitative characteristics of this\ndistribution that can be rather straightforwardly linked to data uniformity.\nThe experimental section includes validation in four distinct setups, thus\nsubstantiating the potential of the new uniformity measure on practical\ndata-science applications.\n", "versions": [{"version": "v1", "created": "Sun, 12 Apr 2020 20:48:50 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Sidiropoulos", "Panagiotis", ""]]}, {"id": "2004.05990", "submitter": "Takeyuki Sasai", "authors": "Takeyuki Sasai and Hironori Fujisawa", "title": "Robust estimation with Lasso when outputs are adversarially contaminated", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider robust estimation when outputs are adversarially contaminated.\nNguyen and Tran (2012) proposed an extended Lasso for robust parameter\nestimation and then they showed the convergence rate of the estimation error.\nRecently, Dalalyan and Thompson (2019) gave some useful inequalities and then\nthey showed a faster convergence rate than Nguyen and Tran (2012). They focused\non the fact that the minimization problem of the extended Lasso can become that\nof the penalized Huber loss function with $L_1$ penalty. The distinguishing\npoint is that the Huber loss function includes an extra tuning parameter, which\nis different from the conventional method. We give the proof, which is\ndifferent from Dalalyan and Thompson (2019) and then we give the same\nconvergence rate as Dalalyan and Thompson (2019). The significance of our proof\nis to use some specific properties of the Huber function. Such techniques have\nnot been used in the past proofs.\n", "versions": [{"version": "v1", "created": "Mon, 13 Apr 2020 15:06:45 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 17:13:44 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 16:01:45 GMT"}, {"version": "v4", "created": "Sat, 10 Oct 2020 06:22:28 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Sasai", "Takeyuki", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "2004.06296", "submitter": "Xiao Han", "authors": "Xiao Han, Xin Tong and Yingying Fan", "title": "Eigen selection in spectral clustering: a theory guided practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a Gaussian mixture type model , we derive an eigen selection\nprocedure that improves the usual spectral clustering in high-dimensional\nsettings. Concretely, we derive the asymptotic expansion of the spiked\neigenvalues under eigenvalue multiplicity and eigenvalue ratio concentration\nresults, giving rise to the first theory-backed eigen selection procedure in\nspectral clustering. The resulting eigen-selected spectral clustering (ESSC)\nalgorithm enjoys better stability and compares favorably against canonical\nalternatives. We demonstrate the advantages of ESSC using extensive simulation\nand multiple real data studies.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 04:28:51 GMT"}, {"version": "v2", "created": "Sun, 7 Jun 2020 00:21:04 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Han", "Xiao", ""], ["Tong", "Xin", ""], ["Fan", "Yingying", ""]]}, {"id": "2004.06328", "submitter": "Tin Lok James Ng", "authors": "Tin Lok James Ng, Kwok-Kun Kwong", "title": "Universal Approximation on the Hypersphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that any continuous probability density function on\n$\\mathbb{R}^m$ can be approximated arbitrarily well by a finite mixture of\nnormal distributions, provided that the number of mixture components is\nsufficiently large. The von-Mises-Fisher distribution, defined on the unit\nhypersphere $S^m$ in $\\mathbb{R}^{m+1}$, has properties that are analogous to\nthose of the multivariate normal on $\\mathbb{R}^{m+1}$. We prove that any\ncontinuous probability density function on $S^m$ can be approximated to\narbitrary degrees of accuracy by a finite mixture of von-Mises-Fisher\ndistributions.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 07:20:11 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Ng", "Tin Lok James", ""], ["Kwong", "Kwok-Kun", ""]]}, {"id": "2004.06329", "submitter": "Anthony Coolen", "authors": "ACC Coolen, M Sheikh, A Mozeika, F Aguirre-Lopez, F Antenucci", "title": "Replica analysis of overfitting in generalized linear models", "comments": "45 pages, 7 figures, accepted for publication in Journal of Physics A", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearly all statistical inference methods were developed for the regime where\nthe number $N$ of data samples is much larger than the data dimension $p$.\nInference protocols such as maximum likelihood (ML) or maximum a posteriori\nprobability (MAP) are unreliable if $p=O(N)$, due to overfitting. This\nlimitation has for many disciplines with increasingly high-dimensional data\nbecome a serious bottleneck. We recently showed that in Cox regression for\ntime-to-event data the overfitting errors are not just noise but take mostly\nthe form of a bias, and how with the replica method from statistical physics\nonce can model and predict this bias and the noise statistics. Here we extend\nour approach to arbitrary generalized linear regression models (GLM), with\npossibly correlated covariates. We analyse overfitting in ML/MAP inference\nwithout having to specify data types or regression models, relying only on the\nGLM form, and derive generic order parameter equations for the case of $L2$\npriors. Second, we derive the probabilistic relationship between true and\ninferred regression coefficients in GLMs, and show that, for the relevant\nhyperparameter scaling and correlated covariates, the $L2$ regularization\ncauses a predictable direction change of the coefficient vector. Our results,\nillustrated by application to linear, logistic, and Cox regression, enable one\nto correct ML and MAP inferences in GLMs systematically for overfitting bias,\nand thus extend their applicability into the hitherto forbidden regime\n$p=O(N)$.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 07:27:48 GMT"}, {"version": "v2", "created": "Sat, 18 Apr 2020 10:43:53 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 14:52:24 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Coolen", "ACC", ""], ["Sheikh", "M", ""], ["Mozeika", "A", ""], ["Aguirre-Lopez", "F", ""], ["Antenucci", "F", ""]]}, {"id": "2004.06433", "submitter": "Daniel Kressner", "authors": "Zvonimir Bujanovi\\'c, Daniel Kressner", "title": "Norm and trace estimation with random rank-one vectors", "comments": "Revised version, with new Theorem 3.4", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A few matrix-vector multiplications with random vectors are often sufficient\nto obtain reasonably good estimates for the norm of a general matrix or the\ntrace of a symmetric positive semi-definite matrix. Several such probabilistic\nestimators have been proposed and analyzed for standard Gaussian and Rademacher\nrandom vectors. In this work, we consider the use of rank-one random vectors,\nthat is, Kronecker products of (smaller) Gaussian or Rademacher vectors. It is\nnot only cheaper to sample such vectors but it can sometimes also be much\ncheaper to multiply a matrix with a rank-one vector instead of a general\nvector. In this work, theoretical and numerical evidence is given that the use\nof rank-one instead of unstructured random vectors still leads to good\nestimates. In particular, it is shown that our rank-one estimators multiplied\nwith a modest constant constitute, with high probability, upper bounds of the\nquantity of interest. Partial results are provided for the case of lower\nbounds. The application of our techniques to condition number estimation for\nmatrix functions is illustrated.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 11:48:12 GMT"}, {"version": "v2", "created": "Sun, 9 Aug 2020 08:10:05 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Bujanovi\u0107", "Zvonimir", ""], ["Kressner", "Daniel", ""]]}, {"id": "2004.06467", "submitter": "Mikhail Boldin", "authors": "Michael Boldin", "title": "On Symmetrized Pearson's Type Test for Normality of Autoregression:\n  Power under Local Alternatives", "comments": "in Russian. arXiv admin note: significant text overlap with\n  arXiv:2003.13072", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stationary linear AR($p$) model with observations subject to\ngross errors (outliers). The autoregression parameters as well as the\ndistribution function (d.f.) $G$ of innovations are unknown. The distribution\nof outliers $\\Pi$ is unknown and arbitrary, their intensity is $\\gamma\nn^{-1/2}$ with an unknown $\\gamma$, $n$ is the sample size. We test the\nhypothesis for normality of innovations $$\\mathbf{H}_\\Phi \\colon G \\in\n\\{\\Phi(x/\\theta),\\,\\theta>0\\},$$ $\\Phi(x)$ is the d.f. $\\mathbf{N}(0,1)$. Our\ntest is the special symmetrized Pearson's type test. We find the power of this\ntest under local alternatives $$\\mathbf{H}_{1n}(\\rho)\\colon\nG(x)=A_n(x):=(1-\\rho n^{-1/2})\\Phi(x/\\theta_0)+\\rho n^{-1/2}H(x), $$ $\\rho\\geq\n0,\\,\\theta_0$ is the unknown (under $\\mathbf{H}_\\Phi$) variance of innovations.\nFirst of all we estimate the autoregression parameters and then using the\nresiduals from the estimated autoregression we construct a kind of empirical\ndistribution function (r.e.d.f.), which is a counterpart of the (inaccessible)\ne.d.f. of the autoregression innovations. After this we construct the\nsymmetrized variant r.e.d.f. Our test statistic is the functional from\nsymmetrized r.e.d.f. We obtain a stochastic expansion of this symmetrized\nr.e.d.f. under $\\mathbf{H}_{1n}(\\rho)$ , which enables us to investigate our\ntest. We establish qualitative robustness of this test in terms of uniform\nequicontinuity of the limiting power (as functions of $\\gamma,\\rho$ and $\\Pi$)\nwith respect to $\\gamma$ in a neighborhood of $\\gamma=0$.\n", "versions": [{"version": "v1", "created": "Sat, 4 Apr 2020 15:37:56 GMT"}], "update_date": "2020-04-15", "authors_parsed": [["Boldin", "Michael", ""]]}, {"id": "2004.06574", "submitter": "Annika Betken", "authors": "Annika Betken and Martin Wendler", "title": "Rank-based change-point analysis for long-range dependent time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider change-point tests based on rank statistics to test for\nstructural changes in long-range dependent observations. Under the hypothesis\nof stationary time series and under the assumption of a change with decreasing\nchange-point height, the asymptotic distributions of corresponding test\nstatistics are derived. For this, a uniform reduction principle for the\nsequential empirical process in a two-parameter Skorohod space equipped with a\nweighted supremum norm is proved. Moreover, we compare the efficiency of rank\ntests resulting from the consideration of different score functions. Under\nGaussianity, the asymptotic relative efficiency of rank-based tests with\nrespect to the CuSum test is 1, irrespective of the score function. Regarding\nthe practical implementation of rank-based change-point tests, we suggest to\ncombine self-normalized rank statistics with subsampling. The theoretical\nresults are accompanied by simulation studies that, in particular, allow for a\ncomparison of rank tests resulting from different score functions. With respect\nto the finite sample performance of rank-based change-point tests, the Van der\nWaerden rank test proves to be favorable in a broad range of situations.\nFinally, we analyze data sets from economy, hydrology, and network traffic\nmonitoring in view of structural changes and compare our results to previous\nanalysis of the data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:00:24 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 14:33:45 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Betken", "Annika", ""], ["Wendler", "Martin", ""]]}, {"id": "2004.06586", "submitter": "Xiaochun Meng", "authors": "Carol Alexander, Xiaochun Meng, Wei Wei", "title": "Extensions of Random Orthogonal Matrix Simulation for Targetting Kollo\n  Skewness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST q-fin.CP q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling multivariate systems is important for many applications in\nengineering and operational research. The multivariate distributions under\nscrutiny usually have no analytic or closed form. Therefore their modelling\nemploys a numerical technique, typically multivariate simulations, which can\nhave very high dimensions. Random Orthogonal Matrix (ROM) simulation is a\nmethod that has gained some popularity because of the absence of certain\nsimulation errors. Specifically, it exactly matches a target mean, covariance\nmatrix and certain higher moments with every simulation. This paper extends the\nROM simulation algorithm presented by Hanke et al. (2017), hereafter referred\nto as HPSW, which matches the target mean, covariance matrix and Kollo skewness\nvector exactly. Our first contribution is to establish necessary and sufficient\nconditions for the HPSW algorithm to work. Our second contribution is to\ndevelop a general approach for constructing admissible values in the HPSW. Our\nthird theoretical contribution is to analyse the effect of multivariate sample\nconcatenation on the target Kollo skewness. Finally, we illustrate the\nextensions we develop here using a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 15:18:20 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 16:51:37 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Alexander", "Carol", ""], ["Meng", "Xiaochun", ""], ["Wei", "Wei", ""]]}, {"id": "2004.06615", "submitter": "Yuan Zhang", "authors": "Yuan Zhang, Dong Xia", "title": "Edgeworth expansions for network moments", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network method of moments arXiv:1202.5101 is an important tool for\nnonparametric network inference. However, there has been little investigation\non accurate descriptions of the sampling distributions of network moment\nstatistics. In this paper, we present the first higher-order accurate\napproximation to the sampling CDF of a studentized network moment by Edgeworth\nexpansion. In sharp contrast to classical literature on noiseless U-statistics,\nwe show that the Edgeworth expansion of a network moment statistic as a noisy\nU-statistic can achieve higher-order accuracy without non-lattice or smoothness\nassumptions but just requiring weak regularity conditions. Behind this result\nis our surprising discovery that the two typically-hated factors in network\nanalysis, namely, sparsity and edge-wise observational errors, jointly play a\nblessing role, contributing a crucial self-smoothing effect in the network\nmoment statistic and making it analytically tractable. Our assumptions match\nthe minimum requirements in related literature. For sparse networks, our theory\nshows a simple normal approximation achieves a gradually depreciating\nBerry-Esseen bound as the network becomes sparser. This result also refines the\nbest previous theoretical result.\n  For practitioners, our empirical Edgeworth expansion is highly accurate, fast\nand easy to implement. We demonstrate the clear advantage of our method by\ncomprehensive simulation studies.\n  We showcase three applications of our results in network inference. We prove,\nto our knowledge, the first theoretical guarantee of higher-order accuracy for\nsome network bootstrap schemes, and moreover, the first theoretical guidance\nfor selecting the sub-sample size for network sub-sampling. We also derive\none-sample test and Cornish-Fisher confidence interval for a given moment with\nhigher-order accurate controls of confidence level and type I error,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 14 Apr 2020 16:02:26 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 05:08:25 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhang", "Yuan", ""], ["Xia", "Dong", ""]]}, {"id": "2004.07039", "submitter": "Mikhail  Ermakov s", "authors": "Mikhail Ermakov", "title": "On uniform consistency of nonparametric tests II", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Kolmogorov test we find necessary and sufficient conditions of uniform\nconsistency of sets of alternatives approaching to hypothesis. Sets of\nalternatives can be defined both in terms of distribution functions and in\nterms of densities.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 12:20:10 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 11:11:08 GMT"}, {"version": "v3", "created": "Wed, 17 Jun 2020 18:28:40 GMT"}, {"version": "v4", "created": "Sun, 2 Aug 2020 17:48:12 GMT"}, {"version": "v5", "created": "Tue, 8 Sep 2020 09:30:32 GMT"}, {"version": "v6", "created": "Sun, 11 Oct 2020 13:22:33 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ermakov", "Mikhail", ""]]}, {"id": "2004.07310", "submitter": "Daniel Rudolf", "authors": "Michael Habeck, Daniel Rudolf, Bj\\\"orn Sprungk", "title": "Stability of doubly-intractable distributions", "comments": "16 pages, to appear in Electronic Communications in Probability", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doubly-intractable distributions appear naturally as posterior distributions\nin Bayesian inference frameworks whenever the likelihood contains a normalizing\nfunction $Z$. Having two such functions $Z$ and $\\widetilde Z$ we provide\nestimates of the total variation and Wasserstein distance of the resulting\nposterior probability measures. As a consequence this leads to local Lipschitz\ncontinuity w.r.t. $Z$. In the more general framework of a random function\n$\\widetilde Z$ we derive bounds on the expected total variation and expected\nWasserstein distance. The applicability of the estimates is illustrated within\nthe setting of two representative Monte Carlo recovery scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 19:40:13 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 08:33:56 GMT"}, {"version": "v3", "created": "Wed, 12 Aug 2020 11:32:38 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Habeck", "Michael", ""], ["Rudolf", "Daniel", ""], ["Sprungk", "Bj\u00f6rn", ""]]}, {"id": "2004.07332", "submitter": "Bruno Ebner", "authors": "Bruno Ebner and Norbert Henze", "title": "Tests for multivariate normality -- a critical review with emphasis on\n  weighted $L^2$-statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article gives a synopsis on new developments in affine invariant tests\nfor multivariate normality in an i.i.d.-setting, with special emphasis on\nasymptotic properties of several classes of weighted $L^2$-statistics. Since\nweighted $L^2$-statistics typically have limit normal distributions under fixed\nalternatives to normality, they open ground for a neighborhood of model\nvalidation for normality. The paper also reviews several other invariant tests\nfor this problem, notably the energy test, and it presents the results of a\nlarge-scale simulation study. All tests under study are implemented in the\naccompanying R-package mnt.\n", "versions": [{"version": "v1", "created": "Wed, 15 Apr 2020 20:41:33 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Ebner", "Bruno", ""], ["Henze", "Norbert", ""]]}, {"id": "2004.07649", "submitter": "Bj\\\"orn B\\\"ottcher", "authors": "Bj\\\"orn B\\\"ottcher", "title": "Notes on the interpretation of dependence measures", "comments": "20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Besides the classical distinction of correlation and dependence, many\ndependence measures bear further pitfalls in their application and\ninterpretation. The aim of this paper is to raise and recall awareness of some\nof these limitations by explicitly discussing Pearson's correlation and the\nmultivariate dependence measures: distance correlation, distance\nmulticorrelations and their copula versions. The discussed aspects include\ntypes of dependence, bias of empirical measures, influence of marginal\ndistributions and dimensions.\n  In general it is recommended to use a proper dependence measure instead of\nPearson's correlation. Moreover, a measure which is distribution-free (at least\nin some sense) can help to avoid certain systematic errors. Nevertheless, in a\ntruly multivariate setting only the p-values of the corresponding independence\ntests provide always values with indubitable interpretation.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 13:38:08 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["B\u00f6ttcher", "Bj\u00f6rn", ""]]}, {"id": "2004.07659", "submitter": "Sitan Chen", "authors": "Sitan Chen, Ankur Moitra", "title": "Algorithmic Foundations for the Diffraction Limit", "comments": "55 pages, 5 figures, v2: improved lower bound going beyond the Abbe\n  limit", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST physics.optics stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For more than a century and a half it has been widely-believed (but was never\nrigorously shown) that the physics of diffraction imposes certain fundamental\nlimits on the resolution of an optical system. However our understanding of\nwhat exactly can and cannot be resolved has never risen above heuristic\narguments which, even worse, appear contradictory. In this work we remedy this\ngap by studying the diffraction limit as a statistical inverse problem and,\nbased on connections to provable algorithms for learning mixture models, we\nrigorously prove upper and lower bounds on the statistical and algorithmic\ncomplexity needed to resolve closely spaced point sources. In particular we\nshow that there is a phase transition where the sample complexity goes from\npolynomial to exponential. Surprisingly, we show that this does not occur at\nthe Abbe limit, which has long been presumed to be the true diffraction limit.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 13:50:53 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 18:56:48 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Chen", "Sitan", ""], ["Moitra", "Ankur", ""]]}, {"id": "2004.07735", "submitter": "Leonid Monin", "authors": "Mateusz Micha{\\l}ek, Leonid Monin, Jaros{\\l}aw Wi\\'sniewski", "title": "Maximum likelihood degree, complete quadrics and ${\\mathbb C}^*$-action", "comments": "Minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum likelihood (ML) degree of linear concentration models in\nalgebraic statistics. We relate it to an intersection problem on the variety of\ncomplete quadrics. This allows us to provide an explicit, basic, albeit of high\ncomputational complexity, formula for the ML-degree. The variety of complete\nquadrics is an exact analog for symmetric matrices of the permutohedron variety\nfor the diagonal matrices.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 16:16:18 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 12:16:31 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 13:04:02 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Micha\u0142ek", "Mateusz", ""], ["Monin", "Leonid", ""], ["Wi\u015bniewski", "Jaros\u0142aw", ""]]}, {"id": "2004.07781", "submitter": "Qin Fang", "authors": "Qin Fang, Shaojun Guo and Xinghao Qiao", "title": "A New Perspective on Dependence in High-Dimensional Functional/Scalar\n  Time Series: Finite Sample Theory and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis of high-dimensional functional times series arises in\nvarious applications. Under this scenario, in addition to the intrinsic\ninfinite-dimensionality of functional data, the number of functional variables\ncan grow with the number of serially dependent functional observations. In this\npaper, we focus on the theoretical analysis of relevant estimated\ncross-(auto)covariance terms between two multivariate functional time series or\na mixture of multivariate functional and scalar time series beyond the\nGaussianity assumption. We introduce a new perspective on dependence by\nproposing functional cross-spectral stability measure to characterize the\neffect of dependence on these estimated cross terms, which are essential in the\nestimates for additive functional linear regressions. With the proposed\nfunctional cross-spectral stability measure, we develop useful concentration\ninequalities for estimated cross-(auto)covariance matrix functions to\naccommodate more general sub-Gaussian functional linear processes and,\nfurthermore, establish finite sample theory for relevant estimated terms under\na commonly adopted functional principal component analysis framework. Using our\nderived non-asymptotic results, we investigate the convergence properties of\nthe regularized estimates for two additive functional linear regression\napplications under sparsity assumptions including functional linear lagged\nregression and partially functional linear regression in the context of\nhigh-dimensional functional/scalar time series.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 17:22:33 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Fang", "Qin", ""], ["Guo", "Shaojun", ""], ["Qiao", "Xinghao", ""]]}, {"id": "2004.07900", "submitter": "Dennis Kristensen", "authors": "Mogens Fosgerau and Dennis Kristensen", "title": "Identification of a class of index models: A topological approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish nonparametric identification in a class of so-called index\nmodels using a novel approach that relies on general topological results. Our\nproof strategy requires substantially weaker conditions on the functions and\ndistributions characterizing the model compared to existing strategies; in\nparticular, it does not require any large support conditions on the regressors\nof our model. We apply the general identification result to additive random\nutility and competing risk models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Apr 2020 19:43:57 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Fosgerau", "Mogens", ""], ["Kristensen", "Dennis", ""]]}, {"id": "2004.08085", "submitter": "Remi Gribonval", "authors": "R\\'emi Gribonval (PANAMA, DANTE), Gilles Blanchard (LMO), Nicolas\n  Keriven (GIPSA-GAIA), Yann Traonmilin (IMB)", "title": "Statistical Learning Guarantees for Compressive Clustering and\n  Compressive Mixture Modeling", "comments": "This preprint results from a split and profound restructuring and\n  improvements of of https://hal.inria.fr/hal-01544609v2It is a companion paper\n  to https://hal.inria.fr/hal-01544609v3", "journal-ref": "Mathematical Statistics and Learning, EMS Publishing House, In\n  press", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide statistical learning guarantees for two unsupervised learning\ntasks in the context of compressive statistical learning, a general framework\nfor resource-efficient large-scale learning that we introduced in a companion\npaper. The principle of compressive statistical learning is to compress a\ntraining collection, in one pass, into a low-dimensional sketch (a vector of\nrandom empirical generalized moments) that captures the information relevant to\nthe considered learning task. We explicit random feature functions which\nempirical averages preserve the needed information for compressive clustering\nand compressive Gaussian mixture modeling with fixed known variance, and\nestablish sufficient sketch sizes given the problem dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 07:05:29 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 09:23:11 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Gribonval", "R\u00e9mi", "", "PANAMA, DANTE"], ["Blanchard", "Gilles", "", "LMO"], ["Keriven", "Nicolas", "", "GIPSA-GAIA"], ["Traonmilin", "Yann", "", "IMB"]]}, {"id": "2004.08102", "submitter": "Kyoungjae Lee", "authors": "Kyoungjae Lee and Xuan Cao", "title": "Bayesian inference for high-dimensional decomposable graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider high-dimensional Gaussian graphical models where\nthe true underlying graph is decomposable. A hierarchical $G$-Wishart prior is\nproposed to conduct a Bayesian inference for the precision matrix and its graph\nstructure. Although the posterior asymptotics using the $G$-Wishart prior has\nreceived increasing attention in recent years, most of results assume moderate\nhigh-dimensional settings, where the number of variables $p$ is smaller than\nthe sample size $n$. However, this assumption might not hold in many real\napplications such as genomics, speech recognition and climatology. Motivated by\nthis gap, we investigate asymptotic properties of posteriors under the\nhigh-dimensional setting where $p$ can be much larger than $n$. The pairwise\nBayes factor consistency, posterior ratio consistency and graph selection\nconsistency are obtained in this high-dimensional setting. Furthermore, the\nposterior convergence rate for precision matrices under the matrix\n$\\ell_1$-norm is derived, which turns out to coincide with the minimax\nconvergence rate for sparse precision matrices. A simulation study confirms\nthat the proposed Bayesian procedure outperforms competitors.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 08:08:52 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 09:49:22 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 23:52:41 GMT"}, {"version": "v4", "created": "Wed, 17 Feb 2021 05:25:23 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Lee", "Kyoungjae", ""], ["Cao", "Xuan", ""]]}, {"id": "2004.08159", "submitter": "Xiao Fang", "authors": "Xiao Fang, David Siegmund", "title": "Detection and Estimation of Local Signals", "comments": "50 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the maximum score statistic to detect and estimate local signals in\nthe form of change-points in the level, slope, or other property of a sequence\nof observations, and to segment the sequence when there appear to be multiple\nchanges. We find that when observations are serially dependent, the\nchange-points can lead to upwardly biased estimates of autocorrelations,\nresulting in a sometimes serious loss of power. Examples involving temperature\nvariations, the level of atmospheric greenhouse gases, suicide rates and daily\nincidence of COVID-19 illustrate the general theory.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 10:32:31 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Fang", "Xiao", ""], ["Siegmund", "David", ""]]}, {"id": "2004.08436", "submitter": "Martin Wahl", "authors": "Alain Celisse and Martin Wahl", "title": "Analyzing the discrepancy principle for kernelized spectral filter\n  learning algorithms", "comments": "68 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the construction of early stopping rules in the nonparametric\nregression problem where iterative learning algorithms are used and the optimal\niteration number is unknown. More precisely, we study the discrepancy\nprinciple, as well as modifications based on smoothed residuals, for kernelized\nspectral filter learning algorithms including gradient descent. Our main\ntheoretical bounds are oracle inequalities established for the empirical\nestimation error (fixed design), and for the prediction error (random design).\nFrom these finite-sample bounds it follows that the classical discrepancy\nprinciple is statistically adaptive for slow rates occurring in the hard\nlearning scenario, while the smoothed discrepancy principles are adaptive over\nranges of faster rates (resp. higher smoothness parameters). Our approach\nrelies on deviation inequalities for the stopping rules in the fixed design\nsetting, combined with change-of-norm arguments to deal with the random design\nsetting.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 20:08:44 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Celisse", "Alain", ""], ["Wahl", "Martin", ""]]}, {"id": "2004.08472", "submitter": "Tirthankar Dasgupta", "authors": "Xiaokang Luo, Tirthankar Dasgupta, Minge Xie, Regina Liu", "title": "Leveraging the Fisher randomization test using confidence distributions:\n  inference, combination and fusion learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The flexibility and wide applicability of the Fisher randomization test (FRT)\nmakes it an attractive tool for assessment of causal effects of interventions\nfrom modern-day randomized experiments that are increasing in size and\ncomplexity. This paper provides a theoretical inferential framework for FRT by\nestablishing its connection with confidence distributions Such a connection\nleads to development of (i) an unambiguous procedure for inversion of FRTs to\ngenerate confidence intervals with guaranteed coverage, (ii) generic and\nspecific methods to combine FRTs from multiple independent experiments with\ntheoretical guarantees and (iii) new insights on the effect of size of the\nMonte Carlo sample on the results of FRT. Our developments pertain to finite\nsample settings but have direct extensions to large samples. Simulations and a\ncase example demonstrate the benefit of these new developments.\n", "versions": [{"version": "v1", "created": "Fri, 17 Apr 2020 22:20:35 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Luo", "Xiaokang", ""], ["Dasgupta", "Tirthankar", ""], ["Xie", "Minge", ""], ["Liu", "Regina", ""]]}, {"id": "2004.08580", "submitter": "Wang Zhou", "authors": "Xuejun Ma, Shaochen Wang, Wang Zhou", "title": "Statistical inference in massive datasets by empirical likelihood", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new statistical inference method for massive data\nsets, which is very simple and efficient by combining divide-and-conquer method\nand empirical likelihood. Compared with two popular methods (the bag of little\nbootstrap and the subsampled double bootstrap), we make full use of data sets,\nand reduce the computation burden. Extensive numerical studies and real data\nanalysis demonstrate the effectiveness and flexibility of our proposed method.\nFurthermore, the asymptotic property of our method is derived.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 10:18:07 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Ma", "Xuejun", ""], ["Wang", "Shaochen", ""], ["Zhou", "Wang", ""]]}, {"id": "2004.08597", "submitter": "Ananya Uppal", "authors": "Ananya Uppal, Shashank Singh, Barnabas Poczos", "title": "Robust Density Estimation under Besov IPM Losses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study minimax convergence rates of nonparametric density estimation in the\nHuber contamination model, in which a proportion of the data comes from an\nunknown outlier distribution. We provide the first results for this problem\nunder a large family of losses, called Besov integral probability metrics\n(IPMs), that includes $\\mathcal{L}^p$, Wasserstein, Kolmogorov-Smirnov, and\nother common distances between probability distributions. Specifically, under a\nrange of smoothness assumptions on the population and outlier distributions, we\nshow that a re-scaled thresholding wavelet series estimator achieves minimax\noptimal convergence rates under a wide variety of losses. Finally, based on\nconnections that have recently been shown between nonparametric density\nestimation under IPM losses and generative adversarial networks (GANs), we show\nthat certain GAN architectures also achieve these minimax rates.\n", "versions": [{"version": "v1", "created": "Sat, 18 Apr 2020 11:30:35 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Uppal", "Ananya", ""], ["Singh", "Shashank", ""], ["Poczos", "Barnabas", ""]]}, {"id": "2004.08807", "submitter": "Jere Koskela", "authors": "Jere Koskela", "title": "Zig-zag sampling for discrete structures and non-reversible phylogenetic\n  MCMC", "comments": "21 pages, 10 figures, 3 tables. This is a major revision which\n  introduces a generic zig-zag process for a hybrid target with both discrete\n  and continuous parameters. Applications to the coalescent from earlier\n  versions are retained as examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST q-bio.PE stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a zig-zag process targeting a posterior distribution defined on\na hybrid state space consisting of both discrete and continuous variables. The\nconstruction does not require any assumptions on the structure among discrete\nvariables. We demonstrate our method on two examples in genetics based on the\nKingman coalescent, showing that the zig-zag process can lead to efficiency\ngains of up to several orders of magnitude over classical Metropolis--Hastings\nalgorithms, and that it is well suited to parallel computation. Our\nconstruction resembles existing techniques for Hamiltonian Monte Carlo on a\nhybrid state space, which suffers from implementationally and analytically\ncomplex boundary crossings when applied to the coalescent. We demonstrate that\nthe continuous-time zig-zag process avoids these complications.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 10:30:34 GMT"}, {"version": "v2", "created": "Sun, 13 Dec 2020 14:19:59 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 15:14:32 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Koskela", "Jere", ""]]}, {"id": "2004.08867", "submitter": "Yulong Lu", "authors": "Yulong Lu and Jianfeng Lu", "title": "A Universal Approximation Theorem of Deep Neural Networks for Expressing\n  Probability Distributions", "comments": "Accepted in the Thirty-fourth Conference on Neural Information\n  Processing Systems (NeurIPS 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NA math.NA math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the universal approximation property of deep neural\nnetworks for representing probability distributions. Given a target\ndistribution $\\pi$ and a source distribution $p_z$ both defined on\n$\\mathbb{R}^d$, we prove under some assumptions that there exists a deep neural\nnetwork $g:\\mathbb{R}^d\\rightarrow \\mathbb{R}$ with ReLU activation such that\nthe push-forward measure $(\\nabla g)_\\# p_z$ of $p_z$ under the map $\\nabla g$\nis arbitrarily close to the target measure $\\pi$. The closeness are measured by\nthree classes of integral probability metrics between probability\ndistributions: $1$-Wasserstein distance, maximum mean distance (MMD) and\nkernelized Stein discrepancy (KSD). We prove upper bounds for the size (width\nand depth) of the deep neural network in terms of the dimension $d$ and the\napproximation error $\\varepsilon$ with respect to the three discrepancies. In\nparticular, the size of neural network can grow exponentially in $d$ when\n$1$-Wasserstein distance is used as the discrepancy, whereas for both MMD and\nKSD the size of neural network only depends on $d$ at most polynomially. Our\nproof relies on convergence estimates of empirical measures under\naforementioned discrepancies and semi-discrete optimal transport.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 14:45:47 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 19:05:19 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 04:29:47 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Lu", "Yulong", ""], ["Lu", "Jianfeng", ""]]}, {"id": "2004.08909", "submitter": "Patrick Michl", "authors": "Patrick Michl", "title": "Applications of Structural Statistics: Geometric Inference in\n  Exponential Families", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Exponential families comprise a broad class of statistical models and\nparametric families like normal distributions, binomial distributions, gamma\ndistributions or exponential distributions. Thereby the formal representation\nof its probability distributions induces a confined intrinsic structure, which\nappears to be that of a dually flat statistical manifold. Conversely it can be\nshown, that any dually flat statistical manifold, which is given by a regular\nBregman divergence uniquely induced a regular exponential family, such that\nexponential families may - with some restrictions - be regarded as a universal\nrepresentation of dually flat statistical manifolds. This article reviews the\npioneering work of Shun'ichi Amari about the intrinsic structure of exponential\nfamilies in terms of structural stratistics.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 16:53:45 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 14:30:57 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Michl", "Patrick", ""]]}, {"id": "2004.08935", "submitter": "Robert Lunde", "authors": "Qiaohui Lin, Robert Lunde, Purnamrita Sarkar", "title": "On the Theoretical Properties of the Network Jackknife", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the properties of a leave-node-out jackknife procedure for network\ndata. Under the sparse graphon model, we prove an Efron-Stein-type inequality,\nshowing that the network jackknife leads to conservative estimates of the\nvariance (in expectation) for any network functional that is invariant to node\npermutation. For a general class of count functionals, we also establish\nconsistency of the network jackknife. We complement our theoretical analysis\nwith a range of simulated and real-data examples and show that the network\njackknife offers competitive performance in cases where other resampling\nmethods are known to be valid. In fact, for several network statistics, we see\nthat the jackknife provides more accurate inferences compared to related\nmethods such as subsampling.\n", "versions": [{"version": "v1", "created": "Sun, 19 Apr 2020 19:03:32 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 08:53:12 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Lin", "Qiaohui", ""], ["Lunde", "Robert", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "2004.09106", "submitter": "Ulrike Schneider", "authors": "Ulrike Schneider and Patrick Tardivel", "title": "The Geometry of Uniqueness, Sparsity and Clustering in Penalized\n  Estimation", "comments": "new title, minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a necessary and sufficient condition for the uniqueness of\npenalized least-squares estimators whose penalty term is given by a norm with a\npolytope unit ball, covering a wide range of methods including SLOPE and LASSO,\nas well as the related method of basis pursuit. We consider a strong type of\nuniqueness that is relevant for statistical problems. The uniqueness condition\nis geometric and involves how the row span of the design matrix intersects the\nfaces of the dual norm unit ball, which for SLOPE is given by the sign\npermutahedron. Further considerations based this condition also allow to derive\nresults on sparsity and clustering features. In particular, we define the\nnotion of a SLOPE model to describe both sparsity and clustering properties of\nthis method and also provide a geometric characterization of accessible SLOPE\nmodels.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 07:45:37 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 11:32:35 GMT"}, {"version": "v3", "created": "Tue, 18 Aug 2020 15:00:08 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Schneider", "Ulrike", ""], ["Tardivel", "Patrick", ""]]}, {"id": "2004.09181", "submitter": "Jack Kuipers", "authors": "Jack Kuipers and Giusi Moffa", "title": "The Variance of Causal Effect Estimators for Binary V-structures", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adjusting for covariates is a well established method to estimate the total\ncausal effect of an exposure variable on an outcome of interest. Depending on\nthe causal structure of the mechanism under study there may be different\nadjustment sets, equally valid from a theoretical perspective, leading to\nidentical causal effects. However, in practice, with finite data, estimators\nbuilt on different sets may display different precision. To investigate the\nextent of this variability we consider the simplest non-trivial non-linear\nmodel of a v-structure on three nodes for binary data. We explicitly compute\nand compare the variance of the two possible different causal estimators.\nFurther, by going beyond leading order asymptotics we show that there are\nparameter regimes where the set with the asymptotically optimal variance does\ndepend on the edge coefficients, a result which is not captured by the recent\nleading order developments for general causal models. As a practical\nconsequence, the adjustment set selection needs to account for the relative\nmagnitude of the relationships between variables with respect to the sample\nsize, and cannot rely on purely graphical criteria.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 10:16:01 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 18:35:37 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 08:09:44 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kuipers", "Jack", ""], ["Moffa", "Giusi", ""]]}, {"id": "2004.09306", "submitter": "Xuan Cao", "authors": "Xuan Cao and Kyoungjae Lee", "title": "Joint Bayesian Variable and DAG Selection Consistency for\n  High-dimensional Regression Models with Network-structured Covariates", "comments": null, "journal-ref": null, "doi": "10.5705/ss.202019.0202", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the joint sparse estimation of regression coefficients and the\ncovariance matrix for covariates in a high-dimensional regression model, where\nthe predictors are both relevant to a response variable of interest and\nfunctionally related to one another via a Gaussian directed acyclic graph (DAG)\nmodel. Gaussian DAG models introduce sparsity in the Cholesky factor of the\ninverse covariance matrix, and the sparsity pattern in turn corresponds to\nspecific conditional independence assumptions on the underlying predictors. A\nvariety of methods have been developed in recent years for Bayesian inference\nin identifying such network-structured predictors in regression setting, yet\ncrucial sparsity selection properties for these models have not been thoroughly\ninvestigated. In this paper, we consider a hierarchical model with spike and\nslab priors on the regression coefficients and a flexible and general class of\nDAG-Wishart distributions with multiple shape parameters on the Cholesky\nfactors of the inverse covariance matrix. Under mild regularity assumptions, we\nestablish the joint selection consistency for both the variable and the\nunderlying DAG of the covariates when the dimension of predictors is allowed to\ngrow much larger than the sample size. We demonstrate that our method\noutperforms existing methods in selecting network-structured predictors in\nseveral simulation settings.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 14:00:17 GMT"}], "update_date": "2020-04-21", "authors_parsed": [["Cao", "Xuan", ""], ["Lee", "Kyoungjae", ""]]}, {"id": "2004.09370", "submitter": "Yuval Dagan", "authors": "Yuval Dagan, Constantinos Daskalakis, Nishanth Dikkala, Anthimos\n  Vardis Kandiros", "title": "Learning Ising models from one or multiple samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been two separate lines of work on estimating Ising models: (1)\nestimating them from multiple independent samples under minimal assumptions\nabout the model's interaction matrix; and (2) estimating them from one sample\nin restrictive settings. We propose a unified framework that smoothly\ninterpolates between these two settings, enabling significantly richer\nestimation guarantees from one, a few, or many samples.\n  Our main theorem provides guarantees for one-sample estimation, quantifying\nthe estimation error in terms of the metric entropy of a family of interaction\nmatrices. As corollaries of our main theorem, we derive bounds when the model's\ninteraction matrix is a (sparse) linear combination of known matrices, or it\nbelongs to a finite set, or to a high-dimensional manifold. In fact, our main\nresult handles multiple independent samples by viewing them as one sample from\na larger model, and can be used to derive estimation bounds that are\nqualitatively similar to those obtained in the afore-described multiple-sample\nliterature. Our technical approach benefits from sparsifying a model's\ninteraction network, conditioning on subsets of variables that make the\ndependencies in the resulting conditional distribution sufficiently weak. We\nuse this sparsification technique to prove strong concentration and\nanti-concentration results for the Ising model, which we believe have\napplications beyond the scope of this paper.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 15:17:05 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2020 01:53:59 GMT"}, {"version": "v3", "created": "Thu, 10 Dec 2020 16:27:23 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Dagan", "Yuval", ""], ["Daskalakis", "Constantinos", ""], ["Dikkala", "Nishanth", ""], ["Kandiros", "Anthimos Vardis", ""]]}, {"id": "2004.09477", "submitter": "Rina Barber", "authors": "Rina Foygel Barber", "title": "Is distribution-free inference possible for binary regression?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a regression problem with a binary label response, we examine the problem\nof constructing confidence intervals for the label probability conditional on\nthe features. In a setting where we do not have any information about the\nunderlying distribution, we would ideally like to provide confidence intervals\nthat are distribution-free---that is, valid with no assumptions on the\ndistribution of the data. Our results establish an explicit lower bound on the\nlength of any distribution-free confidence interval, and construct a procedure\nthat can approximately achieve this length. In particular, this lower bound is\nindependent of the sample size and holds for all distributions with no point\nmasses, meaning that it is not possible for any distribution-free procedure to\nbe adaptive with respect to any type of special structure in the distribution.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 17:53:56 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 17:57:04 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 18:55:35 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Barber", "Rina Foygel", ""]]}, {"id": "2004.09588", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay, Kaijun Wang", "title": "On The Problem of Relevance in Statistical Inference", "comments": "Revised (much-improved) version. The procedure (including all the\n  datasets) is implemented in the R-package LPRelevance", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is dedicated to the \"50 Years of the Relevance Problem\" - a\nlong-neglected topic that begs attention from practical statisticians who are\nconcerned with the problem of drawing inference from large-scale heterogeneous\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 19:31:00 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 00:32:32 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 18:24:47 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""], ["Wang", "Kaijun", ""]]}, {"id": "2004.09627", "submitter": "Serena Ng", "authors": "Jean-Jacques Forneron and Serena Ng", "title": "Inference by Stochastic Optimization: A Free-Lunch Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing sampling uncertainty in extremum estimation can be challenging when\nthe asymptotic variance is not analytically tractable. Bootstrap inference\noffers a feasible solution but can be computationally costly especially when\nthe model is complex. This paper uses iterates of a specially designed\nstochastic optimization algorithm as draws from which both point estimates and\nbootstrap standard errors can be computed in a single run. The draws are\ngenerated by the gradient and Hessian computed from batches of data that are\nresampled at each iteration. We show that these draws yield consistent\nestimates and asymptotically valid frequentist inference for a large class of\nregular problems. The algorithm provides accurate standard errors in simulation\nexamples and empirical applications at low computational costs. The draws from\nthe algorithm also provide a convenient way to detect data irregularities.\n", "versions": [{"version": "v1", "created": "Mon, 20 Apr 2020 20:43:28 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 15:32:06 GMT"}, {"version": "v3", "created": "Mon, 14 Sep 2020 14:28:24 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Forneron", "Jean-Jacques", ""], ["Ng", "Serena", ""]]}, {"id": "2004.09709", "submitter": "Yunpeng Zhao", "authors": "Yunpeng Zhao, Peter Bickel and Charles Weko", "title": "Identifiability and consistency of network inference using the hub model\n  and variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical network analysis primarily focuses on inferring the parameters of\nan observed network. In many applications, especially in the social sciences,\nthe observed data is the groups formed by individual subjects. In these\napplications, the network is itself a parameter of a statistical model. Zhao\nand Weko (2019) propose a model-based approach, called the hub model, to infer\nimplicit networks from grouping behavior. The hub model assumes that each\nmember of the group is brought together by a member of the group called the\nhub. The hub model belongs to the family of Bernoulli mixture models.\nIdentifiability of parameters is a notoriously difficult problem for Bernoulli\nmixture models. This paper proves identifiability of the hub model parameters\nand estimation consistency under mild conditions. Furthermore, this paper\ngeneralizes the hub model by introducing a model component that allows hubless\ngroups in which individual nodes spontaneously appear independent of any other\nindividual. We refer to this additional component as the null component. The\nnew model bridges the gap between the hub model and the degenerate case of the\nmixture model -- the Bernoulli product. Identifiability and consistency are\nalso proved for the new model. Numerical studies are provided to demonstrate\nthe theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 02:01:00 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2020 17:10:21 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2020 23:52:37 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Zhao", "Yunpeng", ""], ["Bickel", "Peter", ""], ["Weko", "Charles", ""]]}, {"id": "2004.09748", "submitter": "Timothy Molloy", "authors": "Timothy L. Molloy", "title": "Misspecified and Asymptotically Minimax Robust Quickest Change Diagnosis", "comments": "19 pages, 2 figures, Accepted for publication in IEEE Transactions on\n  Automatic Control", "journal-ref": null, "doi": "10.1109/TAC.2020.2985975", "report-no": null, "categories": "eess.SY cs.SY math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of quickly diagnosing an unknown change in a stochastic process\nis studied. We establish novel bounds on the performance of misspecified\ndiagnosis algorithms designed for changes that differ from those of the\nprocess, and pose and solve a new robust quickest change diagnosis problem in\nthe asymptotic regime of few false alarms and false isolations. Simulations\nsuggest that our asymptotically robust solution offers a computationally\nefficient alternative to generalised likelihood ratio algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 04:37:01 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Molloy", "Timothy L.", ""]]}, {"id": "2004.09833", "submitter": "Koki Shimizu", "authors": "Koki Shimizu and Hiroki Hashiguchi", "title": "Expressing the largest eigenvalue of a singular beta F-matrix with\n  heterogeneous hypergeometric functions", "comments": "The title is changed (the old title is \"The exact distribution of the\n  largest eigenvalue of a singular beta F-matrix for Roy's test\")", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the exact distribution of the largest eigenvalue of a singular\nrandom matrix for multivariate analysis of variance (MANOVA) is discussed. The\nkey to developing the distribution theory of eigenvalues of a singular random\nmatrix is to use heterogeneous hypergeometric functions with two matrix\narguments. In this study, we define the singular beta F-matrix and extend the\ndistributions of a nonsingular beta F -matrix to the singular case. We also\ngive the joint density of eigenvalues and the exact distribution of the largest\neigenvalue in terms of heterogeneous hypergeometric functions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 08:59:18 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 14:03:07 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Shimizu", "Koki", ""], ["Hashiguchi", "Hiroki", ""]]}, {"id": "2004.09881", "submitter": "Leonie Selk", "authors": "Leonie Selk, Charles Tillier and Orlando Marigliano", "title": "Multivariate boundary regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we consider a multivariate regression model with one-sided\nerrors. We assume for the regression function to lie in a general H\\\"{o}lder\nclass and estimate it via a nonparametric local polynomial approach that\nconsists of minimization of the local integral of a polynomial approximation\nlying above the data points. While the consideration of multivariate covariates\noffers an undeniable opportunity from an application-oriented standpoint, it\nrequires a new method of proof to replace the established ones for the\nunivariate case. The main purpose of this paper is to show the uniform\nconsistency and to provide the rates of convergence of the considered\nnonparametric estimator for both multivariate random covariates and\nmultivariate deterministic design points. To demonstrate the performance of the\nestimators, the small sample behavior is investigated in a simulation study in\ndimension two and three.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 10:19:34 GMT"}, {"version": "v2", "created": "Wed, 10 Feb 2021 12:36:17 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Selk", "Leonie", ""], ["Tillier", "Charles", ""], ["Marigliano", "Orlando", ""]]}, {"id": "2004.09999", "submitter": "R\\'emy Tuy\\'eras", "authors": "R\\'emy Tuy\\'eras", "title": "A category theoretical argument for causal inference", "comments": "48 pages, 1 figure; v2: typos corrected and clarifications linking\n  the results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI math.AG math.CT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to design a causal inference method accounting for\ncomplex interactions between causal factors. The proposed method relies on a\ncategory theoretical reformulation of the definitions of dependent variables,\nindependent variables and latent variables in terms of products and arrows in\nthe category of unlabeled partitions. Throughout the paper, we demonstrate how\nthe proposed method accounts for possible hidden variables, such as\nenvironmental variables or noise, and how it can be interpreted statistically\nin terms of $p$-values. This interpretation, from category theory to\nstatistics, is implemented through a collection of propositions highlighting\nthe functorial properties of ANOVA. We use these properties in combination with\nour category theoretical framework to provide solutions to causal inference\nproblems with both sound algebraic and statistical properties. As an\napplication, we show how the proposed method can be used to design a\ncombinatorial genome-wide association algorithm for the field of genetics.\n", "versions": [{"version": "v1", "created": "Thu, 9 Apr 2020 05:48:46 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 21:43:32 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Tuy\u00e9ras", "R\u00e9my", ""]]}, {"id": "2004.10243", "submitter": "Michel Ad\\`es", "authors": "Michel Ad\\`es, Matthieu Dufour, Serge B. Provost and Marie-Claude\n  Vachon", "title": "A class of copulae associated with Brownian motion processes and their\n  maxima", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main objective of this paper consists in creating a new class of copulae\nfrom various joint distributions occurring in connection with certain Brownian\nmotion processes. We focus our attention on the distributions of univariate\nBrownian motions having a drift parameter and their maxima and on correlated\nbivariate Brownian motions by considering the maximum value of one of them. The\ncopulae generated therefrom and their associated density functions are\nexplicitly given as well as graphically represented.\n", "versions": [{"version": "v1", "created": "Tue, 21 Apr 2020 19:12:21 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Ad\u00e8s", "Michel", ""], ["Dufour", "Matthieu", ""], ["Provost", "Serge B.", ""], ["Vachon", "Marie-Claude", ""]]}, {"id": "2004.10369", "submitter": "Juan Kalemkerian", "authors": "Juan Kalemkerian", "title": "Parameter Estimation for Discretely Observed Fractional Iterated\n  Ornstein--Uhlenbeck Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the theoretical results for any FOU(p) processes for the case in\nwhich the Hurst parameter is less than 1/2 and we show theoretically and by\nsimulations that under some conditions on T and the sample size n it is\npossible to obtain consistent estimators of the parameters when the process is\nobserved in a discretized and equispaced interval [0, T ]. Also we will show\nthat the FOU(p) processes can be used to model a wide range of time series\nvarying from short range dependence to large range dependence with similar\nresults as the ARMA or ARFIMA models, and in several cases outperforms those.\nLastly, we give a way to obtain explicit formulas for the auto-covariance\nfunction for any FOU(p) and we present an application for FOU(2) and FOU(3).\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 02:07:31 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2020 18:11:18 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Kalemkerian", "Juan", ""]]}, {"id": "2004.10521", "submitter": "Ezequiel Smucler", "authors": "Ezequiel Smucler, Facundo Sapienza and Andrea Rotnitzky", "title": "Efficient adjustment sets in causal graphical models with hidden\n  variables", "comments": "Fixed an error in Example 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the selection of covariate adjustment sets for estimating the value\nof point exposure dynamic policies, also known as dynamic treatment regimes,\nassuming a non-parametric causal graphical model with hidden variables, in\nwhich at least one adjustment set is fully observable. We show that recently\ndeveloped criteria, for graphs without hidden variables, to compare the\nasymptotic variance of non-parametric estimators of static policy values that\ncontrol for certain adjustment sets, are also valid under dynamic policies and\ngraphs with hidden variables. We show that there exist adjustment sets that are\noptimal minimal (minimum), in the sense of yielding estimators with the\nsmallest variance among those that control for adjustment sets that are minimal\n(of minimum cardinality). Moreover, we show that if either no variables are\nhidden or if all the observable variables are ancestors of either treatment,\noutcome, or the variables that are used to decide treatment, a globally optimal\nadjustment set exists. We provide polynomial time algorithms to compute the\nglobally optimal (when it exists), optimal minimal, and optimal minimum\nadjustment sets. Our results are based on the construction of an undirected\ngraph in which vertex cuts between the treatment and outcome variables\ncorrespond to adjustment sets. In this undirected graph, a partial order\nbetween minimal vertex cuts can be defined that makes the set of minimal cuts a\nlattice. This partial order corresponds directly to the ordering of the\nasymptotic variances of the corresponding non-parametrically adjusted\nestimators.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 12:22:01 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 14:50:19 GMT"}, {"version": "v3", "created": "Tue, 26 May 2020 15:32:54 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Smucler", "Ezequiel", ""], ["Sapienza", "Facundo", ""], ["Rotnitzky", "Andrea", ""]]}, {"id": "2004.10522", "submitter": "Lucie Perrotta", "authors": "Lucie Perrotta", "title": "Practical calibration of the temperature parameter in Gibbs posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  PAC-Bayesian algorithms and Gibbs posteriors are gaining popularity due to\ntheir robustness against model misspecification even when Bayesian inference is\ninconsistent. The PAC-Bayesian alpha-posterior is a generalization of the\nstandard Bayes posterior which can be tempered with a parameter alpha to handle\ninconsistency. Data driven methods for tuning alpha have been proposed but are\nstill few, and are often computationally heavy. Additionally, the adequacy of\nthese methods in cases where we use variational approximations instead of exact\nalpha-posteriors is not clear. This narrows their usage to simple models and\nprevents their application to large-scale problems. We hence need fast methods\nto tune alpha that work with both exact and variational alpha-posteriors.\nFirst, we propose two data driven methods for tuning alpha, based on\nsample-splitting and bootstrapping respectively. Second, we formulate the\n(exact or variational) posteriors of three popular statistical models, and\nmodify them into alpha-posteriors. For each model, we test our strategies and\ncompare them with standard Bayes and Grunwald's SafeBayes. While bootstrapping\nachieves mixed results, sample-splitting and SafeBayes perform well on the\nexact and variational alpha-posteriors we describe, and achieve better results\nthan standard Bayes in misspecified or complex models. Additionally,\nsample-splitting outperforms SafeBayes in terms of speed. Sample-splitting\noffers a fast and easy solution to inconsistency and typically performs\nsimilarly or better than Bayesian inference. Our results provide hints on the\ncalibration of alpha in PAC-Bayesian and Gibbs posteriors, and may facilitate\nusing these methods in large and complex models.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 12:23:45 GMT"}], "update_date": "2020-04-23", "authors_parsed": [["Perrotta", "Lucie", ""]]}, {"id": "2004.10708", "submitter": "Vishal Katariya", "authors": "Vishal Katariya and Mark M. Wilde", "title": "Geometric distinguishability measures limit quantum channel estimation\n  and discrimination", "comments": "170 pages, 7 figures, accepted for publication in Quantum Information\n  Processing", "journal-ref": "Quantum Information Processing vol. 20, Article no. 78, February\n  2021", "doi": "10.1007/s11128-021-02992-7", "report-no": null, "categories": "quant-ph cs.IT math-ph math.IT math.MP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum channel estimation and discrimination are fundamentally related\ninformation processing tasks of interest in quantum information science. In\nthis paper, we analyze these tasks by employing the right logarithmic\nderivative Fisher information and the geometric R\\'enyi relative entropy,\nrespectively, and we also identify connections between these distinguishability\nmeasures. A key result of our paper is that a chain-rule property holds for the\nright logarithmic derivative Fisher information and the geometric R\\'enyi\nrelative entropy for the interval $\\alpha\\in(0,1) $ of the R\\'enyi parameter\n$\\alpha$. In channel estimation, these results imply a condition for the\nunattainability of Heisenberg scaling, while in channel discrimination, they\nlead to improved bounds on error rates in the Chernoff and Hoeffding error\nexponent settings. More generally, we introduce the amortized quantum Fisher\ninformation as a conceptual framework for analyzing general sequential\nprotocols that estimate a parameter encoded in a quantum channel, and we use\nthis framework, beyond the aforementioned application, to show that Heisenberg\nscaling is not possible when a parameter is encoded in a classical-quantum\nchannel. We then identify a number of other conceptual and technical\nconnections between the tasks of estimation and discrimination and the\ndistinguishability measures involved in analyzing each. As part of this work,\nwe present a detailed overview of the geometric R\\'enyi relative entropy of\nquantum states and channels, as well as its properties, which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 17:11:34 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 14:10:38 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Katariya", "Vishal", ""], ["Wilde", "Mark M.", ""]]}, {"id": "2004.10732", "submitter": "Vurukonda Sathish", "authors": "Vurukonda Sathish, Siuli Mukhopadhyay and Rashmi Tiwari", "title": "ARMA Models for Zero Inflated Count Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero inflation is a common nuisance while monitoring disease progression over\ntime. This article proposes a new observation driven model for zero inflated\nand over-dispersed count time series. The counts given the past history of the\nprocess and available information on covariates is assumed to be distributed as\na mixture of a Poisson distribution and a distribution degenerate at zero, with\na time dependent mixing probability, $\\pi_t$. Since, count data usually suffers\nfrom overdispersion, a Gamma distribution is used to model the excess\nvariation, resulting in a zero inflated Negative Binomial (NB) regression model\nwith mean parameter $\\lambda_t$. Linear predictors with auto regressive and\nmoving average (ARMA) type terms, covariates, seasonality and trend are fitted\nto $\\lambda_t$ and $\\pi_t$ through canonical link generalized linear models.\nEstimation is done using maximum likelihood aided by iterative algorithms, such\nas Newton Raphson (NR) and Expectation and Maximization (EM). Theoretical\nresults on the consistency and asymptotic normality of the estimators are\ngiven. The proposed model is illustrated using in-depth simulation studies and\na dengue data set.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 17:34:30 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 10:25:20 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 13:56:26 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Sathish", "Vurukonda", ""], ["Mukhopadhyay", "Siuli", ""], ["Tiwari", "Rashmi", ""]]}, {"id": "2004.10818", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus, Arnold Janssen and Markus Pauly", "title": "Permutation inference in factorial survival designs with the CASANOVA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose inference procedures for general nonparametric factorial survival\ndesigns with possibly right-censored data. Similar to additive Aalen models,\nnull hypotheses are formulated in terms of cumulative hazards. Thereby,\ndeviations are measured in terms of quadratic forms in Nelson-Aalen-type\nintegrals. Different to existing approaches this allows to work without\nrestrictive model assumptions as proportional hazards. In particular, crossing\nsurvival or hazard curves can be detected without a significant loss of power.\nFor a distribution-free application of the method, a permutation strategy is\nsuggested. The resulting procedures' asymptotic validity as well as their\nconsistency are proven and their small sample performances are analyzed in\nextensive simulations. Their applicability is finally illustrated by analyzing\nan oncology data set.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 19:35:44 GMT"}, {"version": "v2", "created": "Wed, 24 Jun 2020 11:46:53 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ditzhaus", "Marc", ""], ["Janssen", "Arnold", ""], ["Pauly", "Markus", ""]]}, {"id": "2004.10910", "submitter": "Lourdes Montenegro Mrs.", "authors": "Mariana C. Ara\\'ujo, Audrey H.M.A. Cysneiros and Lourdes C. Montenegro", "title": "Bartlett and Bartlett-type corrections in heteroscedastic symmetric\n  nonlinear regression models", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides general expression for Bartlett and Bartlett-type\ncorrection factors for the likelihood ratio and gradient statistics to test the\ndispersion parameter in heteroscedastic symmetric nonlinear models. This class\nof regression models is potentially useful for modeling data containing\noutlying observations. We consider a partition on the dispersion parameter\nvector in order to test the parameters of interest. Furthermore, we develop\nMonte Carlo simulations to compare the finite sample performances of the\ncorrected tests proposed with the usual and modified score tests, likelihood\nand gradient tests, the Bartlett-type corrected score test and bootstrap\ncorrected tests. Our simulation results favor the score and gradient corrected\ntests as well as the bootstrap tests. An empirical application is presented for\nillustrative purposes.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 00:29:21 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Ara\u00fajo", "Mariana C.", ""], ["Cysneiros", "Audrey H. M. A.", ""], ["Montenegro", "Lourdes C.", ""]]}, {"id": "2004.10922", "submitter": "Yandi Shen", "authors": "Yandi Shen, Qiyang Han and Fang Han", "title": "On a phase transition in general order spline regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Gaussian sequence model $Y= \\theta_0 + \\varepsilon$ in $\\mathbb{R}^n$,\nwe study the fundamental limit of approximating the signal $\\theta_0$ by a\nclass $\\Theta(d,d_0,k)$ of (generalized) splines with free knots. Here $d$ is\nthe degree of the spline, $d_0$ is the order of differentiability at each inner\nknot, and $k$ is the maximal number of pieces. We show that, given any integer\n$d\\geq 0$ and $d_0\\in\\{-1,0,\\ldots,d-1\\}$, the minimax rate of estimation over\n$\\Theta(d,d_0,k)$ exhibits the following phase transition: \\begin{equation*}\n\\begin{aligned} \\inf_{\\widetilde{\\theta}}\\sup_{\\theta\\in\\Theta(d,d_0,\nk)}\\mathbb{E}_\\theta\\|\\widetilde{\\theta} - \\theta\\|^2 \\asymp_d \\begin{cases}\nk\\log\\log(16n/k), & 2\\leq k\\leq k_0,\\\\ k\\log(en/k), & k \\geq k_0+1. \\end{cases}\n\\end{aligned} \\end{equation*} The transition boundary $k_0$, which takes the\nform $\\lfloor{(d+1)/(d-d_0)\\rfloor} + 1$, demonstrates the critical role of the\nregularity parameter $d_0$ in the separation between a faster $\\log \\log(16n)$\nand a slower $\\log(en)$ rate. We further show that, once encouraging an\nadditional '$d$-monotonicity' shape constraint (including monotonicity for $d =\n0$ and convexity for $d=1$), the above phase transition is eliminated and the\nfaster $k\\log\\log(16n/k)$ rate can be achieved for all $k$. These results\nprovide theoretical support for developing $\\ell_0$-penalized\n(shape-constrained) spline regression procedures as useful alternatives to\n$\\ell_1$- and $\\ell_2$-penalized ones.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 01:16:57 GMT"}, {"version": "v2", "created": "Thu, 7 May 2020 03:12:39 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Shen", "Yandi", ""], ["Han", "Qiyang", ""], ["Han", "Fang", ""]]}, {"id": "2004.11094", "submitter": "Alec Koppel", "authors": "Alec Koppel, Hrusikesha Pradhan, Ketan Rajawat", "title": "Consistent Online Gaussian Process Regression Without the Sample\n  Complexity Bottleneck", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes provide a framework for nonlinear nonparametric Bayesian\ninference widely applicable across science and engineering. Unfortunately,\ntheir computational burden scales cubically with the training sample size,\nwhich in the case that samples arrive in perpetuity, approaches infinity. This\nissue necessitates approximations for use with streaming data, which to date\nmostly lack convergence guarantees. Thus, we develop the first online Gaussian\nprocess approximation that preserves convergence to the population posterior,\ni.e., asymptotic posterior consistency, while ameliorating its intractable\ncomplexity growth with the sample size. We propose an online compression scheme\nthat, following each a posteriori update, fixes an error neighborhood with\nrespect to the Hellinger metric centered at the current posterior, and greedily\ntosses out past kernel dictionary elements until its boundary is hit. We call\nthe resulting method Parsimonious Online Gaussian Processes (POG). For\ndiminishing error radius, exact asymptotic consistency is preserved (Theorem\n1(i)) at the cost of unbounded memory in the limit. On the other hand, for\nconstant error radius, POG converges to a neighborhood of the population\nposterior (Theorem 1(ii))but with finite memory at-worst determined by the\nmetric entropy of the feature space (Theorem 2). Experimental results are\npresented on several nonlinear regression problems which illuminates the merits\nof this approach as compared with alternatives that fix the subspace dimension\ndefining the history of past points.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 11:52:06 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 15:49:51 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Koppel", "Alec", ""], ["Pradhan", "Hrusikesha", ""], ["Rajawat", "Ketan", ""]]}, {"id": "2004.11234", "submitter": "Juan-Pablo Ortega", "authors": "Lukas Gonon, Lyudmila Grigoryeva, and Juan-Pablo Ortega", "title": "Memory and forecasting capacities of nonlinear recurrent networks", "comments": "27 pages, 1 figure. To appear in Physica D", "journal-ref": null, "doi": "10.1016/j.physd.2020.132721", "report-no": null, "categories": "math.OC cs.LG cs.NE math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of memory capacity, originally introduced for echo state and\nlinear networks with independent inputs, is generalized to nonlinear recurrent\nnetworks with stationary but dependent inputs. The presence of dependence in\nthe inputs makes natural the introduction of the network forecasting capacity,\nthat measures the possibility of forecasting time series values using network\nstates. Generic bounds for memory and forecasting capacities are formulated in\nterms of the number of neurons of the nonlinear recurrent network and the\nautocovariance function or the spectral density of the input. These bounds\ngeneralize well-known estimates in the literature to a dependent inputs setup.\nFinally, for the particular case of linear recurrent networks with independent\ninputs it is proved that the memory capacity is given by the rank of the\nassociated controllability matrix, a fact that has been for a long time assumed\nto be true without proof by the community.\n", "versions": [{"version": "v1", "created": "Wed, 22 Apr 2020 15:10:51 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2020 10:53:00 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Gonon", "Lukas", ""], ["Grigoryeva", "Lyudmila", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "2004.11241", "submitter": "Hossein-Ali Mohtashami-Borzadaran", "authors": "H. A. Mohtashami-Borzadaran, M. Amini, H. Jabbari and A. Dolati", "title": "Marshall-Olkin exponential shock model covering all range of dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new Marshall-Olkin exponential shock model. The\nnew construction method gives the proposed model further ability to allocate\nthe common joint shock on each of the components, making it suitable for\napplication in fields like reliability and credit risk. The given model has a\nsingular part and supports both positive and negative dependence structure.\nMain dependence properties of the model is given and an analysis of\nstress-strength is presented. After a performance analysis on the estimator of\nparameters, a real data is studied. Finally, we give the multivariate version\nof the proposed model and its main properties.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 15:29:04 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2020 15:56:49 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Mohtashami-Borzadaran", "H. A.", ""], ["Amini", "M.", ""], ["Jabbari", "H.", ""], ["Dolati", "A.", ""]]}, {"id": "2004.11354", "submitter": "Wanli Qiao", "authors": "Wanli Qiao", "title": "Asymptotic Confidence Regions for Density Ridges", "comments": "46 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop large sample theory including nonparametric confidence regions for\n$r$-dimensional ridges of probability density functions on $\\mathbb{R}^d$,\nwhere $1\\leq r<d$. We view ridges as the intersections of level sets of some\nspecial functions. The vertical variation of the plug-in kernel estimators for\nthese functions constrained on the ridges is used as the measure of maximal\ndeviation for ridge estimation. Our confidence regions for the ridges are\ndetermined by the asymptotic distribution of this maximal deviation, which is\nestablished by utilizing the extreme value distribution of nonstationary\n$\\chi$-fields indexed by manifolds.\n", "versions": [{"version": "v1", "created": "Thu, 23 Apr 2020 17:54:14 GMT"}], "update_date": "2020-04-24", "authors_parsed": [["Qiao", "Wanli", ""]]}, {"id": "2004.11615", "submitter": "Guillaume Basse", "authors": "Kevin Guo and Guillaume Basse", "title": "The Generalized Oaxaca-Blinder Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After performing a randomized experiment, researchers often use\nordinary-least squares (OLS) regression to adjust for baseline covariates when\nestimating the average treatment effect. It is widely known that the resulting\nconfidence interval is valid even if the linear model is misspecified. In this\npaper, we generalize that conclusion to covariate adjustment with nonlinear\nmodels. We introduce an intuitive way to use any \"simple\" nonlinear model to\nconstruct a covariate-adjusted confidence interval for the average treatment\neffect. The confidence interval derives its validity from randomization alone,\nand when nonlinear models fit the data better than linear models, it is\nnarrower than the usual interval from OLS adjustment.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 09:32:18 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Guo", "Kevin", ""], ["Basse", "Guillaume", ""]]}, {"id": "2004.11638", "submitter": "Thierry Denoeux", "authors": "Thierry Denoeux", "title": "Belief functions induced by random fuzzy sets: A general framework for\n  representing uncertain and fuzzy evidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit Zadeh's notion of \"evidence of the second kind\" and show that it\nprovides the foundation for a general theory of epistemic random fuzzy sets,\nwhich generalizes both the Dempster-Shafer theory of belief functions and\npossibility theory. In this perspective, Dempster-Shafer theory deals with\nbelief functions generated by random sets, while possibility theory deals with\nbelief functions induced by fuzzy sets. The more general theory allows us to\nrepresent and combine evidence that is both uncertain and fuzzy. We demonstrate\nthe application of this formalism to statistical inference, and show that it\nmakes it possible to reconcile the possibilistic interpretation of likelihood\nwith Bayesian inference.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 10:14:54 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 14:48:02 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Denoeux", "Thierry", ""]]}, {"id": "2004.11734", "submitter": "Jules Depersin", "authors": "Jules Depersin", "title": "Robust subgaussian estimation with VC-dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Median-of-means (MOM) based procedures provide non-asymptotic and strong\ndeviation bounds even when data are heavy-tailed and/or corrupted. This work\nproposes a new general way to bound the excess risk for MOM estimators. The\ncore technique is the use of VC-dimension (instead of Rademacher complexity) to\nmeasure the statistical complexity. In particular, this allows to give the\nfirst robust estimators for sparse estimation which achieves the so-called\nsubgaussian rate only assuming a finite second moment for the uncorrupted data.\nBy comparison, previous works using Rademacher complexities required a number\nof finite moments that grows logarithmically with the dimension. With this\ntechnique, we derive new robust sugaussian bounds for mean estimation in any\nnorm. We also derive a new robust estimator for covariance estimation that is\nthe first to achieve subgaussian bounds without $L_4-L_2$ norm equivalence.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 13:21:09 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 08:59:18 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 16:16:13 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Depersin", "Jules", ""]]}, {"id": "2004.11770", "submitter": "Alfred M\\\"uller", "authors": "Carole Bernard, Alfred M\\\"uller", "title": "Dependence uncertainty bounds for the energy score and the multivariate\n  Gini mean difference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The energy distance and energy scores became important tools in multivariate\nstatistics and multivariate probabilistic forecasting in recent years. They are\nboth based on the expected distance of two independent samples. In this paper\nwe study dependence uncertainty bounds for these quantities under the\nassumption that we know the marginals but do not know the dependence structure.\nWe find some interesting sharp analytic bounds, where one of them is obtained\nfor an unusual spherically symmetric copula. These results should help to\nbetter understand the sensitivity of these measures to misspecifications in the\ncopula.\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 14:23:02 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Bernard", "Carole", ""], ["M\u00fcller", "Alfred", ""]]}, {"id": "2004.11878", "submitter": "Gunnar Taraldsen", "authors": "Gunnar Taraldsen", "title": "Fiducial Symmetry in Action", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry is key in classical and modern physics. A striking example is\nconservation of energy as a consequence of time-shift invariance from Noether's\ntheorem. Symmetry is likewise a key element in statistics, which, as also\nphysics, provide models for real world phenomena. Sufficiency, conditionality,\nand invariance are examples of basic principles. Galili and Meilijson (2016)\nand Mandel (2020) illustrate the first two principles very nicely by\nconsidering the scaled uniform model. We illustrate the third principle by\nproviding further results which give optimal inference for the scaled uniform\nby symmetry considerations. The proofs are simplified by relying on fiducial\narguments as initiated by Fisher (1930).\n  Keywords: Data generating equation; Optimal equivariant estimate; Scale\nfamily; Conditionality principle; Minimal sufficient; Uniform distribution;\n", "versions": [{"version": "v1", "created": "Fri, 24 Apr 2020 17:41:29 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Taraldsen", "Gunnar", ""]]}, {"id": "2004.12019", "submitter": "Niladri Chatterji", "authors": "Niladri S. Chatterji, Philip M. Long", "title": "Finite-sample Analysis of Interpolating Linear Classifiers in the\n  Overparameterized Regime", "comments": "Corrected typographical errors from the previous version of this\n  paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove bounds on the population risk of the maximum margin algorithm for\ntwo-class linear classification. For linearly separable training data, the\nmaximum margin algorithm has been shown in previous work to be equivalent to a\nlimit of training with logistic loss using gradient descent, as the training\nerror is driven to zero. We analyze this algorithm applied to random data\nincluding misclassification noise. Our assumptions on the clean data include\nthe case in which the class-conditional distributions are standard normal\ndistributions. The misclassification noise may be chosen by an adversary,\nsubject to a limit on the fraction of corrupted labels. Our bounds show that,\nwith sufficient over-parameterization, the maximum margin algorithm trained on\nnoisy data can achieve nearly optimal population risk.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 00:06:18 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2020 05:46:13 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 21:45:53 GMT"}, {"version": "v4", "created": "Tue, 1 Jun 2021 18:03:02 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Chatterji", "Niladri S.", ""], ["Long", "Philip M.", ""]]}, {"id": "2004.12182", "submitter": "Sebastian Engelke", "authors": "Sebastian Engelke and Jevgenijs Ivanovs", "title": "Sparse Structures for Multivariate Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extreme value statistics provides accurate estimates for the small occurrence\nprobabilities of rare events. While theory and statistical tools for univariate\nextremes are well-developed, methods for high-dimensional and complex data sets\nare still scarce. Appropriate notions of sparsity and connections to other\nfields such as machine learning, graphical models and high-dimensional\nstatistics have only recently been established. This article reviews the new\ndomain of research concerned with the detection and modeling of sparse patterns\nin rare events. We first describe the different forms of extremal dependence\nthat can arise between the largest observations of a multivariate random\nvector. We then discuss the current research topics including clustering,\nprincipal component analysis and graphical modeling for extremes.\nIdentification of groups of variables which can be concomitantly extreme is\nalso addressed. The methods are illustrated with an application to flood risk\nassessment.\n", "versions": [{"version": "v1", "created": "Sat, 25 Apr 2020 16:32:21 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 06:12:07 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Engelke", "Sebastian", ""], ["Ivanovs", "Jevgenijs", ""]]}, {"id": "2004.12293", "submitter": "Yichen Zhu", "authors": "Yichen Zhu, Cheng Li and David B. Dunson", "title": "Classification Trees for Imbalanced and Sparse Data: Surface-to-Volume\n  Regularization", "comments": "Submitted to Journal of American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification algorithms face difficulties when one or more classes have\nlimited training data. We are particularly interested in classification trees,\ndue to their interpretability and flexibility. When data are limited in one or\nmore of the classes, the estimated decision boundaries are often irregularly\nshaped due to the limited sample size, leading to poor generalization error. We\npropose a novel approach that penalizes the Surface-to-Volume Ratio (SVR) of\nthe decision set, obtaining a new class of SVR-Tree algorithms. We develop a\nsimple and computationally efficient implementation while proving estimation\nconsistency for SVR-Tree and rate of convergence for an idealized empirical\nrisk minimizer of SVR-Tree. SVR-Tree is compared with multiple algorithms that\nare designed to deal with imbalance through real data applications.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 06:22:47 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 05:41:40 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhu", "Yichen", ""], ["Li", "Cheng", ""], ["Dunson", "David B.", ""]]}, {"id": "2004.12364", "submitter": "Holger Dette", "authors": "Holger Dette, Kevin Kokot", "title": "Efficient tests for bio-equivalence in functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing the equivalence of functional parameters\n(such as the mean or variance function) in the two sample functional data\nproblem. In contrast to previous work, which reduces the functional problem to\na multiple testing problem for the equivalence of scalar data by comparing the\nfunctions at each point, our approach is based on an estimate of a distance\nmeasuring the maximum deviation between the two functional parameters.\nEquivalence is claimed if the estimate for the maximum deviation does not\nexceed a given threshold. A bootstrap procedure is proposed to obtain quantiles\nfor the distribution of the test statistic and consistency of the corresponding\ntest is proved in the large sample scenario. As the methods proposed here avoid\nthe use of the intersection-union principle they are less conservative and more\npowerful than the currently available methodology.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 12:30:16 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dette", "Holger", ""], ["Kokot", "Kevin", ""]]}, {"id": "2004.12491", "submitter": "Roberto Vila Gabriel", "authors": "R. Vila, L. Ferreira, H. Saulo, F. Prataviera and E.M.M. Ortega", "title": "A bimodal gamma distribution: Properties, regression model and\n  applications", "comments": "26 pages, 13 figures. Accepted for publication in Statistics: A\n  Journal of Theoretical and Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a bimodal gamma distribution using a quadratic\ntransformation based on the alpha-skew-normal model. We discuss several\nproperties of this distribution such as mean, variance, moments, hazard rate\nand entropy measures. Further, we propose a new regression model with censored\ndata based on the bimodal gamma distribution. This regression model can be very\nuseful to the analysis of real data and could give more realistic fits than\nother special regression models. Monte Carlo simulations were performed to\ncheck the bias in the maximum likelihood estimation. The proposed models are\napplied to two real data sets found in literature.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 22:17:52 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 18:27:52 GMT"}], "update_date": "2020-05-08", "authors_parsed": [["Vila", "R.", ""], ["Ferreira", "L.", ""], ["Saulo", "H.", ""], ["Prataviera", "F.", ""], ["Ortega", "E. M. M.", ""]]}, {"id": "2004.12493", "submitter": "Philip Dawid", "authors": "A. Philip Dawid", "title": "Decision-theoretic foundations for statistical causality", "comments": "44 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a mathematical and interpretative foundation for the enterprise of\ndecision-theoretic statistical causality (DT), which is a straightforward way\nof representing and addressing causal questions. DT reframes causal inference\nas \"assisted decision-making\", and aims to understand when, and how, I can make\nuse of external data, typically observational, to help me solve a decision\nproblem by taking advantage of assumed relationships between the data and my\nproblem.\n  The relationships embodied in any representation of a causal problem require\ndeeper justification, which is necessarily context-dependent. Here we clarify\nthe considerations needed to support applications of the DT methodology.\nExchangeability considerations are used to structure the required\nrelationships, and a distinction drawn between intention to treat and\nintervention to treat forms the basis for the enabling condition of\n\"ignorability\". We also show how the DT perspective unifies and sheds light on\nother popular formalisations of statistical causality, including potential\nresponses and directed acyclic graphs.\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 22:34:12 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dawid", "A. Philip", ""]]}, {"id": "2004.12496", "submitter": "Amit Levi", "authors": "Xi Chen, Rajesh Jayaram, Amit Levi, Erik Waingarten", "title": "Learning and Testing Junta Distributions with Subcube Conditioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problems of learning and testing junta distributions on\n$\\{-1,1\\}^n$ with respect to the uniform distribution, where a distribution $p$\nis a $k$-junta if its probability mass function $p(x)$ depends on a subset of\nat most $k$ variables. The main contribution is an algorithm for finding\nrelevant coordinates in a $k$-junta distribution with subcube conditioning\n[BC18, CCKLW20]. We give two applications:\n  1. An algorithm for learning $k$-junta distributions with\n$\\tilde{O}(k/\\epsilon^2) \\log n + O(2^k/\\epsilon^2)$ subcube conditioning\nqueries, and\n  2. An algorithm for testing $k$-junta distributions with $\\tilde{O}((k +\n\\sqrt{n})/\\epsilon^2)$ subcube conditioning queries.\n  All our algorithms are optimal up to poly-logarithmic factors.\n  Our results show that subcube conditioning, as a natural model for accessing\nhigh-dimensional distributions, enables significant savings in learning and\ntesting junta distributions compared to the standard sampling model. This\naddresses an open question posed by Aliakbarpour, Blais, and Rubinfeld [ABR17].\n", "versions": [{"version": "v1", "created": "Sun, 26 Apr 2020 22:52:53 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Chen", "Xi", ""], ["Jayaram", "Rajesh", ""], ["Levi", "Amit", ""], ["Waingarten", "Erik", ""]]}, {"id": "2004.12639", "submitter": "Mervyn Silvapulle", "authors": "Svetlana Litvinova and Mervyn J. Silvapulle", "title": "Consistency of full-sample bootstrap for estimating high-quantile, tail\n  probability, and tail index", "comments": "Main paper is 12 pages long and contains 2 figures. The Supplementary\n  Materials containing the proofs is 20 pages long", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the full-sample bootstrap is asymptotically valid for\nconstructing confidence intervals for high-quantiles, tail probabilities, and\nother tail parameters of a univariate distribution. This resolves the doubts\nthat have been raised about the validity of such bootstrap methods. In our\nextensive simulation study, the overall performance of the bootstrap method was\nbetter than that of the standard asymptotic method, indicating that the\nbootstrap method is at least as good, if not better than, the asymptotic method\nfor inference. This paper also lays the foundation for developing bootstrap\nmethods for inference about tail events in multivariate statistics; this is\nparticularly important because some of the non-bootstrap methods are complex.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 08:37:40 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Litvinova", "Svetlana", ""], ["Silvapulle", "Mervyn J.", ""]]}, {"id": "2004.12680", "submitter": "Geoffrey Wolfer", "authors": "Doron Cohen, Aryeh Kontorovich, Geoffrey Wolfer", "title": "Learning discrete distributions with infinite support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach to estimating discrete distributions with\n(potentially) infinite support in the total variation metric. In a departure\nfrom the established paradigm, we make no structural assumptions whatsoever on\nthe sampling distribution. In such a setting, distribution-free risk bounds are\nimpossible, and the best one could hope for is a fully empirical data-dependent\nbound. We derive precisely such bounds, and demonstrate that these are, in a\nwell-defined sense, the best possible. Our main discovery is that the half-norm\nof the empirical distribution provides tight upper and lower estimates on the\nempirical risk. Furthermore, this quantity decays at a nearly optimal rate as a\nfunction of the true distribution. The optimality follows from a minimax\nresult, of possible independent interest. Additional structural results are\nprovided, including an exact Rademacher complexity calculation and apparently a\nfirst connection between the total variation risk and the missing mass.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 09:59:22 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2020 20:28:20 GMT"}, {"version": "v3", "created": "Thu, 15 Oct 2020 14:31:14 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Cohen", "Doron", ""], ["Kontorovich", "Aryeh", ""], ["Wolfer", "Geoffrey", ""]]}, {"id": "2004.12714", "submitter": "Sandra Schluttenhofer", "authors": "Sandra Schluttenhofer, Jan Johannes", "title": "Minimax testing and quadratic functional estimation for circular\n  convolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a circular convolution model, we aim to infer on the density of a circular\nrandom variable using observations contaminated by an additive measurement\nerror. We highlight the interplay of the two problems: optimal testing and\nquadratic functional estimation. Under general regularity assumptions, we\ndetermine an upper bound for the minimax risk of estimation for the quadratic\nfunctional. The upper bound consists of two terms, one that mimics a classical\nbias-variance trade-off and a second that causes the typical elbow effect in\nquadratic functional estimation. Using a minimax optimal estimator of the\nquadratic functional as a test statistic, we derive an upper bound for the\nnonasymptotic minimax radius of testing for nonparametric alternatives.\nInterestingly, the term causing the elbow effect in the estimation case\nvanishes in the radius of testing. We provide a matching lower bound for the\ntesting problem. By showing that any lower bound for the testing problem also\nyields a lower bound for the quadratic functional estimation problem, we obtain\na lower bound for the risk of estimation. Lastly, we prove a matching lower\nbound for the term causing the elbow effect in the estimation problem. The\nresults are illustrated considering Sobolev spaces and ordinary or super smooth\nerror densities.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 11:23:12 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Schluttenhofer", "Sandra", ""], ["Johannes", "Jan", ""]]}, {"id": "2004.12716", "submitter": "Guy Nason Prof.", "authors": "Rebecca Killick, Marina I. Knight, Guy P. Nason and Idris A. Eckley", "title": "The Local Partial Autocorrelation Function and Some Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical regular and partial autocorrelation functions are powerful\ntools for stationary time series modelling and analysis. However, it is\nincreasingly recognized that many time series are not stationary and the use of\nclassical global autocorrelations can give misleading answers. This article\nintroduces two estimators of the local partial autocorrelation function and\nestablishes their asymptotic properties. The article then illustrates the use\nof these new estimators on both simulated and real time series. The examples\nclearly demonstrate the strong practical benefits of local estimators for time\nseries that exhibit nonstationarities.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 11:32:05 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Killick", "Rebecca", ""], ["Knight", "Marina I.", ""], ["Nason", "Guy P.", ""], ["Eckley", "Idris A.", ""]]}, {"id": "2004.12736", "submitter": "Peter Kevei", "authors": "Peter Kevei and Lillian Oluoch and Laszlo Viharos", "title": "Limit laws for the norms of extremal samples", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let denote $S_n(p) = k_n^{-1} \\sum_{i=1}^{k_n} \\left( \\log (X_{n+1-i,n} /\nX_{n-k_n, n}) \\right)^p$, where $p > 0$, $k_n \\leq n$ is a sequence of integers\nsuch that $k_n \\to \\infty$ and $k_n / n \\to 0$, and $X_{1,n} \\leq \\ldots \\leq\nX_{n,n}$ is the order statistics of iid random variables with regularly varying\nupper tail. The estimator $\\widehat \\gamma(n) = (S_n(p)/\\Gamma(p+1))^{1/p}$ is\nan extension of the Hill estimator. We investigate the asymptotic properties of\n$S_n(p)$ and $\\widehat \\gamma(n)$ both for fixed $p > 0$ and for $p = p_n \\to\n\\infty$. We prove strong consistency and asymptotic normality under appropriate\nassumptions. Applied to real data we find that for larger $p$ the estimator is\nless sensitive to the change in $k_n$ than the Hill estimator.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 12:17:54 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kevei", "Peter", ""], ["Oluoch", "Lillian", ""], ["Viharos", "Laszlo", ""]]}, {"id": "2004.12787", "submitter": "Chanchal Kundu", "authors": "Chanchal Kundu", "title": "On Cumulative Residual (Past) Extropy of Extreme Order Statistics", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent information-theoretic literature, the concept of extropy has\nbeen studied for order statistics. In the present communication we consider a\ncumulative analogue of extropy in the same vein of cumulative residual (past)\nentropy and study it in context with extreme order statistics. A dynamic\nversion of cumulative residual (past) extropy for smallest (largest) order\nstatistic is also studied here. It is shown that the proposed measures (and\ntheir dynamic versions) of extreme order statistics determine the distribution\nuniquely. Some characterizations of the generalized Pareto and power\ndistributions, which are commonly used in reliability modeling, are given.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 13:33:58 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Kundu", "Chanchal", ""]]}, {"id": "2004.12842", "submitter": "Christian Berg", "authors": "Christian Berg", "title": "A unified view of space-time covariance functions through Gelfand pairs", "comments": "26 pages. New references. To appear in J. Fourier Anal. Appl", "journal-ref": null, "doi": "10.1007/s00041-020-09793-z", "report-no": null, "categories": "math.CA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a characterization of positive definite integrable functions on a\nproduct of two Gelfand pairs as an integral of positive definite functions on\none of the Gelfand pairs with respect to the Plancherel measure on the dual of\nthe other Gelfand pair.\n  In the very special case where the Gelfand pairs are Euclidean groups and the\ncompact subgroups are reduced to the identity, the characterization is a much\ncited result in spatio-temporal statistics due to Cressie, Huang and Gneiting.\n  When one of the Gelfand pairs is compact the characterization leads to\nresults about expansions in spherical functions with positive definite\nexpansion functions, thereby recovering recent results of the author in\ncollaboration with Peron and Porcu. In the special case when the compact\nGelfand pair consists of orthogonal groups, the characterization is important\nin geostatistics and covers a recent result of Porcu and White.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 14:50:21 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 10:13:20 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Berg", "Christian", ""]]}, {"id": "2004.12932", "submitter": "Nestor Parolya Dr.", "authors": "Taras Bodnar and Nestor Parolya", "title": "Spectral analysis of large reflexive generalized inverse and\n  Moore-Penrose inverse matrices", "comments": "13 pages, 1 figure, a letter/short article", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reflexive generalized inverse and the Moore-Penrose inverse are often\nconfused in statistical literature but in fact they have completely different\nbehaviour in case the population covariance matrix is not a multiple of\nidentity. In this paper, we study the spectral properties of a reflexive\ngeneralized inverse and of the Moore-Penrose inverse of the sample covariance\nmatrix. The obtained results are used to assess the difference in the\nasymptotic behaviour of their eigenvalues.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 16:45:20 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Bodnar", "Taras", ""], ["Parolya", "Nestor", ""]]}, {"id": "2004.13151", "submitter": "Hajo Holzmann", "authors": "Isaia Albisetti, Fadoua Balabdaoui and Hajo Holzmann", "title": "Testing for spherical and elliptical symmetry", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct new testing procedures for spherical and elliptical symmetry\nbased on the characterization that a random vector $X$ with finite mean has a\nspherical distribution if and only if $\\Ex[u^\\top X | v^\\top X] = 0$ holds for\nany two perpendicular vectors $u$ and $v$. Our test is based on the\nKolmogorov-Smirnov statistic, and its rejection region is found via the\nspherically symmetric bootstrap. We show the consistency of the spherically\nsymmetric bootstrap test using a general Donsker theorem which is of some\nindependent interest. For the case of testing for elliptical symmetry, the\nKolmogorov-Smirnov statistic has an asymptotic drift term due to the estimated\nlocation and scale parameters. Therefore, an additional standardization is\nrequired in the bootstrap procedure. In a simulation study, the size and the\npower properties of our tests are assessed for several distributions and the\nperformance is compared to that of several competing procedures.\n", "versions": [{"version": "v1", "created": "Mon, 27 Apr 2020 20:15:41 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Albisetti", "Isaia", ""], ["Balabdaoui", "Fadoua", ""], ["Holzmann", "Hajo", ""]]}, {"id": "2004.13537", "submitter": "Miklos Z. Racz", "authors": "Miklos Z. Racz, Anirudh Sridhar", "title": "Correlated randomly growing graphs", "comments": "62 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DM cs.SI math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new model of correlated randomly growing graphs and study the\nfundamental questions of detecting correlation and estimating aspects of the\ncorrelated structure. The model is simple and starts with any model of randomly\ngrowing graphs, such as uniform attachment (UA) or preferential attachment\n(PA). Given such a model, a pair of graphs $(G_1, G_2)$ is grown in two stages:\nuntil time $t_{\\star}$ they are grown together (i.e., $G_1 = G_2$), after which\nthey grow independently according to the underlying growth model.\n  We show that whenever the seed graph has an influence in the underlying graph\ngrowth model---this has been shown for PA and UA trees and is conjectured to\nhold broadly---then correlation can be detected in this model, even if the\ngraphs are grown together for just a single time step. We also give a general\nsufficient condition (which holds for PA and UA trees) under which detection is\npossible with probability going to $1$ as $t_{\\star} \\to \\infty$. Finally, we\nshow for PA and UA trees that the amount of correlation, measured by\n$t_{\\star}$, can be estimated with vanishing relative error as $t_{\\star} \\to\n\\infty$.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 14:09:19 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Racz", "Miklos Z.", ""], ["Sridhar", "Anirudh", ""]]}, {"id": "2004.13779", "submitter": "Piotr Zwiernik", "authors": "David Rossell and Piotr Zwiernik", "title": "Dependence in elliptical partial correlation graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian model equips strong properties that facilitate studying and\ninterpreting graphical models. Specifically it reduces conditional independence\nand the study of positive association to determining partial correlations and\ntheir signs. When Gaussianity does not hold partial correlation graphs are a\nuseful relaxation of graphical models, but it is not clear what information\nthey contain (besides the obvious lack of linear association). We study\nelliptical and transelliptical distributions as middle-ground between the\nGaussian and other families that are more flexible but either do not embed\nstrong properties or do not lead to simple interpretation. We characterize the\nmeaning of zero partial correlations in the elliptical family and\ntranselliptical copula models and show that it retains much of the dependence\nstructure from the Gaussian case. Regarding positive dependence, we prove\nimpossibility results to learn (trans)elliptical graphical models, including\nthat an elliptical distribution that is multivariate totally positive of order\ntwo for all dimensions must be essentially Gaussian. We then show how to\ninterpret positive partial correlations as a relaxation, and obtain important\nproperties related to faithfulness and Simpson's paradox. We illustrate the\ntranselliptical model potential to study tail dependence in S&P500 data, and of\npositivity to improve regularized inference.\n", "versions": [{"version": "v1", "created": "Tue, 28 Apr 2020 19:14:56 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 10:29:20 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Rossell", "David", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "2004.13953", "submitter": "Chien-Ming Chi", "authors": "Chien-Ming Chi, Patrick Vossler, Yingying Fan and Jinchi Lv", "title": "Asymptotic Properties of High-Dimensional Random Forests", "comments": "63 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a flexible nonparametric learning tool, random forests has been widely\napplied to various real applications with appealing empirical performance, even\nin the presence of high-dimensional feature space. Unveiling the underlying\nmechanisms has led to some important recent theoretical results on the\nconsistency of the random forests algorithm and its variants. However, to our\nknowledge, all existing works concerning random forests consistency under the\nsetting of high dimensionality were done for various modified random forests\nmodels where the splitting rules are independent of the response. In light of\nthis, in this paper we derive the consistency rates for the random forests\nalgorithm associated with the sample CART splitting criterion, which is the one\nused in the original version of the algorithm in Breiman (2001), in a general\nhigh-dimensional nonparametric regression setting through a bias-variance\ndecomposition analysis. Our new theoretical results show that random forests\ncan indeed adapt to high dimensionality and allow for discontinuous regression\nfunction. Our bias analysis characterizes explicitly how the random forests\nbias depends on the sample size, tree height, and column subsampling parameter.\nSome limitations of our current results are also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 04:23:37 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 05:36:00 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chi", "Chien-Ming", ""], ["Vossler", "Patrick", ""], ["Fan", "Yingying", ""], ["Lv", "Jinchi", ""]]}, {"id": "2004.13967", "submitter": "Subhadra Dasgupta", "authors": "Subhadra Dasgupta, Siuli Mukhopadhyay and Jonathan Keith", "title": "Optimal designs for some bivariate cokriging models", "comments": "37 Pages, 1 figure. Page 26 and 27 typo corrected for calculating the\n  risk functions. Page 27 typo in Table 3 header", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article focuses on the estimation and design aspects of a bivariate\ncollocated cokriging experiment. For a large class of covariance matrices a\nlinear dependency criterion is identified, which allows the best linear\nunbiased estimator of the primary variable in a bivariate collocated cokriging\nsetup to reduce to a univariate kriging estimator. Exact optimal designs for\nefficient prediction for such simple and ordinary cokriging models, with one\ndimensional inputs are determined. Designs are found by minimizing the maximum\nand integrated prediction variance. For simple and ordinary cokriging models\nwith known covariance parameters, the equispaced design is shown to be optimal\nfor both criterion functions. The more realistic scenario of unknown covariance\nparameters is addressed by assuming prior distributions on the parameter\nvector, thus adopting a Bayesian approach to the design problem. The equispaced\ndesign is proved to be the Bayesian optimal design for both criteria. The work\nis motivated by designing an optimal water monitoring system for an Indian\nriver.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 06:03:44 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 10:19:54 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 06:01:46 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Dasgupta", "Subhadra", ""], ["Mukhopadhyay", "Siuli", ""], ["Keith", "Jonathan", ""]]}, {"id": "2004.13998", "submitter": "Yozo Tonaki", "authors": "Yozo Tonaki, Yusuke Kaino, Masayuki Uchida", "title": "Adaptive tests for parameter changes in ergodic diffusion processes from\n  discrete observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the adaptive test for the parameter change in discretely observed\nergodic diffusion processes based on the cusum test. Using two test statistics\nbased on the two quasi-log likelihood functions of the diffusion parameter and\nthe drift parameter, we perform the change point tests for both diffusion and\ndrift parameters of the diffusion process. It is shown that the test statistics\nhave the limiting distribution of the sup of the norm of a Brownian bridge.\nSimulation results are illustrated for the 1-dimensional Ornstein-Uhlenbeck\nprocess.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 07:38:02 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Tonaki", "Yozo", ""], ["Kaino", "Yusuke", ""], ["Uchida", "Masayuki", ""]]}, {"id": "2004.14330", "submitter": "Bryant Davis", "authors": "Bryant Davis, James P. Hobert", "title": "On the convergence complexity of Gibbs samplers for a family of simple\n  Bayesian random effects models", "comments": "Revision resubmitted to Methodology and Computing in Applied\n  Probability, 34 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of big data has led to so-called convergence complexity\nanalysis, which is the study of how Markov chain Monte Carlo (MCMC) algorithms\nbehave as the sample size, $n$, and/or the number of parameters, $p$, in the\nunderlying data set increase. This type of analysis is often quite challenging,\nin part because existing results for fixed $n$ and $p$ are simply not sharp\nenough to yield good asymptotic results. One of the first convergence\ncomplexity results for an MCMC algorithm on a continuous state space is due to\nYang and Rosenthal (2019), who established a mixing time result for a Gibbs\nsampler (for a simple Bayesian random effects model) that was introduced and\nstudied by Rosenthal (1996). The asymptotic behavior of the spectral gap of\nthis Gibbs sampler is, however, still unknown. We use a recently developed\nsimulation technique (Qin et. al., 2019) to provide substantial numerical\nevidence that the gap is bounded away from 0 as $n \\rightarrow \\infty$. We also\nestablish a pair of rigorous convergence complexity results for two different\nGibbs samplers associated with a generalization of the random effects model\nconsidered by Rosenthal (1996). Our results show that, under strong regularity\nconditions, the spectral gaps of these Gibbs samplers converge to 1 as the\nsample size increases.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 16:57:49 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 17:30:48 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Davis", "Bryant", ""], ["Hobert", "James P.", ""]]}, {"id": "2004.14441", "submitter": "Marco Scutari", "authors": "Tjebbe Bodewes and Marco Scutari", "title": "Learning Bayesian Networks from Incomplete Data with the Node-Average\n  Likelihood", "comments": "27 pages, 5 figures", "journal-ref": "Proceedings of Machine Learning Research (138, PGM 2020), 29-40", "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network (BN) structure learning from complete data has been\nextensively studied in the literature. However, fewer theoretical results are\navailable for incomplete data, and most are related to the\nExpectation-Maximisation (EM) algorithm. Balov (2013) proposed an alternative\napproach called Node-Average Likelihood (NAL) that is competitive with EM but\ncomputationally more efficient; and he proved its consistency and model\nidentifiability for discrete BNs.\n  In this paper, we give general sufficient conditions for the consistency of\nNAL; and we prove consistency and identifiability for conditional Gaussian BNs,\nwhich include discrete and Gaussian BNs as special cases. Furthermore, we\nconfirm our results and the results in Balov (2013) with an independent\nsimulation study. Hence we show that NAL has a much wider applicability than\noriginally implied in Balov (2013), and that it is competitive with EM for\nconditional Gaussian BNs as well.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 19:20:17 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 09:12:28 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 08:14:30 GMT"}, {"version": "v4", "created": "Thu, 13 May 2021 15:10:38 GMT"}, {"version": "v5", "created": "Thu, 22 Jul 2021 15:51:20 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Bodewes", "Tjebbe", ""], ["Scutari", "Marco", ""]]}, {"id": "2004.14455", "submitter": "Florian Sch\\\"afer", "authors": "Florian Sch\\\"afer, Matthias Katzfuss, and Houman Owhadi", "title": "Sparse Cholesky factorization by Kullback-Leibler minimization", "comments": "The code used to run the numerical experiments can be found under\n  https://github.com/f-t-s/cholesky_by_KL_minimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.OC math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to compute a sparse approximate inverse Cholesky factor $L$ of a\ndense covariance matrix $\\Theta$ by minimizing the Kullback-Leibler divergence\nbetween the Gaussian distributions $\\mathcal{N}(0, \\Theta)$ and $\\mathcal{N}(0,\nL^{-\\top} L^{-1})$, subject to a sparsity constraint. Surprisingly, this\nproblem has a closed-form solution that can be computed efficiently, recovering\nthe popular Vecchia approximation in spatial statistics. Based on recent\nresults on the approximate sparsity of inverse Cholesky factors of $\\Theta$\nobtained from pairwise evaluation of Green's functions of elliptic\nboundary-value problems at points $\\{x_{i}\\}_{1 \\leq i \\leq N} \\subset\n\\mathbb{R}^{d}$, we propose an elimination ordering and sparsity pattern that\nallows us to compute $\\epsilon$-approximate inverse Cholesky factors of such\n$\\Theta$ in computational complexity $\\mathcal{O}(N \\log(N/\\epsilon)^d)$ in\nspace and $\\mathcal{O}(N \\log(N/\\epsilon)^{2d})$ in time. To the best of our\nknowledge, this is the best asymptotic complexity for this class of problems.\nFurthermore, our method is embarrassingly parallel, automatically exploits\nlow-dimensional structure in the data, and can perform Gaussian-process\nregression in linear (in $N$) space complexity. Motivated by the optimality\nproperties of our methods, we propose methods for applying it to the joint\ncovariance of training and prediction points in Gaussian-process regression,\ngreatly improving stability and computational cost. Finally, we show how to\napply our method to the important setting of Gaussian processes with additive\nnoise, sacrificing neither accuracy nor computational complexity.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 20:04:53 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 18:55:32 GMT"}], "update_date": "2020-11-03", "authors_parsed": [["Sch\u00e4fer", "Florian", ""], ["Katzfuss", "Matthias", ""], ["Owhadi", "Houman", ""]]}, {"id": "2004.14497", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy", "title": "Optimal doubly robust estimation of heterogeneous causal effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous effect estimation plays a crucial role in causal inference,\nwith applications across medicine and social science. Many methods for\nestimating conditional average treatment effects (CATEs) have been proposed in\nrecent years, but there are important theoretical gaps in understanding if and\nwhen such methods are optimal. This is especially true when the CATE has\nnontrivial structure (e.g., smoothness or sparsity). Our work contributes in\nseveral main ways. First, we study a two-stage doubly robust CATE estimator and\ngive a generic model-free error bound, which, despite its generality, yields\nsharper results than those in the current literature. We apply the bound to\nderive error rates in nonparametric models with smoothness or sparsity, and\ngive sufficient conditions for oracle efficiency. Underlying our error bound is\na general oracle inequality for regression with estimated or imputed outcomes,\nwhich is of independent interest; this is the second main contribution. The\nthird contribution is aimed at understanding the fundamental statistical limits\nof CATE estimation. To that end, we propose and study a local polynomial\nadaptation of double-residual regression. We show that this estimator can be\noracle efficient under even weaker conditions, if used with a specialized form\nof sample splitting and careful choices of tuning parameters. These are the\nweakest conditions currently found in the literature, and we conjecture that\nthey are minimal in a minimax sense. We go on to give error bounds in the\nnon-trivial regime where oracle rates cannot be achieved. Some finite-sample\nproperties are explored with simulations.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 22:02:59 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 05:53:11 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Kennedy", "Edward H.", ""]]}, {"id": "2004.14502", "submitter": "Guy Martial Nkiet", "authors": "Emmanuel de Dieu Nkou and Guy Martial Nkiet", "title": "Wavelet-based estimation in a semiparametric regression model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a wavelet-based method for estimating the EDR\nspace in Li's semiparametric regression model for achieving dimension\nreduction. This method is obtained by using linear wavelet estimators of the\ndensity and regression functions that are involved in the covariance matrix of\nconditional expectation whose spectral analysis gives the EDR directions. Then,\nconsistency of the proposed estimators is proved. A simulation study that allow\none to evaluate the performance of the proposal with comparison to existing\nmethods is presented.\n", "versions": [{"version": "v1", "created": "Wed, 29 Apr 2020 22:18:10 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 11:32:39 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Nkou", "Emmanuel de Dieu", ""], ["Nkiet", "Guy Martial", ""]]}, {"id": "2004.14521", "submitter": "Robert Bassett", "authors": "Robert Bassett and Julio Deride", "title": "One-Step Estimation With Scaled Proximal Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study statistical estimators computed using iterative optimization methods\nthat are not run until completion. Classical results on maximum likelihood\nestimators (MLEs) assert that a one-step estimator (OSE), in which a single\nNewton-Raphson iteration is performed from a starting point with certain\nproperties, is asymptotically equivalent to the MLE. We further develop these\nearly-stopping results by deriving properties of one-step estimators defined by\na single iteration of scaled proximal methods. Our main results show the\nasymptotic equivalence of the likelihood-based estimator and various one-step\nestimators defined by scaled proximal methods. By interpreting OSEs as the last\nof a sequence of iterates, our results provide insight on scaling numerical\ntolerance with sample size. Our setting contains scaled proximal gradient\ndescent applied to certain composite models as a special case, making our\nresults applicable to many problems of practical interest. Additionally, our\nresults provide support for the utility of the scaled Moreau envelope as a\nstatistical smoother by interpreting scaled proximal descent as a quasi-Newton\nmethod applied to the scaled Moreau envelope.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 00:06:54 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 17:33:53 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 19:44:17 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Bassett", "Robert", ""], ["Deride", "Julio", ""]]}, {"id": "2004.14531", "submitter": "Xiaodong Li", "authors": "Lihua Lei, Xiaodong Li, and Xingmei Lou", "title": "Consistency of Spectral Clustering on Hierarchical Stochastic Block\n  Models", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generic network model, based on the Stochastic Block Model, to\nstudy the hierarchy of communities in real-world networks, under which the\nconnection probabilities are structured in a binary tree. Under the network\nmodel, we show that the eigenstructure of the expected unnormalized graph\nLaplacian reveals the community structure of the network as well as the\nhierarchy of communities in a recursive fashion. Inspired by the nice property\nof the population eigenstructure, we develop a recursive bi-partitioning\nalgorithm that divides the network into two communities based on the Fiedler\nvector of the unnormalized graph Laplacian and repeats the split until a\nstopping rule indicates no further community structures. We prove the weak and\nstrong consistency of our algorithm for sparse networks with the expected node\ndegree in $O(\\log n)$ order, based on newly developed theory on\n$\\ell_{2\\rightarrow\\infty}$ eigenspace perturbation, without knowing the total\nnumber of communities in advance. Unlike most of existing work, our theory\ncovers multi-scale networks where the connection probabilities may differ in\norder of magnitude, which comprise an important class of models that are\npractically relevant but technically challenging to deal with. Finally we\ndemonstrate the performance of our algorithm on synthetic data and real-world\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 01:08:59 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Lei", "Lihua", ""], ["Li", "Xiaodong", ""], ["Lou", "Xingmei", ""]]}, {"id": "2004.14681", "submitter": "Dylan Foster", "authors": "Dylan J. Foster, Alexander Rakhlin, Tuhin Sarkar", "title": "Learning nonlinear dynamical systems from a single trajectory", "comments": "To appear at L4DC 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce algorithms for learning nonlinear dynamical systems of the form\n$x_{t+1}=\\sigma(\\Theta^{\\star}x_t)+\\varepsilon_t$, where $\\Theta^{\\star}$ is a\nweight matrix, $\\sigma$ is a nonlinear link function, and $\\varepsilon_t$ is a\nmean-zero noise process. We give an algorithm that recovers the weight matrix\n$\\Theta^{\\star}$ from a single trajectory with optimal sample complexity and\nlinear running time. The algorithm succeeds under weaker statistical\nassumptions than in previous work, and in particular i) does not require a\nbound on the spectral norm of the weight matrix $\\Theta^{\\star}$ (rather, it\ndepends on a generalization of the spectral radius) and ii) enjoys guarantees\nfor non-strictly-increasing link functions such as the ReLU. Our analysis has\ntwo key components: i) we give a general recipe whereby global stability for\nnonlinear dynamical systems can be used to certify that the state-vector\ncovariance is well-conditioned, and ii) using these tools, we extend well-known\nalgorithms for efficiently learning generalized linear models to the dependent\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 10:42:48 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Foster", "Dylan J.", ""], ["Rakhlin", "Alexander", ""], ["Sarkar", "Tuhin", ""]]}, {"id": "2004.14728", "submitter": "Randolf Altmeyer", "authors": "Randolf Altmeyer, Igor Cialenco, Gregor Pasemann", "title": "Parameter estimation for semilinear SPDEs from local measurements", "comments": "corrected version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work contributes to the limited literature on estimating the diffusivity\nor drift coefficient of nonlinear SPDEs driven by additive noise. Assuming that\nthe solution is measured locally in space and over a finite time interval, we\nshow that the augmented maximum likelihood estimator introduced in Altmeyer,\nReiss (2020) retains its asymptotic properties when used for semilinear SPDEs\nthat satisfy some abstract, and verifiable, conditions. The proofs of\nasymptotic results are based on splitting the solution in linear and nonlinear\nparts and fine regularity properties in $L^p$-spaces. The obtained general\nresults are applied to particular classes of equations, including stochastic\nreaction-diffusion equations. The stochastic Burgers equation, as an example\nwith first order nonlinearity, is an interesting borderline case of the general\nresults, and is treated by a Wiener chaos expansion. We conclude with numerical\nexamples that validate the theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 12:34:01 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 18:53:08 GMT"}], "update_date": "2020-05-19", "authors_parsed": [["Altmeyer", "Randolf", ""], ["Cialenco", "Igor", ""], ["Pasemann", "Gregor", ""]]}, {"id": "2004.14824", "submitter": "Mats Julius Stensrud", "authors": "Mats J. Stensrud, Miguel A. Hern\\'an, Eric J. Tchetgen Tchetgen, James\n  M. Robins, Vanessa Didelez, Jessica G. Young", "title": "Generalized interpretation and identification of separable effects in\n  competing event settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In competing event settings, a counterfactual contrast of cause-specific\ncumulative incidences quantifies the total causal effect of a treatment on the\nevent of interest. However, effects of treatment on the competing event may\nindirectly contribute to this total effect, complicating its interpretation. We\npreviously proposed the separable effects (Stensrud et al, 2019) to define\ndirect and indirect effects of the treatment on the event of interest. This\ndefinition presupposes a treatment decomposition into two components acting\nalong two separate causal pathways, one exclusively outside of the competing\nevent and the other exclusively through it. Unlike previous definitions of\ndirect and indirect effects, the separable effects can be subject to empirical\nscrutiny in a study where separate interventions on the treatment components\nare available. Here we extend and generalize the notion of the separable\neffects in several ways, allowing for interpretation, identification and\nestimation under considerably weaker assumptions. We propose and discuss a\ndefinition of separable effects that is applicable to general time-varying\nstructures, where the separable effects can still be meaningfully interpreted,\neven when they cannot be regarded as direct and indirect effects. We further\nderive weaker conditions for identification of separable effects in\nobservational studies where decomposed treatments are not yet available; in\nparticular, these conditions allow for time-varying common causes of the event\nof interest, the competing events and loss to follow-up. For these general\nsettings, we propose semi-parametric weighted estimators that are\nstraightforward to implement. As an illustration, we apply the estimators to\nstudy the separable effects of intensive blood pressure therapy on acute kidney\ninjury, using data from a randomized clinical trial.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 14:32:29 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 02:47:07 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Stensrud", "Mats J.", ""], ["Hern\u00e1n", "Miguel A.", ""], ["Tchetgen", "Eric J. Tchetgen", ""], ["Robins", "James M.", ""], ["Didelez", "Vanessa", ""], ["Young", "Jessica G.", ""]]}, {"id": "2004.14954", "submitter": "Guang Cheng", "authors": "Ruiqi Liu, Zuofeng Shang, Guang Cheng", "title": "On Deep Instrumental Variables Estimate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The endogeneity issue is fundamentally important as many empirical\napplications may suffer from the omission of explanatory variables, measurement\nerror, or simultaneous causality. Recently, \\cite{hllt17} propose a \"Deep\nInstrumental Variable (IV)\" framework based on deep neural networks to address\nendogeneity, demonstrating superior performances than existing approaches. The\naim of this paper is to theoretically understand the empirical success of the\nDeep IV. Specifically, we consider a two-stage estimator using deep neural\nnetworks in the linear instrumental variables model. By imposing a latent\nstructural assumption on the reduced form equation between endogenous variables\nand instrumental variables, the first-stage estimator can automatically capture\nthis latent structure and converge to the optimal instruments at the minimax\noptimal rate, which is free of the dimension of instrumental variables and thus\nmitigates the curse of dimensionality. Additionally, in comparison with\nclassical methods, due to the faster convergence rate of the first-stage\nestimator, the second-stage estimator has {a smaller (second order) estimation\nerror} and requires a weaker condition on the smoothness of the optimal\ninstruments. Given that the depth and width of the employed deep neural network\nare well chosen, we further show that the second-stage estimator achieves the\nsemiparametric efficiency bound. Simulation studies on synthetic data and\napplication to automobile market data confirm our theory.\n", "versions": [{"version": "v1", "created": "Thu, 30 Apr 2020 17:03:00 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Liu", "Ruiqi", ""], ["Shang", "Zuofeng", ""], ["Cheng", "Guang", ""]]}]