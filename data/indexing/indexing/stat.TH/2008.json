[{"id": "2008.00043", "submitter": "Benjamin Hollering", "authors": "Jane Ivy Coons, Joseph Cummings, Benjamin Hollering, Aida Maraj", "title": "Generalized Cut Polytopes for Binary Hierarchical Models", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal polytopes are important geometric objects that arise in statistics\nas the polytopes underlying hierarchical log-linear models. These polytopes can\nbe used to answer geometric questions about these models, such as determining\nthe existence of maximum likelihood estimates or the normality of the\nassociated semigroup. Cut polytopes of graphs have been useful in analyzing\nbinary marginal polytopes in the case where the simplicial complex underlying\nthe hierarchical model is a graph. We introduce a generalized cut polytope that\nis isomorphic to the binary marginal polytope of an arbitrary simplicial\ncomplex via a generalized covariance map. This polytope is full dimensional in\nits ambient space and has a natural switching operation among its facets that\ncan be used to deduce symmetries between the facets of the correlation and\nbinary marginal polytopes. We find complete H-representations of the\ngeneralized cut polytope for some important families of simplicial complexes.\nWe also compute the volume of these polytopes in some instances.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jul 2020 19:12:59 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Coons", "Jane Ivy", ""], ["Cummings", "Joseph", ""], ["Hollering", "Benjamin", ""], ["Maraj", "Aida", ""]]}, {"id": "2008.00130", "submitter": "Chuancun Yin", "authors": "Yeshunying Wang, Chuancun Yin", "title": "A New Class of Multivariate Elliptically Contoured Distributions with\n  Inconsistency Property", "comments": "34 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of multivariate elliptically symmetric distributions\nincluding elliptically symmetric logistic distributions and Kotz type\ndistributions. We investigate the various probabilistic properties including\nmarginal distributions, conditional distributions, linear transformations,\ncharacteristic functions and dependence measure in the perspective of the\ninconsistency property. In addition, we provide a real data example to show\nthat the new distributions have reasonable flexibility.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 00:17:56 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Wang", "Yeshunying", ""], ["Yin", "Chuancun", ""]]}, {"id": "2008.00242", "submitter": "Anand Dixit", "authors": "Anand Dixit and Vivekananda Roy", "title": "Posterior Impropriety of some Sparse Bayesian Learning Models", "comments": "13 pages", "journal-ref": "Statistics and Probability Letters, 171: 109039 (2021)", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Bayesian learning models are typically used for prediction in datasets\nwith significantly greater number of covariates than observations. Such models\noften take a reproducing kernel Hilbert space (RKHS) approach to carry out the\ntask of prediction and can be implemented using either proper or improper\npriors. In this article we show that a few sparse Bayesian learning models in\nthe literature, when implemented using improper priors, lead to improper\nposteriors.\n", "versions": [{"version": "v1", "created": "Sat, 1 Aug 2020 10:58:02 GMT"}, {"version": "v2", "created": "Sat, 19 Jun 2021 07:48:51 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Dixit", "Anand", ""], ["Roy", "Vivekananda", ""]]}, {"id": "2008.00520", "submitter": "Matteo Marsili", "authors": "Cl\\'elia de Mulatier, Paolo P. Mazza, Matteo Marsili", "title": "Statistical Inference of Minimally Complex Models", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.ST physics.data-an q-bio.QM stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding the best model that describes a high dimensional dataset, is a\ndaunting task. For binary data, we show that this becomes feasible, if the\nsearch is restricted to simple models. These models -- that we call Minimally\nComplex Models (MCMs) -- are simple because they are composed of independent\ncomponents of minimal complexity, in terms of description length. Simple models\nare easy to infer and to sample from. In addition, model selection within the\nMCMs' class is invariant with respect to changes in the representation of the\ndata. They portray the structure of dependencies among variables in a simple\nway. They provide robust predictions on dependencies and symmetries, as\nillustrated in several examples. MCMs may contain interactions between\nvariables of any order. So, for example, our approach reveals whether a dataset\nis appropriately described by a pairwise interaction model.\n", "versions": [{"version": "v1", "created": "Sun, 2 Aug 2020 16:57:02 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["de Mulatier", "Cl\u00e9lia", ""], ["Mazza", "Paolo P.", ""], ["Marsili", "Matteo", ""]]}, {"id": "2008.00683", "submitter": "Agustin G. Nogales", "authors": "A.G. Nogales", "title": "On Bayesian Estimation of Densities and Sampling Distributions: the\n  Posterior Predictive Distribution as the Bayes Estimator", "comments": "Corrected some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimality results for two outstanding Bayesian estimation problems are given\nin this paper: the estimation of the sampling distribution for the squared\ntotal variation function and the estimation of the density for the\n$L^1$-squared loss function. The posterior predictive distribution provides the\nsolution to these problems. Some examples are presented to illustrate it. The\nBayesian estimation problem of a distribution function is also addressed.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 07:26:51 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 07:55:56 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Nogales", "A. G.", ""]]}, {"id": "2008.00847", "submitter": "Dmytro Marushkevych Mr", "authors": "Gabriela Ciolek, Dmytro Marushkevych, Mark Podolskij", "title": "On Dantzig and Lasso estimators of the drift in a high dimensional\n  Ornstein-Uhlenbeck model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present new theoretical results for the Dantzig and Lasso\nestimators of the drift in a high dimensional Ornstein-Uhlenbeck model under\nsparsity constraints. Our focus is on oracle inequalities for both estimators\nand error bounds with respect to several norms. In the context of the Lasso\nestimator our paper is strongly related to [11], who investigated the same\nproblem under row sparsity. We improve their rates and also prove the\nrestricted eigenvalue property solely under ergodicity assumption on the model.\nFinally, we demonstrate a numerical analysis to uncover the finite sample\nperformance of the Dantzig and Lasso estimators.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:52:09 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Ciolek", "Gabriela", ""], ["Marushkevych", "Dmytro", ""], ["Podolskij", "Mark", ""]]}, {"id": "2008.00848", "submitter": "Tahani Coolen-Maturi Dr", "authors": "Tahani Coolen-Maturi and Frank P.A. Coolen", "title": "A monotonicity property of weighted log-rank tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logrank test is a well-known nonparametric test which is often used to\ncompare the survival distributions of two samples including right censored\nobservations, it is also known as the Mantel-Haenszel test. The $G^{\\rho}$\nfamily of tests, introduced by Harrington and Fleming (1982), generalizes the\nlogrank test by using weights assigned to observations. In this paper, we\npresent a monotonicity property for the $G^{\\rho}$ family of tests, which was\nmotivated by the need to derive bounds for the test statistic in case of\nimprecise data observations.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 12:53:50 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Coolen-Maturi", "Tahani", ""], ["Coolen", "Frank P. A.", ""]]}, {"id": "2008.01006", "submitter": "Se Yoon Lee", "authors": "Se Yoon Lee", "title": "Gibbs sampler and coordinate ascent variational inference: a\n  set-theoretical review", "comments": null, "journal-ref": null, "doi": "10.1080/03610926.2021.1921214", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental problems in Bayesian statistics is the approximation\nof the posterior distribution. Gibbs sampler and coordinate ascent variational\ninference are renownedly utilized approximation techniques that rely on\nstochastic and deterministic approximations. In this paper, we define\nfundamental sets of densities frequently used in Bayesian inference. We shall\nbe concerned with the clarification of the two schemes from the set-theoretical\npoint of view. This new way provides an alternative mechanism for analyzing the\ntwo schemes endowed with pedagogical insights.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 16:43:34 GMT"}, {"version": "v2", "created": "Thu, 20 Aug 2020 21:58:04 GMT"}, {"version": "v3", "created": "Sat, 5 Sep 2020 03:42:08 GMT"}, {"version": "v4", "created": "Sun, 29 Nov 2020 03:17:36 GMT"}, {"version": "v5", "created": "Wed, 10 Mar 2021 06:02:35 GMT"}, {"version": "v6", "created": "Thu, 15 Apr 2021 17:16:04 GMT"}, {"version": "v7", "created": "Sun, 2 May 2021 22:14:53 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lee", "Se Yoon", ""]]}, {"id": "2008.01036", "submitter": "Lin Chen", "authors": "Lin Chen, Yifei Min, Mikhail Belkin, Amin Karbasi", "title": "Multiple Descent: Design Your Own Generalization Curve", "comments": "Improved presentation of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the generalization loss of linear regression in variably\nparameterized families of models, both under-parameterized and\nover-parameterized. We show that the generalization curve can have an arbitrary\nnumber of peaks, and moreover, locations of those peaks can be explicitly\ncontrolled. Our results highlight the fact that both classical U-shaped\ngeneralization curve and the recently observed double descent curve are not\nintrinsic properties of the model family. Instead, their emergence is due to\nthe interaction between the properties of the data and the inductive biases of\nlearning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 17:22:21 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 16:38:35 GMT"}, {"version": "v3", "created": "Thu, 22 Oct 2020 17:25:39 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2020 18:06:02 GMT"}, {"version": "v5", "created": "Tue, 9 Feb 2021 00:45:00 GMT"}, {"version": "v6", "created": "Tue, 1 Jun 2021 17:03:22 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Chen", "Lin", ""], ["Min", "Yifei", ""], ["Belkin", "Mikhail", ""], ["Karbasi", "Amin", ""]]}, {"id": "2008.01125", "submitter": "Iosif Pinelis", "authors": "Iosif Pinelis", "title": "Monotonicity properties of the Poisson approximation to the binomial\n  distribution", "comments": "10 pages; to appear in Statistics and Probability Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Certain monotonicity properties of the Poisson approximation to the binomial\ndistribution are established. As a natural application of these results, exact\n(rather than approximate) tests of hypotheses on an unknown value of the\nparameter $p$ of the binomial distribution are presented.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 18:37:25 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Pinelis", "Iosif", ""]]}, {"id": "2008.01130", "submitter": "Kolyan Ray", "authors": "Kolyan Ray, Aad van der Vaart", "title": "On the Bernstein-von Mises theorem for the Dirichlet process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish that Laplace transforms of the posterior Dirichlet process\nconverge to those of the limiting Brownian bridge process in a neighbourhood\nabout zero, uniformly over Glivenko-Cantelli function classes. For real-valued\nrandom variables and functions of bounded variation, we strengthen this result\nto hold for all real numbers. This last result is proved via an explicit strong\napproximation coupling inequality.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 18:49:33 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 18:24:04 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Ray", "Kolyan", ""], ["van der Vaart", "Aad", ""]]}, {"id": "2008.01153", "submitter": "Stefan Steinerberger", "authors": "Stefan Steinerberger", "title": "Using Expander Graphs to test whether samples are i.i.d", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this note is to point out that the theory of expander graphs\nleads to an interesting test whether $n$ real numbers $x_1, \\dots, x_n$ could\nbe $n$ independent samples of a random variable. To any distinct, real numbers\n$x_1, \\dots, x_n$, we associate a 4-regular graph $G$ as follows: using $\\pi$\nto denote the permutation ordering the elements, $x_{\\pi(1)} < x_{\\pi(2)} <\n\\dots < x_{\\pi(n)}$, we build a graph on $\\left\\{1, \\dots, n\\right\\}$ by\nconnecting $i$ and $i+1$ (cyclically) and $\\pi(i)$ and $\\pi(i+1)$ (cyclically).\nIf the numbers are i.i.d. samples, then a result of Friedman implies that $G$\nis close to Ramanujan. This suggests a test for whether these numbers are\ni.i.d: compute the second largest (in absolute value) eigenvalue of the\nadjacency matrix. The larger $\\lambda - 2\\sqrt{3}$, the less likely it is for\nthe numbers to be i.i.d. We explain why this is a reasonable test and give many\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 19:43:01 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Steinerberger", "Stefan", ""]]}, {"id": "2008.01244", "submitter": "Subhashis Ghosal", "authors": "Moumita Chakraborty and Subhashis Ghosal", "title": "Convergence Rates for Bayesian Estimation and Testing in Monotone\n  Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape restrictions such as monotonicity on functions often arise naturally in\nstatistical modeling.\n  We consider a Bayesian approach to the problem of estimation of a monotone\nregression function and testing for monotonicity. We construct a prior\ndistribution using piecewise constant functions. For estimation, a prior\nimposing monotonicity of the heights of these steps is sensible, but the\nresulting posterior is harder to analyze theoretically. We consider a\n``projection-posterior'' approach, where a conjugate normal prior is used, but\nthe monotonicity constraint is imposed on posterior samples by a projection map\non the space of monotone functions. We show that the resulting posterior\ncontracts at the optimal rate $n^{-1/3}$ under the $L_1$-metric and at a nearly\noptimal rate under the empirical $L_p$-metrics for $0<p\\le 2$. The\nprojection-posterior approach is also computationally more convenient. We also\nconstruct a Bayesian test for the hypothesis of monotonicity using the\nposterior probability of a shrinking neighborhood of the set of monotone\nfunctions. We show that the resulting test has a universal consistency property\nand obtain the separation rate which ensures that the resulting power function\napproaches one.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 23:45:09 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Chakraborty", "Moumita", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "2008.01245", "submitter": "Alexander Cloninger", "authors": "Alexander Cloninger, Hrushikesh Mhaskar", "title": "Cautious Active Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of classification of points sampled from an unknown\nprobability measure on a Euclidean space. We study the question of querying the\nclass label at a very small number of judiciously chosen points so as to be\nable to attach the appropriate class label to every point in the set. Our\napproach is to consider the unknown probability measure as a convex combination\nof the conditional probabilities for each class. Our technique involves the use\nof a highly localized kernel constructed from Hermite polynomials, in order to\ncreate a hierarchical estimate of the supports of the constituent probability\nmeasures. We do not need to make any assumptions on the nature of any of the\nprobability measures nor know in advance the number of classes involved. We\ngive theoretical guarantees measured by the $F$-score for our classification\nscheme. Examples include classification in hyper-spectral images and MNIST\nclassification.\n", "versions": [{"version": "v1", "created": "Mon, 3 Aug 2020 23:47:31 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 03:41:49 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Cloninger", "Alexander", ""], ["Mhaskar", "Hrushikesh", ""]]}, {"id": "2008.01304", "submitter": "Naoki Hayashi", "authors": "Naoki Hayashi", "title": "The Exact Asymptotic Form of Bayesian Generalization Error in Latent\n  Dirichlet Allocation", "comments": "20 pages, 3 figures, 2 tables. Accepted at Neural Networks (Elsevier)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet allocation (LDA) obtains essential information from data by\nusing Bayesian inference. It is applied to knowledge discovery via dimension\nreducing and clustering in many fields. However, its generalization error had\nnot been yet clarified since it is a singular statistical model where there is\nno one-to-one mapping from parameters to probability distributions. In this\npaper, we give the exact asymptotic form of its generalization error and\nmarginal likelihood, by theoretical analysis of its learning coefficient using\nalgebraic geometry. The theoretical result shows that the Bayesian\ngeneralization error in LDA is expressed in terms of that in matrix\nfactorization and a penalty from the simplex restriction of LDA's parameter\nregion. A numerical experiment is consistent to the theoretical result.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 03:26:16 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2021 15:49:34 GMT"}], "update_date": "2021-01-26", "authors_parsed": [["Hayashi", "Naoki", ""]]}, {"id": "2008.01312", "submitter": "Yuetian Luo", "authors": "Yuetian Luo, Rungang Han and Anru R. Zhang", "title": "A Schatten-$q$ Matrix Perturbation Theory via Perturbation Projection\n  Error Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper studies the Schatten-$q$ error of low-rank matrix estimation by\nsingular value decomposition under perturbation. Specifically, we establish a\ntight perturbation bound on the low-rank matrix estimation via a perturbation\nprojection error bound. This new proof technique has provable advantages over\nthe classic approaches. Then, we establish lower bounds to justify the\ntightness of the upper bound on the low-rank matrix estimation error. Based on\nthe matrix perturbation projection error bound, we further develop a unilateral\nand a user-friendly sin$\\Theta$ bound for singular subspace perturbation.\nFinally, we demonstrate the advantage of our results over the ones in the\nliterature by simulation.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 03:54:27 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2020 03:44:49 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Luo", "Yuetian", ""], ["Han", "Rungang", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2008.01375", "submitter": "Hongsong Yuan", "authors": "Fengnan Gao, Zongming Ma, Hongsong Yuan", "title": "Community detection in sparse latent space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a simple community detection algorithm originated from\nstochastic blockmodel literature achieves consistency, and even optimality, for\na broad and flexible class of sparse latent space models. The class of models\nincludes latent eigenmodels (arXiv:0711.1146). The community detection\nalgorithm is based on spectral clustering followed by local refinement via\nnormalized edge counting.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 07:12:15 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Gao", "Fengnan", ""], ["Ma", "Zongming", ""], ["Yuan", "Hongsong", ""]]}, {"id": "2008.01720", "submitter": "Sudhansu Sekhar Maiti", "authors": "Indrani Mukherjee, Sudhansu S. Maiti and Rama Shanker", "title": "On estimation of the PMF and the CDF of a natural discrete one parameter\n  polynomial exponential distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a new natural discrete analog of the one parameter\npolynomial exponential(OPPE) distribution as a mixture of a number of negative\nbinomial distributions has been proposed and is called as a natural discrete\none parameter polynomial exponential (NDOPPE) distribution. This distribution\nis a generalized version of natural discrete Lindley (NDL) distribution,\nproposed and studied by Ahmed and Afify (2019). Two estimators viz., MLE and\nUMVUE of the PMF and the CDF of a NDOPPE distribution have been derived. The\nestimators have been compared with respect to their MSEs. Simulation study has\nbeen conducted to verify the consistency of the estimators. A real data\nillustration has been reported.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 17:51:33 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Mukherjee", "Indrani", ""], ["Maiti", "Sudhansu S.", ""], ["Shanker", "Rama", ""]]}, {"id": "2008.01724", "submitter": "Bingyan Wang", "authors": "Yuxin Chen, Jianqing Fan, Bingyan Wang, Yuling Yan", "title": "Convex and Nonconvex Optimization Are Both Minimax-Optimal for Noisy\n  Blind Deconvolution under Random Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG eess.SP math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the effectiveness of convex relaxation and nonconvex\noptimization in solving bilinear systems of equations under two different\ndesigns (i.e.$~$a sort of random Fourier design and Gaussian design). Despite\nthe wide applicability, the theoretical understanding about these two paradigms\nremains largely inadequate in the presence of random noise. The current paper\nmakes two contributions by demonstrating that: (1) a two-stage nonconvex\nalgorithm attains minimax-optimal accuracy within a logarithmic number of\niterations. (2) convex relaxation also achieves minimax-optimal statistical\naccuracy vis-\\`a-vis random noise. Both results significantly improve upon the\nstate-of-the-art theoretical guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 4 Aug 2020 17:57:02 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 01:56:10 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Chen", "Yuxin", ""], ["Fan", "Jianqing", ""], ["Wang", "Bingyan", ""], ["Yan", "Yuling", ""]]}, {"id": "2008.01939", "submitter": "Amine Amimour", "authors": "Amine Amimour and Karima Belaide", "title": "A long memory time series with a periodic degree of fractional\n  differencing", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a periodic version of a time varying parameter\nfractional process in the stationary region. It is a partial extension of\nHosking (1981)'s article which dealt with the case where the coefficients are\ninvariant in time. We will describe the probabilistic theories of this periodic\nmodel. The results are followed by a graphical representation of the\nautocovariances functions.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 05:17:42 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Amimour", "Amine", ""], ["Belaide", "Karima", ""]]}, {"id": "2008.02048", "submitter": "Lolian Shtembari", "authors": "Lolian Shtembari and Allen Caldwell", "title": "On the sum of ordered spacings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide the analytic forms of the distributions for the sum of ordered\nspacings. We do this both for the case where the boundaries are included in the\ncalculation of the spacings and the case where they are excluded. Both the\nprobability densities as well as their cumulatives are provided. These results\nwill have useful applications in the physical sciences and possibly elsewhere.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 11:12:06 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Shtembari", "Lolian", ""], ["Caldwell", "Allen", ""]]}, {"id": "2008.02211", "submitter": "Bo Shen", "authors": "Bo Shen, Zhenyu (James) Kong", "title": "Robust Tensor Principal Component Analysis: Exact Recovery via\n  Deterministic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor, also known as multi-dimensional array, arises from many applications\nin signal processing, manufacturing processes, healthcare, among others. As one\nof the most popular methods in tensor literature, Robust tensor principal\ncomponent analysis (RTPCA) is a very effective tool to extract the low rank and\nsparse components in tensors. In this paper, a new method to analyze RTPCA is\nproposed based on the recently developed tensor-tensor product and tensor\nsingular value decomposition (t-SVD). Specifically, it aims to solve a convex\noptimization problem whose objective function is a weighted combination of the\ntensor nuclear norm and the l1-norm. In most of literature of RTPCA, the exact\nrecovery is built on the tensor incoherence conditions and the assumption of a\nuniform model on the sparse support. Unlike this conventional way, in this\npaper, without any assumption of randomness, the exact recovery can be achieved\nin a completely deterministic fashion by characterizing the tensor\nrank-sparsity incoherence, which is an uncertainty principle between the\nlow-rank tensor spaces and the pattern of sparse tensor.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 16:26:10 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Shen", "Bo", "", "James"], ["Zhenyu", "", "", "James"], ["Kong", "", ""]]}, {"id": "2008.02269", "submitter": "Alexander Wein", "authors": "Tselil Schramm and Alexander S. Wein", "title": "Computational Barriers to Estimation from Low-Degree Polynomials", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CC cs.DS stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One fundamental goal of high-dimensional statistics is to detect or recover\nstructure from noisy data. In many cases, the data can be faithfully modeled by\na planted structure (such as a low-rank matrix) perturbed by random noise. But\neven for these simple models, the computational complexity of estimation is\nsometimes poorly understood. A growing body of work studies low-degree\npolynomials as a proxy for computational complexity: it has been demonstrated\nin various settings that low-degree polynomials of the data can match the\nstatistical performance of the best known polynomial-time algorithms for\ndetection. While prior work has studied the power of low-degree polynomials for\nthe task of detecting the presence of hidden structures, it has failed to\naddress the estimation problem in settings where detection is qualitatively\neasier than estimation.\n  In this work, we extend the method of low-degree polynomials to address\nproblems of estimation and recovery. For a large class of \"signal plus noise\"\nproblems, we give a user-friendly lower bound for the best possible mean\nsquared error achievable by any degree-D polynomial. To our knowledge, this is\nthe first instance in which the low-degree polynomial method can establish\nlow-degree hardness of recovery problems where the associated detection problem\nis easy. As applications, we give a tight characterization of the low-degree\nminimum mean squared error for the planted submatrix and planted dense subgraph\nproblems, resolving (in the low-degree framework) open problems about the\ncomputational complexity of recovery in both cases.\n", "versions": [{"version": "v1", "created": "Wed, 5 Aug 2020 17:52:10 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Schramm", "Tselil", ""], ["Wein", "Alexander S.", ""]]}, {"id": "2008.02437", "submitter": "Yuetian Luo", "authors": "Yuetian Luo and Garvesh Raskutti and Ming Yuan and Anru R. Zhang", "title": "A Sharp Blockwise Tensor Perturbation Bound for Orthogonal Iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.NA math.NA stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we develop novel perturbation bounds for the high-order\northogonal iteration (HOOI) [DLDMV00b]. Under mild regularity conditions, we\nestablish blockwise tensor perturbation bounds for HOOI with guarantees for\nboth tensor reconstruction in Hilbert-Schmidt norm $\\|\\widehat{\\bcT} - \\bcT\n\\|_{\\tHS}$ and mode-$k$ singular subspace estimation in Schatten-$q$ norm $\\|\n\\sin \\Theta (\\widehat{\\U}_k, \\U_k) \\|_q$ for any $q \\geq 1$. We show the upper\nbounds of mode-$k$ singular subspace estimation are unilateral and converge\nlinearly to a quantity characterized by blockwise errors of the perturbation\nand signal strength. For the tensor reconstruction error bound, we express the\nbound through a simple quantity $\\xi$, which depends only on perturbation and\nthe multilinear rank of the underlying signal. Rate matching deterministic\nlower bound for tensor reconstruction, which demonstrates the optimality of\nHOOI, is also provided. Furthermore, we prove that one-step HOOI (i.e., HOOI\nwith only a single iteration) is also optimal in terms of tensor reconstruction\nand can be used to lower the computational cost. The perturbation results are\nalso extended to the case that only partial modes of $\\bcT$ have low-rank\nstructure. We support our theoretical results by extensive numerical studies.\nFinally, we apply the novel perturbation bounds of HOOI on two applications,\ntensor denoising and tensor co-clustering, from machine learning and\nstatistics, which demonstrates the superiority of the new perturbation results.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 03:01:28 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 20:34:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Luo", "Yuetian", ""], ["Raskutti", "Garvesh", ""], ["Yuan", "Ming", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2008.02455", "submitter": "Guanyang Wang", "authors": "Guanyang Wang", "title": "Exact Convergence Rate Analysis of the Independent Metropolis-Hastings\n  Algorithms", "comments": "25 pages, 2 figures, 1 table. Typos fixed. Theorem 2 and 3 in the\n  previous version are strengthened and combined as Theorem 2 in the new\n  version. Some changes in the Introduction", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.SP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A well-known difficult problem regarding Metropolis-Hastings algorithms is to\nget sharp bounds on their convergence rates. Moreover, a fundamental but often\noverlooked problem in Markov chain theory is to study the convergence rates for\ndifferent initializations. In this paper, we study the two issues mentioned\nabove of the Independent Metropolis-Hastings (IMH) algorithms on both general\nand discrete state spaces. We derive the exact convergence rate and prove that\nthe IMH algorithm's different deterministic initializations have the same\nconvergence rate. Surprisingly, under mild conditions, we get the exact\nconvergence speed for IMH algorithms on general state spaces, which is the\nfirst `exact convergence' result for general state space MCMC algorithms to the\nauthor's best knowledge. Connections with the Random Walk Metropolis-Hastings\n(RWMH) algorithm are also discussed, which solve a conjecture proposed by\nAtchad\\'{e} and Perron \\cite{atchade2007geometric} using a counterexample.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 04:49:57 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 08:18:28 GMT"}, {"version": "v3", "created": "Fri, 21 Aug 2020 23:47:59 GMT"}, {"version": "v4", "created": "Fri, 11 Sep 2020 01:07:52 GMT"}, {"version": "v5", "created": "Mon, 21 Dec 2020 00:13:12 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Wang", "Guanyang", ""]]}, {"id": "2008.02479", "submitter": "Mikkel Slot Nielsen", "authors": "Richard A. Davis and Mikkel S. Nielsen", "title": "Modeling of time series using random forests: theoretical developments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study asymptotic properties of random forests within the\nframework of nonlinear time series modeling. While random forests have been\nsuccessfully applied in various fields, the theoretical justification has not\nbeen considered for their use in a time series setting. Under mild conditions,\nwe prove a uniform concentration inequality for regression trees built on\nnonlinear autoregressive processes and, subsequently, we use this result to\nprove consistency for a large class of random forests. The results are\nsupported by various simulations.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 07:02:10 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Davis", "Richard A.", ""], ["Nielsen", "Mikkel S.", ""]]}, {"id": "2008.02545", "submitter": "Timo Klock", "authors": "Alexander Cloninger and Timo Klock", "title": "A deep network construction that adapts to intrinsic dimensionality\n  beyond the domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the approximation of two-layer compositions $f(x) = g(\\phi(x))$ via\ndeep networks with ReLU activation, where $\\phi$ is a geometrically intuitive,\ndimensionality reducing feature map. We focus on two intuitive and practically\nrelevant choices for $\\phi$: the projection onto a low-dimensional embedded\nsubmanifold and a distance to a collection of low-dimensional sets. We achieve\nnear optimal approximation rates, which depend only on the complexity of the\ndimensionality reducing map $\\phi$ rather than the ambient dimension. Since\n$\\phi$ encapsulates all nonlinear features that are material to the function\n$f$, this suggests that deep nets are faithful to an intrinsic dimension\ngoverned by $f$ rather than the complexity of the domain of $f$. In particular,\nthe prevalent assumption of approximating functions on low-dimensional\nmanifolds can be significantly relaxed using functions of type $f(x) =\ng(\\phi(x))$ with $\\phi$ representing an orthogonal projection onto the same\nmanifold.\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 09:50:29 GMT"}, {"version": "v2", "created": "Sat, 23 Jan 2021 10:24:45 GMT"}, {"version": "v3", "created": "Mon, 26 Apr 2021 09:05:48 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Cloninger", "Alexander", ""], ["Klock", "Timo", ""]]}, {"id": "2008.02904", "submitter": "Moreno Bevilacqua", "authors": "M.Bevilacqua, C. Caama\\~no-Carrillo, E. Porcu", "title": "Unifying Compactly Supported and Matern Covariance Functions in Spatial\n  Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Mat{\\'e}rn family of covariance functions has played a central role in\nspatial statistics for decades, being a flexible parametric class with one\nparameter determining the smoothness of the paths of the underlying spatial\nfield.\n  This paper proposes a new family of spatial covariance functions, which stems\nfrom a reparameterization of the generalized Wendland family. As for the\nMat{\\'e}rn case, the new class allows for a continuous parameterization of the\nsmoothness of the underlying Gaussian random field, being additionally\ncompactly supported.\n  More importantly, we show that the proposed covariance family generalizes the\nMat{\\'e}rn model which is attained as a special limit case. The practical\nimplication of our theoretical results questions the effective flexibility of\nthe Mat{\\'e}rn covariance from modeling and computational viewpoints.\n  Our numerical experiments elucidate the speed of convergence of the proposed\nmodel to the Mat{\\'e}rn model. We also inspect the level of sparseness of the\nassociated (inverse) covariance matrix and the asymptotic distribution of the\nmaximum likelihood estimator under increasing and fixed domain asymptotics. The\neffectiveness of our proposal is illustrated by analyzing a georeferenced\ndataset on maximum temperatures over the southeastern United States, and\nperforming a re-analysis of a large spatial point referenced dataset of yearly\ntotal precipitation anomalies\n", "versions": [{"version": "v1", "created": "Thu, 6 Aug 2020 23:46:08 GMT"}, {"version": "v2", "created": "Mon, 1 Feb 2021 15:00:06 GMT"}], "update_date": "2021-02-02", "authors_parsed": [["Bevilacqua", "M.", ""], ["Caama\u00f1o-Carrillo", "C.", ""], ["Porcu", "E.", ""]]}, {"id": "2008.02978", "submitter": "Amine Amimour", "authors": "Amine Amimour and Karima Belaide", "title": "On the invertibility in periodic ARFIMA models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper, characterizes the invertibility and causality conditions\nof a periodic ARFIMA (PARFIMA) models. We first, discuss the conditions in the\nmultivariate case, by considering the corresponding p-variate stationary ARFIMA\nmodels. Second, we construct the conditions using the univariate case and we\ndeduce a new infinite autoregressive representation for the PARFIMA model, the\nresults are investigated through a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 03:40:04 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Amimour", "Amine", ""], ["Belaide", "Karima", ""]]}, {"id": "2008.03038", "submitter": "Subhro Ghosh", "authors": "Subhroshekhar Ghosh, Krishnakumar Balasubramanian, Xiaochuan Yang", "title": "Fractal Gaussian Networks: A sparse random graph model based on Gaussian\n  Multiplicative Chaos", "comments": "Part of this work has been accepted to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel stochastic network model, called Fractal Gaussian Network\n(FGN), that embodies well-defined and analytically tractable fractal\nstructures. Such fractal structures have been empirically observed in diverse\napplications. FGNs interpolate continuously between the popular purely random\ngeometric graphs (a.k.a. the Poisson Boolean network), and random graphs with\nincreasingly fractal behavior. In fact, they form a parametric family of sparse\nrandom geometric graphs that are parametrized by a fractality parameter $\\nu$\nwhich governs the strength of the fractal structure. FGNs are driven by the\nlatent spatial geometry of Gaussian Multiplicative Chaos (GMC), a canonical\nmodel of fractality in its own right. We asymptotically characterize the\nexpected number of edges and triangle in FGNs. We then examine the natural\nquestion of detecting the presence of fractality and the problem of parameter\nestimation based on observed network data, in addition to fundamental\nproperties of the FGN as a random graph model. We also explore fractality in\ncommunity structures by unveiling a natural stochastic block model in the\nsetting of FGNs.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 08:37:36 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Ghosh", "Subhroshekhar", ""], ["Balasubramanian", "Krishnakumar", ""], ["Yang", "Xiaochuan", ""]]}, {"id": "2008.03060", "submitter": "Bertrand Iooss", "authors": "Clement Gauchy (CEA-DES (ex-DEN), EDF R&D PRISME), Jerome Stenger (EDF\n  R&D PRISME, IMT), Roman Sueur (EDF R&D PRISME), Bertrand Iooss (EDF R&D, IMT,\n  GdR MASCOT-NUM)", "title": "An information geometry approach for robustness analysis in uncertainty\n  quantification of computer codes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robustness analysis is an emerging field in the domain of uncertainty\nquantification. It consists of analysing the response of a computer model with\nuncertain inputs to the perturbation of one or several of its input\ndistributions. Thus, a practical robustness analysis methodology should rely on\na coherent definition of a distribution perturbation. This paper addresses this\nissue by exposing a rigorous way of perturbing densities. The proposed\nmethodology is based the Fisher distance on manifolds of probability\ndistributions. A numerical method to calculate perturbed densities in practice\nis presented. This method comes from Lagrangian mechanics and consists of\nsolving an ordinary differential equations system. This perturbation definition\nis then used to compute quantile-oriented robustness indices. The resulting\nPerturbed-Law based sensitivity Indices (PLI) are illustrated on several\nnumerical models. This methodology is also applied to an industrial study\n(simulation of a loss of coolant accident in a nuclear reactor), where several\ntens of the model physical parameters are uncertain with limited knowledge\nconcerning their distributions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 09:39:34 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 14:34:15 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Gauchy", "Clement", "", "CEA-DES"], ["Stenger", "Jerome", "", "EDF\n  R&D PRISME, IMT"], ["Sueur", "Roman", "", "EDF R&D PRISME"], ["Iooss", "Bertrand", "", "EDF R&D, IMT,\n  GdR MASCOT-NUM"]]}, {"id": "2008.03181", "submitter": "Shayan Aziznejad", "authors": "Leello Dadi, Shayan Aziznejad, Michael Unser", "title": "Generating Sparse Stochastic Processes Using Matched Splines", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2020.3011632", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an algorithm to generate trajectories of sparse stochastic\nprocesses that are solutions of linear ordinary differential equations driven\nby L\\'evy white noises. A recent paper showed that these processes are limits\nin law of generalized compound-Poisson processes. Based on this result, we\nderive an off-the-grid algorithm that generates arbitrarily close\napproximations of the target process. Our method relies on a B-spline\nrepresentation of generalized compound-Poisson processes. We illustrate\nnumerically the validity of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 13:42:17 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Dadi", "Leello", ""], ["Aziznejad", "Shayan", ""], ["Unser", "Michael", ""]]}, {"id": "2008.03244", "submitter": "Shuheng Zhou", "authors": "Shuheng Zhou", "title": "The Tensor Quadratic Forms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the following data perturbation model, where the covariates incur\nmultiplicative errors. For two $n \\times m$ random matrices $U, X$, we denote\nby $U \\circ X$ the Hadamard or Schur product, which is defined as $(U \\circ\nX)_{ij} = (U_{ij}) \\cdot (X_{ij})$. In this paper, we study the subgaussian\nmatrix variate model, where we observe the matrix variate data $X$ through a\nrandom mask $U$:\n  \\begin{equation*} {\\mathcal X} = U \\circ X \\; \\; \\; \\text{ where} \\; \\; \\;X =\nB^{1/2} {\\mathbb Z} A^{1/2}, \\end{equation*} where ${\\mathbb Z}$ is a random\nmatrix with independent subgaussian entries, and $U$ is a mask matrix with\neither zero or positive entries, where ${\\mathbb E} U_{ij} \\in [0, 1]$ and all\nentries are mutually independent. Subsampling in rows, or columns, or random\nsampling of entries of $X$ are special cases of this model. Under the\nassumption of independence between $U$ and $X$, we introduce componentwise\nunbiased estimators for estimating covariance $A$ and $B$, and prove the\nconcentration of measure bounds in the sense of guaranteeing the restricted\neigenvalue conditions to hold on the estimator for $B$, when columns of data\nmatrix $X$ are sampled with different rates. Our results provide insight for\nsparse recovery for relationships among people (samples, locations, items) when\nfeatures (variables, time points, user ratings) are present in the observed\ndata matrix ${\\mathcal X}$ with heterogenous rates. Our proof techniques can\ncertainly be extended to other scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 16:06:14 GMT"}], "update_date": "2020-08-10", "authors_parsed": [["Zhou", "Shuheng", ""]]}, {"id": "2008.03326", "submitter": "Marco Mondelli", "authors": "Marco Mondelli, Christos Thrampoulidis and Ramji Venkataramanan", "title": "Optimal Combination of Linear and Spectral Estimators for Generalized\n  Linear Models", "comments": "49 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering an unknown signal $\\boldsymbol x$ given\nmeasurements obtained from a generalized linear model with a Gaussian sensing\nmatrix. Two popular solutions are based on a linear estimator $\\hat{\\boldsymbol\nx}^{\\rm L}$ and a spectral estimator $\\hat{\\boldsymbol x}^{\\rm s}$. The former\nis a data-dependent linear combination of the columns of the measurement\nmatrix, and its analysis is quite simple. The latter is the principal\neigenvector of a data-dependent matrix, and a recent line of work has studied\nits performance. In this paper, we show how to optimally combine\n$\\hat{\\boldsymbol x}^{\\rm L}$ and $\\hat{\\boldsymbol x}^{\\rm s}$. At the heart\nof our analysis is the exact characterization of the joint empirical\ndistribution of $(\\boldsymbol x, \\hat{\\boldsymbol x}^{\\rm L}, \\hat{\\boldsymbol\nx}^{\\rm s})$ in the high-dimensional limit. This allows us to compute the\nBayes-optimal combination of $\\hat{\\boldsymbol x}^{\\rm L}$ and\n$\\hat{\\boldsymbol x}^{\\rm s}$, given the limiting distribution of the signal\n$\\boldsymbol x$. When the distribution of the signal is Gaussian, then the\nBayes-optimal combination has the form $\\theta\\hat{\\boldsymbol x}^{\\rm\nL}+\\hat{\\boldsymbol x}^{\\rm s}$ and we derive the optimal combination\ncoefficient. In order to establish the limiting distribution of $(\\boldsymbol\nx, \\hat{\\boldsymbol x}^{\\rm L}, \\hat{\\boldsymbol x}^{\\rm s})$, we design and\nanalyze an Approximate Message Passing (AMP) algorithm whose iterates give\n$\\hat{\\boldsymbol x}^{\\rm L}$ and approach $\\hat{\\boldsymbol x}^{\\rm s}$.\nNumerical simulations demonstrate the improvement of the proposed combination\nwith respect to the two methods considered separately.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 18:20:05 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 14:24:56 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 07:15:43 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Mondelli", "Marco", ""], ["Thrampoulidis", "Christos", ""], ["Venkataramanan", "Ramji", ""]]}, {"id": "2008.03349", "submitter": "Micha\\\"el Lalancette", "authors": "Micha\\\"el Lalancette, Sebastian Engelke, Stanislav Volgushev", "title": "Rank-based Estimation under Asymptotic Dependence and Independence, with\n  Applications to Spatial Extremes", "comments": "64 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate extreme value theory is concerned with modeling the joint tail\nbehavior of several random variables. Existing work mostly focuses on\nasymptotic dependence, where the probability of observing a large value in one\nof the variables is of the same order as observing a large value in all\nvariables simultaneously. However, there is growing evidence that asymptotic\nindependence is equally important in real world applications. Available\nstatistical methodology in the latter setting is scarce and not well understood\ntheoretically. We revisit non-parametric estimation and introduce rank-based\nM-estimators for parametric models that simultaneously work under asymptotic\ndependence and asymptotic independence, without requiring prior knowledge on\nwhich of the two regimes applies. Asymptotic normality of the proposed\nestimators is established under weak regularity conditions. We further show how\nbivariate estimators can be leveraged to obtain parametric estimators in\nspatial tail models, and again provide a thorough theoretical justification for\nour approach.\n", "versions": [{"version": "v1", "created": "Fri, 7 Aug 2020 19:46:21 GMT"}, {"version": "v2", "created": "Fri, 8 Jan 2021 18:18:13 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Lalancette", "Micha\u00ebl", ""], ["Engelke", "Sebastian", ""], ["Volgushev", "Stanislav", ""]]}, {"id": "2008.03481", "submitter": "F. Richard Guo", "authors": "F. Richard Guo and Emilija Perkovi\\'c", "title": "Efficient Least Squares for Estimating Total Effects under Linearity and\n  Causal Sufficiency", "comments": "Edits to Introduction and Discussion", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive linear structural equation models are widely used to postulate\ncausal mechanisms underlying observational data. In these models, each variable\nequals a linear combination of a subset of the remaining variables plus an\nerror term. When there is no unobserved confounding or selection bias, the\nerror terms are assumed to be independent. We consider estimating a total\ncausal effect in this setting. The causal structure is assumed to be known only\nup to a maximally oriented partially directed acyclic graph (MPDAG), a general\nclass of graphs that can represent a Markov equivalence class of directed\nacyclic graphs (DAGs) with added background knowledge. We propose a simple\nestimator based on recursive least squares, which can consistently estimate any\nidentified total causal effect, under point or joint intervention. We show that\nthis estimator is the most efficient among all regular estimators that are\nbased on the sample covariance, which includes covariate adjustment and the\nestimators employed by the joint-IDA algorithm. Notably, our result holds\nwithout assuming Gaussian errors.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 09:10:23 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 02:05:47 GMT"}, {"version": "v3", "created": "Fri, 5 Mar 2021 00:11:52 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Guo", "F. Richard", ""], ["Perkovi\u0107", "Emilija", ""]]}, {"id": "2008.03600", "submitter": "Andrii Babii", "authors": "Andrii Babii and Ryan T. Ball and Eric Ghysels and Jonas Striaukas", "title": "Machine Learning Panel Data Regressions with an Application to\n  Nowcasting Price Earnings Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces structured machine learning regressions for prediction\nand nowcasting with panel data consisting of series sampled at different\nfrequencies. Motivated by the empirical problem of predicting corporate\nearnings for a large cross-section of firms with macroeconomic, financial, and\nnews time series sampled at different frequencies, we focus on the sparse-group\nLASSO regularization. This type of regularization can take advantage of the\nmixed frequency time series panel data structures and we find that it\nempirically outperforms the unstructured machine learning methods. We obtain\noracle inequalities for the pooled and fixed effects sparse-group LASSO panel\ndata estimators recognizing that financial and economic data exhibit heavier\nthan Gaussian tails. To that end, we leverage on a novel Fuk-Nagaev\nconcentration inequality for panel data consisting of heavy-tailed\n$\\tau$-mixing processes which may be of independent interest in other\nhigh-dimensional panel data settings.\n", "versions": [{"version": "v1", "created": "Sat, 8 Aug 2020 21:12:33 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Babii", "Andrii", ""], ["Ball", "Ryan T.", ""], ["Ghysels", "Eric", ""], ["Striaukas", "Jonas", ""]]}, {"id": "2008.03650", "submitter": "Khashayar Gatmiry", "authors": "Khashayar Gatmiry (1), Maryam Aliakbarpour (1), Stefanie Jegelka (1)\n  ((1) Massachusetts Institute of Technology)", "title": "Testing Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are popular probabilistic models of\ndiversity. In this paper, we investigate DPPs from a new perspective: property\ntesting of distributions. Given sample access to an unknown distribution $q$\nover the subsets of a ground set, we aim to distinguish whether $q$ is a DPP\ndistribution, or $\\epsilon$-far from all DPP distributions in\n$\\ell_1$-distance. In this work, we propose the first algorithm for testing\nDPPs. Furthermore, we establish a matching lower bound on the sample complexity\nof DPP testing. This lower bound also extends to showing a new hardness result\nfor the problem of testing the more general class of log-submodular\ndistributions.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 04:45:16 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Gatmiry", "Khashayar", "", "Massachusetts Institute of Technology"], ["Aliakbarpour", "Maryam", "", "Massachusetts Institute of Technology"], ["Jegelka", "Stefanie", "", "Massachusetts Institute of Technology"]]}, {"id": "2008.03738", "submitter": "Shulei Wang", "authors": "Ruoqi Yu and Shulei Wang", "title": "Treatment Effects Estimation by Uniform Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, balancing covariates in different treatment groups\nis essential to estimate treatment effects. One of the most commonly used\nmethods for such purposes is weighting. The performance of this class of\nmethods usually depends on strong regularity conditions for the underlying\nmodel, which might not hold in practice. In this paper, we investigate\nweighting methods from a functional estimation perspective and argue that the\nweights needed for covariate balancing could differ from those needed for\ntreatment effects estimation under low regularity conditions. Motivated by this\nobservation, we introduce a new framework of weighting that directly targets\nthe treatment effects estimation. Unlike existing methods, the resulting\nestimator for a treatment effect under this new framework is a simple\nkernel-based $U$-statistic after applying a data-driven transformation to the\nobserved covariates. We characterize the theoretical properties of the new\nestimators of treatment effects under a nonparametric setting and show that\nthey are able to work robustly under low regularity conditions. The new\nframework is also applied to several numerical examples to demonstrate its\npractical merits.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 14:56:55 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 03:17:52 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Yu", "Ruoqi", ""], ["Wang", "Shulei", ""]]}, {"id": "2008.03820", "submitter": "Pengsheng Ji", "authors": "Zhe Wang, Yingbin Liang and Pengsheng Ji", "title": "Spectral Algorithms for Community Detection in Directed Networks", "comments": "Journal of Machine Learning Research 2020, to appear", "journal-ref": "Journal of Machine Learning Research 2020. (153):1-45,", "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Community detection in large social networks is affected by degree\nheterogeneity of nodes. The D-SCORE algorithm for directed networks was\nintroduced to reduce this effect by taking the element-wise ratios of the\nsingular vectors of the adjacency matrix before clustering. Meaningful results\nwere obtained for the statistician citation network, but rigorous analysis on\nits performance was missing. First, this paper establishes theoretical\nguarantee for this algorithm and its variants for the directed degree-corrected\nblock model (Directed-DCBM). Second, this paper provides significant\nimprovements for the original D-SCORE algorithms by attaching the nodes outside\nof the community cores using the information of the original network instead of\nthe singular vectors.\n", "versions": [{"version": "v1", "created": "Sun, 9 Aug 2020 21:43:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Zhe", ""], ["Liang", "Yingbin", ""], ["Ji", "Pengsheng", ""]]}, {"id": "2008.03971", "submitter": "Yunxiao Chen", "authors": "Yunxiao Chen, Irini Moustaki, Haoran Zhang", "title": "A Note on Likelihood Ratio Tests for Models with Latent Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood ratio test (LRT) is widely used for comparing the relative fit\nof nested latent variable models. Following Wilks' theorem, the LRT is\nconducted by comparing the LRT statistic with its asymptotic distribution under\nthe restricted model, a $\\chi^2$-distribution with degrees of freedom equal to\nthe difference in the number of free parameters between the two nested models\nunder comparison. For models with latent variables such as factor analysis,\nstructural equation models and random effects models, however, it is often\nfound that the $\\chi^2$ approximation does not hold. In this note, we show how\nthe regularity conditions of Wilks' theorem may be violated using three\nexamples of models with latent variables. In addition, a more general theory\nfor LRT is given that provides the correct asymptotic theory for these LRTs.\nThis general theory was first established in Chernoff (1954) and discussed in\nboth van der Vaart (2000) and Drton (2009), but it does not seem to have\nreceived enough attention. We illustrate this general theory with the three\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 09:09:04 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 16:08:07 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Chen", "Yunxiao", ""], ["Moustaki", "Irini", ""], ["Zhang", "Haoran", ""]]}, {"id": "2008.03974", "submitter": "Anthony J  Webster", "authors": "Anthony J. Webster", "title": "Clustering parametric models and normally distributed data", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent UK Biobank study clustered 156 parameterised models associating risk\nfactors with common diseases, to identify shared causes of disease. Parametric\nmodels are often more familiar and interpretable than clustered data, can\nbuild-in prior knowledge, adjust for known confounders, and use marginalisation\nto emphasise parameters of interest. Estimates include a Maximum Likelihood\nEstimate (MLE) that is (approximately) normally distributed, and its\ncovariance. Clustering models rarely consider the covariances of data points,\nthat are usually unavailable. Here a clustering model is formulated that\naccounts for covariances of the data, and assumes that all MLEs in a cluster\nare the same. The log-likelihood is exactly calculated in terms of the fitted\nparameters, with the unknown cluster means removed by marginalisation. The\nprocedure is equivalent to calculating the Bayesian Information Criterion (BIC)\nwithout approximation, and can be used to assess the optimum number of clusters\nfor a given clustering algorithm. The log-likelihood has terms to penalise poor\nfits and model complexity, and can be maximised to determine the number and\ncomposition of clusters. Results can be similar to using the ad-hoc \"elbow\ncriterion\", but are less subjective. The model is also formulated as a\nDirichlet process mixture model (DPMM). The overall approach is equivalent to a\nmulti-layer algorithm that characterises features through the normally\ndistributed MLEs of a fitted model, and then clusters the normal distributions.\nExamples include simulated data, and clustering of diseases in UK Biobank data\nusing estimated associations with risk factors. The results can be applied\ndirectly to measured data and their estimated covariances, to the output from\nclustering models, or the DPMM implementation can be used to cluster fitted\nmodels directly.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 09:18:14 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 16:14:33 GMT"}, {"version": "v3", "created": "Fri, 30 Apr 2021 14:49:18 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Webster", "Anthony J.", ""]]}, {"id": "2008.04101", "submitter": "Rishabh Dudeja", "authors": "Rishabh Dudeja and Daniel Hsu", "title": "Statistical Query Lower Bounds for Tensor PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Tensor PCA problem introduced by Richard and Montanari (2014), one is\ngiven a dataset consisting of $n$ samples $\\mathbf{T}_{1:n}$ of i.i.d. Gaussian\ntensors of order $k$ with the promise that $\\mathbb{E}\\mathbf{T}_1$ is a rank-1\ntensor and $\\|\\mathbb{E} \\mathbf{T}_1\\| = 1$. The goal is to estimate\n$\\mathbb{E} \\mathbf{T}_1$. This problem exhibits a large conjectured hard phase\nwhen $k>2$: When $d \\lesssim n \\ll d^{\\frac{k}{2}}$ it is information\ntheoretically possible to estimate $\\mathbb{E} \\mathbf{T}_1$, but no polynomial\ntime estimator is known. We provide a sharp analysis of the optimal sample\ncomplexity in the Statistical Query (SQ) model and show that SQ algorithms with\npolynomial query complexity not only fail to solve Tensor PCA in the\nconjectured hard phase, but also have a strictly sub-optimal sample complexity\ncompared to some polynomial time estimators such as the Richard-Montanari\nspectral estimator. Our analysis reveals that the optimal sample complexity in\nthe SQ model depends on whether $\\mathbb{E} \\mathbf{T}_1$ is symmetric or not.\nFor symmetric, even order tensors, we also isolate a sample size regime in\nwhich it is possible to test if $\\mathbb{E} \\mathbf{T}_1 = \\mathbf{0}$ or\n$\\mathbb{E}\\mathbf{T}_1 \\neq \\mathbf{0}$ with polynomially many queries but not\nestimate $\\mathbb{E}\\mathbf{T}_1$. Our proofs rely on the Fourier analytic\napproach of Feldman, Perkins and Vempala (2018) to prove sharp SQ lower bounds.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 13:14:34 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2021 19:00:15 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Dudeja", "Rishabh", ""], ["Hsu", "Daniel", ""]]}, {"id": "2008.04166", "submitter": "Xiucai Ding", "authors": "Xiucai Ding and Fan Yang", "title": "Tracy-Widom distribution for the edge eigenvalues of Gram type random\n  matrices", "comments": "67 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large dimensional Gram type matrices are common objects in high-dimensional\nstatistics and machine learning. In this paper, we study the limiting\ndistribution of the edge eigenvalues for a general class of high-dimensional\nGram type random matrices, including separable sample covariance matrices,\nsparse sample covariance matrices, bipartite stochastic block model and random\nGram matrices with general variance profiles. Specifically, we prove that under\n(almost) sharp moment conditions and certain tractable regularity assumptions,\nthe edge eigenvalues, i.e., the largest few eigenvalues of non-spiked Gram type\nrandom matrices or the extremal bulk eigenvalues of spiked Gram type random\nmatrices, satisfy the Tracy-Widom distribution asymptotically. Our results can\nbe used to construct adaptive, accurate and powerful statistics for\nhigh-dimensional statistical inference. In particular, we propose\ndata-dependent statistics to infer the number of signals under general noise\nstructure, test the one-sided sphericity of separable matrix, and test the\nstructure of bipartite stochastic block model. Numerical simulations show\nstrong support of our proposed statistics. The core of our proof is to\nestablish the edge universality and Tracy-Widom distribution for a rectangular\nDyson Brownian motion with regular initial data. This is a general strategy to\nstudy the edge statistics for high-dimensional Gram type random matrices\nwithout exploring the specific independence structure of the target matrices.\nIt has potential to be applied to more general random matrices that are beyond\nthe ones considered in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 14:45:54 GMT"}, {"version": "v2", "created": "Sun, 4 Oct 2020 03:03:20 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Ding", "Xiucai", ""], ["Yang", "Fan", ""]]}, {"id": "2008.04270", "submitter": "Dustin Mixon", "authors": "Dustin G. Mixon, Kaiying Xie", "title": "Sketching semidefinite programs for faster clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DS cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clustering problems enjoy solutions by semidefinite programming.\nTheoretical results in this vein frequently consider data with a planted\nclustering and a notion of signal strength such that the semidefinite program\nexactly recovers the planted clustering when the signal strength is\nsufficiently large. In practice, semidefinite programs are notoriously slow,\nand so speedups are welcome. In this paper, we show how to sketch a popular\nsemidefinite relaxation of a graph clustering problem known as minimum\nbisection, and our analysis supports a meta-claim that the clustering task is\nless computationally burdensome when there is more signal.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 17:10:29 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Mixon", "Dustin G.", ""], ["Xie", "Kaiying", ""]]}, {"id": "2008.04286", "submitter": "Dustin Mixon", "authors": "Charles Clum, Dustin G. Mixon", "title": "Parameter estimation in the SIR model from early infections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard model for epidemics is the SIR model on a graph. We introduce a\nsimple algorithm that uses the early infection times from a sample path of the\nSIR model to estimate the parameters this model, and we provide a performance\nguarantee in the setting of locally tree-like graphs.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 17:30:51 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Clum", "Charles", ""], ["Mixon", "Dustin G.", ""]]}, {"id": "2008.04348", "submitter": "Wei Zheng", "authors": "Xiangshun Kong and Wei Zheng", "title": "Design based incomplete U-statistics", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  U-statistics are widely used in fields such as economics, machine learning,\nand statistics. However, while they enjoy desirable statistical properties,\nthey have an obvious drawback in that the computation becomes impractical as\nthe data size $n$ increases. Specifically, the number of combinations, say $m$,\nthat a U-statistic of order $d$ has to evaluate is $O(n^d)$. Many efforts have\nbeen made to approximate the original U-statistic using a small subset of\ncombinations since Blom (1976), who referred to such an approximation as an\nincomplete U-statistic. To the best of our knowledge, all existing methods\nrequire $m$ to grow at least faster than $n$, albeit more slowly than $n^d$, in\norder for the corresponding incomplete U-statistic to be asymptotically\nefficient in terms of the mean squared error. In this paper, we introduce a new\ntype of incomplete U-statistic that can be asymptotically efficient, even when\n$m$ grows more slowly than $n$. In some cases, $m$ is only required to grow\nfaster than $\\sqrt{n}$. Our theoretical and empirical results both show\nsignificant improvements in the statistical efficiency of the new incomplete\nU-statistic.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 18:20:48 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Kong", "Xiangshun", ""], ["Zheng", "Wei", ""]]}, {"id": "2008.04387", "submitter": "Karthik Devarajan", "authors": "Majid Asadi, Karthik Devarajan, Nader Ebrahimi, Ehsan Soofi, Lauren\n  Spirko-Burns", "title": "Probability Link Models with Symmetric Information Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces link functions for transforming one probability\ndistribution to another such that the Kullback-Leibler and R\\'enyi divergences\nbetween the two distributions are symmetric. Two general classes of link models\nare proposed. The first model links two survival functions and is applicable to\nmodels such as the proportional odds and change point, which are used in\nsurvival analysis and reliability modeling. A prototype application involving\nthe proportional odds model demonstrates advantages of symmetric divergence\nmeasures over asymmetric measures for assessing the efficacy of features and\nfor model averaging purposes. The advantages include providing unique ranks for\nmodels and unique information weights for model averaging with one-half as much\ncomputation requirement of asymmetric divergences. The second model links two\ncumulative probability distribution functions. This model produces a\ngeneralized location model which are continuous counterparts of the binary\nprobability models such as probit and logit models. Examples include the\ngeneralized probit and logit models which have appeared in the survival\nanalysis literature, and a generalized Laplace model and a generalized\nStudent-$t$ model, which are survival time models corresponding to the\nrespective binary probability models. Lastly, extensions to symmetric\ndivergence between survival functions and conditions for copula dependence\ninformation are presented.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 19:49:51 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Asadi", "Majid", ""], ["Devarajan", "Karthik", ""], ["Ebrahimi", "Nader", ""], ["Soofi", "Ehsan", ""], ["Spirko-Burns", "Lauren", ""]]}, {"id": "2008.04423", "submitter": "J\\\"uri Lember", "authors": "Annika Krutto and J\\\"uri Lember", "title": "Estimating the logarithm of characteristic function and stability\n  parameter for symmetric stable laws", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_1,\\ldots,X_n$ be an i.i.d. sample from symmetric stable distribution\nwith stability parameter $\\alpha$ and scale parameter $\\gamma$. Let $\\varphi_n$\nbe the empirical characteristic function. We prove an uniform large deviation\ninequality: given preciseness $\\epsilon>0$ and probability $p\\in (0,1)$, there\nexists universal (depending on $\\epsilon$ and $p$ but not depending on $\\alpha$\nand $\\gamma$) constant $\\bar{r}>0$ so that $$P\\big(\\sup_{u>0:r(u)\\leq\n\\bar{r}}|r(u)-\\hat{r}(u)|\\geq \\epsilon\\big)\\leq p,$$ where\n$r(u)=(u\\gamma)^{\\alpha}$ and $\\hat{r}(u)=-\\ln|\\varphi_n(u)|$. As an\napplications of the result, we show how it can be used in estimation unknown\nstability parameter $\\alpha$.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 21:27:36 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Krutto", "Annika", ""], ["Lember", "J\u00fcri", ""]]}, {"id": "2008.04475", "submitter": "Mar\\'ia Fernanda Gil-Leyva Villa", "authors": "Mar\\'ia F. Gil-Leyva, Rams\\'es H. Mena", "title": "Stick-breaking processes with exchangeable length variables", "comments": "Accepted for publication by the Journal of the American Statistical\n  Association. 44 pages, 11 figures, supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our object of study is the general class of stick-breaking processes with\nexchangeable length variables. These generalize well-known Bayesian\nnon-parametric priors in an unexplored direction. We give conditions to assure\nthe respective species sampling process is proper and the corresponding prior\nhas full support. For a rich sub-class we explain how, by tuning a single\n$[0,1]$-valued parameter, the stochastic ordering of the weights can be\nmodulated, and Dirichlet and Geometric priors can be recovered. A general\nformula for the distribution of the latent allocation variables is derived and\nan MCMC algorithm is proposed for density estimation purposes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 02:00:04 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 07:09:07 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gil-Leyva", "Mar\u00eda F.", ""], ["Mena", "Rams\u00e9s H.", ""]]}, {"id": "2008.04537", "submitter": "Debdeep Pati", "authors": "Anirban Bhattacharya, Debdeep Pati and Sean Plummer", "title": "Evidence bounds in singular models: probabilistic and variational\n  perspectives", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The marginal likelihood or evidence in Bayesian statistics contains an\nintrinsic penalty for larger model sizes and is a fundamental quantity in\nBayesian model comparison. Over the past two decades, there has been steadily\nincreasing activity to understand the nature of this penalty in singular\nstatistical models, building on pioneering work by Sumio Watanabe. Unlike\nregular models where the Bayesian information criterion (BIC) encapsulates a\nfirst-order expansion of the logarithm of the marginal likelihood, parameter\ncounting gets trickier in singular models where a quantity called the real log\ncanonical threshold (RLCT) summarizes the effective model dimensionality. In\nthis article, we offer a probabilistic treatment to recover non-asymptotic\nversions of established evidence bounds as well as prove a new result based on\nthe Gibbs variational inequality. In particular, we show that mean-field\nvariational inference correctly recovers the RLCT for any singular model in its\ncanonical or normal form. We additionally exhibit sharpness of our bound by\nanalyzing the dynamics of a general purpose coordinate ascent algorithm (CAVI)\npopularly employed in variational inference.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 06:20:43 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Bhattacharya", "Anirban", ""], ["Pati", "Debdeep", ""], ["Plummer", "Sean", ""]]}, {"id": "2008.04584", "submitter": "Daniel G. Rasines", "authors": "Daniel G. Rasines and G. Alastair Young", "title": "Bayesian Selective Inference: Non-informative Priors", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss Bayesian inference for parameters selected using the data. First,\nwe provide a critical analysis of the existing positions in the literature\nregarding the correct Bayesian approach under selection. Second, we propose two\ntypes of non-informative priors for selection models. These priors may be\nemployed to produce a posterior distribution in the absence of prior\ninformation as well as to provide well-calibrated frequentist inference for the\nselected parameter. We test the proposed priors empirically in several\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 08:47:06 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 12:00:24 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Rasines", "Daniel G.", ""], ["Young", "G. Alastair", ""]]}, {"id": "2008.04688", "submitter": "Piotr Zwiernik", "authors": "Steffen Lauritzen and Piotr Zwiernik", "title": "Locally associated graphical models and mixed convex exponential\n  families", "comments": "Supplementary material available at\n  http://econ.upf.edu/~piotr/supps/2020-LZ-golazo.zip", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of multivariate total positivity has proved to be useful in\nfinance and psychology but may be too restrictive in other applications. In\nthis paper we propose a concept of local association, where highly connected\ncomponents in a graphical model are positively associated and study its\nproperties. Our main motivation comes from gene expression data, where\ngraphical models have become a popular exploratory tool. The models are\ninstances of what we term mixed convex exponential families and we show that a\nmixed dual likelihood estimator has simple exact properties for such families\nas well as asymptotic properties similar to the maximum likelihood estimator.\nWe further relax the positivity assumption by penalizing negative partial\ncorrelations in what we term the positive graphical lasso. Finally, we develop\na GOLAZO algorithm based on block-coordinate descent that applies to a number\nof optimization procedures that arise in the context of graphical models,\nincluding the estimation problems described above. We derive results on\nexistence of the optimum for such problems.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 13:21:45 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 09:22:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Lauritzen", "Steffen", ""], ["Zwiernik", "Piotr", ""]]}, {"id": "2008.04708", "submitter": "Andreas Dzemski", "authors": "Andreas Dzemski, Ryo Okui", "title": "Convergence rate of estimators of clustered panel models with\n  misclassification", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study kmeans clustering estimation of panel data models with a latent\ngroup structure and $N$ units and $T$ time periods under long panel\nasymptotics. We show that the group-specific coefficients can be estimated at\nthe parametric root $NT$ rate even if error variances diverge as $T \\to \\infty$\nand some units are asymptotically misclassified. This limit case approximates\nempirically relevant settings and is not covered by existing asymptotic\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 14:05:58 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Dzemski", "Andreas", ""], ["Okui", "Ryo", ""]]}, {"id": "2008.04790", "submitter": "Maximilien Dreveton", "authors": "Konstantin Avrachenkov, Maximilien Dreveton, Lasse Leskel\\\"a", "title": "Estimation of Static Community Memberships from Temporal Network Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the estimation of static community memberships from\ntemporally correlated pair interactions represented by an $N$-by-$N$-by-$T$\ntensor where $N$ is the number of nodes and $T$ is the length of the time\nhorizon. We present several estimation algorithms, both offline and online,\nwhich fully utilise the temporal nature of the observed data. As an\ninformation-theoretic benchmark, we study data sets generated by a dynamic\nstochastic block model, and derive fundamental information criteria for the\nrecoverability of the community memberships as $N \\to \\infty$ both for bounded\nand diverging $T$. These results show that (i) even a small increase in $T$ may\nhave a big impact on the recoverability of community memberships, (ii)\nconsistent recovery is possible even for very sparse data (e.g. bounded average\ndegree) when $T$ is large enough. We analyse the accuracy of the proposed\nestimation algorithms under various assumptions on data sparsity and\nidentifiability, and prove that an efficient online algorithm is strongly\nconsistent up to the information-theoretic threshold under suitable\ninitialisation. Numerical experiments show that even a poor initial estimate\n(e.g., blind random guess) of the community assignment leads to high accuracy\nafter a small number of iterations, and remarkably so also in very sparse\nregimes.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 15:33:59 GMT"}, {"version": "v2", "created": "Wed, 12 Aug 2020 15:31:33 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Avrachenkov", "Konstantin", ""], ["Dreveton", "Maximilien", ""], ["Leskel\u00e4", "Lasse", ""]]}, {"id": "2008.04893", "submitter": "Song Fang", "authors": "Song Fang and Quanyan Zhu", "title": "Channel Leakage, Information-Theoretic Limitations of Obfuscation, and\n  Optimal Privacy Mask Design for Streaming Data", "comments": "The title was changed from \"Channel Leakage and Information Theoretic\n  Privacy-Distortion Tradeoffs for Streaming Data\" to the current one on 29th\n  September 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.CR cs.LG eess.SP math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we first introduce the notion of channel leakage as the\nminimum mutual information between the channel input and channel output. As its\nname indicates, channel leakage quantifies the minimum information leakage to\nthe malicious receiver. In a broad sense, it can be viewed as a dual concept of\nchannel capacity, which characterizes the maximum information transmission to\nthe targeted receiver. We obtain explicit formulas of channel leakage for the\nwhite Gaussian case, the colored Gaussian case, and the fading case. We then\nutilize this notion to investigate the fundamental limitations of obfuscation\nin terms of privacy-distortion tradeoffs (as well as privacy-power tradeoffs)\nfor streaming data; particularly, we derive analytical tradeoff equations for\nthe stationary case, the non-stationary case, and the finite-time case. Our\nresults also indicate explicitly how to design the privacy masks in an optimal\nway.\n", "versions": [{"version": "v1", "created": "Tue, 11 Aug 2020 17:55:47 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 16:47:49 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 16:16:33 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2020 23:32:36 GMT"}, {"version": "v5", "created": "Tue, 29 Sep 2020 21:15:37 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Fang", "Song", ""], ["Zhu", "Quanyan", ""]]}, {"id": "2008.05052", "submitter": "Sisi Ma", "authors": "Sisi Ma, Roshan Tourani", "title": "Predictive and Causal Implications of using Shapley Value for Model\n  Interpretation", "comments": "Accepted by KDD CD workshop 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shapley value is a concept from game theory. Recently, it has been used for\nexplaining complex models produced by machine learning techniques. Although the\nmathematical definition of Shapley value is straight-forward, the implication\nof using it as a model interpretation tool is yet to be described. In the\ncurrent paper, we analyzed Shapley value in the Bayesian network framework. We\nestablished the relationship between Shapley value and conditional\nindependence, a key concept in both predictive and causal modeling. Our results\nindicate that, eliminating a variable with high Shapley value from a model do\nnot necessarily impair predictive performance, whereas eliminating a variable\nwith low Shapley value from a model could impair performance. Therefore, using\nShapley value for feature selection do not result in the most parsimonious and\npredictively optimal model in the general case. More importantly, Shapley value\nof a variable do not reflect their causal relationship with the target of\ninterest.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 01:08:08 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Ma", "Sisi", ""], ["Tourani", "Roshan", ""]]}, {"id": "2008.05177", "submitter": "Gil Kalai", "authors": "Yosef Rinott, Tomer Shoham, and Gil Kalai", "title": "Statistical Aspects of the Quantum Supremacy Demonstration", "comments": "38 pages, 9 figures (v3. some additional analysis), to appear in\n  Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.CC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notable claim of quantum supremacy presented by Google's team in 2019\nconsists of demonstrating the ability of a quantum circuit to generate, albeit\nwith considerable noise, bitstrings from a distribution that is considered hard\nto simulate on classical computers. Verifying that the generated data is indeed\nfrom the claimed distribution and assessing the circuit's noise level and its\nfidelity is a purely statistical undertaking. The objective of this paper is to\nexplain the relations between quantum computing and some of the statistical\naspects involved in demonstrating quantum supremacy in terms that are\naccessible to statisticians, computer scientists, and mathematicians. Starting\nwith the statistical analysis in Google's demonstration, which we explain, we\nstudy various estimators of the fidelity, and different approaches to testing\nthe distributions generated by the quantum computer. We propose different noise\nmodels, and discuss their implications. A preliminary study of the Google data,\nfocusing mostly on circuits of 12 and 14 qubits is discussed throughout the\npaper.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 08:46:02 GMT"}, {"version": "v2", "created": "Fri, 14 Aug 2020 07:34:45 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 19:24:41 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Rinott", "Yosef", ""], ["Shoham", "Tomer", ""], ["Kalai", "Gil", ""]]}, {"id": "2008.05353", "submitter": "Yusuke Kaino", "authors": "Yusuke Kaino, Masayuki Uchida", "title": "Adaptive estimator for a parabolic linear SPDE with a small noise", "comments": "36 pages, 36 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with parametric estimation for a parabolic linear second order\nstochastic partial differential equation (SPDE) with a small dispersion\nparameter based on high frequency data which are observed in time and space. By\nusing the thinned data with respect to space obtained from the high frequency\ndata, the minimum contrast estimators of two coefficient parameters of the SPDE\nare proposed. With these estimators and the thinned data with respect to time\nobtained from the high frequency data, we construct an approximation of the\ncoordinate process of the SPDE. Using the approximate coordinate process, we\nobtain the adaptive estimator of a coefficient parameter of the SPDE. Moreover,\nwe give simulation results of the proposed estimators of the SPDE.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 14:50:11 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Kaino", "Yusuke", ""], ["Uchida", "Masayuki", ""]]}, {"id": "2008.05448", "submitter": "Rodrigo Labouriau", "authors": "Rodrigo Labouriau", "title": "Construction and Extension of Dispersion Models", "comments": "19 pages, 2 figures. Added examples and corrected minor typos in the\n  version v2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two main classes of dispersion models studied in the literature:\nproper (PDM), and exponential dispersion models (EDM). Dispersion models that\nare neither proper nor exponential dispersion models are termed here\nnon-standard dispersion models (NSDM). This paper exposes a technique for\nconstructing new PDMs and NSDMs. This construction provides a solution to an\nopen question in the theory of dispersion models about the extension of\nnon-standard dispersion models. Given a unit deviance function, a dispersion\nmodel is usually constructed by calculating a normalising function that makes\nthe density function integrates one. This calculation involves the solution of\nnon-trivial integral equations. The main idea explored here is to use\ncharacteristic functions of real non-lattice symmetric probability measures to\nconstruct a family of unit deviances that are sufficiently regular to make the\nassociated integral equations tractable. The integral equations associated to\nthose unit deviances admit a trivial solution, in the sense that the\nnormalising function is a constant function independent of the observed values.\nHowever, we show, using the machinery of distributions (i.e., generalised\nfunctions) and expansions of the normalising function with respect to specially\nconstructed Riez systems, that those integral equations also admit infinitely\nmany non-trivial solutions, generating many NSDMs. We conclude that, the\ncardinality of the class of non-standard dispersion models is larger than the\ncardinality of the class of real non-lattice symmetric probability measures.\n", "versions": [{"version": "v1", "created": "Wed, 12 Aug 2020 17:24:28 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 12:51:47 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Labouriau", "Rodrigo", ""]]}, {"id": "2008.05793", "submitter": "Valentin De Bortoli", "authors": "Valentin De Bortoli, Alain Durmus, Ana F. Vidal, Marcelo Pereyra", "title": "Maximum likelihood estimation of regularisation parameters in\n  high-dimensional inverse problems: an empirical Bayesian approach. Part II:\n  Theoretical Analysis", "comments": "SIIMS 2020 - 30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a detailed theoretical analysis of the three stochastic\napproximation proximal gradient algorithms proposed in our companion paper [49]\nto set regularization parameters by marginal maximum likelihood estimation. We\nprove the convergence of a more general stochastic approximation scheme that\nincludes the three algorithms of [49] as special cases. This includes\nasymptotic and non-asymptotic convergence results with natural and easily\nverifiable conditions, as well as explicit bounds on the convergence rates.\nImportantly, the theory is also general in that it can be applied to other\nintractable optimisation problems. A main novelty of the work is that the\nstochastic gradient estimates of our scheme are constructed from inexact\nproximal Markov chain Monte Carlo samplers. This allows the use of samplers\nthat scale efficiently to large problems and for which we have precise\ntheoretical guarantees.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 10:10:00 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["De Bortoli", "Valentin", ""], ["Durmus", "Alain", ""], ["Vidal", "Ana F.", ""], ["Pereyra", "Marcelo", ""]]}, {"id": "2008.05974", "submitter": "Yinqiu He", "authors": "Yinqiu He, Bo Meng, Zhenghao Zeng, and Gongjun Xu", "title": "On the Phase Transition of Wilk's Phenomenon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wilk's theorem, which offers universal chi-squared approximations for\nlikelihood ratio tests, is widely used in many scientific hypothesis testing\nproblems. For modern datasets with increasing dimension, researchers have found\nthat the conventional Wilk's phenomenon of the likelihood ratio test statistic\noften fails. Although new approximations have been proposed in high dimensional\nsettings, there still lacks a clear statistical guideline regarding how to\nchoose between the conventional and newly proposed approximations, especially\nfor moderate-dimensional data. To address this issue, we develop the necessary\nand sufficient phase transition conditions for Wilk's phenomenon under popular\ntests on multivariate mean and covariance structures. Moreover, we provide an\nin-depth analysis of the accuracy of chi-squared approximations by deriving\ntheir asymptotic biases. These results may provide helpful insights into the\nuse of chi-squared approximations in scientific practices.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 15:42:56 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["He", "Yinqiu", ""], ["Meng", "Bo", ""], ["Zeng", "Zhenghao", ""], ["Xu", "Gongjun", ""]]}, {"id": "2008.06022", "submitter": "Arun Kumar", "authors": "Neha Gupta and Arun Kumar", "title": "Fractional Poisson Processes of Order k and Beyond", "comments": "21 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce fractional Poisson felds of order k in\nn-dimensional Euclidean space $R_n^+$. We also work on time-fractional Poisson\nprocess of order k, space-fractional Poisson process of order k and tempered\nversion of time-space fractional Poisson process of order k in one dimensional\nEuclidean space $R_1^+$. These processes are defined in terms of fractional\ncompound Poisson processes. Time-fractional Poisson process of order k\nnaturally generalizes the Poisson process and Poisson process of order k to a\nheavy tailed waiting times counting process. The space-fractional Poisson\nprocess of order k, allows on average infinite number of arrivals in any\ninterval. We derive the marginal probabilities, governing\ndifference-differential equations of the introduced processes. We also provide\nWatanabe martingale characterization for some time-changed Poisson processes.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 17:22:46 GMT"}, {"version": "v2", "created": "Sat, 27 Feb 2021 10:00:36 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 10:58:12 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Gupta", "Neha", ""], ["Kumar", "Arun", ""]]}, {"id": "2008.06136", "submitter": "Guenther Walther", "authors": "Guenther Walther and Andrew Perry", "title": "Calibrating the scan statistic: finite sample performance vs.\n  asymptotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting an elevated mean on an interval with\nunknown location and length in the univariate Gaussian sequence model. Recent\nresults have shown that using scale-dependent critical values for the scan\nstatistic allows to attain asymptotically optimal detection simultaneously for\nall signal lengths, thereby improving on the traditional scan, but this\nprocedure has been criticized for losing too much power for short signals. We\nexplain this discrepancy by showing that these asymptotic optimality results\nwill necessarily be too imprecise to discern the performance of scan statistics\nin a practically relevant way, even in a large sample context. Instead, we\npropose to assess the performance with a new finite sample criterion. We then\npresent three calibrations for scan statistics that perform well across a range\nof relevant signal lengths: The first calibration uses a particular adjustment\nto the critical values and is therefore tailored to the Gaussian case. The\nsecond calibration uses a scale-dependent adjustment to the significance levels\nand is therefore applicable to arbitrary known null distributions. The third\ncalibration restricts the scan to a particular sparse subset of the scan\nwindows and then applies a weighted Bonferroni adjustment to the corresponding\ntest statistics. This {\\sl Bonferroni scan} is also applicable to arbitrary\nnull distributions and in addition is very simple to implement. We show how to\napply these calibrations for scanning in a number of distributional settings:\nfor normal observations with an unknown baseline and a known or unknown\nconstant variance,for observations from a natural exponential family, for\npotentially heteroscadastic observations from a symmetric density by employing\nself-normalization in a novel way, and for exchangeable observations using\ntests based on permutations, ranks or signs.\n", "versions": [{"version": "v1", "created": "Thu, 13 Aug 2020 23:44:18 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 17:39:54 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Walther", "Guenther", ""], ["Perry", "Andrew", ""]]}, {"id": "2008.06400", "submitter": "Likun Zhang", "authors": "Likun Zhang and Benjamin Shaby", "title": "Uniqueness and global optimality of the maximum likelihood estimator for\n  the generalized extreme value distribution", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The three-parameter generalized extreme value distribution arises from\nclassical univariate extreme value theory and is in common use for analyzing\nthe far tail of observed phenomena. Curiously, important asymptotic properties\nof likelihood-based estimation under this standard model have yet to be\nestablished. In this paper, we formally prove that the maximum likelihood\nestimator is global and unique. An interesting secondary result entails the\nuniform consistency of a class of limit relations in a tight neighborhood of\nthe shape parameter.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 14:57:07 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Zhang", "Likun", ""], ["Shaby", "Benjamin", ""]]}, {"id": "2008.06596", "submitter": "Yinqiu He", "authors": "Yinqiu He, Zi Wang, and Gongjun Xu", "title": "A Note on the Likelihood Ratio Test in High-Dimensional Exploratory\n  Factor Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The likelihood ratio test is widely used in exploratory factor analysis to\nassess the model fit and determine the number of latent factors. Despite its\npopularity and clear statistical rationale, researchers have found that when\nthe dimension of the response data is large compared to the sample size, the\nclassical chi-square approximation of the likelihood ratio test statistic often\nfails. Theoretically, it has been an open problem when such a phenomenon\nhappens as the dimension of data increases; practically, the effect of high\ndimensionality is less examined in exploratory factor analysis, and there lacks\na clear statistical guideline on the validity of the conventional chi-square\napproximation. To address this problem, we investigate the failure of the\nchi-square approximation of the likelihood ratio test in high-dimensional\nexploratory factor analysis, and derive the necessary and sufficient condition\nto ensure the validity of the chi-square approximation. The results yield\nsimple quantitative guidelines to check in practice and would also provide\nuseful statistical insights into the practice of exploratory factor analysis.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 22:46:05 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2021 23:14:55 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["He", "Yinqiu", ""], ["Wang", "Zi", ""], ["Xu", "Gongjun", ""]]}, {"id": "2008.06620", "submitter": "Seonghyun Jeong", "authors": "Seonghyun Jeong and Veronika Rockova", "title": "The art of BART: On flexibility of Bayesian forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerable effort has been directed to developing asymptotically minimax\nprocedures in problems of recovering functions and densities. These methods\noften rely on somewhat arbitrary and restrictive assumptions such as isotropy\nor spatial homogeneity. This work enhances theoretical understanding of\nBayesian forests (including BART) under substantially relaxed smoothness\nassumptions. In particular, we provide a comprehensive study of asymptotic\noptimality and posterior contraction of Bayesian forests when the regression\nfunction has anisotropic smoothness that possibly varies over the function\ndomain. We introduce a new class of sparse piecewise heterogeneous anisotropic\nH\\\"{o}lder functions and derive their minimax rate of estimation in\nhigh-dimensional scenarios under the $L_2$ loss. Next, we find that the default\nBayesian CART prior, coupled with a subset selection prior for sparse\nestimation in high-dimensional scenarios, adapts to unknown heterogeneous\nsmoothness and sparsity. These results show that Bayesian forests are uniquely\nsuited for more general estimation problems which would render other default\nmachine learning tools, such as Gaussian processes, suboptimal. Beyond\nnonparametric regression, we also show that Bayesian forests can be\nsuccessfully applied to many other problems including density estimation and\nbinary classification.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 01:28:58 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 16:40:43 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Jeong", "Seonghyun", ""], ["Rockova", "Veronika", ""]]}, {"id": "2008.06664", "submitter": "Yun S. Song", "authors": "Dan D. Erdmann-Pham, Jonathan Terhorst, Yun S. Song", "title": "Generalized Spacing-Statistics and a New Family of Non-Parametric Tests", "comments": "31 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random divisions of an interval arise in various context, including\nstatistics, physics, and geometric analysis. For testing the uniformity of a\nrandom partition of the unit interval $[0,1]$ into $k$ disjoint subintervals of\nsize $(S_k[1],\\ldots,S_k[k])$, Greenwood (1946) suggested using the squared\n$\\ell_2$-norm of this size vector as a test statistic, prompting a number of\nsubsequent studies. Despite much progress on understanding its power and\nasymptotic properties, attempts to find its exact distribution have succeeded\nso far for only small values of $k$. Here, we develop an efficient method to\ncompute the distribution of the Greenwood statistic and more general\nspacing-statistics for an arbitrary value of $k$. Specifically, we consider\nrandom divisions of $\\{1,2,\\dots,n\\}$ into $k$ subsets of consecutive integers\nand study $\\|S_{n,k}\\|^p_{p,w}$, the $p$th power of the weighted $\\ell_p$-norm\nof the subset size vector $S_{n,k}=(S_{n,k}[1],\\ldots,S_{n,k}[k])$ for\narbitrary weights $w=(w_1,\\ldots,w_k)$. We present an exact and quickly\ncomputable formula for its moments, as well as a simple algorithm to accurately\nreconstruct a probability distribution using the moment sequence. We also study\nvarious scaling limits, one of which corresponds to the Greenwood statistic in\nthe case of $p=2$ and $w=(1,\\ldots,1)$, and this connection allows us to obtain\ninformation about regularity, monotonicity and local behavior of its\ndistribution. Lastly, we devise a new family of non-parametric tests using\n$\\|S_{n,k}\\|^p_{p,w}$ and demonstrate that they exhibit substantially improved\npower for a large class of alternatives, compared to existing popular methods\nsuch as the Kolmogorov-Smirnov, Cramer-von Mises, and Mann-Whitney/Wilcoxon\nrank-sum tests.\n", "versions": [{"version": "v1", "created": "Sat, 15 Aug 2020 06:53:55 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Erdmann-Pham", "Dan D.", ""], ["Terhorst", "Jonathan", ""], ["Song", "Yun S.", ""]]}, {"id": "2008.06858", "submitter": "Denis Belomestny", "authors": "D. Belomestny, L. Iosipoi, E. Moulines, A. Naumov, S. Samsonov", "title": "Variance reduction for dependent sequences with applications to\n  Stochastic Gradient MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel and practical variance reduction approach\nfor additive functionals of dependent sequences. Our approach combines the use\nof control variates with the minimisation of an empirical variance estimate. We\nanalyse finite sample properties of the proposed method and derive finite-time\nbounds of the excess asymptotic variance to zero. We apply our methodology to\nStochastic Gradient MCMC (SGMCMC) methods for Bayesian inference on large data\nsets and combine it with existing variance reduction methods for SGMCMC. We\npresent empirical results carried out on a number of benchmark examples showing\nthat our variance reduction method achieves significant improvement as compared\nto state-of-the-art methods at the expense of a moderate increase of\ncomputational overhead.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 08:33:19 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Belomestny", "D.", ""], ["Iosipoi", "L.", ""], ["Moulines", "E.", ""], ["Naumov", "A.", ""], ["Samsonov", "S.", ""]]}, {"id": "2008.06874", "submitter": "Ryan Martin", "authors": "Chuanhai Liu and Ryan Martin", "title": "Inferential models and possibility measures", "comments": "21 pages, 3 figures. Comments welcome at\n  https://www.researchers.one/article/2020-08-29", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inferential model (IM) framework produces data-dependent, non-additive\ndegrees of belief about the unknown parameter that are provably valid. The\nvalidity property guarantees, among other things, that inference procedures\nderived from the IM control frequentist error rates at the nominal level. A\ntechnical complication is that IMs are built on a relatively unfamiliar theory\nof random sets. Here we develop an alternative---and practically\nequivalent---formulation, based on a theory of possibility measures, which is\nsimpler in many respects. This new perspective also sheds light on the\nrelationship between IMs and Fisher's fiducial inference, as well as on the\nconstruction of optimal IMs.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 10:34:16 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Liu", "Chuanhai", ""], ["Martin", "Ryan", ""]]}, {"id": "2008.06987", "submitter": "Soumik Purkayastha", "authors": "Soumik Purkayastha and Ayanendranath Basu", "title": "On minimum Bregman divergence inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper a new family of minimum divergence estimators based on the\nBregman divergence is proposed. The popular density power divergence (DPD)\nclass of estimators is a sub-class of Bregman divergences. We propose and study\na new sub-class of Bregman divergences called the exponentially weighted\ndivergence (EWD). Like the minimum DPD estimator, the minimum EWD estimator is\nrecognised as an M-estimator. This characterisation is useful while discussing\nthe asymptotic behaviour as well as the robustness properties of this class of\nestimators. Performances of the two classes are compared -- both through\nsimulations as well as through real life examples. We develop an estimation\nprocess not only for independent and homogeneous data, but also for\nnon-homogeneous data. General tests of parametric hypotheses based on the\nBregman divergences are also considered. We establish the asymptotic null\ndistribution of our proposed test statistic and explore its behaviour when\napplied to real data. The inference procedures generated by the new EWD\ndivergence appear to be competitive or better that than the DPD based\nprocedures.\n", "versions": [{"version": "v1", "created": "Sun, 16 Aug 2020 20:10:03 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Purkayastha", "Soumik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "2008.07107", "submitter": "Yang Ning", "authors": "Yang Ning, Guang Cheng", "title": "Sparse Confidence Sets for Normal Mean Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new framework to construct confidence sets for a\n$d$-dimensional unknown sparse parameter $\\theta$ under the normal mean model\n$X\\sim N(\\theta,\\sigma^2I)$. A key feature of the proposed confidence set is\nits capability to account for the sparsity of $\\theta$, thus named as {\\em\nsparse} confidence set. This is in sharp contrast with the classical methods,\nsuch as Bonferroni confidence intervals and other resampling based procedures,\nwhere the sparsity of $\\theta$ is often ignored. Specifically, we require the\ndesired sparse confidence set to satisfy the following two conditions: (i)\nuniformly over the parameter space, the coverage probability for $\\theta$ is\nabove a pre-specified level; (ii) there exists a random subset $S$ of\n$\\{1,...,d\\}$ such that $S$ guarantees the pre-specified true negative rate\n(TNR) for detecting nonzero $\\theta_j$'s. To exploit the sparsity of $\\theta$,\nwe define that the confidence interval for $\\theta_j$ degenerates to a single\npoint 0 for any $j\\notin S$. Under this new framework, we first consider\nwhether there exist sparse confidence sets that satisfy the above two\nconditions. To address this question, we establish a non-asymptotic minimax\nlower bound for the non-coverage probability over a suitable class of sparse\nconfidence sets. The lower bound deciphers the role of sparsity and minimum\nsignal-to-noise ratio (SNR) in the construction of sparse confidence sets.\nFurthermore, under suitable conditions on the SNR, a two-stage procedure is\nproposed to construct a sparse confidence set. To evaluate the optimality, the\nproposed sparse confidence set is shown to attain a minimax lower bound of some\nproperly defined risk function up to a constant factor. Finally, we develop an\nadaptive procedure to the unknown sparsity and SNR. Numerical studies are\nconducted to verify the theoretical results.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 06:16:18 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ning", "Yang", ""], ["Cheng", "Guang", ""]]}, {"id": "2008.07330", "submitter": "Puja Sahu", "authors": "Puja Sahu and Nandyala Hemachandra", "title": "Optimal Posteriors for Chi-squared Divergence based PAC-Bayesian Bounds\n  and Comparison with KL-divergence based Optimal Posteriors and\n  Cross-Validation Procedure", "comments": "arXiv admin note: text overlap with arXiv:1912.06803", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigate optimal posteriors for recently introduced \\cite{begin2016pac}\nchi-squared divergence based PAC-Bayesian bounds in terms of nature of their\ndistribution, scalability of computations, and test set performance. For a\nfinite classifier set, we deduce bounds for three distance functions:\nKL-divergence, linear and squared distances. Optimal posterior weights are\nproportional to deviations of empirical risks, usually with subset support. For\nuniform prior, it is sufficient to search among posteriors on classifier\nsubsets ordered by these risks. We show the bound minimization for linear\ndistance as a convex program and obtain a closed-form expression for its\noptimal posterior. Whereas that for squared distance is a quasi-convex program\nunder a specific condition, and the one for KL-divergence is non-convex\noptimization (a difference of convex functions). To compute such optimal\nposteriors, we derive fast converging fixed point (FP) equations. We apply\nthese approaches to a finite set of SVM regularization parameter values to\nyield stochastic SVMs with tight bounds. We perform a comprehensive performance\ncomparison between our optimal posteriors and known KL-divergence based\nposteriors on a variety of UCI datasets with varying ranges and variances in\nrisk values, etc. Chi-squared divergence based posteriors have weaker bounds\nand worse test errors, hinting at an underlying regularization by KL-divergence\nbased posteriors. Our study highlights the impact of divergence function on the\nperformance of PAC-Bayesian classifiers. We compare our stochastic classifiers\nwith cross-validation based deterministic classifier. The latter has better\ntest errors, but ours is more sample robust, has quantifiable generalization\nguarantees, and is computationally much faster.\n", "versions": [{"version": "v1", "created": "Fri, 14 Aug 2020 03:15:23 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Sahu", "Puja", ""], ["Hemachandra", "Nandyala", ""]]}, {"id": "2008.07382", "submitter": "Philip Boeken", "authors": "Philip A. Boeken, Joris M. Mooij", "title": "A Bayesian Nonparametric Conditional Two-sample Test with an Application\n  to Local Causal Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a continuous random variable $Z$, testing conditional independence $X\n\\perp \\!\\!\\! \\perp Y |Z$ is known to be a particularly hard problem. It\nconstitutes a key ingredient of many constraint-based causal discovery\nalgorithms. These algorithms are often applied to datasets containing binary\n(or discrete) variables, which indicate the 'context' of the observations, e.g.\na control or treatment group within an experiment. In these settings,\nconditional independence testing with $X$ or $Y$ discrete (and the other\ncontinuous) is paramount to the performance of the causal discovery algorithm.\nTo our knowledge no such conditional independence test currently exists, and in\npractice tests which assume all variables to be continuous are used instead. In\nthis paper we aim to fill this gap, as we combine elements of Holmes et al.\n(2015) and Teymur and Filippi (2020) to propose a novel Bayesian nonparametric\nconditional two-sample test. Applied to the Local Causal Discovery algorithm,\nwe investigate its performance on both synthetic and real-world data, and\ncompare with state-of-the-art continuous conditional independence tests.\n", "versions": [{"version": "v1", "created": "Mon, 17 Aug 2020 14:51:45 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 09:00:36 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Boeken", "Philip A.", ""], ["Mooij", "Joris M.", ""]]}, {"id": "2008.07826", "submitter": "Maria Longobardi Prof.Dr.", "authors": "Narayanaswamy Balakrishnan, Francesco Buono and Maria Longobardi", "title": "On Weighted Extropies", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The extropy is a measure of information introduced by Lad et al. (2015) as\ndual to entropy. As the entropy, it is a shift-independent information measure.\nWe introduce here the notion of weighted extropy, a shift-dependent information\nmeasure which gives higher weights to large values of observed random\nvariables. We also study the weighted residual and past extropies as weighted\nversions of extropy for residual and past lifetimes. Bivariate versions extropy\nand weighted extropy are also provided. Several examples are presented through\nout to illustrate the various concepts introduced here.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 09:39:08 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Balakrishnan", "Narayanaswamy", ""], ["Buono", "Francesco", ""], ["Longobardi", "Maria", ""]]}, {"id": "2008.07954", "submitter": "Hao Chen Dr.", "authors": "Hao Chen, Lanshan Han and Alvin Lim", "title": "A Note on the Sum of Non-Identically Distributed Doubly Truncated Normal\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is proved that the sum of n independent but non-identically distributed\ndoubly truncated Normal distributions converges in distribution to a Normal\ndistribution. It is also shown how the result can be applied in estimating a\nconstrained mixed effects model.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 14:33:07 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2020 19:44:26 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 15:15:12 GMT"}, {"version": "v4", "created": "Thu, 15 Jul 2021 14:52:55 GMT"}, {"version": "v5", "created": "Sat, 17 Jul 2021 23:21:59 GMT"}, {"version": "v6", "created": "Wed, 21 Jul 2021 00:12:33 GMT"}, {"version": "v7", "created": "Thu, 22 Jul 2021 20:59:17 GMT"}, {"version": "v8", "created": "Thu, 29 Jul 2021 01:58:49 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Chen", "Hao", ""], ["Han", "Lanshan", ""], ["Lim", "Alvin", ""]]}, {"id": "2008.08071", "submitter": "Lunjia Hu", "authors": "Lunjia Hu, Omer Reingold", "title": "Robust Mean Estimation on Highly Incomplete Data with Arbitrary Outliers", "comments": "29 pages, 2 figures. Published in AISTATS 2021. More details in the\n  proof of Claim 14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of robustly estimating the mean of a $d$-dimensional\ndistribution given $N$ examples, where most coordinates of every example may be\nmissing and $\\varepsilon N$ examples may be arbitrarily corrupted. Assuming\neach coordinate appears in a constant factor more than $\\varepsilon N$\nexamples, we show algorithms that estimate the mean of the distribution with\ninformation-theoretically optimal dimension-independent error guarantees in\nnearly-linear time $\\widetilde O(Nd)$. Our results extend recent work on\ncomputationally-efficient robust estimation to a more widely applicable\nincomplete-data setting.\n", "versions": [{"version": "v1", "created": "Tue, 18 Aug 2020 17:53:34 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 07:50:25 GMT"}, {"version": "v3", "created": "Tue, 2 Mar 2021 01:13:12 GMT"}, {"version": "v4", "created": "Sat, 6 Mar 2021 19:39:54 GMT"}, {"version": "v5", "created": "Mon, 3 May 2021 04:25:56 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hu", "Lunjia", ""], ["Reingold", "Omer", ""]]}, {"id": "2008.08244", "submitter": "Yihong Wu", "authors": "Yury Polyanskiy and Yihong Wu", "title": "Self-regularizing Property of Nonparametric Maximum Likelihood Estimator\n  in Mixture Models", "comments": "Refer conjecture of [DYPS20] in Sec 5.4 to the arxiv version\n  arXiv:1901.03264v4", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Introduced by Kiefer and Wolfowitz \\cite{KW56}, the nonparametric maximum\nlikelihood estimator (NPMLE) is a widely used methodology for learning mixture\nodels and empirical Bayes estimation. Sidestepping the non-convexity in mixture\nlikelihood, the NPMLE estimates the mixing distribution by maximizing the total\nlikelihood over the space of probability measures, which can be viewed as an\nextreme form of overparameterization.\n  In this paper we discover a surprising property of the NPMLE solution.\nConsider, for example, a Gaussian mixture model on the real line with a\nsubgaussian mixing distribution. Leveraging complex-analytic techniques, we\nshow that with high probability the NPMLE based on a sample of size $n$ has\n$O(\\log n)$ atoms (mass points), significantly improving the deterministic\nupper bound of $n$ due to Lindsay \\cite{lindsay1983geometry1}. Notably, any\nsuch Gaussian mixture is statistically indistinguishable from a finite one with\n$O(\\log n)$ components (and this is tight for certain mixtures). Thus, absent\nany explicit form of model selection, NPMLE automatically chooses the right\nmodel complexity, a property we term \\emph{self-regularization}. Extensions to\nother exponential families are given. As a statistical application, we show\nthat this structural property can be harnessed to bootstrap existing Hellinger\nrisk bound of the (parametric) MLE for finite Gaussian mixtures to the NPMLE\nfor general Gaussian mixtures, recovering a result of Zhang\n\\cite{zhang2009generalized}.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 03:39:13 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 01:10:12 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Polyanskiy", "Yury", ""], ["Wu", "Yihong", ""]]}, {"id": "2008.08275", "submitter": "Xun Zhang", "authors": "Xun Zhang, Zhisheng Ye, William B. Haskell", "title": "Asymptotic Analysis for Data-Driven Inventory Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study periodic review stochastic inventory control in the data-driven\nsetting, in which the retailer makes ordering decisions based only on\nhistorical demand observations without any knowledge of the probability\ndistribution of the demand. Since an $(s, S)$-policy is optimal when the demand\ndistribution is known, we investigate the statistical properties of the\ndata-driven $(s, S)$-policy obtained by recursively computing the empirical\ncost-to-go functions. This policy is inherently challenging to analyze because\nthe recursion induces propagation of the estimation error backwards in time. In\nthis work, we establish the asymptotic properties of this data-driven policy by\nfully accounting for the error propagation. First, we rigorously show the\nconsistency of the estimated parameters by filling in some gaps (due to\nunaccounted error propagation) in the existing studies. On the other hand,\nempirical process theory cannot be directly applied to show asymptotic\nnormality. To explain, the empirical cost-to-go functions for the estimated\nparameters are not i.i.d. sums, again due to the error propagation. Our main\nmethodological innovation comes from an asymptotic representation for\nmulti-sample $U$-processes in terms of i.i.d. sums. This representation enables\nus to apply empirical process theory to derive the influence functions of the\nestimated parameters and establish joint asymptotic normality. Based on these\nresults, we also propose an entirely data-driven estimator of the optimal\nexpected cost and we derive its asymptotic distribution. We demonstrate some\nuseful applications of our asymptotic results, including sample size\ndetermination, as well as interval estimation and hypothesis testing on vital\nparameters of the inventory problem. The results from our numerical simulations\nconform to our theoretical analysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 05:38:50 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["Zhang", "Xun", ""], ["Ye", "Zhisheng", ""], ["Haskell", "William B.", ""]]}, {"id": "2008.08718", "submitter": "Yaroslav Averyanov", "authors": "Yaroslav Averyanov and Alain Celisse", "title": "Minimum discrepancy principle strategy for choosing $k$ in $k$-NN\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel data-driven strategy to choose the hyperparameter $k$ in\nthe $k$-NN regression estimator. We treat the problem of choosing the\nhyperparameter as an iterative procedure (over $k$) and propose using an easily\nimplemented in practice strategy based on the idea of early stopping and the\nminimum discrepancy principle. This model selection strategy is proven to be\nminimax-optimal, under the fixed-design assumption on covariates, over some\nsmoothness function classes, for instance, the Lipschitz functions class on a\nbounded domain. The novel method often improves statistical performance on\nartificial and real-world data sets in comparison to other model selection\nstrategies, such as the Hold-out method and 5-fold cross-validation. The\nnovelty of the strategy comes from reducing the computational time of the model\nselection procedure while preserving the statistical (minimax) optimality of\nthe resulting estimator. More precisely, given a sample of size $n$, assuming\nthat the nearest neighbors are already precomputed, if one should choose $k$\namong $\\left\\{ 1, \\ldots, n \\right\\}$, the strategy reduces the computational\ntime of the generalized cross-validation or Akaike's AIC criteria from\n$\\mathcal{O}\\left( n^3 \\right)$ to $\\mathcal{O}\\left( n^2 (n - k) \\right)$,\nwhere $k$ is the proposed (minimum discrepancy principle) value of the nearest\nneighbors. Code for the simulations is provided at\nhttps://github.com/YaroslavAveryanov/Minimum-discrepancy-principle-for-choosing-k.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 00:13:19 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2021 15:11:35 GMT"}, {"version": "v3", "created": "Fri, 26 Feb 2021 11:53:33 GMT"}, {"version": "v4", "created": "Wed, 5 May 2021 11:33:16 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Averyanov", "Yaroslav", ""], ["Celisse", "Alain", ""]]}, {"id": "2008.08754", "submitter": "Irfan Alam", "authors": "Irfan Alam", "title": "Generalizing the de Finetti--Hewitt--Savage theorem", "comments": "80 pages, 1 diagram, 2 appendices; updated with the version submitted\n  to a journal---added some references, corrected typos/notation, expanded the\n  discussion at a couple of places", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.LO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The original formulation of de Finetti's theorem says that an exchangeable\nsequence of Bernoulli random variables is a mixture of iid sequences of random\nvariables. Following the work of Hewitt and Savage, this theorem is known for\nseveral classes of exchangeable random variables (for instance, for Baire\nmeasurable random variables taking values in a compact Hausdorff space, and for\nBorel measurable random variables taking values in a Polish space). Under an\nassumption of the underlying common distribution being Radon, we show that de\nFinetti's theorem holds for a sequence of Borel measurable exchangeable random\nvariables taking values in any Hausdorff space. This includes and generalizes\nthe currently known versions of de Finetti's theorem. We use nonstandard\nanalysis to first study the empirical measures induced by hyperfinitely many\nidentically distributed random variables, which leads to a proof of de\nFinetti's theorem in great generality while retaining the combinatorial\nintuition of proofs of simpler versions of de Finetti's theorem. The required\ntools from topological measure theory are developed with the aid of\nperspectives provided by nonstandard measure theory. One highlight of this\ndevelopment is a new generalization of Prokhorov's theorem.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 03:37:48 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 21:41:47 GMT"}], "update_date": "2020-11-05", "authors_parsed": [["Alam", "Irfan", ""]]}, {"id": "2008.08778", "submitter": "Kengne William", "authors": "William Kengne", "title": "Strong consistent model selection for general causal time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the strongly consistent question for model selection in a large\nclass of causal time series models, including AR($\\infty$), ARCH($\\infty$),\nTARCH($\\infty$), ARMA-GARCH and many classical others processes.\n  We propose a penalized criterion based on the quasi likelihood of the model.\n  We provide sufficient conditions that ensure the strong consistency of the\nproposed procedure. Also, the estimator of the parameter of the selected model\nobeys the law of iterated logarithm.\n  It appears that, unlike the result of the weak consistency obtained by Bardet\n{\\it et al.} \\cite{Bardet2020}, a dependence between the regularization\nparameter and the model structure is not needed.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 05:19:46 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Kengne", "William", ""]]}, {"id": "2008.08857", "submitter": "Maciej Skorski", "authors": "Maciej Skorski", "title": "Simple Analysis of Johnson-Lindenstrauss Transform under Neuroscience\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper re-analyzes a version of the celebrated Johnson-Lindenstrauss\nLemma, in which matrices are subjected to constraints that naturally emerge\nfrom neuroscience applications: a) sparsity and b) sign-consistency. This\nparticular variant was studied first by Allen-Zhu, Gelashvili, Micali, Shavit\nand more recently by Jagadeesan (RANDOM'19).\n  The contribution of this work is a novel proof, which in contrast to previous\nworks a) uses the modern probability toolkit, particularly basics of\nsub-gaussian and sub-gamma estimates b) is self-contained, with no dependencies\non subtle third-party results c) offers explicit constants.\n  At the heart of our proof is a novel variant of Hanson-Wright Lemma (on\nconcentration of quadratic forms). Of independent interest are also auxiliary\nfacts on sub-gaussian random variables.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 09:31:52 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Skorski", "Maciej", ""]]}, {"id": "2008.08990", "submitter": "Camilla Cali'", "authors": "Mohammad Reza Kazemia, Saeid Tahmasebib, Camilla Cal\\`i, Maria\n  Longobardi", "title": "Cumulative Residual Extropy of Minimum Ranked Set Sampling with Unequal\n  Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recently, an alternative measure of uncertainty called cumulative residual\nextropy (CREX) was proposed by Jahanshahi et al. (2019). In this paper, we\nconsider uncertainty measures of minimum ranked set sampling procedure with\nunequal samples (MinRSSU) in terms of CREX and its dynamic version and we\ncompare the uncertainty and information content of CREX based on MinRSSU and\nsimple random sampling (SRS) designs. Also, using simulation, we study on new\nestimators of CREX for MinRSSU and SRS designs in terms of bias and mean square\nerror. Finally, we provide a new discrimination measure of disparity between\nthe distribution of MinRSSU and parental data SRS.\n", "versions": [{"version": "v1", "created": "Thu, 20 Aug 2020 14:23:46 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Kazemia", "Mohammad Reza", ""], ["Tahmasebib", "Saeid", ""], ["Cal\u00ec", "Camilla", ""], ["Longobardi", "Maria", ""]]}, {"id": "2008.09239", "submitter": "Jing Liu", "authors": "Jing Liu, Aditya Deshmukh, Venugopal V. Veeravalli", "title": "Robust Mean Estimation in High Dimensions via $\\ell_0$ Minimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG eess.SP math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the robust mean estimation problem in high dimensions, where $\\alpha\n<0.5$ fraction of the data points can be arbitrarily corrupted. Motivated by\ncompressive sensing, we formulate the robust mean estimation problem as the\nminimization of the $\\ell_0$-`norm' of the outlier indicator vector, under\nsecond moment constraints on the inlier data points. We prove that the global\nminimum of this objective is order optimal for the robust mean estimation\nproblem, and we propose a general framework for minimizing the objective. We\nfurther leverage the $\\ell_1$ and $\\ell_p$ $(0<p<1)$, minimization techniques\nin compressive sensing to provide computationally tractable solutions to the\n$\\ell_0$ minimization problem. Both synthetic and real data experiments\ndemonstrate that the proposed algorithms significantly outperform\nstate-of-the-art robust mean estimation methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 00:19:48 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Liu", "Jing", ""], ["Deshmukh", "Aditya", ""], ["Veeravalli", "Venugopal V.", ""]]}, {"id": "2008.09259", "submitter": "Abdullah Qayed", "authors": "Abdullah Qayed and Dong Han", "title": "Homogeneity Test of Several High-Dimensional Covariance Matrices for\n  Stationary Processes under Non-normality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a homogeneity test for testing the equality of several\nhigh-dimensional covariance matrices for stationary processes with ignoring the\nassumption of normality. We give the asymptotic distribution of the proposed\ntest. The simulation illustrates that the proposed test has perfect\nperformance. Moreover, the power of the test can approach any high probability\nuniformly on a set of covariance matrices.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 01:45:43 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Qayed", "Abdullah", ""], ["Han", "Dong", ""]]}, {"id": "2008.09480", "submitter": "Alexis Derumigny", "authors": "Alexis Derumigny and Jean-David Fermanian", "title": "Conditional empirical copula processes and generalized dependence\n  measures", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the weak convergence of conditional empirical copula processes, when\nthe conditioning event has a nonzero probability. The validity of several\nbootstrap schemes is stated, including the exchangeable bootstrap. We define\ngeneral - possibly conditional - multivariate dependence measures and their\nestimators. By applying our theoretical results, we prove the asymptotic\nnormality of some estimators of such dependence measures.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 13:54:26 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Derumigny", "Alexis", ""], ["Fermanian", "Jean-David", ""]]}, {"id": "2008.09496", "submitter": "Antoine Maillard", "authors": "Antoine Maillard", "title": "Large deviations of extreme eigenvalues of generalized sample covariance\n  matrices", "comments": "7 pages + 7 pages appendix. Updated version matching the published\n  article", "journal-ref": null, "doi": "10.1209/0295-5075/133/20005", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.dis-nn math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an analytical technique to compute the probability of rare events\nin which the largest eigenvalue of a random matrix is atypically large (i.e.\\\nthe right tail of its large deviations). The results also transfer to the left\ntail of the large deviations of the smallest eigenvalue. The technique improves\nupon past methods by not requiring the explicit law of the eigenvalues, and we\napply it to a large class of random matrices that were previously out of reach.\nIn particular, we solve an open problem related to the performance of principal\ncomponents analysis on highly correlated data, and open the way towards\nanalyzing the high-dimensional landscapes of complex inference models. We probe\nour results using an importance sampling approach, effectively simulating\nevents with probability as small as $10^{-100}$.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 14:24:01 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 14:22:10 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Maillard", "Antoine", ""]]}, {"id": "2008.09498", "submitter": "Alexis Derumigny", "authors": "Alexis Derumigny, Jean-David Fermanian and Aleksey Min", "title": "Testing for equality between conditional copulas given discretized\n  conditioning events", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several procedures have been recently proposed to test the simplifying\nassumption for conditional copulas. Instead of considering pointwise\nconditioning events, we study the constancy of the conditional dependence\nstructure when some covariates belong to general borelian conditioning subsets.\nSeveral test statistics based on the equality of conditional Kendall's tau are\nintroduced, and we derive their asymptotic distributions under the null. When\nsuch conditioning events are not fixed ex ante, we propose a data-driven\nprocedure to recursively build such relevant subsets. It is based on decision\ntrees that maximize the differences between the conditional Kendall's taus\ncorresponding to the leaves of the trees. The performances of such tests are\nillustrated in a simulation experiment. Moreover, a study of the conditional\ndependence between financial stock returns is managed, given some clustering of\ntheir past values. The last application deals with the conditional dependence\nbetween coverage amounts in an insurance dataset.\n", "versions": [{"version": "v1", "created": "Fri, 21 Aug 2020 14:26:05 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Derumigny", "Alexis", ""], ["Fermanian", "Jean-David", ""], ["Min", "Aleksey", ""]]}, {"id": "2008.09787", "submitter": "TrungTin Nguyen", "authors": "TrungTin Nguyen, Faicel Chamroukhi, Hien D Nguyen, Geoffrey J\n  McLachlan", "title": "Approximation of probability density functions via location-scale finite\n  mixtures in Lebesgue spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of location-scale finite mixtures is of enduring interest both from\napplied and theoretical perspectives of probability and statistics. We prove\nthe following results: to an arbitrary degree of accuracy, (a) location-scale\nmixtures of a continuous probability density function (PDF) can approximate any\ncontinuous PDF, uniformly, on a compact set; and (b) for any finite $p\\ge1$,\nlocation-scale mixtures of an essentially bounded PDF can approximate any PDF\nin $\\mathcal{L}_{p}$, in the $\\mathcal{L}_{p}$ norm.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 09:24:41 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Nguyen", "TrungTin", ""], ["Chamroukhi", "Faicel", ""], ["Nguyen", "Hien D", ""], ["McLachlan", "Geoffrey J", ""]]}, {"id": "2008.09865", "submitter": "Serge Aleshin-Guendel", "authors": "Serge Aleshin-Guendel", "title": "On the Identifiability of Latent Class Models for Multiple-Systems\n  Estimation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent class models have recently become popular for multiple-systems\nestimation in human rights applications. However, it is currently unknown when\na given family of latent class models is identifiable in this context. We\nprovide necessary and sufficient conditions on the number of latent classes\nneeded for a family of latent class models to be identifiable. Along the way we\nprovide a mechanism for verifying identifiability in a class of\nmultiple-systems estimation models that allow for individual heterogeneity.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 16:14:52 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Aleshin-Guendel", "Serge", ""]]}, {"id": "2008.09897", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Paula Navarro-Esteban, Juan A.\n  Cuesta-Albertos", "title": "On a projection-based class of uniformity tests on the hypersphere", "comments": "26 pages, 3 figures, 6 tables. Supplementary material: 26 pages, 2\n  figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a projection-based class of uniformity tests on the hypersphere\nusing statistics that integrate, along all possible directions, the weighted\nquadratic discrepancy between the empirical cumulative distribution function of\nthe projected data and the projected uniform distribution. Simple expressions\nfor several test statistics are obtained for the circle and sphere, and\nrelatively tractable forms for higher dimensions. Despite its different origin,\nthe proposed class is shown to be related with the well-studied Sobolev class\nof uniformity tests. Our new class proves itself advantageous by allowing to\nderive new tests for hyperspherical data that neatly extend the circular tests\nby Watson, Ajne, and Rothman, and by introducing the first instance of an\nAnderson-Darling-like test for such data. The asymptotic distributions and the\nlocal optimality against certain alternatives of the new tests are obtained. A\nsimulation study evaluates the theoretical findings and evidences that, for\ncertain scenarios, the new tests are competitive against previous proposals.\nThe new tests are employed in three astronomical applications.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 18:28:14 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 10:25:28 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Navarro-Esteban", "Paula", ""], ["Cuesta-Albertos", "Juan A.", ""]]}, {"id": "2008.09925", "submitter": "Bhaswar Bhattacharya", "authors": "Bhaswar B. Bhattacharya and Kavita Ramanan", "title": "Parameter Estimation for Undirected Graphical Models with Hard\n  Constraints", "comments": "Introduction reorganized. 34 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The hardcore model on a graph $G$ with parameter $\\lambda>0$ is a probability\nmeasure on the collection of all independent sets of $G$, that assigns to each\nindependent set $I$ a probability proportional to $\\lambda^{|I|}$. In this\npaper we consider the problem of estimating the parameter $\\lambda$ given a\nsingle sample from the hardcore model on a graph $G$. To bypass the\ncomputational intractability of the maximum likelihood method, we use the\nmaximum pseudo-likelihood (MPL) estimator, which for the hardcore model has a\nsurprisingly simple closed form expression. We show that for any sequence of\ngraphs $\\{G_N\\}_{N\\geq 1}$, where $G_N$ is a graph on $N$ vertices, the MPL\nestimate of $\\lambda$ is $\\sqrt N$-consistent, whenever the graph sequence has\nuniformly bounded average degree. We then derive sufficient conditions under\nwhich the MPL estimate of the activity parameters is $\\sqrt N$-consistent given\na single sample from a general $H$-coloring model, in which restrictions\nbetween adjacent colors are encoded by a constraint graph $H$. We verify the\nsufficient conditions for models where there is at least one unconstrained\ncolor as long as the graph sequence has uniformly bounded average degree. This\napplies to many $H$-coloring examples such as the Widom-Rowlinson and\nmulti-state hard-core models. On the other hand, for the $q$-coloring model,\nwhich falls outside this class, we show that consistent estimation may be\nimpossible even for graphs with bounded average degree. Nevertheless, we show\nthat the MPL estimate is $\\sqrt N$-consistent in the $q$-coloring model when\n$\\{G_N\\}_{N\\geq 1}$ has bounded average double neighborhood. The presence of\nhard constraints, as opposed to soft constraints, leads to new challenges, and\nour proofs entail applications of the method of exchangeable pairs as well as\ncombinatorial arguments that employ the probabilistic method.\n", "versions": [{"version": "v1", "created": "Sat, 22 Aug 2020 23:04:42 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 05:10:24 GMT"}, {"version": "v3", "created": "Thu, 24 Jun 2021 03:47:45 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Bhattacharya", "Bhaswar B.", ""], ["Ramanan", "Kavita", ""]]}, {"id": "2008.10097", "submitter": "Sophie H. Yu", "authors": "Yihong Wu, Jiaming Xu, Sophie H. Yu", "title": "Testing correlation of unlabeled random graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO math.PR stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the problem of detecting the edge correlation between two random\ngraphs with $n$ unlabeled nodes. This is formalized as a hypothesis testing\nproblem, where under the null hypothesis, the two graphs are independently\ngenerated; under the alternative, the two graphs are edge-correlated under some\nlatent node correspondence, but have the same marginal distributions as the\nnull. For both Gaussian-weighted complete graphs and dense Erd\\H{o}s-R\\'enyi\ngraphs (with edge probability $n^{-o(1)}$), we determine the sharp threshold at\nwhich the optimal testing error probability exhibits a phase transition from\nzero to one as $n\\to \\infty$. For sparse Erd\\H{o}s-R\\'enyi graphs with edge\nprobability $n^{-\\Omega(1)}$, we determine the threshold within a constant\nfactor.\n  The proof of the impossibility results is an application of the conditional\nsecond-moment method, where we bound the truncated second moment of the\nlikelihood ratio by carefully conditioning on the typical behavior of the\nintersection graph (consisting of edges in both observed graphs) and taking\ninto account the cycle structure of the induced random permutation on the\nedges. Notably, in the sparse regime, this is accomplished by leveraging the\npseudoforest structure of subcritical Erd\\H{o}s-R\\'enyi graphs and a careful\nenumeration of subpseudoforests that can be assembled from short orbits of the\nedge permutation.\n", "versions": [{"version": "v1", "created": "Sun, 23 Aug 2020 19:19:45 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2021 01:57:36 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Wu", "Yihong", ""], ["Xu", "Jiaming", ""], ["Yu", "Sophie H.", ""]]}, {"id": "2008.10177", "submitter": "Sky Cao", "authors": "Sky Cao, Peter J. Bickel", "title": "Correlations with tailored extremal properties", "comments": "33 pages. Added important references. Section 4.4 rewritten. Added\n  Section 4.5", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Chatterjee has introduced a new coefficient of correlation which\nhas several natural properties. In particular, the coefficient attains its\nmaximal value if and only if one variable is a measurable function of the other\nvariable. In this paper, we seek to define correlations which have a similar\nproperty, except now the measurable function must belong to a pre-specified\nclass, which amounts to a shape restriction on the function. We will then look\nspecifically at the correlation corresponding to the class of monotone\nnondecreasing functions, in which case we can prove various asymptotic results,\nas well as perform local power calculations.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 03:31:18 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 04:04:10 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Cao", "Sky", ""], ["Bickel", "Peter J.", ""]]}, {"id": "2008.10230", "submitter": "Seonghyun Jeong", "authors": "Seonghyun Jeong, Subhashis Ghosal", "title": "Unified Bayesian theory of sparse linear regression with nuisance\n  parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study frequentist asymptotic properties of Bayesian procedures for\nhigh-dimensional Gaussian sparse regression when unknown nuisance parameters\nare involved. Nuisance parameters can be finite-, high-, or\ninfinite-dimensional. A mixture of point masses at zero and continuous\ndistributions is used for the prior distribution on sparse regression\ncoefficients, and appropriate prior distributions are used for nuisance\nparameters. The optimal posterior contraction of sparse regression\ncoefficients, hampered by the presence of nuisance parameters, is also examined\nand discussed. It is shown that the procedure yields strong model selection\nconsistency. A Bernstein-von Mises-type theorem for sparse regression\ncoefficients is also obtained for uncertainty quantification through credible\nsets with guaranteed frequentist coverage. Asymptotic properties of numerous\nexamples are investigated using the theories developed in this study.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 07:09:50 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 14:26:58 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Jeong", "Seonghyun", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "2008.10346", "submitter": "Anatol Eugen Wegner", "authors": "Anatol E. Wegner, Sofia Olhede", "title": "Atomic subgraphs and the statistical mechanics of networks", "comments": "15 pages, 2 figures", "journal-ref": "Phys. Rev. E 103, 042311 (2021)", "doi": "10.1103/PhysRevE.103.042311", "report-no": null, "categories": "math.ST cs.SI physics.data-an physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop random graph models where graphs are generated by connecting not\nonly pairs of vertices by edges but also larger subsets of vertices by copies\nof small atomic subgraphs of arbitrary topology. This allows the for the\ngeneration of graphs with extensive numbers of triangles and other network\nmotifs commonly observed in many real world networks. More specifically we\nfocus on maximum entropy ensembles under constraints placed on the counts and\ndistributions of atomic subgraphs and derive general expressions for the\nentropy of such models. We also present a procedure for combining distributions\nof multiple atomic subgraphs that enables the construction of models with fewer\nparameters. Expanding the model to include atoms with edge and vertex labels we\nobtain a general class of models that can be parametrized in terms of basic\nbuilding blocks and their distributions that includes many widely used models\nas special cases. These models include random graphs with arbitrary\ndistributions of subgraphs, random hypergraphs, bipartite models, stochastic\nblock models, models of multilayer networks and their degree corrected and\ndirected versions. We show that the entropy for all these models can be derived\nfrom a single expression that is characterized by the symmetry groups of atomic\nsubgraphs.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 12:07:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wegner", "Anatol E.", ""], ["Olhede", "Sofia", ""]]}, {"id": "2008.10446", "submitter": "Hideto Nakashima", "authors": "Hideto Nakashima, Piotr Graczyk", "title": "Wigner and Wishart Ensembles for graphical models", "comments": "27 pages, supplemental material 53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vinberg cones and the ambient vector spaces are important in modern\nstatistics of sparse models and of graphical models. The aim of this paper is\nto study eigenvalue distributions of Gaussian, Wigner and covariance matrices\nrelated to growing Vinberg matrices, corresponding to growing daisy graphs. For\nGaussian or Wigner ensembles, we give an explicit formula for the limiting\ndistribution. For Wishart ensembles defined naturally on Vinberg cones, their\nlimiting Stieltjes transforms, support and atom at 0 are described explicitly\nin terms of the Lambert-Tsallis functions, which are defined by using the\nTsallis $q$-exponential functions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Aug 2020 14:33:05 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 10:29:42 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Nakashima", "Hideto", ""], ["Graczyk", "Piotr", ""]]}, {"id": "2008.10503", "submitter": "Rishabh Dudeja", "authors": "Rishabh Dudeja and Milad Bakhshizadeh", "title": "Universality of Linearized Message Passing for Phase Retrieval with\n  Structured Sensing Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the phase retrieval problem one seeks to recover an unknown $n$\ndimensional signal vector $\\mathbf{x}$ from $m$ measurements of the form $y_i =\n|(\\mathbf{A} \\mathbf{x})_i|$ where $\\mathbf{A}$ denotes the sensing matrix. A\npopular class of algorithms for this problem are based on approximate message\npassing. For these algorithms, it is known that if the sensing matrix\n$\\mathbf{A}$ is generated by sub-sampling $n$ columns of a uniformly random\n(i.e. Haar distributed) orthogonal matrix, in the high dimensional asymptotic\nregime ($m,n \\rightarrow \\infty, n/m \\rightarrow \\kappa$), the dynamics of the\nalgorithm are given by a deterministic recursion known as the state evolution.\nFor the special class of linearized message passing algorithms, we show that\nthe state evolution is universal: it continues to hold even when $\\mathbf{A}$\nis generated by randomly sub-sampling columns of certain deterministic\northogonal matrices such as the Hadamard-Walsh matrix, provided the signal is\ndrawn from a Gaussian prior.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 15:12:24 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2020 23:45:41 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Dudeja", "Rishabh", ""], ["Bakhshizadeh", "Milad", ""]]}, {"id": "2008.10526", "submitter": "Saeed Ghadimi", "authors": "Krishnakumar Balasubramanian, Saeed Ghadimi, Anthony Nguyen", "title": "Stochastic Multi-level Composition Optimization Algorithms with\n  Level-Independent Convergence Rates", "comments": "Refined the convergence analysis in Section 3 under weaker\n  assumptions", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study smooth stochastic multi-level composition\noptimization problems, where the objective function is a nested composition of\n$T$ functions. We assume access to noisy evaluations of the functions and their\ngradients, through a stochastic first-order oracle. For solving this class of\nproblems, we propose two algorithms using moving-average stochastic estimates,\nand analyze their convergence to an $\\epsilon$-stationary point of the problem.\nWe show that the first algorithm, which is a generalization of\n\\cite{GhaRuswan20} to the $T$ level case, can achieve a sample complexity of\n$\\mathcal{O}(1/\\epsilon^6)$ by using mini-batches of samples in each iteration.\nBy modifying this algorithm using linearized stochastic estimates of the\nfunction values, we improve the sample complexity to\n$\\mathcal{O}(1/\\epsilon^4)$. {\\color{black}This modification not only removes\nthe requirement of having a mini-batch of samples in each iteration, but also\nmakes the algorithm parameter-free and easy to implement}. To the best of our\nknowledge, this is the first time that such an online algorithm designed for\nthe (un)constrained multi-level setting, obtains the same sample complexity of\nthe smooth single-level setting, under standard assumptions (unbiasedness and\nboundedness of the second moments) on the stochastic first-order oracle.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 15:57:50 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 15:59:02 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 04:36:11 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Ghadimi", "Saeed", ""], ["Nguyen", "Anthony", ""]]}, {"id": "2008.10552", "submitter": "Leonard Soicher", "authors": "R. A. Bailey and Leonard H. Soicher", "title": "Uniform semi-Latin squares and their pairwise-variance aberrations", "comments": "21 pages; accepted manuscript", "journal-ref": "Journal of Statistical Planning and Inference 213 (2021) 282-291", "doi": "10.1016/j.jspi.2020.12.003", "report-no": null, "categories": "math.ST math.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For integers $n>2$ and $k>0$, an $(n\\times n)/k$ semi-Latin square is an\n$n\\times n$ array of $k$-subsets (called blocks) of an $nk$-set (of\ntreatments), such that each treatment occurs once in each row and once in each\ncolumn of the array. A semi-Latin square is uniform if every pair of blocks,\nnot in the same row or column, intersect in the same positive number of\ntreatments. We show that when a uniform $(n\\times n)/k$ semi-Latin square\nexists, the Schur optimal $(n\\times n)/k$ semi-Latin squares are precisely the\nuniform ones. We then compare uniform semi-Latin squares using the criterion of\npairwise-variance (PV) aberration, introduced by J.P. Morgan for affine\nresolvable designs, and determine the uniform $(n\\times n)/k$ semi-Latin\nsquares with minimum PV aberration when there exist $n-1$ mutually orthogonal\nLatin squares (MOLS) of order $n$. These do not exist when $n=6$, and the\nsmallest uniform semi-Latin squares in this case have size $(6\\times 6)/10$. We\npresent a complete classification of the uniform $(6\\times 6)/10$ semi-Latin\nsquares, and display the one with least PV aberration. We give a construction\nproducing a uniform $((n+1)\\times (n+1))/((n-2)n)$ semi-Latin square when there\nexist $n-1$ MOLS of order $n$, and determine the PV aberration of such a\nuniform semi-Latin square. Finally, we describe how certain affine resolvable\ndesigns and balanced incomplete-block designs (BIBDs) can be constructed from\nuniform semi-Latin squares. From the uniform $(6\\times 6)/10$ semi-Latin\nsquares we classified, we obtain (up to block design isomorphism) exactly 16875\naffine resolvable designs for 72 treatments in 36 blocks of size 12 and 8615\nBIBDs for 36 treatments in 84 blocks of size 6. In particular, this shows that\nthere are at least 16875 pairwise non-isomorphic orthogonal arrays\n$\\mathrm{OA}(72,6,6,2)$.\n", "versions": [{"version": "v1", "created": "Wed, 19 Aug 2020 18:15:03 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 16:22:56 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Bailey", "R. A.", ""], ["Soicher", "Leonard H.", ""]]}, {"id": "2008.10675", "submitter": "Tong Liu", "authors": "Yu Hang Jiang, Tong Liu, Zhiya Lou, Jeffrey S. Rosenthal, Shanshan\n  Shangguan, Fei Wang, and Zixuan Wu", "title": "The Coupling/Minorization/Drift Approach to Markov Chain Convergence\n  Rates", "comments": "14 pages, 2 figures. For web appendix please see\n  www.probability.ca/NoticesApp. This is the updated version of previous paper:\n  Markov Chain Convergence Rates from Coupling Constructions", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This review paper provides an introduction of Markov chains and their\nconvergence rates which is an important and interesting mathematical topic\nwhich also has important applications for very widely used Markov chain Monte\nCarlo (MCMC) algorithm. We first discuss eigenvalue analysis for Markov chains\non finite state spaces. Then, using the coupling construction, we prove two\nquantitative bounds based on minorization condition and drift conditions, and\nprovide descriptive and intuitive examples to showcase how these theorems can\nbe implemented in practice. This paper is meant to provide a general overview\nof the subject and spark interest in new Markov chain research areas.\n", "versions": [{"version": "v1", "created": "Mon, 24 Aug 2020 19:44:09 GMT"}, {"version": "v2", "created": "Thu, 15 Oct 2020 17:49:25 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2020 18:31:22 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Jiang", "Yu Hang", ""], ["Liu", "Tong", ""], ["Lou", "Zhiya", ""], ["Rosenthal", "Jeffrey S.", ""], ["Shangguan", "Shanshan", ""], ["Wang", "Fei", ""], ["Wu", "Zixuan", ""]]}, {"id": "2008.10930", "submitter": "Valentin Courgeau", "authors": "Valentin Courgeau, Almut E.D. Veraart", "title": "High-frequency Estimation of the L\\'evy-driven Graph Ornstein-Uhlenbeck\n  process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the Graph Ornstein-Uhlenbeck (GrOU) process observed on a\nnon-uniform discrete time grid and introduce discretised maximum likelihood\nestimators with parameters specific to the whole graph or specific to each\ncomponent, or node. Under a high-frequency sampling scheme, we study the\nasymptotic behaviour of those estimators as the mesh size of the observation\ngrid goes to zero. We prove two stable central limit theorems to the same\ndistribution as in the continuously-observed case under both finite and\ninfinite jump activity for the L\\'evy driving noise. When a graph structure is\nnot explicitly available, the stable convergence allows to consider\npurpose-specific sparse inference procedures, i.e. pruning, on the edges\nthemselves in parallel to the GrOU inference and preserve its asymptotic\nproperties. We apply the new estimators to wind capacity factor measurements,\ni.e. the ratio between the wind power produced locally compared to its rated\npeak power, across fifty locations in Northern Spain and Portugal. We show the\nsuperiority of those estimators compared to the standard least squares\nestimator through a simulation study extending known univariate results across\ngraph configurations, noise types and amplitudes.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 10:26:40 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Courgeau", "Valentin", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "2008.10979", "submitter": "Oleg Lepski", "authors": "Alexander Goldenshluger and Oleg Lepski", "title": "Minimax estimation of norms of a probability density: I. Lower bounds", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with the problem of nonparametric estimating the $L_p$--norm,\n$p\\in (1,\\infty)$, of a probability density on $R^d$, $d\\geq 1$ from\nindependent observations. The unknown density %to be estimated is assumed to\nbelong to a ball in the anisotropic Nikolskii's space. We adopt the minimax\napproach, and derive lower bounds on the minimax risk. In particular, we\ndemonstrate that accuracy of estimation procedures essentially depends on\nwhether $p$ is integer or not. Moreover, we develop a general technique for\nderivation of lower bounds on the minimax risk in the problems of estimating\nnonlinear functionals. The proposed technique is applicable for a broad class\nof nonlinear functionals, and it is used for derivation of the lower bounds in\nthe~$L_p$--norm estimation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 13:14:12 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Goldenshluger", "Alexander", ""], ["Lepski", "Oleg", ""]]}, {"id": "2008.10987", "submitter": "Oleg Lepski", "authors": "Alexander Goldenshluger and Oleg Lepski", "title": "Minimax estimation of norms of a probability density: II. Rate-optimal\n  estimation procedures", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop rate--optimal estimation procedures in the problem\nof estimating the $L_p$--norm, $p\\in (0, \\infty)$ of a probability density from\nindependent observations. The density is assumed to be defined on $R^d$, $d\\geq\n1$ and to belong to a ball in the anisotropic Nikolskii space. We adopt the\nminimax approach and construct rate--optimal estimators in the case of integer\n$p\\geq 2$. We demonstrate that, depending on parameters of Nikolskii's class\nand the norm index $p$, the risk asymptotics ranges from inconsistency to\n$\\sqrt{n}$--estimation. The results in this paper complement the minimax lower\nbounds derived in the companion paper \\cite{gl20}.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 13:27:56 GMT"}], "update_date": "2020-08-26", "authors_parsed": [["Goldenshluger", "Alexander", ""], ["Lepski", "Oleg", ""]]}, {"id": "2008.11095", "submitter": "George Wynne", "authors": "George Wynne, Andrew B. Duncan", "title": "A Kernel Two-Sample Test for Functional Data", "comments": "Added to numerics section", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric two-sample test procedure based on Maximum Mean\nDiscrepancy (MMD) for testing the hypothesis that two samples of functions have\nthe same underlying distribution, using kernels defined on function spaces.\nThis construction is motivated by a scaling analysis of the efficiency of\nMMD-based tests for datasets of increasing dimension. Theoretical properties of\nkernels on function spaces and their associated MMD are established and\nemployed to ascertain the efficacy of the newly proposed test, as well as to\nassess the effects of using functional reconstructions based on discretised\nfunction samples. The theoretical results are demonstrated over a range of\nsynthetic and real world datasets.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 15:19:02 GMT"}, {"version": "v2", "created": "Mon, 19 Oct 2020 12:02:06 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wynne", "George", ""], ["Duncan", "Andrew B.", ""]]}, {"id": "2008.11140", "submitter": "Sokbae Lee", "authors": "Xiaohong Chen, Sokbae Lee, Myung Hwan Seo", "title": "Powerful Inference", "comments": "29 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an inference method for a (sub)vector of parameters identified by\nconditional moment restrictions, which are implied by economic models such as\nrational behavior and Euler equations. Building on Bierens (1990), we propose\npenalized maximum statistics and combine bootstrap inference with model\nselection. Our method is optimized to be powerful against a set of local\nalternatives of interest by solving a data-dependent max-min problem for tuning\nparameter selection. We demonstrate the efficacy of our method by a proof of\nconcept using two empirical examples: rational unbiased reporting of ability\nstatus and the elasticity of intertemporal substitution.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 16:11:37 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2021 15:24:07 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Chen", "Xiaohong", ""], ["Lee", "Sokbae", ""], ["Seo", "Myung Hwan", ""]]}, {"id": "2008.11178", "submitter": "Vishal Katariya", "authors": "Vishal Katariya and Mark M. Wilde", "title": "RLD Fisher Information Bound for Multiparameter Estimation of Quantum\n  Channels", "comments": "v3: 31 pages, 3 figures", "journal-ref": "New Journal of Physics, vol. 23, page 073040, July 2021", "doi": "10.1088/1367-2630/ac1186", "report-no": null, "categories": "quant-ph math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the fundamental tasks in quantum metrology is to estimate multiple\nparameters embedded in a noisy process, i.e., a quantum channel. In this paper,\nwe study fundamental limits to quantum channel estimation via the concept of\namortization and the right logarithmic derivative (RLD) Fisher information\nvalue. Our key technical result is the proof of a chain-rule inequality for the\nRLD Fisher information value, which implies that amortization, i.e., access to\na catalyst state family, does not increase the RLD Fisher information value of\nquantum channels. This technical result leads to a fundamental and efficiently\ncomputable limitation for multiparameter channel estimation in the sequential\nsetting, in terms of the RLD Fisher information value. As a consequence, we\nconclude that if the RLD Fisher information value is finite, then Heisenberg\nscaling is unattainable in the multiparameter setting.\n", "versions": [{"version": "v1", "created": "Tue, 25 Aug 2020 17:20:41 GMT"}, {"version": "v2", "created": "Tue, 1 Sep 2020 17:58:39 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 02:15:55 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Katariya", "Vishal", ""], ["Wilde", "Mark M.", ""]]}, {"id": "2008.11619", "submitter": "Fang Han", "authors": "Hongjian Shi, Mathias Drton, and Fang Han", "title": "On the power of Chatterjee rank correlation", "comments": "to appear in Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chatterjee (2021) introduced a simple new rank correlation coefficient that\nhas attracted much recent attention. The coefficient has the unusual appeal\nthat it not only estimates a population quantity first proposed by Dette et al.\n(2013) that is zero if and only if the underlying pair of random variables is\nindependent, but also is asymptotically normal under independence. This paper\ncompares Chatterjee's new correlation coefficient to three established rank\ncorrelations that also facilitate consistent tests of independence, namely,\nHoeffding's $D$, Blum-Kiefer-Rosenblatt's $R$, and Bergsma-Dassios-Yanagimoto's\n$\\tau^*$. We contrast their computational efficiency in light of recent\nadvances, and investigate their power against local rotation and mixture\nalternatives. Our main results show that Chatterjee's coefficient is\nunfortunately rate sub-optimal compared to $D$, $R$, and $\\tau^*$. The\nsituation is more subtle for a related earlier estimator of Dette et al.\n(2013). These results favor $D$, $R$, and $\\tau^*$ over Chatterjee's new\ncorrelation coefficient for the purpose of testing independence.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 15:22:59 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 03:17:39 GMT"}, {"version": "v3", "created": "Sat, 6 Feb 2021 17:19:28 GMT"}, {"version": "v4", "created": "Sun, 25 Apr 2021 17:42:15 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Shi", "Hongjian", ""], ["Drton", "Mathias", ""], ["Han", "Fang", ""]]}, {"id": "2008.11765", "submitter": "Masoud Faridi", "authors": "Masoud Faridi and Majid Jafari Khaledi", "title": "The polar-generalized normal distribution", "comments": "25 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an extension to the normal distribution through the\npolar method to capture bimodality and asymmetry, which are often observed\ncharacteristics of empirical data. The later two features are entirely\ncontrolled by a separate scalar parameter. Explicit expressions for the\ncumulative distribution function, the density function and the moments were\nderived. The stochastic representation of the distribution facilitates\nimplementing Bayesian estimation via the Markov chain Monte Carlo methods. Some\nreal-life data as well as simulated data are analyzed to illustrate the\nflexibility of the distribution for modeling asymmetric bimodality.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 18:56:07 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Faridi", "Masoud", ""], ["Khaledi", "Majid Jafari", ""]]}, {"id": "2008.11809", "submitter": "Ruiyi Yang", "authors": "Daniel Sanz-Alonso and Ruiyi Yang", "title": "Unlabeled Data Help in Graph-Based Semi-Supervised Learning: A Bayesian\n  Nonparametrics Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we analyze the graph-based approach to semi-supervised learning\nunder a manifold assumption. We adopt a Bayesian perspective and demonstrate\nthat, for a suitable choice of prior constructed with sufficiently many\nunlabeled data, the posterior contracts around the truth at a rate that is\nminimax optimal up to a logarithmic factor. Our theory covers both regression\nand classification.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 20:51:30 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 22:51:53 GMT"}, {"version": "v3", "created": "Sun, 13 Jun 2021 03:01:31 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Sanz-Alonso", "Daniel", ""], ["Yang", "Ruiyi", ""]]}, {"id": "2008.11840", "submitter": "Pierre C. Bellec", "authors": "Pierre C Bellec", "title": "Out-of-sample error estimate for robust M-estimators with convex penalty", "comments": "This version adds simulations for the nuclear norm penalty", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A generic out-of-sample error estimate is proposed for robust $M$-estimators\nregularized with a convex penalty in high-dimensional linear regression where\n$(X,y)$ is observed and $p,n$ are of the same order. If $\\psi$ is the\nderivative of the robust data-fitting loss $\\rho$, the estimate depends on the\nobserved data only through the quantities $\\hat\\psi = \\psi(y-X\\hat\\beta)$,\n$X^\\top \\hat\\psi$ and the derivatives $(\\partial/\\partial y) \\hat\\psi$ and\n$(\\partial/\\partial y) X\\hat\\beta$ for fixed $X$.\n  The out-of-sample error estimate enjoys a relative error of order $n^{-1/2}$\nin a linear model with Gaussian covariates and independent noise, either\nnon-asymptotically when $p/n\\le \\gamma$ or asymptotically in the\nhigh-dimensional asymptotic regime $p/n\\to\\gamma'\\in(0,\\infty)$. General\ndifferentiable loss functions $\\rho$ are allowed provided that $\\psi=\\rho'$ is\n1-Lipschitz. The validity of the out-of-sample error estimate holds either\nunder a strong convexity assumption, or for the $\\ell_1$-penalized Huber\nM-estimator if the number of corrupted observations and sparsity of the true\n$\\beta$ are bounded from above by $s_*n$ for some small enough constant\n$s_*\\in(0,1)$ independent of $n,p$.\n  For the square loss and in the absence of corruption in the response, the\nresults additionally yield $n^{-1/2}$-consistent estimates of the noise\nvariance and of the generalization error. This generalizes, to arbitrary convex\npenalty, estimates that were previously known for the Lasso.\n", "versions": [{"version": "v1", "created": "Wed, 26 Aug 2020 21:50:41 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 23:23:24 GMT"}, {"version": "v3", "created": "Mon, 12 Oct 2020 14:48:21 GMT"}, {"version": "v4", "created": "Thu, 3 Jun 2021 20:17:13 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bellec", "Pierre C", ""]]}, {"id": "2008.11892", "submitter": "Zhou Fan", "authors": "Zhou Fan", "title": "Approximate Message Passing algorithms for rotationally invariant\n  matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.IT math.IT math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Message Passing (AMP) algorithms have seen widespread use across\na variety of applications. However, the precise forms for their Onsager\ncorrections and state evolutions depend on properties of the underlying random\nmatrix ensemble, limiting the extent to which AMP algorithms derived for white\nnoise may be applicable to data matrices that arise in practice.\n  In this work, we study more general AMP algorithms for random matrices $W$\nthat satisfy orthogonal rotational invariance in law, where $W$ may have a\nspectral distribution that is different from the semicircle and Marcenko-Pastur\nlaws characteristic of white noise. The Onsager corrections and state\nevolutions in these algorithms are defined by the free cumulants or rectangular\nfree cumulants of the spectral distribution of $W$. Their forms were derived\npreviously by Opper, \\c{C}akmak, and Winther using non-rigorous dynamic\nfunctional theory techniques, and we provide rigorous proofs.\n  Our motivating application is a Bayes-AMP algorithm for Principal Components\nAnalysis, when there is prior structure for the principal components (PCs) and\npossibly non-white noise. For sufficiently large signal strengths and any\nnon-Gaussian prior distributions for the PCs, we show that this algorithm\nprovably achieves higher estimation accuracy than the sample PCs.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 02:35:16 GMT"}, {"version": "v2", "created": "Wed, 9 Sep 2020 19:40:49 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 23:18:11 GMT"}, {"version": "v4", "created": "Fri, 30 Apr 2021 14:49:14 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Fan", "Zhou", ""]]}, {"id": "2008.11903", "submitter": "Zhigang Bao", "authors": "Zhigang Bao, Xiucai Ding, Jingming Wang, Ke Wang", "title": "Statistical inference for principal components of spiked covariance\n  matrices", "comments": "This work largely extends our previous work arXiv:1907.12251, and the\n  latter shall be regarded as a very special case of the current work. In\n  addition, various statistical applications are discussed", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the asymptotic behavior of the extreme eigenvalues\nand eigenvectors of the high dimensional spiked sample covariance matrices, in\nthe supercritical case when a reliable detection of spikes is possible.\nEspecially, we derive the joint distribution of the extreme eigenvalues and the\ngeneralized components of the associated eigenvectors, i.e., the projections of\nthe eigenvectors onto arbitrary given direction, assuming that the dimension\nand sample size are comparably large. In general, the joint distribution is\ngiven in terms of linear combinations of finitely many Gaussian and Chi-square\nvariables, with parameters depending on the projection direction and the\nspikes. Our assumption on the spikes is fully general. First, the strengths of\nspikes are only required to be slightly above the critical threshold and no\nupper bound on the strengths is needed. Second, multiple spikes, i.e., spikes\nwith the same strength, are allowed. Third, no structural assumption is imposed\non the spikes. Thanks to the general setting, we can then apply the results to\nvarious high dimensional statistical hypothesis testing problems involving both\nthe eigenvalues and eigenvectors. Specifically, we propose accurate and\npowerful statistics to conduct hypothesis testing on the principal components.\nThese statistics are data-dependent and adaptive to the underlying true spikes.\nNumerical simulations also confirm the accuracy and powerfulness of our\nproposed statistics and illustrate significantly better performance compared to\nthe existing methods in the literature. Especially, our methods are accurate\nand powerful even when either the spikes are small or the dimension is large.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 03:36:12 GMT"}, {"version": "v2", "created": "Thu, 3 Sep 2020 07:36:54 GMT"}], "update_date": "2020-09-04", "authors_parsed": [["Bao", "Zhigang", ""], ["Ding", "Xiucai", ""], ["Wang", "Jingming", ""], ["Wang", "Ke", ""]]}, {"id": "2008.11957", "submitter": "Giacomo Francisci", "authors": "Giacomo Francisci, Claudio Agostinelli, Alicia Nieto-Reyes, and Anand\n  N. Vidyashankar", "title": "Analytical and statistical properties of local depth functions motivated\n  by clustering applications", "comments": "35+56 pages, 3+9 figures, 3+8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local depth functions (LDFs) are used for describing the local geometric\nfeatures of multivariate distributions, especially in multimodal models. In\nthis paper, we undertake a rigorous systematic study of the LDFs and use it to\ndevelop a theoretically validated algorithm for clustering. For this reason, we\nestablish several analytical and statistical properties of LDFs. First, we show\nthat, when the underlying probability distribution is absolutely continuous,\nunder an appropriate scaling that converge to zero (referred to as extreme\nlocalization), LDFs converge uniformly to a power of the density and obtain a\nrelated rate of convergence result. Second, we establish that the centered and\nscaled sample LDFs converge in distribution to a centered Gaussian process,\nuniformly in the space of bounded functions on R p x [0,infinity], as the\nsample size diverges to infinity. Third, under an extreme localization that\ndepends on the sample size, we determine the correct centering and scaling for\nthe sample LDFs to possess a limiting normal distribution. Fourth, invoking the\nabove results, we develop a new clustering algorithm that uses the LDFs and\ntheir differentiability properties. Fifth, for the last purpose, we establish\nseveral results concerning the gradient systems related to LDFs. Finally, we\nillustrate the finite sample performance of our results using simulations and\napply them to two datasets.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 07:18:36 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Francisci", "Giacomo", ""], ["Agostinelli", "Claudio", ""], ["Nieto-Reyes", "Alicia", ""], ["Vidyashankar", "Anand N.", ""]]}, {"id": "2008.11964", "submitter": "Yanjun Han", "authors": "Yanjun Han", "title": "On the High Accuracy Limitation of Adaptive Property Estimation", "comments": null, "journal-ref": "Published in AISTATS 2021", "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the success of adaptive (or unified) approaches\nin estimating symmetric properties of discrete distributions, where one first\nobtains a distribution estimator independent of the target property, and then\nplugs the estimator into the target property as the final estimator. Several\nsuch approaches have been proposed and proved to be adaptively optimal, i.e.\nthey achieve the optimal sample complexity for a large class of properties\nwithin a low accuracy, especially for a large estimation error $\\varepsilon\\gg\nn^{-1/3}$ where $n$ is the sample size.\n  In this paper, we characterize the high accuracy limitation, or the penalty\nfor adaptation, for all such approaches. Specifically, we show that under a\nmild assumption that the distribution estimator is close to the true sorted\ndistribution in expectation, any adaptive approach cannot achieve the optimal\nsample complexity for every $1$-Lipschitz property within accuracy $\\varepsilon\n\\ll n^{-1/3}$. In particular, this result disproves a conjecture in [Acharya et\nal. 2017] that the profile maximum likelihood (PML) plug-in approach is optimal\nin property estimation for all ranges of $\\varepsilon$, and confirms a\nconjecture in [Han and Shiragur, 2021] that their competitive analysis of the\nPML is tight.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 07:41:03 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 06:40:05 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Han", "Yanjun", ""]]}, {"id": "2008.12026", "submitter": "Florian Pausinger", "authors": "Markus Kiderlen, Florian Pausinger", "title": "Discrepancy of stratified samples from partitions of the unit cube", "comments": "29 pages, 12 figures, revised manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.NT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the notion of jittered sampling to arbitrary partitions and study\nthe discrepancy of the related point sets. Let\n$\\mathbf{\\Omega}=(\\Omega_1,\\ldots,\\Omega_N)$ be a partition of $[0,1]^d$ and\nlet the $i$th point in $\\mathcal{P}$ be chosen uniformly in the $i$th set of\nthe partition (and stochastically independent of the other points),\n$i=1,\\ldots,N$. For the study of such sets we introduce the concept of a\nuniformly distributed triangular array and compare this notion to related\nnotions in the literature. We prove that the expected\n${\\mathcal{L}_p}$-discrepancy, $\\mathbb{E}\n{\\mathcal{L}_p}(\\mathcal{P}_{\\mathbf{\\Omega}})^p$, of a point set\n$\\mathcal{P}_\\mathbf{\\Omega}$ generated from any equivolume partition\n$\\mathbf{\\Omega}$ is always strictly smaller than the expected\n${\\mathcal{L}_p}$-discrepancy of a set of $N$ uniform random samples for $p>1$.\nFor fixed $N$ we consider classes of stratified samples based on equivolume\npartitions of the unit cube into convex sets or into sets with a uniform\npositive lower bound on their reach. It is shown that these classes contain at\nleast one minimizer of the expected ${\\mathcal{L}_p}$-discrepancy. We\nillustrate our results with explicit constructions for small $N$. In addition,\nwe present a family of partitions that seems to improve the expected\ndiscrepancy of Monte Carlo sampling by a factor of 2 for every $N$.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 10:04:19 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2021 10:22:14 GMT"}], "update_date": "2021-02-01", "authors_parsed": [["Kiderlen", "Markus", ""], ["Pausinger", "Florian", ""]]}, {"id": "2008.12070", "submitter": "Tim Sullivan", "authors": "Ilja Klebanov and Bj\\\"orn Sprungk and T. J. Sullivan", "title": "The linear conditional expectation in Hilbert space", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The linear conditional expectation (LCE) provides a best linear (or rather,\naffine) estimate of the conditional expectation and hence plays an important\nr\\^ole in approximate Bayesian inference, especially the Bayes linear approach.\nThis article establishes the analytical properties of the LCE in an\ninfinite-dimensional Hilbert space context. In addition, working in the space\nof affine Hilbert--Schmidt operators, we establish a regularisation procedure\nfor this LCE. As an important application, we obtain a simple alternative\nderivation and intuitive justification of the conditional mean embedding\nformula, a concept widely used in machine learning to perform the conditioning\nof random variables by embedding them into reproducing kernel Hilbert spaces.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 11:56:51 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2020 14:35:32 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Klebanov", "Ilja", ""], ["Sprungk", "Bj\u00f6rn", ""], ["Sullivan", "T. J.", ""]]}, {"id": "2008.12195", "submitter": "Martin Hanik", "authors": "Martin Hanik, Hans-Christian Hege, Christoph von Tycowicz", "title": "Bi-invariant Two-Sample Tests in Lie Groups for Shape Analysis", "comments": "To be published in: Shape in Medical Imaging (ShapeMI at MICCAI 2020)", "journal-ref": "Shape in Medical Imaging (2020) 44--54", "doi": "10.1007/978-3-030-61056-2_4", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose generalizations of the Hotelling's $T^2$ statistic and the\nBhattacharayya distance for data taking values in Lie groups. A key feature of\nthe derived measures is that they are compatible with the group structure even\nfor manifolds that do not admit any bi-invariant metric. This property, e.g.,\nassures analysis that does not depend on the reference shape, thus, preventing\nbias due to arbitrary choices thereof. Furthermore, the generalizations agree\nwith the common definitions for the special case of flat vector spaces\nguaranteeing consistency. Employing a permutation test setup, we further obtain\nnonparametric, two-sample testing procedures that themselves are bi-invariant\nand consistent. We validate our method in group tests revealing significant\ndifferences in hippocampal shape between individuals with mild cognitive\nimpairment and normal controls.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 15:44:52 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Hanik", "Martin", ""], ["Hege", "Hans-Christian", ""], ["von Tycowicz", "Christoph", ""]]}, {"id": "2008.12236", "submitter": "Mohamed Ndaoud", "authors": "Mohamed Ndaoud", "title": "Scaled minimax optimality in high-dimensional linear regression: A\n  non-convex algorithmic regularization approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of fast convergence in the classical problem of high dimensional\nlinear regression has been extensively studied. Arguably, one of the fastest\nprocedures in practice is Iterative Hard Thresholding (IHT). Still, IHT relies\nstrongly on the knowledge of the true sparsity parameter $s$. In this paper, we\npresent a novel fast procedure for estimation in the high dimensional linear\nregression. Taking advantage of the interplay between estimation, support\nrecovery and optimization we achieve both optimal statistical accuracy and fast\nconvergence. The main advantage of our procedure is that it is fully adaptive,\nmaking it more practical than state of the art IHT methods. Our procedure\nachieves optimal statistical accuracy faster than, for instance, classical\nalgorithms for the Lasso. Moreover, we establish sharp optimal results for both\nestimation and support recovery. As a consequence, we present a new iterative\nhard thresholding algorithm for high dimensional linear regression that is\nscaled minimax optimal (achieves the estimation error of the oracle that knows\nthe sparsity pattern if possible), fast and adaptive.\n", "versions": [{"version": "v1", "created": "Thu, 27 Aug 2020 16:34:46 GMT"}], "update_date": "2020-08-28", "authors_parsed": [["Ndaoud", "Mohamed", ""]]}, {"id": "2008.12434", "submitter": "Anru R. Zhang", "authors": "T. Tony Cai and Rungang Han and Anru R. Zhang", "title": "On the Non-Asymptotic Concentration of Heteroskedastic Wishart-type\n  Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper focuses on the non-asymptotic concentration of the heteroskedastic\nWishart-type matrices. Suppose $Z$ is a $p_1$-by-$p_2$ random matrix and\n$Z_{ij} \\sim N(0,\\sigma_{ij}^2)$ independently, we prove that\n  \\begin{equation*}\n  \\bbE \\left\\|ZZ^\\top - \\bbE ZZ^\\top\\right\\| \\leq\n(1+\\epsilon)\\left\\{2\\sigma_C\\sigma_R + \\sigma_C^2 +\nC\\sigma_R\\sigma_*\\sqrt{\\log(p_1 \\wedge p_2)} + C\\sigma_*^2\\log(p_1 \\wedge\np_2)\\right\\},\n  \\end{equation*}\n  where $\\sigma_C^2 := \\max_j \\sum_{i=1}^{p_1}\\sigma_{ij}^2$, $\\sigma_R^2 :=\n\\max_i \\sum_{j=1}^{p_2}\\sigma_{ij}^2$ and $\\sigma_*^2 :=\n\\max_{i,j}\\sigma_{ij}^2$. A minimax lower bound is developed that matches this\nupper bound. Then, we derive the concentration inequalities, moments, and tail\nbounds for the heteroskedastic Wishart-type matrix under more general\ndistributions, such as sub-Gaussian and heavy-tailed distributions. Next, we\nconsider the cases where $Z$ has homoskedastic columns or rows (i.e.,\n$\\sigma_{ij} \\approx \\sigma_i$ or $\\sigma_{ij} \\approx \\sigma_j$) and derive\nthe rate-optimal Wishart-type concentration bounds. Finally, we apply the\ndeveloped tools to identify the sharp signal-to-noise ratio threshold for\nconsistent clustering in the heteroskedastic clustering problem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 01:50:41 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Cai", "T. Tony", ""], ["Han", "Rungang", ""], ["Zhang", "Anru R.", ""]]}, {"id": "2008.12443", "submitter": "Li Tian", "authors": "Yong Chen, Li Tian and Ying Li", "title": "Second Moment Estimator for An AR(1) Model Driven by A Long Memory\n  Gaussian Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider an inference problem for the first order\nautoregressive process driven by a long memory stationary Gaussian process.\nSuppose that the covariance function of the noise can be expressed as\n$\\abs{k}^{2H-2}$ times a function slowly varying at infinity. The fractional\nGaussian noise and the fractional ARIMA model and some others Gaussian noise\nare special examples that satisfy this assumption. We propose a second moment\nestimator and prove the strong consistency and give the asymptotic\ndistribution. Moreover, when the limit distribution is Gaussian, we give the\nupper Berry-Ess\\'een bound by means of Fourth moment theorem.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 02:21:34 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 12:23:41 GMT"}, {"version": "v3", "created": "Mon, 21 Sep 2020 08:37:32 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2020 02:58:57 GMT"}, {"version": "v5", "created": "Wed, 2 Dec 2020 02:47:54 GMT"}], "update_date": "2020-12-03", "authors_parsed": [["Chen", "Yong", ""], ["Tian", "Li", ""], ["Li", "Ying", ""]]}, {"id": "2008.12466", "submitter": "Farhad Farokhi", "authors": "Farhad Farokhi", "title": "Deconvoluting Kernel Density Estimation and Regression for Locally\n  Differentially Private Data", "comments": "updated reference list, deeper numerical analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local differential privacy has become the gold-standard of privacy literature\nfor gathering or releasing sensitive individual data points in a\nprivacy-preserving manner. However, locally differential data can twist the\nprobability density of the data because of the additive noise used to ensure\nprivacy. In fact, the density of privacy-preserving data (no matter how many\nsamples we gather) is always flatter in comparison with the density function of\nthe original data points due to convolution with privacy-preserving noise\ndensity function. The effect is especially more pronounced when using\nslow-decaying privacy-preserving noises, such as the Laplace noise. This can\nresult in under/over-estimation of the heavy-hitters. This is an important\nchallenge facing social scientists due to the use of differential privacy in\nthe 2020 Census in the United States. In this paper, we develop density\nestimation methods using smoothing kernels. We use the framework of\ndeconvoluting kernel density estimators to remove the effect of\nprivacy-preserving noise. This approach also allows us to adapt the results\nfrom non-parameteric regression with errors-in-variables to develop regression\nmodels based on locally differentially private data. We demonstrate the\nperformance of the developed methods on financial and demographic datasets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 03:39:17 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 03:32:15 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Farokhi", "Farhad", ""]]}, {"id": "2008.12485", "submitter": "Phil Howlett", "authors": "Phil Howlett, Anatoli Totokhti", "title": "An optimal linear filter for estimation of random functions in Hilbert\n  space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let ${\\mbox{$\\mbox{\\boldmath $f$}$}}$ be a square-integrable, zero-mean,\nrandom vector with observable realizations in a Hilbert space $H$, and let\n${\\mbox{$\\mbox{\\boldmath $g$}$}}$ be an associated square-integrable,\nzero-mean, random vector with realizations, which are not observable, in a\nHilbert space $K$. We seek an optimal filter in the form of a closed linear\noperator $X$ acting on the observable realizations of a proximate vector\n${\\mbox{$\\mbox{\\boldmath $f$}$}}_{\\epsilon} \\approx {\\mbox{$\\mbox{\\boldmath\n$f$}$}}$ that provides the best estimate $\\widehat{{\\mbox{$\\mbox{\\boldmath\n$g$}$}}}_{\\epsilon} = X {\\mbox{$\\mbox{\\boldmath $f$}$}}_{\\epsilon}$ of the\nvector ${\\mbox{$\\mbox{\\boldmath $f$}$}}$. We assume the required covariance\noperators are known. The results are illustrated with a typical example.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 05:15:48 GMT"}], "update_date": "2020-08-31", "authors_parsed": [["Howlett", "Phil", ""], ["Totokhti", "Anatoli", ""]]}, {"id": "2008.12653", "submitter": "Sara Mazzonetto", "authors": "Sara Mazzonetto and Paolo Pigato", "title": "Drift estimation of the threshold Ornstein-Uhlenbeck process from\n  continuous and discrete observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We refer by threshold Ornstein-Uhlenbeck to a continuous-time threshold\nautoregressive process. It follows the Ornstein-Uhlenbeck dynamics when above\nor below a fixed level, yet at this level (threshold) its coefficients can be\ndiscontinuous. We discuss (quasi)-maximum likelihood estimation of the drift\nparameters, both assuming continuous and discrete time observations. In the\nergodic case, we derive consistency and speed of convergence of these\nestimators in long time and high frequency. Based on these results, we develop\na test for the presence of a threshold in the dynamics. Finally, we apply these\nstatistical tools to short-term US interest rates modeling.\n", "versions": [{"version": "v1", "created": "Fri, 28 Aug 2020 13:36:17 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 16:48:15 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Mazzonetto", "Sara", ""], ["Pigato", "Paolo", ""]]}, {"id": "2008.12882", "submitter": "Bhaswar Bhattacharya", "authors": "Somabha Mukherjee, Jaesung Son, and Bhaswar B. Bhattacharya", "title": "Estimation in Tensor Ising Models", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $p$-tensor Ising model is a one-parameter discrete exponential family for\nmodeling dependent binary data, where the sufficient statistic is a\nmulti-linear form of degree $p \\geq 2$. This is a natural generalization of the\nmatrix Ising model, that provides a convenient mathematical framework for\ncapturing higher-order dependencies in complex relational data. In this paper,\nwe consider the problem of estimating the natural parameter of the $p$-tensor\nIsing model given a single sample from the distribution on $N$ nodes. Our\nestimate is based on the maximum pseudo-likelihood (MPL) method, which provides\na computationally efficient algorithm for estimating the parameter that avoids\ncomputing the intractable partition function. We derive general conditions\nunder which the MPL estimate is $\\sqrt N$-consistent, that is, it converges to\nthe true parameter at rate $1/\\sqrt N$. In particular, we show the $\\sqrt\nN$-consistency of the MPL estimate in the $p$-spin Sherrington-Kirkpatrick (SK)\nmodel, spin systems on general $p$-uniform hypergraphs, and Ising models on the\nhypergraph stochastic block model (HSBM). In fact, for the HSBM we pin down the\nexact location of the phase transition threshold, which is determined by the\npositivity of a certain mean-field variational problem, such that above this\nthreshold the MPL estimate is $\\sqrt N$-consistent, while below the threshold\nno estimator is consistent. Finally, we derive the precise fluctuations of the\nMPL estimate in the special case of the $p$-tensor Curie-Weiss model. An\ninteresting consequence of our results is that the MPL estimate in the\nCurie-Weiss model saturates the Cramer-Rao lower bound at all points above the\nestimation threshold, that is, the MPL estimate incurs no loss in asymptotic\nefficiency, even though it is obtained by minimizing only an approximation of\nthe true likelihood function for computational tractability.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 00:06:58 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Mukherjee", "Somabha", ""], ["Son", "Jaesung", ""], ["Bhattacharya", "Bhaswar B.", ""]]}, {"id": "2008.12885", "submitter": "Xinghao Qiao", "authors": "Jinyuan Chang and Cheng Chen and Xinghao Qiao", "title": "An autocovariance-based learning framework for high-dimensional\n  functional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and economic applications involve the analysis of\nhigh-dimensional functional time series, which stands at the intersection\nbetween functional time series and high-dimensional statistics gathering\nchallenges of infinite-dimensionality with serial dependence and\nnon-asymptotics. In this paper, we model observed functional time series, which\nare subject to errors in the sense that each functional datum arises as the sum\nof two uncorrelated components, one dynamic and one white noise. Motivated from\na simple fact that the autocovariance function of observed functional time\nseries automatically filters out the noise term, we propose an\nautocovariance-based three-step procedure by first performing\nautocovariance-based dimension reduction and then formulating a novel\nautocovariance-based block regularized minimum distance (RMD) estimation\nframework to produce block sparse estimates, from which we can finally recover\nfunctional sparse estimates. We investigate non-asymptotic properties of\nrelevant estimated terms under such autocovariance-based dimension reduction\nframework. To provide theoretical guarantees for the second step, we also\npresent convergence analysis of the block RMD estimator. Finally, we illustrate\nthe proposed autocovariance-based learning framework using applications of\nthree sparse high-dimensional functional time series models. With derived\ntheoretical results, we study convergence properties of the associated\nestimators. We demonstrate via simulated and real datasets that our proposed\nestimators significantly outperform the competitors.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 00:33:26 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Chang", "Jinyuan", ""], ["Chen", "Cheng", ""], ["Qiao", "Xinghao", ""]]}, {"id": "2008.13033", "submitter": "Ayed Alrashdi", "authors": "Ayed M. Alrashdi, Houssem Sifaou, Abla Kammoun, Mohamed-Slim Alouini\n  and Tareq Y. Al-Naffouri", "title": "Precise Error Analysis of the LASSO under Correlated Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of recovering a sparse signal from\nnoisy linear measurements using the so called LASSO formulation. We assume a\ncorrelated Gaussian design matrix with additive Gaussian noise. We precisely\nanalyze the high dimensional asymptotic performance of the LASSO under\ncorrelated design matrices using the Convex Gaussian Min-max Theorem (CGMT). We\ndefine appropriate performance measures such as the mean-square error (MSE),\nprobability of support recovery, element error rate (EER) and cosine\nsimilarity. Numerical simulations are presented to validate the derived\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 18:52:50 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2020 21:48:24 GMT"}], "update_date": "2020-09-18", "authors_parsed": [["Alrashdi", "Ayed M.", ""], ["Sifaou", "Houssem", ""], ["Kammoun", "Abla", ""], ["Alouini", "Mohamed-Slim", ""], ["Al-Naffouri", "Tareq Y.", ""]]}, {"id": "2008.13042", "submitter": "Marcelo Moreira", "authors": "Marcelo J. Moreira, Geert Ridder", "title": "Efficiency Loss of Asymptotically Efficient Tests in an Instrumental\n  Variables Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an instrumental variable model, the score statistic can be stochastically\nbounded for any alternative in parts of the parameter space. These regions\ninvolve a constraint on the first-stage regression coefficients and the\nreduced-form covariance matrix. As a consequence, the Lagrange Multiplier (LM)\ntest can have power close to size, despite being efficient under standard\nasymptotics. This loss of information limits the power of conditional tests\nwhich use only the Anderson-Rubin (AR) and the score statistic. In particular,\nthe conditional quasi-likelihood ratio (CQLR) test also suffers severe losses\nbecause its power can be bounded for any alternative.\n  A necessary condition for drastic power loss to occur is that the Hermitian\nof the reduced-form covariance matrix has eigenvalues of opposite signs. These\ncases are denoted impossibility designs or impossibility DGPs (ID). This\nrestriction cannot be satisfied with homoskedastic errors, but it can happen\nwith heteroskedastic, autocorrelated, and/or clustered (HAC) errors. We show\nthese situations can happen in practice, by applying our theory to the problem\nof inference on the intertemporal elasticity of substitution (IES) with weak\ninstruments. Out of eleven countries studied by Yogo (2004) and Andrews (2016),\nthe data in nine of them are consistent with impossibility designs at the 95%\nconfidence level. For these countries, the noncentrality parameter of the score\nstatistic can be very close to zero. Therefore, the power loss is sufficiently\nextensive to dissuade practitioners from blindly using LM-based tests with HAC\nerrors.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 19:37:11 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Moreira", "Marcelo J.", ""], ["Ridder", "Geert", ""]]}, {"id": "2008.13056", "submitter": "Zhi Wang", "authors": "Zhi Wang, Xueying Tang and Jingchen Liu", "title": "Statistical Analysis of Multi-Relational Network Recovery", "comments": "34 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop asymptotic theories for a class of latent variable\nmodels for large-scale multi-relational networks. In particular, we establish\nconsistency results and asymptotic error bounds for the (penalized) maximum\nlikelihood estimators when the size of the network tends to infinity. The basic\ntechnique is to develop a non-asymptotic error bound for the maximum likelihood\nestimators through large deviations analysis of random fields. We also show\nthat these estimators are nearly optimal in terms of minimax risk.\n", "versions": [{"version": "v1", "created": "Sat, 29 Aug 2020 21:20:44 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Wang", "Zhi", ""], ["Tang", "Xueying", ""], ["Liu", "Jingchen", ""]]}, {"id": "2008.13174", "submitter": "Kyoungjae Lee", "authors": "Kyoungjae Lee, Minwoo Chae and Lizhen Lin", "title": "Bayesian High-dimensional Semi-parametric Inference beyond sub-Gaussian\n  Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a sparse linear regression model with unknown symmetric error\nunder the high-dimensional setting. The true error distribution is assumed to\nbelong to the locally $\\beta$-H\\\"{o}lder class with an exponentially decreasing\ntail, which does not need to be sub-Gaussian. We obtain posterior convergence\nrates of the regression coefficient and the error density, which are nearly\noptimal and adaptive to the unknown sparsity level. Furthermore, we derive the\nsemi-parametric Bernstein-von Mises (BvM) theorem to characterize asymptotic\nshape of the marginal posterior for regression coefficients. Under the\nsub-Gaussianity assumption on the true score function, strong model selection\nconsistency for regression coefficients are also obtained, which eventually\nasserts the frequentist's validity of credible sets.\n", "versions": [{"version": "v1", "created": "Sun, 30 Aug 2020 13:55:01 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lee", "Kyoungjae", ""], ["Chae", "Minwoo", ""], ["Lin", "Lizhen", ""]]}, {"id": "2008.13372", "submitter": "Yury Polyanskiy", "authors": "Yury Polyanskiy and Yihong Wu", "title": "Note on approximating the Laplace transform of a Gaussian on a complex\n  disk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short note we study how well a Gaussian distribution can be\napproximated by distributions supported on $[-a,a]$. Perhaps, the natural\nconjecture is that for large $a$ the almost optimal choice is given by\ntruncating the Gaussian to $[-a,a]$. Indeed, such approximation achieves the\noptimal rate of $e^{-\\Theta(a^2)}$ in terms of the $L_\\infty$-distance between\ncharacteristic functions. However, if we consider the $L_\\infty$-distance\nbetween Laplace transforms on a complex disk, the optimal rate is\n$e^{-\\Theta(a^2 \\log a)}$, while truncation still only attains\n$e^{-\\Theta(a^2)}$. The optimal rate can be attained by the Gauss-Hermite\nquadrature. As corollary, we also construct a ``super-flat'' Gaussian mixture\nof $\\Theta(a^2)$ components with means in $[-a,a]$ and whose density has all\nderivatives bounded by $e^{-\\Omega(a^2 \\log(a))}$ in the $O(1)$-neighborhood of\nthe origin.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 05:19:33 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Polyanskiy", "Yury", ""], ["Wu", "Yihong", ""]]}, {"id": "2008.13657", "submitter": "Ken Duffy", "authors": "Giulio Prevedello and Ken R. Duffy", "title": "Discrete convolution statistic for hypothesis testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of testing for equality in distribution between two linear\nmodels, each consisting of sums of distinct discrete independent random\nvariables with unequal numbers of observations, has emerged from the biological\nresearch. In this case, the computation of classical $\\chi^2$ statistics, which\nwould not include all observations, results in loss of power, especially when\nsample sizes are small. Here, as an alternative that uses all data, the\nnonparametric maximum likelihood estimator for the distribution of sum of\ndiscrete and independent random variables, which we call the convolution\nstatistic, is proposed and its limiting normal covariance matrix determined. To\nchallenge null hypotheses about the distribution of this sum, the generalized\nWald's method is applied to define a testing statistic whose distribution is\nasymptotic to a $\\chi^2$ with as many degrees of freedom as the rank of such\ncovariance matrix. Rank analysis also reveals a connection with the roots of\nthe probability generating functions associated to the addend variables of the\nlinear models. A simulation study is performed to compare the convolution test\nwith Pearson's $\\chi^2$, and to provide usage guidelines.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 14:49:41 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Prevedello", "Giulio", ""], ["Duffy", "Ken R.", ""]]}, {"id": "2008.13735", "submitter": "Jingqiu Ding", "authors": "Jingqiu Ding, Samuel B.Hopkins, David Steurer", "title": "Estimating Rank-One Spikes from Heavy-Tailed Noise via Self-Avoiding\n  Walks", "comments": "38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study symmetric spiked matrix models with respect to a general class of\nnoise distributions. Given a rank-1 deformation of a random noise matrix, whose\nentries are independently distributed with zero mean and unit variance, the\ngoal is to estimate the rank-1 part. For the case of Gaussian noise, the top\neigenvector of the given matrix is a widely-studied estimator known to achieve\noptimal statistical guarantees, e.g., in the sense of the celebrated BBP phase\ntransition. However, this estimator can fail completely for heavy-tailed noise.\nIn this work, we exhibit an estimator that works for heavy-tailed noise up to\nthe BBP threshold that is optimal even for Gaussian noise. We give a\nnon-asymptotic analysis of our estimator which relies only on the variance of\neach entry remaining constant as the size of the matrix grows: higher moments\nmay grow arbitrarily fast or even fail to exist. Previously, it was only known\nhow to achieve these guarantees if higher-order moments of the noises are\nbounded by a constant independent of the size of the matrix. Our estimator can\nbe evaluated in polynomial time by counting self-avoiding walks via a color\n-coding technique. Moreover, we extend our estimator to spiked tensor models\nand establish analogous results.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 16:57:20 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ding", "Jingqiu", ""], ["Hopkins", "Samuel B.", ""], ["Steurer", "David", ""]]}, {"id": "2008.13777", "submitter": "Lijun Ding", "authors": "Lijun Ding, Yuqian Zhang, Yudong Chen", "title": "Low-rank matrix recovery with non-quadratic loss: projected gradient\n  method and regularity projection oracle", "comments": "Main text has 13 pages. Reading first seven pages (takes around 10-15\n  minutes) should give a good understanding of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing results for low-rank matrix recovery largely focus on quadratic\nloss, which enjoys favorable properties such as restricted strong\nconvexity/smoothness (RSC/RSM) and well conditioning over all low rank\nmatrices. However, many interesting problems involve non-quadratic loss do not\nsatisfy such properties; examples including one-bit matrix sensing, one-bit\nmatrix completion, and rank aggregation. For these problems, standard nonconvex\napproaches such as projected gradient with rank constraint alone (a.k.a.\niterative hard thresholding) and Burer-Monteiro approach may perform badly in\npractice and have no satisfactory theory in guaranteeing global and efficient\nconvergence.\n  In this paper, we show that the critical component in low-rank recovery with\nnon-quadratic loss is a regularity projection oracle, which restricts iterates\nto low-rank matrix within an appropriate bounded set, over which the loss\nfunction is well behaved and satisfies a set of relaxed RSC/RSM conditions.\nAccordingly, we analyze an (averaged) projected gradient method equipped with\nsuch an oracle, and prove that it converges globally and linearly. Our results\napply to a wide range of non-quadratic problems including rank aggregation, one\nbit matrix sensing/completion, and more broadly generalized linear models with\nrank constraint.\n", "versions": [{"version": "v1", "created": "Mon, 31 Aug 2020 17:56:04 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ding", "Lijun", ""], ["Zhang", "Yuqian", ""], ["Chen", "Yudong", ""]]}]