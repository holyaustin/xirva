[{"id": "1812.00001", "submitter": "Kazuto Fukuchi", "authors": "Kazuto Fukuchi and Jun Sakuma", "title": "Minimax Optimal Additive Functional Estimation with Discrete\n  Distribution", "comments": "This paper was presented in part at the 2017 IEEE International\n  Symposium on Information Theory (ISIT), Aachen, Germany and 2018 IEEE\n  International Symposium on Information Theory (ISIT), Vail, USA. arXiv admin\n  note: text overlap with arXiv:1801.05362", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses a problem of estimating an additive functional given $n$\ni.i.d. samples drawn from a discrete distribution $P=(p_1,...,p_k)$ with\nalphabet size $k$. The additive functional is defined as\n$\\theta(P;\\phi)=\\sum_{i=1}^k\\phi(p_i)$ for a function $\\phi$, which covers the\nmost of the entropy-like criteria. The minimax optimal risk of this problem has\nbeen already known for some specific $\\phi$, such as $\\phi(p)=p^\\alpha$ and\n$\\phi(p)=-p\\ln p$. However, there is no generic methodology to derive the\nminimax optimal risk for the additive function estimation problem. In this\npaper, we reveal the property of $\\phi$ that characterizes the minimax optimal\nrisk of the additive functional estimation problem; this analysis is applicable\nto general $\\phi$. More precisely, we reveal that the minimax optimal risk of\nthis problem is characterized by the divergence speed of the function $\\phi$.\n", "versions": [{"version": "v1", "created": "Wed, 28 Nov 2018 11:03:17 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Fukuchi", "Kazuto", ""], ["Sakuma", "Jun", ""]]}, {"id": "1812.00100", "submitter": "Guy Martial Nkiet", "authors": "Armando Sosthene Kali Balogoun, Guy Martial Nkiet and Carlos\n  Ogouyandjou", "title": "Kernel based method for the $k$-sample problem", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we deal with the problem of testing for the equality of $k$\nprobability distributions defined on $(\\mathcal{X},\\mathcal{B})$, where\n$\\mathcal{X}$ is a metric space and $\\mathcal{B}$ is the corresponding Borel\n$\\sigma$-field. We introduce a test statistic based on reproducing kernel\nHilbert space embeddings and derive its asymptotic distribution under the null\nhypothesis. Simulations show that the introduced procedure outperforms known\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 23:48:19 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Balogoun", "Armando Sosthene Kali", ""], ["Nkiet", "Guy Martial", ""], ["Ogouyandjou", "Carlos", ""]]}, {"id": "1812.00258", "submitter": "Wenge Guo", "authors": "Wenge Guo, Gavin Lynch, Joseph P. Romano", "title": "A New Approach for Large Scale Multiple Testing with Application to FDR\n  Control for Graphically Structured Hypotheses", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many large scale multiple testing applications, the hypotheses often have\na known graphical structure, such as gene ontology in gene expression data.\nExploiting this graphical structure in multiple testing procedures can improve\npower as well as aid in interpretation. However, incorporating the structure\ninto large scale testing procedures and proving that an error rate, such as the\nfalse discovery rate (FDR), is controlled can be challenging. In this paper, we\nintroduce a new general approach for large scale multiple testing, which can\naid in developing new procedures under various settings with proven control of\ndesired error rates. This approach is particularly useful for developing FDR\ncontrolling procedures, which is simplified as the problem of developing\nper-family error rate (PFER) controlling procedures. Specifically, for testing\nhypotheses with a directed acyclic graph (DAG) structure, by using the general\napproach, under the assumption of independence, we first develop a specific\nPFER controlling procedure and based on this procedure, then develop a new FDR\ncontrolling procedure, which can preserve the desired DAG structure among the\nrejected hypotheses. Through a small simulation study and a real data analysis,\nwe illustrate nice performance of the proposed FDR controlling procedure for\nDAG-structured hypotheses.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 20:23:19 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Guo", "Wenge", ""], ["Lynch", "Gavin", ""], ["Romano", "Joseph P.", ""]]}, {"id": "1812.00260", "submitter": "Andrea Arf\\`e", "authors": "Andrea Arf\\`e, Stefano Peluso, Pietro Muliere", "title": "The semi-Markov beta-Stacy process: a Bayesian non-parametric prior for\n  semi-Markov processes", "comments": "Accepted for publication in the journal Statistical Inference for\n  Stochastic Processes on July 23, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on Bayesian methods for the analysis of discrete-time\nsemi-Markov processes is sparse. In this paper, we introduce the semi-Markov\nbeta-Stacy process, a stochastic process useful for the Bayesian non-parametric\nanalysis of semi-Markov processes. The semi-Markov beta-Stacy process is\nconjugate with respect to data generated by a semi-Markov process, a property\nwhich makes it easy to obtain probabilistic forecasts. Its predictive\ndistributions are characterized by a reinforced random walk on a system of\nurns.\n", "versions": [{"version": "v1", "created": "Sat, 1 Dec 2018 20:36:54 GMT"}, {"version": "v2", "created": "Thu, 23 Jul 2020 13:53:59 GMT"}], "update_date": "2020-07-24", "authors_parsed": [["Arf\u00e8", "Andrea", ""], ["Peluso", "Stefano", ""], ["Muliere", "Pietro", ""]]}, {"id": "1812.00308", "submitter": "Dongha Kim", "authors": "Yongdai Kim and Dongha Kim", "title": "On variation of gradients of deep neural networks", "comments": "30 pages, 6 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a theoretical explanation of the role of the number of nodes at\neach layer in deep neural networks. We prove that the largest variation of a\ndeep neural network with ReLU activation function arises when the layer with\nthe fewest nodes changes its activation pattern. An important implication is\nthat deep neural network is a useful tool to generate functions most of whose\nvariations are concentrated on a smaller area of the input space near the\nboundaries corresponding to the layer with the fewest nodes. In turn, this\nproperty makes the function more invariant to input transformation. That is,\nour theoretical result gives a clue about how to design the architecture of a\ndeep neural network to increase complexity and transformation invariancy\nsimultaneously.\n", "versions": [{"version": "v1", "created": "Sun, 2 Dec 2018 02:37:56 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Kim", "Yongdai", ""], ["Kim", "Dongha", ""]]}, {"id": "1812.00532", "submitter": "Yiming Sun", "authors": "Yiming Sun, Yige Li, Amy Kuceyeski, Sumanta Basu", "title": "Large Spectral Density Matrix Estimation by Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral density matrix estimation of multivariate time series is a classical\nproblem in time series and signal processing. In modern neuroscience, spectral\ndensity based metrics are commonly used for analyzing functional connectivity\namong brain regions. In this paper, we develop a non-asymptotic theory for\nregularized estimation of high-dimensional spectral density matrices of\nGaussian and linear processes using thresholded versions of averaged\nperiodograms. Our theoretical analysis ensures that consistent estimation of\nspectral density matrix of a $p$-dimensional time series using $n$ samples is\npossible under high-dimensional regime $\\log p / n \\rightarrow 0$ as long as\nthe true spectral density is approximately sparse. A key technical component of\nour analysis is a new concentration inequality of average periodogram around\nits expectation, which is of independent interest. Our estimation consistency\nresults complement existing results for shrinkage based estimators of\nmultivariate spectral density, which require no assumption on sparsity but only\nensure consistent estimation in a regime $p^2/n \\rightarrow 0$. In addition,\nour proposed thresholding based estimators perform consistent and automatic\nedge selection when learning coherence networks among the components of a\nmultivariate time series. We demonstrate the advantage of our estimators using\nsimulation studies and a real data application on functional connectivity\nanalysis with fMRI data.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 03:04:08 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Sun", "Yiming", ""], ["Li", "Yige", ""], ["Kuceyeski", "Amy", ""], ["Basu", "Sumanta", ""]]}, {"id": "1812.00584", "submitter": "Khadija Musayeva", "authors": "Khadija Musayeva (ABC), Fabien Lauer (ABC), Yann Guermeur (ABC)", "title": "Rademacher Complexity and Generalization Performance of Multi-category\n  Margin Classifiers", "comments": "Neurocomputing, Elsevier, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main open problems in the theory of multi-category margin\nclassification is the form of the optimal dependency of a guaranteed risk on\nthe number C of categories, the sample size m and the margin parameter gamma.\nFrom a practical point of view, the theoretical analysis of generalization\nperformance contributes to the development of new learning algorithms. In this\npaper, we focus only on the theoretical aspect of the question posed. More\nprecisely, under minimal learnability assumptions, we derive a new risk bound\nfor multi-category margin classifiers. We improve the dependency on C over the\nstate of the art when the margin loss function considered satisfies the\nLipschitz condition. We start with the basic supremum inequality that involves\na Rademacher complexity as a capacity measure. This capacity measure is then\nlinked to the metric entropy through the chaining method. In this context, our\nimprovement is based on the introduction of a new combinatorial metric entropy\nbound.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 07:41:07 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Musayeva", "Khadija", "", "ABC"], ["Lauer", "Fabien", "", "ABC"], ["Guermeur", "Yann", "", "ABC"]]}, {"id": "1812.00721", "submitter": "Beatriz Bueno-Larraz", "authors": "Beatriz Bueno-Larraz, Jos\\'e R. Berrendero, Antonio Cuevas", "title": "On functional logistic regression: some conceptual issues", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main ideas behind the classical multivariate logistic regression model\nmake sense when translated to the functional setting, where the explanatory\nvariable $X$ is a function and the response $Y$ is binary. However, some\nimportant technical issues appear (or are aggravated with respect to those of\nthe multivariate case) due to the functional nature of the explanatory\nvariable. First, the mere definition of the model can be questioned: while most\napproaches so far proposed rely on the $L_2$-based model, we suggest an\nalternative (in some sense, more general) approach, based on the theory of\nReproducing Kernel Hilbert Spaces (RKHS). The validity conditions of such\nRKHS-based model, as well as its relation with the $L_2$-based one are\ninvestigated and made explicit in two formal results. Some relevant particular\ncases are considered as well. Second we show that, under very general\nconditions, the maximum likelihood (ML) of the logistic model parameters fail\nto exist in the functional case. Third, on a more positive side, we suggest an\nRKHS-based restricted version of the ML estimator. This is a methodological\npaper, aimed at a better understanding of the functional logistic model, rather\nthan focusing on numerical and practical issues.\n", "versions": [{"version": "v1", "created": "Mon, 3 Dec 2018 13:16:37 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 09:54:52 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Bueno-Larraz", "Beatriz", ""], ["Berrendero", "Jos\u00e9 R.", ""], ["Cuevas", "Antonio", ""]]}, {"id": "1812.00769", "submitter": "Aditya Gangrade", "authors": "Aditya Gangrade, Praveen Venkatesh, Bobak Nazer and Venkatesh\n  Saligrama", "title": "Testing Changes in Communities for the Stochastic Block Model", "comments": "Version 3 includes material on unbalanced but linearly sized\n  communities. This version is to appear in NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG cs.SI math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze the problems of \\textit{community goodness-of-fit and\ntwo-sample testing} for stochastic block models (SBM), where changes arise due\nto modification in community memberships of nodes. Motivated by practical\napplications, we consider the challenging sparse regime, where expected node\ndegrees are constant, and the inter-community mean degree ($b$) scales\nproportionally to intra-community mean degree ($a$). Prior work has sharply\ncharacterized partial or full community recovery in terms of a \"signal-to-noise\nratio\" ($\\mathrm{SNR}$) based on $a$ and $b$. For both problems, we propose\ncomputationally-efficient tests that can succeed far beyond the regime where\nrecovery of community membership is even possible. Overall, for large changes,\n$s \\gg \\sqrt{n}$, we need only $\\mathrm{SNR}= O(1)$ whereas a na\\\"ive test\nbased on community recovery with $O(s)$ errors requires $\\mathrm{SNR}=\n\\Theta(\\log n)$. Conversely, in the small change regime, $s \\ll \\sqrt{n}$, via\nan information-theoretic lower bound, we show that, surprisingly, no algorithm\ncan do better than the na\\\"ive algorithm that first estimates the community up\nto $O(s)$ errors and then detects changes. We validate these phenomena\nnumerically on SBMs and on real-world datasets as well as Markov Random Fields\nwhere we only observe node data rather than the existence of links.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 20:09:21 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 05:12:21 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 03:20:52 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Gangrade", "Aditya", ""], ["Venkatesh", "Praveen", ""], ["Nazer", "Bobak", ""], ["Saligrama", "Venkatesh", ""]]}, {"id": "1812.01188", "submitter": "Yilin Zhang", "authors": "Yilin Zhang, Karl Rohe, Sebastien Roch", "title": "Reducing Seed Bias in Respondent-Driven Sampling by Estimating Block\n  Transition Probabilities", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI math.PR stat.ME stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Respondent-driven sampling (RDS) is a popular approach to study marginalized\nor hard-to-reach populations. It collects samples from a networked population\nby incentivizing participants to refer their friends into the study. One major\nchallenge in analyzing RDS samples is seed bias. Seed bias refers to the fact\nthat when the social network is divided into multiple communities (or blocks),\nthe RDS sample might not provide a balanced representation of the different\ncommunities in the population, and such unbalance is correlated with the\ninitial participant (or the seed). In this case, the distributions of\nestimators are typically non-trivial mixtures, which are determined (1) by the\nseed and (2) by how the referrals transition from one block to another. This\npaper shows that (1) block-transition probabilities are easy to estimate with\nhigh accuracy, and (2) we can use these estimated block-transition\nprobabilities to estimate the stationary distribution over blocks and thus, an\nestimate of the block proportions. This stationary distribution on blocks has\npreviously been used in the RDS literature to evaluate whether the sampling\nprocess has appeared to `mix'. We use these estimated block proportions in a\nsimple post-stratified (PS) estimator that greatly diminishes seed bias. By\naggregating over the blocks/strata in this way, we prove that the PS estimator\nis $\\sqrt{n}$-consistent under a Markov model, even when other estimators are\nnot. Simulations show that the PS estimator has smaller Root Mean Square Error\n(RMSE) compared to the state-of-the-art estimators.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 03:08:38 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Zhang", "Yilin", ""], ["Rohe", "Karl", ""], ["Roch", "Sebastien", ""]]}, {"id": "1812.01255", "submitter": "Teng Zhang", "authors": "Teng Zhang", "title": "Phase Retrieval by Alternating Minimization with Random Initialization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a phase retrieval problem, where the goal is to reconstruct a\n$n$-dimensional complex vector from its phaseless scalar products with $m$\nsensing vectors, independently sampled from complex normal distributions. We\nshow that, with a random initialization, the classical algorithm of alternating\nminimization succeeds with high probability as $n,m\\rightarrow\\infty$ when\n${m}/{\\log^3m}\\geq Mn^{3/2}\\log^{1/2}n$ for some $M>0$. This is a step toward\nproving the conjecture in \\cite{Waldspurger2016}, which conjectures that the\nalgorithm succeeds when $m=O(n)$. The analysis depends on an approach that\nenables the decoupling of the dependency between the algorithmic iterates and\nthe sensing vectors.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 07:34:26 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Zhang", "Teng", ""]]}, {"id": "1812.01314", "submitter": "Gunnar Taraldsen", "authors": "Gunnar Taraldsen, Jarle Tufto, and Bo H. Lindqvist", "title": "Statistics with improper posteriors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1933 Kolmogorov constructed a general theory that defines the modern\nconcept of conditional probability. In 1955 Renyi fomulated a new axiomatic\ntheory for probability motivated by the need to include unbounded measures. We\nintroduce a general concept of conditional probability in Renyi spaces. In this\ntheory improper priors are allowed, and the resulting posteriors can also be\nimproper.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 10:23:28 GMT"}], "update_date": "2018-12-05", "authors_parsed": [["Taraldsen", "Gunnar", ""], ["Tufto", "Jarle", ""], ["Lindqvist", "Bo H.", ""]]}, {"id": "1812.01367", "submitter": "Yuting Lan", "authors": "Ning Zhang, Wenxin Jiang and Yuting Lan", "title": "On the sure screening properties of iteratively sure independence\n  screening algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fan and Lv (2008) proposed the path-breaking theory of sure independence\nscreening (SIS) and an iterative algorithm (ISIS) to effectively reduce the\npredictor dimension for further variable selection approaches. Fan et al.\n(2009) extended ISIS to generalized linear models and introduced the Vanilla\nISIS (Van-ISIS) algorithm, allowing selected predictors to be screened out in\nupcoming iterations. The success of SIS depends on its sure screening property,\nwhich was obtained by Fan and Lv (2008) under the marginal correlation\nassumption. However, despite wide applications of ISIS and Van-ISIS in various\nscientific fields, their sure screening properties have not been proved during\nthe past decade. To fill this gap, we prove the sure screening properties of\nthree different types of iterative algorithms for linear models without relying\non the marginal correlation assumption, where ISIS and Van-ISIS can be regarded\nas two special cases of them.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 12:22:03 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 13:36:43 GMT"}, {"version": "v3", "created": "Wed, 16 Oct 2019 13:03:11 GMT"}, {"version": "v4", "created": "Sun, 17 Nov 2019 13:52:41 GMT"}], "update_date": "2019-11-19", "authors_parsed": [["Zhang", "Ning", ""], ["Jiang", "Wenxin", ""], ["Lan", "Yuting", ""]]}, {"id": "1812.01734", "submitter": "Sebastian Engelke", "authors": "Sebastian Engelke, Adrien S. Hitz", "title": "Graphical Models for Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional independence, graphical models and sparsity are key notions for\nparsimonious statistical models and for understanding the structural\nrelationships in the data. The theory of multivariate and spatial extremes\ndescribes the risk of rare events through asymptotically justified limit models\nsuch as max-stable and multivariate Pareto distributions. Statistical modelling\nin this field has been limited to moderate dimensions so far, partly owing to\ncomplicated likelihoods and a lack of understanding of the underlying\nprobabilistic structures. We introduce a general theory of conditional\nindependence for multivariate Pareto distributions that allows the definition\nof graphical models and sparsity for extremes. A Hammersley-Clifford theorem\nlinks this new notion to the factorization of densities of extreme value models\non graphs. For the popular class of H\\\"usler-Reiss distributions we show that,\nsimilarly to the Gaussian case, the sparsity pattern of a general extremal\ngraphical model can be read off from suitable inverse covariance matrices. New\nparametric models can be built in a modular way and statistical inference can\nbe simplified to lower-dimensional marginals. We discuss learning of minimum\nspanning trees and model selection for extremal graph structures, and\nillustrate their use with an application to flood risk assessment on the Danube\nriver.\n", "versions": [{"version": "v1", "created": "Tue, 4 Dec 2018 22:47:43 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 16:27:47 GMT"}], "update_date": "2019-11-14", "authors_parsed": [["Engelke", "Sebastian", ""], ["Hitz", "Adrien S.", ""]]}, {"id": "1812.01831", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin and Fang Yao", "title": "Intrinsic Riemannian Functional Data Analysis", "comments": "46 pages, 3 figures, 3 tables", "journal-ref": "The Annals of Statistics 2019, Vol. 47, No. 6, 3533-3577", "doi": "10.1214/18-AOS1787", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop a novel and foundational framework for analyzing\ngeneral Riemannian functional data, in particular a new development of tensor\nHilbert spaces along curves on a manifold. Such spaces enable us to derive\nKarhunen-Loeve expansion for Riemannian random processes. This framework also\nfeatures an approach to compare objects from different tensor Hilbert spaces,\nwhich paves the way for asymptotic analysis in Riemannian functional data\nanalysis. Built upon intrinsic geometric concepts such as vector field,\nLevi-Civita connection and parallel transport on Riemannian manifolds, the\ndeveloped framework applies to not only Euclidean submanifolds but also\nmanifolds without a natural ambient space. As applications of this framework,\nwe develop intrinsic Riemannian functional principal component analysis\n(iRFPCA) and intrinsic Riemannian functional linear regression (iRFLR) that are\ndistinct from their traditional and ambient counterparts. We also provide\nestimation procedures for iRFPCA and iRFLR, and investigate their asymptotic\nproperties within the intrinsic geometry. Numerical performance is illustrated\nby simulated and real examples.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 06:46:20 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 13:12:57 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Lin", "Zhenhua", ""], ["Yao", "Fang", ""]]}, {"id": "1812.01938", "submitter": "Ioannis Kosmidis", "authors": "Ioannis Kosmidis, David Firth", "title": "Jeffreys-prior penalty, finiteness and shrinkage in binomial-response\n  generalized linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalization of the likelihood by Jeffreys' invariant prior, or by a positive\npower thereof, is shown to produce finite-valued maximum penalized likelihood\nestimates in a broad class of binomial generalized linear models. The class of\nmodels includes logistic regression, where the Jeffreys-prior penalty is known\nadditionally to reduce the asymptotic bias of the maximum likelihood estimator;\nand also models with other commonly used link functions such as probit and\nlog-log. Shrinkage towards equiprobability across observations, relative to the\nmaximum likelihood estimator, is established theoretically and is studied\nthrough illustrative examples. Some implications of finiteness and shrinkage\nfor inference are discussed, particularly when inference is based on Wald-type\nprocedures. A widely applicable procedure is developed for computation of\nmaximum penalized likelihood estimates, by using repeated maximum likelihood\nfits with iteratively adjusted binomial responses and totals. These theoretical\nresults and methods underpin the increasingly widespread use of reduced-bias\nand similarly penalized binomial regression models in many applied fields.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 12:02:39 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 16:28:10 GMT"}, {"version": "v3", "created": "Wed, 14 Aug 2019 17:38:59 GMT"}, {"version": "v4", "created": "Mon, 23 Mar 2020 20:17:37 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Kosmidis", "Ioannis", ""], ["Firth", "David", ""]]}, {"id": "1812.01948", "submitter": "Zhe Liu", "authors": "Zhe Liu", "title": "Least absolute deviations uncertain regression with imprecise\n  observations", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally regression analysis answers questions about the relationships\namong variables based on the assumption that the observation values of\nvariables are precise numbers. It has long been dominated by least squares\ntechniques, mostly due to their elegant theoretical foundation and ease of\nimplementation. However, in many cases, we can only get imprecise observation\nvalues and the assumptions upon which the least squares is based may not be\nvalid. So this paper characterizes the imprecise data in terms of uncertain\nvariables and proposes a novel robust approach under the principle of least\nabsolute deviations to estimate the unknown parameters in uncertain regression\nmodels. Finally, numerical examples are documented to illustrate our method.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 12:11:44 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Liu", "Zhe", ""]]}, {"id": "1812.02061", "submitter": "Yacouba Boubacar Mainassara", "authors": "Yacouba Boubacar Ma\\\"inassara (LMB), Othman Kadmiri (LMB), Bruno\n  Saussereau (LMB)", "title": "Estimation of multivariate asymmetric power GARCH models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now widely accepted that volatility models have to incorporate the\nso-called leverage effect in order to to model the dynamics of daily financial\nreturns.We suggest a new class of multivariate power transformed asymmetric\nmodels. It includes several functional forms of multivariate GARCH models which\nare of great interest in financial modeling and time series literature. We\nprovide an explicit necessary and sufficient condition to establish the strict\nstationarity of the model. We derive the asymptotic properties of the\nquasi-maximum likelihood estimator of the parameters. These properties are\nestablished both when the power of the transformation is known or is unknown.\nThe asymptotic results are illustrated by Monte Carlo experiments. An\napplication to real financial data is also proposed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 15:53:57 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 08:18:46 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ma\u00efnassara", "Yacouba Boubacar", "", "LMB"], ["Kadmiri", "Othman", "", "LMB"], ["Saussereau", "Bruno", "", "LMB"]]}, {"id": "1812.02072", "submitter": "Andrew Sills", "authors": "Andrew V. Sills and Charles W. Champ", "title": "The exponential distribution analog of the Grubbs--Weaver method", "comments": "10 pages; to appear in Communications in Statistics: Theory and\n  Methods", "journal-ref": "Communications in Statistics: Theory and Methods 49 (2020) pp.\n  1894-1903", "doi": "10.1080/03610926.2019.1565839", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grubbs and Weaver (JASA 42 (1947) 224--241) suggest a minimum-variance\nunbiased estimator for the population standard deviation of a normal random\nvariable, where a random sample is drawn and a weighted sum of the ranges of\nsubsamples is calculated. The optimal choice involves using as many subsamples\nof size eight as possible. They verified their results numerically for samples\nof size up to 100, and conjectured that their \"rule of eights\" is valid for all\nsample sizes. Here we examine the analogous problem where the underlying\ndistribution is exponential and find that a \"rule of fours\" yields optimality\nand prove the result rigorously.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 16:07:40 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Sills", "Andrew V.", ""], ["Champ", "Charles W.", ""]]}, {"id": "1812.02127", "submitter": "Konstantinos Spiliopoulos", "authors": "Konstantinos Spiliopoulos", "title": "Information geometry for approximate Bayesian computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to explore the basic Approximate Bayesian\nComputation (ABC) algorithm via the lens of information theory. ABC is a widely\nused algorithm in cases where the likelihood of the data is hard to work with\nor intractable, but one can simulate from it. We use relative entropy ideas to\nanalyze the behavior of the algorithm as a function of the threshold parameter\nand of the size of the data. Relative entropy here is data driven as it depends\non the values of the observed statistics. Relative entropy also allows us to\nexplore the effect of the distance metric and sets up a mathematical framework\nfor sensitivity analysis allowing to find important directions which could lead\nto lower computational cost of the algorithm for the same level of accuracy. In\naddition, we also investigate the bias of the estimators for generic\nobservables as a function of both the threshold parameters and the size of the\ndata. Our analysis provides error bounds on performance for positive tolerances\nand finite sample sizes. Simulation studies complement and illustrate the\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 17:30:03 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 19:35:19 GMT"}], "update_date": "2019-08-14", "authors_parsed": [["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1812.02150", "submitter": "Ryan Martin", "authors": "Ryan Martin and Bo Ning", "title": "Empirical priors and coverage of posterior credible sets in a sparse\n  normal mean model", "comments": "18 pages, 1 figure. Comments welcome at\n  https://www.researchers.one/article/2018-12-6", "journal-ref": "Sankhya A, 2020, volume 82, pages 477--498; special issue in\n  memory of Professor J. K. Ghosh", "doi": "10.1007/s13171-019-00189-w", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods provide a natural means for uncertainty quantification, that\nis, credible sets can be easily obtained from the posterior distribution. But\nis this uncertainty quantification valid in the sense that the posterior\ncredible sets attain the nominal frequentist coverage probability? This paper\ninvestigates the frequentist validity of posterior uncertainty quantification\nbased on a class of empirical priors in the sparse normal mean model. In\nparticular, we show that our marginal posterior credible intervals achieve the\nnominal frequentist coverage probability under conditions slightly weaker than\nneeded for selection consistency and a Bernstein--von Mises theorem for the\nfull posterior, and numerical investigations suggest that our empirical Bayes\nmethod has superior frequentist coverage probability properties compared to\nother fully Bayes methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Dec 2018 18:36:11 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 14:50:12 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Martin", "Ryan", ""], ["Ning", "Bo", ""]]}, {"id": "1812.02337", "submitter": "Qihui Chen", "authors": "Qihui Chen, Zheng Fang", "title": "Improved Inference on the Rank of a Matrix", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a general framework for conducting inference on the rank\nof an unknown matrix $\\Pi_0$. A defining feature of our setup is the null\nhypothesis of the form $\\mathrm H_0: \\mathrm{rank}(\\Pi_0)\\le r$. The problem is\nof first order importance because the previous literature focuses on $\\mathrm\nH_0': \\mathrm{rank}(\\Pi_0)= r$ by implicitly assuming away\n$\\mathrm{rank}(\\Pi_0)<r$, which may lead to invalid rank tests due to\nover-rejections. In particular, we show that limiting distributions of test\nstatistics under $\\mathrm H_0'$ may not stochastically dominate those under\n$\\mathrm{rank}(\\Pi_0)<r$. A multiple test on the nulls\n$\\mathrm{rank}(\\Pi_0)=0,\\ldots,r$, though valid, may be substantially\nconservative. We employ a testing statistic whose limiting distributions under\n$\\mathrm H_0$ are highly nonstandard due to the inherent irregular natures of\nthe problem, and then construct bootstrap critical values that deliver size\ncontrol and improved power. Since our procedure relies on a tuning parameter, a\ntwo-step procedure is designed to mitigate concerns on this nuisance. We\nadditionally argue that our setup is also important for estimation. We\nillustrate the empirical relevance of our results through testing\nidentification in linear IV models that allows for clustered data and inference\non sorting dimensions in a two-sided matching model with transferrable utility.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 03:46:11 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 13:46:29 GMT"}], "update_date": "2019-03-26", "authors_parsed": [["Chen", "Qihui", ""], ["Fang", "Zheng", ""]]}, {"id": "1812.02435", "submitter": "Joon Kwon", "authors": "Joon Kwon and Guillaume Lecu\\'e and Matthieu Lerasle", "title": "A MOM-based ensemble method for robustness, subsampling and\n  hyperparameter tuning", "comments": "17 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameters tuning and model selection are important steps in machine\nlearning. Unfortunately, classical hyperparameter calibration and model\nselection procedures are sensitive to outliers and heavy-tailed data. In this\nwork, we construct a selection procedure which can be seen as a robust\nalternative to cross-validation and is based on a median-of-means principle.\nUsing this procedure, we also build an ensemble method which, trained with\nalgorithms and corrupted heavy-tailed data, selects an algorithm, trains it\nwith a large uncorrupted subsample and automatically tune its hyperparameters.\nThe construction relies on a divide-and-conquer methodology, making this method\neasily scalable for autoML given a corrupted database. This method is tested\nwith the LASSO which is known to be highly sensitive to outliers.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 10:13:37 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 12:39:27 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Kwon", "Joon", ""], ["Lecu\u00e9", "Guillaume", ""], ["Lerasle", "Matthieu", ""]]}, {"id": "1812.02703", "submitter": "Max Fathi", "authors": "Max Fathi", "title": "Higher-order Stein kernels for Gaussian approximation", "comments": "16 pages, comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.FA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce higher-order Stein kernels relative to the standard Gaussian\nmeasure, which generalize the usual Stein kernels by involving higher-order\nderivatives of test functions. We relate the associated discrepancies to\nvarious metrics on the space of probability measures and prove new functional\ninequalities involving them. As an application, we obtain new explicit improved\nrates of convergence in the classical multidimensional CLT under higher moment\nand regularity assumptions.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 18:37:36 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Fathi", "Max", ""]]}, {"id": "1812.02709", "submitter": "Sotirios Sabanis", "authors": "M. Barkhagen, N. H. Chau, \\'E. Moulines, M. R\\'asonyi, S. Sabanis, Y.\n  Zhang", "title": "On stochastic gradient Langevin dynamics with dependent data streams in\n  the logconcave case", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sampling from a probability distribution $\\pi$ on\n$\\rset^d$ which has a density \\wrt\\ the Lebesgue measure known up to a\nnormalization factor $x \\mapsto \\rme^{-U(x)} / \\int_{\\rset^d} \\rme^{-U(y)} \\rmd\ny$. We analyze a sampling method based on the Euler discretization of the\nLangevin stochastic differential equations under the assumptions that the\npotential $U$ is continuously differentiable, $\\nabla U$ is Lipschitz, and $U$\nis strongly concave. We focus on the case where the gradient of the log-density\ncannot be directly computed but unbiased estimates of the gradient from\npossibly dependent observations are available. This setting can be seen as a\ncombination of a stochastic approximation (here stochastic gradient) type\nalgorithms with discretized Langevin dynamics. We obtain an upper bound of the\nWasserstein-2 distance between the law of the iterates of this algorithm and\nthe target distribution $\\pi$ with constants depending explicitly on the\nLipschitz and strong convexity constants of the potential and the dimension of\nthe space. Finally, under weaker assumptions on $U$ and its gradient but in the\npresence of independent observations, we obtain analogous results in\nWasserstein-2 distance.\n", "versions": [{"version": "v1", "created": "Thu, 6 Dec 2018 18:42:28 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 20:59:35 GMT"}, {"version": "v3", "created": "Sun, 15 Sep 2019 15:58:09 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Barkhagen", "M.", ""], ["Chau", "N. H.", ""], ["Moulines", "\u00c9.", ""], ["R\u00e1sonyi", "M.", ""], ["Sabanis", "S.", ""], ["Zhang", "Y.", ""]]}, {"id": "1812.03090", "submitter": "Monika Bhattacharjee", "authors": "Monika Bhattacharjee, Moulinath Banerjee, George Michailidis", "title": "Change Point Estimation in a Dynamic Stochastic Block Model", "comments": "Please see the .pdf file for an extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the location of a single change point\nin a dynamic stochastic block model. We propose two methods of estimating the\nchange point, together with the model parameters. The first employs a least\nsquares criterion function and takes into consideration the full structure of\nthe stochastic block model and is evaluated at each point in time. Hence, as an\nintermediate step, it requires estimating the community structure based on a\nclustering algorithm at every time point. The second method comprises of the\nfollowing two steps: in the first one, a least squares function is used and\nevaluated at each time point, but ignores the community structures and just\nconsiders a random graph generating mechanism exhibiting a change point. Once\nthe change point is identified, in the second step, all network data before and\nafter it are used together with a clustering algorithm to obtain the\ncorresponding community structures and subsequently estimate the generating\nstochastic block model parameters. A comparison between these two methods is\nillustrated. Further, for both methods under their respective identifiability\nand certain additional regularity conditions, we establish rates of convergence\nand derive the asymptotic distributions of the change point estimators. The\nresults are illustrated on synthetic data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 16:25:10 GMT"}, {"version": "v2", "created": "Wed, 20 May 2020 14:13:56 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Bhattacharjee", "Monika", ""], ["Banerjee", "Moulinath", ""], ["Michailidis", "George", ""]]}, {"id": "1812.03108", "submitter": "Piotr Kokoszka", "authors": "Piotr Kokoszka, Stilian Stoev, Qian Xiong", "title": "Principal components analysis of regularly varying functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is concerned with asymptotic properties of the principal components\nanalysis of functional data. The currently available results assume the\nexistence of the fourth moment. We develop analogous results in a setting which\ndoes not require this assumption. Instead, we assume that the observed\nfunctions are regularly varying. We derive the asymptotic distribution of the\nsample covariance operator and of the sample functional principal components.\nWe obtain a number of results on the convergence of moments and almost sure\nconvergence. We apply the new theory to establish the consistency of the\nregression operator in a functional linear model.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 17:14:28 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Kokoszka", "Piotr", ""], ["Stoev", "Stilian", ""], ["Xiong", "Qian", ""]]}, {"id": "1812.03121", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca", "title": "Variable selection in high-dimensional linear model with possibly\n  asymmetric or heavy-tailed errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of automatic variable selection in a linear model\nwith asymmetric or heavy-tailed errors when the number of explanatory variables\ndiverges with the sample size. For this high-dimensional model, the penalized\nleast square method is not appropriate and the quantile framework makes the\ninference more difficult because to the non differentiability of the loss\nfunction. We propose and study an estimation method by penalizing the expectile\nprocess with an adaptive LASSO penalty. Two cases are considered: the number of\nmodel parameters is smaller and afterwards larger than the sample size, the two\ncases being distinct by the adaptive penalties considered. For each case we\ngive the rate convergence and establish the oracle properties of the adaptive\nLASSO expectile estimator. The proposed estimators are evaluated through Monte\nCarlo simulations and compared with the adaptive LASSO quantile estimator. We\napplied also our estimation method to real data in genetics when the number of\nparameters is greater than the sample size.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 17:36:00 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Ciuperca", "Gabriela", ""]]}, {"id": "1812.03150", "submitter": "Majid Mojirsheibani", "authors": "Ali Al-Sharadqah and Majid Mojirsheibani", "title": "A simple approach to construct confidence bands for a regression\n  function with incomplete data", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long-standing problem in the construction of asymptotically correct\nconfidence bands for a regression function $m(x)=E[Y|X=x]$, where $Y$ is the\nresponse variable influenced by the covariate $X$, involves the situation where\n$Y$ values may be missing at random, and where the selection probability, the\ndensity function $f(x)$ of $X$, and the conditional variance of $Y$ given $X$\nare all completely unknown. This can be particularly more complicated in\nnonparametric situations. In this paper we propose a new kernel-type regression\nestimator and study the limiting distribution of the properly normalized\nversions of the maximal deviation of the proposed estimator from the true\nregression curve. The resulting limiting distribution will be used to construct\nuniform confidence bands for the underlying regression curve with\nasymptotically correct coverages. The focus of the current paper is on the case\nwhere $X\\in \\mathbb{R}$. We also perform numerical studies to assess the\nfinite-sample performance of the proposed method. In this paper, both mechanics\nand the theoretical validity of our methods are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 18:20:05 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Al-Sharadqah", "Ali", ""], ["Mojirsheibani", "Majid", ""]]}, {"id": "1812.03225", "submitter": "Joakim And\\'en", "authors": "Joakim And\\'en and Jos\\'e Luis Romero", "title": "Multitaper estimation on arbitrary domains", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multitaper estimators have enjoyed significant success in estimating spectral\ndensities from finite samples using as tapers Slepian functions defined on the\nacquisition domain. Unfortunately, the numerical calculation of these Slepian\ntapers is only tractable for certain symmetric domains, such as rectangles or\ndisks. In addition, no performance bounds are currently available for the mean\nsquared error of the spectral density estimate. This situation is inadequate\nfor applications such as cryo-electron microscopy, where noise models must be\nestimated from irregular domains with small sample sizes. We show that the\nmultitaper estimator only depends on the linear space spanned by the tapers. As\na result, Slepian tapers may be replaced by proxy tapers spanning the same\nsubspace (validating the common practice of using partially converged solutions\nto the Slepian eigenproblem as tapers). These proxies may consequently be\ncalculated using standard numerical algorithms for block diagonalization. We\nalso prove a set of performance bounds for multitaper estimators on arbitrary\ndomains. The method is demonstrated on synthetic and experimental datasets from\ncryo-electron microscopy, where it reduces mean squared error by a factor of\ntwo or more compared to traditional methods.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 21:54:03 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 20:54:39 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 22:06:52 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["And\u00e9n", "Joakim", ""], ["Romero", "Jos\u00e9 Luis", ""]]}, {"id": "1812.03240", "submitter": "Tingyi Zhu", "authors": "Tingyi Zhu, Dimitris N. Politis", "title": "Higher-order Accurate Spectral Density Estimation of Functional Time\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the frequency domain framework for weakly dependent functional time\nseries, a key element is the spectral density kernel which encapsulates the\nsecond-order dynamics of the process. We propose a class of spectral density\nkernel estimators based on the notion of a flat-top kernel. The new class of\nestimators employs the inverse Fourier transform of a flat-top function as the\nweight function employed to smooth the periodogram. It is shown that using a\nflat-top kernel yields a bias reduction and results in a higher-order accuracy\nin terms of optimizing the integrated mean square error (IMSE). Notably, the\nhigher-order accuracy of flat-top estimation comes at the sacrifice of the\npositive semi-definite property. Nevertheless, we show how a flat-top estimator\ncan be modified to become positive semi-definite (even strictly positive\ndefinite) in finite samples while retaining its favorable asymptotic\nproperties. In addition, we introduce a data-driven bandwidth selection\nprocedure realized by an automatic inspection of the estimated correlation\nstructure. Our asymptotic results are complemented by a finite-sample\nsimulation where the higher-order accuracy of flat-top estimators is manifested\nin practice.\n", "versions": [{"version": "v1", "created": "Fri, 7 Dec 2018 23:37:16 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Zhu", "Tingyi", ""], ["Politis", "Dimitris N.", ""]]}, {"id": "1812.03403", "submitter": "Aukosh Jagannath", "authors": "Aukosh Jagannath, Patrick Lopatto, Leo Miolane", "title": "Statistical thresholds for Tensor PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the statistical limits of testing and estimation for a rank one\ndeformation of a Gaussian random tensor. We compute the sharp thresholds for\nhypothesis testing and estimation by maximum likelihood and show that they are\nthe same. Furthermore, we find that the maximum likelihood estimator achieves\nthe maximal correlation with the planted vector among measurable estimators\nabove the estimation threshold. In this setting, the maximum likelihood\nestimator exhibits a discontinuous BBP-type transition: below the critical\nthreshold the estimator is orthogonal to the planted vector, but above the\ncritical threshold, it achieves positive correlation which is uniformly bounded\naway from zero.\n", "versions": [{"version": "v1", "created": "Sat, 8 Dec 2018 23:01:34 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 15:27:59 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Jagannath", "Aukosh", ""], ["Lopatto", "Patrick", ""], ["Miolane", "Leo", ""]]}, {"id": "1812.03510", "submitter": "Natsuki Kariya", "authors": "Natsuki Kariya, and Sumio Watanabe", "title": "Asymptotic Analysis of the Bayesian Likelihood Ratio for Testing\n  Homogeneity in Normal Mixture Models", "comments": "29 pages, 4 figures, Section 5 is added to the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we use the normal mixture model, the optimal number of the components\ndescribing the data should be determined. Testing homogeneity is good for this\npurpose; however, to construct its theory is challenging, since the test\nstatistic does not converge to the $\\chi^{2}$ distribution even asymptotically.\nThe reason for such asymptotic behavior is that the parameter set describing\nthe null hypothesis (N.H.) contains singularities in the space of the\nalternative hypothesis (A.H.). Recently, a $\\it{Bayesian}$ theory for singular\nmodels was developed, and it has elucidated various problems of statistical\ninference. However, its application to hypothesis tests for singular models has\nbeen limited. In this paper, we introduce a scaling technique that greatly\nsimplifies the derivation and study testing of homogeneity for the first time\nthe basis of Bayesian theory. We derive the asymptotic distributions of the\nmarginal likelihood ratios in three cases:\n  (1) only the mixture ratio is a variable in the A.H. ;\n  (2) the mixture ratio and the mean of the mixed distribution are variables;\n  And (3) the mixture ratio, the mean, and the variance of the mixed\ndistribution are variables.; In all cases, the results are complex, but can be\ndescribed as functions of random variables obeying normal distributions. A\ntesting scheme based on them was constructed, and their validity was confirmed\nthrough numerical experiments.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 16:06:48 GMT"}, {"version": "v2", "created": "Sun, 22 Dec 2019 15:00:30 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Kariya", "Natsuki", ""], ["Watanabe", "Sumio", ""]]}, {"id": "1812.03523", "submitter": "Stanislav Minsker", "authors": "Stanislav Minsker", "title": "Uniform bounds for robust mean estimators", "comments": "Several improvements to the main results; the case of adversarial\n  contamination is treated in more detail", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the estimators of the mean that provide strong\nnon-asymptotic guarantees under minimal assumptions on the underlying\ndistribution. The main ideas behind proposed techniques are based on bridging\nthe notions of symmetry and robustness. We show that existing methods, such as\nmedian-of-means and Catoni's estimators, can often be viewed as special cases\nof our construction. The main contribution of the paper is the proof of uniform\nbounds for the deviations of the stochastic process defined by proposed\nestimators. Moreover, we extend our results to the case of adversarial\ncontamination where a constant fraction of the observations is arbitrarily\ncorrupted. Finally, we apply our methods to the problem of robust multivariate\nmean estimation and show that obtained inequalities achieve optimal dependence\non the proportion of corrupted samples.\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 17:05:17 GMT"}, {"version": "v2", "created": "Wed, 19 Dec 2018 15:39:23 GMT"}, {"version": "v3", "created": "Sat, 29 Dec 2018 14:33:07 GMT"}, {"version": "v4", "created": "Fri, 3 May 2019 19:05:35 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Minsker", "Stanislav", ""]]}, {"id": "1812.03548", "submitter": "Nikita Zhivotovskiy", "authors": "Yegor Klochkov, Nikita Zhivotovskiy", "title": "Uniform Hanson-Wright type concentration inequalities for unbounded\n  entries via the entropy method", "comments": "29 pages; reviewers' suggestions taken into account", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to uniform versions of the Hanson-Wright inequality for\na random vector $X \\in \\mathbb{R}^n$ with independent subgaussian components.\nThe core technique of the paper is based on the entropy method combined with\ntruncations of both gradients of functions of interest and of the coordinates\nof $X$ itself. Our results recover, in particular, the classic uniform bound of\nTalagrand (1996) for Rademacher chaoses and the more recent uniform result of\nAdamczak (2015), which holds under certain rather strong assumptions on the\ndistribution of $X$. We provide several applications of our techniques: we\nestablish a version of the standard Hanson-Wright inequality, which is tighter\nin some regimes. Extending our techniques we show a version of the\ndimension-free matrix Bernstein inequality that holds for random matrices with\na subexponential spectral norm. We apply the derived inequality to the problem\nof covariance estimation with missing observations and prove an almost optimal\nhigh probability version of the recent result of Lounici (2014). Finally, we\nshow a uniform Hanson-Wright type inequality in the Ising model under\nDobrushin's condition. A closely related question was posed by Marton (2003).\n", "versions": [{"version": "v1", "created": "Sun, 9 Dec 2018 19:14:43 GMT"}, {"version": "v2", "created": "Wed, 7 Aug 2019 21:04:53 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Klochkov", "Yegor", ""], ["Zhivotovskiy", "Nikita", ""]]}, {"id": "1812.03599", "submitter": "Dongha Kim", "authors": "Yongdai Kim, Ilsang Ohn and Dongha Kim", "title": "Fast convergence rates of deep neural networks for classification", "comments": "35 pages, 2 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the fast convergence rates of a deep neural network (DNN)\nclassifier with the rectified linear unit (ReLU) activation function learned\nusing the hinge loss. We consider three cases for a true model: (1) a smooth\ndecision boundary, (2) smooth conditional class probability, and (3) the margin\ncondition (i.e., the probability of inputs near the decision boundary is\nsmall). We show that the DNN classifier learned using the hinge loss achieves\nfast rate convergences for all three cases provided that the architecture\n(i.e., the number of layers, number of nodes and sparsity). is carefully\nselected. An important implication is that DNN architectures are very flexible\nfor use in various cases without much modification. In addition, we consider a\nDNN classifier learned by minimizing the cross-entropy, and show that the DNN\nclassifier achieves a fast convergence rate under the condition that the\nconditional class probabilities of most data are sufficiently close to either 1\nor zero. This assumption is not unusual for image recognition because human\nbeings are extremely good at recognizing most images. To confirm our\ntheoretical explanation, we present the results of a small numerical study\nconducted to compare the hinge loss and cross-entropy.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 02:41:06 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 08:23:48 GMT"}], "update_date": "2019-06-19", "authors_parsed": [["Kim", "Yongdai", ""], ["Ohn", "Ilsang", ""], ["Kim", "Dongha", ""]]}, {"id": "1812.03652", "submitter": "Karthik Sriram", "authors": "Karthik Sriram and R.V. Ramamoorthi", "title": "On posterior concentration rates for Bayesian quantile regression based\n  on the misspecified asymmetric Laplace likelihood", "comments": "There is an error in the paper. We have realized that Assumption 2b\n  in page 5 of the paper may be incorrect and the issue remains unresolved as\n  of August 8, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asymmetric Laplace density (ALD) is used as a working likelihood for\nBayesian quantile regression. Sriram et al.(2013) derived posterior consistency\nfor Bayesian linear quantile regression based on the misspecified ALD. While\ntheir paper also argued for $\\sqrt{n}-$consistency, Sriram and Ramamoorthi\n(2017) highlighted that the argument was only valid for a rate less than\n$\\sqrt{n}$. So, the question of $\\sqrt{n}-$rate has remained unaddressed, but\nis necessary to carry out meaningful Bayesian inference based on the ALD. In\nthis paper, we derive results to obtain posterior consistency rates for\nBayesian quantile regression based on the misspecified ALD. We derive our\nresults in a slightly general setting where the quantile function can possibly\nbe non-linear. In particular, we give sufficient conditions for\n$\\sqrt{n}-$consistency for the Bayesian linear quantile regression using ALD.\nWe also provide examples of Bayesian non-linear quantile regression.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 07:18:58 GMT"}, {"version": "v2", "created": "Wed, 6 May 2020 06:05:04 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 06:40:05 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Sriram", "Karthik", ""], ["Ramamoorthi", "R. V.", ""]]}, {"id": "1812.03659", "submitter": "Lili Zheng", "authors": "Lili Zheng and Garvesh Raskutti", "title": "Testing for high-dimensional network parameters in auto-regressive\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional auto-regressive models provide a natural way to model\ninfluence between $M$ actors given multi-variate time series data for $T$ time\nintervals. While there has been considerable work on network estimation, there\nis limited work in the context of inference and hypothesis testing. In\nparticular, prior work on hypothesis testing in time series has been restricted\nto linear Gaussian auto-regressive models. From a practical perspective, it is\nimportant to determine suitable statistical tests for connections between\nactors that go beyond the Gaussian assumption. In the context of\n\\emph{high-dimensional} time series models, confidence intervals present\nadditional estimators since most estimators such as the Lasso and Dantzig\nselectors are biased which has led to \\emph{de-biased} estimators. In this\npaper we address these challenges and provide convergence in distribution\nresults and confidence intervals for the multi-variate AR(p) model with\nsub-Gaussian noise, a generalization of Gaussian noise that broadens\napplicability and presents numerous technical challenges. The main technical\nchallenge lies in the fact that unlike Gaussian random vectors, for\nsub-Gaussian vectors zero correlation does not imply independence. The proof\nrelies on using an intricate truncation argument to develop novel concentration\nbounds for quadratic forms of dependent sub-Gaussian random variables. Our\nconvergence in distribution results hold provided $T = \\Omega((s \\vee \\rho)^2\n\\log^2 M)$, where $s$ and $\\rho$ refer to sparsity parameters which matches\nexisted results for hypothesis testing with i.i.d. samples. We validate our\ntheoretical results with simulation results for both block-structured and\nchain-structured networks.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 07:43:35 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 20:19:50 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Zheng", "Lili", ""], ["Raskutti", "Garvesh", ""]]}, {"id": "1812.03766", "submitter": "Alexey Lebedev", "authors": "Alexey V. Lebedev", "title": "On the Interrelation between Dependence Coefficients of Extreme Value\n  Copulas", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For extreme value copulas with a known upper tail dependence coefficient we\nfind pointwise upper and lower bounds, which are used to establish upper and\nlower bounds of the Spearman and Kendall correlation coefficients. We shown\nthat in all cases the lower bounds are attained on Marshall--Olkin copulas, and\nthe upper ones, on copulas with piecewise linear dependence functions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 12:47:59 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Lebedev", "Alexey V.", ""]]}, {"id": "1812.03970", "submitter": "Caterina May", "authors": "Nancy Flournoy, Caterina May, Chiara Tommasi", "title": "The Effects of Adaptation on Inference for Non-Linear Regression Models\n  with Normal Errors", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work studies the properties of the maximum likelihood estimator (MLE) of\na non-linear model with Gaussian errors and multidimensional parameter. The\nobservations are collected in a two-stage experimental design and are dependent\nsince the second stage design is determined by the observations at the first\nstage; the MLE maximizes the total likelihood. Differently from the most of the\nliterature, the first stage sample size is small, and hence asymptotic\napproximation is used only in the second stage. It is proved that the MLE is\nconsistent and that its asymptotic distribution is a specific Gaussian mixture,\nvia stable convergence. Finally, a simulation study is provided in the case of\na dose-response Emax model.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 18:39:42 GMT"}, {"version": "v2", "created": "Sat, 27 Jul 2019 10:08:13 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 16:13:00 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Flournoy", "Nancy", ""], ["May", "Caterina", ""], ["Tommasi", "Chiara", ""]]}, {"id": "1812.04249", "submitter": "Jake Soloff", "authors": "Jake A. Soloff, Adityanand Guntuboyina, Jim Pitman", "title": "Distribution-free properties of isotonic regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the isotonic least squares estimator is characterized\nas the derivative of the greatest convex minorant of a random walk. Provided\nthe walk has exchangeable increments, we prove that the slopes of the greatest\nconvex minorant are distributed as order statistics of the running averages.\nThis result implies an exact non-asymptotic formula for the squared error risk\nof least squares in isotonic regression when the true sequence is constant that\nholds for every exchangeable error distribution.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 07:48:48 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Soloff", "Jake A.", ""], ["Guntuboyina", "Adityanand", ""], ["Pitman", "Jim", ""]]}, {"id": "1812.04343", "submitter": "Alejandro  Cholaquidis", "authors": "Alejandro Cholaquidis, Ricardo Fraiman, Badih Ghattas, Juan\n  Kalemkerian", "title": "A combined strategy for multivariate density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear aggregation strategies have recently been proposed in response to\nthe problem of how to combine, in a non-linear way, estimators of the\nregression function (see for instance \\cite{biau:16}), classification rules\n(see \\cite{ch:16}), among others. Although there are several linear strategies\nto aggregate density estimators, most of them are hard to compute (even in\nmoderate dimensions). Our approach aims to overcome this problem by estimating\nthe density at a point $x$ using not just sample points close to $x$ but in a\nneighborhood of the (estimated) level set $f(x)$. We show, both theoretically\nand through a simulation study, that the mean squared error of our proposal is\nsmaller than that of the aggregated densities. A Central Limit Theorem is also\nproven.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 12:01:09 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 17:08:14 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Cholaquidis", "Alejandro", ""], ["Fraiman", "Ricardo", ""], ["Ghattas", "Badih", ""], ["Kalemkerian", "Juan", ""]]}, {"id": "1812.04356", "submitter": "Clement Levrard", "authors": "Aur\\'elie Fischer (LPSM (UMR\\_8001)), Cl\\'ement Levrard (DATASHAPE,\n  LPSM (UMR\\_8001)), Claire Br\\'echeteau (ECN, LMJL, DATASHAPE)", "title": "Robust Bregman Clustering", "comments": "Annals of Statistics, Institute of Mathematical Statistics, In press", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a trimming approach, we investigate a k-means type method based on\nBregman divergences for clustering data possibly corrupted with clutter noise.\nThe main interest of Bregman divergences is that the standard Lloyd algorithm\nadapts to these distortion measures, and they are well-suited for clustering\ndata sampled according to mixture models from exponential families. We prove\nthat there exists an optimal codebook, and that an empirically optimal codebook\nconverges a.s. to an optimal codebook in the distortion sense. Moreover, we\nobtain the sub-Gaussian rate of convergence for k-means 1 $\\sqrt$ n under mild\ntail assumptions. Also, we derive a Lloyd-type algorithm with a trimming\nparameter that can be selected from data according to some heuristic, and\npresent some experimental results.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 12:35:36 GMT"}, {"version": "v2", "created": "Fri, 10 Apr 2020 15:15:17 GMT"}, {"version": "v3", "created": "Wed, 9 Sep 2020 14:11:38 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Fischer", "Aur\u00e9lie", "", "LPSM"], ["Levrard", "Cl\u00e9ment", "", "DATASHAPE,\n  LPSM"], ["Br\u00e9cheteau", "Claire", "", "ECN, LMJL, DATASHAPE"]]}, {"id": "1812.04700", "submitter": "Konstantinos Nikolakakis", "authors": "Konstantinos E. Nikolakakis, Dionysios S. Kalogerias and Anand D.\n  Sarwate", "title": "Predictive Learning on Hidden Tree-Structured Ising Models", "comments": "82 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide high-probability sample complexity guarantees for exact structure\nrecovery and accurate predictive learning using noise-corrupted samples from an\nacyclic (tree-shaped) graphical model. The hidden variables follow a\ntree-structured Ising model distribution, whereas the observable variables are\ngenerated by a binary symmetric channel taking the hidden variables as its\ninput (flipping each bit independently with some constant probability $q\\in\n[0,1/2)$). In the absence of noise, predictive learning on Ising models was\nrecently studied by Bresler and Karzand (2020); this paper quantifies how noise\nin the hidden model impacts the tasks of structure recovery and marginal\ndistribution estimation by proving upper and lower bounds on the sample\ncomplexity. Our results generalize state-of-the-art bounds reported in prior\nwork, and they exactly recover the noiseless case ($q=0$). In fact, for any\ntree with $p$ vertices and probability of incorrect recovery $\\delta>0$, the\nsufficient number of samples remains logarithmic as in the noiseless case,\ni.e., $\\mathcal{O}(\\log(p/\\delta))$, while the dependence on $q$ is\n$\\mathcal{O}\\big( 1/(1-2q)^{4} \\big)$, for both aforementioned tasks. We also\npresent a new equivalent of Isserlis' Theorem for sign-valued tree-structured\ndistributions, yielding a new low-complexity algorithm for higher-order moment\nestimation.\n", "versions": [{"version": "v1", "created": "Tue, 11 Dec 2018 21:32:16 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 04:57:01 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2020 16:53:09 GMT"}, {"version": "v4", "created": "Wed, 17 Feb 2021 03:11:05 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Nikolakakis", "Konstantinos E.", ""], ["Kalogerias", "Dionysios S.", ""], ["Sarwate", "Anand D.", ""]]}, {"id": "1812.04795", "submitter": "Amadou Diadie Ba", "authors": "Ba Amadou Diadie and Gane Samb Lo", "title": "Divergence measures estimation and its asymptotic normality theory in\n  the discrete case", "comments": "28 pages, 8 figures. arXiv admin note: substantial text overlap with\n  arXiv:1704.04536", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide the asymptotic theory of the general of\n$\\phi$-divergences measures, which include the most common divergence measures\n: Renyi and Tsallis families and the Kullback-Leibler measure. We are\ninterested in divergence measures in the discrete case. One sided and two-sided\nstatistical tests are derived as well as symmetrized estimators. Almost sure\nrates of convergence and asymptotic normality theorem are obtained in the\ngeneral case, and next particularized for the Renyi and Tsallis families and\nfor the Kullback-Leibler measure as well. Our theorical results are validated\nby simulations.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 03:37:19 GMT"}, {"version": "v2", "created": "Mon, 11 Mar 2019 08:57:57 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 11:49:59 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Diadie", "Ba Amadou", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1812.05005", "submitter": "Guang Cheng", "authors": "Jiexin Duan, Xingye Qiao and Guang Cheng", "title": "Distributed Nearest Neighbor Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor is a popular nonparametric method for classification and\nregression with many appealing properties. In the big data era, the sheer\nvolume and spatial/temporal disparity of big data may prohibit centrally\nprocessing and storing the data. This has imposed considerable hurdle for\nnearest neighbor predictions since the entire training data must be memorized.\nOne effective way to overcome this issue is the distributed learning framework.\nThrough majority voting, the distributed nearest neighbor classifier achieves\nthe same rate of convergence as its oracle version in terms of both the regret\nand instability, up to a multiplicative constant that depends solely on the\ndata dimension. The multiplicative difference can be eliminated by replacing\nmajority voting with the weighted voting scheme. In addition, we provide sharp\ntheoretical upper bounds of the number of subsamples in order for the\ndistributed nearest neighbor classifier to reach the optimal convergence rate.\nIt is interesting to note that the weighted voting scheme allows a larger\nnumber of subsamples than the majority voting one. Our findings are supported\nby numerical studies using both simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 16:19:57 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Duan", "Jiexin", ""], ["Qiao", "Xingye", ""], ["Cheng", "Guang", ""]]}, {"id": "1812.05068", "submitter": "Tijana Zrnic", "authors": "Tijana Zrnic, Aaditya Ramdas, Michael I. Jordan", "title": "Asynchronous Online Testing of Multiple Hypotheses", "comments": "36 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of asynchronous online testing, aimed at providing\ncontrol of the false discovery rate (FDR) during a continual stream of data\ncollection and testing, where each test may be a sequential test that can start\nand stop at arbitrary times. This setting increasingly characterizes real-world\napplications in science and industry, where teams of researchers across large\norganizations may conduct tests of hypotheses in a decentralized manner. The\noverlap in time and space also tends to induce dependencies among test\nstatistics, a challenge for classical methodology, which either assumes (overly\noptimistically) independence or (overly pessimistically) arbitrary dependence\nbetween test statistics. We present a general framework that addresses both of\nthese issues via a unified computational abstraction that we refer to as\n\"conflict sets.\" We show how this framework yields algorithms with formal FDR\nguarantees under a more intermediate, local notion of dependence. We illustrate\nour algorithms in simulations by comparing to existing algorithms for online\nFDR control.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2018 18:12:55 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 19:15:28 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Zrnic", "Tijana", ""], ["Ramdas", "Aaditya", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1812.05202", "submitter": "Lin Wang", "authors": "Lin Wang, Hongquan Xu", "title": "A Class of Multilevel Nonregular Designs for Studying Quantitative\n  Factors", "comments": null, "journal-ref": "Statistica Sinica 2021", "doi": "10.5705/ss.202020.0223", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fractional factorial designs are widely used for designing screening\nexperiments. Nonregular fractional factorial designs can have better properties\nthan regular designs, but their construction is challenging. Current research\non the construction of nonregular designs focuses on two-level designs. We\nprovide a novel class of multilevel nonregular designs by permuting levels of\nregular designs. We develop a theory illustrating how levels can be permuted\nwithout computer search and accordingly propose a sequential method for\nconstructing nonregular designs. Compared to regular designs, these nonregular\ndesigns can provide more accurate estimations on factorial effects and more\nefficient screening for experiments with quantitative factors. We further\nexplore the space-filling property of the obtained designs and demonstrate\ntheir superiority.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 00:05:42 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 16:04:24 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Wang", "Lin", ""], ["Xu", "Hongquan", ""]]}, {"id": "1812.05421", "submitter": "Michael Vogt", "authors": "Michael Vogt", "title": "On the Differences between L2-Boosting and the Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that L2-Boosting lacks a theoretical property which is central to\nthe behaviour of l1-penalized methods such as basis pursuit and the Lasso:\nWhereas l1-penalized methods are guaranteed to recover the sparse parameter\nvector in a high-dimensional linear model under an appropriate restricted\nnullspace property, L2-Boosting is not guaranteed to do so. Hence, L2-Boosting\nbehaves quite differently from l1-penalized methods when it comes to parameter\nrecovery/estimation in high-dimensional linear models.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 13:41:38 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Vogt", "Michael", ""]]}, {"id": "1812.05537", "submitter": "Line K\\\"uhnel", "authors": "Line K\\\"uhnel, Alexis Arnaudon, Tom Fletcher, and Stefan Sommer", "title": "Stochastic Image Deformation in Frequency Domain and Parameter\n  Estimation using Moment Evolutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling deformation of anatomical objects observed in medical images can\nhelp describe disease progression patterns and variations in anatomy across\npopulations. We apply a stochastic generalisation of the Large Deformation\nDiffeomorphic Metric Mapping (LDDMM) framework to model differences in the\nevolution of anatomical objects detected in populations of image data. The\ncomputational challenges that are prevalent even in the deterministic LDDMM\nsetting are handled by extending the FLASH LDDMM representation to the\nstochastic setting keeping a finite discretisation of the infinite dimensional\nspace of image deformations. In this computationally efficient setting, we\nperform estimation to infer parameters for noise correlations and local\nvariability in datasets of images. Fundamental for the optimisation procedure\nis using the finite dimensional Fourier representation to derive approximations\nof the evolution of moments for the stochastic warps. Particularly, the first\nmoment allows us to infer deformation mean trajectories. The second moment\nencodes variation around the mean, and thus provides information on the noise\ncorrelation. We show on simulated datasets of 2D MR brain images that the\nestimation algorithm can successfully recover parameters of the stochastic\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 17:46:06 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["K\u00fchnel", "Line", ""], ["Arnaudon", "Alexis", ""], ["Fletcher", "Tom", ""], ["Sommer", "Stefan", ""]]}, {"id": "1812.05553", "submitter": "Holger Dette", "authors": "Holger Dette, Maria Konstantinou, Kirsten Schorning", "title": "Optimal designs for series estimation in nonparametric regression with\n  correlated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the problem of designing experiments for series\nestimators in nonparametric regression models with correlated observations. We\nuse projection based estimators to derive an explicit solution of the best\nlinear oracle estimator in the continuous time model for all Markovian-type\nerror processes. These solutions are then used to construct estimators, which\ncan be calculated from the available data along with their corresponding\noptimal design points. Our results are illustrated by means of a simulation\nstudy, which demonstrates that the new series estimator has a better\nperformance than the commonly used techniques based on the optimal linear\nunbiased estimators. Moreover, we show that the performance of the estimators\nproposed in this paper can be further improved by choosing the design points\nappropriately.\n", "versions": [{"version": "v1", "created": "Thu, 13 Dec 2018 18:08:50 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Dette", "Holger", ""], ["Konstantinou", "Maria", ""], ["Schorning", "Kirsten", ""]]}, {"id": "1812.05893", "submitter": "Erwan Koch", "authors": "Erwan Koch and Christian Y. Robert", "title": "Stochastic derivative estimation for max-stable random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider expected performances based on max-stable random fields and we\nare interested in their derivatives with respect to the spatial dependence\nparameters of those fields. Max-stable fields, such as the Brown--Resnick and\nSmith fields, are very popular in spatial extremes. We focus on the two most\npopular unbiased stochastic derivative estimation approaches: the likelihood\nratio method (LRM) and the infinitesimal perturbation analysis (IPA). LRM\nrequires the multivariate density of the max-stable field to be explicit, and\nIPA necessitates the computation of the derivative with respect to the\nparameters for each simulated value. We propose convenient and tractable\nconditions ensuring the validity of LRM and IPA in the cases of the\nBrown--Resnick and Smith field, respectively. Obtaining such conditions is\nintricate owing to the very structure of max-stable fields. Then we focus on\nrisk and dependence measures, which constitute one of the several frameworks\nwhere our theoretical results can be useful. We perform a simulation study\nwhich shows that both LRM and IPA perform well in various configurations, and\nprovide a real case study that is valuable for the insurance industry.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 12:49:29 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 05:58:21 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 14:13:34 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Koch", "Erwan", ""], ["Robert", "Christian Y.", ""]]}, {"id": "1812.06037", "submitter": "Keisuke Yano", "authors": "Keisuke Yano, Ryoya Kaneko, Fumiyasu Komaki", "title": "Minimax Predictive Density for Sparse Count Data", "comments": "49 pages; the supplement is included in pp. 32-49 Accepted for\n  publication in Bernoulli journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses predictive densities under the Kullback--Leibler loss\nfor high-dimensional Poisson sequence models under sparsity constraints.\nSparsity in count data implies zero-inflation. We present a class of Bayes\npredictive densities that attain asymptotic minimaxity in sparse Poisson\nsequence models. We also show that our class with an estimator of unknown\nsparsity level plugged-in is adaptive in the asymptotically minimax sense. For\napplication, we extend our results to settings with quasi-sparsity and with\nmissing-completely-at-random observations. The simulation studies as well as\napplication to real data illustrate the efficiency of the proposed Bayes\npredictive densities.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 17:21:12 GMT"}, {"version": "v2", "created": "Mon, 25 Mar 2019 02:12:39 GMT"}, {"version": "v3", "created": "Tue, 12 Nov 2019 09:40:38 GMT"}, {"version": "v4", "created": "Sat, 5 Sep 2020 14:04:24 GMT"}], "update_date": "2020-09-08", "authors_parsed": [["Yano", "Keisuke", ""], ["Kaneko", "Ryoya", ""], ["Komaki", "Fumiyasu", ""]]}, {"id": "1812.06046", "submitter": "Ben Berckmoes", "authors": "Ben Berckmoes, Anna Ivanova, Geert Molenberghs", "title": "Conditional bias reduction can be dangerous: a key example from\n  sequential analysis", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a key example from sequential analysis, which illustrates that\nconditional bias reduction can cause infinite mean absolute error.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 17:37:58 GMT"}, {"version": "v2", "created": "Mon, 17 Dec 2018 11:51:30 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Berckmoes", "Ben", ""], ["Ivanova", "Anna", ""], ["Molenberghs", "Geert", ""]]}, {"id": "1812.06063", "submitter": "Tommy Reddad", "authors": "Luc Devroye and Tommy Reddad", "title": "Discrete minimax estimation with trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a simple recursive data-based partitioning scheme which produces\npiecewise-constant or piecewise-linear density estimates on intervals, and show\nhow this scheme can determine the optimal $L_1$ minimax rate for some discrete\nnonparametric classes.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 18:25:16 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 20:10:44 GMT"}, {"version": "v3", "created": "Thu, 27 Jun 2019 22:41:06 GMT"}], "update_date": "2019-07-01", "authors_parsed": [["Devroye", "Luc", ""], ["Reddad", "Tommy", ""]]}, {"id": "1812.06167", "submitter": "Ben Boukai", "authors": "Ben Boukai and Yue Zhang", "title": "Recycled Least Squares Estimation in Nonlinear Regression", "comments": "19 pages with 4 figures and 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a resampling scheme for parameters estimates in nonlinear\nregression models. We provide an estimation procedure which recycles, via\nrandom weighting, the relevant parameters estimates to construct consistent\nestimates of the sampling distribution of the various estimates. We establish\nthe asymptotic normality of the resampled estimates and demonstrate the\napplicability of the recycling approach in a small simulation study and via\nexample.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 21:12:17 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Boukai", "Ben", ""], ["Zhang", "Yue", ""]]}, {"id": "1812.06189", "submitter": "Fang Han", "authors": "Mathias Drton, Fang Han, and Hongjian Shi", "title": "High dimensional consistent independence testing with maxima of rank\n  correlations", "comments": "to appear in the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing mutual independence for high-dimensional observations is a\nfundamental statistical challenge. Popular tests based on linear and simple\nrank correlations are known to be incapable of detecting non-linear,\nnon-monotone relationships, calling for methods that can account for such\ndependences. To address this challenge, we propose a family of tests that are\nconstructed using maxima of pairwise rank correlations that permit consistent\nassessment of pairwise independence. Built upon a newly developed\nCram\\'{e}r-type moderate deviation theorem for degenerate U-statistics, our\nresults cover a variety of rank correlations including Hoeffding's $D$,\nBlum-Kiefer-Rosenblatt's $R$, and Bergsma-Dassios-Yanagimoto's $\\tau^*$. The\nproposed tests are distribution-free in the class of multivariate distributions\nwith continuous margins, implementable without the need for permutation, and\nare shown to be rate-optimal against sparse alternatives under the Gaussian\ncopula model. As a by-product of the study, we reveal an identity between the\naforementioned three rank correlation statistics, and hence make a step towards\nproving a conjecture of Bergsma and Dassios.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 22:06:53 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2020 17:27:12 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Drton", "Mathias", ""], ["Han", "Fang", ""], ["Shi", "Hongjian", ""]]}, {"id": "1812.06209", "submitter": "Amin Jaber", "authors": "Amin Jaber, Jiji Zhang, Elias Bareinboim", "title": "Causal Identification under Markov Equivalence", "comments": "10 pages, 4 figures, In Proceedings of the 34th Conference on\n  Uncertainty in Artificial Intelligence (UAI2018). AUAI Press: 978-987", "journal-ref": null, "doi": null, "report-no": "Report-no: R-35", "categories": "cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the magnitude of cause-and-effect relations is one of the central\nchallenges found throughout the empirical sciences. The problem of\nidentification of causal effects is concerned with determining whether a causal\neffect can be computed from a combination of observational data and substantive\nknowledge about the domain under investigation, which is formally expressed in\nthe form of a causal graph. In many practical settings, however, the knowledge\navailable for the researcher is not strong enough so as to specify a unique\ncausal graph. Another line of investigation attempts to use observational data\nto learn a qualitative description of the domain called a Markov equivalence\nclass, which is the collection of causal graphs that share the same set of\nobserved features. In this paper, we marry both approaches and study the\nproblem of causal identification from an equivalence class, represented by a\npartial ancestral graph (PAG). We start by deriving a set of graphical\nproperties of PAGs that are carried over to its induced subgraphs. We then\ndevelop an algorithm to compute the effect of an arbitrary set of variables on\nan arbitrary outcome set. We show that the algorithm is strictly more powerful\nthan the current state of the art found in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 00:31:52 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Jaber", "Amin", ""], ["Zhang", "Jiji", ""], ["Bareinboim", "Elias", ""]]}, {"id": "1812.06270", "submitter": "Burim Ramosaj", "authors": "Burim Ramosaj and Markus Pauly", "title": "Consistent Estimation of Residual Variance with Random Forest Out-Of-Bag\n  Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of estimating residual variance in regression models has\nexperienced relatively little attention in the machine learning community.\nHowever, the estimate is of primary interest in many practical applications,\ne.g. as a primary step towards the construction of prediction intervals. Here,\nwe consider this issue for the random forest. Therein, the functional\nrelationship between covariates and response variable is modeled by a weighted\nsum of the latter. The dependence structure is, however, involved in the\nweights that are constructed during the tree construction process making the\nmodel complex in mathematical analysis. Restricting to L2-consistent random\nforest models, we provide random forest based residual variance estimators and\nprove their consistency.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 11:14:57 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Ramosaj", "Burim", ""], ["Pauly", "Markus", ""]]}, {"id": "1812.06282", "submitter": "Jiho Lee", "authors": "Paul Jung, Jiho Lee, Sam Staton, and Hongseok Yang", "title": "A Generalization of Hierarchical Exchangeability on Trees to Directed\n  Acyclic Graphs", "comments": "35 pages, 10 figures. Accepted version before re-formatting", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.LO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the problem of designing inference-friendly Bayesian\nnonparametric models in probabilistic programming languages, we introduce a\ngeneral class of partially exchangeable random arrays which generalizes the\nnotion of hierarchical exchangeability introduced in Austin and Panchenko\n(2014). We say that our partially exchangeable arrays are DAG-exchangeable\nsince their partially exchangeable structure is governed by a collection of\nDirected Acyclic Graphs. More specifically, such a random array is indexed by\n$\\mathbb{N}^{|V|}$ for some DAG $G=(V,E)$, and its exchangeability structure is\ngoverned by the edge set $E$. We prove a representation theorem for such arrays\nwhich generalizes the Aldous-Hoover and Austin-Panchenko representation\ntheorems.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 13:13:42 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 06:41:27 GMT"}, {"version": "v3", "created": "Fri, 20 Sep 2019 06:31:54 GMT"}, {"version": "v4", "created": "Fri, 24 Jul 2020 17:17:01 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Jung", "Paul", ""], ["Lee", "Jiho", ""], ["Staton", "Sam", ""], ["Yang", "Hongseok", ""]]}, {"id": "1812.06295", "submitter": "Kirankumar Shiragur", "authors": "Arun Jambulapati, Kirankumar Shiragur and Aaron Sidford", "title": "Efficient Structured Matrix Recovery and Nearly-Linear Time Algorithms\n  for Solving Inverse Symmetric $M$-Matrices", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how to recover a spectral approximations to broad\nclasses of structured matrices using only a polylogarithmic number of adaptive\nlinear measurements to either the matrix or its inverse. Leveraging this result\nwe obtain faster algorithms for variety of linear algebraic problems. Key\nresults include:\n  $\\bullet$ A nearly linear time algorithm for solving the inverse of symmetric\n$M$-matrices, a strict superset of Laplacians and SDD matrices.\n  $\\bullet$ An $\\tilde{O}(n^2)$ time algorithm for solving $n \\times n$ linear\nsystems that are constant spectral approximations of Laplacians or more\ngenerally, SDD matrices.\n  $\\bullet$ An $\\tilde{O}(n^2)$ algorithm to recover a spectral approximation\nof a $n$-vertex graph using only $\\tilde{O}(1)$ matrix-vector multiplies with\nits Laplacian matrix.\n  The previous best results for each problem either used a trivial number of\nqueries to exactly recover the matrix or a trivial $O(n^\\omega)$ running time,\nwhere $\\omega$ is the matrix multiplication constant.\n  We achieve these results by generalizing recent semidefinite programming\nbased linear sized sparsifier results of Lee and Sun (2017) and providing\niterative methods inspired by the semistreaming sparsification results of\nKapralov, Lee, Musco, Musco and Sidford (2014) and input sparsity time linear\nsystem solving results of Li, Miller, and Peng (2013). We hope that by\ninitiating study of these natural problems, expanding the robustness and scope\nof recent nearly linear time linear system solving research, and providing\ngeneral matrix recovery machinery this work may serve as a stepping stone for\nfaster algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 14:37:28 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Jambulapati", "Arun", ""], ["Shiragur", "Kirankumar", ""], ["Sidford", "Aaron", ""]]}, {"id": "1812.06310", "submitter": "Moreno Bevilacqua", "authors": "M. Bevilacqua, C. Caama\\~no, R. B. Arellano Valle, V. Morales-On\\~nate", "title": "Non-Gaussian Geostatistical Modeling using (skew) t Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new model for regression and dependence analysis when addressing\nspatial data with possibly heavy tails and an asymmetric marginal distribution.\nWe first propose a stationary process with $t$ marginals obtained through scale\nmixing of a Gaussian process with an inverse square root process with Gamma\nmarginals. We then generalize this construction by considering a skew-Gaussian\nprocess, thus obtaining a process with skew-t marginal distributions. For the\nproposed (skew) $t$ process we study the second-order and geometrical\nproperties and in the $t$ case, we provide analytic expressions for the\nbivariate distribution. In an extensive simulation study, we investigate the\nuse of the weighted pairwise likelihood as a method of estimation for the $t$\nprocess. Moreover we compare the performance of the optimal linear predictor of\nthe $t$ process versus the optimal Gaussian predictor. Finally, the\neffectiveness of our methodology is illustrated by analyzing a georeferenced\ndataset on maximum temperatures in Australia\n", "versions": [{"version": "v1", "created": "Sat, 15 Dec 2018 15:44:26 GMT"}, {"version": "v2", "created": "Mon, 22 Apr 2019 16:05:50 GMT"}, {"version": "v3", "created": "Thu, 19 Dec 2019 11:15:35 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Bevilacqua", "M.", ""], ["Caama\u00f1o", "C.", ""], ["Valle", "R. B. Arellano", ""], ["Morales-On\u00f1ate", "V.", ""]]}, {"id": "1812.06551", "submitter": "Shinjini Nandi", "authors": "Shinjini Nandi and Sanat K. Sarkar", "title": "Adapting BH to One- and Two-Way Classified Structures of Hypotheses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple testing literature contains ample research on controlling false\ndiscoveries for hypotheses classified according to one criterion, which we\nrefer to as one-way classified hypotheses. Although simultaneous classification\nof hypotheses according to two different criteria, resulting in two-way\nclassified hypotheses, do often occur in scientific studies, no such research\nhas taken place yet, as far as we know, under this structure. This article\nproduces procedures, both in their oracle and data-adaptive forms, for\ncontrolling the overall false discovery rate (FDR) across all hypotheses\neffectively capturing the underlying one- or two-way classification structure.\nThey have been obtained by using results associated with weighted\nBenjamini-Hochberg (BH) procedure in their more general forms providing\nguidance on how to adapt the original BH procedure to the underlying one- or\ntwo-way classification structure through an appropriate choice of the weights.\nThe FDR is maintained non-asymptotically by our proposed procedures in their\noracle forms under positive regression dependence on subset of null $p$-values\n(PRDS) and in their data-adaptive forms under independence of the $p$-values.\nPossible control of FDR for our data-adaptive procedures in certain scenarios\ninvolving dependent $p$-values have been investigated through simulations. The\nfact that our suggested procedures can be superior to contemporary practices\nhas been demonstrated through their applications in simulated scenarios and to\nreal-life data sets. While the procedures proposed here for two-way classified\nhypotheses are new, the data-adaptive procedure obtained for one-way classified\nhypotheses is alternative to and often more powerful than those proposed in Hu\net al. (2010).\n", "versions": [{"version": "v1", "created": "Sun, 16 Dec 2018 22:44:47 GMT"}, {"version": "v2", "created": "Sat, 9 Mar 2019 03:33:04 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Nandi", "Shinjini", ""], ["Sarkar", "Sanat K.", ""]]}, {"id": "1812.06894", "submitter": "Yinqiu He", "authors": "Yinqiu He, Tiefeng Jiang, Jiyang Wen and Gongjun Xu", "title": "Likelihood Ratio Test in Multivariate Linear Regression: from Low to\n  High Dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate linear regressions are widely used statistical tools in many\napplications to model the associations between multiple related responses and a\nset of predictors. To infer such associations, it is often of interest to test\nthe structure of the regression coefficients matrix, and the likelihood ratio\ntest (LRT) is one of the most popular approaches in practice. Despite its\npopularity, it is known that the classical $\\chi^2$ approximations for LRTs\noften fail in high-dimensional settings, where the dimensions of responses and\npredictors $(m,p)$ are allowed to grow with the sample size $n$. Though various\ncorrected LRTs and other test statistics have been proposed in the literature,\nthe fundamental question of when the classic LRT starts to fail is less\nstudied, an answer to which would provide insights for practitioners,\nespecially when analyzing data with $m/n$ and $p/n$ small but not negligible.\nMoreover, the power performance of the LRT in high-dimensional data analysis\nremains underexplored. To address these issues, the first part of this work\ngives the asymptotic boundary where the classical LRT fails and develops the\ncorrected limiting distribution of the LRT for a general asymptotic regime. The\nsecond part of this work further studies the test power of the LRT in the\nhigh-dimensional setting. The result not only advances the current\nunderstanding of asymptotic behavior of the LRT under alternative hypothesis,\nbut also motivates the development of a power-enhanced LRT. The third part of\nthis work considers the setting with $p>n$, where the LRT is not well-defined.\nWe propose a two-step testing procedure by first performing dimension reduction\nand then applying the proposed LRT. Theoretical properties are developed to\nensure the validity of the proposed method. Numerical studies are also\npresented to demonstrate its good performance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 17:00:34 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 20:44:40 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["He", "Yinqiu", ""], ["Jiang", "Tiefeng", ""], ["Wen", "Jiyang", ""], ["Xu", "Gongjun", ""]]}, {"id": "1812.06948", "submitter": "Paulo Serra", "authors": "Paulo Serra, Tatyana Krivobokova, Francisco Rosales", "title": "Adaptive Non-parametric Estimation of Mean and Autocovariance in\n  Regression with Dependent Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a fully automatic non-parametric approach to simultaneous\nestimation of mean and autocovariance functions in regression with dependent\nerrors. Our empirical Bayesian approach is adaptive, numerically efficient and\nallows for the construction of confidence sets for the regression function.\nConsistency of the estimators is shown and small sample performance is\ndemonstrated in simulations and real data analysis. The method is implemented\nin the R package eBsc that accompanies the paper.\n", "versions": [{"version": "v1", "created": "Mon, 17 Dec 2018 18:39:07 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Serra", "Paulo", ""], ["Krivobokova", "Tatyana", ""], ["Rosales", "Francisco", ""]]}, {"id": "1812.07130", "submitter": "Shanshan Cao", "authors": "Shanshan Cao, Xiaoming Huo, Jong-Shi Pang", "title": "A Unifying Framework of High-Dimensional Sparse Estimation with\n  Difference-of-Convex (DC) Regularizations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the linear regression framework, we study the variable selection\nproblem when the underlying model is assumed to have a small number of nonzero\ncoefficients (i.e., the underlying linear model is sparse). Non-convex\npenalties in specific forms are well-studied in the literature for sparse\nestimation. A recent work \\cite{ahn2016difference} has pointed out that nearly\nall existing non-convex penalties can be represented as difference-of-convex\n(DC) functions, which can be expressed as the difference of two convex\nfunctions, while itself may not be convex. There is a large existing literature\non the optimization problems when their objectives and/or constraints involve\nDC functions. Efficient numerical solutions have been proposed. Under the DC\nframework, directional-stationary (d-stationary) solutions are considered, and\nthey are usually not unique. In this paper, we show that under some mild\nconditions, a certain subset of d-stationary solutions in an optimization\nproblem (with a DC objective) has some ideal statistical properties: namely,\nasymptotic estimation consistency, asymptotic model selection consistency,\nasymptotic efficiency. The aforementioned properties are the ones that have\nbeen proven by many researchers for a range of proposed non-convex penalties in\nthe sparse estimation. Our assumptions are either weaker than or comparable\nwith those conditions that have been adopted in other existing works. This work\nshows that DC is a nice framework to offer a unified approach to these existing\nwork where non-convex penalty is involved. Our work bridges the communities of\noptimization and statistics.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 01:39:37 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Cao", "Shanshan", ""], ["Huo", "Xiaoming", ""], ["Pang", "Jong-Shi", ""]]}, {"id": "1812.07290", "submitter": "Andriy Olenko", "authors": "Tareq Alodat, Nikolai Leonenko, Andriy Olenko", "title": "Limit theorems for filtered long-range dependent random fields", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates general scaling settings and limit distributions of\nfunctionals of filtered random fields. The filters are defined by the\nconvolution of non-random kernels with functions of Gaussian random fields. The\ncase of long-range dependent fields and increasing observation windows is\nstudied. The obtained limit random processes are non-Gaussian. Most known\nresults on this topic give asymptotic processes that always exhibit\nnon-negative auto-correlation structures and have the self-similar parameter\n$H\\in(\\frac{1}{2},1)$. In this work we also obtain convergence for the case\n$H\\in(0,\\frac{1}{2})$ and show how the Hurst parameter $H$ can depend on the\nshape of the observation windows. Various examples are presented.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 10:53:23 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Alodat", "Tareq", ""], ["Leonenko", "Nikolai", ""], ["Olenko", "Andriy", ""]]}, {"id": "1812.07479", "submitter": "Rida Benhaddou", "authors": "Rida Benhaddou and Qing Liu", "title": "Anisotropic functional deconvolution with long-memory noise: the case of\n  a multi-parameter fractional Wiener sheet", "comments": "31 pages, 2 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We look into the minimax results for the anisotropic two-dimensional\nfunctional deconvolution model with the two-parameter fractional Gaussian\nnoise. We derive the lower bounds for the $L^p$-risk, $1 \\leq p < \\infty$, and\ntaking advantage of the Riesz poly-potential, we apply a wavelet-vaguelette\nexpansion to de-correlate the anisotropic fractional Gaussian noise. We\nconstruct an adaptive wavelet hard-thresholding estimator that attains\nasymptotically quasi-optimal convergence rates in a wide range of Besov balls.\nSuch convergence rates depend on a delicate balance between the parameters of\nthe Besov balls, the degree of ill-posedness of the convolution operator and\nthe parameters of the fractional Gaussian noise. A limited simulations study\nconfirms theoretical claims of the paper. The proposed approach is extended to\nthe general $r$-dimensional case, with $r> 2$, and the corresponding\nconvergence rates do not suffer from the curse of dimensionality.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 16:56:06 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Benhaddou", "Rida", ""], ["Liu", "Qing", ""]]}, {"id": "1812.07485", "submitter": "Michail Tsagris", "authors": "Yannis Pantazis, Michail Tsagris and Andrew T.A. Wood", "title": "Gaussian asymptotic limits for the $\\alpha$-transformation in the\n  analysis of compositional data", "comments": "This is a preprint of the original publication that is available at\n  https://link.springer.com/article/10.1007/s13171-018-00160-1", "journal-ref": null, "doi": "10.1007/s13171-018-00160-1", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional data consists of vectors of proportions whose components sum to\n1. Such vectors lie in the standard simplex, which is a manifold with boundary.\nOne issue that has been rather controversial within the field of compositional\ndata analysis is the choice of metric on the simplex. One popular possibility\nhas been to use the metric implied by logtransforming the data, as proposed by\nAitchison [1, 2]; and another popular approach has been to use the standard\nEuclidean metric inherited from the ambient space. Tsagris et al. [21] proposed\na one-parameter family of power transformations, the $\\alpha$-transformations,\nwhich include both the metric implied by Aitchison's transformation and the\nEuclidean metric as particular cases. Our underlying philosophy is that, with\nmany datasets, it may make sense to use the data to help us determine a\nsuitable metric. A related possibility is to apply the $\\alpha$-transformations\nto a parametric family of distributions, and then estimate a along with the\nother parameters. However, as we shall see, when one follows this last approach\nwith the Dirichlet family, some care is needed in a certain limiting case which\narises $(\\alpha \\neq 0)$, as we found out when fitting this model to real and\nsimulated data. Specifically, when the maximum likelihood estimator of a is\nclose to 0, the other parameters tend to be large. The main purpose of the\npaper is to study this limiting case both theoretically and numerically and to\nprovide insight into these numerical findings.\n", "versions": [{"version": "v1", "created": "Thu, 29 Nov 2018 19:33:54 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2019 09:49:31 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Pantazis", "Yannis", ""], ["Tsagris", "Michail", ""], ["Wood", "Andrew T. A.", ""]]}, {"id": "1812.07496", "submitter": "Debasis Kundu Professor", "authors": "Swagata Nandi and Debasis Kundu", "title": "Estimating the fundamental frequency using modified Newton-Raphson\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a modified Newton-Raphson algorithm to estimate the\nfrequency parameter in the fundamental frequency model in presence of an\nadditive stationary error. The proposed estimator is super efficient in nature\nin the sense that its asymptotic variance is less than the asymptotic variance\nof the least squares estimator. With a proper step factor modification, the\nproposed modified Newton-Raphson algorithm produces an estimator with the rate\n$O_p(n^{-\\frac{3}{2}})$, the same rate as the least squares estimator.\nNumerical experiments are performed for different sample sizes, different error\nvariances and for different models. For illustrative purposes, two real data\nsets are analyzed using the fundamental frequency model and the estimators are\nobtained using the proposed algorithm. It is observed the model and the\nproposed algorithm work quite well in both cases.\n", "versions": [{"version": "v1", "created": "Thu, 22 Nov 2018 11:44:40 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Nandi", "Swagata", ""], ["Kundu", "Debasis", ""]]}, {"id": "1812.07497", "submitter": "Yusuke Kaino", "authors": "Yusuke Kaino, Shogo H. Nakakita, and Masayuki Uchida", "title": "Hybrid estimation for ergodic diffusion processes based on noisy\n  discrete observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider parametric estimation for ergodic diffusion processes with noisy\nsampled data based on the hybrid method, that is, the multi-step estimation\nwith the initial Bayes type estimators. In order to select proper initial\nvalues for optimisation of the quasi likelihood function of ergodic diffusion\nprocesses with noisy observations, we construct the initial Bayes type\nestimator based on the local means of the noisy observations. The asymptotic\nproperties of the initial Bayes type estimators and the hybrid multi-step\nestimators with the initial Bayes type estimators are shown, and a concrete\nexample and the simulation results are given.\n", "versions": [{"version": "v1", "created": "Tue, 27 Nov 2018 03:06:54 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Kaino", "Yusuke", ""], ["Nakakita", "Shogo H.", ""], ["Uchida", "Masayuki", ""]]}, {"id": "1812.07619", "submitter": "Shaojun Guo", "authors": "Shaojun Guo and Xinghao Qiao", "title": "A General Theory for Large-Scale Curve Time Series via Functional\n  Stability Measure", "comments": "53 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling a large bundle of curves arises in a broad spectrum of real\napplications. However, existing literature relies primarily on the critical\nassumption of independent curve observations. In this paper, we provide a\ngeneral theory for large-scale Gaussian curve time series, where the temporal\nand cross-sectional dependence across multiple curve observations exist and the\nnumber of functional variables, $p,$ may be large relative to the number of\nobservations, $n.$ We propose a novel functional stability measure for\nmultivariate stationary processes based on their spectral properties and use it\nto establish some useful concentration bounds on the sample covariance matrix\nfunction. These concentration bounds serve as a fundamental tool for further\ntheoretical analysis, in particular, for deriving nonasymptotic upper bounds on\nthe errors of the regularized estimates in high dimensional settings. As {\\it\nfunctional principle component analysis} (FPCA) is one of the key techniques to\nhandle functional data, we also investigate the concentration properties of the\nrelevant estimated terms under a FPCA framework. To illustrate with an\nimportant application, we consider {\\it vector functional autoregressive\nmodels} and develop a regularization approach to estimate autoregressive\ncoefficient functions under the sparsity constraint. Using our derived\nnonasymptotic results, we investigate the theoretical properties of the\nregularized estimate in a \"large $p,$ small $n$\" regime. The finite sample\nperformance of the proposed method is examined through simulation studies.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 19:37:05 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 05:57:22 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Guo", "Shaojun", ""], ["Qiao", "Xinghao", ""]]}, {"id": "1812.07685", "submitter": "Peter Forrester", "authors": "P.J. Forrester and Jiyuan Zhang", "title": "Parametrising correlation matrices", "comments": "17 pages", "journal-ref": "Journal of Multivariate Analysis, vol. 178, 2020, p. 104619", "doi": null, "report-no": null, "categories": "math.ST math-ph math.MP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correlation matrices are the sub-class of positive definite real matrices\nwith all entries on the diagonal equal to unity. Earlier work has exhibited a\nparametrisation of the corresponding Cholesky factorisation in terms of partial\ncorrelations, and also in terms of hyperspherical co-ordinates. We show how the\ntwo are relating, starting from the definition of the partial correlations in\nterms of the Schur complement. We extend this to the generalisation of\ncorrelation matrices to the cases of complex and quaternion entries. As in the\nreal case, we show how the hyperspherical parametrisation leads naturally to a\ndistribution on the space of correlation matrices $\\{R\\}$ with probability\ndensity function proportional to $( \\det R)^a$. For certain $a$, a construction\nof random correlation matrices realising this distribution is given in terms of\nrectangular standard Gaussian matrices.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 23:05:56 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Forrester", "P. J.", ""], ["Zhang", "Jiyuan", ""]]}, {"id": "1812.07700", "submitter": "Luis Nieto-Barajas Dr.", "authors": "Ricardo Hoyos and Luis Nieto-Barajas", "title": "A Bayesian semiparametric Archimedean copula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Archimedean copula is characterised by its generator. This is a real\nfunction whose inverse behaves as a survival function. We propose a\nsemiparametric generator based on a quadratic spline. This is achieved by\nmodelling the first derivative of a hazard rate function, in a survival\nanalysis context, as a piecewise constant function. Convexity of our\nsemiparametric generator is obtained by imposing some simple constraints. The\ninduced semiparametric Archimedean copula produces Kendall's tau association\nmeasure that covers the whole range $(-1,1)$. Inference on the model is done\nunder a Bayesian approach and for some prior specifications we are able to\nperform an independence test. Properties of the model are illustrated with a\nsimulation study as well as with a real dataset.\n", "versions": [{"version": "v1", "created": "Tue, 18 Dec 2018 23:52:59 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 16:00:20 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Hoyos", "Ricardo", ""], ["Nieto-Barajas", "Luis", ""]]}, {"id": "1812.07706", "submitter": "Jun Yang", "authors": "Jun Yang and Zhou Zhou", "title": "Spectral Inference under Complex Temporal Dynamics", "comments": "To appear in Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST eess.SP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop unified theory and methodology for the inference of evolutionary\nFourier power spectra for a general class of locally stationary and possibly\nnonlinear processes. In particular, simultaneous confidence regions (SCR) with\nasymptotically correct coverage rates are constructed for the evolutionary\nspectral densities on a nearly optimally dense grid of the joint time-frequency\ndomain. A simulation based bootstrap method is proposed to implement the SCR.\nThe SCR enables researchers and practitioners to visually evaluate the\nmagnitude and pattern of the evolutionary power spectra with asymptotically\naccurate statistical guarantee. The SCR also serves as a unified tool for a\nwide range of statistical inference problems in time-frequency analysis ranging\nfrom tests for white noise, stationarity and time-frequency separability to the\nvalidation for non-stationary linear models.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 00:28:19 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2018 06:14:00 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 01:29:41 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Yang", "Jun", ""], ["Zhou", "Zhou", ""]]}, {"id": "1812.07730", "submitter": "Budhi Arta Surya", "authors": "H. Frydman and B.A. Surya", "title": "The Mixture of Markov Jump Processes: Monte Carlo Method and the EM\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses tractable development and statistical estimation of a\ncontinuous time stochastic process with a finite state space having non-Markov\nproperty. The process is formed by a finite mixture of right-continuous Markov\njump processes moving at different speeds on the same finite state space,\nwhereas the speed regimes are assumed to be unobservable. The mixture was first\nproposed by Frydman (J. Am. Stat. Assoc., 100, 1046-1053, 2005) in 2005 and\nrecently generalized in Surya (Stoch. Syst. 8, 29-44, 2018), in which\ndistributional properties and explicit identities of the process are given in\nits full generality. The contribution of this paper is two fold. First, we\npresent Monte Carlo method for constructing the process and show distributional\nequivalence between the simulated process and the actual process. Secondly, we\nperform statistical inference on the distribution parameters of the process.\nUnder complete observation of the sample paths, maximum likelihood estimates\nare given in explicit form in terms of sufficient statistics of the process.\nEstimation under incomplete observation is performed using the EM algorithm.\nThe estimation results completely characterize the process in terms of the\ninitial probability of starting the process in any phase of the state space,\nintensity matrices of the underlying Markov jump processes, and the switching\nprobability matrix of the process. Some numerical examples are given to test\nthe performance of the developed method. The proposed estimation generalizes\nthe existing statistical inferences for the Markov model by Albert (Ann. Math.\nStatist., 38, p.727-753., 1961), the mover-stayer model by Frydman (J. Am.\nStat. Assoc.79, 632-638., 1984) and the Markov mixture model by Frydman (J. Am.\nStat. Assoc., 100, 1046-1053, 2005).\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 02:09:28 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 01:43:10 GMT"}], "update_date": "2019-02-04", "authors_parsed": [["Frydman", "H.", ""], ["Surya", "B. A.", ""]]}, {"id": "1812.07813", "submitter": "Xiaojun Mao", "authors": "Xiaojun Mao, Raymond K. W. Wong and Song Xi Chen", "title": "Matrix Completion under Low-Rank Missing Mechanism", "comments": "29 pages, 0 figures", "journal-ref": null, "doi": "10.5705/ss.202019.0196", "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion is a modern missing data problem where both the missing\nstructure and the underlying parameter are high dimensional. Although missing\nstructure is a key component to any missing data problems, existing matrix\ncompletion methods often assume a simple uniform missing mechanism. In this\nwork, we study matrix completion from corrupted data under a novel low-rank\nmissing mechanism. The probability matrix of observation is estimated via a\nhigh dimensional low-rank matrix estimation procedure, and further used to\ncomplete the target matrix via inverse probabilities weighting. Due to both\nhigh dimensional and extreme (i.e., very small) nature of the true probability\nmatrix, the effect of inverse probability weighting requires careful study. We\nderive optimal asymptotic convergence rates of the proposed estimators for both\nthe observation probabilities and the target matrix.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 08:46:50 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 02:56:27 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Mao", "Xiaojun", ""], ["Wong", "Raymond K. W.", ""], ["Chen", "Song Xi", ""]]}, {"id": "1812.07944", "submitter": "James Duffy", "authors": "James A. Duffy and Ioannis Kasparis", "title": "Estimation and Inference in the Presence of Fractional d=1/2 and Weakly\n  Nonstationary Processes", "comments": "Authors' accepted manuscript; to appear in the Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide new limit theory for functionals of a general class of processes\nlying at the boundary between stationarity and nonstationarity -- what we term\nweakly nonstationary processes (WNPs). This includes, as leading examples,\nfractional processes with d=1/2, and arrays of autoregressive processes with\nroots drifting slowly towards unity. We first apply the theory to study\ninference in parametric and nonparametric regression models involving WNPs as\ncovariates. We then use these results to develop a new specification test for\nparametric regression models. By construction, our specification test statistic\nhas a chi-squared limiting distribution regardless of the form and extent of\npersistence of the regressor, implying that a practitioner can validly perform\nthe test using a fixed critical value, while remaining agnostic about the\nmechanism generating the regressor. Simulation exercises confirm that the test\ncontrols size across a wide range of data generating processes, and outperforms\na comparable test due to Wang and Phillips (2012, Ann. Stat.) against many\nalternatives.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 13:50:47 GMT"}, {"version": "v2", "created": "Thu, 20 Feb 2020 16:39:44 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 23:45:01 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Duffy", "James A.", ""], ["Kasparis", "Ioannis", ""]]}, {"id": "1812.07955", "submitter": "Benedikt M. P\u00c3\u00b6tscher", "authors": "Hannes Leeb, Benedikt M. P\\\"otscher, and Danijel Kivaranovic", "title": "Discussion on \"Model Confidence Bounds for Variable Selection\" by Yang\n  Li, Yuetian Luo, Davide Ferrari, Xiaonan Hu, and Yichen Qin", "comments": null, "journal-ref": "Biometrics 75 (2019), 407-410", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a comment on the article mentioned in the title.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 14:10:37 GMT"}, {"version": "v2", "created": "Thu, 20 Dec 2018 15:32:57 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Leeb", "Hannes", ""], ["P\u00f6tscher", "Benedikt M.", ""], ["Kivaranovic", "Danijel", ""]]}, {"id": "1812.08037", "submitter": "Christof Sch\\\"otz", "authors": "Christof Sch\\\"otz", "title": "Convergence Rates for the Generalized Fr\\'echet Mean via the Quadruple\n  Inequality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For sets $\\mathcal Q$ and $\\mathcal Y$, the generalized Fr\\'echet mean $m \\in\n\\mathcal Q$ of a random variable $Y$, which has values in $\\mathcal Y$, is any\nminimizer of $q\\mapsto \\mathbb E[\\mathfrak c(q,Y)]$, where $\\mathfrak c \\colon\n\\mathcal Q \\times \\mathcal Y \\to \\mathbb R$ is a cost function. There are\nlittle restrictions to $\\mathcal Q$ and $\\mathcal Y$. In particular, $\\mathcal\nQ$ can be a non-Euclidean metric space. We provide convergence rates for the\nempirical generalized Fr\\'echet mean. Conditions for rates in probability and\nrates in expectation are given. In contrast to previous results on Fr\\'echet\nmeans, we do not require a finite diameter of the $\\mathcal Q$ or $\\mathcal Y$.\nInstead, we assume an inequality, which we call quadruple inequality. It\ngeneralizes an otherwise common Lipschitz condition on the cost function. This\nquadruple inequality is known to hold in Hadamard spaces. We show that it also\nholds in a suitable way for certain powers of a Hadamard-metric.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 15:53:37 GMT"}, {"version": "v2", "created": "Mon, 1 Apr 2019 16:11:46 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 14:38:18 GMT"}, {"version": "v4", "created": "Thu, 19 Sep 2019 09:27:40 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Sch\u00f6tz", "Christof", ""]]}, {"id": "1812.08078", "submitter": "Mohamed Ndaoud", "authors": "Mohamed Ndaoud", "title": "Sharp optimal recovery in the two-component Gaussian Mixture Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of clustering in the two-component Gaussian\nmixture model where the centers are separated by $2\\Delta$ for some $\\Delta>0$.\nWe characterize the exact phase transition threshold, given by $$\n\\bar{\\Delta}_n^{2} = \\sigma^{2}\\left(1 + \\sqrt{1+\\frac{2p}{n\\log{n}}}\n\\right)\\log{n}, $$ such that perfect recovery of the communities is possible\nwith high probability if $\\Delta\\ge(1+\\varepsilon)\\bar \\Delta_n$, and\nimpossible if $\\Delta\\le (1-\\varepsilon)\\bar \\Delta_n$ for any constant\n$\\varepsilon>0$. This implies an elbow effect at a critical dimension\n$p^{*}=n\\log{n}$.\n  We present a non-asymptotic lower bound for the corresponding minimax Hamming\nrisk improving on existing results. It is, to our knowledge, the first lower\nbound capturing the right dependence on $p$. We also propose an optimal,\nefficient and adaptive procedure that is minimax rate optimal. The rate\noptimality is moreover sharp in the asymptotics when the sample size goes to\ninfinity. Our procedure is based on a variant of Lloyd's iterations initialized\nby a spectral method; a popular clustering algorithm widely used by\npractitioners. Numerical studies confirm our theoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 16:53:12 GMT"}, {"version": "v2", "created": "Fri, 8 Feb 2019 08:58:41 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 20:24:38 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Ndaoud", "Mohamed", ""]]}, {"id": "1812.08089", "submitter": "Yuan Liao", "authors": "Victor Chernozhukov, Christian Hansen, Yuan Liao, Yinchu Zhu", "title": "Inference for Heterogeneous Effects using Low-Rank Estimation of Factor\n  Slopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a panel data model with general heterogeneous effects where slopes\nare allowed to vary across both individuals and over time. The key dimension\nreduction assumption we employ is that the heterogeneous slopes can be\nexpressed as having a factor structure so that the high-dimensional slope\nmatrix is low-rank and can thus be estimated using low-rank regularized\nregression. We provide a simple multi-step estimation procedure for the\nheterogeneous effects. The procedure makes use of sample-splitting and\northogonalization to accommodate inference following the use of penalized\nlow-rank estimation. We formally verify that the resulting estimator is\nasymptotically normal allowing simple construction of inferential statements\nfor {the individual-time-specific effects and for cross-sectional averages of\nthese effects}. We illustrate the proposed method in simulation experiments and\nby estimating the effect of the minimum wage on employment.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 17:05:26 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 05:35:51 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 19:17:42 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2019 17:55:36 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Hansen", "Christian", ""], ["Liao", "Yuan", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1812.08217", "submitter": "Jinyuan Chang", "authors": "Jinyuan Chang, Qiao Hu, Cheng Liu, Cheng Yong Tang", "title": "Optimal covariance matrix estimation for high-dimensional noise in\n  high-frequency data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider high-dimensional measurement errors with high-frequency data. Our\nfocus is on recovering the covariance matrix of the random errors with\noptimality. In this problem, not all components of the random vector are\nobserved at the same time and the measurement errors are latent variables,\nleading to major challenges besides high data dimensionality. We propose a new\ncovariance matrix estimator in this context with appropriate localization and\nthresholding. By developing a new technical device integrating the\nhigh-frequency data feature with the conventional notion of $\\alpha$-mixing,\nour analysis successfully accommodates the challenging serial dependence in the\nmeasurement errors. Our theoretical analysis establishes the minimax optimal\nconvergence rates associated with two commonly used loss functions. We then\nestablish cases when the proposed localized estimator with thresholding\nachieves the minimax optimal convergence rates. Considering that the variances\nand covariances can be small in reality, we conduct a second-order theoretical\nanalysis that further disentangles the dominating bias in the estimator. A\nbias-corrected estimator is then proposed to ensure its practical finite sample\nperformance. We illustrate the promising empirical performance of the proposed\nestimator with extensive simulation studies and a real data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Dec 2018 19:49:31 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 08:08:23 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chang", "Jinyuan", ""], ["Hu", "Qiao", ""], ["Liu", "Cheng", ""], ["Tang", "Cheng Yong", ""]]}, {"id": "1812.08292", "submitter": "Daniil Ryabko", "authors": "Daniil Ryabko", "title": "Finite-time optimality of Bayesian predictors", "comments": null, "journal-ref": "On Asymptotic and Finite-Time Optimality of Bayesian Predictors,\n  Journal of Machine Learning Research (149):1-24, 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of sequential probability forecasting is considered in the most\ngeneral setting: a model set C is given, and it is required to predict as well\nas possible if any of the measures (environments) in C is chosen to generate\nthe data. No assumptions whatsoever are made on the model class C, in\nparticular, no independence or mixing assumptions; C may not be measurable;\nthere may be no predictor whose loss is sublinear, etc. It is shown that the\ncumulative loss of any possible predictor can be matched by that of a Bayesian\npredictor whose prior is discrete and is concentrated on C, up to an additive\nterm of order $\\log n$, where $n$ is the time step. The bound holds for every\n$n$ and every measure in C. This is the first non-asymptotic result of this\nkind. In addition, a non-matching lower bound is established: it goes to\ninfinity with $n$ but may do so arbitrarily slow.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 00:18:28 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 07:07:21 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Ryabko", "Daniil", ""]]}, {"id": "1812.08343", "submitter": "Hamzeh Torabi", "authors": "Hossein Nadeb, Hamzeh Torabi, Ali Dolati", "title": "Stochastic comparisons of the largest claim amounts from two sets of\n  interdependent heterogeneous portfolios", "comments": "arXiv admin note: text overlap with arXiv:1812.06078 and\n  arXiv:1812.06166", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $ X_{\\lambda_1},\\ldots,X_{\\lambda_n}$ be dependent non-negative random\nvariables and $Y_i=I_{p_i} X_{\\lambda_i}$, $i=1,\\ldots,n$, where\n$I_{p_1},\\ldots,I_{p_n}$ are independent Bernoulli random variables independent\nof $X_{\\lambda_i}$'s, with ${\\rm E}[I_{p_i}]=p_i$, $i=1,\\ldots,n$. In actuarial\nsciences, $Y_i$ corresponds to the claim amount in a portfolio of risks. In\nthis paper, we compare the largest claim amounts of two sets of interdependent\nportfolios, in the sense of usual stochastic order, when the variables in one\nset have the parameters $\\lambda_1,\\ldots,\\lambda_n$ and $p_1,\\ldots,p_n$ and\nthe variables in the other set have the parameters\n$\\lambda^{*}_1,\\ldots,\\lambda^{*}_n$ and $p^*_1,\\ldots,p^*_n$. For\nillustration, we apply the results to some important models in actuary.\n", "versions": [{"version": "v1", "created": "Fri, 14 Dec 2018 19:31:38 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Nadeb", "Hossein", ""], ["Torabi", "Hamzeh", ""], ["Dolati", "Ali", ""]]}, {"id": "1812.08357", "submitter": "Sebastian Roch", "authors": "Sebastien Roch", "title": "On the variance of internode distance under the multispecies coalescent", "comments": null, "journal-ref": "RECOMB-CG 2018", "doi": null, "report-no": null, "categories": "q-bio.PE cs.CE math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating species trees from unrooted gene tree\ntopologies in the presence of incomplete lineage sorting, a common phenomenon\nthat creates gene tree heterogeneity in multilocus datasets. One popular class\nof reconstruction methods in this setting is based on internode distances, i.e.\nthe average graph distance between pairs of species across gene trees. While\nstatistical consistency in the limit of large numbers of loci has been\nestablished in some cases, little is known about the sample complexity of such\nmethods. Here we make progress on this question by deriving a lower bound on\nthe worst-case variance of internode distance which depends linearly on the\ncorresponding graph distance in the species tree. We also discuss some\nalgorithmic implications.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 04:48:19 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Roch", "Sebastien", ""]]}, {"id": "1812.08409", "submitter": "Paolo Paruolo", "authors": "Karim M. Abadir, Alessandra Luati, Paolo Paruolo", "title": "GARCH density and functional forecasts", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives the analytic form of the $h$-step ahead prediction density\nof a GARCH(1,1) process under Gaussian innovations, with a possibly asymmetric\nnews impact curve. The contributions of the paper consists both in the\nderivation of the analytic form of the density, and in its application to a\nnumber of econometric problems. A first application of the explicit formulae is\nto characterize the degree of non-Gaussianity of the prediction distribution;\nfor some values encountered in applications, deviations of the prediction\ndistribution from the Gaussian are found to be small, and sometimes not. the\nGaussian density as an approximation of the true prediction density. A second\napplication of the formulae is to compute exact tail probabilities and\nfunctionals, such as the Value at Risk and the Expected Shortfall, that measure\nrisk when the underlying asset return is generated by a Gaussian GARCH(1,1).\nThis improves on existing methods based on Monte Carlo simulations and\n(non-parametric) estimation techniques, because the present exact formulae are\nfree of Monte Carlo estimation uncertainty. A third application is the\ndefinition of uncertainty regions for functionals of the prediction\ndistribution that reflect in-sample estimation uncertainty. These applications\nare illustrated on selected empirical examples.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 08:22:08 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 20:33:34 GMT"}, {"version": "v3", "created": "Thu, 4 Mar 2021 10:07:27 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Abadir", "Karim M.", ""], ["Luati", "Alessandra", ""], ["Paruolo", "Paolo", ""]]}, {"id": "1812.08509", "submitter": "Toni Karvonen", "authors": "Toni Karvonen, Motonobu Kanagawa, Simo S\\\"arkk\\\"a", "title": "On the positivity and magnitudes of Bayesian quadrature weights", "comments": "Accepted for publication in Statistics and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reviews and studies the properties of Bayesian quadrature\nweights, which strongly affect stability and robustness of the quadrature rule.\nSpecifically, we investigate conditions that are needed to guarantee that the\nweights are positive or to bound their magnitudes. First, it is shown that the\nweights are positive in the univariate case if the design points locally\nminimise the posterior integral variance and the covariance kernel is totally\npositive (e.g., Gaussian and Hardy kernels). This suggests that gradient-based\noptimisation of design points may be effective in constructing stable and\nrobust Bayesian quadrature rules. Secondly, we show that magnitudes of the\nweights admit an upper bound in terms of the fill distance and separation\nradius if the RKHS of the kernel is a Sobolev space (e.g., Mat\\'ern kernels),\nsuggesting that quasi-uniform points should be used. A number of numerical\nexamples demonstrate that significant generalisations and improvements appear\nto be possible, manifesting the need for further research.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 12:18:24 GMT"}, {"version": "v2", "created": "Fri, 2 Aug 2019 13:01:16 GMT"}], "update_date": "2019-08-05", "authors_parsed": [["Karvonen", "Toni", ""], ["Kanagawa", "Motonobu", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1812.08520", "submitter": "Serge Iovleff", "authors": "Serge Iovleff (MODAL,LPP), Seydou Syllla, Cheikh Loucoubar", "title": "Block clustering of Binary Data with Gaussian Co-variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simultaneous grouping of rows and columns is an important technique that\nis increasingly used in large-scale data analysis. In this paper, we present a\nnovel co-clustering method using co-variables in its construction. It is based\non a latent block model taking into account the problem of grouping variables\nand clustering individuals by integrating information given by sets of\nco-variables. Numerical experiments on simulated data sets and an application\non real genetic data highlight the interest of this approach.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 12:32:40 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Iovleff", "Serge", "", "MODAL,LPP"], ["Syllla", "Seydou", ""], ["Loucoubar", "Cheikh", ""]]}, {"id": "1812.08638", "submitter": "Bruno Ebner", "authors": "Bruno Ebner and Franz Nestmann and Matthias Schulte", "title": "Testing multivariate uniformity based on random geometric graphs", "comments": "36 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new families of goodness-of-fit tests of uniformity on a\nfull-dimensional set $W\\subset\\R^d$ based on statistics related to edge lengths\nof random geometric graphs. Asymptotic normality of these statistics is proven\nunder the null hypothesis as well as under fixed alternatives. The derived\ntests are consistent and their behaviour for some contiguous alternatives can\nbe controlled. A simulation study suggests that the procedures can compete with\nor are better than established goodness-of-fit tests. We show with a real data\nexample that the new tests can detect non-uniformity of a small sample data\nset, where most of the competitors fail.\n", "versions": [{"version": "v1", "created": "Thu, 20 Dec 2018 15:38:05 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 14:04:55 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ebner", "Bruno", ""], ["Nestmann", "Franz", ""], ["Schulte", "Matthias", ""]]}, {"id": "1812.08924", "submitter": "Ilmun Kim", "authors": "Ilmun Kim", "title": "Multinomial Goodness-of-Fit Based on U-Statistics: High-Dimensional\n  Asymptotic and Minimax Optimality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider multinomial goodness-of-fit tests in the high-dimensional regime\nwhere the number of bins increases with the sample size. In this regime,\nPearson's chi-squared test can suffer from low power due to the substantial\nbias as well as high variance of its statistic. To resolve these issues, we\nintroduce a family of U-statistic for multinomial goodness-of-fit and study\ntheir asymptotic behaviors in high-dimensions. Specifically, we establish\nconditions under which the considered U-statistic is asymptotically Poisson or\nGaussian, and investigate its power function under each asymptotic regime.\nFurthermore, we introduce a class of weights for the U-statistic that results\nin minimax rate optimal tests.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 03:17:55 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Kim", "Ilmun", ""]]}, {"id": "1812.08944", "submitter": "Hang Deng", "authors": "Hang Deng and Cun-Hui Zhang", "title": "Isotonic Regression in Multi-Dimensional Spaces and Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study minimax and adaptation rates in general isotonic\nregression. For uniform deterministic and random designs in $[0,1]^d$ with\n$d\\ge 2$ and $N(0,1)$ noise, the minimax rate for the $\\ell_2$ risk is known to\nbe bounded from below by $n^{-1/d}$ when the unknown mean function $f$ is\nnondecreasing and its range is bounded by a constant, while the least squares\nestimator (LSE) is known to nearly achieve the minimax rate up to a factor\n$(\\log n)^\\gamma$ where $n$ is sample size, $\\gamma = 4$ in the lattice design\nand $\\gamma = \\max\\{9/2, (d^2+d+1)/2 \\}$ in the random design. Moreover, the\nLSE is known to achieve the adaptation rate $(K/n)^{-2/d}\\{1\\vee\n\\log(n/K)\\}^{2\\gamma}$ when $f$ is piecewise constant on $K$ hyperrectangles in\na partition of $[0,1]^d$.\n  Due to the minimax theorem, the LSE is identical on every design point to\nboth the max-min and min-max estimators over all upper and lower sets\ncontaining the design point. This motivates our consideration of estimators\nwhich lie in-between the max-min and min-max estimators over possibly smaller\nclasses of upper and lower sets, including a subclass of block estimators.\nUnder a $q$-th moment condition on the noise, we develop $\\ell_q$ risk bounds\nfor such general estimators for isotonic regression on graphs. For uniform\ndeterministic and random designs in $[0,1]^d$ with $d\\ge 3$, our $\\ell_2$ risk\nbound for the block estimator matches the minimax rate $n^{-1/d}$ when the\nrange of $f$ is bounded and achieves the near parametric adaptation rate\n$(K/n)\\{1\\vee\\log(n/K)\\}^{d}$ when $f$ is $K$-piecewise constant. Furthermore,\nthe block estimator possesses the following oracle property in variable\nselection: When $f$ depends on only a subset $S$ of variables, the $\\ell_2$\nrisk of the block estimator automatically achieves up to a poly-logarithmic\nfactor the minimax rate based on the oracular knowledge of $S$.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 04:58:14 GMT"}, {"version": "v2", "created": "Fri, 10 Jan 2020 03:39:18 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Deng", "Hang", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "1812.08965", "submitter": "Weijie J. Su", "authors": "Weijie J. Su", "title": "The FDR-Linking Theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the \\texttt{FDR-linking} theorem, a novel technique for\nunderstanding \\textit{non-asymptotic} FDR control of the Benjamini--Hochberg\n(BH) procedure under arbitrary dependence of the $p$-values. This theorem\noffers a principled and flexible approach to linking all $p$-values and the\nnull $p$-values from the FDR control perspective, suggesting a profound\nimplication that, to a large extent, the FDR of the BH procedure relies mostly\non the null $p$-values. To illustrate the use of this theorem, we propose a new\ntype of dependence only concerning the null $p$-values, which, while strictly\n\\textit{relaxing} the state-of-the-art PRDS dependence (Benjamini and\nYekutieli, 2001), ensures the FDR of the BH procedure below a level that is\nindependent of the number of hypotheses. This level is, furthermore, shown to\nbe optimal under this new dependence structure. Next, we present a concept\nreferred to as \\textit{FDR consistency} that is weaker but more amenable than\nFDR control, and the \\texttt{FDR-linking} theorem shows that FDR consistency is\ncompletely determined by the joint distribution of the null $p$-values, thereby\nreducing the analysis of this new concept to the global null case. Finally,\nthis theorem is used to obtain a sharp FDR bound under arbitrary dependence,\nwhich improves the $\\log$-correction FDR bound (Benjamini and Yekutieli, 2001)\nin certain regimes.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 06:15:44 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Su", "Weijie J.", ""]]}, {"id": "1812.08986", "submitter": "Heidi S{\\o}gaard Christensen", "authors": "Jesper M{\\o}ller, Heidi S. Christensen, Francisco Cuevas-Pacheco,\n  Andreas D. Christoffersen", "title": "Structured space-sphere point processes and $K$-functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns space-sphere point processes, that is, point processes on\nthe product space of $\\mathbb R^d$ (the $d$-dimensional Euclidean space) and\n$\\mathbb S^k$ (the $k$-dimen\\-sional sphere). We consider specific classes of\nmodels for space-sphere point processes, which are adaptations of existing\nmodels for either spherical or spatial point processes. For model checking or\nfitting, we present the space-sphere $K$-function which is a natural extension\nof the inhomogeneous $K$-function for point processes on $\\mathbb R^d$ to the\ncase of space-sphere point processes. Under the assumption that the intensity\nand pair correlation function both have a certain separable structure, the\nspace-sphere $K$-function is shown to be proportional to the product of the\ninhomogeneous spatial and spherical $K$-functions. For the presented\nspace-sphere point process models, we discuss cases where such a separable\nstructure can be obtained. The usefulness of the space-sphere $K$-function is\nillustrated for real and simulated datasets with varying dimensions $d$ and\n$k$.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 07:56:04 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 11:01:42 GMT"}, {"version": "v3", "created": "Wed, 20 Mar 2019 13:47:10 GMT"}, {"version": "v4", "created": "Tue, 21 May 2019 07:44:23 GMT"}, {"version": "v5", "created": "Tue, 29 Oct 2019 18:18:38 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["M\u00f8ller", "Jesper", ""], ["Christensen", "Heidi S.", ""], ["Cuevas-Pacheco", "Francisco", ""], ["Christoffersen", "Andreas D.", ""]]}, {"id": "1812.09063", "submitter": "Thorsten Dickhaus", "authors": "Jonathan von Schroeder and Thorsten Dickhaus", "title": "Efficient Calculation of the Joint Distribution of Order Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of computing the joint distribution of order\nstatistics of stochastically independent random variables in one- and two-group\nmodels. While recursive formulas for evaluating the joint cumulative\ndistribution function of such order statistics exist in the literature for a\nlonger time, their numerical implementation remains a challenging task. We\ntackle this task by presenting novel generalizations of known recursions which\nwe utilize to obtain exact results (calculated in rational arithmetic) as well\nas faithfully rounded results. Finally, some applications in stepwise multiple\nhypothesis testing are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 11:43:07 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["von Schroeder", "Jonathan", ""], ["Dickhaus", "Thorsten", ""]]}, {"id": "1812.09066", "submitter": "Stefano Sarao Mannelli", "authors": "Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent\n  Krzakala, Pierfrancesco Urbani, Lenka Zdeborov\\'a", "title": "Marvels and Pitfalls of the Langevin Algorithm in Noisy High-dimensional\n  Inference", "comments": "11 pages and 5 figures + appendix", "journal-ref": "Phys. Rev. X 10, 011057 (2020)", "doi": "10.1103/PhysRevX.10.011057", "report-no": null, "categories": "cs.LG cond-mat.dis-nn math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-descent-based algorithms and their stochastic versions have\nwidespread applications in machine learning and statistical inference. In this\nwork we perform an analytic study of the performances of one of them, the\nLangevin algorithm, in the context of noisy high-dimensional inference. We\nemploy the Langevin algorithm to sample the posterior probability measure for\nthe spiked matrix-tensor model. The typical behaviour of this algorithm is\ndescribed by a system of integro-differential equations that we call the\nLangevin state evolution, whose solution is compared with the one of the state\nevolution of approximate message passing (AMP). Our results show that,\nremarkably, the algorithmic threshold of the Langevin algorithm is sub-optimal\nwith respect to the one given by AMP. We conjecture this phenomenon to be due\nto the residual glassiness present in that region of parameters. Finally we\nshow how a landscape-annealing protocol, that uses the Langevin algorithm but\nviolate the Bayes-optimality condition, can approach the performance of AMP.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 11:56:50 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 15:40:21 GMT"}, {"version": "v3", "created": "Mon, 1 Jul 2019 07:39:43 GMT"}, {"version": "v4", "created": "Mon, 13 Jan 2020 11:03:07 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Mannelli", "Stefano Sarao", ""], ["Biroli", "Giulio", ""], ["Cammarota", "Chiara", ""], ["Krzakala", "Florent", ""], ["Urbani", "Pierfrancesco", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1812.09071", "submitter": "Heidi S{\\o}gaard Christensen", "authors": "Jakob G. Rasmussen, Heidi S. Christensen", "title": "Point processes on directed linear network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider point processes specified on directed linear\nnetworks, i.e. linear networks with associated directions. We adapt the\nso-called conditional intensity function used for specifying point processes on\nthe time line to the setting of directed linear networks. For models specified\nby such a conditional intensity function, we derive an explicit expression for\nthe likelihood function, specify two simulation algorithms (the inverse method\nand Ogata's modified thinning algorithm), and consider methods for model\nchecking through the use of residuals. We also extend the results and methods\nto the case of a marked point process on a directed linear network.\nFurthermore, we consider specific classes of point process models on directed\nlinear networks (Poisson processes, Hawkes processes, non-linear Hawkes\nprocesses, self-correcting processes, and marked Hawkes processes), all adapted\nfrom well-known models in the temporal setting. Finally, we apply the results\nand methods to analyse simulated and neurological data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 12:03:26 GMT"}, {"version": "v2", "created": "Wed, 2 Jan 2019 10:08:14 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Rasmussen", "Jakob G.", ""], ["Christensen", "Heidi S.", ""]]}, {"id": "1812.09150", "submitter": "Jeremie Bigot", "authors": "Bernard Bercu and J\\'er\\'emie Bigot", "title": "Asymptotic distribution and convergence rates of stochastic algorithms\n  for entropic optimal transportation between probability measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is devoted to the stochastic approximation of entropically\nregularized Wasserstein distances between two probability measures, also known\nas Sinkhorn divergences. The semi-dual formulation of such regularized optimal\ntransportation problems can be rewritten as a non-strongly concave optimisation\nproblem. It allows to implement a Robbins-Monro stochastic algorithm to\nestimate the Sinkhorn divergence using a sequence of data sampled from one of\nthe two distributions. Our main contribution is to establish the almost sure\nconvergence and the asymptotic normality of a new recursive estimator of the\nSinkhorn divergence between two probability measures in the discrete and\nsemi-discrete settings. We also study the rate of convergence of the expected\nexcess risk of this estimator in the absence of strong concavity of the\nobjective function. Numerical experiments on synthetic and real datasets are\nalso provided to illustrate the usefulness of our approach for data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 14:34:12 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 22:22:44 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2019 13:43:35 GMT"}, {"version": "v4", "created": "Tue, 12 May 2020 09:17:36 GMT"}, {"version": "v5", "created": "Wed, 10 Jun 2020 12:02:53 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bercu", "Bernard", ""], ["Bigot", "J\u00e9r\u00e9mie", ""]]}, {"id": "1812.09168", "submitter": "Baptiste Broto", "authors": "Baptiste Broto (CEA), Fran\\c{c}ois Bachoc (IMT), Marine Depecker (CEA)", "title": "Variance reduction for estimation of Shapley effects and adaptation to\n  unknown input distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Shapley effects are global sensitivity indices: they quantify the impact\nof each input variable on the output variable in a model. In this work, we\nsuggest new estimators of these sensitivity indices. When the input\ndistribution is known, we investigate the already existing estimator and\nsuggest a new one with a lower variance. Then, when the distribution of the\ninputs is unknown, we extend these estimators. Finally, we provide asymptotic\nproperties of the estimators studied in this article.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 14:56:43 GMT"}, {"version": "v2", "created": "Thu, 13 Feb 2020 09:30:12 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Broto", "Baptiste", "", "CEA"], ["Bachoc", "Fran\u00e7ois", "", "IMT"], ["Depecker", "Marine", "", "CEA"]]}, {"id": "1812.09187", "submitter": "Klaus Nordhausen", "authors": "Fran\\c{c}ois Bachoc (IMT), Marc G. Genton (KAUST), Klaus Nordhausen\n  (TU WIEN), Anne Ruiz-Gazen (TSE), Joni Virta", "title": "Spatial Blind Source Separation", "comments": null, "journal-ref": "Biometrika 107: 627-646 (2020)", "doi": "10.1093/biomet/asz079", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently a blind source separation model was suggested for spatial data\ntogether with an estimator based on the simultaneous diagonalisation of two\nscatter matrices. The asymptotic properties of this estimator are derived here\nand a new estimator, based on the joint diagonalisation of more than two\nscatter matrices, is proposed. The asymptotic properties and merits of the\nnovel estimator are verified in simulation studies. A real data example\nillustrates the method.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 15:25:01 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 19:29:06 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Bachoc", "Fran\u00e7ois", "", "IMT"], ["Genton", "Marc G.", "", "KAUST"], ["Nordhausen", "Klaus", "", "TU WIEN"], ["Ruiz-Gazen", "Anne", "", "TSE"], ["Virta", "Joni", ""]]}, {"id": "1812.09250", "submitter": "Peter Kramlinger", "authors": "Peter Kramlinger and Tatyana Krivobokova and Stefan Sperlich", "title": "Marginal and Conditional Multiple Inference for Linear Mixed Model\n  Predictors", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In spite of its high practical relevance, cluster specific multiple inference\nfor linear mixed model predictors has hardly been addressed so far. While\nmarginal inference for population parameters is well understood, conditional\ninference for the cluster specific predictors is more intricate. This work\nintroduces a general framework for multiple inference in linear mixed models\nfor cluster specific predictors. Consistent simultaneous confidence sets for\ncluster specific predictors are constructed. Furthermore, it is shown that,\nwhile these simultaneous conditional confidence sets are feasible, remarkably,\ncorresponding marginal confidence sets are also asymptotically valid for\nconditional inference. Those lend themselves for testing linear hypotheses\nusing standard quantiles without the need of re-sampling techniques. All\nfindings are validated in simulations and illustrated along a study on Spanish\nincome data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 16:45:14 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 14:22:56 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2020 16:23:58 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 12:15:39 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Kramlinger", "Peter", ""], ["Krivobokova", "Tatyana", ""], ["Sperlich", "Stefan", ""]]}, {"id": "1812.09322", "submitter": "Christof Str\\\"ahl", "authors": "Christof Str\\\"ahl and Johanna F. Ziegel and Lutz Duembgen", "title": "Local Estimation of a Multivariate Density and its Derivatives", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze four different approaches to estimate a multivariate probability\ndensity (or the log-density) and its first and second order derivatives. Two\nmethods, local log-likelihood and local Hyv\\\"arinen score estimation, are in\nterms of weighted scoring rules with local quadratic models. The other two\napproaches are matching of local moments and kernel density estimation. All\nestimators depend on a general kernel, and we use the Gaussian kernel to\nprovide explicit examples. Asymptotic properties of the estimators are derived\nand compared. In terms of rates of convergence, a refined local moment matching\nestimator is the best.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 16:15:44 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 15:18:30 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Str\u00e4hl", "Christof", ""], ["Ziegel", "Johanna F.", ""], ["Duembgen", "Lutz", ""]]}, {"id": "1812.09367", "submitter": "Davy Paindaveine", "authors": "Davy Paindaveine, Julien Remy and Thomas Verdebout", "title": "Sign tests for weak principal directions", "comments": "34 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference on the first principal direction of a $p$-variate\nelliptical distribution. We do so in challenging double asymptotic scenarios\nfor which this direction eventually fails to be identifiable. In order to\nachieve robustness not only with respect to such weak identifiability but also\nwith respect to heavy tails, we focus on sign-based statistical procedures,\nthat is, on procedures that involve the observations only through their\ndirection from the center of the distribution. We actually consider the generic\nproblem of testing the null hypothesis that the first principal direction\ncoincides with a given direction of $\\mathbb{R}^p$. We first focus on weak\nidentifiability setups involving single spikes (that is, involving spectra for\nwhich the smallest eigenvalue has multiplicity $p-1$). We show that,\nirrespective of the degree of weak identifiability, such setups offer local\nalternatives for which the corresponding sequence of statistical experiments\nconverges in the Le Cam sense. Interestingly, the limiting experiments depend\non the degree of weak identifiability. We exploit this convergence result to\nbuild optimal sign tests for the problem considered. In classical asymptotic\nscenarios where the spectrum is fixed, these tests are shown to be\nasymptotically equivalent to the sign-based likelihood ratio tests available in\nthe literature. Unlike the latter, however, the proposed sign tests are robust\nto arbitrarily weak identifiability. We show that our tests meet the asymptotic\nlevel constraint irrespective of the structure of the spectrum, hence also in\npossibly multi-spike setups. We fully characterize the non-null asymptotic\ndistributions of the corresponding test statistics under weak identifiability,\nwhich allows us to quantify the corresponding local asymptotic powers.\n", "versions": [{"version": "v1", "created": "Fri, 21 Dec 2018 20:52:00 GMT"}, {"version": "v2", "created": "Thu, 29 Aug 2019 07:53:16 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Paindaveine", "Davy", ""], ["Remy", "Julien", ""], ["Verdebout", "Thomas", ""]]}, {"id": "1812.09514", "submitter": "Maryna Prus", "authors": "Maryna Prus", "title": "Optimal Designs for Prediction in Two Treatment Groups Random\n  Coefficient Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subject of this work is two treatment groups random coefficient\nregression models, in which observational units receive some group-specific\ntreatments. We provide A- and D-optimality criteria for the estimation of the\nfixed parameter and the prediction of the random effects. We illustrate the\nbehavior of optimal designs by a simple example.\n", "versions": [{"version": "v1", "created": "Sat, 22 Dec 2018 12:00:15 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 11:25:35 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Prus", "Maryna", ""]]}, {"id": "1812.09947", "submitter": "Jo\\~ao Lita Da Silva", "authors": "Jo\\~ao Lita da Silva", "title": "Almost sure convergence for weighted sums of pairwise PQD random\n  variables", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain Marcinkiewicz-Zygmund strong laws of large numbers for weighted\nsums of pairwise positively quadrant dependent random variables stochastically\ndominated by a random variable $X \\in \\mathscr{L}_{p}$, $1 \\leqslant p < 2$. We\nuse our results to establish the strong consistency of estimators which emerge\nfrom regression models having pairwise positively quadrant dependent errors.\n", "versions": [{"version": "v1", "created": "Mon, 24 Dec 2018 16:16:41 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["da Silva", "Jo\u00e3o Lita", ""]]}, {"id": "1812.10013", "submitter": "Qifan Song", "authors": "Qifan Song and Guang Cheng", "title": "Optimal False Discovery Control of Minimax Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two major research tasks lie at the heart of high dimensional data analysis:\naccurate parameter estimation and correct support recovery. Existing literature\nmostly aim for either the best parameter estimation or the best model selection\nresult, however little has been done to understand the potential interaction\nbetween the estimation precision and the selection behavior. In this work, our\nminimax result shows that an estimator's performance of type I error control\ncritically depends on its $L_2$ estimation error rate, and reveals a trade-off\nphenomenon between the rate of convergence and the false discovery control:\nbetter estimation accuracy leads to more false discoveries. In particular, we\ncharacterize the false discovery control behavior of rate-optimal and\nrate-suboptimal estimators under different sparsity regimes, and discover a\nrigid dichotomy between these two estimators under near-linear and linear\nsparsity settings. In addition, this work provides a rigorous explanation to\nthe incompatibility phenomenon between selection consistency and\nrate-minimaxity which has been frequently observed in the high dimensional\nliterature.\n", "versions": [{"version": "v1", "created": "Tue, 25 Dec 2018 02:17:52 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 15:39:11 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Song", "Qifan", ""], ["Cheng", "Guang", ""]]}, {"id": "1812.10389", "submitter": "Gilles Stoltz", "authors": "Rapha\\\"el Deswarte (CMAP), V\\'eronique Gervais (IFPEN), Gilles Stoltz\n  (LMO), S\\'ebastien da Veiga", "title": "Sequential model aggregation for production forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Production forecasting is a key step to design the future development of a\nreservoir. A classical way to generate such forecasts consists in simulating\nfuture production for numerical models representative of the reservoir.\nHowever, identifying such models can be very challenging as they need to be\nconstrained to all available data. In particular, they should reproduce past\nproduction data, which requires to solve a complex non-linear inverse problem.\nIn this paper, we thus propose to investigate the potential of machine learning\nalgorithms to predict the future production of a reservoir based on past\nproduction data without model calibration. We focus more specifically on robust\nonline aggregation, a deterministic approach that provides a robust framework\nto make forecasts on a regular basis. This method does not rely on any specific\nassumption or need for stochastic modeling. Forecasts are first simulated for a\nset of base reservoir models representing the prior uncertainty, and then\ncombined to predict production at the next time step. The weight associated to\neach forecast is related to its past performance. Three different algorithms\nare considered for weight computations: the exponentially weighted average\nalgorithm, ridge regression and the Lasso regression. They are applied on a\nsynthetic reservoir case study, the Brugge case, for sequential predictions. To\nestimate the potential of development scenarios, production forecasts are\nneeded on long periods of time without intermediary data acquisition. An\nextension of the deterministic aggregation approach is thus proposed in this\npaper to provide such multi-step-ahead forecasts.\n", "versions": [{"version": "v1", "created": "Fri, 30 Nov 2018 12:56:56 GMT"}, {"version": "v2", "created": "Tue, 27 Aug 2019 12:45:55 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Deswarte", "Rapha\u00ebl", "", "CMAP"], ["Gervais", "V\u00e9ronique", "", "IFPEN"], ["Stoltz", "Gilles", "", "LMO"], ["da Veiga", "S\u00e9bastien", ""]]}, {"id": "1812.10519", "submitter": "Jes\\'us Daniel Arroyo Reli\\'on", "authors": "Jes\\'us Arroyo, Daniel L. Sussman, Carey E. Priebe, Vince Lyzinski", "title": "Maximum Likelihood Estimation and Graph Matching in Errorfully Observed\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a pair of graphs with the same number of vertices, the inexact graph\nmatching problem consists in finding a correspondence between the vertices of\nthese graphs that minimizes the total number of induced edge disagreements. We\nstudy this problem from a statistical framework in which one of the graphs is\nan errorfully observed copy of the other. We introduce a corrupting channel\nmodel, and show that in this model framework, the solution to the graph\nmatching problem is a maximum likelihood estimator. Necessary and sufficient\nconditions for consistency of this MLE are presented, as well as a relaxed\nnotion of consistency in which a negligible fraction of the vertices need not\nbe matched correctly. The results are used to study matchability in several\nfamilies of random graphs, including edge independent models, random regular\ngraphs and small-world networks. We also use these results to introduce\nmeasures of matching feasibility, and experimentally validate the results on\nsimulated and real-world networks.\n", "versions": [{"version": "v1", "created": "Wed, 26 Dec 2018 20:01:30 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2019 20:23:08 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 18:45:41 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2020 20:27:33 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Arroyo", "Jes\u00fas", ""], ["Sussman", "Daniel L.", ""], ["Priebe", "Carey E.", ""], ["Lyzinski", "Vince", ""]]}, {"id": "1812.10556", "submitter": "Rodrigo S. Targino", "authors": "Milan Merkle and Yuri F. Saporito and Rodrigo S. Targino", "title": "Bayesian Approach for Parameter Estimation of Continuous-Time Stochastic\n  Volatility Models using Fourier Transform Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two stage procedure for the estimation of the parameters of a\nfairly general, continuous-time stochastic volatility. An important ingredient\nof the proposed method is the Cuchiero-Teichmann volatility estimator, which is\nbased on Fourier transforms and provides a continuous time estimate of the\nlatent process. This estimate is then used to construct an approximate\nlikelihood for the parameters of interest, whose restrictions are taken into\naccount through prior distributions. The procedure is shown to be highly\nsuccessful for constructing the posterior distribution of the parameters of a\nHeston model, while limited success is achieved when applied to the highly\nparametrized exponential-Ornstein-Uhlenbeck.\n", "versions": [{"version": "v1", "created": "Mon, 10 Dec 2018 20:10:45 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Merkle", "Milan", ""], ["Saporito", "Yuri F.", ""], ["Targino", "Rodrigo S.", ""]]}, {"id": "1812.10596", "submitter": "Veson Lee", "authors": "Veson Lee, Jan Vrbik", "title": "Asymptotic Distribution of Centralized $r$ When Sampling from Cauchy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assume that $X$ and $Y$ are independent random variables, each having a\nCauchy distribution with a known median. Taking a random independent sample of\nsize $n$ of each $X$ and $Y$, one can then compute their centralized empirical\ncorrelation coefficient $r$. Analytically investigating the sampling\ndistribution of this $r$ appears possible only in the large $n$ limit; this is\nwhat we have done in this article, deriving several new and interesting\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 02:27:12 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Lee", "Veson", ""], ["Vrbik", "Jan", ""]]}, {"id": "1812.10678", "submitter": "Tomohiro Hayase", "authors": "Tomohiro Hayase", "title": "Identifiability of parametric random matrix models", "comments": null, "journal-ref": "Infinite Dimensional Analysis, Quantum Probability and Related\n  Topics Vol. 22, No. 03, 1950018 (2019)", "doi": null, "report-no": null, "categories": "math.PR math.OA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate parameter identifiability of spectral distributions of random\nmatrices. In particular, we treat compound Wishart type and signal-plus-noise\ntype. We show that each model is identifiable up to some kind of rotation of\nparameter space. Our method is based on free probability theory.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 10:00:28 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Hayase", "Tomohiro", ""]]}, {"id": "1812.10741", "submitter": "Hailin Sang", "authors": "Aleksandr Beknazaryan, Xin Dang and Hailin Sang", "title": "On mutual information estimation for mixed-pair random variables", "comments": "10 pages, 3 figures, accepted by Statistics and Probability Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the mutual information estimation for mixed-pair random variables.\nOne random variable is discrete and the other one is continuous. We develop a\nkernel method to estimate the mutual information between the two random\nvariables. The estimates enjoy a central limit theorem under some regular\nconditions on the distributions. The theoretical results are demonstrated by\nsimulation study.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 15:25:52 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Beknazaryan", "Aleksandr", ""], ["Dang", "Xin", ""], ["Sang", "Hailin", ""]]}, {"id": "1812.10742", "submitter": "Royi Jacobovic", "authors": "Royi Jacobovic", "title": "Asymptotic comparison of two-stage selection procedures under\n  quasi-Bayesian framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper revisits the procedures suggested by Dudewicz and Dalal (1975) and\nRinott (1978) which are designed for selecting the population with the highest\nmean among independent Gaussian populations with unknown and possibly different\nvariances. In a previous paper Jacobovic and Zuk (2017) made a conjecture that\nthe relative asymptotic efficiency of these procedures equals to the ratio of\ntwo certain sequences. This work suggests a quasi-Bayesian modelling of the\nproblem under which this conjecture is valid. In addition, this paper motivates\nan open question regarding the extreme value distribution of the maxima of\ntriangular array of independent student-t random variables with an increasing\nnumber of degrees of freedom.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 15:27:21 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Jacobovic", "Royi", ""]]}, {"id": "1812.10752", "submitter": "David Preinerstorfer", "authors": "David Preinerstorfer", "title": "How to avoid the zero-power trap in testing for correlation", "comments": null, "journal-ref": null, "doi": "10.1017/S0266466621000062", "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In testing for correlation of the errors in regression models the power of\ntests can be very low for strongly correlated errors. This counterintuitive\nphenomenon has become known as the \"zero-power trap\". Despite a considerable\namount of literature devoted to this problem, mainly focusing on its detection,\na convincing solution has not yet been found. In this article we first discuss\ntheoretical results concerning the occurrence of the zero-power trap\nphenomenon. Then, we suggest and compare three ways to avoid it. Given an\ninitial test that suffers from the zero-power trap, the method we recommend for\npractice leads to a modified test whose power converges to one as the\ncorrelation gets very strong. Furthermore, the modified test has approximately\nthe same power function as the initial test, and thus approximately preserves\nall of its optimality properties. We also provide some numerical illustrations\nin the context of testing for network generated correlation.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 16:10:19 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Preinerstorfer", "David", ""]]}, {"id": "1812.10758", "submitter": "Li-Pang Chen", "authors": "Li-Pang Chen", "title": "Semiparametric Estimation for the Transformation Model with\n  Length-Biased Data and Covariate Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of survival data with biased samples caused by left-truncation or\nlength-biased sampling has received extensive interest. Many inference methods\nhave been developed for various survival models. These methods, however, break\ndown when survival data are typically error-contaminated. Although error-prone\nsurvival data commonly arise in practice, little work has been available in the\nliterature for handling length-biased data with measurement error. In survival\nanalysis, the transformation model is one of the frequently used models.\nHowever, methods of analyzing the transformation model with those complex\nfeatures have not been fully explored. In this paper, we study this important\nproblem and develop a valid inference method under the transformation model. We\nestablish asymptotic results for the proposed estimators. The proposed method\nenjoys appealing features in that there is no need to specify the distribution\nof the covariates and the increasing function in the transformation model.\nNumerical studies are reported to assess the performance of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 27 Dec 2018 16:26:41 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Chen", "Li-Pang", ""]]}, {"id": "1812.10911", "submitter": "Xinran Li", "authors": "Xinran Li, Peng Ding, Donald B. Rubin", "title": "Rerandomization in $2^K$ Factorial Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With many pretreatment covariates and treatment factors, the classical\nfactorial experiment often fails to balance covariates across multiple\nfactorial effects simultaneously. Therefore, it is intuitive to restrict the\nrandomization of the treatment factors to satisfy certain covariate balance\ncriteria, possibly conforming to the tiers of factorial effects and covariates\nbased on their relative importances. This is rerandomization in factorial\nexperiments. We study the asymptotic properties of this experimental design\nunder the randomization inference framework without imposing any distributional\nor modeling assumptions of the covariates and outcomes. We derive the joint\nasymptotic sampling distribution of the usual estimators of the factorial\neffects, and show that it is symmetric, unimodal, and more \"concentrated\" at\nthe true factorial effects under rerandomization than under the classical\nfactorial experiment. We quantify this advantage of rerandomization using the\nnotions of \"central convex unimodality\" and \"peakedness\" of the joint\nasymptotic sampling distribution. We also construct conservative large-sample\nconfidence sets for the factorial effects.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 06:55:06 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Li", "Xinran", ""], ["Ding", "Peng", ""], ["Rubin", "Donald B.", ""]]}, {"id": "1812.11167", "submitter": "Xiyu Zhai", "authors": "Alexander Rakhlin and Xiyu Zhai", "title": "Consistency of Interpolation with Laplace Kernels is a High-Dimensional\n  Phenomenon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that minimum-norm interpolation in the Reproducing Kernel Hilbert\nSpace corresponding to the Laplace kernel is not consistent if input dimension\nis constant. The lower bound holds for any choice of kernel bandwidth, even if\nselected based on data. The result supports the empirical observation that\nminimum-norm interpolation (that is, exact fit to training data) in RKHS\ngeneralizes well for some high-dimensional datasets, but not for\nlow-dimensional ones.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 18:52:53 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Rakhlin", "Alexander", ""], ["Zhai", "Xiyu", ""]]}, {"id": "1812.11269", "submitter": "Zhixin Zhou", "authors": "Zhixin Zhou, Ping Li", "title": "Non-Asymptotic Chernoff Lower Bound and Its Application to Community\n  Detection in Stochastic Block Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chernoff coefficient is an upper bound of Bayes error probability in\nclassification problem. In this paper, we will develop sharp Chernoff type\nbound on Bayes error probability. The new bound is not only an upper bound but\nalso a lower bound of Bayes error probability up to a constant in a\nnon-asymptotic setting. Moreover, we will apply this result to community\ndetection in stochastic block model. As a clustering problem, the optimal error\nrate of community detection can be characterized by our Chernoff type bound.\nThis can be formalized by deriving a minimax error rate over certain class of\nparameter space, then achieving such error rate by a feasible algorithm employ\nmultiple steps of EM type updates.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 02:18:09 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Zhou", "Zhixin", ""], ["Li", "Ping", ""]]}, {"id": "1812.11330", "submitter": "Eric Gautier", "authors": "Eric Gautier (TSE), Alexandre B. Tsybakov (CREST)", "title": "High-dimensional instrumental variables regression and confidence sets\n  -- v2/2012", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This was a revision of arXiv:1105.2454v1 from 2012. It considers a variation\non the STIV estimator where, instead of one conic constraint, there are as many\nconic constraints as moments (instruments) allowing to use more directly\nmoderate deviations for self-normalized sums. The idea first appeared in\nformula (6.5) in arXiv:1105.2454v1 when some instruments can be endogenous. For\nreference and to avoid confusion with the STIV estimator, this estimator should\nbe called C-STIV.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 10:36:48 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 07:28:32 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Gautier", "Eric", "", "TSE"], ["Tsybakov", "Alexandre B.", "", "CREST"]]}, {"id": "1812.11433", "submitter": "Emmanuel Candes", "authors": "Rina Foygel Barber and Emmanuel Candes", "title": "On the Construction of Knockoffs in Case-Control Studies", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a case-control study in which we have a random sample, constructed\nin such a way that the proportion of cases in our sample is different from that\nin the general population---for instance, the sample is constructed to achieve\na fixed ratio of cases to controls. Imagine that we wish to determine which of\nthe potentially many covariates under study truly influence the response by\napplying the new model-X knockoffs approach. This paper demonstrates that it\nsuffices to design knockoff variables using data that may have a different\nratio of cases to controls. For example, the knockoff variables can be\nconstructed using the distribution of the original variables under any of the\nfollowing scenarios: (1) a population of controls only; (2) a population of\ncases only; (3) a population of cases and controls mixed in an arbitrary\nproportion (irrespective of the fraction of cases in the sample at hand). The\nconsequence is that knockoff variables may be constructed using unlabeled data,\nwhich is often available more easily than labeled data, while maintaining\nType-I error guarantees.\n", "versions": [{"version": "v1", "created": "Sat, 29 Dec 2018 20:25:03 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Barber", "Rina Foygel", ""], ["Candes", "Emmanuel", ""]]}, {"id": "1812.11476", "submitter": "Cl\\'ement Canonne", "authors": "Jayadev Acharya, Cl\\'ement L. Canonne, Himanshu Tyagi", "title": "Inference under Information Constraints I: Lower Bounds from Chi-Square\n  Contraction", "comments": "To appear in IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.DM cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple players are each given one independent sample, about which they can\nonly provide limited information to a central referee. Each player is allowed\nto describe its observed sample to the referee using a channel from a family of\nchannels $\\mathcal{W}$, which can be instantiated to capture both the\ncommunication- and privacy-constrained settings and beyond. The referee uses\nthe messages from players to solve an inference problem for the unknown\ndistribution that generated the samples. We derive lower bounds for sample\ncomplexity of learning and testing discrete distributions in this\ninformation-constrained setting.\n  Underlying our bounds is a characterization of the contraction in chi-square\ndistances between the observed distributions of the samples when information\nconstraints are placed. This contraction is captured in a local neighborhood in\nterms of chi-square and decoupled chi-square fluctuations of a given channel,\ntwo quantities we introduce. The former captures the average distance between\ndistributions of channel output for two product distributions on the input, and\nthe latter for a product distribution and a mixture of product distribution on\nthe input. Our bounds are tight for both public- and private-coin protocols.\nInterestingly, the sample complexity of testing is order-wise higher when\nrestricted to private-coin protocols.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 06:29:14 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 18:04:55 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 01:55:18 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2020 04:30:54 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Acharya", "Jayadev", ""], ["Canonne", "Cl\u00e9ment L.", ""], ["Tyagi", "Himanshu", ""]]}, {"id": "1812.11605", "submitter": "Mazza", "authors": "Corina Ciobotaru and Christian Mazza", "title": "Geometrical and statistical properties of M-estimates of scatter on\n  Grassmann manifolds", "comments": "We have modified some previous computations and give a Lemma 5.12 on\n  the strict geodesic convexity of M-functionals", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider data from the Grassmann manifold $G(m,r)$ of all vector subspaces\nof dimension $r$ of $\\mathbb{R}^m$, and focus on the Grassmannian statistical\nmodel which is of common use in signal processing and statistics. Canonical\nGrassmannian distributions $\\mathbb{G}_{\\Sigma}$ on $G(m,r)$ are indexed by\nparameters $\\Sigma$ from the manifold $\\mathcal{M}= Pos_{sym}^{1}(m)$ of\npositive definite symmetric matrices of determinant $1$. Robust M-estimates of\nscatter (GE) for general probability measures $\\mathcal{P}$ on $G(m,r)$ are\nstudied. Such estimators are defined to be the maximizers of the Grassmannian\nlog-likelihood $-\\ell_{\\mathcal{P}}(\\Sigma)$ as function of $\\Sigma$. One of\nthe novel features of this work is a strong use of the fact that $\\mathcal{M}$\nis a CAT(0) space with known visual boundary at infinity $\\partial\n\\mathcal{M}$. We also recall that the sample space $G(m,r)$ is a part of\n$\\partial \\mathcal{M}$, show the distributions $\\mathbb{G}_{\\Sigma}$ are\n$SL(m,\\mathbb{R})$--quasi-invariant, and that $\\ell_{\\mathcal{P}}(\\Sigma)$ is a\nweighted Busemann function. Let $\\mathcal{P}_n\n=(\\delta_{U_1}+\\cdots+\\delta_{U_n})/n$ be the empirical probability measure for\n$n$-samples of random i.i.d. subspaces $U_i\\in G(m,r)$ of common distribution\n$\\mathcal{P}$, whose support spans $\\mathbb{R}^m$. For $\\Sigma_n$ and\n$\\Sigma_{\\mathcal{P}}$ the GEs of $\\mathcal{P}_n$ and $\\mathcal{P}$, we show\nthe almost sure convergence of $\\Sigma_n$ towards $\\Sigma$ as $n\\to\\infty$\nusing methods from geometry, and provide a central limit theorem for the\nrescaled process $C_n = \\frac{m}{tr(\\Sigma_{\\mathcal{P}}^{-1} \\Sigma_n)}g^{-1}\n\\Sigma_n g^{-1}$, where $\\Sigma =gg$ with $g\\in SL(m,\\mathbb{R})$ the unique\nsymmetric positive-definite square root of $\\Sigma$.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 20:26:11 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 15:29:52 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Ciobotaru", "Corina", ""], ["Mazza", "Christian", ""]]}, {"id": "1812.11634", "submitter": "Richard Samworth", "authors": "Oliver Y. Feng, Adityanand Guntuboyina, Arlene K. H. Kim and Richard\n  J. Samworth", "title": "Adaptation in multivariate log-concave density estimation", "comments": "97 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the adaptation properties of the multivariate log-concave maximum\nlikelihood estimator over three subclasses of log-concave densities. The first\nconsists of densities with polyhedral support whose logarithms are piecewise\naffine. The complexity of such densities~$f$ can be measured in terms of the\nsum $\\Gamma(f)$ of the numbers of facets of the subdomains in the polyhedral\nsubdivision of the support induced by $f$. Given $n$ independent observations\nfrom a $d$-dimensional log-concave density with $d \\in \\{2,3\\}$, we prove a\nsharp oracle inequality, which in particular implies that the Kullback--Leibler\nrisk of the log-concave maximum likelihood estimator for such densities is\nbounded above by $\\Gamma(f)/n$, up to a polylogarithmic factor. Thus, the rate\ncan be essentially parametric, even in this multivariate setting. For the\nsecond type of adaptation, we consider densities that are bounded away from\nzero on a polytopal support; we show that up to polylogarithmic factors, the\nlog-concave maximum likelihood estimator attains the rate $n^{-4/7}$ when\n$d=3$, which is faster than the worst-case rate of $n^{-1/2}$. Finally, our\nthird type of subclass consists of densities whose contours are well-separated;\nthese new classes are constructed to be affine invariant and turn out to\ncontain a wide variety of densities, including those that satisfy H\\\"older\nregularity conditions. Here, we prove another sharp oracle inequality, which\nreveals in particular that the log-concave maximum likelihood estimator attains\na risk bound of order\n$n^{-\\min\\bigl(\\frac{\\beta+3}{\\beta+7},\\frac{4}{7}\\bigr)}$ when $d=3$ over the\nclass of $\\beta$-H\\\"older log-concave densities with $\\beta\\in (1,3]$, again up\nto a polylogarithmic factor.\n", "versions": [{"version": "v1", "created": "Sun, 30 Dec 2018 23:22:22 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 16:45:50 GMT"}], "update_date": "2019-10-21", "authors_parsed": [["Feng", "Oliver Y.", ""], ["Guntuboyina", "Adityanand", ""], ["Kim", "Arlene K. H.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1812.11954", "submitter": "Qiang Sun", "authors": "Anna Little, Yuying Xie, Qiang Sun", "title": "Exact Cluster Recovery via Classical Multidimensional Scaling", "comments": "42 pages in cluding appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical multidimensional scaling is an important dimension reduction\ntechnique. Yet few theoretical results characterizing its statistical\nperformance exist. This paper provides a theoretical framework for analyzing\nthe quality of embedded samples produced by classical multidimensional scaling.\nThis lays the foundation for various downstream statistical analyses, and we\nfocus on clustering noisy data. Our results provide scaling conditions on the\nsample size, ambient dimensionality, between-class distance, and noise level\nunder which classical multidimensional scaling followed by a distance-based\nclustering algorithm can recover the cluster labels of all samples with high\nprobability. Numerical simulations confirm these scaling conditions are\nnear-sharp. Applications to both human genomics data and natural language data\nlend strong support to the methodology and theory.\n", "versions": [{"version": "v1", "created": "Mon, 31 Dec 2018 18:48:33 GMT"}, {"version": "v2", "created": "Sun, 13 Jan 2019 23:37:57 GMT"}, {"version": "v3", "created": "Tue, 15 Jan 2019 05:28:28 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 03:16:10 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Little", "Anna", ""], ["Xie", "Yuying", ""], ["Sun", "Qiang", ""]]}, {"id": "1812.11973", "submitter": "Li-Pang Chen", "authors": "Li-Pang Chen", "title": "Semiparametric Estimation for Cure Survival Model with Left-Truncated\n  and Right-Censored Data and Covariate Measurement Error", "comments": "arXiv admin note: substantial text overlap with arXiv:1812.10758", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we mainly discuss the cure model with survival data. Different\nfrom the usual survival data with right-censoring, we incorporate the features\nof left-truncation and measurement error in covariates. Generally speaking,\nleft-truncation causes a biased sample in survival analysis; measurement error\nin covariates may incur a tremendous bias if we do not deal with it properly.\nTo deal with these challenges, we propose a flexible way to analyze\nleft-truncated survival data and correct measurement error in covariates. The\ntheoretical results are also established in this paper.\n", "versions": [{"version": "v1", "created": "Fri, 28 Dec 2018 18:51:02 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Chen", "Li-Pang", ""]]}]