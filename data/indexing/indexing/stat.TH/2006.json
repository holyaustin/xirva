[{"id": "2006.00032", "submitter": "Andrew Davis", "authors": "Andrew D. Davis and Youssef Marzouk and Aaron Smith and Natesh Pillai", "title": "Rate-optimal refinement strategies for local approximation MCMC", "comments": "32 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many Bayesian inference problems involve target distributions whose density\nfunctions are computationally expensive to evaluate. Replacing the target\ndensity with a local approximation based on a small number of carefully chosen\ndensity evaluations can significantly reduce the computational expense of\nMarkov chain Monte Carlo (MCMC) sampling. Moreover, continual refinement of the\nlocal approximation can guarantee asymptotically exact sampling. We devise a\nnew strategy for balancing the decay rate of the bias due to the approximation\nwith that of the MCMC variance. We prove that the error of the resulting local\napproximation MCMC (LA-MCMC) algorithm decays at roughly the expected\n$1/\\sqrt{T}$ rate, and we demonstrate this rate numerically. We also introduce\nan algorithmic parameter that guarantees convergence given very weak tail\nbounds, significantly strengthening previous convergence results. Finally, we\napply LA-MCMC to a computationally intensive Bayesian inverse problem arising\nin groundwater hydrology.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 18:45:33 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 20:33:17 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Davis", "Andrew D.", ""], ["Marzouk", "Youssef", ""], ["Smith", "Aaron", ""], ["Pillai", "Natesh", ""]]}, {"id": "2006.00243", "submitter": "Mohamed Haddouche", "authors": "Mohamed Anis Haddouche, Dominique Fourdrinier and Fatiha Mezoued", "title": "Scale matrix estimation under data-based loss in high and low dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the scale matrix $\\Sigma$ of the\nadditif model $Y_{p\\times n} = M + \\mathcal{E}$, under a theoretical decision\npoint of view. Here, $ p $ is the number of variables, $ n$ is the number of\nobservations, $ M $ is a matrix of unknown parameters with rank $q<p$ and $\n\\mathcal {E}$ is a random noise, whose distribution is elliptically symmetric\nwith covariance matrix proportional to $ I_n \\otimes \\Sigma $\\,. We deal with a\ncanonical form of this model where $Y$ is decomposed in two matrices, namely,\n$Z_{q\\times p}$ which summarizes the information contained in $ M $, and $\nU_{m\\times p}$, where $m=n-q$, which summarizes the sufficient information to\nestimate $ \\Sigma $. As the natural estimators of the form ${\\hat\n{\\Sigma}}_a=a\\, S$ (where $ S=U^{T}\\,U$ and $a$ is a positive constant) perform\npoorly when $p >m$ (S non-invertible), we propose estimators of the form\n${\\hat{\\Sigma}}_{a, G} = a\\big( S+ S \\, {S^{+}\\,G(Z,S)}\\big)$ where ${S^{+}}$\nis the Moore-Penrose inverse of $ S$ (which coincides with $S^{-1}$ when $S$ is\ninvertible). We provide conditions on the correction matrix $SS^{+}{G(Z,S)}$\nsuch that ${\\hat {\\Sigma}}_{a, G}$ improves over ${\\hat {\\Sigma}}_a$ under the\ndata-based loss $L _S( \\Sigma, \\hat { \\Sigma}) ={\\rm tr} \\big (\nS^{+}\\Sigma\\,({\\hat{\\Sigma}} \\, {\\Sigma} ^ {- 1} - {I}_ {p} )^ {2}\\big) $. We\nadopt a unified approach of the two cases where $ S$ is invertible ($p \\leq m$)\nand $ S$ is non-invertible ($p>m$).\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 11:49:46 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Haddouche", "Mohamed Anis", ""], ["Fourdrinier", "Dominique", ""], ["Mezoued", "Fatiha", ""]]}, {"id": "2006.00278", "submitter": "Johannes Schmidt-Hieber", "authors": "Alexis Derumigny and Johannes Schmidt-Hieber", "title": "On lower bounds for the bias-variance trade-off", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a common phenomenon that for high-dimensional and nonparametric\nstatistical models, rate-optimal estimators balance squared bias and variance.\nAlthough this balancing is widely observed, little is known whether methods\nexist that could avoid the trade-off between bias and variance. We propose a\ngeneral strategy to obtain lower bounds on the variance of any estimator with\nbias smaller than a prespecified bound. This shows to which extent the\nbias-variance trade-off is unavoidable and allows to quantify the loss of\nperformance for methods that do not obey it. The approach is based on a number\nof abstract lower bounds for the variance involving the change of expectation\nwith respect to different probability measures as well as information measures\nsuch as the Kullback-Leibler or chi-square divergence. Some of these\ninequalities rely on a new concept of information matrices. In a second part of\nthe article, the abstract lower bounds are applied to several statistical\nmodels including the Gaussian white noise model, a boundary estimation problem,\nthe Gaussian sequence model and the high-dimensional linear regression model.\nFor these specific statistical applications, different types of bias-variance\ntrade-offs occur that vary considerably in their strength. For the trade-off\nbetween integrated squared bias and integrated variance in the Gaussian white\nnoise model, we propose to combine the general strategy for lower bounds with a\nreduction technique. This allows us to reduce the original problem to a lower\nbound on the bias-variance trade-off for estimators with additional symmetry\nproperties in a simpler statistical model. To highlight possible extensions of\nthe proposed framework, we moreover briefly discuss the trade-off between bias\nand mean absolute deviation.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 14:07:43 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Derumigny", "Alexis", ""], ["Schmidt-Hieber", "Johannes", ""]]}, {"id": "2006.00294", "submitter": "Johannes Lederer", "authors": "Mahsa Taheri and Fang Xie and Johannes Lederer", "title": "Statistical Guarantees for Regularized Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.NE math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks have become standard tools in the analysis of data, but they\nlack comprehensive mathematical theories. For example, there are very few\nstatistical guarantees for learning neural networks from data, especially for\nclasses of estimators that are used in practice or at least similar to such. In\nthis paper, we develop a general statistical guarantee for estimators that\nconsist of a least-squares term and a regularizer. We then exemplify this\nguarantee with $\\ell_1$-regularization, showing that the corresponding\nprediction error increases at most sub-linearly in the number of layers and at\nmost logarithmically in the total number of parameters. Our results establish a\nmathematical basis for regularized estimation of neural networks, and they\ndeepen our mathematical understanding of neural networks and deep learning more\ngenerally.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 15:28:47 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2020 09:18:34 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Taheri", "Mahsa", ""], ["Xie", "Fang", ""], ["Lederer", "Johannes", ""]]}, {"id": "2006.00334", "submitter": "Lam Ho", "authors": "Lam Si Tung Ho, Vu Dinh", "title": "Consistent feature selection for neural networks via Adaptive Group\n  Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One main obstacle for the wide use of deep learning in medical and\nengineering sciences is its interpretability. While neural network models are\nstrong tools for making predictions, they often provide little information\nabout which features play significant roles in influencing the prediction\naccuracy. To overcome this issue, many regularization procedures for learning\nwith neural networks have been proposed for dropping non-significant features.\nUnfortunately, the lack of theoretical results casts doubt on the applicability\nof such pipelines. In this work, we propose and establish a theoretical\nguarantee for the use of the adaptive group lasso for selecting important\nfeatures of neural networks. Specifically, we show that our feature selection\nmethod is consistent for single-output feed-forward neural networks with one\nhidden layer and hyperbolic tangent activation function. We demonstrate its\napplicability using both simulation and data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 18:50:56 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 04:35:59 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Ho", "Lam Si Tung", ""], ["Dinh", "Vu", ""]]}, {"id": "2006.00426", "submitter": "Lingzhou Xue", "authors": "Xiufan Yu, Danning Li, and Lingzhou Xue", "title": "Fisher's combined probability test for high-dimensional covariance\n  matrices", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing large covariance matrices is of fundamental importance in statistical\nanalysis with high-dimensional data. In the past decade, three types of test\nstatistics have been studied in the literature: quadratic form statistics,\nmaximum form statistics, and their weighted combination. It is known that\nquadratic form statistics would suffer from low power against sparse\nalternatives and maximum form statistics would suffer from low power against\ndense alternatives. The weighted combination methods were introduced to enhance\nthe power of quadratic form statistics or maximum form statistics when the\nweights are appropriately chosen. In this paper, we provide a new perspective\nto exploit the full potential of quadratic form statistics and maximum form\nstatistics for testing high-dimensional covariance matrices. We propose a\nscale-invariant power enhancement test based on Fisher's method to combine the\np-values of quadratic form statistics and maximum form statistics. After\ncarefully studying the asymptotic joint distribution of quadratic form\nstatistics and maximum form statistics, we prove that the proposed combination\nmethod retains the correct asymptotic size and boosts the power against more\ngeneral alternatives. Moreover, we demonstrate the finite-sample performance in\nsimulation studies and a real application.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 03:32:26 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Yu", "Xiufan", ""], ["Li", "Danning", ""], ["Xue", "Lingzhou", ""]]}, {"id": "2006.00517", "submitter": "Daisuke Hotta", "authors": "Daisuke Hotta and Yoichiro Ota", "title": "What limits the number of observations that can be effectively\n  assimilated by EnKF?", "comments": "52 pages, 10 figures; submitted to the Quarterly Journal of the Royal\n  Meteorological Society", "journal-ref": null, "doi": "10.1002/qj.3970", "report-no": null, "categories": "physics.data-an math.ST physics.ao-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of ensemble Kalman filter (EnKF) algorithms to extract\ninformation from observations is analyzed with the aid of the concept of the\ndegrees of freedom for signal (DFS). A simple mathematical argument shows that\nDFS for EnKF is bounded from above by the ensemble size, which entails that\nassimilating much more observations than the ensemble size automatically leads\nto DFS underestimation. Since DFS is a trace of the posterior error covariance\nmapped onto the normalized observation space, underestimated DFS implies\noverconfidence (underdispersion) in the analysis spread, which, in a cycled\ncontext, requires covariance inflation to be applied. The theory is then\nextended to cases where covariance localization schemes (either B-localization\nor R-localization) are applied to show how they alleviate the DFS\nunderestimation issue. These findings from mathematical argument are\ndemonstrated with a simple one-dimensional covariance model. Finally, the DFS\nconcept is used to form speculative arguments about how to interpret several\npuzzling features of LETKF previously reported in the literature such as why\nusing less observations can lead to better performance, when optimal\nlocalization scales tend to occur, and why covariance inflation methods based\non relaxation to prior information approach are particularly successful when\nobservations are inhomogeneously distributed. A presumably first application of\nDFS diagnostics to a quasi-operational global EnKF system is presented in\nAppendix.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 13:22:34 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Hotta", "Daisuke", ""], ["Ota", "Yoichiro", ""]]}, {"id": "2006.00636", "submitter": "Holger Dette", "authors": "Josua G\\\"osmann, Christina Stoehr, Johannes Heiny and Holger Dette", "title": "Sequential change point detection in high dimensional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change point detection in high dimensional data has found considerable\ninterest in recent years. Most of the literature either designs methodology for\na retrospective analysis, where the whole sample is already available when the\nstatistical inference begins, or considers online detection schemes controlling\nthe average time until a false alarm. This paper takes a different point of\nview and develops monitoring schemes for the online scenario, where high\ndimensional data arrives successively and the goal is to detect changes as fast\nas possible controlling at the same time the probability of a type I error of a\nfalse alarm. We develop a sequential procedure capable of detecting changes in\nthe mean vector of a successively observed high dimensional time series with\nspatial and temporal dependence. The statistical properties of the method are\nanalyzed in the case where both, thesample size and dimension tend to infinity.\nIn this scenario, it is shown that the new monitoring scheme has asymptotic\nlevel alpha under the null hypothesis of no change and is consistent under the\nalternative of a change in at least one component of the high dimensional mean\nvector. The approach is based on a new type of monitoring scheme for\none-dimensional data which turns out to be often more powerful than the usually\nused CUSUM and Page-CUSUM methods, and the component-wise statistics are\naggregated by the maximum statistic. For the analysis of the asymptotic\nproperties of our monitoring scheme we prove that the range of a Brownian\nmotion on a given interval is in the domain of attraction of the Gumbel\ndistribution, which is a result of independent interest in extreme value\ntheory. The finite sample properties of the new methodology are illustrated by\nmeans of a simulation study and in the analysis of a data example.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 23:17:06 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2020 12:31:49 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["G\u00f6smann", "Josua", ""], ["Stoehr", "Christina", ""], ["Heiny", "Johannes", ""], ["Dette", "Holger", ""]]}, {"id": "2006.00683", "submitter": "HaiYing Wang", "authors": "HaiYing Wang", "title": "Logistic Regression for Massive Data with Rare Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies binary logistic regression for rare events data, or\nimbalanced data, where the number of events (observations in one class, often\ncalled cases) is significantly smaller than the number of nonevents\n(observations in the other class, often called controls). We first derive the\nasymptotic distribution of the maximum likelihood estimator (MLE) of the\nunknown parameter, which shows that the asymptotic variance convergences to\nzero in a rate of the inverse of the number of the events instead of the\ninverse of the full data sample size. This indicates that the available\ninformation in rare events data is at the scale of the number of events instead\nof the full data sample size. Furthermore, we prove that under-sampling a small\nproportion of the nonevents, the resulting under-sampled estimator may have\nidentical asymptotic distribution to the full data MLE. This demonstrates the\nadvantage of under-sampling nonevents for rare events data, because this\nprocedure may significantly reduce the computation and/or data collection\ncosts. Another common practice in analyzing rare events data is to over-sample\n(replicate) the events, which has a higher computational cost. We show that\nthis procedure may even result in efficiency loss in terms of parameter\nestimation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 03:09:49 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Wang", "HaiYing", ""]]}, {"id": "2006.00704", "submitter": "Tudor Manole", "authors": "Tudor Manole, Nhat Ho", "title": "Uniform Convergence Rates for Maximum Likelihood Estimation under\n  Two-Component Gaussian Mixture Models", "comments": "Both authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive uniform convergence rates for the maximum likelihood estimator and\nminimax lower bounds for parameter estimation in two-component location-scale\nGaussian mixture models with unequal variances. We assume the mixing\nproportions of the mixture are known and fixed, but make no separation\nassumption on the underlying mixture components. A phase transition is shown to\nexist in the optimal parameter estimation rate, depending on whether or not the\nmixture is balanced. Key to our analysis is a careful study of the dependence\nbetween the parameters of location-scale Gaussian mixture models, as captured\nthrough systems of polynomial equalities and inequalities whose solution set\ndrives the rates we obtain. A simulation study illustrates the theoretical\nfindings of this work.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 04:13:48 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Manole", "Tudor", ""], ["Ho", "Nhat", ""]]}, {"id": "2006.00769", "submitter": "Thomas Royen", "authors": "Thomas Royen", "title": "Some improved Gaussian correlation inequalities for symmetrical\n  n-rectangles extended to some multivariate gamma distributions and some\n  further probability inequalities", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian correlation inequality (GCI) for symmetrical n-rectangles is\nimproved if the absolute components have a joint MTP2-distribution\n(multivariate totally positive of order 2). Inequalities of the here given type\nhold at least for all MTP2-probability measures on R^n or (0,infinity)^n with\neverywhere positive smooth densities. In particular, at least some infinitely\ndivisible multivariate chi-square distributions (gamma distributions in the\nsense of Krishnamoorthy and Parthasarathy) with any positive real \"degree of\nfreedom\" are shown to be MTP2. Moreover, further numerically calculable\nprobability inequalities for a broad class of multivariate gamma distributions\nare derived and a different improvement for inequalities of the GCI-type - and\nof a similar type with three instead of two groups of components - with more\nspecial correlation structures. The main idea behind these inequalities is to\nfind for a given correlation matrix with positive correlations a further\ncorrelation matrix with smaller correlations whose inverse is an M-matrix and\nwhere the corresponding multivariate gamma distribution function is numerically\navailable.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 07:58:03 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 17:26:26 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Royen", "Thomas", ""]]}, {"id": "2006.00840", "submitter": "Mathieu Gerber", "authors": "Pierre Alquier and Mathieu Gerber", "title": "Universal Robust Regression via Maximum Mean Discrepancy", "comments": "43 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many datasets are collected automatically, and are thus easily contaminated\nby outliers. In order to overcome this issue there was recently a regain of\ninterest in robust estimation. However, most robust estimation methods are\ndesigned for specific models. In regression, methods have been notably\ndeveloped for estimating the regression coefficients in generalized linear\nmodels, while some other approaches have been proposed e.g.\\ for robust\ninference in beta regression or in sample selection models. In this paper, we\npropose Maximum Mean Discrepancy optimization as a universal framework for\nrobust regression. We prove non-asymptotic error bounds, showing that our\nestimators are robust to Huber-type contamination. We also provide a\n(stochastic) gradient algorithm for computing these estimators, whose\nimplementation requires only to be able to sample from the model and to compute\nthe gradient of its log-likelihood function. We finally illustrate the proposed\napproach by a set of simulations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 10:34:18 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 14:04:48 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Alquier", "Pierre", ""], ["Gerber", "Mathieu", ""]]}, {"id": "2006.00952", "submitter": "Tao Zhang", "authors": "Tao Zhang, Kengo Kato and David Ruppert", "title": "Bootstrap inference for quantile-based modal regression", "comments": "78 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop uniform inference methods for the conditional mode\nbased on quantile regression. Specifically, we propose to estimate the\nconditional mode by minimizing the derivative of the estimated conditional\nquantile function defined by smoothing the linear quantile regression\nestimator, and develop two bootstrap methods, a novel pivotal bootstrap and the\nnonparametric bootstrap, for our conditional mode estimator. Building on\nhigh-dimensional Gaussian approximation techniques, we establish the validity\nof simultaneous confidence rectangles constructed from the two bootstrap\nmethods for the conditional mode. We also extend the preceding analysis to the\ncase where the dimension of the covariate vector is increasing with the sample\nsize. Finally, we conduct simulation experiments and a real data analysis using\nU.S. wage data to demonstrate the finite sample performance of our inference\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 13:59:36 GMT"}, {"version": "v2", "created": "Thu, 28 Jan 2021 18:20:59 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 22:59:30 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhang", "Tao", ""], ["Kato", "Kengo", ""], ["Ruppert", "David", ""]]}, {"id": "2006.01036", "submitter": "Yuexia Zhang", "authors": "Yuexia Zhang, Linbo Wang", "title": "Conditional Independence Beyond Domain Separability: Discussion of\n  Engelke and Hitz (2020)", "comments": "18 pages, Discussion of a paper which will be published in Journal of\n  the Royal Statistical Society: Series B (Statistical Methodology)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We congratulate Engelke and Hitz on a thought-provoking paper on graphical\nmodels for extremes. A key contribution of the paper is the introduction of a\nnovel definition of conditional independence for a multivariate Pareto\ndistribution. Here, we outline a proposal for independence and conditional\nindependence of general random variables whose support is a general set Omega\nin multidimensional real number space. Our proposal includes the authors'\ndefinition of conditional independence, and the analogous definition of\nindependence as special cases. By making our proposal independent of the\ncontext of extreme value theory, we highlight the importance of the authors'\ncontribution beyond this particular context.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 16:04:10 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Zhang", "Yuexia", ""], ["Wang", "Linbo", ""]]}, {"id": "2006.01073", "submitter": "Fanghui Liu", "authors": "Fanghui Liu, Lei Shi, Xiaolin Huang, Jie Yang and Johan A.K. Suykens", "title": "Analysis of Regularized Least Squares in Reproducing Kernel Krein Spaces", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the asymptotic properties of regularized least\nsquares with indefinite kernels in reproducing kernel Krein spaces (RKKS). By\nintroducing a bounded hyper-sphere constraint to such non-convex regularized\nrisk minimization problem, we theoretically demonstrate that this problem has a\nglobally optimal solution with a closed form on the sphere, which makes\napproximation analysis feasible in RKKS. Regarding to the original regularizer\ninduced by the indefinite inner product, we modify traditional error\ndecomposition techniques, prove convergence results for the introduced\nhypothesis error based on matrix perturbation theory, and derive learning rates\nof such regularized regression problem in RKKS. Under some conditions, the\nderived learning rates in RKKS are the same as that in reproducing kernel\nHilbert spaces (RKHS), which is actually the first work on approximation\nanalysis of regularized learning algorithms in RKKS.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 16:55:35 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 19:18:06 GMT"}], "update_date": "2020-11-26", "authors_parsed": [["Liu", "Fanghui", ""], ["Shi", "Lei", ""], ["Huang", "Xiaolin", ""], ["Yang", "Jie", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "2006.01191", "submitter": "Anton Skrobotov", "authors": "Rustam Ibragimov and Jihyun Kim and Anton Skrobotov", "title": "New robust inference for predictive regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two robust methods for testing hypotheses on unknown parameters of\npredictive regression models under heterogeneous and persistent volatility as\nwell as endogenous, persistent and/or fat-tailed regressors and errors. The\nproposed robust testing approaches are applicable both in the case of discrete\nand continuous time models. Both of the methods use the Cauchy estimator to\neffectively handle the problems of endogeneity, persistence and/or\nfat-tailedness in regressors and errors. The difference between our two methods\nis how the heterogeneous volatility is controlled. The first method relies on\nrobust t-statistic inference using group estimators of a regression parameter\nof interest proposed in Ibragimov and Muller, 2010. It is simple to implement,\nbut requires the exogenous volatility assumption. To relax the exogenous\nvolatility assumption, we propose another method which relies on the\nnonparametric correction of volatility. The proposed methods perform well\ncompared with widely used alternative inference procedures in terms of their\nfinite sample properties.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 18:27:56 GMT"}, {"version": "v2", "created": "Mon, 10 Aug 2020 13:10:02 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Ibragimov", "Rustam", ""], ["Kim", "Jihyun", ""], ["Skrobotov", "Anton", ""]]}, {"id": "2006.01212", "submitter": "Anton Skrobotov", "authors": "Rustam Ibragimov and Rasmus Pedersen and Anton Skrobotov", "title": "New Approaches to Robust Inference on Market (Non-)Efficiency,\n  Volatility Clustering and Nonlinear Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many financial and economic variables, including financial returns, exhibit\nnonlinear dependence, heterogeneity and heavy-tailedness. These properties may\nmake problematic the analysis of (non-)efficiency and volatility clustering in\neconomic and financial markets using traditional approaches that appeal to\nasymptotic normality of sample autocorrelation functions of returns and their\nsquares.\n  This paper presents new approaches to deal with the above problems. We\nprovide the results that motivate the use of measures of market\n(non-)efficiency and volatility clustering based on (small) powers of absolute\nreturns and their signed versions.\n  We further provide new approaches to robust inference on the measures in the\ncase of general time series, including GARCH-type processes. The approaches are\nbased on robust $t-$statistics tests and new results on their applicability are\npresented. In the approaches, parameter estimates (e.g., estimates of measures\nof nonlinear dependence) are computed for groups of data, and the inference is\nbased on $t-$statistics in the resulting group estimates. This results in valid\nrobust inference under heterogeneity and dependence assumptions satisfied in\nreal-world financial markets. Numerical results and empirical applications\nconfirm the advantages and wide applicability of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2020 19:25:23 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 20:11:32 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Ibragimov", "Rustam", ""], ["Pedersen", "Rasmus", ""], ["Skrobotov", "Anton", ""]]}, {"id": "2006.01350", "submitter": "Zejian Liu", "authors": "Zejian Liu and Meng Li", "title": "On the Estimation of Derivatives Using Plug-in KRR Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of estimating the derivatives of the regression\nfunction, which has a wide range of applications as a key nonparametric\nfunctional of unknown functions. Standard analysis may be tailored to specific\nderivative orders, and parameter tuning remains a daunting challenge\nparticularly for high-order derivatives. In this article, we propose a simple\nplug-in kernel ridge regression (KRR) estimator in nonparametric regression\nwith random design that is broadly applicable for multi-dimensional support and\narbitrary mixed-partial derivatives. We provide a non-asymptotic analysis to\nstudy the behavior of the proposed estimator, leading to two error bounds for a\ngeneral class of kernels under the strong $L_\\infty$ norm. In a concrete\nexample specialized to kernels with polynomially decaying eigenvalues, the\nproposed estimator recovers the minimax optimal rate up to a logarithmic factor\nfor estimating derivatives of functions in H\\\"older class. Interestingly, the\nproposed estimator achieves the optimal rate of convergence with the same\nchoice of tuning parameter for any order of derivatives. Hence, the proposed\nestimator enjoys a remarkable \\textit{plug-in property} for derivatives in that\nit automatically adapts to the order of derivatives to be estimated, enabling\neasy tuning in practice. Our simulation studies show favorable finite sample\nperformance of the proposed method relative to several existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 02:32:39 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 20:03:18 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Liu", "Zejian", ""], ["Li", "Meng", ""]]}, {"id": "2006.01631", "submitter": "Toby St. Clere Smithe", "authors": "Toby St. Clere Smithe", "title": "Bayesian Updates Compose Optically", "comments": "40 pages. v2: fix minor typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Bayes' rule tells us how to invert a causal process in order to update our\nbeliefs in light of new evidence. If the process is believed to have a complex\ncompositional structure, we may ask whether composing the inversions of the\ncomponent processes gives the same belief update as the inversion of the whole.\nWe answer this question affirmatively, showing that the relevant compositional\nstructure is precisely that of the lens pattern, and that we can think of\nBayesian inversion as a particular instance of a state-dependent morphism in a\ncorresponding fibred category. We define a general notion of (mixed) Bayesian\nlens, and discuss the (un)lawfulness of these lenses when their contravariant\ncomponents are exact Bayesian inversions. We prove our main result both\nabstractly and concretely, for both discrete and continuous states, taking care\nto illustrate the common structures.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 11:55:39 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2020 14:05:14 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Smithe", "Toby St. Clere", ""]]}, {"id": "2006.01662", "submitter": "Sheng Xu", "authors": "Sheng Xu, Zhou Fan, Sahand Negahban", "title": "Tree-Projected Gradient Descent for Estimating Gradient-Sparse\n  Parameters on Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study estimation of a gradient-sparse parameter vector\n$\\boldsymbol{\\theta}^* \\in \\mathbb{R}^p$, having strong gradient-sparsity\n$s^*:=\\|\\nabla_G \\boldsymbol{\\theta}^*\\|_0$ on an underlying graph $G$. Given\nobservations $Z_1,\\ldots,Z_n$ and a smooth, convex loss function $\\mathcal{L}$\nfor which $\\boldsymbol{\\theta}^*$ minimizes the population risk\n$\\mathbb{E}[\\mathcal{L}(\\boldsymbol{\\theta};Z_1,\\ldots,Z_n)]$, we propose to\nestimate $\\boldsymbol{\\theta}^*$ by a projected gradient descent algorithm that\niteratively and approximately projects gradient steps onto spaces of vectors\nhaving small gradient-sparsity over low-degree spanning trees of $G$. We show\nthat, under suitable restricted strong convexity and smoothness assumptions for\nthe loss, the resulting estimator achieves the squared-error risk\n$\\frac{s^*}{n} \\log (1+\\frac{p}{s^*})$ up to a multiplicative constant that is\nindependent of $G$. In contrast, previous polynomial-time algorithms have only\nbeen shown to achieve this guarantee in more specialized settings, or under\nadditional assumptions for $G$ and/or the sparsity pattern of $\\nabla_G\n\\boldsymbol{\\theta}^*$. As applications of our general framework, we apply our\nresults to the examples of linear models and generalized linear models with\nrandom design.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 20:08:13 GMT"}], "update_date": "2020-06-03", "authors_parsed": [["Xu", "Sheng", ""], ["Fan", "Zhou", ""], ["Negahban", "Sahand", ""]]}, {"id": "2006.01799", "submitter": "Olli Saarela", "authors": "Olli Saarela, David A. Stephens, Erica E. M. Moodie", "title": "The role of exchangeability in causal inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of exchangeability has been recognized in the causal inference\nliterature in various guises, but only rarely in the original Bayesian meaning\nas a symmetry property between individual units in statistical inference. Since\nthe latter is a standard ingredient in Bayesian inference, we argue that in\nBayesian causal inference it is natural to link the causal model, including the\nnotion of confounding and definition of causal contrasts of interest, to the\nconcept of exchangeability. Here we relate the Bayesian notion of\nexchangeability to alternative conditions for unconfounded inferences, commonly\nstated using potential outcomes, and define causal contrasts in the presence of\nexchangeability in terms of limits of posterior predictive expectations for\nfurther exchangeable units. While our main focus is in a point treatment\nsetting, we also investigate how this reasoning carries over to longitudinal\nsettings.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 17:33:33 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 02:03:15 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Saarela", "Olli", ""], ["Stephens", "David A.", ""], ["Moodie", "Erica E. M.", ""]]}, {"id": "2006.01986", "submitter": "Mohamed Ndaoud", "authors": "Stanislav Minsker, Mohamed Ndaoud", "title": "Robust and efficient mean estimation: approach based on the properties\n  of self-normalized sums", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X$ be a random variable with unknown mean and finite variance. We\npresent a new estimator of the mean of $X$ that is robust with respect to the\npossible presence of outliers in the sample, provides tight sub-Gaussian\ndeviation guarantees without any additional assumptions on the shape or tails\nof the distribution, and moreover is asymptotically efficient. This is the\nfirst estimator that provably combines all these qualities in one package. Our\nconstruction is inspired by robustness properties possessed by the\nself-normalized sums. Finally, theoretical findings are supplemented by\nnumerical simulations highlighting the strong performance of the proposed\nestimator in comparison with previously known techniques.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 23:59:38 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Minsker", "Stanislav", ""], ["Ndaoud", "Mohamed", ""]]}, {"id": "2006.02037", "submitter": "Caroline Wormell", "authors": "Caroline L. Wormell and Sebastian Reich", "title": "Spectral convergence of diffusion maps: improved error bounds and an\n  alternative normalisation", "comments": "Electronic copy of the final peer-reviewed manuscript accepted for\n  publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG cs.NA math.NA math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion maps is a manifold learning algorithm widely used for\ndimensionality reduction. Using a sample from a distribution, it approximates\nthe eigenvalues and eigenfunctions of associated Laplace-Beltrami operators.\nTheoretical bounds on the approximation error are however generally much weaker\nthan the rates that are seen in practice. This paper uses new approaches to\nimprove the error bounds in the model case where the distribution is supported\non a hypertorus. For the data sampling (variance) component of the error we\nmake spatially localised compact embedding estimates on certain Hardy spaces;\nwe study the deterministic (bias) component as a perturbation of the\nLaplace-Beltrami operator's associated PDE, and apply relevant spectral\nstability results. Using these approaches, we match long-standing pointwise\nerror bounds for both the spectral data and the norm convergence of the\noperator discretisation.\n  We also introduce an alternative normalisation for diffusion maps based on\nSinkhorn weights. This normalisation approximates a Langevin diffusion on the\nsample and yields a symmetric operator approximation. We prove that it has\nbetter convergence compared with the standard normalisation on flat domains,\nand present a highly efficient algorithm to compute the Sinkhorn weights.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 04:23:43 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 04:11:24 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 22:44:45 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Wormell", "Caroline L.", ""], ["Reich", "Sebastian", ""]]}, {"id": "2006.02044", "submitter": "Adityanand Guntuboyina", "authors": "Gil Kur, Fuchang Gao, Adityanand Guntuboyina and Bodhisattva Sen", "title": "Convex Regression in Multidimensions: Suboptimality of Least Squares\n  Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The least squares estimator (LSE) is shown to be suboptimal in squared error\nloss in the usual nonparametric regression model with Gaussian errors for $d\n\\geq 5$ for each of the following families of functions: (i) convex functions\nsupported on a polytope (in fixed design), (ii) bounded convex functions\nsupported on a polytope (in random design), and (iii) convex Lipschitz\nfunctions supported on any convex domain (in random design). For each of these\nfamilies, the risk of the LSE is proved to be of the order $n^{-2/d}$ (up to\nlogarithmic factors) while the minimax risk is $n^{-4/(d+4)}$, for $d \\ge 5$.\nIn addition, the first rate of convergence results (worst case and adaptive)\nfor the full convex LSE are established for polytopal domains for all $d \\geq\n1$. Some new metric entropy results for convex functions are also proved which\nare of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 04:57:05 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Kur", "Gil", ""], ["Gao", "Fuchang", ""], ["Guntuboyina", "Adityanand", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "2006.02087", "submitter": "Baptiste Broto", "authors": "Baptiste Broto (CEA), Fran\\c{c}ois Bachoc (GdR MASCOT-NUM), Marine\n  Depecker (LTCI), Jean-Marc Martinez (LMA)", "title": "Gaussian linear approximation for the estimation of the Shapley effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the estimation of the sensitivity indices called\n\"Shapley eects\". These sensitivity indices enable to handle dependent input\nvariables. The Shapley eects are generally dicult to estimate, but they are\neasily computable in the Gaussian linear framework. The aim of this work is to\nuse the values of the Shapley eects in an approximated Gaussian linear\nframework as estimators of the true Shapley eects corresponding to a non-linear\nmodel. First, we assume that the input variables are Gaussian with small\nvariances. We provide rates of convergence of the estimated Shapley eects to\nthe true Shapley eects. Then, we focus on the case where the inputs are given\nby an non-Gaussian empirical mean. We prove that, under some mild assumptions,\nwhen the number of terms in the empirical mean increases, the dierence between\nthe true Shapley eects and the estimated Shapley eects given by the Gaussian\nlinear approximation converges to 0. Our theoretical results are supported by\nnumerical studies, showing that the Gaussian linear approximation is accurate\nand enables to decrease the computational time signicantly.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 07:45:56 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Broto", "Baptiste", "", "CEA"], ["Bachoc", "Fran\u00e7ois", "", "GdR MASCOT-NUM"], ["Depecker", "Marine", "", "LTCI"], ["Martinez", "Jean-Marc", "", "LMA"]]}, {"id": "2006.02229", "submitter": "Philippe Berthet", "authors": "Philippe Berthet and John H.J. Einmahl", "title": "Cube root weak convergence of empirical estimators of a density level\n  set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given $n$ independent random vectors with common density $f$ on\n$\\mathbb{R}^d$, we study the weak convergence of three empirical-measure based\nestimators of the convex $\\lambda$-level set $L_\\lambda$ of $f$, namely the\nexcess mass set, the minimum volume set and the maximum probability set, all\nselected from a class of convex sets $\\mathcal{A}$ that contains $L_\\lambda$.\nSince these set-valued estimators approach $L_\\lambda$, even the formulation of\ntheir weak convergence is non-standard. We identify the joint limiting\ndistribution of the symmetric difference of $L_\\lambda$ and each of the three\nestimators, at rate $n^{-1/3}$. It turns out that the minimum volume set and\nthe maximum probability set estimators are asymptotically indistinguishable,\nwhereas the excess mass set estimator exhibits \"richer\" limit behavior.\nArguments rely on the boundary local empirical process, its cylinder\nrepresentation, dimension-free concentration around the boundary of\n$L_\\lambda$, and the set-valued argmax of a drifted Wiener process.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2020 08:54:31 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Berthet", "Philippe", ""], ["Einmahl", "John H. J.", ""]]}, {"id": "2006.02329", "submitter": "Vladimir Vovk", "authors": "Vladimir Vovk", "title": "Conformal e-prediction for change detection", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt conformal e-prediction to change detection, defining analogues of\nthe Shiryaev-Roberts and CUSUM procedures for detecting violations of the IID\nassumption. Asymptotically, the frequency of false alarms for these analogues\ndoes not exceed the usual bounds.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 15:19:58 GMT"}], "update_date": "2020-06-04", "authors_parsed": [["Vovk", "Vladimir", ""]]}, {"id": "2006.02397", "submitter": "Jordan Awan", "authors": "Jordan Awan and Zhanrui Cai", "title": "Approximate Co-Sufficient Sampling for Goodness-of-fit Tests and\n  Synthetic Data", "comments": "35 pages double spaced, before references", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CR stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Co-sufficient sampling refers to resampling the data conditional on a\nsufficient statistic, a useful technique for statistical problems such as\ngoodness-of-fit tests, model selection, and confidence interval construction;\nit is also a powerful tool to generate synthetic data which limits the\ndisclosure risk of sensitive data. However, sampling from such conditional\ndistributions is both technically and computationally challenging, and is\ninapplicable in models without low-dimensional sufficient statistics.\n  We study an indirect inference approach to approximate co-sufficient\nsampling, which only requires an efficient statistic rather than a sufficient\nstatistic. Given an efficient estimator, we prove that the expected KL\ndivergence goes to zero between the true conditional distribution and the\nresulting approximate distribution. We also propose a one-step approximate\nsolution to the optimization problem that preserves the original estimator with\nan error of $o_p(n^{-1/2})$, which suffices for asymptotic optimality. The\none-step method is easily implemented, highly computationally efficient, and\napplicable to a wide variety of models, only requiring the ability to sample\nfrom the model and compute an efficient statistic. We implement our methods via\nsimulations to tackle problems in synthetic data, hypothesis testing, and\ndifferential privacy.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 17:12:11 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2020 19:15:28 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 18:12:09 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Awan", "Jordan", ""], ["Cai", "Zhanrui", ""]]}, {"id": "2006.02509", "submitter": "Sinho Chewi", "authors": "Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, Philippe Rigollet", "title": "SVGD as a kernelized Wasserstein gradient flow of the chi-squared\n  divergence", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein Variational Gradient Descent (SVGD), a popular sampling algorithm, is\noften described as the kernelized gradient flow for the Kullback-Leibler\ndivergence in the geometry of optimal transport. We introduce a new perspective\non SVGD that instead views SVGD as the (kernelized) gradient flow of the\nchi-squared divergence which, we show, exhibits a strong form of uniform\nexponential ergodicity under conditions as weak as a Poincar\\'e inequality.\nThis perspective leads us to propose an alternative to SVGD, called Laplacian\nAdjusted Wasserstein Gradient Descent (LAWGD), that can be implemented from the\nspectral decomposition of the Laplacian operator associated with the target\ndensity. We show that LAWGD exhibits strong convergence guarantees and good\npractical performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 20:20:21 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Chewi", "Sinho", ""], ["Gouic", "Thibaut Le", ""], ["Lu", "Chen", ""], ["Maunu", "Tyler", ""], ["Rigollet", "Philippe", ""]]}, {"id": "2006.02568", "submitter": "Hengrui Luo", "authors": "Hengrui Luo, Steve N. MacEachern, Mario Peruggia", "title": "Asymptotics of Lower Dimensional Zero-Density Regions", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis (TDA) allows us to explore the topological features\nof a dataset. Among topological features, lower dimensional ones have recently\ndrawn the attention of practitioners in mathematics and statistics due to their\npotential to aid the discovery of low dimensional structure in a data set.\nHowever, lower dimensional features are usually challenging to detect from a\nprobabilistic perspective.\n  In this paper, lower dimensional topological features occurring as\nzero-density regions of density functions are introduced and thoroughly\ninvestigated. Specifically, we consider sequences of coverings for the support\nof a density function in which the coverings are comprised of balls with\nshrinking radii. We show that, when these coverings satisfy certain sufficient\nconditions as the sample size goes to infinity, we can detect lower\ndimensional, zero-density regions with increasingly higher probability while\nguarding against false detection. We supplement the theoretical developments\nwith the discussion of simulated experiments that elucidate the behavior of the\nmethodology for different choices of the tuning parameters that govern the\nconstruction of the covering sequences and characterize the asymptotic results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 22:54:33 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Luo", "Hengrui", ""], ["MacEachern", "Steve N.", ""], ["Peruggia", "Mario", ""]]}, {"id": "2006.02572", "submitter": "Hicham Janati", "authors": "Hicham Janati, Boris Muzellec, Gabriel Peyr\\'e, Marco Cuturi", "title": "Entropic Optimal Transport between Unbalanced Gaussian Measures has a\n  Closed Form", "comments": null, "journal-ref": "Thirty-fourth Conference on Neural Information Processing Systems\n  2020", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although optimal transport (OT) problems admit closed form solutions in a\nvery few notable cases, e.g. in 1D or between Gaussians, these closed forms\nhave proved extremely fecund for practitioners to define tools inspired from\nthe OT geometry. On the other hand, the numerical resolution of OT problems\nusing entropic regularization has given rise to many applications, but because\nthere are no known closed-form solutions for entropic regularized OT problems,\nthese approaches are mostly algorithmic, not informed by elegant closed forms.\nIn this paper, we propose to fill the void at the intersection between these\ntwo schools of thought in OT by proving that the entropy-regularized optimal\ntransport problem between two Gaussian measures admits a closed form. Contrary\nto the unregularized case, for which the explicit form is given by the\nWasserstein-Bures distance, the closed form we obtain is differentiable\neverywhere, even for Gaussians with degenerate covariance matrices. We obtain\nthis closed form solution by solving the fixed-point equation behind Sinkhorn's\nalgorithm, the default method for computing entropic regularized OT.\nRemarkably, this approach extends to the generalized unbalanced case -- where\nGaussian measures are scaled by positive constants. This extension leads to a\nclosed form expression for unbalanced Gaussians as well, and highlights the\nmass transportation / destruction trade-off seen in unbalanced optimal\ntransport. Moreover, in both settings, we show that the optimal transportation\nplans are (scaled) Gaussians and provide analytical formulas of their\nparameters. These formulas constitute the first non-trivial closed forms for\nentropy-regularized optimal transport, thus providing a ground truth for the\nanalysis of entropic OT and Sinkhorn's algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 23:00:27 GMT"}, {"version": "v2", "created": "Mon, 14 Dec 2020 10:51:40 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Janati", "Hicham", ""], ["Muzellec", "Boris", ""], ["Peyr\u00e9", "Gabriel", ""], ["Cuturi", "Marco", ""]]}, {"id": "2006.02643", "submitter": "Lele Wang", "authors": "Alankrita Bhatt, Ziao Wang, Chi Wang, Lele Wang", "title": "Universal Graph Compression: Stochastic Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.DB math.IT math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the prevalent data science applications of processing and mining\nlarge-scale graph data such as social networks, web graphs, and biological\nnetworks, as well as the high I/O and communication costs of storing and\ntransmitting such data, this paper investigates lossless compression of data\nappearing in the form of a labeled graph. A universal graph compression scheme\nis proposed, which does not depend on the underlying statistics/distribution of\nthe graph model. For graphs generated by a stochastic block model, which is a\nwidely used random graph model capturing the clustering effects in social\nnetworks, the proposed scheme achieves the optimal theoretical limit of\nlossless compression without the need to know edge probabilities, community\nlabels, or the number of communities.\n  The key ideas in establishing universality for stochastic block models\ninclude: 1) block decomposition of the adjacency matrix of the graph; 2)\ngeneralization of the Krichevsky-Trofimov probability assignment, which was\ninitially designed for i.i.d. random processes. In four benchmark graph\ndatasets (protein-to-protein interaction, LiveJournal friendship, Flickr, and\nYouTube), the compressed files from competing algorithms (including CSR,\nLigra+, PNG image compressor, and Lempel-Ziv compressor for two-dimensional\ndata) take 2.4 to 27 times the space needed by the proposed scheme.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 04:51:26 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 02:12:38 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Bhatt", "Alankrita", ""], ["Wang", "Ziao", ""], ["Wang", "Chi", ""], ["Wang", "Lele", ""]]}, {"id": "2006.02667", "submitter": "Annika Betken", "authors": "Annika Betken, Davide Giraudo, Rafa{\\l} Kulik", "title": "Change-point tests for the tail parameter of Long Memory Stochastic\n  Volatility time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a change-point test based on the Hill estimator to test for\nstructural changes in the tail index of Long Memory Stochastic Volatility time\nseries. In order to determine the asymptotic distribution of the corresponding\ntest statistic, we prove a uniform reduction principle for the tail empirical\nprocess in a two-parameter Skorohod space. It is shown that such a process\ndisplays a dichotomous behavior according to an interplay between the Hurst\nparameter, i.e., a parameter characterizing the dependence in the data, and the\ntail index. Our theoretical results are accompanied by simulation studies and\nthe analysis of financial time series with regard to structural changes in the\ntail index.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 06:49:20 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Betken", "Annika", ""], ["Giraudo", "Davide", ""], ["Kulik", "Rafa\u0142", ""]]}, {"id": "2006.02705", "submitter": "Jack Noonan", "authors": "Jack Noonan and Anatoly Zhigljavsky", "title": "Non-lattice covering and quanitization of high dimensional sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main problem considered in this paper is construction and theoretical\nstudy of efficient $n$-point coverings of a $d$-dimensional cube $[-1,1]^d$.\nTargeted values of $d$ are between 5 and 50; $n$ can be in hundreds or\nthousands and the designs (collections of points) are nested. This paper is a\ncontinuation of our paper \\cite{us}, where we have theoretically investigated\nseveral simple schemes and numerically studied many more. In this paper, we\nextend the theoretical constructions of \\cite{us} for studying the designs\nwhich were found to be superior to the ones theoretically investigated in\n\\cite{us}. We also extend our constructions for new construction schemes which\nprovide even better coverings (in the class of nested designs) than the ones\nnumerically found in \\cite{us}. In view of a close connection of the problem of\nquantization to the problem of covering, we extend our theoretical\napproximations and practical recommendations to the problem of construction of\nefficient quantization designs in a cube $[-1,1]^d$. In the last section, we\ndiscuss the problems of covering and quantization in a $d$-dimensional simplex;\npractical significance of this problem has been communicated to the authors by\nProfessor Michael Vrahatis, a co-editor of the present volume.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 08:53:51 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Noonan", "Jack", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "2006.02745", "submitter": "Remi Leluc", "authors": "R\\'emi Leluc and Fran\\c{c}ois Portier", "title": "Asymptotic Optimality of Conditioned Stochastic Gradient Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a general class of stochastic gradient descent\n(SGD) algorithms, called conditioned SGD, based on a preconditioning of the\ngradient direction. Under some mild assumptions, namely the $L$-smoothness of\nthe non-convex objective function and some weak growth condition on the noise,\nwe establish the almost sure convergence and the asymptotic normality for a\nbroad class of conditioning matrices. In particular, when the conditioning\nmatrix is an estimate of the inverse Hessian at the optimal point, the\nalgorithm is proved to be asymptotically optimal. The benefits of this approach\nare validated on simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 10:08:05 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 08:53:50 GMT"}, {"version": "v3", "created": "Fri, 9 Jul 2021 14:43:08 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Leluc", "R\u00e9mi", ""], ["Portier", "Fran\u00e7ois", ""]]}, {"id": "2006.02765", "submitter": "Matthew Thorpe", "authors": "Jeff Calder, Dejan Slep\\v{c}ev and Matthew Thorpe", "title": "Rates of Convergence for Laplacian Semi-Supervised Learning with Low\n  Labeling Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study graph-based Laplacian semi-supervised learning at low labeling\nrates. Laplacian learning uses harmonic extension on a graph to propagate\nlabels. At very low label rates, Laplacian learning becomes degenerate and the\nsolution is roughly constant with spikes at each labeled data point. Previous\nwork has shown that this degeneracy occurs when the number of labeled data\npoints is finite while the number of unlabeled data points tends to infinity.\nIn this work we allow the number of labeled data points to grow to infinity\nwith the number of labels. Our results show that for a random geometric graph\nwith length scale $\\varepsilon>0$ and labeling rate $\\beta>0$, if $\\beta\n\\ll\\varepsilon^2$ then the solution becomes degenerate and spikes form, and if\n$\\beta\\gg \\varepsilon^2$ then Laplacian learning is well-posed and consistent\nwith a continuum Laplace equation. Furthermore, in the well-posed setting we\nprove quantitative error estimates of $O(\\varepsilon\\beta^{-1/2})$ for the\ndifference between the solutions of the discrete problem and continuum PDE, up\nto logarithmic factors. We also study $p$-Laplacian regularization and show the\nsame degeneracy result when $\\beta \\ll \\varepsilon^p$. The proofs of our\nwell-posedness results use the random walk interpretation of Laplacian learning\nand PDE arguments, while the proofs of the ill-posedness results use\n$\\Gamma$-convergence tools from the calculus of variations. We also present\nnumerical results on synthetic and real data to illustrate our results.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 10:46:01 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Calder", "Jeff", ""], ["Slep\u010dev", "Dejan", ""], ["Thorpe", "Matthew", ""]]}, {"id": "2006.02806", "submitter": "Julia Gaudio", "authors": "David Gamarnik and Julia Gaudio", "title": "Estimation of Monotone Multi-Index Models", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a multi-index model with $k$ index vectors, the input variables are\ntransformed by taking inner products with the index vectors. A transfer\nfunction $f: \\mathbb{R}^k \\to \\mathbb{R}$ is applied to these inner products to\ngenerate the output. Thus, multi-index models are a generalization of linear\nmodels. In this paper, we consider monotone multi-index models. Namely, the\ntransfer function is assumed to be coordinate-wise monotone. The monotone\nmulti-index model therefore generalizes both linear regression and isotonic\nregression, which is the estimation of a coordinate-wise monotone function. We\nconsider the case of nonnegative index vectors. We provide an algorithm based\non integer programming for the estimation of monotone multi-index models, and\nprovide guarantees on the $L_2$ loss of the estimated function relative to the\nground truth.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 12:10:53 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Gamarnik", "David", ""], ["Gaudio", "Julia", ""]]}, {"id": "2006.03041", "submitter": "Yuxin Chen", "authors": "Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, Yuxin Chen", "title": "Sample Complexity of Asynchronous Q-Learning: Sharper Analysis and\n  Variance Reduction", "comments": "accepted in part to Neural Information Processing Systems (NeurIPS)\n  2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG eess.SP math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asynchronous Q-learning aims to learn the optimal action-value function (or\nQ-function) of a Markov decision process (MDP), based on a single trajectory of\nMarkovian samples induced by a behavior policy. Focusing on a\n$\\gamma$-discounted MDP with state space $\\mathcal{S}$ and action space\n$\\mathcal{A}$, we demonstrate that the $\\ell_{\\infty}$-based sample complexity\nof classical asynchronous Q-learning -- namely, the number of samples needed to\nyield an entrywise $\\varepsilon$-accurate estimate of the Q-function -- is at\nmost on the order of \\begin{equation*}\n\\frac{1}{\\mu_{\\mathsf{min}}(1-\\gamma)^5\\varepsilon^2}+\n\\frac{t_{\\mathsf{mix}}}{\\mu_{\\mathsf{min}}(1-\\gamma)} \\end{equation*} up to\nsome logarithmic factor, provided that a proper constant learning rate is\nadopted. Here, $t_{\\mathsf{mix}}$ and $\\mu_{\\mathsf{min}}$ denote respectively\nthe mixing time and the minimum state-action occupancy probability of the\nsample trajectory. The first term of this bound matches the complexity in the\ncase with independent samples drawn from the stationary distribution of the\ntrajectory. The second term reflects the expense taken for the empirical\ndistribution of the Markovian trajectory to reach a steady state, which is\nincurred at the very beginning and becomes amortized as the algorithm runs.\nEncouragingly, the above bound improves upon the state-of-the-art result by a\nfactor of at least $|\\mathcal{S}||\\mathcal{A}|$. Further, the scaling on the\ndiscount complexity can be improved by means of variance reduction.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 17:51:00 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2020 16:17:35 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Li", "Gen", ""], ["Wei", "Yuting", ""], ["Chi", "Yuejie", ""], ["Gu", "Yuantao", ""], ["Chen", "Yuxin", ""]]}, {"id": "2006.03283", "submitter": "Yi Yu", "authors": "Yi Yu and Oscar Hernan Madrid Padilla and Daren Wang and Alessandro\n  Rinaldo", "title": "A Note on Online Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate sequential change point estimation and detection in univariate\nnonparametric settings, where a stream of independent observations from\nsub-Gaussian distributions with a common variance factor and piecewise-constant\nbut otherwise unknown means are collected. We develop a simple CUSUM-based\nmethodology that provably control the probability of false alarms or the\naverage run length while minimizing, in a minimax sense, the detection delay.\nWe allow for all the model parameters to vary in order to capture a broad range\nof levels of statistical hardness for the problem at hand. We further show how\nour methodology is applicable to the case in which multiple change points are\nto be estimated sequentially.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 08:06:26 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 18:25:25 GMT"}, {"version": "v3", "created": "Fri, 13 Nov 2020 10:42:40 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Yu", "Yi", ""], ["Padilla", "Oscar Hernan Madrid", ""], ["Wang", "Daren", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "2006.03288", "submitter": "Shaogao Lv", "authors": "Yifan Xia, Yongchao Hou, Shaogao Lv", "title": "Learning rates for partially linear support vector machine in high\n  dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes a new regularized learning scheme for high dimensional\npartially linear support vector machine. The proposed approach consists of an\nempirical risk and the Lasso-type penalty for linear part, as well as the\nstandard functional norm for nonlinear part. Here the linear kernel is used for\nmodel interpretation and feature selection, while the nonlinear kernel is\nadopted to enhance algorithmic flexibility. In this paper, we develop a new\ntechnical analysis on the weighted empirical process, and establish the sharp\nlearning rates for the semi-parametric estimator under the regularized\nconditions. Specially, our derived learning rates for semi-parametric SVM\ndepend on not only the sample size and the functional complexity, but also the\nsparsity and the margin parameters.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 08:10:35 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Xia", "Yifan", ""], ["Hou", "Yongchao", ""], ["Lv", "Shaogao", ""]]}, {"id": "2006.03311", "submitter": "Ilya Soloveychik", "authors": "Ilya Soloveychik", "title": "Reliable Covariance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance or scatter matrix estimation is ubiquitous in most modern\nstatistical and machine learning applications. The task becomes especially\nchallenging since most real-world datasets are essentially non-Gaussian. The\ndata is often contaminated by outliers and/or has heavy-tailed distribution\ncausing the sample covariance to behave very poorly and calling for robust\nestimation methodology. The natural framework for the robust scatter matrix\nestimation is based on elliptical populations. Here, Tyler's estimator stands\nout by being distribution-free within the elliptical family and easy to\ncompute. The existing works thoroughly study the performance of Tyler's\nestimator assuming ellipticity but without providing any tools to verify this\nassumption when the covariance is unknown in advance. We address the following\nopen question: Given the sampled data and having no prior on the data\ngenerating process, how to assess the quality of the scatter matrix estimator?\nIn this work we show that this question can be reformulated as an asymptotic\nuniformity test for certain sequences of exchangeable vectors on the unit\nsphere. We develop a consistent and easily applicable goodness-of-fit test\nagainst all alternatives to ellipticity when the scatter matrix is unknown. The\nfindings are supported by numerical simulations demonstrating the power of the\nsuggest technique.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 08:51:16 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 12:47:03 GMT"}, {"version": "v3", "created": "Fri, 3 Jul 2020 16:06:25 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Soloveychik", "Ilya", ""]]}, {"id": "2006.03378", "submitter": "Gilles Stoltz", "authors": "H\\'edi Hadiji (LMO, CELESTE), Gilles Stoltz (LMO, CELESTE)", "title": "Adaptation to the Range in $K$-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic bandit problems with $K$ arms, each associated with a\nbounded distribution supported on the range $[m,M]$. We do not assume that the\nrange $[m,M]$ is known and show that there is a cost for learning this range.\nIndeed, a new trade-off between distribution-dependent and distribution-free\nregret bounds arises, which prevents from simultaneously achieving the typical\n$\\ln T$ and \\smash{$\\sqrt{T}$} bounds. For instance, a \\smash{$\\sqrt{T}$}\ndistribution-free regret bound may only be achieved if the\ndistribution-dependent regret bounds are at least of order \\smash{$\\sqrt{T}$}.\nWe exhibit a strategy achieving the rates for regret indicated by the new\ntrade-off.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 11:26:35 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2020 08:56:39 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Hadiji", "H\u00e9di", "", "LMO, CELESTE"], ["Stoltz", "Gilles", "", "LMO, CELESTE"]]}, {"id": "2006.03452", "submitter": "Guillaume Kon Kam King", "authors": "Guillaume Kon Kam King, Omiros Papaspiliopoulos, Matteo Ruggiero", "title": "Exact inference for a class of non-linear hidden Markov models on\n  general state spaces", "comments": "39 pages, 10 figures in main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact inference for hidden Markov models requires the evaluation of all\ndistributions of interest - filtering, prediction, smoothing and likelihood -\nwith a finite computational effort. This article provides sufficient conditions\nfor exact inference for a class of hidden Markov models on general state spaces\ngiven a set of discretely collected indirect observations linked non linearly\nto the signal, and a set of practical algorithms for inference. The conditions\nwe obtain are concerned with the existence of a certain type of dual process,\nwhich is an auxiliary process embedded in the time reversal of the signal, that\nin turn allows to represent the distributions and functions of interest as\nfinite mixtures of elementary densities or products thereof. We describe\nexplicitly how to update recursively the parameters involved, yielding\nqualitatively similar results to those obtained with Baum--Welch filters on\nfinite state spaces. We then provide practical algorithms for implementing the\nrecursions, as well as approximations thereof via an informed pruning of the\nmixtures, and we show superior performance to particle filters both in accuracy\nand computational efficiency. The code for optimal filtering, smoothing and\nparameter inference is made available in the Julia package\nDualOptimalFiltering.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 13:48:57 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 13:29:30 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["King", "Guillaume Kon Kam", ""], ["Papaspiliopoulos", "Omiros", ""], ["Ruggiero", "Matteo", ""]]}, {"id": "2006.03572", "submitter": "Yi Yu", "authors": "Daren Wang and Yi Yu and Rebecca Willett", "title": "Detecting Abrupt Changes in High-Dimensional Self-Exciting Poisson\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional self-exciting point processes have been widely used in many\napplication areas to model discrete event data in which past and current events\naffect the likelihood of future events. In this paper, we are concerned with\ndetecting abrupt changes of the coefficient matrices in discrete-time\nhigh-dimensional self-exciting Poisson processes, which have yet to be studied\nin the existing literature due to both theoretical and computational challenges\nrooted in the non-stationary and high-dimensional nature of the underlying\nprocess. We propose a penalized dynamic programming approach which is supported\nby a theoretical rate analysis and numerical evidence.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 17:36:37 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Wang", "Daren", ""], ["Yu", "Yi", ""], ["Willett", "Rebecca", ""]]}, {"id": "2006.03959", "submitter": "Mayya Zhilova", "authors": "Mayya Zhilova", "title": "New Edgeworth-type expansions with finite sample guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish higher-order expansions for a difference between probability\ndistributions of sums of i.i.d. random vectors in a Euclidean space. The\nderived bounds are uniform over two classes of sets: the set of all Euclidean\nballs and the set of all half-spaces. These results allow to account for an\nimpact of higher-order moments or cumulants of the considered distributions;\nthe obtained error terms depend on a sample size and a dimension explicitly.\nThe new inequalities outperform accuracy of the normal approximation in\nexisting Berry--Esseen inequalities under very general conditions. For\nsymmetrically distributed random summands, the obtained results are optimal in\nterms of the ratio between the dimension and the sample size. Using the new\nhigher-order inequalities, we study accuracy of the nonparametric bootstrap\napproximation and propose a bootstrap score test under possible model\nmisspecification. The proposed results include also explicit error bounds for\ngeneral elliptical confidence regions for an expected value of the random\nsummands, and optimality of the Gaussian anti-concentration inequality over the\nset of all Euclidean balls.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 20:16:17 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2020 12:38:20 GMT"}, {"version": "v3", "created": "Mon, 7 Dec 2020 13:19:12 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zhilova", "Mayya", ""]]}, {"id": "2006.03989", "submitter": "Jon A. Wellner", "authors": "Nilanjana Laha, Zhen Miao, and Jon A. Wellner", "title": "Bi-$s^*$-Concave Distributions", "comments": "68 pages, 24 figures; replaces and extends arXiv:2006.03989 by Laha,\n  Miao, and Wellner", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new shape-constrained classes of distribution functions on R,\nthe bi-$s^*$-concave classes. In parallel to results of D\\\"umbgen, Kolesnyk,\nand Wilke (2017) for what they called the class of bi-log-concave distribution\nfunctions, we show that every $s$-concave density $f$ has a bi-$s^*$-concave\ndistribution function $F$ for $s^*\\leq s/(s+1)$. Confidence bands building on\nexisting nonparametric bands, but accounting for the shape constraint of\nbi-$s^*$-concavity, are also considered. The new bands extend those developed\nby D\\\"umbgen et al. (2017) for the constraint of bi-log-concavity. We also make\nconnections between bi-$s^*$-concavity and finiteness of the Cs\\\"org\\H{o} -\nR\\'ev\\'esz constant of $F$ which plays an important role in the theory of\nquantile processes.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 22:12:51 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 16:03:23 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Laha", "Nilanjana", ""], ["Miao", "Zhen", ""], ["Wellner", "Jon A.", ""]]}, {"id": "2006.04024", "submitter": "Myung Geun Kim", "authors": "Myung Geun Kim", "title": "Sources of high leverage in linear regression model", "comments": null, "journal-ref": "This paper was published in Journal of Applied Mathematics and\n  Computing, Vol 16, pp.509-513 (2004)", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some reasons for high leverage are analytically investigated by decomposing\nleverage into meaningful components. The results in this work can be used for\nremedial action as a next step of data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 02:39:38 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Kim", "Myung Geun", ""]]}, {"id": "2006.04046", "submitter": "Gil Kur", "authors": "Gil Kur, Alexander Rakhlin and Adityanand Guntuboyina", "title": "On Suboptimality of Least Squares with Application to Estimation of\n  Convex Bodies", "comments": "To appaer in Conference on Learning Theory (COLT) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CG cs.LG math.MG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a technique for establishing lower bounds on the sample complexity\nof Least Squares (or, Empirical Risk Minimization) for large classes of\nfunctions. As an application, we settle an open problem regarding optimality of\nLeast Squares in estimating a convex set from noisy support function\nmeasurements in dimension $d\\geq 6$. Specifically, we establish that Least\nSquares is mimimax sub-optimal, and achieves a rate of\n$\\tilde{\\Theta}_d(n^{-2/(d-1)})$ whereas the minimax rate is\n$\\Theta_d(n^{-4/(d+3)})$.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 05:19:00 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Kur", "Gil", ""], ["Rakhlin", "Alexander", ""], ["Guntuboyina", "Adityanand", ""]]}, {"id": "2006.04052", "submitter": "Fumiyasu Komaki", "authors": "Fumiyasu Komaki", "title": "Shrinkage priors for nonparametric Bayesian prediction of nonhomogeneous\n  Poisson processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric Bayesian estimation and prediction for\nnonhomogeneous Poisson process models with unknown intensity functions. We\npropose a class of improper priors for intensity functions. Nonparametric\nBayesian inference with kernel mixture based on the class improper priors is\nshown to be useful, although improper priors have not been widely used for\nnonparametric Bayes problems. Several theorems corresponding to those for\nfinite-dimensional independent Poisson models hold for nonhomogeneous Poisson\nprocess models with infinite-dimensional parameter spaces. Bayesian estimation\nand prediction based on the improper priors are shown to be admissible under\nthe Kullback--Leibler loss. Numerical methods for Bayesian inference based on\nthe priors are investigated.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 05:55:47 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 13:39:59 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Komaki", "Fumiyasu", ""]]}, {"id": "2006.04188", "submitter": "Graciela Boente Prof.", "authors": "Juan Lucas Bali and Graciela Boente", "title": "Principal points and elliptical distributions from the multivariate\n  setting to the functional case", "comments": null, "journal-ref": "Statistics and Probability Letters (2009), 79, 1858-1865", "doi": "10.1016/j.spl.2009.05.016", "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $k$ principal points of a random vector $\\mathbf{X}$ are defined as a set\nof points which minimize the expected squared distance between $\\mathbf{X}$ and\nthe nearest point in the set. They are thoroughly studied in Flury (1990,\n1993), Tarpey (1995) and Tarpey, Li and Flury (1995). For their treatment, the\nexamination is usually restricted to the family of elliptical distributions. In\nthis paper, we present an extension of the previous results to the functional\nelliptical distribution case, i.e., when dealing with random elements over a\nseparable Hilbert space ${\\cal H}$. Principal points for gaussian processes\nwere defined in Tarpey and Kinateder (2003). In this paper, we generalize the\nconcepts of principal points, self-consistent points and elliptical\ndistributions so as to fit them in this functional framework. Results linking\nself-consistency and the eigenvectors of the covariance operator are\nre-obtained in this new setting as well as an explicit formula for the $k=2$\ncase so as to include elliptically distributed random elements in ${\\cal H}$.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 16:11:41 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Bali", "Juan Lucas", ""], ["Boente", "Graciela", ""]]}, {"id": "2006.04215", "submitter": "Patrick Michl", "authors": "Patrick Michl", "title": "A Generalization of the Pearson Correlation to Riemannian Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.DG stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The increasing application of deep-learning is accompanied by a shift towards\nhighly non-linear statistical models. In terms of their geometry it is natural\nto identify these models with Riemannian manifolds. The further analysis of the\nstatistical models therefore raises the issue of a correlation measure, that in\nthe cutting planes of the tangent spaces equals the respective Pearson\ncorrelation and extends to a correlation measure that is normalized with\nrespect to the underlying manifold. In this purpose the article reconstitutes\nelementary properties of the Pearson correlation to successively derive a\nlinear generalization to multiple dimensions and thereupon a nonlinear\ngeneralization to principal manifolds, given by the Riemann-Pearson\nCorrelation.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 18:06:04 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 15:06:19 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Michl", "Patrick", ""]]}, {"id": "2006.04266", "submitter": "Jason Klusowski M", "authors": "Jason M. Klusowski", "title": "Sparse learning with CART", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision trees with binary splits are popularly constructed using\nClassification and Regression Trees (CART) methodology. For regression models,\nthis approach recursively divides the data into two near-homogenous daughter\nnodes according to a split point that maximizes the reduction in sum of squares\nerror (the impurity) along a particular variable. This paper aims to study the\nstatistical properties of regression trees constructed with CART methodology.\nIn doing so, we find that the training error is governed by the Pearson\ncorrelation between the optimal decision stump and response data in each node,\nwhich we bound by constructing a prior distribution on the split points and\nsolving a nonlinear optimization problem. We leverage this connection between\nthe training error and Pearson correlation to show that CART with\ncost-complexity pruning achieves an optimal complexity/goodness-of-fit tradeoff\nwhen the depth scales with the logarithm of the sample size. Data dependent\nquantities, which adapt to the dimensionality and latent structure of the\nregression model, are seen to govern the rates of convergence of the prediction\nerror.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 20:55:52 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2020 21:42:04 GMT"}], "update_date": "2020-11-20", "authors_parsed": [["Klusowski", "Jason M.", ""]]}, {"id": "2006.04347", "submitter": "Ian Waudby-Smith", "authors": "Ian Waudby-Smith, Aaditya Ramdas", "title": "Confidence sequences for sampling without replacement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many practical tasks involve sampling sequentially without replacement (WoR)\nfrom a finite population of size $N$, in an attempt to estimate some parameter\n$\\theta^\\star$. Accurately quantifying uncertainty throughout this process is a\nnontrivial task, but is necessary because it often determines when we stop\ncollecting samples and confidently report a result. We present a suite of tools\nfor designing confidence sequences (CS) for $\\theta^\\star$. A CS is a sequence\nof confidence sets $(C_n)_{n=1}^N$, that shrink in size, and all contain\n$\\theta^\\star$ simultaneously with high probability. We present a generic\napproach to constructing a frequentist CS using Bayesian tools, based on the\nfact that the ratio of a prior to the posterior at the ground truth is a\nmartingale. We then present Hoeffding- and empirical-Bernstein-type\ntime-uniform CSs and fixed-time confidence intervals for sampling WoR, which\nimprove on previous bounds in the literature and explicitly quantify the\nbenefit of WoR sampling.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 04:30:25 GMT"}, {"version": "v2", "created": "Thu, 22 Oct 2020 18:31:11 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2021 02:42:04 GMT"}, {"version": "v4", "created": "Fri, 8 Jan 2021 15:45:00 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Waudby-Smith", "Ian", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2006.04454", "submitter": "Arindam Panja", "authors": "Arindam Panja, Pradip Kundu, Biswabrata Pradhan", "title": "Variability and skewness ordering of sample extremes from dependent\n  random variables following the proportional odds model", "comments": "8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proportional odds (PO) model not only capable of generating new family of\nflexible distributions but also is a very important model in reliability theory\nand survival analysis. In this study, we investigate comparisons of minimums as\nwell as maximums of samples from dependent random variables following the PO\nmodel and with Archimedean copulas, in terms of dispersive and star orders.\nNumerical examples are provided to illustrate the the findings.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 10:17:53 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Panja", "Arindam", ""], ["Kundu", "Pradip", ""], ["Pradhan", "Biswabrata", ""]]}, {"id": "2006.04499", "submitter": "Julio Stern", "authors": "Marcio Alves Diniz, Carlos Alberto de Braganca Pereira, Julio Michael\n  Stern", "title": "Cointegration and unit root tests: A fully Bayesian approach", "comments": null, "journal-ref": "Entropy 2020, 22(9), 968, 1-23", "doi": "10.3390/e22090968", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform statistical inference for time series, one should be able to\nassess if they present deterministic or stochastic trends. For univariate\nanalysis one way to detect stochastic trends is to test if the series has unit\nroots, and for multivariate studies it is often relevant to search for\nstationary linear relationships between the series, or if they cointegrate. The\nmain goal of this article is to briefly review the shortcomings of unit root\nand cointegration tests proposed by the Bayesian approach of statistical\ninference and to show how they can be overcome by the fully Bayesian\nsignificance test (FBST), a procedure designed to test sharp or precise\nhypothesis. We will compare its performance with the most used frequentist\nalternatives, namely, the Augmented Dickey-Fuller for unit roots and the\nmaximum eigenvalue test for cointegration. Keywords: Time series; Bayesian\ninference; Hypothesis testing; Unit root; Cointegration.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 12:09:16 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 19:36:25 GMT"}, {"version": "v3", "created": "Sat, 8 Aug 2020 18:32:49 GMT"}, {"version": "v4", "created": "Sat, 12 Sep 2020 10:25:41 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Diniz", "Marcio Alves", ""], ["Pereira", "Carlos Alberto de Braganca", ""], ["Stern", "Julio Michael", ""]]}, {"id": "2006.04630", "submitter": "Hiroki Masuda", "authors": "Hiroki Masuda", "title": "Optimal stable Ornstein-Uhlenbeck regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove some efficient inference results concerning estimation of a\nOrnstein-Uhlenbeck regression model, which is driven by a non-Gaussian stable\nLevy process and where the output process is observed at high-frequency over a\nfixed time period. Local asymptotics for the likelihood function is presented,\nfollowed by a way to construct an asymptotically efficient estimator through a\nsuboptimal yet very simple preliminary estimator, which enables us to bypass\nnot only numerical optimization of the likelihood function, but also the\nmultiple-root problem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:29:45 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 15:15:20 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Masuda", "Hiroki", ""]]}, {"id": "2006.04632", "submitter": "Seongoh Park Dr", "authors": "Seongoh Park, Xinlei Wang, Johan Lim", "title": "Estimating High-dimensional Covariance and Precision Matrices under\n  General Missing Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sample covariance matrix $\\boldsymbol{S}$ of completely observed data is\nthe key statistic in a large variety of multivariate statistical procedures,\nsuch as structured covariance/precision matrix estimation, principal component\nanalysis, and testing of equality of mean vectors. However, when the data are\npartially observed, the sample covariance matrix from the available data is\nbiased and does not provide valid multivariate procedures. To correct the bias,\na simple adjustment method called inverse probability weighting (IPW) has been\nused in previous research, yielding the IPW estimator. The estimator plays the\nrole of $\\boldsymbol{S}$ in the missing data context so that it can be plugged\ninto off-the-shelf multivariate procedures. However, theoretical properties\n(e.g. concentration) of the IPW estimator have been only established under very\nsimple missing structures; every variable of each sample is independently\nsubject to missing with equal probability.\n  We investigate the deviation of the IPW estimator when observations are\npartially observed under general missing dependency. We prove the optimal\nconvergence rate $O_p(\\sqrt{\\log p / n})$ of the IPW estimator based on the\nelement-wise maximum norm. We also derive similar deviation results even when\nimplicit assumptions (known mean and/or missing probability) are relaxed. The\noptimal rate is especially crucial in estimating a precision matrix, because of\nthe \"meta-theorem\" that claims the rate of the IPW estimator governs that of\nthe resulting precision matrix estimator. In the simulation study, we discuss\nnon-positive semi-definiteness of the IPW estimator and compare the estimator\nwith imputation methods, which are practically important.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 14:30:33 GMT"}, {"version": "v2", "created": "Sun, 8 Nov 2020 13:50:09 GMT"}, {"version": "v3", "created": "Sat, 17 Apr 2021 12:56:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Park", "Seongoh", ""], ["Wang", "Xinlei", ""], ["Lim", "Johan", ""]]}, {"id": "2006.04656", "submitter": "Frank R\\\"ottger", "authors": "Fritjof Freise, Ulrike Gra{\\ss}hoff, Frank R\\\"ottger, Rainer Schwabe", "title": "$ D $-optimal designs for Poisson regression with synergetic interaction\n  effect", "comments": "21 pages, 6 figures", "journal-ref": "TEST (2021)", "doi": "10.1007/s11749-020-00752-w", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We characterize $D$-optimal designs in the two-dimensional Poisson regression\nmodel with synergetic interaction and provide an explicit proof. The proof is\nbased on the idea of reparameterization of the design region in terms of\ncontours of constant intensity. This approach leads to a substantial reduction\nof complexity as properties of the sensitivity can be treated along and across\nthe contours separately. Furthermore, some extensions of this result to higher\ndimensions are presented.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 15:01:41 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Freise", "Fritjof", ""], ["Gra\u00dfhoff", "Ulrike", ""], ["R\u00f6ttger", "Frank", ""], ["Schwabe", "Rainer", ""]]}, {"id": "2006.04740", "submitter": "Umut \\c{S}im\\c{s}ekli", "authors": "Mert Gurbuzbalaban, Umut \\c{S}im\\c{s}ekli, Lingjiong Zhu", "title": "The Heavy-Tail Phenomenon in SGD", "comments": null, "journal-ref": "Published as a conference paper at International Conference on\n  Machine Learning (ICML) 2021", "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, various notions of capacity and complexity have been\nproposed for characterizing the generalization properties of stochastic\ngradient descent (SGD) in deep learning. Some of the popular notions that\ncorrelate well with the performance on unseen data are (i) the `flatness' of\nthe local minimum found by SGD, which is related to the eigenvalues of the\nHessian, (ii) the ratio of the stepsize $\\eta$ to the batch-size $b$, which\nessentially controls the magnitude of the stochastic gradient noise, and (iii)\nthe `tail-index', which measures the heaviness of the tails of the network\nweights at convergence. In this paper, we argue that these three seemingly\nunrelated perspectives for generalization are deeply linked to each other. We\nclaim that depending on the structure of the Hessian of the loss at the\nminimum, and the choices of the algorithm parameters $\\eta$ and $b$, the SGD\niterates will converge to a \\emph{heavy-tailed} stationary distribution. We\nrigorously prove this claim in the setting of quadratic optimization: we show\nthat even in a simple linear regression problem with independent and\nidentically distributed data whose distribution has finite moments of all\norder, the iterates can be heavy-tailed with infinite variance. We further\ncharacterize the behavior of the tails with respect to algorithm parameters,\nthe dimension, and the curvature. We then translate our results into insights\nabout the behavior of SGD in deep learning. We support our theory with\nexperiments conducted on synthetic data, fully connected, and convolutional\nneural networks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 16:43:56 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2020 16:04:31 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2021 02:36:01 GMT"}, {"version": "v4", "created": "Tue, 8 Jun 2021 15:34:12 GMT"}, {"version": "v5", "created": "Mon, 14 Jun 2021 15:45:36 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gurbuzbalaban", "Mert", ""], ["\u015eim\u015fekli", "Umut", ""], ["Zhu", "Lingjiong", ""]]}, {"id": "2006.04787", "submitter": "Sitan Chen", "authors": "Sitan Chen, Frederic Koehler, Ankur Moitra, Morris Yau", "title": "Classification Under Misspecification: Halfspaces, Generalized Linear\n  Models, and Connections to Evolvability", "comments": "51 pages, comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we revisit some classic problems on classification under\nmisspecification. In particular, we study the problem of learning halfspaces\nunder Massart noise with rate $\\eta$. In a recent work, Diakonikolas,\nGoulekakis, and Tzamos resolved a long-standing problem by giving the first\nefficient algorithm for learning to accuracy $\\eta + \\epsilon$ for any\n$\\epsilon > 0$. However, their algorithm outputs a complicated hypothesis,\nwhich partitions space into $\\text{poly}(d,1/\\epsilon)$ regions. Here we give a\nmuch simpler algorithm and in the process resolve a number of outstanding open\nquestions:\n  (1) We give the first proper learner for Massart halfspaces that achieves\n$\\eta + \\epsilon$. We also give improved bounds on the sample complexity\nachievable by polynomial time algorithms.\n  (2) Based on (1), we develop a blackbox knowledge distillation procedure to\nconvert an arbitrarily complex classifier to an equally good proper classifier.\n  (3) By leveraging a simple but overlooked connection to evolvability, we show\nany SQ algorithm requires super-polynomially many queries to achieve\n$\\mathsf{OPT} + \\epsilon$.\n  Moreover we study generalized linear models where $\\mathbb{E}[Y|\\mathbf{X}] =\n\\sigma(\\langle \\mathbf{w}^*, \\mathbf{X}\\rangle)$ for any odd, monotone, and\nLipschitz function $\\sigma$. This family includes the previously mentioned\nhalfspace models as a special case, but is much richer and includes other\nfundamental models like logistic regression. We introduce a challenging new\ncorruption model that generalizes Massart noise, and give a general algorithm\nfor learning in this setting. Our algorithms are based on a small set of core\nrecipes for learning to classify in the presence of misspecification.\n  Finally we study our algorithm for learning halfspaces under Massart noise\nempirically and find that it exhibits some appealing fairness properties.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 17:59:11 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chen", "Sitan", ""], ["Koehler", "Frederic", ""], ["Moitra", "Ankur", ""], ["Yau", "Morris", ""]]}, {"id": "2006.04797", "submitter": "Gunnar Taraldsen", "authors": "Gunnar Taraldsen and Bo H. Lindqvist", "title": "Conditional probability and improper priors", "comments": null, "journal-ref": "Communications in Statistics - Theory and Methods, 45:17,\n  5007-5016", "doi": "10.1080/03610926.2014.935430", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to present a mathematical theory that can be\nused as a foundation for statistics that include improper priors. This theory\nincludes improper laws in the initial axioms and has in particular Bayes\ntheorem as a consequence. Another consequence is that some of the usual\ncalculation rules are modified. This is important in relation to common\nstatistical practice which usually include improper priors, but tends to use\nunaltered calculation rules. In some cases the results are valid, but in other\ncases inconsistencies may appear. The famous marginalization paradoxes\nexemplify this latter case. An alternative mathematical theory for the\nfoundations of statistics can be formulated in terms of conditional probability\nspaces. In this case the appearance of improper laws is a consequence of the\ntheory. It is proved here that the resulting mathematical structures for the\ntwo theories are equivalent. The conclusion is that the choice of the first or\nthe second formulation for the initial axioms can be considered a matter of\npersonal preference. Readers that initially have concerns regarding improper\npriors can possibly be more open toward a formulation of the initial axioms in\nterms of conditional probabilities. The interpretation of an improper law is\ngiven by the corresponding conditional probabilities.\n  Keywords: Axioms of statistics, Conditional probability space, Improper\nprior, Projective space\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 22:19:21 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Taraldsen", "Gunnar", ""], ["Lindqvist", "Bo H.", ""]]}, {"id": "2006.04873", "submitter": "Mert G\\\"urb\\\"uzbalaban", "authors": "Mert G\\\"urb\\\"uzbalaban, Andrzej Ruszczy\\'nski and Landi Zhu", "title": "A Stochastic Subgradient Method for Distributionally Robust Non-Convex\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a distributionally robust formulation of stochastic optimization\nproblems arising in statistical learning, where robustness is with respect to\nuncertainty in the underlying data distribution. Our formulation builds on\nrisk-averse optimization techniques and the theory of coherent risk measures.\nIt uses semi-deviation risk for quantifying uncertainty, allowing us to compute\nsolutions that are robust against perturbations in the population data\ndistribution. We consider a large family of loss functions that can be\nnon-convex and non-smooth and develop an efficient stochastic subgradient\nmethod. We prove that it converges to a point satisfying the optimality\nconditions. To our knowledge, this is the first method with rigorous\nconvergence guarantees in the context of non-convex non-smooth distributionally\nrobust stochastic optimization. Our method can achieve any desired level of\nrobustness with little extra computational cost compared to population risk\nminimization. We also illustrate the performance of our algorithm on real\ndatasets arising in convex and non-convex supervised learning problems.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 18:52:40 GMT"}, {"version": "v2", "created": "Sat, 15 Aug 2020 20:02:44 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 01:24:32 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["G\u00fcrb\u00fczbalaban", "Mert", ""], ["Ruszczy\u0144ski", "Andrzej", ""], ["Zhu", "Landi", ""]]}, {"id": "2006.05022", "submitter": "Qinqing Zheng", "authors": "Arun Kumar Kuchibhotla and Qinqing Zheng", "title": "Near-Optimal Confidence Sequences for Bounded Random Variables", "comments": "Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many inference problems, such as sequential decision problems like A/B\ntesting, adaptive sampling schemes like bandit selection, are often online in\nnature. The fundamental problem for online inference is to provide a sequence\nof confidence intervals that are valid uniformly over the growing-into-infinity\nsample sizes. To address this question, we provide a near-optimal confidence\nsequence for bounded random variables by utilizing Bentkus' concentration\nresults. We show that it improves on the existing approaches that use the\nCram{\\'e}r-Chernoff technique such as the Hoeffding, Bernstein, and Bennett\ninequalities. The resulting confidence sequence is confirmed to be favorable in\nboth synthetic coverage problems and an application to adaptive stopping\nalgorithms.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 02:50:01 GMT"}, {"version": "v2", "created": "Sun, 14 Feb 2021 20:35:34 GMT"}, {"version": "v3", "created": "Thu, 3 Jun 2021 20:43:21 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""], ["Zheng", "Qinqing", ""]]}, {"id": "2006.05199", "submitter": "Jean Michel Loubes", "authors": "Eustasio del Barrio and Jean-Michel Loubes", "title": "The statistical effect of entropic regularization in optimal\n  transportation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to tackle the problem of understanding the effect of\nregularization in Sinkhorn algotihms. In the case of Gaussian distributions we\nprovide a closed form for the regularized optimal transport which enables to\nprovide a better understanding of the effect of the regularization from a\nstatistical framework.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 11:48:37 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 12:49:28 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["del Barrio", "Eustasio", ""], ["Loubes", "Jean-Michel", ""]]}, {"id": "2006.05228", "submitter": "Antoine Maillard", "authors": "Antoine Maillard, Bruno Loureiro, Florent Krzakala, Lenka Zdeborov\\'a", "title": "Phase retrieval in high dimensions: Statistical and computational phase\n  transitions", "comments": "12 pages (main text and references), 26 pages of supplementary\n  material. v2 matches the final version accepted at NeurIPS 2021", "journal-ref": "Advances in Neural Information Processing Systems, v33, pages\n  11071--11082, 2020", "doi": null, "report-no": null, "categories": "math.ST cond-mat.dis-nn cs.IT cs.LG math.IT math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the phase retrieval problem of reconstructing a $n$-dimensional\nreal or complex signal $\\mathbf{X}^{\\star}$ from $m$ (possibly noisy)\nobservations $Y_\\mu = | \\sum_{i=1}^n \\Phi_{\\mu i} X^{\\star}_i/\\sqrt{n}|$, for a\nlarge class of correlated real and complex random sensing matrices\n$\\mathbf{\\Phi}$, in a high-dimensional setting where $m,n\\to\\infty$ while\n$\\alpha = m/n=\\Theta(1)$. First, we derive sharp asymptotics for the lowest\npossible estimation error achievable statistically and we unveil the existence\nof sharp phase transitions for the weak- and full-recovery thresholds as a\nfunction of the singular values of the matrix $\\mathbf{\\Phi}$. This is achieved\nby providing a rigorous proof of a result first obtained by the replica method\nfrom statistical mechanics. In particular, the information-theoretic transition\nto perfect recovery for full-rank matrices appears at $\\alpha=1$ (real case)\nand $\\alpha=2$ (complex case). Secondly, we analyze the performance of the\nbest-known polynomial time algorithm for this problem -- approximate\nmessage-passing -- establishing the existence of a statistical-to-algorithmic\ngap depending, again, on the spectral properties of $\\mathbf{\\Phi}$. Our work\nprovides an extensive classification of the statistical and algorithmic\nthresholds in high-dimensional phase retrieval for a broad class of random\nmatrices.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 13:03:29 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 15:27:51 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Maillard", "Antoine", ""], ["Loureiro", "Bruno", ""], ["Krzakala", "Florent", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "2006.05381", "submitter": "Yifan Cui", "authors": "Yifan Cui, Jan Hannig", "title": "A fiducial approach to nonparametric deconvolution problem: discrete\n  case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fiducial inference, as generalized by Hannig et al. (2016), is applied to\nnonparametric g-modeling (Efron, 2016) in the discrete case. We propose a\ncomputationally efficient algorithm to sample from the fiducial distribution,\nand use generated samples to construct point estimates and confidence\nintervals. We study the theoretical properties of the fiducial distribution and\nperform extensive simulations in various scenarios. The proposed approach gives\nrise to surprisingly good statistical performance in terms of the mean squared\nerror of point estimators and coverage of confidence intervals. Furthermore, we\napply the proposed fiducial method to estimate the probability of each\nsatellite site being malignant using gastric adenocarcinoma data with 844\npatients (Efron, 2016).\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 16:29:24 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2020 23:53:44 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 00:33:59 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Cui", "Yifan", ""], ["Hannig", "Jan", ""]]}, {"id": "2006.05458", "submitter": "Miguel Lafuente", "authors": "Ra\\'ul Gouet (1), Miguel Lafuente (2), F. Javier L\\'opez (2 and 3) and\n  Gerardo Sanz (2 and 3) ((1) Universidad de Chile, (2) University of Zaragoza,\n  (3) Institute for Biocomputation and Physics of Complex Systems)", "title": "Exact and asymptotic properties of $\\delta$-records in the linear drift\n  model", "comments": "30 pages, 12 figures", "journal-ref": null, "doi": "10.1088/1742-5468/abb4dc", "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of records in the Linear Drift Model (LDM) has attracted much\nattention recently due to applications in several fields. In the present paper\nwe study $\\delta$-records in the LDM, defined as observations which are greater\nthan all previous observations, plus a fixed real quantity $\\delta$. We give\nanalytical properties of the probability of $\\delta$-records and study the\ncorrelation between $\\delta$-record events. We also analyse the asymptotic\nbehaviour of the number of $\\delta$-records among the first $n$ observations\nand give conditions for convergence to the Gaussian distribution. As a\nconsequence of our results, we solve a conjecture posed in J. Stat. Mech. 2010,\nP10013, regarding the total number of records in a LDM with negative drift.\nExamples of application to particular distributions, such as Gumbel or Pareto\nare also provided. We illustrate our results with a real data set of summer\ntemperatures in Spain, where the LDM is consistent with the global-warming\nphenomenon.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 18:54:22 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 10:49:03 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Gouet", "Ra\u00fal", "", "2 and 3"], ["Lafuente", "Miguel", "", "2 and 3"], ["L\u00f3pez", "F. Javier", "", "2 and 3"], ["Sanz", "Gerardo", "", "2 and 3"]]}, {"id": "2006.05487", "submitter": "Luiz F. O. Chamon", "authors": "Luiz F. O. Chamon and Alejandro Ribeiro", "title": "Probably Approximately Correct Constrained Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As learning solutions reach critical applications in social, industrial, and\nmedical domains, the need to curtail their behavior has become paramount. There\nis now ample evidence that without explicit tailoring, learning can lead to\nbiased, unsafe, and prejudiced solutions. To tackle these problems, we develop\na generalization theory of constrained learning based on the probably\napproximately correct (PAC) learning framework. In particular, we show that\nimposing requirements does not make a learning problem harder in the sense that\nany PAC learnable class is also PAC constrained learnable using a constrained\ncounterpart of the empirical risk minimization (ERM) rule. For typical\nparametrized models, however, this learner involves solving a constrained\nnon-convex optimization program for which even obtaining a feasible solution is\nchallenging. To overcome this issue, we prove that under mild conditions the\nempirical dual problem of constrained learning is also a PAC constrained\nlearner that now leads to a practical constrained learning algorithm based\nsolely on solving unconstrained problems. We analyze the generalization\nproperties of this solution and use it to illustrate how constrained learning\ncan address problems in fair and robust classification.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 19:59:29 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 22:42:15 GMT"}], "update_date": "2021-02-19", "authors_parsed": [["Chamon", "Luiz F. O.", ""], ["Ribeiro", "Alejandro", ""]]}, {"id": "2006.05492", "submitter": "Thomas Courtade", "authors": "Kuan-Yun Lee and Thomas A. Courtade", "title": "Linear Models are Most Favorable among Generalized Linear Models", "comments": "To appear in the 2020 IEEE International Symposium on Information\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a nonasymptotic lower bound on the $L_2$ minimax risk for a\nclass of generalized linear models. It is further shown that the minimax risk\nfor the canonical linear model matches this lower bound up to a universal\nconstant. Therefore, the canonical linear model may be regarded as most\nfavorable among the considered class of generalized linear models (in terms of\nminimax risk). The proof makes use of an information-theoretic Bayesian\nCram\\'er-Rao bound for log-concave priors, established by Aras et al. (2019).\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 20:14:03 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Lee", "Kuan-Yun", ""], ["Courtade", "Thomas A.", ""]]}, {"id": "2006.05527", "submitter": "Lutz Duembgen", "authors": "Alexander Henzi and Alexandre Moesching and Lutz Duembgen", "title": "Accelerating the pool-adjacent-violators algorithm for isotonic\n  distributional regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the context of estimating stochastically ordered distribution functions,\nthe pool-adjacent-violators algorithm (PAVA) can be modified such that the\ncomputation times are reduced substantially. This is achieved by studying the\ndependence of antitonic weighted least squares fits on the response vector to\nbe approximated.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 21:58:47 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 12:06:33 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 12:46:12 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Henzi", "Alexander", ""], ["Moesching", "Alexandre", ""], ["Duembgen", "Lutz", ""]]}, {"id": "2006.05539", "submitter": "Kevin Cheng", "authors": "Kevin C. Cheng, Eric L. Miller, Michael C. Hughes, Shuchin Aeron", "title": "On Matched Filtering for Statistical Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric and distribution-free two-sample tests have been the\nfoundation of many change point detection algorithms. However, randomness in\nthe test statistic as a function of time makes them susceptible to false\npositives and localization ambiguity. We address these issues by deriving and\napplying filters matched to the expected temporal signatures of a change for\nvarious sliding window, two-sample tests under IID assumptions on the data.\nThese filters are derived asymptotically with respect to the window size for\nthe Wasserstein quantile test, the Wasserstein-1 distance test, Maximum Mean\nDiscrepancy squared (MMD^2), and the Kolmogorov-Smirnov (KS) test. The matched\nfilters are shown to have two important properties. First, they are\ndistribution-free, and thus can be applied without prior knowledge of the\nunderlying data distributions. Second, they are peak-preserving, which allows\nthe filtered signal produced by our methods to maintain expected statistical\nsignificance. Through experiments on synthetic data as well as activity\nrecognition benchmarks, we demonstrate the utility of this approach for\nmitigating false positives and improving the test precision. Our method allows\nfor the localization of change points without the use of ad-hoc post-processing\nto remove redundant detections common to current methods. We further highlight\nthe performance of statistical tests based on the Quantile-Quantile (Q-Q)\nfunction and show how the invariance property of the Q-Q function to\norder-preserving transformations allows these tests to detect change points of\ndifferent scales with a single threshold within the same dataset.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 22:45:58 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 02:55:13 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 21:44:53 GMT"}, {"version": "v4", "created": "Tue, 27 Oct 2020 21:00:52 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Cheng", "Kevin C.", ""], ["Miller", "Eric L.", ""], ["Hughes", "Michael C.", ""], ["Aeron", "Shuchin", ""]]}, {"id": "2006.05566", "submitter": "Kananart Kuwaranancharoen", "authors": "Kananart Kuwaranancharoen", "title": "On the Conditional Expectation of Mean Shifted Gaussian Distributions", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a property of univariate Gaussian distributions\nnamely conditional expectation shift (or centroid shift). Specifically, we\ncompare two Gaussian distributions in which they differ only in their means.\nEquivalently, we can view this situation as one of the distribution is shifted\nto the right. These two distributions are conditioned on the same event in\nwhich the realizations fall in the right interval or left interval. We show\nthat if a Gaussian distribution is shifted to the right while the conditioning\nevent remains the same then the conditional expectation is shifted to the right\nconcurrently.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 00:05:31 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Kuwaranancharoen", "Kananart", ""]]}, {"id": "2006.05591", "submitter": "Ramesh Johari", "authors": "Peter Glynn, Ramesh Johari, Mohammad Rasouli", "title": "Adaptive Experimental Design with Temporal Interference: A Maximum\n  Likelihood Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose an online platform wants to compare a treatment and control policy,\ne.g., two different matching algorithms in a ridesharing system, or two\ndifferent inventory management algorithms in an online retail site. Standard\nrandomized controlled trials are typically not feasible, since the goal is to\nestimate policy performance on the entire system. Instead, the typical current\npractice involves dynamically alternating between the two policies for fixed\nlengths of time, and comparing the average performance of each over the\nintervals in which they were run as an estimate of the treatment effect.\nHowever, this approach suffers from *temporal interference*: one algorithm\nalters the state of the system as seen by the second algorithm, biasing\nestimates of the treatment effect. Further, the simple non-adaptive nature of\nsuch designs implies they are not sample efficient.\n  We develop a benchmark theoretical model in which to study optimal\nexperimental design for this setting. We view testing the two policies as the\nproblem of estimating the steady state difference in reward between two unknown\nMarkov chains (i.e., policies). We assume estimation of the steady state reward\nfor each chain proceeds via nonparametric maximum likelihood, and search for\nconsistent (i.e., asymptotically unbiased) experimental designs that are\nefficient (i.e., asymptotically minimum variance). Characterizing such designs\nis equivalent to a Markov decision problem with a minimum variance objective;\nsuch problems generally do not admit tractable solutions. Remarkably, in our\nsetting, using a novel application of classical martingale analysis of Markov\nchains via Poisson's equation, we characterize efficient designs via a succinct\nconvex optimization problem. We use this characterization to propose a\nconsistent, efficient online experimental design that adaptively samples the\ntwo Markov chains.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 01:07:08 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Glynn", "Peter", ""], ["Johari", "Ramesh", ""], ["Rasouli", "Mohammad", ""]]}, {"id": "2006.05630", "submitter": "Nian Si", "authors": "Nian Si, Fan Zhang, Zhengyuan Zhou, Jose Blanchet", "title": "Distributional Robust Batch Contextual Bandits", "comments": "The short version has been accepted in ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy learning using historical observational data is an important problem\nthat has found widespread applications. Examples include selecting offers,\nprices, advertisements to send to customers, as well as selecting which\nmedication to prescribe to a patient. However, existing literature rests on the\ncrucial assumption that the future environment where the learned policy will be\ndeployed is the same as the past environment that has generated the data--an\nassumption that is often false or too coarse an approximation. In this paper,\nwe lift this assumption and aim to learn a distributional robust policy with\nincomplete (bandit) observational data. We propose a novel learning algorithm\nthat is able to learn a robust policy to adversarial perturbations and unknown\ncovariate shifts. We first present a policy evaluation procedure in the\nambiguous environment and then give a performance guarantee based on the theory\nof uniform convergence. Additionally, we also give a heuristic algorithm to\nsolve the distributional robust policy learning problems efficiently. Finally,\nwe demonstrate the robustness of our methods in the synthetic and real-world\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 03:11:40 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 15:50:35 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Si", "Nian", ""], ["Zhang", "Fan", ""], ["Zhou", "Zhengyuan", ""], ["Blanchet", "Jose", ""]]}, {"id": "2006.05786", "submitter": "Alexander Marynych", "authors": "Dennis Bohle, Alexander Marynych and Matthias Meiners", "title": "A fundamental problem of hypothesis testing with finite inventory in\n  e-commerce", "comments": "23 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we draw attention to a problem that is often overlooked or\nignored by companies practicing hypothesis testing (A/B testing) in online\nenvironments. We show that conducting experiments on limited inventory that is\nshared between variants in the experiment can lead to high false positive rates\nsince the core assumption of independence between the groups is violated. We\nprovide a detailed analysis of the problem in a simplified setting whose\nparameters are informed by realistic scenarios. The setting we consider is a\n$2$-dimensional random walk in a semi-infinite strip. It is rich enough to take\na finite inventory into account, but is at the same time simple enough to allow\nfor a closed form of the false-positive probability. We prove that high\nfalse-positive rates can occur, and develop tools that are suitable to help\ndesign adequate tests in follow-up work. Our results also show that high\nfalse-negative rates may occur. The proofs rely on a functional limit theorem\nfor the $2$-dimensional random walk in a semi-infinite strip.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 11:54:22 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Bohle", "Dennis", ""], ["Marynych", "Alexander", ""], ["Meiners", "Matthias", ""]]}, {"id": "2006.05800", "submitter": "Denny Wu", "authors": "Denny Wu and Ji Xu", "title": "On the Optimal Weighted $\\ell_2$ Regularization in Overparameterized\n  Linear Regression", "comments": "NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the linear model $\\mathbf{y} = \\mathbf{X} \\mathbf{\\beta}_\\star +\n\\mathbf{\\epsilon}$ with $\\mathbf{X}\\in \\mathbb{R}^{n\\times p}$ in the\noverparameterized regime $p>n$. We estimate $\\mathbf{\\beta}_\\star$ via\ngeneralized (weighted) ridge regression: $\\hat{\\mathbf{\\beta}}_\\lambda =\n\\left(\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{\\Sigma}_w\\right)^\\dagger\n\\mathbf{X}^T\\mathbf{y}$, where $\\mathbf{\\Sigma}_w$ is the weighting matrix.\nUnder a random design setting with general data covariance $\\mathbf{\\Sigma}_x$\nand anisotropic prior on the true coefficients\n$\\mathbb{E}\\mathbf{\\beta}_\\star\\mathbf{\\beta}_\\star^T = \\mathbf{\\Sigma}_\\beta$,\nwe provide an exact characterization of the prediction risk\n$\\mathbb{E}(y-\\mathbf{x}^T\\hat{\\mathbf{\\beta}}_\\lambda)^2$ in the proportional\nasymptotic limit $p/n\\rightarrow \\gamma \\in (1,\\infty)$. Our general setup\nleads to a number of interesting findings. We outline precise conditions that\ndecide the sign of the optimal setting $\\lambda_{\\rm opt}$ for the ridge\nparameter $\\lambda$ and confirm the implicit $\\ell_2$ regularization effect of\noverparameterization, which theoretically justifies the surprising empirical\nobservation that $\\lambda_{\\rm opt}$ can be negative in the overparameterized\nregime. We also characterize the double descent phenomenon for principal\ncomponent regression (PCR) when both $\\mathbf{X}$ and $\\mathbf{\\beta}_\\star$\nare anisotropic. Finally, we determine the optimal weighting matrix\n$\\mathbf{\\Sigma}_w$ for both the ridgeless ($\\lambda\\to 0$) and optimally\nregularized ($\\lambda = \\lambda_{\\rm opt}$) case, and demonstrate the advantage\nof the weighted objective over standard ridge regression and PCR.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 12:38:43 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 16:00:43 GMT"}, {"version": "v3", "created": "Sun, 1 Nov 2020 05:56:19 GMT"}, {"version": "v4", "created": "Tue, 3 Nov 2020 02:20:13 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wu", "Denny", ""], ["Xu", "Ji", ""]]}, {"id": "2006.05816", "submitter": "Laba Handique", "authors": "Laba Handique, Farrukh Jamal and Subrata Chakraborty", "title": "On a family that unifies Generalized Marshall-Olkin and Poisson-G family\n  of distribution", "comments": "18 Pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:2005.10690, arXiv:2005.04506", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unifying the generalized Marshall-Olkin (GMO) and Poisson-G (P-G) a new\nfamily of distribution is proposed. Density and the survival function are\nexpressed as infinite mixtures of P-G family. The quantile function,\nasymptotes, shapes, stochastic ordering, moment generating function, order\nstatistics, probability weighted moments and R\\'enyi entropy are derived.\nMaximum likelihood estimation with large sample properties is presented. A\nMonte Carlo simulation is used to examine the pattern of the bias and the mean\nsquare error of the maximum likelihood estimators. An illustration of\ncomparison with some of the important sub models of the family in modeling a\nreal data reveals the utility of the proposed family.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2020 19:25:56 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Handique", "Laba", ""], ["Jamal", "Farrukh", ""], ["Chakraborty", "Subrata", ""]]}, {"id": "2006.05973", "submitter": "Rohit Agrawal", "authors": "Rohit Agrawal, Thibaut Horel", "title": "Optimal Bounds between $f$-Divergences and Integral Probability Metrics", "comments": null, "journal-ref": "J. Mach. Learn. Res. 22(128):1-59, 2021", "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT math.OC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The families of $f$-divergences (e.g. the Kullback-Leibler divergence) and\nIntegral Probability Metrics (e.g. total variation distance or maximum mean\ndiscrepancies) are widely used to quantify the similarity between probability\ndistributions. In this work, we systematically study the relationship between\nthese two families from the perspective of convex duality. Starting from a\ntight variational representation of the $f$-divergence, we derive a\ngeneralization of the moment-generating function, which we show exactly\ncharacterizes the best lower bound of the $f$-divergence as a function of a\ngiven IPM. Using this characterization, we obtain new bounds while also\nrecovering in a unified manner well-known results, such as Hoeffding's lemma,\nPinsker's inequality and its extension to subgaussian functions, and the\nHammersley-Chapman-Robbins bound. This characterization also allows us to prove\nnew results on topological properties of the divergence which may be of\nindependent interest.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 17:39:11 GMT"}, {"version": "v2", "created": "Mon, 3 Aug 2020 23:40:50 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 19:47:28 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Agrawal", "Rohit", ""], ["Horel", "Thibaut", ""]]}, {"id": "2006.05978", "submitter": "Jun Wu", "authors": "Carlos Am\\'endola, Philipp Dettling, Mathias Drton, Federica Onori,\n  Jun Wu", "title": "Structure Learning for Cyclic Linear Causal Models", "comments": "19 pages, 5 figures, 3 tables, to appear in UAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of structure learning for linear causal models based\non observational data. We treat models given by possibly cyclic mixed graphs,\nwhich allow for feedback loops and effects of latent confounders. Generalizing\nrelated work on bow-free acyclic graphs, we assume that the underlying graph is\nsimple. This entails that any two observed variables can be related through at\nmost one direct causal effect and that (confounding-induced) correlation\nbetween error terms in structural equations occurs only in absence of direct\ncausal effects. We show that, despite new subtleties in the cyclic case, the\nconsidered simple cyclic models are of expected dimension and that a previously\nconsidered criterion for distributional equivalence of bow-free acyclic graphs\nhas an analogue in the cyclic case. Our result on model dimension justifies in\nparticular score-based methods for structure learning of linear Gaussian mixed\ngraph models, which we implement via greedy search.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 17:47:28 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2020 20:52:25 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Am\u00e9ndola", "Carlos", ""], ["Dettling", "Philipp", ""], ["Drton", "Mathias", ""], ["Onori", "Federica", ""], ["Wu", "Jun", ""]]}, {"id": "2006.06020", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee and Sourabh Bhattacharya", "title": "Convergence of Pseudo-Bayes Factors in Forward and Inverse Regression\n  Problems", "comments": "Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Bayesian literature on model comparison, Bayes factors play the\nleading role. In the classical statistical literature, model selection criteria\nare often devised used cross-validation ideas. Amalgamating the ideas of Bayes\nfactor and cross-validation Geisser and Eddy (1979) created the pseudo-Bayes\nfactor. The usage of cross-validation inculcates several theoretical\nadvantages, computational simplicity and numerical stability in Bayes factors\nas the marginal density of the entire dataset is replaced with products of\ncross-validation densities of individual data points.\n  However, the popularity of pseudo-Bayes factors is still negligible in\ncomparison with Bayes factors, with respect to both theoretical investigations\nand practical applications. In this article, we establish almost sure\nexponential convergence of pseudo-Bayes factors for large samples under a\ngeneral setup consisting of dependent data and model misspecifications. We\nparticularly focus on general parametric and nonparametric regression setups in\nboth forward and inverse contexts.\n  We illustrate our theoretical results with various examples, providing\nexplicit calculations. We also supplement our asymptotic theory with simulation\nexperiments in small sample situations of Poisson log regression and geometric\nlogit and probit regression, additionally addressing the variable selection\nproblem. We consider both linear and nonparametric regression modeled by\nGaussian processes for our purposes. Our simulation results provide quite\ninteresting insights into the usage of pseudo-Bayes factors in forward and\ninverse setups.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 18:12:44 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "2006.06068", "submitter": "Zhiyan Ding", "authors": "Zhiyan Ding and Qin Li", "title": "Variance reduction for Random Coordinate Descent-Langevin Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sampling from a log-concave distribution function is one core problem that\nhas wide applications in Bayesian statistics and machine learning. While most\ngradient free methods have slow convergence rate, the Langevin Monte Carlo\n(LMC) that provides fast convergence requires the computation of gradients. In\npractice one uses finite-differencing approximations as surrogates, and the\nmethod is expensive in high-dimensions.\n  A natural strategy to reduce computational cost in each iteration is to\nutilize random gradient approximations, such as random coordinate descent (RCD)\nor simultaneous perturbation stochastic approximation (SPSA). We show by a\ncounter-example that blindly applying RCD does not achieve the goal in the most\ngeneral setting. The high variance induced by the randomness means a larger\nnumber of iterations are needed, and this balances out the saving in each\niteration.\n  We then introduce a new variance reduction approach, termed Randomized\nCoordinates Averaging Descent (RCAD), and incorporate it with both overdamped\nand underdamped LMC. The methods are termed RCAD-O-LMC and RCAD-U-LMC\nrespectively. The methods still sit in the random gradient approximation\nframework, and thus the computational cost in each iteration is low. However,\nby employing RCAD, the variance is reduced, so the methods converge within the\nsame number of iterations as the classical overdamped and underdamped LMC. This\nleads to a computational saving overall.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 21:08:38 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2020 20:40:10 GMT"}, {"version": "v3", "created": "Tue, 29 Sep 2020 23:45:37 GMT"}, {"version": "v4", "created": "Thu, 22 Oct 2020 01:18:23 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Ding", "Zhiyan", ""], ["Li", "Qin", ""]]}, {"id": "2006.06098", "submitter": "Francesca Mignacco", "authors": "Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, Lenka\n  Zdeborov\\'a", "title": "Dynamical mean-field theory for stochastic gradient descent in Gaussian\n  mixture classification", "comments": "8 pages + appendix, 4 figures", "journal-ref": "Advances in Neural Information Processing Systems, v33, pages\n  9540--9550, 2020", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze in a closed form the learning dynamics of stochastic gradient\ndescent (SGD) for a single layer neural network classifying a high-dimensional\nGaussian mixture where each cluster is assigned one of two labels. This problem\nprovides a prototype of a non-convex loss landscape with interpolating regimes\nand a large generalization gap. We define a particular stochastic process for\nwhich SGD can be extended to a continuous-time limit that we call stochastic\ngradient flow. In the full-batch limit we recover the standard gradient flow.\nWe apply dynamical mean-field theory from statistical physics to track the\ndynamics of the algorithm in the high-dimensional limit via a self-consistent\nstochastic process. We explore the performance of the algorithm as a function\nof control parameters shedding light on how it navigates the loss landscape.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 22:49:41 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Mignacco", "Francesca", ""], ["Krzakala", "Florent", ""], ["Urbani", "Pierfrancesco", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "2006.06102", "submitter": "Mateusz B. Majka", "authors": "Mateusz B. Majka, Marc Sabate-Vidales, {\\L}ukasz Szpruch", "title": "Multi-index Antithetic Stochastic Gradient Algorithm", "comments": "46 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Algorithms (SGAs) are ubiquitous in computational\nstatistics, machine learning and optimisation. Recent years have brought an\ninflux of interest in SGAs and the non-asymptotic analysis of their bias is by\nnow well-developed. However, in order to fully understand the efficiency of\nMonte Carlo algorithms utilizing stochastic gradients, one also needs to carry\nout the analysis of their variance, which turns out to be problem-specific. For\nthis reason, there is no systematic theory that would specify the optimal\nchoice of the random approximation of the gradient in SGAs for a given data\nregime. Furthermore, while there have been numerous attempts to reduce the\nvariance of SGAs, these typically exploit a particular structure of the sampled\ndistribution. In this paper we use the Multi-index Monte Carlo apparatus\ncombined with the antithetic approach to construct the Multi-index Antithetic\nStochastic Gradient Algorithm (MASGA), which can be used to sample from any\nprobability distribution. This, to our knowledge, is the first SGA that, for\nall data regimes and without relying on any specific structure of the target\nmeasure, achieves performance on par with Monte Carlo estimators that have\naccess to unbiased samples from the distribution of interest. In other words,\nMASGA is an optimal estimator from the error-computational cost perspective\nwithin the class of Monte Carlo estimators.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 22:59:23 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Majka", "Mateusz B.", ""], ["Sabate-Vidales", "Marc", ""], ["Szpruch", "\u0141ukasz", ""]]}, {"id": "2006.06138", "submitter": "Lihua Lei", "authors": "Lihua Lei and Emmanuel J. Cand\\`es", "title": "Conformal Inference of Counterfactuals and Individual Treatment Effects", "comments": "Accepted by Journal of the Royal Statistical Society: Series B\n  (JRSSB); 38 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating treatment effect heterogeneity widely informs treatment decision\nmaking. At the moment, much emphasis is placed on the estimation of the\nconditional average treatment effect via flexible machine learning algorithms.\nWhile these methods enjoy some theoretical appeal in terms of consistency and\nconvergence rates, they generally perform poorly in terms of uncertainty\nquantification. This is troubling since assessing risk is crucial for reliable\ndecision-making in sensitive and uncertain environments. In this work, we\npropose a conformal inference-based approach that can produce reliable interval\nestimates for counterfactuals and individual treatment effects under the\npotential outcome framework. For completely randomized or stratified randomized\nexperiments with perfect compliance, the intervals have guaranteed average\ncoverage in finite samples regardless of the unknown data generating mechanism.\nFor randomized experiments with ignorable compliance and general observational\nstudies obeying the strong ignorability assumption, the intervals satisfy a\ndoubly robust property which states the following: the average coverage is\napproximately controlled if either the propensity score or the conditional\nquantiles of potential outcomes can be estimated accurately. Numerical studies\non both synthetic and real datasets empirically demonstrate that existing\nmethods suffer from a significant coverage deficit even in simple models. In\ncontrast, our methods achieve the desired coverage with reasonably short\nintervals.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 01:03:32 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 00:54:35 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Lei", "Lihua", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "2006.06293", "submitter": "Liam Hodgkinson", "authors": "Liam Hodgkinson, Michael W. Mahoney", "title": "Multiplicative noise and heavy tails in stochastic optimization", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although stochastic optimization is central to modern machine learning, the\nprecise mechanisms underlying its success, and in particular, the precise role\nof the stochasticity, still remain unclear. Modelling stochastic optimization\nalgorithms as discrete random recurrence relations, we show that multiplicative\nnoise, as it commonly arises due to variance in local rates of convergence,\nresults in heavy-tailed stationary behaviour in the parameters. A detailed\nanalysis is conducted for SGD applied to a simple linear regression problem,\nfollowed by theoretical results for a much larger class of models (including\nnon-linear and non-convex) and optimizers (including momentum, Adam, and\nstochastic Newton), demonstrating that our qualitative results hold much more\ngenerally. In each case, we describe dependence on key factors, including step\nsize, batch size, and data variability, all of which exhibit similar\nqualitative behavior to recent empirical results on state-of-the-art neural\nnetwork models from computer vision and natural language processing.\nFurthermore, we empirically demonstrate how multiplicative noise and\nheavy-tailed structure improve capacity for basin hopping and exploration of\nnon-convex loss surfaces, over commonly-considered stochastic dynamics with\nonly additive noise and light-tailed structure.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 09:58:01 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Hodgkinson", "Liam", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "2006.06386", "submitter": "Dominic Richards", "authors": "Dominic Richards, Jaouad Mourtada and Lorenzo Rosasco", "title": "Asymptotics of Ridge (less) Regression under General Source Condition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the prediction error of ridge regression in an asymptotic regime\nwhere the sample size and dimension go to infinity at a proportional rate. In\nparticular, we consider the role played by the structure of the true regression\nparameter. We observe that the case of a general deterministic parameter can be\nreduced to the case of a random parameter from a structured prior. The latter\nassumption is a natural adaptation of classic smoothness assumptions in\nnonparametric regression, which are known as source conditions in the the\ncontext of regularization theory for inverse problems. Roughly speaking, we\nassume the large coefficients of the parameter are in correspondence to the\nprincipal components. In this setting a precise characterisation of the test\nerror is obtained, depending on the inputs covariance and regression parameter\nstructure. We illustrate this characterisation in a simplified setting to\ninvestigate the influence of the true parameter on optimal regularisation for\noverparameterized models. We show that interpolation (no regularisation) can be\noptimal even with bounded signal-to-noise ratio (SNR), provided that the\nparameter coefficients are larger on high-variance directions of the data,\ncorresponding to a more regular function than posited by the regularization\nterm. This contrasts with previous work considering ridge regression with\nisotropic prior, in which case interpolation is only optimal in the limit of\ninfinite SNR.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 13:00:21 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2020 12:52:50 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 10:01:35 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Richards", "Dominic", ""], ["Mourtada", "Jaouad", ""], ["Rosasco", "Lorenzo", ""]]}, {"id": "2006.06439", "submitter": "Mohammed Zafar Anis", "authors": "M. Z. Anis and Debsurya De", "title": "Some More Properties of the Unit-Gompertz Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper, Mazucheli et al. (2019) introduced the unit-Gompertz (UG)\ndistribution and studied some of its properties. It is a continuous\ndistribution with bounded support, and hence may be useful for modelling\nlife-time phenomena. We present counter-examples to point out some subtle\nerrors in their work, and subsequently correct them. We also look at some other\ninteresting properties of this new distribution. Further, we also study some\nimportant reliability measures and consider some stochastic orderings\nassociated with this new distribution.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2020 08:27:55 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Anis", "M. Z.", ""], ["De", "Debsurya", ""]]}, {"id": "2006.06467", "submitter": "Vasilis Kontonis", "authors": "Ilias Diakonikolas, Vasilis Kontonis, Christos Tzamos, Nikos Zarifis", "title": "Learning Halfspaces with Tsybakov Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the efficient PAC learnability of halfspaces in the presence of\nTsybakov noise. In the Tsybakov noise model, each label is independently\nflipped with some probability which is controlled by an adversary. This noise\nmodel significantly generalizes the Massart noise model, by allowing the\nflipping probabilities to be arbitrarily close to $1/2$ for a fraction of the\nsamples. Our main result is the first non-trivial PAC learning algorithm for\nthis problem under a broad family of structured distributions -- satisfying\ncertain concentration and (anti-)anti-concentration properties -- including\nlog-concave distributions. Specifically, we given an algorithm that achieves\nmisclassification error $\\epsilon$ with respect to the true halfspace, with\nquasi-polynomial runtime dependence in $1/\\epsilin$. The only previous upper\nbound for this problem -- even for the special case of log-concave\ndistributions -- was doubly exponential in $1/\\epsilon$ (and follows via the\nnaive reduction to agnostic learning). Our approach relies on a novel\ncomputationally efficient procedure to certify whether a candidate solution is\nnear-optimal, based on semi-definite programming. We use this certificate\nprocedure as a black-box and turn it into an efficient learning algorithm by\nsearching over the space of halfspaces via online convex optimization.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 14:25:02 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kontonis", "Vasilis", ""], ["Tzamos", "Christos", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2006.06537", "submitter": "Kelly Moran", "authors": "Kelly R. Moran and Matthew W. Wheeler", "title": "Fast increased fidelity approximate Gibbs samplers for Bayesian Gaussian\n  process regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Gaussian processes (GPs) is supported by efficient sampling\nalgorithms, a rich methodological literature, and strong theoretical grounding.\nHowever, due to their prohibitive computation and storage demands, the use of\nexact GPs in Bayesian models is limited to problems containing at most several\nthousand observations. Sampling requires matrix operations that scale at\n$\\mathcal{O}(n^3),$ where $n$ is the number of unique inputs. Storage of\nindividual matrices scales at $\\mathcal{O}(n^2),$ and can quickly overwhelm the\nresources of most modern computers. To overcome these bottlenecks, we develop a\nsampling algorithm using $\\mathcal{H}$ matrix approximation of the matrices\ncomprising the GP posterior covariance. These matrices can approximate the true\nconditional covariance matrix within machine precision and allow for sampling\nalgorithms that scale at $\\mathcal{O}(n \\ \\mbox{log}^2 n)$ time and storage\ndemands scaling at $\\mathcal{O}(n \\ \\mbox{log} \\ n).$ We also describe how\nthese algorithms can be used as building blocks to model higher dimensional\nsurfaces at $\\mathcal{O}(d \\ n \\ \\mbox{log}^2 n)$, where $d$ is the dimension\nof the surface under consideration, using tensor products of one-dimensional\nGPs. Though various scalable processes have been proposed for approximating\nBayesian GP inference when $n$ is large, to our knowledge, none of these\nmethods show that the approximation's Kullback-Leibler divergence to the true\nposterior can be made arbitrarily small and may be no worse than the\napproximation provided by finite computer arithmetic. We describe\n$\\mathcal{H}-$matrices, give an efficient Gibbs sampler using these matrices\nfor one-dimensional GPs, offer a proposed extension to higher dimensional\nsurfaces, and investigate the performance of this fast increased fidelity\napproximate GP, FIFA-GP, using both simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 15:51:11 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Moran", "Kelly R.", ""], ["Wheeler", "Matthew W.", ""]]}, {"id": "2006.06542", "submitter": "Sebastian Fuchs", "authors": "Thomas Mroz, Sebastian Fuchs and Wolfgang Trutschnig", "title": "How simplifying and flexible is the simplifying assumption in\n  pair-copula constructions -- analytic answers in dimension three and a\n  glimpse beyond", "comments": "36 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the increasing popularity and the seemingly broad applicability\nof pair-copula constructions underlined by numerous publications in the last\ndecade, in this contribution we tackle the unavoidable question on how flexible\nand simplifying the commonly used `simplifying assumption' is from an analytic\nperspective and provide answers to two related open questions posed by Nagler\nand Czado in 2016. Aiming at a simplest possible setup for deriving the main\nresults we first focus on the three-dimensional setting. We prove that the\nfamily of simplified copulas is flexible in the sense that it is dense in the\nset of all three-dimensional co\\-pulas with respect to the uniform metric\n$d_\\infty$ - considering stronger notions of convergence like the one induced\nby the metric $D_1$, by weak conditional convergence, by total variation, or by\nKullback-Leibler divergence, however, the family even turn out to be nowhere\ndense and hence insufficient for any kind of flexible approximation.\nFurthermore, returning to $d_\\infty$ we show that the partial vine copula is\nnever the optimal simplified copula approximation of a given, non-simplified\ncopula $C$, and derive examples illustrating that the corresponding\napproximation error can be strikingly large and extend to more than 28\\% of the\ndiameter of the metric space. Moreover, the mapping $\\psi$ assigning each\nthree-dimensional copula its unique partial vine copula turns out to be\ndiscontinuous with respect to $d_\\infty$ (but continuous with respect to $D_1$\nand to weak conditional convergence), implying a surprising sensitivity of\npartial vine copula approximations. The afore-mentioned main results concerning\n$d_\\infty$ are then extended to the general multivariate setting.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 15:57:18 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2021 08:43:47 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Mroz", "Thomas", ""], ["Fuchs", "Sebastian", ""], ["Trutschnig", "Wolfgang", ""]]}, {"id": "2006.06560", "submitter": "Benjamin Aubin", "authors": "Benjamin Aubin, Florent Krzakala, Yue M. Lu, Lenka Zdeborov\\'a", "title": "Generalization error in high-dimensional perceptrons: Approaching Bayes\n  error with convex optimization", "comments": "11 pages + 45 pages Supplementary Material / 5 figures, v2 revised\n  and accepted at NeurIPS", "journal-ref": "Advances in Neural Information Processing Systems, v33, pages\n  12199--12210, 2020", "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a commonly studied supervised classification of a synthetic\ndataset whose labels are generated by feeding a one-layer neural network with\nrandom iid inputs. We study the generalization performances of standard\nclassifiers in the high-dimensional regime where $\\alpha=n/d$ is kept finite in\nthe limit of a high dimension $d$ and number of samples $n$. Our contribution\nis three-fold: First, we prove a formula for the generalization error achieved\nby $\\ell_2$ regularized classifiers that minimize a convex loss. This formula\nwas first obtained by the heuristic replica method of statistical physics.\nSecondly, focussing on commonly used loss functions and optimizing the $\\ell_2$\nregularization strength, we observe that while ridge regression performance is\npoor, logistic and hinge regression are surprisingly able to approach the\nBayes-optimal generalization error extremely closely. As $\\alpha \\to \\infty$\nthey lead to Bayes-optimal rates, a fact that does not follow from predictions\nof margin-based generalization error bounds. Third, we design an optimal loss\nand regularizer that provably leads to Bayes-optimal generalization error.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 16:14:51 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2020 10:41:55 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Aubin", "Benjamin", ""], ["Krzakala", "Florent", ""], ["Lu", "Yue M.", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "2006.06587", "submitter": "Mahdi S. Hosseini Dr.", "authors": "Mahdi S. Hosseini and Konstantinos N. Plataniotis", "title": "AdaS: Adaptive Scheduling of Stochastic Gradients", "comments": "Code is available at https://github.com/mahdihosseini/AdaS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of step-size used in Stochastic Gradient Descent (SGD)\noptimization is empirically selected in most training procedures. Moreover, the\nuse of scheduled learning techniques such as Step-Decaying, Cyclical-Learning,\nand Warmup to tune the step-size requires extensive practical\nexperience--offering limited insight into how the parameters update--and is not\nconsistent across applications. This work attempts to answer a question of\ninterest to both researchers and practitioners, namely \\textit{\"how much\nknowledge is gained in iterative training of deep neural networks?\"} Answering\nthis question introduces two useful metrics derived from the singular values of\nthe low-rank factorization of convolution layers in deep neural networks. We\nintroduce the notions of \\textit{\"knowledge gain\"} and \\textit{\"mapping\ncondition\"} and propose a new algorithm called Adaptive Scheduling (AdaS) that\nutilizes these derived metrics to adapt the SGD learning rate proportionally to\nthe rate of change in knowledge gain over successive iterations.\nExperimentation reveals that, using the derived metrics, AdaS exhibits: (a)\nfaster convergence and superior generalization over existing adaptive learning\nmethods; and (b) lack of dependence on a validation set to determine when to\nstop training. Code is available at\n\\url{https://github.com/mahdihosseini/AdaS}.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 16:36:31 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Hosseini", "Mahdi S.", ""], ["Plataniotis", "Konstantinos N.", ""]]}, {"id": "2006.06618", "submitter": "Gautam Kamath", "authors": "Sourav Biswas, Yihe Dong, Gautam Kamath, Jonathan Ullman", "title": "CoinPress: Practical Private Mean and Covariance Estimation", "comments": "Code is available at https://github.com/twistedcubic/coin-press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CR cs.DS cs.IT cs.LG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present simple differentially private estimators for the mean and\ncovariance of multivariate sub-Gaussian data that are accurate at small sample\nsizes. We demonstrate the effectiveness of our algorithms both theoretically\nand empirically using synthetic and real-world datasets---showing that their\nasymptotic error rates match the state-of-the-art theoretical bounds, and that\nthey concretely outperform all previous methods. Specifically, previous\nestimators either have weak empirical accuracy at small sample sizes, perform\npoorly for multivariate data, or require the user to provide strong a priori\nestimates for the parameters.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 17:17:28 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Biswas", "Sourav", ""], ["Dong", "Yihe", ""], ["Kamath", "Gautam", ""], ["Ullman", "Jonathan", ""]]}, {"id": "2006.06832", "submitter": "Jane Ivy Coons", "authors": "Jane Ivy Coons and Seth Sullivant", "title": "Quasi-independence models with rational maximum likelihood estimator", "comments": "27 pages, 3 figures, to appear in the Journal of Symbolic Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We classify the two-way independence quasi-independence models (or\nindependence models with structural zeros) that have rational maximum\nlikelihood estimators, or MLEs. We give a necessary and sufficient condition on\nthe bipartite graph associated to the model for the MLE to be rational. In this\ncase, we give an explicit formula for the MLE in terms of combinatorial\nfeatures of this graph. We also use the Horn uniformization to show that for\ngeneral log-linear models $\\mathcal{M}$ with rational MLE, any model obtained\nby restricting to a face of the cone of sufficient statistics of $\\mathcal{M}$\nalso has rational MLE.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 21:19:16 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 15:25:18 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Coons", "Jane Ivy", ""], ["Sullivant", "Seth", ""]]}, {"id": "2006.06843", "submitter": "Drew Lazar", "authors": "Lizhen Lin, Drew Lazar, Bayan Sarpabayeva, and David B. Dunson", "title": "Robust Optimization and Inference on Manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a robust and scalable procedure for general optimization and\ninference problems on manifolds leveraging the classical idea of\n`median-of-means' estimation. This is motivated by ubiquitous examples and\napplications in modern data science in which a statistical learning problem can\nbe cast as an optimization problem over manifolds. Being able to incorporate\nthe underlying geometry for inference while addressing the need for robustness\nand scalability presents great challenges. We address these challenges by first\nproving a key lemma that characterizes some crucial properties of geometric\nmedians on manifolds. In turn, this allows us to prove robustness and tighter\nconcentration of our proposed final estimator in a subsequent theorem. This\nestimator aggregates a collection of subset estimators by taking their\ngeometric median over the manifold. We illustrate bounds on this estimator via\ncalculations in explicit examples. The robustness and scalability of the\nprocedure is illustrated in numerical examples on both simulated and real data\nsets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 21:37:10 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Lin", "Lizhen", ""], ["Lazar", "Drew", ""], ["Sarpabayeva", "Bayan", ""], ["Dunson", "David B.", ""]]}, {"id": "2006.06978", "submitter": "Biswabrata Pradhan", "authors": "Siddhartha Chakraborty, Biswabrata Pradhan", "title": "Generalized Weighted Survival and Failure Entropies and their Dynamic\n  Versions", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weighted forms of generalized survival and failure entropies of order\n($\\alpha,\\beta$) are proposed and some properties are obtained. We further\npropose the dynamic versions of weighted generalized survival and failures\nentropies and obtained some properties and bounds. Characterization for\nRayleigh and power distributions are done by dynamic weighted generalized\nentropies. We further consider the empirical versions of generalized weighted\nsurvival and failure entropies and using the difference between theoretical and\nempirical survival entropies a test for exponentiality is considered.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 07:33:08 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Chakraborty", "Siddhartha", ""], ["Pradhan", "Biswabrata", ""]]}, {"id": "2006.06994", "submitter": "Jakob Zech", "authors": "Jakob Zech and Youssef Marzouk", "title": "Sparse approximation of triangular transports. Part I: the finite\n  dimensional case", "comments": "The original manuscript arXiv:2006.06994v1 has been split into two\n  parts; the present paper is the first part", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two probability measures $\\rho$ and $\\pi$ with analytic densities on the\n$d$-dimensional cube $[-1,1]^d$, we investigate the approximation of the unique\ntriangular monotone Knothe-Rosenblatt transport $T:[-1,1]^d\\to [-1,1]^d$, such\nthat the pushforward $T_\\sharp\\rho$ equals $\\pi$. It is shown that for\n$d\\in\\mathbb{N}$ there exist approximations $\\tilde T$ of $T$, based on either\nsparse polynomial expansions or deep ReLU neural networks, such that the\ndistance between $\\tilde T_\\sharp\\rho$ and $\\pi$ decreases exponentially. More\nprecisely, we prove error bounds of the type $\\exp(-\\beta N^{1/d})$ (or\n$\\exp(-\\beta N^{1/(d+1)})$ for neural networks), where $N$ refers to the\ndimension of the ansatz space (or the size of the network) containing $\\tilde\nT$; the notion of distance comprises the Hellinger distance, the total\nvariation distance, the Wasserstein distance and the Kullback-Leibler\ndivergence. Our construction guarantees $\\tilde T$ to be a monotone triangular\nbijective transport on the hypercube $[-1,1]^d$. Analogous results hold for the\ninverse transport $S=T^{-1}$. The proofs are constructive, and we give an\nexplicit a priori description of the ansatz space, which can be used for\nnumerical implementations.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 08:15:20 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 14:53:48 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Zech", "Jakob", ""], ["Marzouk", "Youssef", ""]]}, {"id": "2006.06997", "submitter": "Stefano Sarao Mannelli", "authors": "Stefano Sarao Mannelli, Giulio Biroli, Chiara Cammarota, Florent\n  Krzakala, Pierfrancesco Urbani and Lenka Zdeborov\\'a", "title": "Complex Dynamics in Simple Neural Networks: Understanding Gradient Flow\n  in Phase Retrieval", "comments": "9 pages, 5 figures + appendix", "journal-ref": "Advances in Neural Information Processing Systems, v22, page\n  3265--327, 2020", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the widespread use of gradient-based algorithms for optimizing\nhigh-dimensional non-convex functions, understanding their ability of finding\ngood minima instead of being trapped in spurious ones remains to a large extent\nan open problem. Here we focus on gradient flow dynamics for phase retrieval\nfrom random measurements. When the ratio of the number of measurements over the\ninput dimension is small the dynamics remains trapped in spurious minima with\nlarge basins of attraction. We find analytically that above a critical ratio\nthose critical points become unstable developing a negative direction toward\nthe signal. By numerical experiments we show that in this regime the gradient\nflow algorithm is not trapped; it drifts away from the spurious critical points\nalong the unstable direction and succeeds in finding the global minimum. Using\ntools from statistical physics we characterize this phenomenon, which is\nrelated to a BBP-type transition in the Hessian of the spurious minima.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 08:21:12 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Mannelli", "Stefano Sarao", ""], ["Biroli", "Giulio", ""], ["Cammarota", "Chiara", ""], ["Krzakala", "Florent", ""], ["Urbani", "Pierfrancesco", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "2006.06998", "submitter": "Kevin Elie-Dit-Cosaque", "authors": "Kevin Elie-Dit-Cosaque (PSPM, ICJ), V\\'eronique Maume-Deschamps (ICJ,\n  PSPM)", "title": "Random forest estimation of conditional distribution functions and\n  conditional quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a theoretical study of two realistic estimators of conditional\ndistribution functions and conditional quantiles using random forests. The\nestimation process uses the bootstrap samples generated from the original\ndataset when constructing the forest. Bootstrap samples are reused to define\nthe first estimator, while the second requires only the original sample, once\nthe forest has been built. We prove that both proposed estimators of the\nconditional distribution functions are consistent uniformly a.s. To the best of\nour knowledge, it is the first proof of consistency including the bootstrap\npart. We also illustrate the estimation procedures on a numerical example.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 08:23:40 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Elie-Dit-Cosaque", "Kevin", "", "PSPM, ICJ"], ["Maume-Deschamps", "V\u00e9ronique", "", "ICJ,\n  PSPM"]]}, {"id": "2006.07001", "submitter": "Quentin Duchemin", "authors": "Yohann de Castro (ICJ), Quentin Duchemin (LAMA)", "title": "Markov Random Geometric Graph (MRGG): A Growth Model for Temporal\n  Dynamic Networks", "comments": "In this first revised version, we provide applications of our growth\n  model with a hypothesis test and link prediction problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.SI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Markov Random Geometric Graphs (MRGGs), a growth model for\ntemporal dynamic networks. It is based on a Markovian latent space dynamic:\nconsecutive latent points are sampled on the Euclidean Sphere using an unknown\nMarkov kernel; and two nodes are connected with a probability depending on a\nunknown function of their latent geodesic distance. More precisely, at each\nstamp-time k we add a latent point X k sampled by jumping from the previous one\nX k--1 in a direction chosen uniformly Y k and with a length r k drawn from an\nunknown distribution called the latitude function. The connection probabilities\nbetween each pair of nodes are equal to the envelope function of the distance\nbetween these two latent points. We provide theoretical guarantees for the\nnon-parametric estimation of the latitude and the envelope functions. We\npropose an efficient algorithm that achieves those non-parametric estimation\ntasks based on an ad-hoc Hierarchical Agglomerative Clustering approach, and we\ndeploy this analysis on a real data-set given by exchange of messages on a\nsocial network.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 08:35:54 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 08:49:59 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["de Castro", "Yohann", "", "ICJ"], ["Duchemin", "Quentin", "", "LAMA"]]}, {"id": "2006.07052", "submitter": "Yasuyuki Hamura", "authors": "Yasuyuki Hamura and Tatsuya Kubokawa", "title": "Bayesian Predictive Density Estimation for a Chi-squared Model Using\n  Information from a Normal Observation with Unknown Mean and Variance", "comments": "18 pages, 1 figure, extensively rewritten", "journal-ref": null, "doi": "10.1016/j.jspi.2021.07.004", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating the density function of\na Chi-squared variable on the basis of observations of another Chi-squared\nvariable and a normal variable under the Kullback-Leibler divergence. We assume\nthat these variables have a common unknown scale parameter and that the mean of\nthe normal variable is also unknown. We compare the risk functions of two\nBayesian predictive densities: one with respect to a hierarchical shrinkage\nprior and the other based on a noninformative prior. The hierarchical Bayesian\npredictive density depends on the normal variable while the Bayesian predictive\ndensity based on the noninformative prior does not. Sufficient conditions for\nthe former to dominate the latter are obtained. These predictive densities are\ncompared by simulation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 10:11:52 GMT"}, {"version": "v2", "created": "Sun, 19 Jul 2020 15:07:43 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Hamura", "Yasuyuki", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "2006.07131", "submitter": "Sebastian Fuchs", "authors": "Thimo M. Kasper, Sebastian Fuchs and Wolfgang Trutschnig", "title": "On weak conditional convergence of bivariate Archimedean and Extreme\n  Value copulas, and consequences to nonparametric estimation", "comments": "23 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Looking at bivariate copulas from the perspective of conditional\ndistributions and considering weak convergence of almost all conditional\ndistributions yields the notion of weak conditional convergence. At first\nglance, this notion of convergence for copulas might seem far too restrictive\nto be of any practical importance - in fact, given samples of a copula $C$ the\ncorresponding empirical copulas do not converge weakly conditional to $C$ with\nprobability one in general. Within the class of Archimedean copulas and the\nclass of Extreme Value copulas, however, standard pointwise convergence and\nweak conditional convergence can even be proved to be equivalent. Moreover, it\ncan be shown that every copula $C$ is the weak conditional limit of a sequence\nof checkerboard copulas. After proving these three main results and pointing\nout some consequences we sketch some implications for two recently introduced\ndependence measures and for the nonparametric estimation of Archimedean and\nExtreme Value copulas.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 12:39:32 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 08:14:07 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Kasper", "Thimo M.", ""], ["Fuchs", "Sebastian", ""], ["Trutschnig", "Wolfgang", ""]]}, {"id": "2006.07167", "submitter": "Indranil SenGupta", "authors": "Shantanu Awasthi and Indranil SenGupta", "title": "First exit-time analysis for an approximate Barndorff-Nielsen and\n  Shephard model with stationary self-decomposable variance process", "comments": "27 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an approximate version of the Barndorff-Nielsen and Shephard\nmodel, driven by a Brownian motion and a L\\'evy subordinator, is formulated.\nThe first-exit time of the log-return process for this model is analyzed. It is\nshown that with certain probability, the first-exit time process of the\nlog-return is decomposable into the sum of the first exit time of the Brownian\nmotion with drift, and the first exit time of a L\\'evy subordinator with drift.\nSubsequently, the probability density functions of the first exit time of some\nspecific L\\'evy subordinators, connected to stationary, self-decomposable\nvariance processes, are studied. Analytical expressions of the probability\ndensity function of the first-exit time of three such L\\'evy subordinators are\nobtained in terms of various special functions. The results are implemented to\nempirical S&P 500 dataset.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 13:23:43 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2020 16:43:44 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 18:03:24 GMT"}, {"version": "v4", "created": "Thu, 7 Jan 2021 20:44:29 GMT"}], "update_date": "2021-01-11", "authors_parsed": [["Awasthi", "Shantanu", ""], ["SenGupta", "Indranil", ""]]}, {"id": "2006.07201", "submitter": "Vasilis Syrgkanis", "authors": "Nishanth Dikkala, Greg Lewis, Lester Mackey, Vasilis Syrgkanis", "title": "Minimax Estimation of Conditional Moment Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an approach for estimating models described via conditional moment\nrestrictions, with a prototypical application being non-parametric instrumental\nvariable regression. We introduce a min-max criterion function, under which the\nestimation problem can be thought of as solving a zero-sum game between a\nmodeler who is optimizing over the hypothesis space of the target model and an\nadversary who identifies violating moments over a test function space. We\nanalyze the statistical estimation rate of the resulting estimator for\narbitrary hypothesis spaces, with respect to an appropriate analogue of the\nmean squared error metric, for ill-posed inverse problems. We show that when\nthe minimax criterion is regularized with a second moment penalty on the test\nfunction and the test function space is sufficiently rich, then the estimation\nrate scales with the critical radius of the hypothesis and test function\nspaces, a quantity which typically gives tight fast rates. Our main result\nfollows from a novel localized Rademacher analysis of statistical learning\nproblems defined via minimax objectives. We provide applications of our main\nresults for several hypothesis spaces used in practice such as: reproducing\nkernel Hilbert spaces, high dimensional sparse linear functions, spaces defined\nvia shape constraints, ensemble estimators such as random forests, and neural\nnetworks. For each of these applications we provide computationally efficient\noptimization methods for solving the corresponding minimax problem (e.g.\nstochastic first-order heuristics for neural networks). In several\napplications, we show how our modified mean squared error rate, combined with\nconditions that bound the ill-posedness of the inverse problem, lead to mean\nsquared error rates. We conclude with an extensive experimental analysis of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 14:02:38 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Dikkala", "Nishanth", ""], ["Lewis", "Greg", ""], ["Mackey", "Lester", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "2006.07279", "submitter": "Benjamin Guedj", "authors": "Maxime Haddouche and Benjamin Guedj and Omar Rivasplata and John\n  Shawe-Taylor", "title": "PAC-Bayes unleashed: generalisation bounds with unbounded losses", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present new PAC-Bayesian generalisation bounds for learning problems with\nunbounded loss functions. This extends the relevance and applicability of the\nPAC-Bayes learning framework, where most of the existing literature focuses on\nsupervised learning problems with a bounded loss function (typically assumed to\ntake values in the interval [0;1]). In order to relax this assumption, we\npropose a new notion called HYPE (standing for \\emph{HYPothesis-dependent\nrangE}), which effectively allows the range of the loss to depend on each\npredictor. Based on this new notion we derive a novel PAC-Bayesian\ngeneralisation bound for unbounded loss functions, and we instantiate it on a\nlinear regression problem. To make our theory usable by the largest audience\npossible, we include discussions on actual computation, practicality and\nlimitations of our assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 15:55:46 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 16:02:45 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Haddouche", "Maxime", ""], ["Guedj", "Benjamin", ""], ["Rivasplata", "Omar", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "2006.07286", "submitter": "Evgenii Chzhen", "authors": "Evgenii Chzhen, Christophe Denis, Mohamed Hebiri, Luca Oneto,\n  Massimiliano Pontil", "title": "Fair Regression with Wasserstein Barycenters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning a real-valued function that satisfies the\nDemographic Parity constraint. It demands the distribution of the predicted\noutput to be independent of the sensitive attribute. We consider the case that\nthe sensitive attribute is available for prediction. We establish a connection\nbetween fair regression and optimal transport theory, based on which we derive\na close form expression for the optimal fair predictor. Specifically, we show\nthat the distribution of this optimum is the Wasserstein barycenter of the\ndistributions induced by the standard regression function on the sensitive\ngroups. This result offers an intuitive interpretation of the optimal fair\nprediction and suggests a simple post-processing algorithm to achieve fairness.\nWe establish risk and distribution-free fairness guarantees for this procedure.\nNumerical experiments indicate that our method is very effective in learning\nfair models, with a relative increase in error rate that is inferior to the\nrelative gain in fairness.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 16:10:41 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 13:22:01 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Chzhen", "Evgenii", ""], ["Denis", "Christophe", ""], ["Hebiri", "Mohamed", ""], ["Oneto", "Luca", ""], ["Pontil", "Massimiliano", ""]]}, {"id": "2006.07291", "submitter": "Holger Dette", "authors": "Holger Dette, Kevin Kokot", "title": "Detecting relevant differences in the covariance operators of functional\n  time series -- a sup-norm approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose statistical inference tools for the covariance\noperators of functional time series in the two sample and change point problem.\nIn contrast to most of the literature the focus of our approach is not testing\nthe null hypothesis of exact equality of the covariance operators. Instead we\npropose to formulate the null hypotheses in them form that \"the distance\nbetween the operators is small\", where we measure deviations by the sup-norm.\nWe provide powerful bootstrap tests for these type of hypotheses, investigate\ntheir asymptotic properties and study their finite sample properties by means\nof a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 16:15:56 GMT"}], "update_date": "2020-06-15", "authors_parsed": [["Dette", "Holger", ""], ["Kokot", "Kevin", ""]]}, {"id": "2006.07315", "submitter": "Jakub Marecek", "authors": "Quan Zhou, Jakub Marecek, Robert N. Shorten", "title": "Fairness in Forecasting and Learning Linear Dynamical Systems", "comments": null, "journal-ref": "Proceedings of the Thirty-Fifth AAAI Conference on Artificial\n  Intelligence, 2021", "doi": null, "report-no": null, "categories": "cs.LG math.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, training data often capture the behaviour of multiple\nsubgroups of some underlying human population. When the amounts of training\ndata for the subgroups are not controlled carefully, under-representation bias\narises. We introduce two natural notions of subgroup fairness and instantaneous\nfairness to address such under-representation bias in time-series forecasting\nproblems. In particular, we consider the subgroup-fair and instant-fair\nlearning of a linear dynamical system (LDS) from multiple trajectories of\nvarying lengths, and the associated forecasting problems. We provide globally\nconvergent methods for the learning problems using hierarchies of\nconvexifications of non-commutative polynomial optimisation problems. Our\nempirical results on a biased data set motivated by insurance applications and\nthe well-known COMPAS data set demonstrate both the beneficial impact of\nfairness considerations on statistical performance and encouraging effects of\nexploiting sparsity on run time.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 16:53:27 GMT"}, {"version": "v2", "created": "Sat, 2 Jan 2021 12:28:19 GMT"}], "update_date": "2021-01-05", "authors_parsed": [["Zhou", "Quan", ""], ["Marecek", "Jakub", ""], ["Shorten", "Robert N.", ""]]}, {"id": "2006.07457", "submitter": "Jelena Bradic", "authors": "Jing Zhou, Gerda Claeskens, Jelena Bradic", "title": "Detangling robustness in high dimensions: composite versus\n  model-averaged estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust methods, though ubiquitous in practice, are yet to be fully understood\nin the context of regularized estimation and high dimensions. Even simple\nquestions become challenging very quickly. For example, classical statistical\ntheory identifies equivalence between model-averaged and composite quantile\nestimation. However, little to nothing is known about such equivalence between\nmethods that encourage sparsity. This paper provides a toolbox to further study\nrobustness in these settings and focuses on prediction. In particular, we study\noptimally weighted model-averaged as well as composite $l_1$-regularized\nestimation. Optimal weights are determined by minimizing the asymptotic mean\nsquared error. This approach incorporates the effects of regularization,\nwithout the assumption of perfect selection, as is often used in practice. Such\nweights are then optimal for prediction quality. Through an extensive\nsimulation study, we show that no single method systematically outperforms\nothers. We find, however, that model-averaged and composite quantile estimators\noften outperform least-squares methods, even in the case of Gaussian model\nnoise. Real data application witnesses the method's practical use through the\nreconstruction of compressed audio signals.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 20:40:15 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Zhou", "Jing", ""], ["Claeskens", "Gerda", ""], ["Bradic", "Jelena", ""]]}, {"id": "2006.07459", "submitter": "Alexander Ritchie", "authors": "Alexander Ritchie, Robert A. Vandermeulen, Clayton Scott", "title": "Consistent Estimation of Identifiable Nonparametric Mixture Models from\n  Grouped Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research has established sufficient conditions for finite mixture\nmodels to be identifiable from grouped observations. These conditions allow the\nmixture components to be nonparametric and have substantial (or even total)\noverlap. This work proposes an algorithm that consistently estimates any\nidentifiable mixture model from grouped observations. Our analysis leverages an\noracle inequality for weighted kernel density estimators of the distribution on\ngroups, together with a general result showing that consistent estimation of\nthe distribution on groups implies consistent estimation of mixture components.\nA practical implementation is provided for paired observations, and the\napproach is shown to outperform existing methods, especially when mixture\ncomponents overlap significantly.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 20:44:22 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Ritchie", "Alexander", ""], ["Vandermeulen", "Robert A.", ""], ["Scott", "Clayton", ""]]}, {"id": "2006.07506", "submitter": "Haoyun Wang", "authors": "Haoyun Wang, Liyan Xie, Alex Cuozzo, Simon Mak, Yao Xie", "title": "Uncertainty Quantification for Inferring Hawkes Networks", "comments": "16 pages including appendix, 1 figure, accepted to 2020 Neurips", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate Hawkes processes are commonly used to model streaming networked\nevent data in a wide variety of applications. However, it remains a challenge\nto extract reliable inference from complex datasets with uncertainty\nquantification. Aiming towards this, we develop a statistical inference\nframework to learn causal relationships between nodes from networked data,\nwhere the underlying directed graph implies Granger causality. We provide\nuncertainty quantification for the maximum likelihood estimate of the network\nmultivariate Hawkes process by providing a non-asymptotic confidence set. The\nmain technique is based on the concentration inequalities of continuous-time\nmartingales. We compare our method to the previously-derived asymptotic Hawkes\nprocess confidence interval, and demonstrate the strengths of our method in an\napplication to neuronal connectivity reconstruction.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2020 23:08:36 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2020 16:24:08 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Wang", "Haoyun", ""], ["Xie", "Liyan", ""], ["Cuozzo", "Alex", ""], ["Mak", "Simon", ""], ["Xie", "Yao", ""]]}, {"id": "2006.07624", "submitter": "Davide Giraudo", "authors": "Herold Dehling, Davide Giraudo, Olimjon Sharipov", "title": "Convergence of the empirical two-sample $U$-statistics with\n  $\\beta$-mixing data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the empirical two-sample $U$-statistic with strictly\n$\\beta$-mixing strictly stationary data and inverstigate its convergence in\nSkorohod spaces. We then provide an application of such convergence.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 11:52:44 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Dehling", "Herold", ""], ["Giraudo", "Davide", ""], ["Sharipov", "Olimjon", ""]]}, {"id": "2006.07642", "submitter": "Andrew McRae", "authors": "Andrew McRae and Justin Romberg and Mark Davenport", "title": "Sample complexity and effective dimension for regression on manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the theory of regression on a manifold using reproducing kernel\nHilbert space methods. Manifold models arise in a wide variety of modern\nmachine learning problems, and our goal is to help understand the effectiveness\nof various implicit and explicit dimensionality-reduction methods that exploit\nmanifold structure. Our first key contribution is to establish a novel\nnonasymptotic version of the Weyl law from differential geometry. From this we\nare able to show that certain spaces of smooth functions on a manifold are\neffectively finite-dimensional, with a complexity that scales according to the\nmanifold dimension rather than any ambient data dimension. Finally, we show\nthat given (potentially noisy) function values taken uniformly at random over a\nmanifold, a kernel regression estimator (derived from the spectral\ndecomposition of the manifold) yields minimax-optimal error bounds that are\ncontrolled by the effective dimension.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 14:09:55 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 13:58:59 GMT"}, {"version": "v3", "created": "Fri, 16 Oct 2020 14:58:46 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["McRae", "Andrew", ""], ["Romberg", "Justin", ""], ["Davenport", "Mark", ""]]}, {"id": "2006.07673", "submitter": "Nicolas Marie", "authors": "H\\'el\\`ene Halconruy and Nicolas Marie", "title": "Kernel Selection in Nonparametric Regression", "comments": "23 pages", "journal-ref": "Mathematical Methods of Statistics 29, 1, 31-55, 2020", "doi": "10.3103/S1066530720010044", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the regression model $Y = b(X) +\\sigma(X)\\varepsilon$, where $X$ has a\ndensity $f$, this paper deals with an oracle inequality for an estimator of\n$bf$, involving a kernel in the sense of Lerasle et al. (2016), selected via\nthe PCO method. In addition to the bandwidth selection for kernel-based\nestimators already studied in Lacour, Massart and Rivoirard (2017) and Comte\nand Marie (2020), the dimension selection for anisotropic projection estimators\nof $f$ and $bf$ is covered.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 16:25:27 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 20:09:38 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Halconruy", "H\u00e9l\u00e8ne", ""], ["Marie", "Nicolas", ""]]}, {"id": "2006.07695", "submitter": "Shuangping Li", "authors": "Emmanuel Abbe, Shuangping Li and Allan Sly", "title": "Learning Sparse Graphons and the Generalized Kesten-Stigum Threshold", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of learning graphons has attracted considerable attention across\nseveral scientific communities, with significant progress over the recent years\nin sparser regimes. Yet, the current techniques still require diverging degrees\nin order to succeed with efficient algorithms in the challenging cases where\nthe local structure of the graph is homogeneous. This paper provides an\nefficient algorithm to learn graphons in the constant expected degree regime.\nThe algorithm is shown to succeed in estimating the rank-$k$ projection of a\ngraphon in the $L_2$ metric if the top $k$ eigenvalues of the graphon satisfy a\ngeneralized Kesten-Stigum condition.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 18:38:05 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Abbe", "Emmanuel", ""], ["Li", "Shuangping", ""], ["Sly", "Allan", ""]]}, {"id": "2006.07727", "submitter": "Cheng Mao", "authors": "Jan-Christian H\\\"utter, Cheng Mao, Philippe Rigollet and Elina Robeva", "title": "Optimal Rates for Estimation of Two-Dimensional Totally Positive\n  Distributions", "comments": "41 pages, 6 figures; accepted for publication in the Electronic\n  Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study minimax estimation of two-dimensional totally positive\ndistributions. Such distributions pertain to pairs of strongly positively\ndependent random variables and appear frequently in statistics and probability.\nIn particular, for distributions with $\\beta$-H\\\"older smooth densities where\n$\\beta \\in (0, 2)$, we observe polynomially faster minimax rates of estimation\nwhen, additionally, the total positivity condition is imposed. Moreover, we\ndemonstrate fast algorithms to compute the proposed estimators and corroborate\nthe theoretical rates of estimation by simulation studies.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2020 21:54:45 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["H\u00fctter", "Jan-Christian", ""], ["Mao", "Cheng", ""], ["Rigollet", "Philippe", ""], ["Robeva", "Elina", ""]]}, {"id": "2006.07904", "submitter": "Lu Yu", "authors": "Lu Yu, Krishnakumar Balasubramanian, Stanislav Volgushev, and Murat A.\n  Erdogdu", "title": "An Analysis of Constant Step Size SGD in the Non-convex Regime:\n  Asymptotic Normality and Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured non-convex learning problems, for which critical points have\nfavorable statistical properties, arise frequently in statistical machine\nlearning. Algorithmic convergence and statistical estimation rates are\nwell-understood for such problems. However, quantifying the uncertainty\nassociated with the underlying training algorithm is not well-studied in the\nnon-convex setting. In order to address this shortcoming, in this work, we\nestablish an asymptotic normality result for the constant step size stochastic\ngradient descent (SGD) algorithm--a widely used algorithm in practice.\nSpecifically, based on the relationship between SGD and Markov Chains [DDB19],\nwe show that the average of SGD iterates is asymptotically normally distributed\naround the expected value of their unique invariant distribution, as long as\nthe non-convex and non-smooth objective function satisfies a dissipativity\nproperty. We also characterize the bias between this expected value and the\ncritical points of the objective function under various local regularity\nconditions. Together, the above two results could be leveraged to construct\nconfidence intervals for non-convex problems that are trained using the SGD\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 13:58:44 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 01:27:47 GMT"}], "update_date": "2020-07-31", "authors_parsed": [["Yu", "Lu", ""], ["Balasubramanian", "Krishnakumar", ""], ["Volgushev", "Stanislav", ""], ["Erdogdu", "Murat A.", ""]]}, {"id": "2006.07941", "submitter": "Pavel S. Ruzankin", "authors": "Pavel S. Ruzankin", "title": "On absolute central moments of Poisson distribution", "comments": null, "journal-ref": "Journal of Statistical Theory and Practice, volume 14, Article\n  number: 56 (2020)", "doi": "10.1007/s42519-020-00121-8", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recurrence formula for absolute central moments of Poisson distribution is\nsuggested.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 16:09:45 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Ruzankin", "Pavel S.", ""]]}, {"id": "2006.07953", "submitter": "Jorio Cocola", "authors": "Jorio Cocola, Paul Hand, Vladislav Voroninski", "title": "Nonasymptotic Guarantees for Spiked Matrix Recovery with Generative\n  Priors", "comments": "To appear at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many problems in statistics and machine learning require the reconstruction\nof a rank-one signal matrix from noisy data. Enforcing additional prior\ninformation on the rank-one component is often key to guaranteeing good\nrecovery performance. One such prior on the low-rank component is sparsity,\ngiving rise to the sparse principal component analysis problem. Unfortunately,\nthere is strong evidence that this problem suffers from a\ncomputational-to-statistical gap, which may be fundamental. In this work, we\nstudy an alternative prior where the low-rank component is in the range of a\ntrained generative network. We provide a non-asymptotic analysis with optimal\nsample complexity, up to logarithmic factors, for rank-one matrix recovery\nunder an expansive-Gaussian network prior. Specifically, we establish a\nfavorable global optimization landscape for a nonlinear least squares\nobjective, provided the number of samples is on the order of the dimensionality\nof the input to the generative model. This result suggests that generative\npriors have no computational-to-statistical gap for structured rank-one matrix\nrecovery in the finite data, nonasymptotic regime. We present this analysis in\nthe case of both the Wishart and Wigner spiked matrix models.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 16:46:16 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 05:08:16 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Cocola", "Jorio", ""], ["Hand", "Paul", ""], ["Voroninski", "Vladislav", ""]]}, {"id": "2006.08010", "submitter": "Thi Phuong Thuy Vo", "authors": "Viet Chi Tran, Thi Phuong Thuy Vo", "title": "Estimation of dense stochastic block models visited by random walks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in recovering information on a stochastic block model from\nthe subgraph discovered by an exploring random walk. Stochastic block models\ncorrespond to populations structured into a finite number of types, where two\nindividuals are connected by an edge independently from the other pairs and\nwith a probability depending on their types. We consider here the dense case\nwhere the random network can be approximated by a graphon. This problem is\nmotivated from the study of chain-referral surveys where each interviewee\nprovides information on her/his contacts in the social network. First, we write\nthe likelihood of the subgraph discovered by the random walk: biases are\nappearing since hubs and majority types are more likely to be sampled. Even for\nthe case where the types are observed, the maximum likelihood estimator is not\nexplicit any more. When the types of the vertices is unobserved, we use an SAEM\nalgorithm to maximize the likelihood. Second, we propose a different estimation\nstrategy using new results by Athreya and Roellin. It consists in de-biasing\nthe maximum likelihood estimator proposed in Daudin et al. and that ignores the\nbiases.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 20:22:14 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 13:39:19 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tran", "Viet Chi", ""], ["Vo", "Thi Phuong Thuy", ""]]}, {"id": "2006.08172", "submitter": "Lenaic Chizat", "authors": "Lenaic Chizat (LMO), Pierre Roussillon (DMA), Flavien L\\'eger (DMA),\n  Fran\\c{c}ois-Xavier Vialard (Univ Gustave Eiffel), Gabriel Peyr\\'e (DMA)", "title": "Faster Wasserstein Distance Estimation with the Sinkhorn Divergence", "comments": null, "journal-ref": "Neural Information Processing Systems, Dec 2020, Vancouver, Canada", "doi": null, "report-no": null, "categories": "math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The squared Wasserstein distance is a natural quantity to compare probability\ndistributions in a non-parametric setting. This quantity is usually estimated\nwith the plug-in estimator, defined via a discrete optimal transport problem\nwhich can be solved to $\\epsilon$-accuracy by adding an entropic regularization\nof order $\\epsilon$ and using for instance Sinkhorn's algorithm. In this work,\nwe propose instead to estimate it with the Sinkhorn divergence, which is also\nbuilt on entropic regularization but includes debiasing terms. We show that,\nfor smooth densities, this estimator has a comparable sample complexity but\nallows higher regularization levels, of order $\\epsilon^{1/2}$, which leads to\nimproved computational complexity bounds and a strong speedup in practice. Our\ntheoretical analysis covers the case of both randomly sampled densities and\ndeterministic discretizations on uniform grids. We also propose and analyze an\nestimator based on Richardson extrapolation of the Sinkhorn divergence which\nenjoys improved statistical and computational efficiency guarantees, under a\ncondition on the regularity of the approximation error, which is in particular\nsatisfied for Gaussian densities. We finally demonstrate the efficiency of the\nproposed estimators with numerical experiments.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 06:58:16 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 15:15:37 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Chizat", "Lenaic", "", "LMO"], ["Roussillon", "Pierre", "", "DMA"], ["L\u00e9ger", "Flavien", "", "DMA"], ["Vialard", "Fran\u00e7ois-Xavier", "", "Univ Gustave Eiffel"], ["Peyr\u00e9", "Gabriel", "", "DMA"]]}, {"id": "2006.08189", "submitter": "Anuran Makur", "authors": "Ali Jadbabaie and Anuran Makur and Devavrat Shah", "title": "Estimation of Skill Distributions", "comments": "37 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning the skill distribution of a\npopulation of agents from observations of pairwise games in a tournament. These\ngames are played among randomly drawn agents from the population. The agents in\nour model can be individuals, sports teams, or Wall Street fund managers.\nFormally, we postulate that the likelihoods of game outcomes are governed by\nthe Bradley-Terry-Luce (or multinomial logit) model, where the probability of\nan agent beating another is the ratio between its skill level and the pairwise\nsum of skill levels, and the skill parameters are drawn from an unknown skill\ndensity of interest. The problem is, in essence, to learn a distribution from\nnoisy, quantized observations. We propose a simple and tractable algorithm that\nlearns the skill density with near-optimal minimax mean squared error scaling\nas $n^{-1+\\varepsilon}$, for any $\\varepsilon>0$, when the density is smooth.\nOur approach brings together prior work on learning skill parameters from\npairwise comparisons with kernel density estimation from non-parametric\nstatistics. Furthermore, we prove minimax lower bounds which establish minimax\noptimality of the skill parameter estimation technique used in our algorithm.\nThese bounds utilize a continuum version of Fano's method along with a covering\nargument. We apply our algorithm to various soccer leagues and world cups,\ncricket world cups, and mutual funds. We find that the entropy of a learnt\ndistribution provides a quantitative measure of skill, which provides rigorous\nexplanations for popular beliefs about perceived qualities of sporting events,\ne.g., soccer league rankings. Finally, we apply our method to assess the skill\ndistributions of mutual funds. Our results shed light on the abundance of low\nquality funds prior to the Great Recession of 2008, and the domination of the\nindustry by more skilled funds after the financial crisis.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 07:35:37 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Jadbabaie", "Ali", ""], ["Makur", "Anuran", ""], ["Shah", "Devavrat", ""]]}, {"id": "2006.08550", "submitter": "Kenta Oono", "authors": "Kenta Oono, Taiji Suzuki", "title": "Optimization and Generalization Analysis of Transduction through\n  Gradient Boosting and Application to Multi-scale Graph Neural Networks", "comments": "9 pages, Reference 6 pages, Supplemental material 18 pages. Accepted\n  at Neural Information Processing Systems (NeurIPS) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the current graph neural networks (GNNs) are difficult to\nmake themselves deep due to the problem known as over-smoothing. Multi-scale\nGNNs are a promising approach for mitigating the over-smoothing problem.\nHowever, there is little explanation of why it works empirically from the\nviewpoint of learning theory. In this study, we derive the optimization and\ngeneralization guarantees of transductive learning algorithms that include\nmulti-scale GNNs. Using the boosting theory, we prove the convergence of the\ntraining error under weak learning-type conditions. By combining it with\ngeneralization gap bounds in terms of transductive Rademacher complexity, we\nshow that a test error bound of a specific type of multi-scale GNNs that\ndecreases corresponding to the number of node aggregations under some\nconditions. Our results offer theoretical explanations for the effectiveness of\nthe multi-scale structure against the over-smoothing problem. We apply boosting\nalgorithms to the training of multi-scale GNNs for real-world node prediction\ntasks. We confirm that its performance is comparable to existing GNNs, and the\npractical behaviors are consistent with theoretical observations. Code is\navailable at https://github.com/delta2323/GB-GNN.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 17:06:17 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 06:18:49 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 14:17:46 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Oono", "Kenta", ""], ["Suzuki", "Taiji", ""]]}, {"id": "2006.08580", "submitter": "Changxiao Cai", "authors": "Changxiao Cai, H. Vincent Poor, Yuxin Chen", "title": "Uncertainty quantification for nonconvex tensor completion: Confidence\n  intervals, heteroscedasticity and optimality", "comments": "Accepted in part to ICML 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.IT cs.LG math.IT math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distribution and uncertainty of nonconvex optimization for noisy\ntensor completion -- the problem of estimating a low-rank tensor given\nincomplete and corrupted observations of its entries. Focusing on a two-stage\nestimation algorithm proposed by Cai et al. (2019), we characterize the\ndistribution of this nonconvex estimator down to fine scales. This\ndistributional theory in turn allows one to construct valid and short\nconfidence intervals for both the unseen tensor entries and the unknown tensor\nfactors. The proposed inferential procedure enjoys several important features:\n(1) it is fully adaptive to noise heteroscedasticity, and (2) it is data-driven\nand automatically adapts to unknown noise distributions. Furthermore, our\nfindings unveil the statistical optimality of nonconvex tensor completion: it\nattains un-improvable $\\ell_{2}$ accuracy -- including both the rates and the\npre-constants -- when estimating both the unknown tensor and the underlying\ntensor factors.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 17:47:13 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Cai", "Changxiao", ""], ["Poor", "H. Vincent", ""], ["Chen", "Yuxin", ""]]}, {"id": "2006.08843", "submitter": "Adrian Bishop", "authors": "Adrian N. Bishop and Pierre Del Moral", "title": "On the Mathematical Theory of Ensemble (Linear-Gaussian) Kalman-Bucy\n  Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this review is to present a comprehensive overview of the\ntheory of ensemble Kalman-Bucy filtering for linear-Gaussian signal models. We\npresent a system of equations that describe the flow of individual particles\nand the flow of the sample covariance and the sample mean in continuous-time\nensemble filtering. We consider these equations and their characteristics in a\nnumber of popular ensemble Kalman filtering variants. Given these equations, we\nstudy their asymptotic convergence to the optimal Bayesian filter. We also\nstudy in detail some non-asymptotic time-uniform fluctuation, stability, and\ncontraction results on the sample covariance and sample mean (or sample error\ntrack). We focus on testable signal/observation model conditions, and we\naccommodate fully unstable (latent) signal models. We discuss the relevance and\nimportance of these results in characterising the filter's behaviour, e.g. it's\nsignal tracking performance, and we contrast these results with those in\nclassical studies of stability in Kalman-Bucy filtering. We provide intuition\nfor how these results extend to nonlinear signal models and comment on their\nconsequence on some typical filter behaviours seen in practice, e.g.\ncatastrophic divergence.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 00:41:18 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Bishop", "Adrian N.", ""], ["Del Moral", "Pierre", ""]]}, {"id": "2006.08855", "submitter": "Ye Tian", "authors": "Ye Tian and Yang Feng", "title": "RaSE: Random Subspace Ensemble Classification", "comments": "93 pages, 13 figures", "journal-ref": "Journal of Machine Learning Research 22, no. 45 (2021): 1-93", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible ensemble classification framework, Random Subspace\nEnsemble (RaSE), for sparse classification. In the RaSE algorithm, we aggregate\nmany weak learners, where each weak learner is a base classifier trained in a\nsubspace optimally selected from a collection of random subspaces. To conduct\nsubspace selection, we propose a new criterion, ratio information criterion\n(RIC), based on weighted Kullback-Leibler divergence. The theoretical analysis\nincludes the risk and Monte-Carlo variance of the RaSE classifier, establishing\nthe screening consistency and weak consistency of RIC, and providing an upper\nbound for the misclassification rate of the RaSE classifier. In addition, we\nshow that in a high-dimensional framework, the number of random subspaces needs\nto be very large to guarantee that a subspace covering signals is selected.\nTherefore, we propose an iterative version of the RaSE algorithm and prove that\nunder some specific conditions, a smaller number of generated random subspaces\nare needed to find a desirable subspace through iteration. An array of\nsimulations under various models and real-data applications demonstrate the\neffectiveness and robustness of the RaSE classifier and its iterative version\nin terms of low misclassification rate and accurate feature ranking. The RaSE\nalgorithm is implemented in the R package RaSEn on CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 01:14:38 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 18:59:19 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 15:54:44 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tian", "Ye", ""], ["Feng", "Yang", ""]]}, {"id": "2006.08945", "submitter": "Evan Patterson", "authors": "Evan Patterson", "title": "The algebra and machine representation of statistical models", "comments": "Revised and extended version of author's PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LO math.CT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the twin movements of open science and open source bring an ever greater\nshare of the scientific process into the digital realm, new opportunities arise\nfor the meta-scientific study of science itself, including of data science and\nstatistics. Future science will likely see machines play an active role in\nprocessing, organizing, and perhaps even creating scientific knowledge. To make\nthis possible, large engineering efforts must be undertaken to transform\nscientific artifacts into useful computational resources, and conceptual\nadvances must be made in the organization of scientific theories, models,\nexperiments, and data.\n  This dissertation takes steps toward digitizing and systematizing two major\nartifacts of data science, statistical models and data analyses. Using tools\nfrom algebra, particularly categorical logic, a precise analogy is drawn\nbetween models in statistics and logic, enabling statistical models to be seen\nas models of theories, in the logical sense. Statistical theories, being\nalgebraic structures, are amenable to machine representation and are equipped\nwith morphisms that formalize the relations between different statistical\nmethods. Turning from mathematics to engineering, a software system for\ncreating machine representations of data analyses, in the form of Python or R\nprograms, is designed and implemented. The representations aim to capture the\nsemantics of data analyses, independent of the programming language and\nlibraries in which they are implemented.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 06:33:50 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Patterson", "Evan", ""]]}, {"id": "2006.09017", "submitter": "Zhan Yu", "authors": "Zhan Yu, Daniel W. C. Ho", "title": "Estimates on Learning Rates for Multi-Penalty Distribution Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with functional learning by utilizing two-stage\nsampled distribution regression. We study a multi-penalty regularization\nalgorithm for distribution regression under the framework of learning theory.\nThe algorithm aims at regressing to real valued outputs from probability\nmeasures. The theoretical analysis on distribution regression is far from\nmaturity and quite challenging, since only second stage samples are observable\nin practical setting. In the algorithm, to transform information from samples,\nwe embed the distributions to a reproducing kernel Hilbert space\n$\\mathcal{H}_K$ associated with Mercer kernel $K$ via mean embedding technique.\nThe main contribution of the paper is to present a novel multi-penalty\nregularization algorithm to capture more features of distribution regression\nand derive optimal learning rates for the algorithm. The work also derives\nlearning rates for distribution regression in the nonstandard setting\n$f_{\\rho}\\notin\\mathcal{H}_K$, which is not explored in existing literature.\nMoreover, we propose a distribution regression-based distributed learning\nalgorithm to face large-scale data or information challenge. The optimal\nlearning rates are derived for the distributed learning algorithm. By providing\nnew algorithms and showing their learning rates, we improve the existing work\nin different aspects in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 09:31:58 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Yu", "Zhan", ""], ["Ho", "Daniel W. C.", ""]]}, {"id": "2006.09223", "submitter": "Vincent Plassier", "authors": "Vincent Plassier, Fran\\c{c}ois Portier, Johan Segers", "title": "Risk bounds when learning infinitely many response functions by ordinary\n  linear regression", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the problem of learning a large number of response functions\nsimultaneously based on the same input variables. The training data consist of\na single independent random sample of the input variables drawn from a common\ndistribution together with the associated responses. The input variables are\nmapped into a high-dimensional linear space, called the feature space, and the\nresponse functions are modelled as linear functionals of the mapped features,\nwith coefficients calibrated via ordinary least squares. We provide convergence\nguarantees on the worst-case excess prediction risk by controlling the\nconvergence rate of the excess risk uniformly in the response function. The\ndimension of the feature map is allowed to tend to infinity with the sample\nsize. The collection of response functions, although potentially infinite, is\nsupposed to have a finite Vapnik-Chervonenkis dimension. The bound derived can\nbe applied when building multiple surrogate models in a reasonable computing\ntime.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 14:54:21 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 14:44:26 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Plassier", "Vincent", ""], ["Portier", "Fran\u00e7ois", ""], ["Segers", "Johan", ""]]}, {"id": "2006.09268", "submitter": "Carl-Johann Simon-Gabriel", "authors": "Carl-Johann Simon-Gabriel and Alessandro Barp and Bernhard Sch\\\"olkopf\n  and Lester Mackey", "title": "Metrizing Weak Convergence with Maximum Mean Discrepancies", "comments": "14 pages. Corrects in particular Thm.12 of Simon-Gabriel and\n  Sch\\\"olkopf, JMLR, 19(44):1-29, 2018. See\n  http://jmlr.org/papers/v19/16-291.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.PR math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper characterizes the maximum mean discrepancies (MMD) that metrize\nthe weak convergence of probability measures for a wide class of kernels. More\nprecisely, we prove that, on a locally compact, non-compact, Hausdorff space,\nthe MMD of a bounded continuous Borel measurable kernel k, whose reproducing\nkernel Hilbert space (RKHS) functions vanish at infinity, metrizes the weak\nconvergence of probability measures if and only if k is continuous and\nintegrally strictly positive definite (i.s.p.d.) over all signed, finite,\nregular Borel measures. We also correct a prior result of Simon-Gabriel &\nSch\\\"olkopf (JMLR, 2018, Thm.12) by showing that there exist both bounded\ncontinuous i.s.p.d. kernels that do not metrize weak convergence and bounded\ncontinuous non-i.s.p.d. kernels that do metrize it.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 15:49:33 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 11:35:16 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Simon-Gabriel", "Carl-Johann", ""], ["Barp", "Alessandro", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Mackey", "Lester", ""]]}, {"id": "2006.09319", "submitter": "Mamikon Gulian", "authors": "Laura Swiler, Mamikon Gulian, Ari Frankel, Cosmin Safta, John Jakeman", "title": "A Survey of Constrained Gaussian Process Regression: Approaches and\n  Implementation Challenges", "comments": "42 pages, 3 figures. Version 3: DOI & Reference added; appeared in\n  Journal of Machine Learning for Modeling and Computing. Version 2 includes\n  minor additions, clarifications and improvements to notation", "journal-ref": "Journal of Machine Learning for Modeling and Computing,\n  1(2):119-156 (2020)", "doi": "10.1615/JMachLearnModelComput.2020035155", "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression is a popular Bayesian framework for surrogate\nmodeling of expensive data sources. As part of a broader effort in scientific\nmachine learning, many recent works have incorporated physical constraints or\nother a priori information within Gaussian process regression to supplement\nlimited data and regularize the behavior of the model. We provide an overview\nand survey of several classes of Gaussian process constraints, including\npositivity or bound constraints, monotonicity and convexity constraints,\ndifferential equation constraints provided by linear PDEs, and boundary\ncondition constraints. We compare the strategies behind each approach as well\nas the differences in implementation, concluding with a discussion of the\ncomputational challenges introduced by constraints.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:03:36 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2020 18:55:38 GMT"}, {"version": "v3", "created": "Wed, 6 Jan 2021 17:45:06 GMT"}], "update_date": "2021-01-07", "authors_parsed": [["Swiler", "Laura", ""], ["Gulian", "Mamikon", ""], ["Frankel", "Ari", ""], ["Safta", "Cosmin", ""], ["Jakeman", "John", ""]]}, {"id": "2006.09355", "submitter": "Huy Tuan Pham", "authors": "Huy Tuan Pham, Phan-Minh Nguyen", "title": "A Note on the Global Convergence of Multilayer Neural Networks in the\n  Mean Field Regime", "comments": "Companion note to arXiv:2001.11443", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent work, we introduced a rigorous framework to describe the mean\nfield limit of the gradient-based learning dynamics of multilayer neural\nnetworks, based on the idea of a neuronal embedding. There we also proved a\nglobal convergence guarantee for three-layer (as well as two-layer) networks\nusing this framework.\n  In this companion note, we point out that the insights in our previous work\ncan be readily extended to prove a global convergence guarantee for multilayer\nnetworks of any depths. Unlike our previous three-layer global convergence\nguarantee that assumes i.i.d. initializations, our present result applies to a\ntype of correlated initialization. This initialization allows to, at any finite\ntraining time, propagate a certain universal approximation property through the\ndepth of the neural network. To achieve this effect, we introduce a\nbidirectional diversity condition.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:50:34 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Pham", "Huy Tuan", ""], ["Nguyen", "Phan-Minh", ""]]}, {"id": "2006.09356", "submitter": "Pierre Tarrago", "authors": "Pierre Tarrago", "title": "Spectral deconvolution of unitarily invariant matrix models", "comments": "Version 2 : minor changes, references added and improved\n  presentation. 60 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper implements a complex analytic method to recover the\nspectrum of a matrix perturbed by either the addition or the multiplication of\na random matrix noise, under the assumption that the distribution of the noise\nis unitarily invariant. This method, introduced by Arizmendi, Tarrago and\nVargas in arXiv:1711.08871, is done in two steps : the first step consists in a\nfixed point method to compute the Stieltjes transform of the desired\ndistribution in a certain domain, and the second step is a classical\ndeconvolution by a Cauchy distribution, whose parameter depends on the\nintensity of the noise. We also provide explicit bounds for the mean squared\nerror of the first step.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 17:51:29 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 15:55:44 GMT"}], "update_date": "2020-11-25", "authors_parsed": [["Tarrago", "Pierre", ""]]}, {"id": "2006.09431", "submitter": "Alexander Heaton", "authors": "Yulia Alexandr, Alexander Heaton", "title": "Logarithmic Voronoi cells", "comments": "19 pages, 5 figures", "journal-ref": "Alg. Stat. 12 (2021) 75-95", "doi": "10.2140/astat.2021.12.75", "report-no": null, "categories": "math.ST math.AG math.MG stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Voronoi cells in the statistical setting by considering preimages of\nthe maximum likelihood estimator that tessellate an open probability simplex.\nIn general, logarithmic Voronoi cells are convex sets. However, for certain\nalgebraic models, namely finite models, models with ML degree 1, linear models,\nand log-linear (or toric) models, we show that logarithmic Voronoi cells are\npolytopes. As a corollary, the algebraic moment map has polytopes for both its\nfibres and its image, when restricted to the simplex. We also compute\nnon-polytopal logarithmic Voronoi cells using numerical algebraic geometry.\nFinally, we determine logarithmic Voronoi polytopes for the finite model\nconsisting of all empirical distributions of a fixed sample size. These\npolytopes are dual to the logarithmic root polytopes of Lie type A, and we\ncharacterize their faces.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 18:23:07 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Alexandr", "Yulia", ""], ["Heaton", "Alexander", ""]]}, {"id": "2006.09439", "submitter": "Song Wei", "authors": "Song Wei, Shixiang Zhu, Minghe Zhang, Yao Xie", "title": "Goodness-of-Fit Test for Mismatched Self-Exciting Processes", "comments": "28 pages, 11 figures, 3 tables. Accepted to AISTATS 2021.\n  Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there have been many research efforts in developing generative\nmodels for self-exciting point processes, partly due to their broad\napplicability for real-world applications. However, rarely can we quantify how\nwell the generative model captures the nature or ground-truth since it is\nusually unknown. The challenge typically lies in the fact that the generative\nmodels typically provide, at most, good approximations to the ground-truth\n(e.g., through the rich representative power of neural networks), but they\ncannot be precisely the ground-truth. We thus cannot use the classic\ngoodness-of-fit (GOF) test framework to evaluate their performance. In this\npaper, we develop a GOF test for generative models of self-exciting processes\nby making a new connection to this problem with the classical statistical\ntheory of Quasi-maximum-likelihood estimator (QMLE). We present a\nnon-parametric self-normalizing statistic for the GOF test: the Generalized\nScore (GS) statistics, and explicitly capture the model misspecification when\nestablishing the asymptotic distribution of the GS statistic. Numerical\nsimulation and real-data experiments validate our theory and demonstrate the\nproposed GS test's good performance.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2020 18:31:33 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 14:26:57 GMT"}, {"version": "v3", "created": "Fri, 12 Feb 2021 16:33:13 GMT"}], "update_date": "2021-02-15", "authors_parsed": [["Wei", "Song", ""], ["Zhu", "Shixiang", ""], ["Zhang", "Minghe", ""], ["Xie", "Yao", ""]]}, {"id": "2006.09666", "submitter": "Xuan Yin", "authors": "Zenan Wang, Xuan Yin, Tianbo Li, Liangjie Hong", "title": "Causal Meta-Mediation Analysis: Inferring Dose-Response Function From\n  Summary Statistics of Many Randomized Experiments", "comments": "In Proceedings of the 26th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD '20), August 23-27, 2020, Virtual Event, CA,\n  USA. ACM, New York, NY, USA, 11 pages", "journal-ref": "Proceedings of the 26th ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD '20), August 23-27, 2020, Virtual Event, CA,\n  USA. ACM, New York, NY, USA, 11 pages", "doi": "10.1145/3394486.3403313", "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common in the internet industry to use offline-developed algorithms to\npower online products that contribute to the success of a business.\nOffline-developed algorithms are guided by offline evaluation metrics, which\nare often different from online business key performance indicators (KPIs). To\nmaximize business KPIs, it is important to pick a north star among all\navailable offline evaluation metrics. By noting that online products can be\nmeasured by online evaluation metrics, the online counterparts of offline\nevaluation metrics, we decompose the problem into two parts. As the offline A/B\ntest literature works out the first part: counterfactual estimators of offline\nevaluation metrics that move the same way as their online counterparts, we\nfocus on the second part: causal effects of online evaluation metrics on\nbusiness KPIs. The north star of offline evaluation metrics should be the one\nwhose online counterpart causes the most significant lift in the business KPI.\nWe model the online evaluation metric as a mediator and formalize its causality\nwith the business KPI as dose-response function (DRF). Our novel approach,\ncausal meta-mediation analysis, leverages summary statistics of many existing\nrandomized experiments to identify, estimate, and test the mediator DRF. It is\neasy to implement and to scale up, and has many advantages over the literature\nof mediation analysis and meta-analysis. We demonstrate its effectiveness by\nsimulation and implementation on real data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 05:41:19 GMT"}], "update_date": "2020-12-16", "authors_parsed": [["Wang", "Zenan", ""], ["Yin", "Xuan", ""], ["Li", "Tianbo", ""], ["Hong", "Liangjie", ""]]}, {"id": "2006.09735", "submitter": "Arnab Bhattacharyya", "authors": "Arnab Bhattacharyya and Rathin Desai and Sai Ganesh Nagarajan and\n  Ioannis Panageas", "title": "Efficient Statistics for Sparse Graphical Models from Truncated Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DS cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study high-dimensional estimation from truncated samples.\nWe focus on two fundamental and classical problems: (i) inference of sparse\nGaussian graphical models and (ii) support recovery of sparse linear models.\n  (i) For Gaussian graphical models, suppose $d$-dimensional samples ${\\bf x}$\nare generated from a Gaussian $N(\\mu,\\Sigma)$ and observed only if they belong\nto a subset $S \\subseteq \\mathbb{R}^d$. We show that ${\\mu}$ and ${\\Sigma}$ can\nbe estimated with error $\\epsilon$ in the Frobenius norm, using\n$\\tilde{O}\\left(\\frac{\\textrm{nz}({\\Sigma}^{-1})}{\\epsilon^2}\\right)$ samples\nfrom a truncated $\\mathcal{N}({\\mu},{\\Sigma})$ and having access to a\nmembership oracle for $S$. The set $S$ is assumed to have non-trivial measure\nunder the unknown distribution but is otherwise arbitrary.\n  (ii) For sparse linear regression, suppose samples $({\\bf x},y)$ are\ngenerated where $y = {\\bf x}^\\top{{\\Omega}^*} + \\mathcal{N}(0,1)$ and $({\\bf\nx}, y)$ is seen only if $y$ belongs to a truncation set $S \\subseteq\n\\mathbb{R}$. We consider the case that ${\\Omega}^*$ is sparse with a support\nset of size $k$. Our main result is to establish precise conditions on the\nproblem dimension $d$, the support size $k$, the number of observations $n$,\nand properties of the samples and the truncation that are sufficient to recover\nthe support of ${\\Omega}^*$. Specifically, we show that under some mild\nassumptions, only $O(k^2 \\log d)$ samples are needed to estimate ${\\Omega}^*$\nin the $\\ell_\\infty$-norm up to a bounded error.\n  For both problems, our estimator minimizes the sum of the finite population\nnegative log-likelihood function and an $\\ell_1$-regularization term.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 09:21:00 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Bhattacharyya", "Arnab", ""], ["Desai", "Rathin", ""], ["Nagarajan", "Sai Ganesh", ""], ["Panageas", "Ioannis", ""]]}, {"id": "2006.09770", "submitter": "Lahcen Douge", "authors": "L. Douge", "title": "A Berry-Esseen theorem for sample quantiles under association", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, the uniformly asymptotic normality for sample quantiles of\nassociated random variables is investigated under some conditions on the decay\nof the covariances. We obtain the rate of normal approximation of order\n$O(n^{-1/2}\\log^2 n)$ if the covariances decrease exponentially to $0$. The\nbest rate is shown as $O(n^{-1/3})$ under a polynomial decay of the\ncovariances.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 10:42:58 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Douge", "L.", ""]]}, {"id": "2006.09813", "submitter": "Peter K\\\"oves\\'arki PhD", "authors": "Peter K\\\"ovesarki", "title": "Occam's Ghost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article applies the principle of Occam's Razor to non-parametric model\nbuilding of statistical data, by finding a model with the minimal number of\nbits, leading to an exceptionally effective regularization method for\nprobability density estimators. The idea comes from the fact that likelihood\nmaximization also minimizes the number of bits required to encode a dataset.\nHowever, traditional methods overlook that the optimization of model parameters\nmay also inadvertently play the part in encoding data points. The article shows\nhow to extend the bit counting to the model parameters as well, providing the\nfirst true measure of complexity for parametric models. Minimizing the total\nbit requirement of a model of a dataset favors smaller derivatives, smoother\nprobability density function estimates and most importantly, a phase space with\nfewer relevant parameters. In fact, it is able prune parameters and detect\nfeatures with small probability at the same time. It is also shown, how it can\nbe applied to any smooth, non-parametric probability density estimator.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2020 20:25:09 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["K\u00f6vesarki", "Peter", ""]]}, {"id": "2006.09975", "submitter": "Gunnar Taraldsen", "authors": "Gunnar Taraldsen and Bo H. Lindqvist", "title": "Fiducial and Posterior Sampling", "comments": null, "journal-ref": "Communications in Statistics - Theory and Methods, 44: 3754-3767,\n  2015", "doi": "10.1080/03610926.2013.823207", "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fiducial coincides with the posterior in a group model equipped with the\nright Haar prior. This result is here generalized. For this the underlying\nprobability space of Kolmogorov is replaced by a $\\sigma$-finite measure space\nand fiducial theory is presented within this frame. Examples are presented that\ndemonstrate that this also gives good alternatives to existing Bayesian\nsampling methods. It is proved that the results provided here for fiducial\nmodels imply that the theory of invariant measures for groups cannot be\ngeneralized directly to loops: There exist a smooth one-dimensional loop where\nan invariant measure does not exist.\n  Keywords: Conditional sampling, Improper prior, Haar prior, Sufficient\nstatistic, Quasi-group\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 12:14:54 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Taraldsen", "Gunnar", ""], ["Lindqvist", "Bo H.", ""]]}, {"id": "2006.10012", "submitter": "Siddharth Vishwanath", "authors": "Siddharth Vishwanath and Kenji Fukumizu and Satoshi Kuriki and Bharath\n  Sriperumbudur", "title": "Robust Persistence Diagrams using Reproducing Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CG cs.LG math.AT stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistent homology has become an important tool for extracting geometric and\ntopological features from data, whose multi-scale features are summarized in a\npersistence diagram. From a statistical perspective, however, persistence\ndiagrams are very sensitive to perturbations in the input space. In this work,\nwe develop a framework for constructing robust persistence diagrams from\nsuperlevel filtrations of robust density estimators constructed using\nreproducing kernels. Using an analogue of the influence function on the space\nof persistence diagrams, we establish the proposed framework to be less\nsensitive to outliers. The robust persistence diagrams are shown to be\nconsistent estimators in bottleneck distance, with the convergence rate\ncontrolled by the smoothness of the kernel. This, in turn, allows us to\nconstruct uniform confidence bands in the space of persistence diagrams.\nFinally, we demonstrate the superiority of the proposed approach on benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:16:52 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Vishwanath", "Siddharth", ""], ["Fukumizu", "Kenji", ""], ["Kuriki", "Satoshi", ""], ["Sriperumbudur", "Bharath", ""]]}, {"id": "2006.10018", "submitter": "Ahad Jamalizadeh", "authors": "Me'raj Abdi, Mohsen Madadi, N. Balakrishnan, Ahad Jamalizadeh", "title": "Family of mean-mixtures of multivariate normal distributions:\n  properties, inference and assessment of multivariate skewness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new mixture family of multivariate normal distributions,\nformed by mixing multivariate normal distribution and skewed distribution, is\nconstructed. Some properties of this family, such as characteristic function,\nmoment generating function, and the first four moments are derived. The\ndistributions of affine transformations and canonical forms of the model are\nalso derived. An EM type algorithm is developed for the maximum likelihood\nestimation of model parameters. We have considered in detail, some special\ncases of the family, using standard gamma and standard exponential mixture\ndistributions, denoted by MMNG and MMNE, respectively. For the proposed family\nof distributions, different multivariate measures of skewness are computed. In\norder to examine the performance of the developed estimation method, some\nsimulation studies are carried out to show that the maximum likelihood\nestimates based on the EM type algorithm do provide good performance. For\ndifferent choices of parameters of MMNE distribution, several multivariate\nmeasures of skewness are computed and compared. Because some measures of\nskewness are scalar and some are vectors, in order to evaluate them properly,\nwe have carried out a simulation study to determine the power of tests, based\non sample versions of skewness measures as test statistics to test the fit of\nthe MMNE distribution. Finally, two real data sets are used to illustrate the\nusefulness of the proposed family of distributions and the associated\ninferential method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:22:30 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2020 07:45:26 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Abdi", "Me'raj", ""], ["Madadi", "Mohsen", ""], ["Balakrishnan", "N.", ""], ["Jamalizadeh", "Ahad", ""]]}, {"id": "2006.10030", "submitter": "Christian Grussler", "authors": "Christian Grussler and Rodolphe Sepulchre", "title": "Variation diminishing linear time-invariant systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the variation diminishing property of $k$-positive linear\ntime-invariant (LTI) systems, which map inputs with $k-1$ sign changes to\noutputs with at most the same variation. We characterize this property for the\nToeplitz and Hankel operators of finite-dimensional systems. Our main result is\nthat these operators have a dominant approximation in the form of series or\nparallel interconnections of $k$ first order positive systems. This is shown by\nexpressing the $k$-positivity of a LTI system as the external positivity (that\nis, $1$-positivity) of $k$ compound LTI systems. Our characterization\ngeneralizes well known properties of externally positive systems ($k=1$) and\ntotally positive systems ($k=\\infty$; also known as relaxation systems).\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 17:49:53 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 02:00:07 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2021 08:29:54 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Grussler", "Christian", ""], ["Sepulchre", "Rodolphe", ""]]}, {"id": "2006.10107", "submitter": "Marius Hofert", "authors": "Marius Hofert", "title": "Right-truncated Archimedean and related copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The copulas of random vectors with standard uniform univariate margins\ntruncated from the right are considered and a general formula for such\nright-truncated conditional copulas is derived. This formula is analytical for\ncopulas that can be inverted analytically as functions of each single argument.\nThis is the case, for example, for Archimedean and related copulas. The\nresulting right-truncated Archimedean copulas are not only analytically\ntractable but can also be characterized as tilted Archimedean copulas. This\nfinding allows one, for example, to more easily derive analytical properties\nsuch as the coefficients of tail dependence or sampling procedures of\nright-truncated Archimedean copulas. As another result, one can easily obtain a\nlimiting Clayton copula for a general vector of truncation points converging to\nzero; this is an important property for (re)insurance and a fact already known\nin the special case of equal truncation points, but harder to prove without\naforementioned characterization. Furthermore, right-truncated Archimax copulas\nwith logistic stable tail dependence functions are characterized as tilted\nouter power Archimedean copulas and an analytical form of right-truncated\nnested Archimedean copulas is also derived.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 19:15:13 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Hofert", "Marius", ""]]}, {"id": "2006.10126", "submitter": "Arvind Thiagarajan", "authors": "Arvind Thiagarajan", "title": "Using Weighted P-Values in Fisher's Method", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher's method prescribes a way to combine p-values from multiple\nexperiments into a single p-value. However, the original method can only\ndetermine a combined p-value analytically if all constituent p-values are\nweighted equally. Here we present, with proof, a method to combine p-values\nwith arbitrary weights.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 19:56:09 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Thiagarajan", "Arvind", ""]]}, {"id": "2006.10163", "submitter": "Zhengjia Wang", "authors": "Zhengjia Wang, John Magnotti, Michael S. Beauchamp and Meng Li", "title": "Functional Group Bridge for Simultaneous Regression and Support\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is motivated by studying multisensory effects on brain\nactivities in intracranial electroencephalography (iEEG) experiments.\nDifferential brain activities to multisensory stimulus presentations are zero\nin most regions and non-zero in some local regions, yielding locally sparse\nfunctions. Such studies are essentially a function-on-scalar regression\nproblem, with interest being focused not only on estimating nonparametric\nfunctions but also on recovering the function supports. We propose a weighted\ngroup bridge approach for simultaneous function estimation and support recovery\nin function-on-scalar mixed effect models, while accounting for heterogeneity\npresent in functional data. We use B-splines to transform sparsity of functions\nto its sparse vector counterpart of increasing dimension, and propose a fast\nnon-convex optimization algorithm using nested alternative direction method of\nmultipliers (ADMM) for estimation. Large sample properties are established. In\nparticular, we show that the estimated coefficient functions are rate optimal\nin the minimax sense under the $L_2$ norm and resemble a phase transition\nphenomenon. For support estimation, we derive a convergence rate under the\n$L_{\\infty}$ norm that leads to a sparsistency property under\n$\\delta$-sparsity, and provide a simple sufficient regularity condition under\nwhich a strict sparsistency property is established. An adjusted extended\nBayesian information criterion is proposed for parameter tuning. The developed\nmethod is illustrated through simulation and an application to a novel iEEG\ndataset to study multisensory integration. We integrate the proposed method\ninto RAVE, an R package that gains increasing popularity in the iEEG community.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 21:18:53 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2020 18:53:06 GMT"}], "update_date": "2020-11-10", "authors_parsed": [["Wang", "Zhengjia", ""], ["Magnotti", "John", ""], ["Beauchamp", "Michael S.", ""], ["Li", "Meng", ""]]}, {"id": "2006.10189", "submitter": "Raaz Dwivedi", "authors": "Raaz Dwivedi, Chandan Singh, Bin Yu, Martin J. Wainwright", "title": "Revisiting complexity and the bias-variance tradeoff", "comments": "First two authors contributed equally. 28 pages, 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of high-dimensional models, such as deep neural networks\n(DNNs), has led many to question the validity of the bias-variance tradeoff\nprinciple in high dimensions. We reexamine it with respect to two key choices:\nthe model class and the complexity measure. We argue that failing to suitably\nspecify either one can falsely suggest that the tradeoff does not hold. This\nobservation motivates us to seek a valid complexity measure, defined with\nrespect to a reasonably good class of models. Building on Rissanen's principle\nof minimum description length (MDL), we propose a novel MDL-based complexity\n(MDL-COMP). We focus on the context of linear models, which have been recently\nused as a stylized tractable approximation to DNNs in high-dimensions. MDL-COMP\nis defined via an optimality criterion over the encodings induced by a good\nRidge estimator class. We derive closed-form expressions for MDL-COMP and show\nthat for a dataset with $n$ observations and $d$ parameters it is \\emph{not\nalways} equal to $d/n$, and is a function of the singular values of the design\nmatrix and the signal-to-noise ratio. For random Gaussian design, we find that\nwhile MDL-COMP scales linearly with $d$ in low-dimensions ($d<n$), for\nhigh-dimensions ($d>n$) the scaling is exponentially smaller, scaling as $\\log\nd$. We hope that such a slow growth of complexity in high-dimensions can help\nshed light on the good generalization performance of several well-tuned\nhigh-dimensional models. Moreover, via an array of simulations and real-data\nexperiments, we show that a data-driven Prac-MDL-COMP can inform\nhyper-parameter tuning for ridge regression in limited data settings, sometimes\nimproving upon cross-validation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 22:45:14 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Dwivedi", "Raaz", ""], ["Singh", "Chandan", ""], ["Yu", "Bin", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "2006.10264", "submitter": "Hang Deng", "authors": "Hang Deng, Qiyang Han and Bodhisattva Sen", "title": "Inference for local parameters in convexity constrained models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of inference for local parameters of a convex\nregression function $f_0: [0,1] \\to \\mathbb{R}$ based on observations from a\nstandard nonparametric regression model, using the convex least squares\nestimator (LSE) $\\widehat{f}_n$. For $x_0 \\in (0,1)$, the local parameters\ninclude the pointwise function value $f_0(x_0)$, the pointwise derivative\n$f_0'(x_0)$, and the anti-mode (i.e., the smallest minimizer) of $f_0$. The\nexisting limiting distribution of the estimation error $(\\widehat{f}_n(x_0) -\nf_0(x_0), \\widehat{f}_n'(x_0) - f_0'(x_0) )$ depends on the unknown second\nderivative $f_0''(x_0)$, and is therefore not directly applicable for\ninference. To circumvent this impasse, we show that the following locally\nnormalized errors (LNEs) enjoy pivotal limiting behavior: Let\n$[\\widehat{u}(x_0), \\widehat{v}(x_0)]$ be the maximal interval containing $x_0$\nwhere $\\widehat{f}_n$ is linear. Then, under standard conditions, $$\\binom{\n\\sqrt{n(\\widehat{v}(x_0)-\\widehat{u}(x_0))}(\\widehat{f}_n(x_0)-f_0(x_0)) }{\n\\sqrt{n(\\widehat{v}(x_0)-\\widehat{u}(x_0))^3}(\\widehat{f}_n'(x_0)-f_0'(x_0))}\n\\rightsquigarrow \\sigma \\cdot \\binom{\\mathbb{L}^{(0)}_2}{\\mathbb{L}^{(1)}_2},$$\nwhere $n$ is the sample size, $\\sigma$ is the standard deviation of the errors,\nand $\\mathbb{L}^{(0)}_2, \\mathbb{L}^{(1)}_2$ are universal random variables.\nThis asymptotically pivotal LNE theory instantly yields a simple tuning-free\nprocedure for constructing CIs with asymptotically exact coverage and optimal\nlength for $f_0(x_0)$ and $f_0'(x_0)$. We also construct an asymptotically\npivotal LNE for the anti-mode of $f_0$, and its limiting distribution does not\neven depend on $\\sigma$. These asymptotically pivotal LNE theories are further\nextended to other convexity/concavity constrained models (e.g., log-concave\ndensity estimation) for which a limit distribution theory is available for\nproblem-specific estimators.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 04:02:20 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Deng", "Hang", ""], ["Han", "Qiyang", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "2006.10564", "submitter": "Chirag Gupta", "authors": "Chirag Gupta, Aleksandr Podkopaev, Aaditya Ramdas", "title": "Distribution-free binary classification: prediction sets, confidence\n  intervals and calibration", "comments": "33 pages, 3 figures, appears as a spotlight at Neural Information\n  Processing Systems (NeurIPS) '20", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study three notions of uncertainty quantification -- calibration,\nconfidence intervals and prediction sets -- for binary classification in the\ndistribution-free setting, that is without making any distributional\nassumptions on the data. With a focus towards calibration, we establish a\n'tripod' of theorems that connect these three notions for score-based\nclassifiers. A direct implication is that distribution-free calibration is only\npossible, even asymptotically, using a scoring function whose level sets\npartition the feature space into at most countably many sets. Parametric\ncalibration schemes such as variants of Platt scaling do not satisfy this\nrequirement, while nonparametric schemes based on binning do. To close the\nloop, we derive distribution-free confidence intervals for binned probabilities\nfor both fixed-width and uniform-mass binning. As a consequence of our 'tripod'\ntheorems, these confidence intervals for binned probabilities lead to\ndistribution-free calibration. We also derive extensions to settings with\nstreaming data and covariate shift.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 14:17:29 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 12:46:59 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 03:11:51 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Gupta", "Chirag", ""], ["Podkopaev", "Aleksandr", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "2006.10689", "submitter": "Alexander Wein", "authors": "G\\'erard Ben Arous, Alexander S. Wein, Ilias Zadik", "title": "Free Energy Wells and Overlap Gap Property in Sparse PCA", "comments": "63 pages. Accepted for presentation at the Conference on Learning\n  Theory (COLT) 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.DS cs.LG math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a variant of the sparse PCA (principal component analysis) problem\nin the \"hard\" regime, where the inference task is possible yet no\npolynomial-time algorithm is known to exist. Prior work, based on the\nlow-degree likelihood ratio, has conjectured a precise expression for the best\npossible (sub-exponential) runtime throughout the hard regime. Following\ninstead a statistical physics inspired point of view, we show bounds on the\ndepth of free energy wells for various Gibbs measures naturally associated to\nthe problem. These free energy wells imply hitting time lower bounds that\ncorroborate the low-degree conjecture: we show that a class of natural MCMC\n(Markov chain Monte Carlo) methods (with worst-case initialization) cannot\nsolve sparse PCA with less than the conjectured runtime. These lower bounds\napply to a wide range of values for two tuning parameters: temperature and\nsparsity misparametrization. Finally, we prove that the Overlap Gap Property\n(OGP), a structural property that implies failure of certain local search\nalgorithms, holds in a significant part of the hard regime.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:18:02 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Arous", "G\u00e9rard Ben", ""], ["Wein", "Alexander S.", ""], ["Zadik", "Ilias", ""]]}, {"id": "2006.10715", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and Daniel Kongsgaard", "title": "List-Decodable Mean Estimation via Iterative Multi-Filtering", "comments": "Fixed typo in title", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of {\\em list-decodable mean estimation} for bounded\ncovariance distributions. Specifically, we are given a set $T$ of points in\n$\\mathbb{R}^d$ with the promise that an unknown $\\alpha$-fraction of points in\n$T$, where $0< \\alpha < 1/2$, are drawn from an unknown mean and bounded\ncovariance distribution $D$, and no assumptions are made on the remaining\npoints. The goal is to output a small list of hypothesis vectors such that at\nleast one of them is close to the mean of $D$. We give the first practically\nviable estimator for this problem. In more detail, our algorithm is sample and\ncomputationally efficient, and achieves information-theoretically near-optimal\nerror. While the only prior algorithm for this setting inherently relied on the\nellipsoid method, our algorithm is iterative and only uses spectral techniques.\nOur main technical innovation is the design of a soft outlier removal procedure\nfor high-dimensional heavy-tailed datasets with a majority of outliers.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 17:47:37 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 18:34:16 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Kongsgaard", "Daniel", ""]]}, {"id": "2006.10840", "submitter": "Nicole M\\\"ucke", "authors": "Nicole M\\\"ucke and Enrico Reiss", "title": "Stochastic Gradient Descent in Hilbert Scales: Smoothness,\n  Preconditioning and Earlier Stopping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Descent (SGD) has become the method of choice for solving\na broad range of machine learning problems. However, some of its learning\nproperties are still not fully understood. We consider least squares learning\nin reproducing kernel Hilbert spaces (RKHSs) and extend the classical SGD\nanalysis to a learning setting in Hilbert scales, including Sobolev spaces and\nDiffusion spaces on compact Riemannian manifolds. We show that even for\nwell-specified models, violation of a traditional benchmark smoothness\nassumption has a tremendous effect on the learning rate. In addition, we show\nthat for miss-specified models, preconditioning in an appropriate Hilbert scale\nhelps to reduce the number of iterations, i.e. allowing for \"earlier stopping\".\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 20:22:04 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["M\u00fccke", "Nicole", ""], ["Reiss", "Enrico", ""]]}, {"id": "2006.10968", "submitter": "Juho Lee", "authors": "Fadhel Ayed, Juho Lee, Fran\\c{c}ois Caron", "title": "The Normal-Generalised Gamma-Pareto process: A novel pure-jump L\\'evy\n  process with flexible tail and jump-activity properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pure-jump L\\'evy processes are popular classes of stochastic processes which\nhave found many applications in finance, statistics or machine learning. In\nthis paper, we propose a novel family of self-decomposable L\\'evy processes\nwhere one can control separately the tail behavior and the jump activity of the\nprocess, via two different parameters. Crucially, we show that one can sample\nexactly increments of this process, at any time scale; this allows the\nimplementation of likelihood-free Markov chain Monte Carlo algorithms for\n(asymptotically) exact posterior inference. We use this novel process in\nL\\'evy-based stochastic volatility models to predict the returns of stock\nmarket data, and show that the proposed class of models leads to superior\npredictive performances compared to classical alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 05:26:28 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Ayed", "Fadhel", ""], ["Lee", "Juho", ""], ["Caron", "Fran\u00e7ois", ""]]}, {"id": "2006.10997", "submitter": "Eric Gautier", "authors": "Eric Gautier (TSE, UT1)", "title": "Relaxing monotonicity in endogenous selection models and application to\n  surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers endogenous selection models, in particular nonparametric\nones. Estimating the unconditional law of the outcomes is possible when one\nuses instrumental variables. Using a selection equation which is additively\nseparable in a one dimensional unobservable has the sometimes undesirable\nproperty of instrument monotonicity. We present models which allow for\nnonmonotonicity and are based on nonparametric random coefficients indices. We\ndiscuss their nonparametric identification and apply these results to inference\non nonlinear statistics such as the Gini index in surveys when the nonresponse\nis not missing at random.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 07:48:14 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 08:37:45 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Gautier", "Eric", "", "TSE, UT1"]]}, {"id": "2006.11041", "submitter": "Davide Ravagli", "authors": "Davide Ravagli and Georgi N. Boshnakov", "title": "Bayesian analysis of mixture autoregressive models covering the complete\n  parameter space", "comments": "27 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture autoregressive (MAR) models provide a flexible way to model time\nseries with predictive distributions which depend on the recent history of the\nprocess and are able to accommodate asymmetry and multimodality. Bayesian\ninference for such models offers the additional advantage of incorporating the\nuncertainty in the estimated models into the predictions. We introduce a new\nway of sampling from the posterior distribution of the parameters of MAR models\nwhich allows for covering the complete parameter space of the models, unlike\nprevious approaches. We also propose a relabelling algorithm to deal a\nposteriori with label switching. We apply our new method to simulated and real\ndatasets, discuss the accuracy and performance of our new method, as well as\nits advantages over previous studies. The idea of density forecasting using\nMCMC output is also introduced.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 09:44:01 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Ravagli", "Davide", ""], ["Boshnakov", "Georgi N.", ""]]}, {"id": "2006.11094", "submitter": "Thomas Lartigue", "authors": "Thomas Lartigue (ARAMIS, CMAP), Stanley Durrleman (ARAMIS),\n  St\\'ephanie Allassonni\\`ere (CRC (UMR\\_S\\_1138 / U1138))", "title": "Mixture of Conditional Gaussian Graphical Models for unlabelled\n  heterogeneous populations in the presence of co-factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional correlation networks, within Gaussian Graphical Models (GGM), are\nwidely used to describe the direct interactions between the components of a\nrandom vector. In the case of an unlabelled Heterogeneous population,\nExpectation Maximisation (EM) algorithms for Mixtures of GGM have been proposed\nto estimate both each sub-population's graph and the class labels. However, we\nargue that, with most real data, class affiliation cannot be described with a\nMixture of Gaussian, which mostly groups data points according to their\ngeometrical proximity. In particular, there often exists external co-features\nwhose values affect the features' average value, scattering across the feature\nspace data points belonging to the same sub-population. Additionally, if the\nco-features' effect on the features is Heterogeneous, then the estimation of\nthis effect cannot be separated from the sub-population identification. In this\narticle, we propose a Mixture of Conditional GGM (CGGM) that subtracts the\nheterogeneous effects of the co-features to regroup the data points into\nsub-population corresponding clusters. We develop a penalised EM algorithm to\nestimate graph-sparse model parameters. We demonstrate on synthetic and real\ndata how this method fulfils its goal and succeeds in identifying the\nsub-populations where the Mixtures of GGM are disrupted by the effect of the\nco-features.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 11:57:30 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2020 14:24:29 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 12:33:49 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Lartigue", "Thomas", "", "ARAMIS, CMAP"], ["Durrleman", "Stanley", "", "ARAMIS"], ["Allassonni\u00e8re", "St\u00e9phanie", "", "CRC"]]}, {"id": "2006.11123", "submitter": "Una Radoji\\v{c}i\\'c", "authors": "Una Radojicic, Klaus Nordhausen, Hannu Oja", "title": "Notion of information and independent component analysis", "comments": null, "journal-ref": "Applications of Mathematics in vol. 65, no. 3 (2020), pp. 311-330", "doi": "10.21136/AM.2020.0326-19", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial orderings and measures of information for continuous univariate\nrandom variables with special roles of Gaussian and uniform distributions are\ndiscussed. The information measures and measures of non-Gaussianity including\nthird and fourth cumulants are generally used as projection indices in the\nprojection pursuit approach for the independent component analysis. The\nconnections between information, non-Gaussianity and statistical independence\nin the context of independent component analysis is discussed in detail.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 13:26:37 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Radojicic", "Una", ""], ["Nordhausen", "Klaus", ""], ["Oja", "Hannu", ""]]}, {"id": "2006.11170", "submitter": "Alisa Kirichenko", "authors": "Alisa Kirichenko and Peter Gr\\\"unwald", "title": "Minimax rates without the fixed sample size assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the notion of minimax convergence rate. In contrast to the\nstandard definition, we do not assume that the sample size is fixed in advance.\nAllowing for varying sample size results in time-robust minimax rates and\nestimators. These can be either strongly adversarial, based on the worst-case\nover all sample sizes, or weakly adversarial, based on the worst-case over all\nstopping times. We show that standard and time-robust rates usually differ by\nat most a logarithmic factor, and that for some (and we conjecture for all)\nexponential families, they differ by exactly an iterated logarithmic factor. In\nmany situations, time-robust rates are arguably more natural to consider. For\nexample, they allow us to simultaneously obtain strong model selection\nconsistency and optimal estimation rates, thus avoiding the \"AIC-BIC dilemma\".\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 14:57:06 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 21:49:06 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kirichenko", "Alisa", ""], ["Gr\u00fcnwald", "Peter", ""]]}, {"id": "2006.11211", "submitter": "Miklos Z. Racz", "authors": "Miklos Z. Racz, Jacob Richey", "title": "Rumor source detection with multiple observations under adaptive\n  diffusions", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.CR math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work, motivated by anonymous messaging platforms, has introduced\nadaptive diffusion protocols which can obfuscate the source of a rumor: a\n\"snapshot adversary\" with access to the subgraph of \"infected\" nodes can do no\nbetter than randomly guessing the entity of the source node. What happens if\nthe adversary has access to multiple independent snapshots? We study this\nquestion when the underlying graph is the infinite $d$-regular tree. We show\nthat (1) a weak form of source obfuscation is still possible in the case of two\nindependent snapshots, but (2) already with three observations there is a\nsimple algorithm that finds the rumor source with constant probability,\nregardless of the adaptive diffusion protocol. We also characterize the\ntradeoff between local spreading and source obfuscation for adaptive diffusion\nprotocols (under a single snapshot). These results raise questions about the\nrobustness of anonymity guarantees when spreading information in social\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 16:27:26 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Racz", "Miklos Z.", ""], ["Richey", "Jacob", ""]]}, {"id": "2006.11362", "submitter": "Lirong Xia", "authors": "Lirong Xia", "title": "Optimal Statistical Hypothesis Testing for Social Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.GT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the following question in this paper: \"What are the most robust\nstatistical methods for social choice?'' By leveraging the theory of uniformly\nleast favorable distributions in the Neyman-Pearson framework to finite models\nand randomized tests, we characterize uniformly most powerful (UMP) tests,\nwhich is a well-accepted statistical optimality w.r.t. robustness, for testing\nwhether a given alternative is the winner under Mallows' model and under\nCondorcet's model, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 20:40:33 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Xia", "Lirong", ""]]}, {"id": "2006.11676", "submitter": "Tianjian Zhou", "authors": "Tianjian Zhou and Yuan Ji", "title": "A Unified Framework for Time-to-Event Dose-Finding Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In dose-finding trials, due to staggered enrollment, it might be desirable to\nmake dose assignment decisions in real-time in the presence of pending toxicity\noutcomes, for example, when patient accrual is fast or the dose-limiting\ntoxicity is late-onset. Patients' time-to-event information may be utilized to\nfacilitate such decisions. We propose a unified statistical framework for\ntime-to-event modeling in dose-finding trials, which leads to two classes of\ntime-to-event designs: TITE deigns and POD designs. TITE designs are based on\ninference on toxicity probabilities, while POD designs are based on inference\non dose-finding decisions. These two classes of designs contain existing\ndesigns as special cases and also give rise to new designs. We discuss and\nstudy the theoretical properties of these designs, including large-sample\nconvergence properties, coherence principles, and the underlying decision\nrules. To facilitate the use of time-to-event designs in practice, we introduce\nefficient computational algorithms and review common practical considerations,\nsuch as safety rules and suspension rules. Finally, the operating\ncharacteristics of several designs are evaluated and compared through computer\nsimulations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2020 23:30:44 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Zhou", "Tianjian", ""], ["Ji", "Yuan", ""]]}, {"id": "2006.11754", "submitter": "Michael Schomaker", "authors": "Michael Schomaker", "title": "Regression and Causality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal effect of an intervention (treatment/exposure) on an outcome can\nbe estimated by: i) specifying knowledge about the data-generating process; ii)\nassessing under what assumptions a target quantity, such as for example a\ncausal odds ratio, can be identified given the specified knowledge (and given\nthe measured data); and then, iii) using appropriate statistical estimation\ntechniques to estimate the desired parameter of interest. As regression is the\ncornerstone of statistical analysis, it seems obvious to ask: is it appropriate\nto use estimated regression parameters for causal effect estimation? It turns\nout that using regression for effect estimation is possible, but typically\nrequires more assumptions than competing methods. This manuscript provides a\ncomprehensive summary of the assumptions needed to identify and estimate a\ncausal parameter using regression and, equally important, discusses the\nresulting implications for statistical practice.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 10:16:48 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 10:48:09 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Schomaker", "Michael", ""]]}, {"id": "2006.11756", "submitter": "Fr\\'ed\\'eric Ouimet", "authors": "Fr\\'ed\\'eric Ouimet", "title": "Asymptotic properties of Bernstein estimators on the simplex. Part 2:\n  the boundary case", "comments": "23 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the asymptotic properties (bias, variance, mean\nsquared error) of Bernstein estimators for cumulative distribution functions\nand density functions near and on the boundary of the $d$-dimensional simplex.\nThe simplex is an important case as it is the natural domain of compositional\ndata and has been neglected in the literature. Our results generalize those\nfound in Leblanc (2012), who treated the case $d=1$, and complement the results\nfrom Ouimet (2020) in the interior of the simplex. Different parts of the\nboundary having different dimensions makes the analysis more complex.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 10:27:45 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2020 02:28:02 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Ouimet", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2006.11899", "submitter": "Viet Chi Tran", "authors": "Myl\\`ene Ma\\\"ida, Tien Dat Nguyen, Thanh Mai Pham Ngoc, Vincent\n  Rivoirard, Viet Chi Tran", "title": "Statistical deconvolution of the free Fokker-Planck equation at fixed\n  time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in reconstructing the initial condition of a non-linear\npartial differential equation (PDE), namely the Fokker-Planck equation, from\nthe observation of a Dyson Brownian motion at a given time $t>0$. The\nFokker-Planck equation describes the evolution of electrostatic repulsive\nparticle systems, and can be seen as the large particle limit of correctly\nrenormalized Dyson Brownian motions. The solution of the Fokker-Planck equation\ncan be written as the free convolution of the initial condition and the\nsemi-circular distribution. We propose a nonparametric estimator for the\ninitial condition obtained by performing the free deconvolution via the\nsubordination functions method. This statistical estimator is original as it\ninvolves the resolution of a fixed point equation, and a classical\ndeconvolution by a Cauchy distribution. This is due to the fact that, in free\nprobability, the analogue of the Fourier transform is the R-transform, related\nto the Cauchy transform. In past literature, there has been a focus on the\nestimation of the initial conditions of linear PDEs such as the heat equation,\nbut to the best of our knowledge, this is the first time that the problem is\ntackled for a non-linear PDE. The convergence of the estimator is proved and\nthe integrated mean square error is computed, providing rates of convergence\nsimilar to the ones known for non-parametric deconvolution methods. Finally, a\nsimulation study illustrates the good performances of our estimator.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 20:16:36 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Ma\u00efda", "Myl\u00e8ne", ""], ["Nguyen", "Tien Dat", ""], ["Ngoc", "Thanh Mai Pham", ""], ["Rivoirard", "Vincent", ""], ["Tran", "Viet Chi", ""]]}, {"id": "2006.11948", "submitter": "Kengne William", "authors": "Mamadou Lamine DIOP and William KENGNE", "title": "Density power divergence for general integer-valued time series with\n  multivariate exogenous covariate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study a robust estimation method for a general class of\ninteger-valued time series models.\n  The conditional distribution of the process belongs to a broad class of\ndistribution and unlike classical autoregressive framework, the conditional\nmean of the process also depends on some multivariate exogenous covariate.\n  We derive a robust inference procedure based on the minimum density power\ndivergence.\n  Under certain regularity conditions, we establish that the proposed estimator\nis consistent and asymptotically normal.\n  Simulation experiments are conducted to illustrate the empirical performances\nof the estimator. An application to the number of transactions per minute for\nthe stock Ericsson B is also provided.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 00:25:14 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["DIOP", "Mamadou Lamine", ""], ["KENGNE", "William", ""]]}, {"id": "2006.11970", "submitter": "Bryon Aragam", "authors": "Ming Gao, Yi Ding, Bryon Aragam", "title": "A polynomial-time algorithm for learning nonparametric causal graphs", "comments": "To appear at NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish finite-sample guarantees for a polynomial-time algorithm for\nlearning a nonlinear, nonparametric directed acyclic graphical (DAG) model from\ndata. The analysis is model-free and does not assume linearity, additivity,\nindependent noise, or faithfulness. Instead, we impose a condition on the\nresidual variances that is closely related to previous work on linear models\nwith equal variances. Compared to an optimal algorithm with oracle knowledge of\nthe variable ordering, the additional cost of the algorithm is linear in the\ndimension $d$ and the number of samples $n$. Finally, we compare the proposed\nalgorithm to existing approaches in a simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 02:21:53 GMT"}, {"version": "v2", "created": "Tue, 10 Nov 2020 21:35:55 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Gao", "Ming", ""], ["Ding", "Yi", ""], ["Aragam", "Bryon", ""]]}, {"id": "2006.12022", "submitter": "Johannes Wiesel", "authors": "Daniel Bartl, Samuel Drapeau, Jan Obloj, Johannes Wiesel", "title": "Robust uncertainty sensitivity analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR math.ST q-fin.MF stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sensitivity of a generic stochastic optimization problem to model\nuncertainty. We take a non-parametric approach and capture model uncertainty\nusing Wasserstein balls around the postulated model. We provide explicit\nformulae for the first order correction to both the value function and the\noptimizer and further extend our results to optimization under linear\nconstraints. We present applications to statistics, machine learning,\nmathematical finance and uncertainty quantification. In particular, we provide\nexplicit first-order approximation for square-root LASSO regression\ncoefficients and deduce coefficient shrinkage compared to the ordinary least\nsquares regression. We consider robustness of call option pricing and deduce a\nnew Black-Scholes sensitivity, a non-parametric version of the so-called Vega.\nWe also compute sensitivities of optimized certainty equivalents in finance and\npropose measures to quantify robustness of neural networks to adversarial\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 06:26:09 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Bartl", "Daniel", ""], ["Drapeau", "Samuel", ""], ["Obloj", "Jan", ""], ["Wiesel", "Johannes", ""]]}, {"id": "2006.12287", "submitter": "Christoph Alexander Weitkamp", "authors": "Christoph Alexander Weitkamp, Katharina Proksch, Carla Tameling, Axel\n  Munk", "title": "Gromov-Wasserstein Distance based Object Matching: Asymptotic Inference", "comments": "For a version with the complete supplement see [v2]", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim to provide a statistical theory for object matching\nbased on the Gromov-Wasserstein distance. To this end, we model general objects\nas metric measure spaces. Based on this, we propose a simple and efficiently\ncomputable asymptotic statistical test for pose invariant object\ndiscrimination. This is based on an empirical version of a $\\beta$-trimmed\nlower bound of the Gromov-Wasserstein distance. We derive for $\\beta\\in[0,1/2)$\ndistributional limits of this test statistic. To this end, we introduce a novel\n$U$-type process indexed in $\\beta$ and show its weak convergence. Finally, the\ntheory developed is investigated in Monte Carlo simulations and applied to\nstructural protein comparisons.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 14:18:01 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 07:57:25 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2020 07:36:58 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Weitkamp", "Christoph Alexander", ""], ["Proksch", "Katharina", ""], ["Tameling", "Carla", ""], ["Munk", "Axel", ""]]}, {"id": "2006.12301", "submitter": "Tianyi Lin", "authors": "Tianyi Lin, Zeyu Zheng, Elynn Y. Chen, Marco Cuturi, Michael I. Jordan", "title": "On Projection Robust Optimal Transport: Sample Complexity and Model\n  Misspecification", "comments": "Accepted by AISTATS 2021; Fix some inaccuracy in the definition and\n  proof; 49 Pages, 41 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal transport (OT) distances are increasingly used as loss functions for\nstatistical inference, notably in the learning of generative models or\nsupervised learning. Yet, the behavior of minimum Wasserstein estimators is\npoorly understood, notably in high-dimensional regimes or under model\nmisspecification. In this work we adopt the viewpoint of projection robust (PR)\nOT, which seeks to maximize the OT cost between two measures by choosing a\n$k$-dimensional subspace onto which they can be projected. Our first\ncontribution is to establish several fundamental statistical properties of PR\nWasserstein distances, complementing and improving previous literature that has\nbeen restricted to one-dimensional and well-specified cases. Next, we propose\nthe integral PR Wasserstein (IPRW) distance as an alternative to the PRW\ndistance, by averaging rather than optimizing on subspaces. Our complexity\nbounds can help explain why both PRW and IPRW distances outperform Wasserstein\ndistances empirically in high-dimensional inference tasks. Finally, we consider\nparametric inference using the PRW distance. We provide an asymptotic guarantee\nof two types of minimum PRW estimators and formulate a central limit theorem\nfor max-sliced Wasserstein estimator under model misspecification. To enable\nour analysis on PRW with projection dimension larger than one, we devise a\nnovel combination of variational analysis and statistical theory.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 14:35:33 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 19:14:24 GMT"}, {"version": "v3", "created": "Tue, 13 Oct 2020 01:30:42 GMT"}, {"version": "v4", "created": "Tue, 16 Feb 2021 22:28:31 GMT"}, {"version": "v5", "created": "Sat, 17 Jul 2021 06:11:56 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Lin", "Tianyi", ""], ["Zheng", "Zeyu", ""], ["Chen", "Elynn Y.", ""], ["Cuturi", "Marco", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2006.12369", "submitter": "Yordan Raykov", "authors": "Adam Farooq and Yordan P. Raykov and Petar Raykov and Max A. Little", "title": "Controlling for sparsity in sparse factor analysis models: adaptive\n  latent feature sharing for piecewise linear dimensionality reduction", "comments": "Interactive demo available at\n  https://colab.research.google.com/drive/1KrrHmAu6mV7tutZtYnpEbVibxs4GCwIo?usp=sharing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ubiquitous linear Gaussian exploratory tools such as principle component\nanalysis (PCA) and factor analysis (FA) remain widely used as tools for:\nexploratory analysis, pre-processing, data visualization and related tasks.\nHowever, due to their rigid assumptions including crowding of high dimensional\ndata, they have been replaced in many settings by more flexible and still\ninterpretable latent feature models. The Feature allocation is usually modelled\nusing discrete latent variables assumed to follow either parametric\nBeta-Bernoulli distribution or Bayesian nonparametric prior. In this work we\npropose a simple and tractable parametric feature allocation model which can\naddress key limitations of current latent feature decomposition techniques. The\nnew framework allows for explicit control over the number of features used to\nexpress each point and enables a more flexible set of allocation distributions\nincluding feature allocations with different sparsity levels. This approach is\nused to derive a novel adaptive Factor analysis (aFA), as well as, an adaptive\nprobabilistic principle component analysis (aPPCA) capable of flexible\nstructure discovery and dimensionality reduction in a wide case of scenarios.\nWe derive both standard Gibbs sampler, as well as, an expectation-maximization\ninference algorithms that converge orders of magnitude faster to a reasonable\npoint estimate solution. The utility of the proposed aPPCA model is\ndemonstrated for standard PCA tasks such as feature learning, data\nvisualization and data whitening. We show that aPPCA and aFA can infer\ninterpretable high level features both when applied on raw MNIST and when\napplied for interpreting autoencoder features. We also demonstrate an\napplication of the aPPCA to more robust blind source separation for functional\nmagnetic resonance imaging (fMRI).\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 16:09:11 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2020 08:33:40 GMT"}, {"version": "v3", "created": "Sun, 28 Feb 2021 19:38:00 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Farooq", "Adam", ""], ["Raykov", "Yordan P.", ""], ["Raykov", "Petar", ""], ["Little", "Max A.", ""]]}, {"id": "2006.12476", "submitter": "Ilias Diakonikolas", "authors": "Ilias Diakonikolas and Daniel M. Kane and Vasilis Kontonis and Nikos\n  Zarifis", "title": "Algorithms and SQ Lower Bounds for PAC Learning One-Hidden-Layer ReLU\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of PAC learning one-hidden-layer ReLU networks with $k$\nhidden units on $\\mathbb{R}^d$ under Gaussian marginals in the presence of\nadditive label noise. For the case of positive coefficients, we give the first\npolynomial-time algorithm for this learning problem for $k$ up to\n$\\tilde{O}(\\sqrt{\\log d})$. Previously, no polynomial time algorithm was known,\neven for $k=3$. This answers an open question posed by~\\cite{Kliv17}.\nImportantly, our algorithm does not require any assumptions about the rank of\nthe weight matrix and its complexity is independent of its condition number. On\nthe negative side, for the more general task of PAC learning one-hidden-layer\nReLU networks with arbitrary real coefficients, we prove a Statistical Query\nlower bound of $d^{\\Omega(k)}$. Thus, we provide a separation between the two\nclasses in terms of efficient learnability. Our upper and lower bounds are\ngeneral, extending to broader families of activation functions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 17:53:54 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Kontonis", "Vasilis", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2006.12489", "submitter": "Xiao Li", "authors": "Xiao Li and William Fithian", "title": "Optimality of the max test for detecting sparse signals with Gaussian or\n  heavier tail", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in high-dimensional testing is that of global null\ntesting: testing whether the null holds simultaneously in all of $n$\nhypotheses. The max test, which uses the smallest of the $n$ marginal p-values\nas its test statistic, enjoys widespread popularity for its simplicity and\nrobustness. However, its theoretical performance relative to other tests has\nbeen called into question. In the Gaussian sequence version of the global\ntesting problem, Donoho and Jin (2004) discovered a so-called \"weak, sparse\"\nasymptotic regime in which the higher criticism and Berk-Jones tests achieve a\nbetter detection boundary than the max test when all of the nonzero signal\nstrengths are identical. We study a more general model in which the non-null\nmeans are drawn from a generic distribution, and show that the detection\nboundary for the max test is optimal in the \"weak, sparse\" regime, provided\nthat the distribution's tail is no lighter than Gaussian. Further, we show\ntheoretically and in simulation that the modified higher criticism of Donoho\nand Jin (2004) can have very low power when the distribution of non-null means\nhas a polynomial tail.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 22:50:47 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Li", "Xiao", ""], ["Fithian", "William", ""]]}, {"id": "2006.12920", "submitter": "Antoine Godichon-Baggioni", "authors": "Peggy C\\'enac (IMB), Antoine Godichon-Baggioni (LPSM (UMR\\_8001)),\n  Bruno Portier (LMI)", "title": "An efficient Averaged Stochastic Gauss-Newton algorithm for estimating\n  parameters of non linear regressions models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non linear regression models are a standard tool for modeling real phenomena,\nwith several applications in machine learning, ecology, econometry...\nEstimating the parameters of the model has garnered a lot of attention during\nmany years. We focus here on a recursive method for estimating parameters of\nnon linear regressions. Indeed, these kinds of methods, whose most famous are\nprobably the stochastic gradient algorithm and its averaged version, enable to\ndeal efficiently with massive data arriving sequentially. Nevertheless, they\ncan be, in practice, very sensitive to the case where the eigen-values of the\nHessian of the functional we would like to minimize are at different scales. To\navoid this problem, we first introduce an online Stochastic Gauss-Newton\nalgorithm. In order to improve the estimates behavior in case of bad\ninitialization, we also introduce a new Averaged Stochastic Gauss-Newton\nalgorithm and prove its asymptotic efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 11:56:30 GMT"}, {"version": "v2", "created": "Tue, 30 Jun 2020 08:24:36 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2020 07:53:18 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["C\u00e9nac", "Peggy", "", "IMB"], ["Godichon-Baggioni", "Antoine", "", "LPSM"], ["Portier", "Bruno", "", "LMI"]]}, {"id": "2006.13003", "submitter": "Jorge Yslas Altamirano", "authors": "Hansjoerg Albrecher, Mogens Bladt and Jorge Yslas", "title": "Fitting inhomogeneous phase-type distributions to data: the univariate\n  and the multivariate case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of inhomogeneous phase-type distributions (IPH) was recently\nintroduced in Albrecher and Bladt (2019) as an extension of the classical\nphase-type (PH) distributions. Like PH distributions, the class of IPH is dense\nin the class of distributions on the positive halfline, but leads to more\nparsimonious models in the presence of heavy tails. In this paper we propose a\nfitting procedure for this class to given data. We furthermore consider an\nanalogous extension of Kulkarni's multivariate phase-type class (Kulkarni,\n1989) to the inhomogeneous framework and study parameter estimation for the\nresulting new and flexible class of multivariate distributions. As a\nby-product, we amend a previously suggested fitting procedure for the\nhomogeneous multivariate phase-type case and provide appropriate adaptations\nfor censored data. The performance of the algorithms is illustrated in several\nnumerical examples, both for simulated and real-life insurance data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 13:50:08 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2020 15:30:15 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Albrecher", "Hansjoerg", ""], ["Bladt", "Mogens", ""], ["Yslas", "Jorge", ""]]}, {"id": "2006.13099", "submitter": "Alexander Giessing", "authors": "Alexander Giessing, Jianqing Fan", "title": "Bootstrapping $\\ell_p$-Statistics in High Dimensions", "comments": "80 pages, 6 figures. Added details to Section 3.2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a new bootstrap procedure to estimate the distribution\nof high-dimensional $\\ell_p$-statistics, i.e. the $\\ell_p$-norms of the sum of\n$n$ independent $d$-dimensional random vectors with $d \\gg n$ and $p \\in [1,\n\\infty]$. We provide a non-asymptotic characterization of the sampling\ndistribution of $\\ell_p$-statistics based on Gaussian approximation and show\nthat the bootstrap procedure is consistent in the Kolmogorov-Smirnov distance\nunder mild conditions on the covariance structure of the data. As an\napplication of the general theory we propose a bootstrap hypothesis test for\nsimultaneous inference on high-dimensional mean vectors. We establish its\nasymptotic correctness and consistency under high-dimensional alternatives, and\ndiscuss the power of the test as well as the size of associated confidence\nsets. We illustrate the bootstrap and testing procedure numerically on\nsimulated data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 15:38:10 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 13:31:49 GMT"}, {"version": "v3", "created": "Mon, 17 Aug 2020 03:49:21 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Giessing", "Alexander", ""], ["Fan", "Jianqing", ""]]}, {"id": "2006.13312", "submitter": "Guanghao Ye", "authors": "Jerry Li, Guanghao Ye", "title": "Robust Gaussian Covariance Estimation in Nearly-Matrix Multiplication\n  Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust covariance estimation is the following, well-studied problem in high\ndimensional statistics: given $N$ samples from a $d$-dimensional Gaussian\n$\\mathcal{N}(\\boldsymbol{0}, \\Sigma)$, but where an $\\varepsilon$-fraction of\nthe samples have been arbitrarily corrupted, output $\\widehat{\\Sigma}$\nminimizing the total variation distance between $\\mathcal{N}(\\boldsymbol{0},\n\\Sigma)$ and $\\mathcal{N}(\\boldsymbol{0}, \\widehat{\\Sigma})$. This corresponds\nto learning $\\Sigma$ in a natural affine-invariant variant of the Frobenius\nnorm known as the \\emph{Mahalanobis norm}. Previous work of Cheng et al\ndemonstrated an algorithm that, given $N = \\Omega (d^2 / \\varepsilon^2)$\nsamples, achieved a near-optimal error of $O(\\varepsilon \\log 1 /\n\\varepsilon)$, and moreover, their algorithm ran in time $\\widetilde{O}(T(N, d)\n\\log \\kappa / \\mathrm{poly} (\\varepsilon))$, where $T(N, d)$ is the time it\ntakes to multiply a $d \\times N$ matrix by its transpose, and $\\kappa$ is the\ncondition number of $\\Sigma$. When $\\varepsilon$ is relatively small, their\npolynomial dependence on $1/\\varepsilon$ in the runtime is prohibitively large.\nIn this paper, we demonstrate a novel algorithm that achieves the same\nstatistical guarantees, but which runs in time $\\widetilde{O} (T(N, d) \\log\n\\kappa)$. In particular, our runtime has no dependence on $\\varepsilon$. When\n$\\Sigma$ is reasonably conditioned, our runtime matches that of the fastest\nalgorithm for covariance estimation without outliers, up to poly-logarithmic\nfactors, showing that we can get robustness essentially \"for free.\"\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 20:21:27 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Li", "Jerry", ""], ["Ye", "Guanghao", ""]]}, {"id": "2006.13330", "submitter": "Masoud Badiei Khuzani", "authors": "Masoud Badiei Khuzani, Yinyu Ye, Sandy Napel, Lei Xing", "title": "A Mean-Field Theory for Learning the Sch\\\"{o}nberg Measure of Radial\n  Basis Functions", "comments": "67 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze a projected particle Langevin optimization method to\nlearn the distribution in the Sch\\\"{o}nberg integral representation of the\nradial basis functions from training samples. More specifically, we\ncharacterize a distributionally robust optimization method with respect to the\nWasserstein distance to optimize the distribution in the Sch\\\"{o}nberg integral\nrepresentation. To provide theoretical performance guarantees, we analyze the\nscaling limits of a projected particle online (stochastic) optimization method\nin the mean-field regime. In particular, we prove that in the scaling limits,\nthe empirical measure of the Langevin particles converges to the law of a\nreflected It\\^{o} diffusion-drift process. Moreover, the drift is also a\nfunction of the law of the underlying process. Using It\\^{o} lemma for\nsemi-martingales and Grisanov's change of measure for the Wiener processes, we\nthen derive a Mckean-Vlasov type partial differential equation (PDE) with Robin\nboundary conditions that describes the evolution of the empirical measure of\nthe projected Langevin particles in the mean-field regime. In addition, we\nestablish the existence and uniqueness of the steady-state solutions of the\nderived PDE in the weak sense. We apply our learning approach to train radial\nkernels in the kernel locally sensitive hash (LSH) functions, where the\ntraining data-set is generated via a $k$-mean clustering method on a small\nsubset of data-base. We subsequently apply our kernel LSH with a trained kernel\nfor image retrieval task on MNIST data-set, and demonstrate the efficacy of our\nkernel learning approach. We also apply our kernel learning approach in\nconjunction with the kernel support vector machines (SVMs) for classification\nof benchmark data-sets.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 21:04:48 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2020 13:43:31 GMT"}], "update_date": "2020-07-06", "authors_parsed": [["Khuzani", "Masoud Badiei", ""], ["Ye", "Yinyu", ""], ["Napel", "Sandy", ""], ["Xing", "Lei", ""]]}, {"id": "2006.13409", "submitter": "Theodor Misiakiewicz Mr.", "authors": "Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, Andrea Montanari", "title": "When Do Neural Networks Outperform Kernel Methods?", "comments": "99 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a certain scaling of the initialization of stochastic gradient descent\n(SGD), wide neural networks (NN) have been shown to be well approximated by\nreproducing kernel Hilbert space (RKHS) methods. Recent empirical work showed\nthat, for some classification tasks, RKHS methods can replace NNs without a\nlarge loss in performance. On the other hand, two-layers NNs are known to\nencode richer smoothness classes than RKHS and we know of special examples for\nwhich SGD-trained NN provably outperform RKHS. This is true even in the wide\nnetwork limit, for a different scaling of the initialization.\n  How can we reconcile the above claims? For which tasks do NNs outperform\nRKHS? If feature vectors are nearly isotropic, RKHS methods suffer from the\ncurse of dimensionality, while NNs can overcome it by learning the best\nlow-dimensional representation. Here we show that this curse of dimensionality\nbecomes milder if the feature vectors display the same low-dimensional\nstructure as the target function, and we precisely characterize this tradeoff.\nBuilding on these results, we present a model that can capture in a unified\nframework both behaviors observed in earlier work.\n  We hypothesize that such a latent low-dimensional structure is present in\nimage classification. We test numerically this hypothesis by showing that\nspecific perturbations of the training distribution degrade the performances of\nRKHS methods much more significantly than NNs.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 01:03:31 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Ghorbani", "Behrooz", ""], ["Mei", "Song", ""], ["Misiakiewicz", "Theodor", ""], ["Montanari", "Andrea", ""]]}, {"id": "2006.13488", "submitter": "Farhad Farokhi", "authors": "Farhad Farokhi", "title": "Distributionally-Robust Machine Learning Using Locally\n  Differentially-Private Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider machine learning, particularly regression, using\nlocally-differentially private datasets. The Wasserstein distance is used to\ndefine an ambiguity set centered at the empirical distribution of the dataset\ncorrupted by local differential privacy noise. The ambiguity set is shown to\ncontain the probability distribution of unperturbed, clean data. The radius of\nthe ambiguity set is a function of the privacy budget, spread of the data, and\nthe size of the problem. Hence, machine learning with locally-differentially\nprivate datasets can be rewritten as a distributionally-robust optimization.\nFor general distributions, the distributionally-robust optimization problem can\nrelaxed as a regularized machine learning problem with the Lipschitz constant\nof the machine learning model as a regularizer. For linear and logistic\nregression, this regularizer is the dual norm of the model parameters. For\nGaussian data, the distributionally-robust optimization problem can be solved\nexactly to find an optimal regularizer. This approach results in an entirely\nnew regularizer for training linear regression models. Training with this novel\nregularizer can be posed as a semi-definite program. Finally, the performance\nof the proposed distributionally-robust machine learning training is\ndemonstrated on practical datasets.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 05:12:10 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Farokhi", "Farhad", ""]]}, {"id": "2006.13489", "submitter": "Haozhe Zhang", "authors": "Haozhe Zhang, Yehua Li", "title": "Unified Principal Component Analysis for Sparse and Dense Functional\n  Data under Spatial Dependency", "comments": null, "journal-ref": null, "doi": "10.1080/07350015.2021.1938085", "report-no": null, "categories": "stat.ME econ.EM math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider spatially dependent functional data collected under a\ngeostatistics setting, where locations are sampled from a spatial point\nprocess. The functional response is the sum of a spatially dependent functional\neffect and a spatially independent functional nugget effect. Observations on\neach function are made on discrete time points and contaminated with\nmeasurement errors. Under the assumption of spatial stationarity and isotropy,\nwe propose a tensor product spline estimator for the spatio-temporal covariance\nfunction. When a coregionalization covariance structure is further assumed, we\npropose a new functional principal component analysis method that borrows\ninformation from neighboring functions. The proposed method also generates\nnonparametric estimators for the spatial covariance functions, which can be\nused for functional kriging. Under a unified framework for sparse and dense\nfunctional data, infill and increasing domain asymptotic paradigms, we develop\nthe asymptotic convergence rates for the proposed estimators. Advantages of the\nproposed approach are demonstrated through simulation studies and two real data\napplications representing sparse and dense functional data, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 05:26:21 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 05:30:35 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhang", "Haozhe", ""], ["Li", "Yehua", ""]]}, {"id": "2006.13516", "submitter": "Samvel Gasparyan", "authors": "Samvel Gasparyan", "title": "Second order asymptotic efficiency for a Poisson process", "comments": "8 pages", "journal-ref": "Journal of Contemporary Mathematical Analysis volume 50, pages\n  98-106 (2015)", "doi": "10.3103/S1068362315020065", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of the estimation of the mean function of an\ninhomogeneous Poisson process when its intensity function is periodic. For the\nmean integrated squared error (MISE) there is a classical lower bound for all\nestimators and the empirical mean function attains that lower bound, thus it is\nasymptotically efficient. Following the ideas of the work by Golubev and Levit,\nwe compare asymptotically efficient estimators and propose an estimator which\nis second order asymptotically efficient. Second order efficiency is done over\nSobolev ellipsoids, following the idea of Pinsker.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 06:58:25 GMT"}], "update_date": "2020-06-25", "authors_parsed": [["Gasparyan", "Samvel", ""]]}, {"id": "2006.13868", "submitter": "V\\'ictor Pe\\~na", "authors": "V\\'ictor Pe\\~na, Kaoru Irie", "title": "On the relationship between beta-Bartlett and Uhlig extended processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic volatility processes are used in multivariate time-series analysis\nto track time-varying patterns in covariance matrices. Uhlig extended and\nbeta-Bartlett processes are especially convenient for analyzing\nhigh-dimensional time-series because they are conjugate with Wishart\nlikelihoods. In this article, we show that Uhlig extended and beta-Bartlett are\nclosely related, but not equivalent: their hyperparameters can be matched so\nthat they have the same forward-filtered posteriors and one-step ahead\nforecasts, but different joint (smoothed) posterior distributions. Under this\ncircumstance, Bayes factors can't discriminate the models and alternative\napproaches to model comparison are needed. We illustrate these issues in a\nretrospective analysis of volatilities of returns of foreign exchange rates.\nAdditionally, we provide a backward sampling algorithm for the beta-Bartlett\nprocess, for which retrospective analysis had not been developed.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 16:51:02 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 11:57:28 GMT"}, {"version": "v3", "created": "Sun, 5 Jul 2020 11:13:58 GMT"}, {"version": "v4", "created": "Tue, 4 May 2021 22:23:19 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Pe\u00f1a", "V\u00edctor", ""], ["Irie", "Kaoru", ""]]}, {"id": "2006.13975", "submitter": "Takaaki Koike", "authors": "Takaaki Koike and Marius Hofert", "title": "Estimation and Comparison of Correlation-based Measures of Concordance", "comments": "30 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating and comparing measures of concordance\nthat arise as Pearson's linear correlation coefficient between two random\nvariables transformed so that they follow the so-called concordance-inducing\ndistribution. The class of such transformed rank correlations includes\nSpearman's rho, Blomqvist's beta and van der Waerden's coefficient as special\ncases. To answer which transformed rank correlation is best to use, we propose\nto compare them in terms of their best and worst asymptotic variances on a\ngiven set of copulas. A criterion derived from this approach is that\nconcordance-inducing distributions with smaller variances of squared random\nvariables are more preferable. In particular, we show that Blomqvist's beta is\nthe optimal transformed rank correlation, and Spearman's rho outperforms van\nder Waerden's coefficient. Moreover, we find that Kendall's tau also attains\nthe optimal asymptotic variances that Blomqvist's beta does, although Kendall's\ntau is not a transformed rank correlation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 18:21:05 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2020 22:15:58 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2020 02:35:21 GMT"}], "update_date": "2020-11-19", "authors_parsed": [["Koike", "Takaaki", ""], ["Hofert", "Marius", ""]]}, {"id": "2006.13984", "submitter": "Henry-Louis de Kergorlay", "authors": "Henry-Louis de Kergorlay, Desmond John Higham", "title": "Consistency of Anchor-based Spectral Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anchor-based techniques reduce the computational complexity of spectral\nclustering algorithms. Although empirical tests have shown promising results,\nthere is currently a lack of theoretical support for the anchoring approach. We\ndefine a specific anchor-based algorithm and show that it is amenable to\nrigorous analysis, as well as being effective in practice. We establish the\ntheoretical consistency of the method in an asymptotic setting where data is\nsampled from an underlying continuous probability distribution. In particular,\nwe provide sharp asymptotic conditions for the algorithm parameters which\nensure that the anchor-based method can recover with high probability disjoint\nclusters that are mutually separated by a positive distance. We illustrate the\nperformance of the algorithm on synthetic data and explain how the theoretical\nconvergence analysis can be used to inform the practical choice of parameter\nscalings. We also test the accuracy and efficiency of the algorithm on two\nlarge scale real data sets. We find that the algorithm offers clear advantages\nover standard spectral clustering. We also find that it is competitive with the\nstate-of-the-art LSC method of Chen and Cai (Twenty-Fifth AAAI Conference on\nArtificial Intelligence, 2011), while having the added benefit of a consistency\nguarantee.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 18:34:41 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2020 12:27:00 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["de Kergorlay", "Henry-Louis", ""], ["Higham", "Desmond John", ""]]}, {"id": "2006.13998", "submitter": "Arnak Dalalyan S.", "authors": "Avetik Karagulyan, Arnak S. Dalalyan", "title": "Penalized Langevin dynamics with vanishing penalty for smooth and\n  log-concave targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sampling from a probability distribution on $\\mathbb\nR^p$ defined via a convex and smooth potential function. We consider a\ncontinuous-time diffusion-type process, termed Penalized Langevin dynamics\n(PLD), the drift of which is the negative gradient of the potential plus a\nlinear penalty that vanishes when time goes to infinity. An upper bound on the\nWasserstein-2 distance between the distribution of the PLD at time $t$ and the\ntarget is established. This upper bound highlights the influence of the speed\nof decay of the penalty on the accuracy of the approximation. As a consequence,\nconsidering the low-temperature limit we infer a new nonasymptotic guarantee of\nconvergence of the penalized gradient flow for the optimization problem.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 18:56:44 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Karagulyan", "Avetik", ""], ["Dalalyan", "Arnak S.", ""]]}, {"id": "2006.14062", "submitter": "Kaizheng Wang", "authors": "Emmanuel Abbe, Jianqing Fan, Kaizheng Wang", "title": "An $\\ell_p$ theory of PCA and spectral clustering", "comments": "72 pages, 2 figures. Added the analysis of multi-class sub-Gaussian\n  mixture model and contextual community detection in sparse networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.PR stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal Component Analysis (PCA) is a powerful tool in statistics and\nmachine learning. While existing study of PCA focuses on the recovery of\nprincipal components and their associated eigenvalues, there are few precise\ncharacterizations of individual principal component scores that yield\nlow-dimensional embedding of samples. That hinders the analysis of various\nspectral methods. In this paper, we first develop an $\\ell_p$ perturbation\ntheory for a hollowed version of PCA in Hilbert spaces which provably improves\nupon the vanilla PCA in the presence of heteroscedastic noises. Through a novel\n$\\ell_p$ analysis of eigenvectors, we investigate entrywise behaviors of\nprincipal component score vectors and show that they can be approximated by\nlinear functionals of the Gram matrix in $\\ell_p$ norm, which includes $\\ell_2$\nand $\\ell_\\infty$ as special examples. For sub-Gaussian mixture models, the\nchoice of $p$ giving optimal bounds depends on the signal-to-noise ratio, which\nfurther yields optimality guarantees for spectral clustering. For contextual\ncommunity detection, the $\\ell_p$ theory leads to a simple spectral algorithm\nthat achieves the information threshold for exact recovery. These also provide\noptimal recovery results for Gaussian mixture and stochastic block models as\nspecial cases.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2020 21:30:28 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 21:23:44 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Abbe", "Emmanuel", ""], ["Fan", "Jianqing", ""], ["Wang", "Kaizheng", ""]]}, {"id": "2006.14126", "submitter": "David Frazier", "authors": "David T. Frazier", "title": "Robust and Efficient Approximate Bayesian Computation: A Minimum\n  Distance Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many instances, the application of approximate Bayesian methods is\nhampered by two practical features: 1) the requirement to project the data down\nto low-dimensional summary, including the choice of this projection, which\nultimately yields inefficient inference; 2) a possible lack of robustness to\ndeviations from the underlying model structure. Motivated by these efficiency\nand robustness concerns, we construct a new Bayesian method that can deliver\nefficient estimators when the underlying model is well-specified, and which is\nsimultaneously robust to certain forms of model misspecification. This new\napproach bypasses the calculation of summaries by considering a norm between\nempirical and simulated probability measures. For specific choices of the norm,\nwe demonstrate that this approach can deliver point estimators that are as\nefficient as those obtained using exact Bayesian inference, while also\nsimultaneously displaying robustness to deviations from the underlying model\nassumptions.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 01:11:02 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Frazier", "David T.", ""]]}, {"id": "2006.14179", "submitter": "Anna Bykhovskaya", "authors": "Anna Bykhovskaya and Vadim Gorin", "title": "Cointegration in large VARs", "comments": "51 pages. V3: extra discussion of model specifications added", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper analyses cointegration in vector autoregressive processes (VARs)\nfor the cases when both the number of coordinates, $N$, and the number of time\nperiods, $T$, are large and of the same order. We propose a way to examine a\nVAR for the presence of cointegration based on a modification of the Johansen\nlikelihood ratio test. The advantage of our procedure over the original\nJohansen test and its finite sample corrections is that our test does not\nsuffer from over-rejection. This is achieved through novel asymptotic theorems\nfor eigenvalues of matrices in the test statistic in the regime of\nproportionally growing $N$ and $T$. Our theoretical findings are supported by\nMonte Carlo simulations and an empirical illustration. Moreover, we find a\nsurprising connection with multivariate analysis of variance (MANOVA) and\nexplain why it emerges.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 05:06:34 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2020 03:14:58 GMT"}, {"version": "v3", "created": "Mon, 18 Jan 2021 00:57:44 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Bykhovskaya", "Anna", ""], ["Gorin", "Vadim", ""]]}, {"id": "2006.14226", "submitter": "Sylvain Le Corff", "authors": "Elisabeth Gassiat (LMO), Sylvain Le Corff (IP Paris, CITI,\n  TIPIC-SAMOVAR), Luc Leh\\'ericy (JAD)", "title": "Deconvolution with unknown noise distribution is possible for\n  multivariate signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the deconvolution problem in the case where the target\nsignal is multidimensional and no information is known about the noise\ndistribution. More precisely, no assumption is made on the noise distribution\nand no samples are available to estimate it: the deconvolution problem is\nsolved based only on the corrupted signal observations. We establish the\nidentifiability of the model up to translation when the signal has a Laplace\ntransform with an exponential growth smaller than $2$ and when it can be\ndecomposed into two dependent components. Then, we propose an estimator of the\nprobability density function of the signal without any assumption on the noise\ndistribution. As this estimator depends of the lightness of the tail of the\nsignal distribution which is usually unknown, a model selection procedure is\nproposed to obtain an adaptive estimator in this parameter with the same rate\nof convergence as the estimator with a known tail parameter. Finally, we\nestablish a lower bound on the minimax rate of convergence that matches the\nupper bound.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 07:46:43 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 08:55:45 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Gassiat", "Elisabeth", "", "LMO"], ["Corff", "Sylvain Le", "", "IP Paris, CITI,\n  TIPIC-SAMOVAR"], ["Leh\u00e9ricy", "Luc", "", "JAD"]]}, {"id": "2006.14409", "submitter": "Marcia Schafgans Dr", "authors": "J. Hidalgo and M. Schafgans", "title": "Inference without smoothing for large panels with cross-sectional and\n  temporal dependence", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses inference in large panel data models in the presence of\nboth cross-sectional and temporal dependence of unknown form. We are interested\nin making inferences that do not rely on the choice of any smoothing parameter\nas is the case with the often employed \"HAC\" estimator for the covariance\nmatrix. To that end, we propose a cluster estimator for the asymptotic\ncovariance of the estimators and valid bootstrap schemes that do not require\nthe selection of a bandwidth or smoothing parameter and accommodate the\nnonparametric nature of both temporal and cross-sectional dependence. Our\napproach is based on the observation that the spectral representation of the\nfixed effect panel data model is such that the errors become approximately\ntemporally uncorrelated. Our proposed bootstrap schemes can be viewed as wild\nbootstraps in the frequency domain. We present some Monte-Carlo simulations to\nshed some light on the small sample performance of our inferential procedure.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 13:48:01 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Hidalgo", "J.", ""], ["Schafgans", "M.", ""]]}, {"id": "2006.14667", "submitter": "Clement de Chaisemartin", "authors": "Cl\\'ement de Chaisemartin and Xavier D'Haultf{\\oe}uille", "title": "Empirical MSE Minimization to Estimate a Scalar Parameter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of a scalar parameter, when two estimators are\navailable. The first is always consistent. The second is inconsistent in\ngeneral, but has a smaller asymptotic variance than the first, and may be\nconsistent if an assumption is satisfied. We propose to use the weighted sum of\nthe two estimators with the lowest estimated mean-squared error (MSE). We show\nthat this third estimator dominates the other two from a minimax-regret\nperspective: the maximum asymptotic-MSE-gain one may incur by using this\nestimator rather than one of the other estimators is larger than the maximum\nasymptotic-MSE-loss.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 19:20:37 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["de Chaisemartin", "Cl\u00e9ment", ""], ["D'Haultf\u0153uille", "Xavier", ""]]}, {"id": "2006.14734", "submitter": "Nilabja Guha", "authors": "Nilabja Guha and Anindya Roy", "title": "Stochastic Approximation Algorithm for Estimating Mixing Distribution\n  for Dependent Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the mixing density of a mixture distribution remains an\ninteresting problem in statistics literature. Using a stochastic approximation\nmethod, Newton and Zhang (1999) introduced a fast recursive algorithm for\nestimating the mixing density of a mixture. Under suitably chosen weights the\nstochastic approximation estimator converges to the true solution. In Tokdar\net. al. (2009) the consistency of this recursive estimation method was\nestablished. However, the proof of consistency of the resulting estimator used\nindependence among observations as an assumption. Here, we extend the\ninvestigation of performance of Newton's algorithm to several dependent\nscenarios. We first prove that the original algorithm under certain conditions\nremains consistent when the observations are arising form a weakly dependent\nprocess with fixed marginal with the target mixture as the marginal density.\nFor some of the common dependent structures where the original algorithm is no\nlonger consistent, we provide a modification of the algorithm that generates a\nconsistent estimator.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 00:00:12 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Guha", "Nilabja", ""], ["Roy", "Anindya", ""]]}, {"id": "2006.14801", "submitter": "Qian Qin", "authors": "Qian Qin, Galin L. Jones", "title": "Convergence Rates of Two-Component MCMC Samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Component-wise MCMC algorithms, including Gibbs and conditional\nMetropolis-Hastings samplers, are commonly used for sampling from multivariate\nprobability distributions. A long-standing question regarding Gibbs algorithms\nis whether a deterministic-scan (systematic-scan) sampler converges faster than\nits random-scan counterpart. We answer this question when the samplers involve\ntwo components by establishing an exact quantitative relationship between the\n$L^2$ convergence rates of the two samplers. The relationship shows that the\ndeterministic-scan sampler converges faster. We also establish qualitative\nrelations among the convergence rates of two-component Gibbs samplers and some\nconditional Metropolis-Hastings variants. For instance, it is shown that if\nsome two-component conditional Metropolis-Hastings samplers are geometrically\nergodic, then so are the associated Gibbs samplers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 05:12:30 GMT"}, {"version": "v2", "created": "Fri, 22 Jan 2021 03:46:25 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 02:48:11 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Qin", "Qian", ""], ["Jones", "Galin L.", ""]]}, {"id": "2006.14818", "submitter": "Alexander Kukush", "authors": "Alexander Kukush, Ivan Senko", "title": "Prediction in polynomial errors-in-variables models", "comments": "Published at https://doi.org/10.15559/20-VMSTA154 in the Modern\n  Stochastics: Theory and Applications (https://vmsta.org/) by VTeX\n  (http://www.vtex.lt/)", "journal-ref": "Modern Stochastics: Theory and Applications 2020, Vol. 7, No. 2,\n  203-219", "doi": "10.15559/20-VMSTA154", "report-no": "VTeX-VMSTA-VMSTA154", "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multivariate errors-in-variables (EIV) model with an intercept term, and a\npolynomial EIV model are considered. Focus is made on a structural\nhomoskedastic case, where vectors of covariates are i.i.d. and measurement\nerrors are i.i.d. as well. The covariates contaminated with errors are normally\ndistributed and the corresponding classical errors are also assumed normal. In\nboth models, it is shown that (inconsistent) ordinary least squares estimators\nof regression parameters yield an a.s. approximation to the best prediction of\nresponse given the values of observable covariates. Thus, not only in the\nlinear EIV, but in the polynomial EIV models as well, consistent estimators of\nregression parameters are useless in the prediction problem, provided the size\nand covariance structure of observation errors for the predicted subject do not\ndiffer from those in the data used for the model fitting.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 06:36:08 GMT"}], "update_date": "2020-06-29", "authors_parsed": [["Kukush", "Alexander", ""], ["Senko", "Ivan", ""]]}, {"id": "2006.15167", "submitter": "Span Spanbauer", "authors": "Span Spanbauer, Cameron Freer, Vikash Mansinghka", "title": "Deep Involutive Generative Models for Neural MCMC", "comments": "13 pages, 6 figures. Revised discussion of the Jacobian determinant\n  factor in the acceptance ratio", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.NE math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce deep involutive generative models, a new architecture for deep\ngenerative modeling, and use them to define Involutive Neural MCMC, a new\napproach to fast neural MCMC. An involutive generative model represents a\nprobability kernel $G(\\phi \\mapsto \\phi')$ as an involutive (i.e.,\nself-inverting) deterministic function $f(\\phi, \\pi)$ on an enlarged state\nspace containing auxiliary variables $\\pi$. We show how to make these models\nvolume preserving, and how to use deep volume-preserving involutive generative\nmodels to make valid Metropolis-Hastings updates based on an auxiliary variable\nscheme with an easy-to-calculate acceptance ratio. We prove that deep\ninvolutive generative models and their volume-preserving special case are\nuniversal approximators for probability kernels. This result implies that with\nenough network capacity and training time, they can be used to learn\narbitrarily complex MCMC updates. We define a loss function and optimization\nalgorithm for training parameters given simulated data. We also provide initial\nexperiments showing that Involutive Neural MCMC can efficiently explore\nmulti-modal distributions that are intractable for Hybrid Monte Carlo, and can\nconverge faster than A-NICE-MC, a recently introduced neural MCMC technique.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 18:45:29 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 15:42:01 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Spanbauer", "Span", ""], ["Freer", "Cameron", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "2006.15202", "submitter": "Anya Katsevich", "authors": "Anya Katsevich and Afonso Bandeira", "title": "Likelihood Maximization and Moment Matching in Low SNR Gaussian Mixture\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive an asymptotic expansion for the log likelihood of Gaussian mixture\nmodels (GMMs) with equal covariance matrices in the low signal-to-noise regime.\nThe expansion reveals an intimate connection between two types of algorithms\nfor parameter estimation: the method of moments and likelihood optimizing\nalgorithms such as Expectation-Maximization (EM). We show that likelihood\noptimization in the low SNR regime reduces to a sequence of least squares\noptimization problems that match the moments of the estimate to the ground\ntruth moments one by one. This connection is a stepping stone toward the\nanalysis of EM and maximum likelihood estimation in a wide range of models. A\nmotivating application for the study of low SNR mixture models is cryo-electron\nmicroscopy data, which can be modeled as a GMM with algebraic constraints\nimposed on the mixture centers. We discuss the application of our expansion to\nalgebraically constrained GMMs, among other example models of interest.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 20:33:43 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Katsevich", "Anya", ""], ["Bandeira", "Afonso", ""]]}, {"id": "2006.15634", "submitter": "Muhammad Fuady Emzir", "authors": "Muhammad Emzir and Sari Lasanen and Zenith Purisha and Lassi Roininen\n  and Simo S\\\"arkk\\\"a", "title": "Non-Stationary Multi-layered Gaussian Priors for Bayesian Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study Bayesian inverse problems with multi-layered\nGaussian priors. We first describe the conditionally Gaussian layers in terms\nof a system of stochastic partial differential equations. We build the\ncomputational inference method using a finite-dimensional Galerkin method. We\nshow that the proposed approximation has a convergence-in-probability property\nto the solution of the original multi-layered model. We then carry out Bayesian\ninference using the preconditioned Crank--Nicolson algorithm which is modified\nto work with multi-layered Gaussian fields. We show via numerical experiments\nin signal deconvolution and computerized X-ray tomography problems that the\nproposed method can offer both smoothing and edge preservation at the same\ntime.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 15:28:18 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Emzir", "Muhammad", ""], ["Lasanen", "Sari", ""], ["Purisha", "Zenith", ""], ["Roininen", "Lassi", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2006.15640", "submitter": "Ryan Martin", "authors": "Huiying Mao and Ryan Martin and Brian Reich", "title": "Valid model-free spatial prediction", "comments": "30 pages, 9 figures, 3 tables. Comments welcome at\n  https://www.researchers.one/article/2020-06-12", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the response at an unobserved location is a fundamental problem in\nspatial statistics. Given the difficulty in modeling spatial dependence,\nespecially in non-stationary cases, model-based prediction intervals are at\nrisk of misspecification bias that can negatively affect their validity. Here\nwe present a new approach for model-free spatial prediction based on the {\\em\nconformal prediction} machinery. Our key observation is that spatial data can\nbe treated as exactly or approximately exchangeable in a wide range of\nsettings. For example, when the spatial locations are deterministic, we prove\nthat the response values are, in a certain sense, locally approximately\nexchangeable for a broad class of spatial processes, and we develop a local\nspatial conformal prediction algorithm that yields valid prediction intervals\nwithout model assumptions. Numerical examples with both real and simulated data\nconfirm that the proposed conformal prediction intervals are valid and\ngenerally more efficient than existing model-based procedures across a range of\nnon-stationary and non-Gaussian settings.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 16:12:57 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Mao", "Huiying", ""], ["Martin", "Ryan", ""], ["Reich", "Brian", ""]]}, {"id": "2006.15667", "submitter": "X. Jessie Jeng", "authors": "X. Jessie Jeng and Yifei Hu", "title": "High-Dimensional Inference for Unidentifiable Signals with False\n  Negative Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  False negative errors are of major concern in applications where missing a\nhigh proportion of true signals may cause serious consequences. False negative\ncontrol, however, raises a bottleneck challenge in high-dimensional inference\nwhen signals are not identifiable at individual levels. We develop a new\nanalytic framework to regulate false negative errors under measures tailored\ntowards modern applications with high-dimensional data. A new method is\nproposed in realistic settings with arbitrary covariance dependence between\nvariables. We explicate the joint effects of covariance dependence and signal\nsparsity on the new method and interpret the results using a phase diagram. It\nshows that signals that are not individually identifiable can be effectively\nretained by the proposed method without incurring excessive false positives.\nSimulation studies are conducted to compare the new method with several\nexisting methods. The new method outperforms the others in adapting to a\nuser-specified false negative control level. We apply the new method to analyze\nan fMRI dataset to locate voxels that are functionally relevant to saccadic eye\nmovements. The new method exhibits a nice balance in retaining signal voxels\nand avoiding excessive noise voxels.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 17:50:19 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 01:16:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Jeng", "X. Jessie", ""], ["Hu", "Yifei", ""]]}, {"id": "2006.15681", "submitter": "Mats Julius Stensrud", "authors": "Mats J. Stensrud, James M. Robins, Aaron Sarvet, Eric J. Tchetgen\n  Tchetgen, Jessica G. Young", "title": "Conditional separable effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers are often interested in treatment effects on outcomes that are\nonly defined conditional on a post-treatment event status. For example, in a\nstudy of the effect of different cancer treatments on quality of life at end of\nfollow-up, the quality of life of individuals who die during the study is\nundefined. In these settings, a naive contrast of outcomes conditional on the\npost-treatment variable is not an average causal effect, even in a randomized\nexperiment. Therefore the effect in the principal stratum of those who would\nhave the same value of the post-treatment variable regardless of treatment,\nsuch as the always survivors in a truncation by death setting, is often\nadvocated for causal inference. While this principal stratum effect is a well\ndefined causal contrast, it is often hard to justify that it is relevant to\nscientists, patients or policy makers, and it cannot be identified without\nrelying on unfalsifiable assumptions. Here we formulate alternative estimands,\nthe conditional separable effects, that have a natural causal interpretation\nunder assumptions that can be falsified in a randomized experiment. We provide\nidentification results and introduce different estimators, including a doubly\nrobust estimator derived from the nonparametric influence function. As an\nillustration, we estimate a conditional separable effect of chemotherapies on\nquality of life in patients with prostate cancer, using data from a randomized\nclinical trial.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 19:09:50 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2020 14:10:01 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 15:46:08 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Stensrud", "Mats J.", ""], ["Robins", "James M.", ""], ["Sarvet", "Aaron", ""], ["Tchetgen", "Eric J. Tchetgen", ""], ["Young", "Jessica G.", ""]]}, {"id": "2006.15738", "submitter": "Pierre-Andr\\'e Maugis", "authors": "P-A. Maugis", "title": "Central limit theorems for local network statistics", "comments": "39 pages, 4 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI math.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgraph counts - in particular the number of occurrences of small shapes\nsuch as triangles - characterize properties of random networks, and as a result\nhave seen wide use as network summary statistics. However, subgraphs are\ntypically counted globally, and existing approaches fail to describe\nvertex-specific characteristics. On the other hand, rooted subgraph counts -\ncounts focusing on any given vertex's neighborhood - are fundamental\ndescriptors of local network properties. We derive the asymptotic joint\ndistribution of rooted subgraph counts in inhomogeneous random graphs, a model\nwhich generalizes many popular statistical network models. This result enables\na shift in the statistical analysis of large graphs, from estimating network\nsummaries, to estimating models linking local network structure and\nvertex-specific covariates. As an example, we consider a school friendship\nnetwork and show that local friendship patterns are significant predictors of\ngender and race.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2020 22:50:51 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Maugis", "P-A.", ""]]}, {"id": "2006.15785", "submitter": "Steve Hanneke", "authors": "Steve Hanneke and Samory Kpotufe", "title": "A No-Free-Lunch Theorem for MultiTask Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multitask learning and related areas such as multi-source domain adaptation\naddress modern settings where datasets from $N$ related distributions $\\{P_t\\}$\nare to be combined towards improving performance on any single such\ndistribution ${\\cal D}$. A perplexing fact remains in the evolving theory on\nthe subject: while we would hope for performance bounds that account for the\ncontribution from multiple tasks, the vast majority of analyses result in\nbounds that improve at best in the number $n$ of samples per task, but most\noften do not improve in $N$. As such, it might seem at first that the\ndistributional settings or aggregation procedures considered in such analyses\nmight be somehow unfavorable; however, as we show, the picture happens to be\nmore nuanced, with interestingly hard regimes that might appear otherwise\nfavorable.\n  In particular, we consider a seemingly favorable classification scenario\nwhere all tasks $P_t$ share a common optimal classifier $h^*,$ and which can be\nshown to admit a broad range of regimes with improved oracle rates in terms of\n$N$ and $n$. Some of our main results are as follows:\n  $\\bullet$ We show that, even though such regimes admit minimax rates\naccounting for both $n$ and $N$, no adaptive algorithm exists; that is, without\naccess to distributional information, no algorithm can guarantee rates that\nimprove with large $N$ for $n$ fixed.\n  $\\bullet$ With a bit of additional information, namely, a ranking of tasks\n$\\{P_t\\}$ according to their distance to a target ${\\cal D}$, a simple\nrank-based procedure can achieve near optimal aggregations of tasks' datasets,\ndespite a search space exponential in $N$. Interestingly, the optimal\naggregation might exclude certain tasks, even though they all share the same\n$h^*$.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 03:03:29 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2020 16:13:18 GMT"}, {"version": "v3", "created": "Thu, 23 Jul 2020 17:43:21 GMT"}, {"version": "v4", "created": "Wed, 5 Aug 2020 18:05:50 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Hanneke", "Steve", ""], ["Kpotufe", "Samory", ""]]}, {"id": "2006.15786", "submitter": "Shrijita Bhattacharya", "authors": "Shrijita Bhattacharya and Tapabrata Maiti", "title": "Statistical Foundation of Variational Bayes Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the popularism of Bayesian neural networks in recent years, its use\nis somewhat limited in complex and big data situations due to the computational\ncost associated with full posterior evaluations. Variational Bayes (VB)\nprovides a useful alternative to circumvent the computational cost and time\ncomplexity associated with the generation of samples from the true posterior\nusing Markov Chain Monte Carlo (MCMC) techniques. The efficacy of the VB\nmethods is well established in machine learning literature. However, its\npotential broader impact is hindered due to a lack of theoretical validity from\na statistical perspective. However there are few results which revolve around\nthe theoretical properties of VB, especially in non-parametric problems. In\nthis paper, we establish the fundamental result of posterior consistency for\nthe mean-field variational posterior (VP) for a feed-forward artificial neural\nnetwork model. The paper underlines the conditions needed to guarantee that the\nVP concentrates around Hellinger neighborhoods of the true density function.\nAdditionally, the role of the scale parameter and its influence on the\nconvergence rates has also been discussed. The paper mainly relies on two\nresults (1) the rate at which the true posterior grows (2) the rate at which\nthe KL-distance between the posterior and variational posterior grows. The\ntheory provides a guideline of building prior distributions for Bayesian NN\nmodels along with an assessment of accuracy of the corresponding VB\nimplementation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 03:04:18 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Bhattacharya", "Shrijita", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2006.15805", "submitter": "Adrian R\\\"ollin", "authors": "Gursharn Kaur and Adrian R\\\"ollin", "title": "Higher-order fluctuations in dense random graph models", "comments": "36 pages, 1 table, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our main results are quantitative bounds in the multivariate normal\napproximation of centred subgraph counts in random graphs generated by a\ngeneral graphon and independent vertex labels. We are interested in these\nstatistics because they are key to understanding fluctuations of regular\nsubgraph counts -- a cornerstone of dense graph limit theory. We also identify\nthe resulting limiting Gaussian stochastic measures by means of the theory of\ngeneralised $U$-statistics and Gaussian Hilbert spaces, which we think is a\nsuitable framework to describe and understand higher-order fluctuations in\ndense random graph models. With this article, we believe we answer the question\n\"What is the central limit theorem of dense graph limit theory?\". We complement\nthe theory with some statistical applications to illustrate the use of centred\nsubgraph counts in network modelling.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 04:35:09 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 14:20:50 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Kaur", "Gursharn", ""], ["R\u00f6llin", "Adrian", ""]]}, {"id": "2006.16193", "submitter": "Jing Dong", "authors": "Jing Dong and Xin T. Tong", "title": "Spectral Gap of Replica Exchange Langevin Diffusion on Mixture\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Langevin diffusion (LD) is one of the main workhorses for sampling problems.\nHowever, its convergence rate can be significantly reduced if the target\ndistribution is a mixture of multiple densities, especially when each component\nconcentrates around a different mode. Replica exchange Langevin diffusion\n(ReLD) is a sampling method that can circumvent this issue. In particular, ReLD\nadds another LD sampling a high-temperature version of the target density, and\nexchange the locations of two LDs according to a Metropolis-Hasting type of\nlaw. This approach can be further extended to multiple replica exchange\nLangevin diffusion (mReLD), where $K$ additional LDs are added to sample\ndistributions at different temperatures and exchanges take place between\nneighboring-temperature processes. While ReLD and mReLD have been used\nextensively in statistical physics, molecular dynamics, and other applications,\nthere is little existing analysis on its convergence rate and choices of\ntemperatures. This paper closes these gaps assuming the target distribution is\na mixture of log-concave densities. We show ReLD can obtain constant or even\nbetter convergence rates even when the density components of the mixture\nconcentrate around isolated modes. We also show using mReLD with $K$ additional\nLDs can achieve the same result while the exchange frequency only needs to be\n$(1/K)$-th power of the one in ReLD.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:01:33 GMT"}, {"version": "v2", "created": "Fri, 10 Jul 2020 19:36:45 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Dong", "Jing", ""], ["Tong", "Xin T.", ""]]}, {"id": "2006.16200", "submitter": "Nikos Zarifis", "authors": "Ilias Diakonikolas, Daniel M. Kane, Nikos Zarifis", "title": "Near-Optimal SQ Lower Bounds for Agnostically Learning Halfspaces and\n  ReLUs under Gaussian Marginals", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the fundamental problems of agnostically learning halfspaces and\nReLUs under Gaussian marginals. In the former problem, given labeled examples\n$(\\mathbf{x}, y)$ from an unknown distribution on $\\mathbb{R}^d \\times \\{ \\pm\n1\\}$, whose marginal distribution on $\\mathbf{x}$ is the standard Gaussian and\nthe labels $y$ can be arbitrary, the goal is to output a hypothesis with 0-1\nloss $\\mathrm{OPT}+\\epsilon$, where $\\mathrm{OPT}$ is the 0-1 loss of the\nbest-fitting halfspace. In the latter problem, given labeled examples\n$(\\mathbf{x}, y)$ from an unknown distribution on $\\mathbb{R}^d \\times\n\\mathbb{R}$, whose marginal distribution on $\\mathbf{x}$ is the standard\nGaussian and the labels $y$ can be arbitrary, the goal is to output a\nhypothesis with square loss $\\mathrm{OPT}+\\epsilon$, where $\\mathrm{OPT}$ is\nthe square loss of the best-fitting ReLU. We prove Statistical Query (SQ) lower\nbounds of $d^{\\mathrm{poly}(1/\\epsilon)}$ for both of these problems. Our SQ\nlower bounds provide strong evidence that current upper bounds for these tasks\nare essentially best possible.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 17:10:10 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Diakonikolas", "Ilias", ""], ["Kane", "Daniel M.", ""], ["Zarifis", "Nikos", ""]]}, {"id": "2006.16244", "submitter": "Dmitri Koroliouk", "authors": "V.S. Koroliuk, D. Koroliouk", "title": "Filtering of stationary Gaussian statistical experiments", "comments": "9 pages, 54 formulas, no figures, 8 refs", "journal-ref": "Ukrainian Matematical Bulletin, Vol. 16, No. 3, pp. 372-382\n  July-September, 2019", "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a new filtering model for stationary Gaussian Markov\nstatistical experiments, given by diffusion-type difference stochastic\nequations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2020 11:07:00 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Koroliuk", "V. S.", ""], ["Koroliouk", "D.", ""]]}, {"id": "2006.16485", "submitter": "Chao Gao", "authors": "Pinhan Chen, Chao Gao, Anderson Y. Zhang", "title": "Partial Recovery for Top-$k$ Ranking: Optimality of MLE and\n  Sub-Optimality of Spectral Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given partially observed pairwise comparison data generated by the\nBradley-Terry-Luce (BTL) model, we study the problem of top-$k$ ranking. That\nis, to optimally identify the set of top-$k$ players. We derive the minimax\nrate with respect to a normalized Hamming loss. This provides the first result\nin the literature that characterizes the partial recovery error in terms of the\nproportion of mistakes for top-$k$ ranking. We also derive the optimal signal\nto noise ratio condition for the exact recovery of the top-$k$ set. The maximum\nlikelihood estimator (MLE) is shown to achieve both optimal partial recovery\nand optimal exact recovery. On the other hand, we show another popular\nalgorithm, the spectral method, is in general sub-optimal. Our results\ncomplement the recent work by Chen et al. (2019) that shows both the MLE and\nthe spectral method achieve the optimal sample complexity for exact recovery.\nIt turns out the leading constants of the sample complexity are different for\nthe two algorithms. Another contribution that may be of independent interest is\nthe analysis of the MLE without any penalty or regularization for the BTL\nmodel. This closes an important gap between theory and practice in the\nliterature of ranking.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 02:32:42 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 06:47:53 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Chen", "Pinhan", ""], ["Gao", "Chao", ""], ["Zhang", "Anderson Y.", ""]]}, {"id": "2006.16501", "submitter": "Dan Yang", "authors": "Xin Chen, Dan Yang, Yan Xu, Yin Xia, Dong Wang, Haipeng Shen", "title": "Testing and Support Recovery of Correlation Structures for Matrix-Valued\n  Observations with an Application to Stock Market Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the covariance matrix of asset returns is crucial to portfolio\nconstruction. As suggested by economic theories, the correlation structure\namong assets differs between emerging markets and developed countries. It is\ntherefore imperative to make rigorous statistical inference on correlation\nmatrix equality between the two groups of countries. However, if the\ntraditional vector-valued approach is undertaken, such inference is either\ninfeasible due to limited number of countries comparing to the relatively\nabundant assets, or invalid due to the violations of temporal independence\nassumption. This highlights the necessity of treating the observations as\nmatrix-valued rather than vector-valued. With matrix-valued observations, our\nproblem of interest can be formulated as statistical inference on covariance\nstructures under matrix normal distributions, i.e., testing independence and\ncorrelation equality, as well as the corresponding support estimations. We\ndevelop procedures that are asymptotically optimal under some regularity\nconditions. Simulation results demonstrate the computational and statistical\nadvantages of our procedures over certain existing state-of-the-art methods.\nApplication of our procedures to stock market data validates several economic\npropositions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 03:18:19 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Chen", "Xin", ""], ["Yang", "Dan", ""], ["Xu", "Yan", ""], ["Xia", "Yin", ""], ["Wang", "Dong", ""], ["Shen", "Haipeng", ""]]}, {"id": "2006.16573", "submitter": "Rameshwar Pratap", "authors": "Amit Deshpande and Rameshwar Pratap", "title": "Subspace approximation with outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.DS math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subspace approximation problem with outliers, for given $n$ points in $d$\ndimensions $x_{1},\\ldots, x_{n} \\in R^{d}$, an integer $1 \\leq k \\leq d$, and\nan outlier parameter $0 \\leq \\alpha \\leq 1$, is to find a $k$-dimensional\nlinear subspace of $R^{d}$ that minimizes the sum of squared distances to its\nnearest $(1-\\alpha)n$ points. More generally, the $\\ell_{p}$ subspace\napproximation problem with outliers minimizes the sum of $p$-th powers of\ndistances instead of the sum of squared distances. Even the case of robust PCA\nis non-trivial, and previous work requires additional assumptions on the input.\nAny multiplicative approximation algorithm for the subspace approximation\nproblem with outliers must solve the robust subspace recovery problem, a\nspecial case in which the $(1-\\alpha)n$ inliers in the optimal solution are\npromised to lie exactly on a $k$-dimensional linear subspace. However, robust\nsubspace recovery is Small Set Expansion (SSE)-hard.\n  We show how to extend dimension reduction techniques and bi-criteria\napproximations based on sampling to the problem of subspace approximation with\noutliers. To get around the SSE-hardness of robust subspace recovery, we assume\nthat the squared distance error of the optimal $k$-dimensional subspace summed\nover the optimal $(1-\\alpha)n$ inliers is at least $\\delta$ times its\nsquared-error summed over all $n$ points, for some $0 < \\delta \\leq 1 -\n\\alpha$. With this assumption, we give an efficient algorithm to find a subset\nof $poly(k/\\epsilon) \\log(1/\\delta) \\log\\log(1/\\delta)$ points whose span\ncontains a $k$-dimensional subspace that gives a multiplicative\n$(1+\\epsilon)$-approximation to the optimal solution. The running time of our\nalgorithm is linear in $n$ and $d$. Interestingly, our results hold even when\nthe fraction of outliers $\\alpha$ is large, as long as the obvious condition $0\n< \\delta \\leq 1 - \\alpha$ is satisfied.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 07:22:33 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Deshpande", "Amit", ""], ["Pratap", "Rameshwar", ""]]}, {"id": "2006.16590", "submitter": "Pierre Humbert", "authors": "Pierre Humbert (ENS Paris Saclay), Batiste Le Bars (ENS Paris Saclay),\n  Ludovic Minvielle (ENS Paris Saclay), Nicolas Vayatis (ENS Paris Saclay)", "title": "Robust Kernel Density Estimation with Median-of-Means principle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a robust nonparametric density estimator\ncombining the popular Kernel Density Estimation method and the Median-of-Means\nprinciple (MoM-KDE). This estimator is shown to achieve robustness to any kind\nof anomalous data, even in the case of adversarial contamination. In\nparticular, while previous works only prove consistency results under known\ncontamination model, this work provides finite-sample high-probability\nerror-bounds without a priori knowledge on the outliers. Finally, when compared\nwith other robust kernel estimators, we show that MoM-KDE achieves competitive\nresults while having significant lower computational complexity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 08:01:07 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Humbert", "Pierre", "", "ENS Paris Saclay"], ["Bars", "Batiste Le", "", "ENS Paris Saclay"], ["Minvielle", "Ludovic", "", "ENS Paris Saclay"], ["Vayatis", "Nicolas", "", "ENS Paris Saclay"]]}, {"id": "2006.16600", "submitter": "Guillaume Chauvet", "authors": "Guillaume Chauvet (IRMAR), Mathieu Gerber (MMT)", "title": "Exponential inequalities for sampling designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a general approach, based on the mar-tingale\nrepresentation of a sampling design and Azuma-Hoeffding's inequality , to\nderive exponential inequalities for the difference between a Horvitz-Thompson\nestimator and its expectation. Applying this idea, we establish such\ninequalities for Chao's procedure, Till{\\'e}'s elimination procedure, the\ngeneralized Midzuno method as well as for Brewer's method. As a by-product, we\nprove that the first three sampling designs are (conditionally) negatively\nassociated. For such sampling designs, we show that that the inequality we\nobtain is usually sharper than the one obtained by applying known results for\nnegatively associated random variables.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 08:28:47 GMT"}, {"version": "v2", "created": "Fri, 23 Oct 2020 13:38:46 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Chauvet", "Guillaume", "", "IRMAR"], ["Gerber", "Mathieu", "", "MMT"]]}, {"id": "2006.16744", "submitter": "Qiang Wu", "authors": "Hongwei Sun (University of Jinan) and Qiang Wu (Middle Tennessee State\n  University)", "title": "Optimal Rates of Distributed Regression with Imperfect Kernels", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed machine learning systems have been receiving increasing\nattentions for their efficiency to process large scale data. Many distributed\nframeworks have been proposed for different machine learning tasks. In this\npaper, we study the distributed kernel regression via the divide and conquer\napproach. This approach has been proved asymptotically minimax optimal if the\nkernel is perfectly selected so that the true regression function lies in the\nassociated reproducing kernel Hilbert space. However, this is usually, if not\nalways, impractical because kernels that can only be selected via prior\nknowledge or a tuning process are hardly perfect. Instead it is more common\nthat the kernel is good enough but imperfect in the sense that the true\nregression can be well approximated by but does not lie exactly in the kernel\nspace. We show distributed kernel regression can still achieves capacity\nindependent optimal rate in this case. To this end, we first establish a\ngeneral framework that allows to analyze distributed regression with response\nweighted base algorithms by bounding the error of such algorithms on a single\ndata set, provided that the error bounds has factored the impact of the\nunexplained variance of the response variable. Then we perform a leave one out\nanalysis of the kernel ridge regression and bias corrected kernel ridge\nregression, which in combination with the aforementioned framework allows us to\nderive sharp error bounds and capacity independent optimal rates for the\nassociated distributed kernel regression algorithms. As a byproduct of the\nthorough analysis, we also prove the kernel ridge regression can achieve rates\nfaster than $N^{-1}$ (where $N$ is the sample size) in the noise free setting\nwhich, to our best knowledge, are first observed and novel in regression\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2020 13:00:16 GMT"}], "update_date": "2020-07-01", "authors_parsed": [["Sun", "Hongwei", "", "University of Jinan"], ["Wu", "Qiang", "", "Middle Tennessee State\n  University"]]}]