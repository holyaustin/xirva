[{"id": "1705.00535", "submitter": "Stavros Stavroyiannis", "authors": "Stavros Stavroyiannis", "title": "A note on the Nelson Cao inequality constraints in the GJR-GARCH model:\n  Is there a leverage effect?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of stylized facts of financial time series and several\nValue-at-Risk measures are modeled via univariate or multivariate GARCH\nprocesses. It is not rare that advanced GARCH models fail to converge for\ncomputational reasons, and a usual parsimonious approach is the GJR-GARCH\nmodel. There is a disagreement in the literature and the specialized\neconometric software, on which constraints should be used for the parameters,\nintroducing indirectly the distinction between asymmetry and leverage. We show\nthat the approach used by various software packages is not consistent with the\nNelson-Cao inequality constraints. Implementing Monte Carlo simulations,\ndespite of the results being empirically correct, the estimated parameters are\nnot theoretically coherent with the Nelson-Cao constraints for ensuring\npositivity of conditional variances. On the other hand ruling out the leverage\nhypothesis, the asymmetry term in the GJR model can take negative values when\ntypical constraints like the condition for the existence of the second and\nfourth moments, are imposed.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 14:20:19 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Stavroyiannis", "Stavros", ""]]}, {"id": "1705.00891", "submitter": "Syed Ali Asad Rizvi", "authors": "Syed Ali Asad Rizvi, Stephen J. Roberts, Michael A. Osborne and Favour\n  Nyikosa", "title": "A Novel Approach to Forecasting Financial Volatility with Gaussian\n  Process Envelopes", "comments": "16 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CE q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we use Gaussian Process (GP) regression to propose a novel\napproach for predicting volatility of financial returns by forecasting the\nenvelopes of the time series. We provide a direct comparison of their\nperformance to traditional approaches such as GARCH. We compare the forecasting\npower of three approaches: GP regression on the absolute and squared returns;\nregression on the envelope of the returns and the absolute returns; and\nregression on the envelope of the negative and positive returns separately. We\nuse a maximum a posteriori estimate with a Gaussian prior to determine our\nhyperparameters. We also test the effect of hyperparameter updating at each\nforecasting step. We use our approaches to forecast out-of-sample volatility of\nfour currency pairs over a 2 year period, at half-hourly intervals. From three\nkernels, we select the kernel giving the best performance for our data. We use\ntwo published accuracy measures and four statistical loss functions to evaluate\nthe forecasting ability of GARCH vs GPs. In mean squared error the GP's perform\n20% better than a random walk model, and 50% better than GARCH for the same\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 10:30:13 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Rizvi", "Syed Ali Asad", ""], ["Roberts", "Stephen J.", ""], ["Osborne", "Michael A.", ""], ["Nyikosa", "Favour", ""]]}, {"id": "1705.01142", "submitter": "Swetava Ganguli", "authors": "Swetava Ganguli, Jared Dunnmon", "title": "Machine Learning for Better Models for Predicting Bond Prices", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bond prices are a reflection of extremely complex market interactions and\npolicies, making prediction of future prices difficult. This task becomes even\nmore challenging due to the dearth of relevant information, and accuracy is not\nthe only consideration--in trading situations, time is of the essence. Thus,\nmachine learning in the context of bond price predictions should be both fast\nand accurate. In this course project, we use a dataset describing the previous\n10 trades of a large number of bonds among other relevant descriptive metrics\nto predict future bond prices. Each of 762,678 bonds in the dataset is\ndescribed by a total of 61 attributes, including a ground truth trade price. We\nevaluate the performance of various supervised learning algorithms for\nregression followed by ensemble methods, with feature and model selection\nconsiderations being treated in detail. We further evaluate all methods on both\naccuracy and speed. Finally, we propose a novel hybrid time-series aided\nmachine learning method that could be applied to such datasets in future work.\n", "versions": [{"version": "v1", "created": "Fri, 31 Mar 2017 15:12:49 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Ganguli", "Swetava", ""], ["Dunnmon", "Jared", ""]]}, {"id": "1705.01144", "submitter": "Jaydip Sen", "authors": "Jaydip Sen and Tamal Datta Chaudhuri", "title": "A Time Series Analysis-Based Forecasting Framework for the Indian\n  Healthcare Sector", "comments": "23 pages, 10 figures, 8 tables. The paper is accepted for publication\n  in \"Journal of Insurance and Financial Management\" Vol 3, No 1, 2017", "journal-ref": "Journal of Insurance and Financial Management, Vol 3, No 1, pp. 1-\n  29, 2017", "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing efficient and robust algorithms for accurate prediction of stock\nmarket prices is one of the most exciting challenges in the field of time\nseries analysis and forecasting. With the exponential rate of development and\nevolution of sophisticated algorithms and with the availability of fast\ncomputing platforms, it has now become possible to effectively and efficiently\nextract, store, process and analyze high volume of stock market data with\ndiversity in its contents. Availability of complex algorithms which can execute\nvery fast on parallel architecture over the cloud has made it possible to\nachieve higher accuracy in forecasting results while reducing the time required\nfor computation. In this paper, we use the time series data of the healthcare\nsector of India for the period January 2010 till December 2016. We first\ndemonstrate a decomposition approach of the time series and then illustrate how\nthe decomposition results provide us with useful insights into the behavior and\nproperties exhibited by the time series. Further, based on the structural\nanalysis of the time series, we propose six different methods of forecasting\nfor predicting the time series index of the healthcare sector. Extensive\nresults are provided on the performance of the forecasting methods to\ndemonstrate their effectiveness.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 15:33:20 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Sen", "Jaydip", ""], ["Chaudhuri", "Tamal Datta", ""]]}, {"id": "1705.01145", "submitter": "Joana Estevens", "authors": "Joana Estevens, Paulo Rocha, Joao Boto, Pedro Lind", "title": "Stochastic modelling of non-stationary financial assets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model non-stationary volume-price distributions with a log-normal\ndistribution and collect the time series of its two parameters. The time series\nof the two parameters are shown to be stationary and Markov-like and\nconsequently can be modelled with Langevin equations, which are derived\ndirectly from their series of values. Having the evolution equations of the\nlog-normal parameters, we reconstruct the statistics of the first moments of\nvolume-price distributions which fit well the empirical data. Finally, the\nproposed framework is general enough to study other non-stationary stochastic\nvariables in other research fields, namely biology, medicine and geology.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 15:24:47 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Estevens", "Joana", ""], ["Rocha", "Paulo", ""], ["Boto", "Joao", ""], ["Lind", "Pedro", ""]]}, {"id": "1705.01406", "submitter": "Longfeng Zhao", "authors": "Longfeng Zhao, Wei Li, Andrea Fenu, Boris Podobnik, Yougui Wang, H.\n  Eugene Stanley", "title": "The q-dependent detrended cross-correlation analysis of stock market", "comments": "25 pages, 16 figures", "journal-ref": null, "doi": "10.1088/1742-5468/aa9db0", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The properties of q-dependent cross-correlation matrices of stock market have\nbeen analyzed by using the random matrix theory and complex network. The\ncorrelation structures of the fluctuations at different magnitudes have unique\nproperties. The cross-correlations among small fluctuations are much stronger\nthan those among large fluctuations. The large and small fluctuations are\ndominated by different groups of stocks. We use complex network representation\nto study these q-dependent matrices and discover some new identities. By\nutilizing those q-dependent correlation-based networks, we are able to\nconstruct some portfolio by those most independent stocks which consistently\nperform the best. The optimal multifractal order for portfolio optimization is\napproximately $q=2$. These results have deepened our understanding about the\ncollective behaviors of the complex financial system.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 20:18:51 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 15:05:15 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Zhao", "Longfeng", ""], ["Li", "Wei", ""], ["Fenu", "Andrea", ""], ["Podobnik", "Boris", ""], ["Wang", "Yougui", ""], ["Stanley", "H. Eugene", ""]]}, {"id": "1705.01407", "submitter": "Sourish Das", "authors": "Sourish Das, Rituparna Sen", "title": "Sparse Portfolio selection via Bayesian Multiple testing", "comments": "23 pages, 8 figures, 9 tables", "journal-ref": "Sankhya-B 2020", "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.PM q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We presented Bayesian portfolio selection strategy, via the $k$ factor asset\npricing model. If the market is information efficient, the proposed strategy\nwill mimic the market; otherwise, the strategy will outperform the market. The\nstrategy depends on the selection of a portfolio via Bayesian multiple testing\nmethodologies. We present the \"discrete-mixture prior\" model and the\n\"hierarchical Bayes model with horseshoe prior.\" We define the Oracle set and\nprove that asymptotically the Bayes rule attains the risk of Bayes Oracle up to\n$O(1)$. Our proposed Bayes Oracle test guarantees statistical power by\nproviding the upper bound of the type-II error. Simulation study indicates that\nthe proposed Bayes oracle test is suitable for the efficient market with few\nstocks inefficiently priced. However, as the model becomes dense, i.e., the\nmarket is highly inefficient, one should not use the Bayes oracle test. The\nstatistical power of the Bayes Oracle portfolio is uniformly better for the\n$k$-factor model ($k>1$) than the one factor CAPM. We present the empirical\nstudy, where we considered the 500 constituent stocks of S\\&P 500 from the New\nYork Stock Exchange (NYSE), and S\\&P 500 index as the benchmark for thirteen\nyears from the year 2006 to 2018. We showed the out-sample risk and return\nperformance of the four different portfolio selection strategies and compared\nwith the S\\&P 500 index as the benchmark market index. Empirical results\nindicate that it is possible to propose a strategy which can outperform the\nmarket.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 05:35:12 GMT"}, {"version": "v2", "created": "Sat, 20 Jan 2018 12:56:00 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 12:42:46 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 10:43:38 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Das", "Sourish", ""], ["Sen", "Rituparna", ""]]}, {"id": "1705.02344", "submitter": "Jakob Knollm\\\"uller", "authors": "Jakob Knollm\\\"uller, Torsten A. En{\\ss}lin", "title": "Noisy independent component analysis of auto-correlated components", "comments": null, "journal-ref": "Phys. Rev. E 96, 042114 (2017)", "doi": "10.1103/PhysRevE.96.042114", "report-no": null, "categories": "stat.ME astro-ph.IM physics.data-an q-bio.QM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for the separation of superimposed, independent,\nauto-correlated components from noisy multi-channel measurement. The presented\nmethod simultaneously reconstructs and separates the components, taking all\nchannels into account and thereby increases the effective signal-to-noise ratio\nconsiderably, allowing separations even in the high noise regime.\nCharacteristics of the measurement instruments can be included, allowing for\napplication in complex measurement situations. Independent posterior samples\ncan be provided, permitting error estimates on all desired quantities. Using\nthe concept of information field theory, the algorithm is not restricted to any\ndimensionality of the underlying space or discretization scheme thereof.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 18:00:04 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 10:17:59 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Knollm\u00fcller", "Jakob", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "1705.03396", "submitter": "Pavel Shevchenko V", "authors": "Philippe Deprez, Pavel V. Shevchenko and Mario V. W\\\"uthrich", "title": "Machine Learning Techniques for Mortality Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various stochastic models have been proposed to estimate mortality rates. In\nthis paper we illustrate how machine learning techniques allow us to analyze\nthe quality of such mortality models. In addition, we present how these\ntechniques can be used for differentiating the different causes of death in\nmortality modeling.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 13:37:37 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Deprez", "Philippe", ""], ["Shevchenko", "Pavel V.", ""], ["W\u00fcthrich", "Mario V.", ""]]}, {"id": "1705.06533", "submitter": "Nestor Parolya Jun.-Prof. Dr.", "authors": "David Bauder, Taras Bodnar, Nestor Parolya and Wolfgang Schmid", "title": "Bayesian Inference of the Multi-Period Optimal Portfolio for an\n  Exponential Utility", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.PM q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation of the multi-period optimal portfolio obtained by\nmaximizing an exponential utility. Employing Jeffreys' non-informative prior\nand the conjugate informative prior, we derive stochastic representations for\nthe optimal portfolio weights at each time point of portfolio reallocation.\nThis provides a direct access not only to the posterior distribution of the\nportfolio weights but also to their point estimates together with uncertainties\nand their asymptotic distributions. Furthermore, we present the posterior\npredictive distribution for the investor's wealth at each time point of the\ninvestment period in terms of a stochastic representation for the future wealth\nrealization. This in turn makes it possible to use quantile-based risk measures\nor to calculate the probability of default. We apply the suggested Bayesian\napproach to assess the uncertainty in the multi-period optimal portfolio by\nconsidering assets from the FTSE 100 in the weeks after the British referendum\nto leave the European Union. The behaviour of the novel portfolio estimation\nmethod in a precarious market situation is illustrated by calculating the\npredictive wealth, the risk associated with the holding portfolio, and the\ndefault probability in each period.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 11:41:04 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Bauder", "David", ""], ["Bodnar", "Taras", ""], ["Parolya", "Nestor", ""], ["Schmid", "Wolfgang", ""]]}, {"id": "1705.06899", "submitter": "Zhongmin Luo", "authors": "Raymond Brummelhuis and Zhongmin Luo", "title": "CDS Rate Construction Methods by Machine Learning Techniques", "comments": "51 pages; 21 Figures; 15 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regulators require financial institutions to estimate counterparty default\nrisks from liquid CDS quotes for the valuation and risk management of OTC\nderivatives. However, the vast majority of counterparties do not have liquid\nCDS quotes and need proxy CDS rates. Existing methods cannot account for\ncounterparty-specific default risks; we propose to construct proxy CDS rates by\nassociating to illiquid counterparty liquid CDS Proxy based on Machine Learning\nTechniques. After testing 156 classifiers from 8 most popular classifier\nfamilies, we found that some classifiers achieve highly satisfactory accuracy\nrates. Furthermore, we have rank-ordered the performances and investigated\nperformance variations amongst and within the 8 classifier families. This paper\nis, to the best of our knowledge, the first systematic study of CDS Proxy\nconstruction by Machine Learning techniques, and the first systematic\nclassifier comparison study based entirely on financial market data. Its\nfindings both confirm and contrast existing classifier performance literature.\nGiven the typically highly correlated nature of financial data, we investigated\nthe impact of correlation on classifier performance. The techniques used in\nthis paper should be of interest for financial institutions seeking a CDS Proxy\nmethod, and can serve for proxy construction for other financial variables.\nSome directions for future research are indicated.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 09:20:30 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Brummelhuis", "Raymond", ""], ["Luo", "Zhongmin", ""]]}, {"id": "1705.08022", "submitter": "Yash Sharma", "authors": "Yash Sharma", "title": "Using Macroeconomic Forecasts to Improve Mean Reverting Trading\n  Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR q-fin.PM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large class of trading strategies focus on opportunities offered by the\nyield curve. In particular, a set of yield curve trading strategies are based\non the view that the yield curve mean-reverts. Based on these strategies'\npositive performance, a multiple pairs trading strategy on major currency pairs\nwas implemented. To improve the algorithm's performance, machine learning\nforecasts of a series of pertinent macroeconomic variables were factored in, by\noptimizing the weights of the trading signals. This resulted in a clear\nimprovement in the APR over the evaluation period, demonstrating that\nmacroeconomic indicators, not only technical indicators, should be considered\nin trading strategies.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 22:10:56 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Sharma", "Yash", ""]]}, {"id": "1705.08240", "submitter": "Jichang Zhao", "authors": "Shan Lu, Jichang Zhao, Huiwen Wang and Ruoen Ren", "title": "Herding boosts too-connected-to-fail risk in stock market of China", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2018.04.020", "report-no": null, "categories": "q-fin.GN physics.soc-ph q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crowd panic and its contagion play non-negligible roles at the time of\nthe stock crash, especially for China where inexperienced investors dominate\nthe market. However, existing models rarely consider investors in networking\nstocks and accordingly miss the exact knowledge of how panic contagion leads to\nabrupt crash. In this paper, by networking stocks of sharing common mutual\nfunds, a new methodology of investigating the market crash is presented. It is\nsurprisingly revealed that the herding, which origins in the mimic of seeking\nfor high diversity across investment strategies to lower individual risk, will\nproduce too-connected-to-fail stocks and reluctantly boosts the systemic risk\nof the entire market. Though too-connected stocks might be relatively stable\nduring the crisis, they are so influential that a small downward fluctuation\nwill cascade to trigger severe drops of massive successor stocks, implying that\ntheir falls might be unexpectedly amplified by the collective panic and result\nin the market crash. Our findings suggest that the whole picture of portfolio\nstrategy has to be carefully supervised to reshape the stock network.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 10:39:31 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 06:57:30 GMT"}, {"version": "v3", "created": "Wed, 25 Apr 2018 14:11:26 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Lu", "Shan", ""], ["Zhao", "Jichang", ""], ["Wang", "Huiwen", ""], ["Ren", "Ruoen", ""]]}, {"id": "1705.08301", "submitter": "Samuel Cohen", "authors": "Samuel N. Cohen", "title": "Data and uncertainty in extreme risks - a nonlinear expectations\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of tail quantities, such as expected shortfall or Value at Risk,\nis a difficult problem. We show how the theory of nonlinear expectations, in\nparticular the Data-robust expectation introduced in [5], can assist in the\nquantification of statistical uncertainty for these problems. However, when we\nare in a heavy-tailed context (in particular when our data are described by a\nPareto distribution, as is common in much of extreme value theory), the theory\nof [5] is insufficient, and requires an additional regularization step which we\nintroduce. By asking whether this regularization is possible, we obtain a\nqualitative requirement for reliable estimation of tail quantities and risk\nmeasures, in a Pareto setting.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 14:18:54 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 15:04:10 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Cohen", "Samuel N.", ""]]}]