[{"id": "2103.00264", "submitter": "Parley Ruogu Yang", "authors": "Parley Ruogu Yang", "title": "Forecasting high-frequency financial time series: an adaptive learning\n  approach with the order book data", "comments": "Key words: forecasting methods, statistical learning, high-frequency\n  order book", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.EM q-fin.TR stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper proposes a forecast-centric adaptive learning model that engages\nwith the past studies on the order book and high-frequency data, with\napplications to hypothesis testing. In line with the past literature, we\nproduce brackets of summaries of statistics from the high-frequency bid and ask\ndata in the CSI 300 Index Futures market and aim to forecast the one-step-ahead\nprices. Traditional time series issues, e.g. ARIMA order selection,\nstationarity, together with potential financial applications are covered in the\nexploratory data analysis, which pave paths to the adaptive learning model. By\ndesigning and running the learning model, we found it to perform well compared\nto the top fixed models, and some could improve the forecasting accuracy by\nbeing more stable and resilient to non-stationarity. Applications to hypothesis\ntesting are shown with a rolling window, and further potential applications to\nfinance and statistics are outlined.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 16:42:02 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yang", "Parley Ruogu", ""]]}, {"id": "2103.00366", "submitter": "Kristof Lommers", "authors": "Kristof Lommers, Ouns El Harzli, Jack Kim", "title": "Confronting Machine Learning With Financial Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study aims to examine the challenges and applications of machine\nlearning for financial research. Machine learning algorithms have been\ndeveloped for certain data environments which substantially differ from the one\nwe encounter in finance. Not only do difficulties arise due to some of the\nidiosyncrasies of financial markets, there is a fundamental tension between the\nunderlying paradigm of machine learning and the research philosophy in\nfinancial economics. Given the peculiar features of financial markets and the\nempirical framework within social science, various adjustments have to be made\nto the conventional machine learning methodology. We discuss some of the main\nchallenges of machine learning in finance and examine how these could be\naccounted for. Despite some of the challenges, we argue that machine learning\ncould be unified with financial research to become a robust complement to the\neconometrician's toolbox. Moreover, we discuss the various applications of\nmachine learning in the research process such as estimation, empirical\ndiscovery, testing, causal inference and prediction.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 01:10:09 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 23:53:13 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Lommers", "Kristof", ""], ["Harzli", "Ouns El", ""], ["Kim", "Jack", ""]]}, {"id": "2103.00395", "submitter": "Nassim Dehouche", "authors": "Nassim Dehouche", "title": "Scale matters: The daily, weekly and monthly volatility and\n  predictability of Bitcoin, Gold, and the S&P 500", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A reputation of high volatility accompanies the emergence of Bitcoin as a\nfinancial asset. This paper intends to nuance this reputation and clarify our\nunderstanding of Bitcoin's volatility. Using daily, weekly, and monthly closing\nprices and log-returns data going from September 2014 to January 2021, we find\nthat Bitcoin is a prime example of an asset for which the two conceptions of\nvolatility diverge. We show that, historically, Bitcoin allies both high\nvolatility (high Standard Deviation) and high predictability (low Approximate\nEntropy), relative to Gold and S&P 500.\n  Moreover, using tools from Extreme Value Theory, we analyze the convergence\nof moments, and the mean excess functions of both the closing prices and the\nlog-returns of the three assets. We find that the closing price of Bitcoin is\nconsistent with a generalized Pareto distribution, when the closing prices of\nthe two other assets (Gold and S&P 500) present thin-tailed distributions.\nHowever, returns for all three assets are heavy tailed and second moments\n(variance, standard deviation) non-convergent. In the case of Bitcoin, lower\nsampling frequencies (monthly vs weekly, weekly vs daily) drastically reduce\nthe Kurtosis of log-returns and increase the convergence of empirical moments\nto their true value. The opposite effect is observed for Gold and S&P 500.\nThese properties suggest that Bitcoin's volatility is essentially an intra-day\nand intra-week phenomenon that is strongly attenuated on a weekly time-scale,\nand make it an attractive store of value to investors and speculators, but its\nhigh standard deviation excludes its use a currency.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 04:43:58 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Dehouche", "Nassim", ""]]}, {"id": "2103.00788", "submitter": "Geoffrey Ducournau", "authors": "Geoffrey Ducournau", "title": "Statistical mechanics and Bayesian Inference addressed to the Osborne\n  Paradox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One of the greatest contributors of the 20th century among all academician in\nthe field of statistical finance, M. F. M. Osborne published in 1956 [6] an\nessential paper and proposed to treat the question of stock market motion\nthrough the prism of both the Law of Weber-Fechner [1, 4] and the branch of\nphysics developed by James Clerk Maxwell, Ludwig Boltzmann and Josiah Willard\nGibbs [3, 5] namely the statistical mechanics. He proposed an improvement of\nthe known research made by his predecessor Louis Jean-Baptiste Alphonse\nBachelier, by not considering the arithmetic changes of stock prices as means\nof statistical measurement, but by drawing on the Weber-Fechner Law, to treat\nthe changes of prices. Osborne emphasized that as in statistical mechanics, the\nprobability distribution of the steady-state of subjective change in prices is\ndetermined by the condition of maximum probability, a statement close to the\nGibbs distribution conditions. However, Osborne also admitted that the\nempirical observation of the probability distribution of logarithmic changes of\nstock prices was emphasizing obvious asymmetries and consequently could not\nperfectly confirm his prior theory. The purpose of this paper is to propose an\nexplanation to what we could call the Osborne paradox and then address an\nalternative approach via Bayesian inference regarding the description of the\nprobability distribution of changes in logarithms of prices that was\nthenceforth under the prism of frequentist inference. We show that the stock\nmarket returns are locally described by equilibrium statistical mechanics with\nconserved statistics variables, whereas globally there is yet other statistics\nwith persistent flowing variables that can be effectively described by a\nsuperposition of several statistics on different time scales, namely, a\nsuperstatistics.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:24:32 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ducournau", "Geoffrey", ""]]}, {"id": "2103.01669", "submitter": "Rolando Gonzales Martinez", "authors": "Rolando Gonzales Martinez", "title": "How good is good? Probabilistic benchmarks and nanofinance+", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Benchmarks are standards that allow to identify opportunities for improvement\namong comparable units. This study suggests a 2-step methodology for\ncalculating probabilistic benchmarks in noisy data sets: (i) double-hyperbolic\nundersampling filters the noise of key performance indicators (KPIs), and (ii)\na relevance vector machine estimates probabilistic benchmarks with denoised\nKPIs. The usefulness of the methods is illustrated with an application to a\ndatabase of nano-finance+. The results indicate that-in the case of\nnano-finance groups-a higher discrimination power is obtained with variables\nthat capture the macro-economic environment of the country where a group\noperates. Also, the estimates show that groups operating in rural regions have\ndifferent probabilistic benchmarks, compared to groups in urban and peri-urban\nareas.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:06:41 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Martinez", "Rolando Gonzales", ""]]}, {"id": "2103.01670", "submitter": "John Cartlidge", "authors": "Zijian Shi, Yu Chen, John Cartlidge", "title": "The LOB Recreation Model: Predicting the Limit Order Book from TAQ\n  History Using an Ordinary Differential Equation Recurrent Neural Network", "comments": "12 pages, preprint accepted for publication in the 35th AAAI\n  Conference on Artificial Intelligence (AAAI-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.CE cs.NE q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an order-driven financial market, the price of a financial asset is\ndiscovered through the interaction of orders - requests to buy or sell at a\nparticular price - that are posted to the public limit order book (LOB).\nTherefore, LOB data is extremely valuable for modelling market dynamics.\nHowever, LOB data is not freely accessible, which poses a challenge to market\nparticipants and researchers wishing to exploit this information. Fortunately,\ntrades and quotes (TAQ) data - orders arriving at the top of the LOB, and\ntrades executing in the market - are more readily available. In this paper, we\npresent the LOB recreation model, a first attempt from a deep learning\nperspective to recreate the top five price levels of the LOB for small-tick\nstocks using only TAQ data. Volumes of orders sitting deep in the LOB are\npredicted by combining outputs from: (1) a history compiler that uses a Gated\nRecurrent Unit (GRU) module to selectively compile prediction relevant quote\nhistory; (2) a market events simulator, which uses an Ordinary Differential\nEquation Recurrent Neural Network (ODE-RNN) to simulate the accumulation of net\norder arrivals; and (3) a weighting scheme to adaptively combine the\npredictions generated by (1) and (2). By the paradigm of transfer learning, the\nsource model trained on one stock can be fine-tuned to enable application to\nother financial assets of the same class with much lower demand on additional\ndata. Comprehensive experiments conducted on two real world intraday LOB\ndatasets demonstrate that the proposed model can efficiently recreate the LOB\nwith high accuracy using only TAQ data as input.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:07:43 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Shi", "Zijian", ""], ["Chen", "Yu", ""], ["Cartlidge", "John", ""]]}, {"id": "2103.05921", "submitter": "Christian Bongiorno", "authors": "Damien Challet, Christian Bongiorno, Guillaume Pelletier", "title": "Financial factors selection with knockoffs: fund replication,\n  explanatory and prediction networks", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2021.126105", "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply the knockoff procedure to factor selection in finance. By building\nfake but realistic factors, this procedure makes it possible to control the\nfraction of false discovery in a given set of factors. To show its versatility,\nwe apply it to fund replication and to the inference of explanatory and\nprediction networks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:25:46 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Challet", "Damien", ""], ["Bongiorno", "Christian", ""], ["Pelletier", "Guillaume", ""]]}, {"id": "2103.07407", "submitter": "Thomas Deschatre", "authors": "Thomas Deschatre and Pierre Gruet", "title": "Electricity intraday price modeling with marked Hawkes processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR math.PR q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a 2-dimensional marked Hawkes process with increasing baseline\nintensity in order to model prices on electricity intraday markets. This model\nallows to represent different empirical facts such as increasing market\nactivity, random jump sizes but above all microstructure noise through the\nsignature plot. This last feature is of particular importance for practitioners\nand has not yet been modeled on those particular markets. We provide analytic\nformulas for first and second moments and for the signature plot, extending the\nclassic results of Bacry et al. (2013) in the context of Hawkes processes with\nrandom jump sizes and time dependent baseline intensity. The tractable model we\npropose is estimated on German data and seems to fit the data well. We also\nprovide a result about the convergence of the price process to a Brownian\nmotion with increasing volatility at macroscopic scales, highlighting the\nSamuelson effect.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:09:23 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 10:24:40 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Deschatre", "Thomas", ""], ["Gruet", "Pierre", ""]]}, {"id": "2103.08148", "submitter": "Andrey Pak", "authors": "Mohamed Abdelghani, Alexander Melnikov and Andrey Pak", "title": "On statistical estimation and inferences in optional regression models", "comments": "to be published in Statistics: A Journal of Theoretical and Applied\n  Statistics", "journal-ref": null, "doi": "10.1080/02331888.2021.1900186", "report-no": null, "categories": "math.ST q-fin.ST stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main object of investigation in this paper is a very general regression\nmodel in optional setting - when an observed process is an optional\nsemimartingale depending on an unknown parameter. It is well-known that\nstatistical data may present an information flow/filtration without usual\nconditions. The estimation problem is achieved by means of structural least\nsquares (LS) estimates and their sequential versions. The main results of the\npaper are devoted to the strong consistency of such LS-estimates. For\nsequential LS-estimates the property of fixed accuracy is proved.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 05:50:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Abdelghani", "Mohamed", ""], ["Melnikov", "Alexander", ""], ["Pak", "Andrey", ""]]}, {"id": "2103.09059", "submitter": "Michel Alexandre", "authors": "Michel Alexandre and Kau\\^e Lopes de Moraes and Francisco Aparecido\n  Rodrigues", "title": "Risk-dependent centrality in the Brazilian stock market", "comments": "12 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to calculate the risk-dependent centrality (RDC)\nof the Brazilian stock market. We computed the RDC for assets traded on the\nBrazilian stock market between January 2008 to June 2020 at different levels of\nexternal risk. We observed that the ranking of assets based on the RDC depends\non the external risk. Rankings' volatility is related to crisis events,\ncapturing the recent Brazilian economic-political crisis. Moreover, we have\nfound a negative correlation between the average volatility of assets' ranking\nbased on the RDC and the average daily returns on the stock market. It goes in\nhand with the hypothesis that the rankings' volatility is higher in periods of\ncrisis.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:48:58 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Alexandre", "Michel", ""], ["de Moraes", "Kau\u00ea Lopes", ""], ["Rodrigues", "Francisco Aparecido", ""]]}, {"id": "2103.09098", "submitter": "Yusen Lin", "authors": "Yusen Lin, Jinming Xue, Louiqa Raschid", "title": "Predicting the Behavior of Dealers in Over-The-Counter Corporate Bond\n  Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trading in Over-The-Counter (OTC) markets is facilitated by broker-dealers,\nin comparison to public exchanges, e.g., the New York Stock Exchange (NYSE).\nDealers play an important role in stabilizing prices and providing liquidity in\nOTC markets. We apply machine learning methods to model and predict the trading\nbehavior of OTC dealers for US corporate bonds. We create sequences of daily\nhistorical transaction reports for each dealer over a vocabulary of US\ncorporate bonds. Using this history of dealer activity, we predict the future\ntrading decisions of the dealer. We consider a range of neural network-based\nprediction models. We propose an extension, the Pointwise-Product ReZero (PPRZ)\nTransformer model, and demonstrate the improved performance of our model. We\nshow that individual history provides the best predictive model for the most\nactive dealers. For less active dealers, a collective model provides improved\nperformance. Further, clustering dealers based on their similarity can improve\nperformance. Finally, prediction accuracy varies based on the activity level of\nboth the bond and the dealer.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 04:22:07 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Lin", "Yusen", ""], ["Xue", "Jinming", ""], ["Raschid", "Louiqa", ""]]}, {"id": "2103.09106", "submitter": "Matloob Khushi Dr", "authors": "Jaideep Singh and Matloob Khushi", "title": "Feature Learning for Stock Price Prediction Shows a Significant Role of\n  Analyst Rating", "comments": null, "journal-ref": "Appl. Syst. Innov. 2021, 4, 17", "doi": "10.3390/asi4010017", "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To reject the Efficient Market Hypothesis a set of 5 technical indicators and\n23 fundamental indicators was identified to establish the possibility of\ngenerating excess returns on the stock market. Leveraging these data points and\nvarious classification machine learning models, trading data of the 505\nequities on the US S&P500 over the past 20 years was analysed to develop a\nclassifier effective for our cause. From any given day, we were able to predict\nthe direction of change in price by 1% up to 10 days in the future. The\npredictions had an overall accuracy of 83.62% with a precision of 85% for buy\nsignals and a recall of 100% for sell signals. Moreover, we grouped equities by\ntheir sector and repeated the experiment to see if grouping similar assets\ntogether positively effected the results but concluded that it showed no\nsignificant improvements in the performance rejecting the idea of sector-based\nanalysis. Also, using feature ranking we could identify an even smaller set of\n6 indicators while maintaining similar accuracies as that from the original 28\nfeatures and also uncovered the importance of buy, hold and sell analyst\nratings as they came out to be the top contributors in the model. Finally, to\nevaluate the effectiveness of the classifier in real-life situations, it was\nbacktested on FAANG equities using a modest trading strategy where it generated\nhigh returns of above 60% over the term of the testing dataset. In conclusion,\nour proposed methodology with the combination of purposefully picked features\nshows an improvement over the previous studies, and our model predicts the\ndirection of 1% price changes on the 10th day with high confidence and with\nenough buffer to even build a robotic trading system.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 03:56:29 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Singh", "Jaideep", ""], ["Khushi", "Matloob", ""]]}, {"id": "2103.09107", "submitter": "Stefania Scocchera", "authors": "Guglielmo D'Amico, Stefania Scocchera, Loriano Storchi", "title": "Randentropy: a software to measure inequality in random systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The software Randentropy is designed to estimate inequality in a random\nsystem where several individuals interact moving among many communities and\nproducing dependent random quantities of an attribute. The overall inequality\nis assessed by computing the Random Theil's Entropy. Firstly, the software\nestimates a piecewise homogeneous Markov chain by identifying the\nchanging-points and the relative transition probability matrices. Secondly, it\nestimates the multivariate distribution function of the attribute using a\ncopula function approach and finally, through a Monte Carlo algorithm,\nevaluates the expected value of the Random Theil's Entropy. Possible\napplications are discussed as related to the fields of finance and human\nmobility\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:18:06 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["D'Amico", "Guglielmo", ""], ["Scocchera", "Stefania", ""], ["Storchi", "Loriano", ""]]}, {"id": "2103.09750", "submitter": "Matloob Khushi Dr", "authors": "Zexin Hu, Yiqi Zhao and Matloob Khushi", "title": "A Survey of Forex and Stock Price Prediction Using Deep Learning", "comments": null, "journal-ref": "Appl. Syst. Innov. 2021, 4, 9", "doi": "10.3390/asi4010009", "report-no": null, "categories": "q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prediction of stock and foreign exchange (Forex) had always been a hot\nand profitable area of study. Deep learning application had proven to yields\nbetter accuracy and return in the field of financial prediction and\nforecasting. In this survey we selected papers from the DBLP database for\ncomparison and analysis. We classified papers according to different deep\nlearning methods, which included: Convolutional neural network (CNN), Long\nShort-Term Memory (LSTM), Deep neural network (DNN), Recurrent Neural Network\n(RNN), Reinforcement Learning, and other deep learning methods such as HAN,\nNLP, and Wavenet. Furthermore, this paper reviewed the dataset, variable,\nmodel, and results of each article. The survey presented the results through\nthe most used performance metrics: RMSE, MAPE, MAE, MSE, accuracy, Sharpe\nratio, and return rate. We identified that recent models that combined LSTM\nwith other methods, for example, DNN, are widely researched. Reinforcement\nlearning and other deep learning method yielded great returns and performances.\nWe conclude that in recent years the trend of using deep-learning based method\nfor financial modeling is exponentially rising.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 03:45:25 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hu", "Zexin", ""], ["Zhao", "Yiqi", ""], ["Khushi", "Matloob", ""]]}, {"id": "2103.09813", "submitter": "Charles-Albert Lehalle", "authors": "Mengda Li and Charles-Albert Lehalle", "title": "Do Word Embeddings Really Understand Loughran-McDonald's Polarities?", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we perform a rigorous mathematical analysis of the word2vec\nmodel, especially when it is equipped with the Skip-gram learning scheme. Our\ngoal is to explain how embeddings, that are now widely used in NLP (Natural\nLanguage Processing), are influenced by the distribution of terms in the\ndocuments of the considered corpus. We use a mathematical formulation to shed\nlight on how the decision to use such a model makes implicit assumptions on the\nstructure of the language. We show how Markovian assumptions, that we discuss,\nlead to a very clear theoretical understanding of the formation of embeddings,\nand in particular the way it captures what we call frequentist synonyms. These\nassumptions allow to produce generative models and to conduct an explicit\nanalysis of the loss function commonly used by these NLP techniques. Moreover,\nwe produce synthetic corpora with different levels of structure and show\nempirically how the word2vec algorithm succeed, or not, to learn them. It leads\nus to empirically assess the capability of such models to capture structures on\na corpus of around 42 millions of financial News covering 12 years. That for,\nwe rely on the Loughran-McDonald Sentiment Word Lists largely used on financial\ntexts and we show that embeddings are exposed to mixing terms with opposite\npolarity, because of the way they can treat antonyms as frequentist synonyms.\nBeside we study the non-stationarity of such a financial corpus, that has\nsurprisingly not be documented in the literature. We do it via time series of\ncosine similarity between groups of polarized words or company names, and show\nthat embedding are indeed capturing a mix of English semantics and joined\ndistribution of words that is difficult to disentangle.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 17:57:39 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Li", "Mengda", ""], ["Lehalle", "Charles-Albert", ""]]}, {"id": "2103.09987", "submitter": "Yu Man Tam", "authors": "Raymond C. W. Leung and Yu-Man Tam", "title": "Statistical Arbitrage Risk Premium by Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.PM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How to hedge factor risks without knowing the identities of the factors? We\nfirst prove a general theoretical result: even if the exact set of factors\ncannot be identified, any risky asset can use some portfolio of similar peer\nassets to hedge against its own factor exposures. A long position of a risky\nasset and a short position of a \"replicate portfolio\" of its peers represent\nthat asset's factor residual risk. We coin the expected return of an asset's\nfactor residual risk as its Statistical Arbitrage Risk Premium (SARP). The\nchallenge in empirically estimating SARP is finding the peers for each asset\nand constructing the replicate portfolios. We use the elastic-net, a machine\nlearning method, to project each stock's past returns onto that of every other\nstock. The resulting high-dimensional but sparse projection vector serves as\ninvestment weights in constructing the stocks' replicate portfolios. We say a\nstock has high (low) Statistical Arbitrage Risk (SAR) if it has low (high)\nR-squared with its peers. The key finding is that \"unique\" stocks have both a\nhigher SARP and higher excess returns than \"ubiquitous\" stocks: in the\ncross-section, high SAR stocks have a monthly SARP (monthly excess returns)\nthat is 1.101% (0.710%) greater than low SAR stocks. The average SAR across all\nstocks is countercyclical. Our results are robust to controlling for various\nknown priced factors and characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 02:18:52 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Leung", "Raymond C. W.", ""], ["Tam", "Yu-Man", ""]]}, {"id": "2103.10157", "submitter": "Tal Miller", "authors": "Tal Miller", "title": "Leveraged ETF Investing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is common knowledge that leverage can increase the potential returns of an\ninvestment, at the expense of increased risk. For a passive investor in the\nstock market, leverage can be achieved using margin debt or leveraged-ETFs. We\nperform bootstrapped Monte-Carlo simulations of leveraged (and unleveraged)\nmixed portfolios of stocks and bonds, based on past stock market data, and show\nthat leverage can amplify the potential returns, without significantly\nincreasing the risk for long-term investors.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:47:47 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Miller", "Tal", ""]]}, {"id": "2103.10989", "submitter": "Fouad Marri", "authors": "Fouad Marri and Khouzeima Moutanabbir", "title": "Risk aggregation and capital allocation using a new generalized\n  Archimedean copula", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP q-fin.PM q-fin.PR q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address risk aggregation and capital allocation problems in\nthe presence of dependence between risks. The dependence structure is defined\nby a mixed Bernstein copula which represents a generalization of the well-known\nArchimedean copulas. Using this new copula, the probability density function\nand the cumulative distribution function of the aggregate risk are obtained.\nThen, closed-form expressions for basic risk measures, such as tail\nvalue-at-risk(TVaR) and TVaR-based allocations, are derived.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 19:06:18 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Marri", "Fouad", ""], ["Moutanabbir", "Khouzeima", ""]]}, {"id": "2103.11706", "submitter": "Mario W\\\"uthrich V.", "authors": "M. Merz, R. Richman, T. Tsanakas, M.V. W\\\"uthrich", "title": "Interpreting Deep Learning Models with Marginal Attribution by\n  Conditioning on Quantiles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vastly growing literature on explaining deep learning models has emerged.\nThis paper contributes to that literature by introducing a global\ngradient-based model-agnostic method, which we call Marginal Attribution by\nConditioning on Quantiles (MACQ). Our approach is based on analyzing the\nmarginal attribution of predictions (outputs) to individual features (inputs).\nSpecificalllly, we consider variable importance by mixing (global) output\nlevels and, thus, explain how features marginally contribute across different\nregions of the prediction space. Hence, MACQ can be seen as a marginal\nattribution counterpart to approaches such as accumulated local effects (ALE),\nwhich study the sensitivities of outputs by perturbing inputs. Furthermore,\nMACQ allows us to separate marginal attribution of individual features from\ninteraction effect, and visually illustrate the 3-way relationship between\nmarginal attribution, output level, and feature value.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:20:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Merz", "M.", ""], ["Richman", "R.", ""], ["Tsanakas", "T.", ""], ["W\u00fcthrich", "M. V.", ""]]}, {"id": "2103.11869", "submitter": "Yiyan Huang", "authors": "Yiyan Huang, Cheuk Hang Leung, Xing Yan, Qi Wu", "title": "Higher-Order Orthogonal Causal Learning for Treatment Effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most existing studies on the double/debiased machine learning method\nconcentrate on the causal parameter estimation recovering from the first-order\northogonal score function. In this paper, we will construct the\n$k^{\\mathrm{th}}$-order orthogonal score function for estimating the average\ntreatment effect (ATE) and present an algorithm that enables us to obtain the\ndebiased estimator recovered from the score function. Such a higher-order\northogonal estimator is more robust to the misspecification of the propensity\nscore than the first-order one does. Besides, it has the merit of being\napplicable with many machine learning methodologies such as Lasso, Random\nForests, Neural Nets, etc. We also undergo comprehensive experiments to test\nthe power of the estimator we construct from the score function using both the\nsimulated datasets and the real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:04:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Huang", "Yiyan", ""], ["Leung", "Cheuk Hang", ""], ["Yan", "Xing", ""], ["Wu", "Qi", ""]]}, {"id": "2103.11948", "submitter": "Phillip Murray", "authors": "Hans Buehler, Phillip Murray, Mikko S. Pakkanen, Ben Wood", "title": "Deep Hedging: Learning Risk-Neutral Implied Volatility Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP math.OC q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a numerically efficient approach for learning a risk-neutral\nmeasure for paths of simulated spot and option prices up to a finite horizon\nunder convex transaction costs and convex trading constraints. This approach\ncan then be used to implement a stochastic implied volatility model in the\nfollowing two steps: 1. Train a market simulator for option prices, as\ndiscussed for example in our recent; 2. Find a risk-neutral density,\nspecifically the minimal entropy martingale measure. The resulting model can be\nused for risk-neutral pricing, or for Deep Hedging in the case of transaction\ncosts or trading constraints. To motivate the proposed approach, we also show\nthat market dynamics are free from \"statistical arbitrage\" in the absence of\ntransaction costs if and only if they follow a risk-neutral measure. We\nadditionally provide a more general characterization in the presence of convex\ntransaction costs and trading constraints. These results can be seen as an\nanalogue of the fundamental theorem of asset pricing for statistical arbitrage\nunder trading frictions and are of independent interest.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:38:25 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 09:43:52 GMT"}, {"version": "v3", "created": "Wed, 14 Jul 2021 13:33:15 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Buehler", "Hans", ""], ["Murray", "Phillip", ""], ["Pakkanen", "Mikko S.", ""], ["Wood", "Ben", ""]]}, {"id": "2103.12461", "submitter": "Konstantin H\\\"ausler", "authors": "Konstantin H\\\"ausler, Wolfgang Karl H\\\"ardle", "title": "Rodeo or Ascot: which hat to wear at the crypto race?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper sheds light on the dynamics of the cryptocurrency (CC) sector. By\nmodeling its dynamics via a stochastic volatility with correlated jumps (SVCJ)\nmodel in combination with several rolling windows, it is possible to capture\nthe extreme ups and downs of the CC market and to understand its dynamics.\nThrough this approach, we obtain time series for each parameter of the model.\nEven though parameter estimates change over time and depend on the window size,\nseveral recurring patterns are observable which are robust to changes of the\nwindow size and supported by clustering of parameter estimates: during bullish\nperiods, volatility stabilizes at low levels and the size and volatility of\njumps in mean decreases. In bearish periods though, volatility increases and\ntakes longer to return to its long-run trend. Furthermore, jumps in mean and\njumps in volatility are independent. With the rise of the CC market in 2017, a\nlevel shift of the volatility of volatility occurred. All codes are available\non Quantlet.com.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:30:11 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["H\u00e4usler", "Konstantin", ""], ["H\u00e4rdle", "Wolfgang Karl", ""]]}, {"id": "2103.13294", "submitter": "Apostolos Chalkis", "authors": "Apostolos Chalkis, Emmanouil Christoforou, Theodore Dalamagkas,\n  Ioannis Z. Emiris", "title": "Modeling of crisis periods in stock markets", "comments": "11 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit a recent computational framework to model and detect financial\ncrises in stock markets, as well as shock events in cryptocurrency markets,\nwhich are characterized by a sudden or severe drop in prices. Our method\nmanages to detect all past crises in the French industrial stock market\nstarting with the crash of 1929, including financial crises after 1990 (e.g.\ndot-com bubble burst of 2000, stock market downturn of 2002), and all past\ncrashes in the cryptocurrency market, namely in 2018, and also in 2020 due to\ncovid-19. We leverage copulae clustering, based on the distance between\nprobability distributions, in order to validate the reliability of the\nframework; we show that clusters contain copulae from similar market states\nsuch as normal states, or crises. Moreover, we propose a novel regression model\nthat can detect successfully all past events using less than 10% of the\ninformation that the previous framework requires. We train our model by\nhistorical data on the industry assets, and we are able to detect all past\nshock events in the cryptocurrency market. Our tools provide the essential\ncomponents of our software framework that offers fast and reliable detection,\nor even prediction, of shock events in stock and cryptocurrency markets of\nhundreds of assets.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 23:22:14 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Chalkis", "Apostolos", ""], ["Christoforou", "Emmanouil", ""], ["Dalamagkas", "Theodore", ""], ["Emiris", "Ioannis Z.", ""]]}, {"id": "2103.14079", "submitter": "Filippo Neri", "authors": "Filippo Neri", "title": "Domain Specific Concept Drift Detectors for Predicting Financial Time\n  Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Concept drift detectors allow learning systems to maintain good accuracy on\nnon-stationary data streams. Financial time series are an instance of\nnon-stationary data streams whose concept drifts (market phases) are so\nimportant to affect investment decisions worldwide. This paper studies how\nconcept drift detectors behave when applied to financial time series. General\nresults are: a) concept drift detectors usually improve the runtime over\ncontinuous learning, b) their computational cost is usually a fraction of the\nlearning and prediction steps of even basic learners, c) it is important to\nstudy concept drift detectors in combination with the learning systems they\nwill operate with, and d) concept drift detectors can be directly applied to\nthe time series of raw financial data and not only to the model's accuracy one.\nMoreover, the study introduces three simple concept drift detectors, tailored\nto financial time series, and shows that two of them can be at least as\neffective as the most sophisticated ones from the state of the art when applied\nto financial time series.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 07:48:31 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 21:19:52 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Neri", "Filippo", ""]]}, {"id": "2103.14080", "submitter": "Firuz Kamalov", "authors": "Firuz Kamalov, Linda Smail, Ikhlaas Gurrib", "title": "Forecasting with Deep Learning: S&P 500 index", "comments": "Published in: 2020 13th International Symposium on Computational\n  Intelligence and Design (ISCID)", "journal-ref": null, "doi": "10.1109/ISCID51228.2020.00102", "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock price prediction has been the focus of a large amount of research but\nan acceptable solution has so far escaped academics. Recent advances in deep\nlearning have motivated researchers to apply neural networks to stock\nprediction. In this paper, we propose a convolution-based neural network model\nfor predicting the future value of the S&P 500 index. The proposed model is\ncapable of predicting the next-day direction of the index based on the previous\nvalues of the index. Experiments show that our model outperforms a number of\nbenchmarks achieving an accuracy rate of over 55%.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 11:51:49 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kamalov", "Firuz", ""], ["Smail", "Linda", ""], ["Gurrib", "Ikhlaas", ""]]}, {"id": "2103.14081", "submitter": "Firuz Kamalov", "authors": "Firuz Kamalov, Linda Smail, Ikhlaas Gurrib", "title": "Stock price forecast with deep learning", "comments": "Published in: 2020 International Conference on Decision Aid Sciences\n  and Application (DASA)", "journal-ref": null, "doi": "10.1109/DASA51403.2020.9317260", "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we compare various approaches to stock price prediction using\nneural networks. We analyze the performance fully connected, convolutional, and\nrecurrent architectures in predicting the next day value of S&P 500 index based\non its previous values. We further expand our analysis by including three\ndifferent optimization techniques: Stochastic Gradient Descent, Root Mean\nSquare Propagation, and Adaptive Moment Estimation. The numerical experiments\nreveal that a single layer recurrent neural network with RMSprop optimizer\nproduces optimal results with validation and test Mean Absolute Error of 0.0150\nand 0.0148 respectively.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 12:52:37 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kamalov", "Firuz", ""], ["Smail", "Linda", ""], ["Gurrib", "Ikhlaas", ""]]}, {"id": "2103.14506", "submitter": "Wenpin Tang", "authors": "Wenpin Tang and Xiao Xu and Xun Yu Zhou", "title": "Asset Selection via Correlation Blockmodel Clustering", "comments": "33 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.CP q-fin.ST", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We aim to cluster financial assets in order to identify a small set of stocks\nto approximate the level of diversification of the whole universe of stocks. We\ndevelop a data-driven approach to clustering based on a correlation blockmodel\nin which assets in the same cluster have the same correlations with all other\nassets. We devise an algorithm to detect the clusters, with a theoretical\nanalysis and a practical guidance. Finally, we conduct an empirical analysis to\nattest the performance of the algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 15:04:22 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Tang", "Wenpin", ""], ["Xu", "Xiao", ""], ["Zhou", "Xun Yu", ""]]}, {"id": "2103.14592", "submitter": "Benjamin Sch\\\"afer", "authors": "Benjamin Sch\\\"afer, Marc Timme, Dirk Witthaut", "title": "Isolating the impact of trading on grid frequency fluctuations", "comments": null, "journal-ref": "Published in: 2018 IEEE PES Innovative Smart Grid Technologies\n  Conference Europe (ISGT-Europe)", "doi": "10.1109/ISGTEurope.2018.8571793", "report-no": null, "categories": "q-fin.ST nlin.AO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To ensure reliable operation of power grids, their frequency shall stay\nwithin strict bounds. Multiple sources of disturbances cause fluctuations of\nthe grid frequency, ranging from changing demand over volatile feed-in to\nenergy trading. Here, we analyze frequency time series from the continental\nEuropean grid in 2011 and 2017 as a case study to isolate the impact of\ntrading. We find that trading at typical trading intervals such as full hours\nmodifies the frequency fluctuation statistics. While particularly large\nfrequency deviations in 2017 are not as frequent as in 2011, large deviations\nare more likely to occur shortly after the trading instances. A comparison\nbetween the two years indicates that trading at shorter intervals might be\nbeneficial for frequency quality and grid stability, because particularly large\nfluctuations are substantially diminished. Furthermore, we observe that the\nstatistics of the frequency fluctuations do not follow Gaussian distributions\nbut are better described using heavy-tailed and asymmetric distributions, for\nexample L\\'evy-stable distributions. Comparing intervals without trading to\nthose with trading instances indicates that frequency deviations near the\ntrading times are distributed more widely and thus extreme deviations are\norders of magnitude more likely. Finally, we briefly review a stochastic\nanalysis that allows a quantitative description of power grid frequency\nfluctuations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:47:29 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Sch\u00e4fer", "Benjamin", ""], ["Timme", "Marc", ""], ["Witthaut", "Dirk", ""]]}, {"id": "2103.14593", "submitter": "Valeriy Kalyagin", "authors": "V.A. Kalyagin, A.P. Koldanov, P.A. Koldanov", "title": "Reliability of MST identification in correlation-based market networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Maximum spanning tree (MST) is a popular tool in market network analysis.\nLarge number of publications are devoted to the MST calculation and it's\ninterpretation for particular stock markets. However, much less attention is\npayed in the literature to the analysis of uncertainty of obtained results. In\nthe present paper we suggest a general framework to measure uncertainty of MST\nidentification. We study uncertainty in the framework of the concept of random\nvariable network (RVN). We consider different correlation based networks in the\nlarge class of elliptical distributions. We show that true MST is the same in\nthree networks: Pearson correlation network, Fechner correlation network, and\nKendall correlation network. We argue that among different measures of\nuncertainty the FDR (False Discovery Rate) is the most appropriated for MST\nidentification. We investigate FDR of Kruskal algorithm for MST identification\nand show that reliability of MST identification is different in these three\nnetworks. In particular, for Pearson correlation network the FDR essentially\ndepends on distribution of stock returns. We prove that for market network with\nFechner correlation the FDR is non sensitive to the assumption on stock's\nreturn distribution. Some interesting phenomena are discovered for Kendall\ncorrelation network. Our experiments show that FDR of Kruskal algorithm for MST\nidentification in Kendall correlation network weakly depend on distribution and\nat the same time the value of FDR is almost the best in comparison with MST\nidentification in other networks. These facts are important in practical\napplications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 20:17:53 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Kalyagin", "V. A.", ""], ["Koldanov", "A. P.", ""], ["Koldanov", "P. A.", ""]]}, {"id": "2103.15096", "submitter": "Jaydip Sen", "authors": "Jaydip Sen and Sidra Mehtab", "title": "Accurate Stock Price Forecasting Using Robust and Optimized Deep\n  Learning Models", "comments": "This paper is an accepted version of our paper in the IEEE\n  International Conference on Intelligent Technologies (IEEE CONIT), which will\n  be organized in Hubli, Karnataka, INDIA, from June 25 to June 27, 2021. The\n  paper is 8 pages long and it contains eleven tables and seventeen figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing robust frameworks for precise prediction of future prices of stocks\nhas always been considered a very challenging research problem. The advocates\nof the classical efficient market hypothesis affirm that it is impossible to\naccurately predict the future prices in an efficiently operating market due to\nthe stochastic nature of the stock price variables. However, numerous\npropositions exist in the literature with varying degrees of sophistication and\ncomplexity that illustrate how algorithms and models can be designed for making\nefficient, accurate, and robust predictions of stock prices. We present a gamut\nof ten deep learning models of regression for precise and robust prediction of\nthe future prices of the stock of a critical company in the auto sector of\nIndia. Using a very granular stock price collected at 5 minutes intervals, we\ntrain the models based on the records from 31st Dec, 2012 to 27th Dec, 2013.\nThe testing of the models is done using records from 30th Dec, 2013 to 9th Jan\n2015. We explain the design principles of the models and analyze the results of\ntheir performance based on accuracy in forecasting and speed of execution.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 09:52:29 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sen", "Jaydip", ""], ["Mehtab", "Sidra", ""]]}, {"id": "2103.15232", "submitter": "Pier Francesco Procacci", "authors": "Pier Francesco Procacci and Tomaso Aste", "title": "Portfolio Optimization with Sparse Multivariate Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.PM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Portfolio optimization approaches inevitably rely on multivariate modeling of\nmarkets and the economy. In this paper, we address three sources of error\nrelated to the modeling of these complex systems: 1. oversimplifying\nhypothesis; 2. uncertainties resulting from parameters' sampling error; 3.\nintrinsic non-stationarity of these systems. For what concerns point 1. we\npropose a L0-norm sparse elliptical modeling and show that sparsification is\neffective. The effects of points 2. and 3. are quantifified by studying the\nmodels' likelihood in- and out-of-sample for parameters estimated over train\nsets of different lengths. We show that models with larger off-sample\nlikelihoods lead to better performing portfolios up to when two to three years\nof daily observations are included in the train set. For larger train sets, we\nfound that portfolio performances deteriorate and detach from the models'\nlikelihood, highlighting the role of non-stationarity. We further investigate\nthis phenomenon by studying the out-of-sample likelihood of individual\nobservations showing that the system changes significantly through time. Larger\nestimation windows lead to stable likelihood in the long run, but at the cost\nof lower likelihood in the short-term: the `optimal' fit in finance needs to be\ndefined in terms of the holding period. Lastly, we show that sparse models\noutperform full-models in that they deliver higher out of sample likelihood,\nlower realized portfolio volatility and improved portfolios' stability,\navoiding typical pitfalls of the Mean-Variance optimization.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 22:05:39 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Procacci", "Pier Francesco", ""], ["Aste", "Tomaso", ""]]}, {"id": "2103.16388", "submitter": "Matloob Khushi Dr", "authors": "Mukul Jaggi, Priyanka Mandal, Shreya Narang, Usman Naseem and Matloob\n  Khushi", "title": "Text Mining of Stocktwits Data for Predicting Stock Prices", "comments": null, "journal-ref": "Appl. Syst. Innov. 2021, 4, 13", "doi": "10.3390/asi4010013", "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stock price prediction can be made more efficient by considering the price\nfluctuations and understanding the sentiments of people. A limited number of\nmodels understand financial jargon or have labelled datasets concerning stock\nprice change. To overcome this challenge, we introduced FinALBERT, an ALBERT\nbased model trained to handle financial domain text classification tasks by\nlabelling Stocktwits text data based on stock price change. We collected\nStocktwits data for over ten years for 25 different companies, including the\nmajor five FAANG (Facebook, Amazon, Apple, Netflix, Google). These datasets\nwere labelled with three labelling techniques based on stock price changes. Our\nproposed model FinALBERT is fine-tuned with these labels to achieve optimal\nresults. We experimented with the labelled dataset by training it on\ntraditional machine learning, BERT, and FinBERT models, which helped us\nunderstand how these labels behaved with different model architectures. Our\nlabelling method competitive advantage is that it can help analyse the\nhistorical data effectively, and the mathematical function can be easily\ncustomised to predict stock movement.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 03:29:14 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Jaggi", "Mukul", ""], ["Mandal", "Priyanka", ""], ["Narang", "Shreya", ""], ["Naseem", "Usman", ""], ["Khushi", "Matloob", ""]]}]