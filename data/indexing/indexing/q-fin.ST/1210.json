[{"id": "1210.0057", "submitter": "Karol Przanowski", "authors": "Karol Przanowski and Jolanta Mamczarz", "title": "Consumer finance data generator - a new approach to Credit Scoring\n  technique comparison", "comments": "21 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to present a general idea of method comparison of Credit\nScoring techniques. Any scorecard can be made in various methods based on\nvariable transformations in the logistic regression model. To make a comparison\nand come up with the proof that one technique is better than another is a big\nchallenge due to the limited availability of data. The same conclusion cannot\nbe guaranteed when using other data from another source. The following research\nchallenge can therefore be formulated: how should the comparison be managed in\norder to get general results that are not biased by particular data? The\nsolution may be in the use of various random data generators. The data\ngenerator uses two approaches: transition matrix and scorings. Here are\npresented both: results of comparison methods and the methodology of these\ncomparison techniques creating. Before building a new model the modeler can\nundertake a comparison exercise that aims at identifying the best method in the\ncase of the particular data. Here are presented various measures of predictive\nmodel like: Gini, Delta Gini, VIF and Max p-value, emphasizing the\nmulti-criteria problem of a \"Good model\". The idea that is being suggested is\nof particular use in the model building process where there are defined complex\ncriteria trying to cover the important problems of model stability over a\nperiod of time, in order to avoid a crisis. Some arguments for choosing Logit\nor WOE approach as the best scorecard technique are presented.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2012 23:30:00 GMT"}], "update_date": "2012-10-02", "authors_parsed": [["Przanowski", "Karol", ""], ["Mamczarz", "Jolanta", ""]]}, {"id": "1210.1866", "submitter": "Matyas Barczy", "authors": "Matyas Barczy, Leif Doering, Zenghu Li, Gyula Pap", "title": "On parameter estimation for critical affine processes", "comments": "45 pages", "journal-ref": "Electronic Journal of Statistics, Vol. 7 (2013) 647-696", "doi": null, "report-no": null, "categories": "math.ST math.PR q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First we provide a simple set of sufficient conditions for the weak\nconvergence of scaled affine processes with state space $R_+ \\times R^d$. We\nspecialize our result to one-dimensional continuous state branching processes\nwith immigration. As an application, we study the asymptotic behavior of least\nsquares estimators of some parameters of a two-dimensional critical affine\ndiffusion process.\n", "versions": [{"version": "v1", "created": "Fri, 5 Oct 2012 20:53:47 GMT"}, {"version": "v2", "created": "Sat, 16 Mar 2013 14:33:29 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Barczy", "Matyas", ""], ["Doering", "Leif", ""], ["Li", "Zenghu", ""], ["Pap", "Gyula", ""]]}, {"id": "1210.2043", "submitter": "Gregor Wei{\\ss}", "authors": "Gregor Wei{\\ss} and Marcus Scheffer", "title": "Smooth Nonparametric Bernstein Vine Copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to use nonparametric Bernstein copulas as bivariate pair-copulas\nin high-dimensional vine models. The resulting smooth and nonparametric vine\ncopulas completely obviate the error-prone need for choosing the pair-copulas\nfrom parametric copula families. By means of a simulation study and an\nempirical analysis of financial market data, we show that our proposed smooth\nnonparametric vine copula model is superior to competing parametric vine models\ncalibrated via Akaike's Information Criterion.\n", "versions": [{"version": "v1", "created": "Sun, 7 Oct 2012 11:35:11 GMT"}], "update_date": "2012-10-09", "authors_parsed": [["Wei\u00df", "Gregor", ""], ["Scheffer", "Marcus", ""]]}, {"id": "1210.5859", "submitter": "Ertugrul Bayraktar", "authors": "Ertugrul Bayraktar, Ayse Humeyra Bilge", "title": "Determination the Parameters of Markowitz Portfolio Optimization Model", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM cs.CE q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this study is the determination of the optimal length of\nthe historical data for the estimation of statistical parameters in Markowitz\nPortfolio Optimization. We present a trading simulation using Markowitz method,\nfor a portfolio consisting of foreign currency exchange rates and selected\nassets from the Istanbul Stock Exchange ISE 30, over the period 2001-2009. In\nthe simulation, the expected returns and the covariance matrix are computed\nfrom historical data observed for past n days and the target returns are chosen\nas multiples of the return of the market index. The trading strategy is to buy\na stock if the simulation resulted in a feasible solution and sell the stock\nafter exactly m days, independently from the market conditions. The actual\nreturns are computed for n and m being equal to 21, 42, 63, 84 and 105 days and\nwe have seen that the best return is obtained when the observation period is 2\nor 3 times the investment period.\n", "versions": [{"version": "v1", "created": "Mon, 22 Oct 2012 10:28:48 GMT"}], "update_date": "2012-10-23", "authors_parsed": [["Bayraktar", "Ertugrul", ""], ["Bilge", "Ayse Humeyra", ""]]}, {"id": "1210.6321", "submitter": "Ryohei Hisano", "authors": "Ryohei Hisano, Didier Sornette, Takayuki Mizuno, Takaaki Ohnishi,\n  Tsutomu Watanabe", "title": "High quality topic extraction from business news explains abnormal\n  financial market volatility", "comments": "The previous version of this article included an error. This is a\n  revised version", "journal-ref": null, "doi": "10.1371/journal.pone.0064846", "report-no": null, "categories": "stat.ML cs.LG cs.SI physics.soc-ph q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the mutual relationships between information flows and social\nactivity in society today is one of the cornerstones of the social sciences. In\nfinancial economics, the key issue in this regard is understanding and\nquantifying how news of all possible types (geopolitical, environmental,\nsocial, financial, economic, etc.) affect trading and the pricing of firms in\norganized stock markets. In this article, we seek to address this issue by\nperforming an analysis of more than 24 million news records provided by\nThompson Reuters and of their relationship with trading activity for 206 major\nstocks in the S&P US stock index. We show that the whole landscape of news that\naffect stock price movements can be automatically summarized via simple\nregularized regressions between trading activity and news information pieces\ndecomposed, with the help of simple topic modeling techniques, into their\n\"thematic\" features. Using these methods, we are able to estimate and quantify\nthe impacts of news on trading. We introduce network-based visualization\ntechniques to represent the whole landscape of news information associated with\na basket of stocks. The examination of the words that are representative of the\ntopic distributions confirms that our method is able to extract the significant\npieces of information influencing the stock market. Our results show that one\nof the most puzzling stylized fact in financial economies, namely that at\ncertain times trading volumes appear to be \"abnormally large,\" can be partially\nexplained by the flow of news. In this sense, our results prove that there is\nno \"excess trading,\" when restricting to times when news are genuinely novel\nand provide relevant financial information.\n", "versions": [{"version": "v1", "created": "Tue, 23 Oct 2012 18:31:46 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2012 17:16:47 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2013 10:29:11 GMT"}, {"version": "v4", "created": "Sat, 23 Mar 2013 14:34:36 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Hisano", "Ryohei", ""], ["Sornette", "Didier", ""], ["Mizuno", "Takayuki", ""], ["Ohnishi", "Takaaki", ""], ["Watanabe", "Tsutomu", ""]]}, {"id": "1210.7215", "submitter": "Gareth Peters Dr", "authors": "Kylie-Anne Richards, Gareth W. Peters, William Dunsmuir", "title": "Heavy-Tailed Features and Empirical Analysis of the Limit Order Book\n  Volume Profiles in Futures Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.TR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper poses a few fundamental questions regarding the attributes of the\nvolume profile of a Limit Order Books stochastic structure by taking into\nconsideration aspects of intraday and interday statistical features, the impact\nof different exchange features and the impact of market participants in\ndifferent asset sectors. This paper aims to address the following questions:\n  1. Is there statistical evidence that heavy-tailed sub-exponential volume\nprofiles occur at different levels of the Limit Order Book on the bid and ask\nand if so does this happen on intra or interday time scales ?\n  2.In futures exchanges, are heavy tail features exchange (CBOT, CME, EUREX,\nSGX and COMEX) or asset class (government bonds, equities and precious metals)\ndependent and do they happen on ultra-high (<1sec) or mid-range (1sec -10min)\nhigh frequency data?\n  3.Does the presence of stochastic heavy-tailed volume profile features evolve\nin a manner that would inform or be indicative of market participant behaviors,\nsuch as high frequency algorithmic trading, quote stuffing and price discovery\nintra-daily?\n  4. Is there statistical evidence for a need to consider dynamic behavior of\nthe parameters of models for Limit Order Book volume profiles on an intra-daily\ntime scale ?\n  Progress on aspects of each question is obtained via statistically rigorous\nresults to verify the empirical findings for an unprecedentedly large set of\nfutures market LOB data. The data comprises several exchanges, several futures\nasset classes and all trading days of 2010, using market depth (Type II) order\nbook data to 5 levels on the bid and ask.\n", "versions": [{"version": "v1", "created": "Fri, 26 Oct 2012 19:05:49 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 08:46:08 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Richards", "Kylie-Anne", ""], ["Peters", "Gareth W.", ""], ["Dunsmuir", "William", ""]]}, {"id": "1210.7608", "submitter": "Olivier Gu\\'eant", "authors": "Olivier Gu\\'eant", "title": "Execution and block trade pricing with optimal constant rate of\n  participation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When executing their orders, investors are proposed different strategies by\nbrokers and investment banks. Most orders are executed using VWAP algorithms.\nOther basic execution strategies include POV (also called PVol) -- for\npercentage of volume --, IS -- implementation shortfall -- or Target Close. In\nthis article dedicated to POV strategies, we develop a liquidation model in\nwhich a trader is constrained to liquidate a portfolio with a constant\nparticipation rate to the market. Considering the functional forms commonly\nused by practitioners for market impact functions, we obtain a closed-form\nexpression for the optimal participation rate. Also, we develop a microfounded\nrisk-liquidity premium that permits to better assess the costs and risks of\nexecution processes and to give a price to a large block of shares. We also\nprovide a thorough comparison between IS strategies and POV strategies in terms\nof risk-liquidity premium.\n", "versions": [{"version": "v1", "created": "Mon, 29 Oct 2012 10:13:25 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2013 21:04:08 GMT"}, {"version": "v3", "created": "Tue, 3 Dec 2013 16:37:05 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Gu\u00e9ant", "Olivier", ""]]}, {"id": "1210.8380", "submitter": "Thomas Bury J", "authors": "Thomas Bury", "title": "Market structure explained by pairwise interactions", "comments": "14 pages, 14 figures", "journal-ref": null, "doi": "10.1016/j.physa.2012.10.046", "report-no": null, "categories": "q-fin.ST cond-mat.dis-nn cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial markets are a typical example of complex systems where interactions\nbetween constituents lead to many remarkable features. Here, we show that a\npairwise maximum entropy model (or auto-logistic model) is able to describe\nswitches between ordered (strongly correlated) and disordered market states. In\nthis framework, the influence matrix may be thought as a dissimilarity measure\nand we explain how it can be used to study market structure. We make the link\nwith the graph-theoretic description of stock markets reproducing the\nnon-random and scale-free topology, shrinking length during crashes and\nmeaningful clustering features as expected. The pairwise model provides an\nalternative method to study financial networks which may be useful for\ncharacterization of abnormal market states (crises and bubbles), in capital\nallocation or for the design of regulation rules.\n", "versions": [{"version": "v1", "created": "Wed, 31 Oct 2012 16:34:13 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2012 16:32:28 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2013 12:56:32 GMT"}, {"version": "v4", "created": "Mon, 27 Jan 2014 18:34:14 GMT"}], "update_date": "2014-01-28", "authors_parsed": [["Bury", "Thomas", ""]]}]