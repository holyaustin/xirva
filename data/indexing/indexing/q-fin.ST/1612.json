[{"id": "1612.01232", "submitter": "Yuta Koike", "authors": "Takaki Hayashi, Yuta Koike", "title": "Wavelet-based methods for high-frequency lead-lag analysis", "comments": "37 pages, 2 figures. To appear in SIAM Journal on Financial\n  Mathematics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework to investigate lead-lag relationships between\ntwo financial assets. Our framework bridges a gap between continuous-time\nmodeling based on Brownian motion and the existing wavelet methods for lead-lag\nanalysis based on discrete-time models and enables us to analyze the\nmulti-scale structure of lead-lag effects. We also present a statistical\nmethodology for the scale-by-scale analysis of lead-lag effects in the proposed\nframework and develop an asymptotic theory applicable to a situation including\nstochastic volatilities and irregular sampling. Finally, we report several\nnumerical experiments to demonstrate how our framework works in practice.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 03:01:56 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 13:35:43 GMT"}, {"version": "v3", "created": "Sat, 10 Nov 2018 05:31:07 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Hayashi", "Takaki", ""], ["Koike", "Yuta", ""]]}, {"id": "1612.02567", "submitter": "Peter Antony Bebbington", "authors": "Peter A. Bebbington and Julius Bonart", "title": "Order statistics of horse racing and the randomly broken stick", "comments": "6 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We find a remarkable agreement between the statistics of a randomly divided\ninterval and the observed statistical patterns and distributions found in horse\nracing betting markets. We compare the distribution of implied winning odds,\nthe average true winning probabilities, the implied odds conditional on a win,\nand the average implied odds of the winning horse with the corresponding\nquantities from the \"randomly broken stick problem\". We observe that the market\nis at least to some degree informationally efficient. From the mapping between\nexponential random variables and the statistics of the random division we\nconclude that horses' true winning abilities are exponentially distributed.\n", "versions": [{"version": "v1", "created": "Thu, 8 Dec 2016 09:07:35 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Bebbington", "Peter A.", ""], ["Bonart", "Julius", ""]]}, {"id": "1612.02666", "submitter": "Barack Wanjawa Mr.", "authors": "Barack Wamkaya Wanjawa", "title": "Evaluating the Performance of ANN Prediction System at Shanghai Stock\n  Market in the Period 21-Sep-2016 to 11-Oct-2016", "comments": "13 pages, 4 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research evaluates the performance of an Artificial Neural Network based\nprediction system that was employed on the Shanghai Stock Exchange for the\nperiod 21-Sep-2016 to 11-Oct-2016. It is a follow-up to a previous paper in\nwhich the prices were predicted and published before September 21. Stock market\nprice prediction remains an important quest for investors and researchers. This\nresearch used an Artificial Intelligence system, being an Artificial Neural\nNetwork that is feedforward multi-layer perceptron with error backpropagation\nfor prediction, unlike other methods such as technical, fundamental or time\nseries analysis. While these alternative methods tend to guide on trends and\nnot the exact likely prices, neural networks on the other hand have the ability\nto predict the real value prices, as was done on this research. Nonetheless,\ndetermination of suitable network parameters remains a challenge in neural\nnetwork design, with this research settling on a configuration of 5:21:21:1\nwith 80% training data or 4-year of training data as a good enough model for\nstock prediction, as already determined in a previous research by the author.\nThe comparative results indicate that neural network can predict typical stock\nmarket prices with mean absolute percentage errors that are as low as 1.95%\nover the ten prediction instances that was studied in this research.\n", "versions": [{"version": "v1", "created": "Mon, 5 Dec 2016 20:25:29 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Wanjawa", "Barack Wamkaya", ""]]}, {"id": "1612.04370", "submitter": "Constantinos Siettos", "authors": "Panagiotis Papaioannou, Thomas Dionysopoulos, Dietmar Janetzko,\n  Constantinos Siettos", "title": "S&P500 Forecasting and Trading using Convolution Analysis of Major Asset\n  Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By monitoring the time evolution of the most liquid Futures contracts traded\nglobally as acquired using the Bloomberg API from 03 January 2000 until 15\nDecember 2014 we were able to forecast the S&P 500 index beating the Buy and\nHold trading strategy. Our approach is based on convolution computations of 42\nof the most liquid Futures contracts of four basic financial asset classes,\nnamely, equities, bonds, commodities and foreign exchange. These key assets\nwere selected on the basis of the global GDP ranking across countries worldwide\naccording to the lists published by the International Monetary Fund (IMF,\nReport for Selected Country Groups and Subjects, 2015). The main hypothesis is\nthat the shifts between the asset classes are smooth and are shaped by slow\ndynamics as trading decisions are shaped by several constraints associated with\nthe portfolios allocation, as well as rules restrictions imposed by state\nfinancial authorities. This hypothesis is grounded on recent research based on\nthe added value generated by diversification targets of market participants\nspecialized on active asset management, who try to efficiently and smoothly\nnavigate the market's volatility.\n", "versions": [{"version": "v1", "created": "Tue, 13 Dec 2016 14:22:13 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Papaioannou", "Panagiotis", ""], ["Dionysopoulos", "Thomas", ""], ["Janetzko", "Dietmar", ""], ["Siettos", "Constantinos", ""]]}, {"id": "1612.04507", "submitter": "Jose Figueroa-Lopez", "authors": "Jos\\'e E. Figueroa-L\\'opez and Cheng Li", "title": "Optimal Kernel Estimation of Spot Volatility of Stochastic Differential\n  Equations", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel Estimation is one of the most widely used estimation methods in\nnon-parametric Statistics, having a wide-range of applications, including spot\nvolatility estimation of stochastic processes. The selection of bandwidth and\nkernel function is of great importance, especially for the finite sample\nsettings commonly encountered in econometric applications. In the context of\nspot volatility estimation, most of the proposed selection methods are either\nlargely heuristic or just formally stated without any feasible implementation.\nIn this work, an objective method of bandwidth and kernel selection is\nproposed, under some mild conditions on the volatility, which not only cover\nclassical Brownian motion driven dynamics but also some processes driven by\nlong-memory fractional Brownian motions or other Gaussian processes. We\ncharacterize the leading order terms of the Mean Squared Error, which are also\nratified by central limit theorems for the estimation error. As a byproduct, an\napproximated optimal bandwidth is then obtained in closed form. This result\nallows us to develop a feasible plug-in type bandwidth selection procedure, for\nwhich, as a sub-problem, we propose a new estimator of the volatility of\nvolatility. The optimal selection of kernel function is also discussed. For\nBrownian Motion type volatilities, the optimal kernel function is proved to be\nthe exponential kernel. For fractional Brownian motion type volatilities,\nnumerical results to compute the optimal kernel are devised and, for the\ndeterministic volatility case, explicit optimal kernel functions of different\norders are derived. Simulation studies further confirm the good performance of\nthe proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 14 Dec 2016 06:25:51 GMT"}], "update_date": "2016-12-15", "authors_parsed": [["Figueroa-L\u00f3pez", "Jos\u00e9 E.", ""], ["Li", "Cheng", ""]]}, {"id": "1612.04990", "submitter": "Olivier Scaillet", "authors": "Patrick Gagliardini, Elisa Ossola and Olivier Scaillet", "title": "A diagnostic criterion for approximate factor structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build a simple diagnostic criterion for approximate factor structure in\nlarge cross-sectional equity datasets. Given a model for asset returns with\nobservable factors, the criterion checks whether the error terms are weakly\ncross-sectionally correlated or share at least one unobservable common factor.\nIt only requires computing the largest eigenvalue of the empirical\ncross-sectional covariance matrix of the residuals of a large unbalanced panel.\nA general version of this criterion allows us to determine the number of\nomitted common factors. The panel data model accommodates both time-invariant\nand time-varying factor structures. The theory applies to random coefficient\npanel models with interactive fixed effects under large cross-section and\ntime-series dimensions. The empirical analysis runs on monthly and quarterly\nreturns for about ten thousand US stocks from January 1968 to December 2011 for\nseveral time-invariant and time-varying specifications. For monthly returns, we\ncan choose either among time-invariant specifications with at least four\nfinancial factors, or a scaled three-factor specification. For quarterly\nreturns, we cannot select macroeconomic models without the market factor.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 09:22:05 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 13:18:25 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Gagliardini", "Patrick", ""], ["Ossola", "Elisa", ""], ["Scaillet", "Olivier", ""]]}, {"id": "1612.05072", "submitter": "Olivier Scaillet", "authors": "Lorenzo Camponovo, Olivier Scaillet and Fabio Trojani", "title": "Predictability Hidden by Anomalous Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing procedures for predictive regressions with lagged autoregressive\nvariables imply a suboptimal inference in presence of small violations of ideal\nassumptions. We propose a novel testing framework resistant to such violations,\nwhich is consistent with nearly integrated regressors and applicable to\nmulti-predictor settings, when the data may only approximately follow a\npredictive regression model. The Monte Carlo evidence demonstrates large\nimprovements of our approach, while the empirical analysis produces a strong\nrobust evidence of market return predictability hidden by anomalous\nobservations, both in- and out-of-sample, using predictive variables such as\nthe dividend yield or the volatility risk premium.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 14:12:17 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Camponovo", "Lorenzo", ""], ["Scaillet", "Olivier", ""], ["Trojani", "Fabio", ""]]}, {"id": "1612.05229", "submitter": "Patrick Laurie Davies Mr", "authors": "Laurie Davies, Walter Kr\\\"amer", "title": "Stylized Facts and Simulating Long Range Financial Data", "comments": "24 pages 12 figures, Discussion papers SFB 823, Technische\n  Universit\\\"at Dortmund, Germany 2015 48/15", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method (implemented in an R-program) to simulate long-range\ndaily stock-price data. The program reproduces various stylized facts much\nbetter than various parametric models from the extended GARCH-family. In\nparticular, the empirically observed changes in unconditional variance are\ntruthfully mirrored in the simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Dec 2016 20:38:21 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Davies", "Laurie", ""], ["Kr\u00e4mer", "Walter", ""]]}, {"id": "1612.07802", "submitter": "Michele Caraglio", "authors": "Michele Caraglio, Fulvio Baldovin and Attilio L. Stella", "title": "How fast does the clock of Finance run? - A time-definition enforcing\n  scale invariance and quantifying overnights", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cond-mat.stat-mech physics.data-an q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A symmetry-guided definition of time may enhance and simplify the analysis of\nhistorical series with recurrent patterns and seasonalities. By enforcing\nsimple-scaling and stationarity of the distributions of returns, we identify a\nsuccessful protocol of time definition in Finance. The essential structure of\nthe stochastic process underlying the series can thus be analyzed within a most\nparsimonious symmetry scheme in which multiscaling is reduced in the quest of a\ntime scale additive and independent of moment-order in the distribution of\nreturns. At the same time, duration of periods in which markets remain inactive\nare properly quantified by the novel clock, and the corresponding (e.g.,\novernight) returns are consistently taken into account for financial\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 22 Dec 2016 11:42:53 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 07:10:04 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Caraglio", "Michele", ""], ["Baldovin", "Fulvio", ""], ["Stella", "Attilio L.", ""]]}, {"id": "1612.07903", "submitter": "Vasily E. Tarasov", "authors": "Vasily E. Tarasov, Valentina V. Tarasova", "title": "Long and Short Memory in Economics: Fractional-Order Difference and\n  Differentiation", "comments": "6 pages, PDF", "journal-ref": "IRA-International Journal of Management and Social Sciences. 2016.\n  Vol.5. No.2. P.327-334", "doi": "10.21013/jmss.v5.n2.p10", "report-no": null, "categories": "q-fin.EC q-fin.MF q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long and short memory in economic processes is usually described by the\nso-called discrete fractional differencing and fractional integration. We prove\nthat the discrete fractional differencing and integration are the\nGrunwald-Letnikov fractional differences of non-integer order d. Equations of\nARIMA(p,d,q) and ARFIMA(p,d,q) models are the fractional-order difference\nequations with the Grunwald-Letnikov differences of order d. We prove that the\nlong and short memory with power law should be described by the exact\nfractional-order differences, for which the Fourier transform demonstrates the\npower law exactly. The fractional differencing and the Grunwald-Letnikov\nfractional differences cannot give exact results for the long and short memory\nwith power law, since the Fourier transform of these discrete operators satisfy\nthe power law in the neighborhood of zero only. We prove that the economic\nprocesses with the continuous time long and short memory, which is\ncharacterized by the power law, should be described by the fractional\ndifferential equations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Dec 2016 08:37:05 GMT"}, {"version": "v2", "created": "Sat, 22 Jul 2017 07:01:50 GMT"}, {"version": "v3", "created": "Sun, 6 Aug 2017 20:21:38 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Tarasov", "Vasily E.", ""], ["Tarasova", "Valentina V.", ""]]}, {"id": "1612.08705", "submitter": "Sabiou Inoua", "authors": "Sabiou Inoua", "title": "Speculation and Power Law", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now well established empirically that financial price changes are\ndistributed according to a power law, with cubic exponent. This is a\nfascinating regularity, as it holds for various classes of securities, on\nvarious markets, and on various time scales. The universality of this law\nsuggests that there must be some basic, general and stable mechanism behind it.\nThe standard (neoclassical) paradigm implies no such mechanism. Agent-based\nmodels of financial markets, on the other hand, exhibit realistic price\nchanges, but they involve relatively complicated, and often mathematically\nintractable, mechanisms. This paper identifies a simple principle behind the\npower law: the feedback intrinsic to the very idea of speculation, namely\nbuying when one expects a price rise (and selling when one expects a price\nfall). By this feedback, price changes follow a random coefficient\nautoregressive process, and therefore they have a power law by Kesten theorem.\n", "versions": [{"version": "v1", "created": "Tue, 27 Dec 2016 18:57:04 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Inoua", "Sabiou", ""]]}, {"id": "1612.09189", "submitter": "Andrey Korotayev", "authors": "Askar Akaev and Andrey Korotayev", "title": "Global economic dynamics of the forthcoming years. A forecast", "comments": "34 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper analyzes the current state of the world economy and offers a\nshort-term forecast of its development. Our analysis of log-periodic\noscillations in the DJIA dynamics suggests that in the second half of 2017 the\nUnited States and other more developed countries could experience a new\nrecession, due to the third phase of the global financial crisis. The economies\nof developing countries will continue their slowdown due to lower prices of raw\ncommodities and the increased pressure of dollar debt load. The bottom of the\nslowdown in global economic growth is likely to be achieved in 2017-2018. Then\nwe expect the start of a new acceleration of global economic growth at the\nupswing phase of the 6th Kondratieff cycle (2018-2050). A speedy and steady\nwithdrawal from the third phase of the global financial crisis requires\ncooperative action between developed and developing countries within G20 to\nstimulate global demand, world trade and a fair solution of the debt problem of\ndeveloping countries.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 16:15:32 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Akaev", "Askar", ""], ["Korotayev", "Andrey", ""]]}, {"id": "1612.09344", "submitter": "Sabiou Inoua", "authors": "Sabiou Inoua", "title": "The Random Walk behind Volatility Clustering", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial price changes obey two universal properties: they follow a power\nlaw and they tend to be clustered in time. The second regularity, known as\nvolatility clustering, entails some predictability in the price changes: while\ntheir sign is uncorrelated in time, their amplitude (or volatility) is\nlong-range correlated. Many models have been proposed to account for these\nregularities, notably agent-based models; but these models often invoke\nrelatively complicated mechanisms. This paper identifies a basic reason behind\nvolatility clustering: the impact of exogenous news on expectations. Indeed the\nexpectations of financial agents clearly vary with the advent of news; the\nsimplest way of modeling this idea is to assume the expectations follow a\nrandom walk. We show that this random walk implies volatility clustering in a\ngeneric way.\n", "versions": [{"version": "v1", "created": "Thu, 29 Dec 2016 23:15:26 GMT"}], "update_date": "2017-01-02", "authors_parsed": [["Inoua", "Sabiou", ""]]}]