[{"id": "2104.00262", "submitter": "Michael Grabinski", "authors": "Maike Torm\\\"ahlen, Galiya Klinkova and Michael Grabinski", "title": "Statistical significance revisited", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": "10.20944/preprints202103.0398.v1", "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical significance measures the reliability of a result obtained from a\nrandom experiment. We investigate the number of repetitions needed for a\nstatistical result to have a certain significance. In the first step, we\nconsider binomially distributed variables in the example of medication testing\nwith fixed placebo efficacy, asking how many experiments are needed in order to\nachieve a significance of 95 %. In the next step, we take the probability\ndistribution of the placebo efficacy into account, which to the best of our\nknowledge has not been done so far. Depending on the specifics, we show that in\norder to obtain identical significance, it may be necessary to perform twice as\nmany experiments than in a setting where the placebo distribution is neglected.\nWe proceed by considering more general probability distributions and close with\ncomments on some erroneous assumptions on probability distributions which lead,\nfor instance, to a trivial explanation of the fat tail.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 05:38:46 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 08:35:20 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Torm\u00e4hlen", "Maike", ""], ["Klinkova", "Galiya", ""], ["Grabinski", "Michael", ""]]}, {"id": "2104.03053", "submitter": "Maksim Malyy", "authors": "Maksim Malyy (1), Zeljko Tekic (1 and 2), Tatiana Podladchikova (1)\n  ((1) Skolkovo Institute of Science and Technology, (2) HSE University,\n  Graduate School of Business)", "title": "The value of big data for analyzing growth dynamics of technology based\n  new ventures", "comments": "38 pages, 10 figures, 9 tables, to be published in Technological\n  Forecasting & Social Change journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.GN q-fin.PR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This study demonstrates that web-search traffic information, in particular,\nGoogle Trends data, is a credible novel source of high-quality and\neasy-to-access data for analyzing technology-based new ventures (TBNVs) growth\ntrajectories. Utilizing the diverse sample of 241 US-based TBNVs, we\ncomparatively analyze the relationship between companies' evolution curves\nrepresented by search activity on the one hand and by valuations achieved\nthrough rounds of venture investments on another. The results suggest that\nTBNV's growth dynamics are positively and strongly correlated with its web\nsearch traffic across the sample. This correlation is more robust when a\ncompany is a) more successful (in terms of valuation achieved) - especially if\nit is a \"unicorn\"; b) consumer-oriented (i.e., b2c); and 3) develops products\nin the form of a digital platform. Further analysis based on fuzzy-set\nQualitative Comparative Analysis (fsQCA) shows that for the most successful\ncompanies (\"unicorns\") and consumer-oriented digital platforms (i.e., b2c\ndigital platform companies) proposed approach may be extremely reliable, while\nfor other high-growth TBNVs it is useful for analyzing their growth dynamics,\nalbeit to a more limited degree. The proposed methodological approach opens a\nwide range of possibilities for analyzing, researching and predicting the\ngrowth of recently formed growth-oriented companies, in practice and academia.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:04:40 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Malyy", "Maksim", "", "1 and 2"], ["Tekic", "Zeljko", "", "1 and 2"], ["Podladchikova", "Tatiana", ""]]}, {"id": "2104.03667", "submitter": "Andrea Bucci", "authors": "Andrea Bucci and Vito Ciciretti", "title": "Market Regime Detection via Realized Covariances: A Comparison between\n  Unsupervised Learning and Nonlinear Models", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is broad empirical evidence of regime switching in financial markets.\nThe transition between different market regimes is mirrored in correlation\nmatrices, whose time-varying coefficients usually jump higher in highly\nvolatile regimes, leading to the failure of common diversification methods. In\nthis article, we aim to identify market regimes from covariance matrices and\ndetect transitions towards highly volatile regimes, hence improving tail-risk\nhedging. Starting from the time series of fractionally differentiated\nsentiment-like future values, two models are applied on monthly realized\ncovariance matrices to detect market regimes. Specifically, the regime\ndetection is implemented via vector logistic smooth transition autoregressive\nmodel (VLSTAR) and through an unsupervised learning methodology, the\nagglomerative hierarchical clustering. Since market regime switches are\nunobservable processes that describe the latent change of market behaviour, the\nability of correctly detecting market regimes is validated in two ways:\nfirstly, randomly generated data are used to assess a correct classification\nwhen regimes are known; secondly, a na\\\"{i}ve trading strategy filtered with\nthe detected regime switches is used to understand whether an improvement is\nshowed when accounting for regime switches. The results point to the VLSTAR as\nthe best performing model for labelling market regimes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 10:32:28 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Bucci", "Andrea", ""], ["Ciciretti", "Vito", ""]]}, {"id": "2104.04036", "submitter": "Matias Selser", "authors": "Matias Selser, Javier Kreiner, Manuel Maurette", "title": "Optimal Market Making by Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG q-fin.ST", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We apply Reinforcement Learning algorithms to solve the classic quantitative\nfinance Market Making problem, in which an agent provides liquidity to the\nmarket by placing buy and sell orders while maximizing a utility function. The\noptimal agent has to find a delicate balance between the price risk of her\ninventory and the profits obtained by capturing the bid-ask spread. We design\nan environment with a reward function that determines an order relation between\npolicies equivalent to the original utility function. When comparing our agents\nwith the optimal solution and a benchmark symmetric agent, we find that the\nDeep Q-Learning algorithm manages to recover the optimal agent.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:13:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Selser", "Matias", ""], ["Kreiner", "Javier", ""], ["Maurette", "Manuel", ""]]}, {"id": "2104.04041", "submitter": "Jia Wang", "authors": "Jia Wang, Tong Sun, Benyuan Liu, Yu Cao, Hongwei Zhu", "title": "CLVSA: A Convolutional LSTM Based Variational Sequence-to-Sequence Model\n  with Attention for Predicting Trends of Financial Markets", "comments": "7 pages, Proceedings of the Twenty-Eighth International Joint\n  Conference on Artificial Intelligence (IJCAI-19)", "journal-ref": null, "doi": "10.24963/ijcai.2019/514", "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Financial markets are a complex dynamical system. The complexity comes from\nthe interaction between a market and its participants, in other words, the\nintegrated outcome of activities of the entire participants determines the\nmarkets trend, while the markets trend affects activities of participants.\nThese interwoven interactions make financial markets keep evolving. Inspired by\nstochastic recurrent models that successfully capture variability observed in\nnatural sequential data such as speech and video, we propose CLVSA, a hybrid\nmodel that consists of stochastic recurrent networks, the sequence-to-sequence\narchitecture, the self- and inter-attention mechanism, and convolutional LSTM\nunits to capture variationally underlying features in raw financial trading\ndata. Our model outperforms basic models, such as convolutional neural network,\nvanilla LSTM network, and sequence-to-sequence model with attention, based on\nbacktesting results of six futures from January 2010 to December 2017. Our\nexperimental results show that, by introducing an approximate posterior, CLVSA\ntakes advantage of an extra regularizer based on the Kullback-Leibler\ndivergence to prevent itself from overfitting traps.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:31:04 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Wang", "Jia", ""], ["Sun", "Tong", ""], ["Liu", "Benyuan", ""], ["Cao", "Yu", ""], ["Zhu", "Hongwei", ""]]}, {"id": "2104.05204", "submitter": "Tianxiang Zhan", "authors": "Tianxiang Zhan, Fuyuan Xiao", "title": "A Fast Evidential Approach for Stock Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the framework of evidence theory, the confidence functions of\ndifferent information can be combined into a combined confidence function to\nsolve uncertain problems. The Dempster combination rule is a classic method of\nfusing different information. This paper proposes a similar confidence function\nfor the time point in the time series. The Dempster combination rule can be\nused to fuse the growth rate of the last time point, and finally a relatively\naccurate forecast data can be obtained. Stock price forecasting is a concern of\neconomics. The stock price data is large in volume, and more accurate forecasts\nare required at the same time. The classic methods of time series, such as\nARIMA, cannot balance forecasting efficiency and forecasting accuracy at the\nsame time. In this paper, the fusion method of evidence theory is applied to\nstock price prediction. Evidence theory deals with the uncertainty of stock\nprice prediction and improves the accuracy of prediction. At the same time, the\nfusion method of evidence theory has low time complexity and fast prediction\nprocessing speed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 04:58:51 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 08:49:12 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhan", "Tianxiang", ""], ["Xiao", "Fuyuan", ""]]}, {"id": "2104.05413", "submitter": "Jia Wang", "authors": "Jia Wang, Tong Sun, Benyuan Liu, Yu Cao, Degang Wang", "title": "Financial Markets Prediction with Deep Learning", "comments": "8 pages, 2018 17th IEEE International Conference on Machine Learning\n  and Applications (ICMLA). IEEE, 2018", "journal-ref": null, "doi": "10.1109/ICMLA.2018.00022", "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Financial markets are difficult to predict due to its complex systems\ndynamics. Although there have been some recent studies that use machine\nlearning techniques for financial markets prediction, they do not offer\nsatisfactory performance on financial returns. We propose a novel\none-dimensional convolutional neural networks (CNN) model to predict financial\nmarket movement. The customized one-dimensional convolutional layers scan\nfinancial trading data through time, while different types of data, such as\nprices and volume, share parameters (kernels) with each other. Our model\nautomatically extracts features instead of using traditional technical\nindicators and thus can avoid biases caused by selection of technical\nindicators and pre-defined coefficients in technical indicators. We evaluate\nthe performance of our prediction model with strictly backtesting on historical\ntrading data of six futures from January 2010 to October 2017. The experiment\nresults show that our CNN model can effectively extract more generalized and\ninformative features than traditional technical indicators, and achieves more\nrobust and profitable financial performance than previous machine learning\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 19:36:48 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wang", "Jia", ""], ["Sun", "Tong", ""], ["Liu", "Benyuan", ""], ["Cao", "Yu", ""], ["Wang", "Degang", ""]]}, {"id": "2104.06044", "submitter": "Giulia Terenzi", "authors": "Andrea Giuseppe Di Iura, Giulia Terenzi", "title": "A Bayesian analysis of gain-loss asymmetry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a quantitative analysis of the gain/loss asymmetry for financial\ntime series by using a Bayesian approach. In particular, we focus on some\nselected indices and analyze the statistical significance of the asymmetry\namount through a Bayesian generalization of the t-Test, which relaxes the\nnormality assumption on the underlying distribution. We propose two different\nmodels for data distribution, we study the convergence of our method and we\nprovide several graphical representations of our numerical results. Finally, we\nperform a sensitivity analysis with respect to model parameters in order to\nstudy the reliability and robustness of our results.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:20:31 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Di Iura", "Andrea Giuseppe", ""], ["Terenzi", "Giulia", ""]]}, {"id": "2104.06254", "submitter": "Jone Ascorbebeitia Bilbatua", "authors": "E. Ferreira (1), S.Orbe (1), J. Ascorbebeitia (2), B. \\'Alvarez\n  Pereira (3), E. Estrada (4) ((1) Department of Quantitative Methods,\n  University of the Basque Country UPV/EHU, (2) Department of Economic\n  Analysis, University of the Basque Country UPV/EHU, (3) Nova School of\n  Business and Economics (Nova SBE), NOVAFRICA, and BELAB, (4) Institute of\n  Mathematics and Applications, University of Zaragoza, ARAID Foundation.\n  Institute for Cross-Disciplinary Physics and Complex Systems (IFISC,\n  UIB-CSIC), Campus Universitat de les Illes Balears)", "title": "Loss of structural balance in stock markets", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use rank correlations as distance functions to establish the\ninterconnectivity between stock returns, building weighted signed networks for\nthe stocks of seven European countries, the US and Japan. We establish the\ntheoretical relationship between the level of balance in a network and stock\npredictability, studying its evolution from 2005 to the third quarter of 2020.\nWe find a clear balance-unbalance transition for six of the nine countries,\nfollowing the August 2011 Black Monday in the US, when the Economic Policy\nUncertainty index for this country reached its highest monthly level before the\nCOVID-19 crisis. This sudden loss of balance is mainly caused by a\nreorganization of the market networks triggered by a group of low\ncapitalization stocks belonging to the non-financial sector. After the\ntransition, the stocks of companies in these groups become all negatively\ncorrelated between them and with most of the rest of the stocks in the market.\nThe implied change in the network topology is directly related to a decrease in\nstocks predictability, a finding with novel important implications for asset\nallocation and portfolio hedging strategies.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 16:20:45 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ferreira", "E.", ""], ["Orbe", "S.", ""], ["Ascorbebeitia", "J.", ""], ["Pereira", "B. \u00c1lvarez", ""], ["Estrada", "E.", ""]]}, {"id": "2104.06259", "submitter": "Jaydip Sen", "authors": "Jaydip Sen, Abhishek Dutta, Sidra Mehtab", "title": "Profitability Analysis in Stock Investment Using an LSTM-Based Deep\n  Learning Model", "comments": "This is the accepted version of our paper in the Second IEEE\n  International Conference on Emerging Technologies (IEEE INCET 2021) which\n  will be organized in Belgaum, Karnataka, INDIA from May 21 to May 23, 2021.\n  The paper is eight pages long, and has fifteen tables and fourteen figures.\n  This is not the final version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing robust systems for precise prediction of future prices of stocks\nhas always been considered a very challenging research problem. Even more\nchallenging is to build a system for constructing an optimum portfolio of\nstocks based on the forecasted future stock prices. We present a deep\nlearning-based regression model built on a long-and-short-term memory network\n(LSTM) network that automatically scraps the web and extracts historical stock\nprices based on a stock's ticker name for a specified pair of start and end\ndates, and forecasts the future stock prices. We deploy the model on 75\nsignificant stocks chosen from 15 critical sectors of the Indian stock market.\nFor each of the stocks, the model is evaluated for its forecast accuracy.\nMoreover, the predicted values of the stock prices are used as the basis for\ninvestment decisions, and the returns on the investments are computed.\nExtensive results are presented on the performance of the model. The analysis\nof the results demonstrates the efficacy and effectiveness of the system and\nenables us to compare the profitability of the sectors from the point of view\nof the investors in the stock market.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:09:51 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Sen", "Jaydip", ""], ["Dutta", "Abhishek", ""], ["Mehtab", "Sidra", ""]]}, {"id": "2104.07260", "submitter": "Christian Diem", "authors": "Christian Diem and Andr\\'as Borsos and Tobias Reisch and J\\'anos\n  Kert\\'esz and Stefan Thurner", "title": "Quantifying firm-level economic systemic risk from nation-wide supply\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.GN cs.SI physics.soc-ph q-fin.EC q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crises like COVID-19 or the Japanese earthquake in 2011 exposed the fragility\nof corporate supply networks. The production of goods and services is a highly\ninterdependent process and can be severely impacted by the default of critical\nsuppliers or customers. While knowing the impact of individual companies on\nnational economies is a prerequisite for efficient risk management, the\nquantitative assessment of the involved economic systemic risks (ESR) is\nhitherto practically non-existent, mainly because of a lack of fine-grained\ndata in combination with coherent methods. Based on a unique value added tax\ndataset we derive the detailed production network of an entire country and\npresent a novel approach for computing the ESR of all individual firms. We\ndemonstrate that a tiny fraction (0.035%) of companies has extraordinarily high\nsystemic risk impacting about 23% of the national economic production should\nany of them default. Firm size alone cannot explain the ESR of individual\ncompanies; their position in the production networks does matter substantially.\nIf companies are ranked according to their economic systemic risk index (ESRI),\nfirms with a rank above a characteristic value have very similar ESRI values,\nwhile for the rest the rank distribution of ESRI decays slowly as a power-law;\n99.8% of all companies have an impact on less than 1% of the economy. We show\nthat the assessment of ESR is impossible with aggregate data as used in\ntraditional Input-Output Economics. We discuss how simple policies of\nintroducing supply chain redundancies can reduce ESR of some extremely risky\ncompanies.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 06:18:46 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Diem", "Christian", ""], ["Borsos", "Andr\u00e1s", ""], ["Reisch", "Tobias", ""], ["Kert\u00e9sz", "J\u00e1nos", ""], ["Thurner", "Stefan", ""]]}, {"id": "2104.07469", "submitter": "Muhammad Ilyas Dr.", "authors": "Nazish Ashfaq, Zubair Nawaz, Muhammad Ilyas", "title": "A comparative study of Different Machine Learning Regressors For Stock\n  Market Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the development of successful share trading strategies, forecasting the\ncourse of action of the stock market index is important. Effective prediction\nof closing stock prices could guarantee investors attractive benefits. Machine\nlearning algorithms have the ability to process and forecast almost reliable\nclosing prices for historical stock patterns. In this article, we intensively\nstudied NASDAQ stock market and targeted to choose the portfolio of ten\ndifferent companies belongs to different sectors. The objective is to compute\nopening price of next day stock using historical data. To fulfill this task\nnine different Machine Learning regressor applied on this data and evaluated\nusing MSE and R2 as performance metric.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 15:37:33 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ashfaq", "Nazish", ""], ["Nawaz", "Zubair", ""], ["Ilyas", "Muhammad", ""]]}, {"id": "2104.07962", "submitter": "Valerio Ficcadenti", "authors": "Marcel Ausloos, Valerio Ficcadenti, Gurjeet Dhesi, Muhammad Shakeel", "title": "Benford's laws tests on S&P500 daily closing values and the\n  corresponding daily log-returns both point to huge non-conformity", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2021.125969", "report-no": null, "categories": "q-fin.ST q-fin.GN q-fin.TR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The so-called Benford's laws are of frequent use in order to observe\nanomalies and regularities in data sets, in particular, in election results and\nfinancial statements. Yet, basic financial market indices have not been much\nstudied, if studied at all, within such a perspective. This paper presents\nfeatures in the distributions of S\\&P500 daily closing values and the\ncorresponding daily log returns over a long time interval, [03/01/1950 -\n22/08/2014], amounting to 16265 data points. We address the frequencies of the\nfirst, second, and first two significant digits counts and explore the\nconformance to Benford's laws of these distributions at five different (equal\nsize) levels of disaggregation. The log returns are studied for either positive\nor negative cases. The results for the S&P500 daily closing values are showing\na huge lack of non-conformity, whatever the different levels of disaggregation.\nSome \"first digits\" and \"first two digits\" values are even missing. The causes\nof this non-conformity are discussed, pointing to the danger in taking\nBenford's laws for granted in huge databases, whence drawing \"definite\nconclusions\". The agreements with Benford's laws are much better for the log\nreturns. Such a disparity in agreements finds an explanation in the data set\nitself: the inherent trend in the index. To further validate this, daily\nreturns have been simulated calibrating the simulations with the observed data\naverages and tested against Benford's laws. One finds that not only the trend\nbut also the standard deviation of the distributions are relevant parameters in\nconcluding about conformity with Benford's laws.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 08:31:17 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Ausloos", "Marcel", ""], ["Ficcadenti", "Valerio", ""], ["Dhesi", "Gurjeet", ""], ["Shakeel", "Muhammad", ""]]}, {"id": "2104.09309", "submitter": "Juan Camilo Henao Londono", "authors": "Juan Camilo Henao Londono and Thomas Guhr", "title": "Foreign exchange markets: price response and spread impact", "comments": "12 pages, 3 figures, 2 tables. arXiv admin note: text overlap with\n  arXiv:2010.15105", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We carry out a detailed large-scale data analysis of price response functions\nin the spot foreign exchange market for different years and different time\nscales. Such response functions provide quantitative information on the\ndeviation from Markovian behavior. The price response functions show an\nincrease to a maximum followed by a slow decrease as the time lag grows, in\ntrade time scale and in physical time scale, for all analyzed years.\nFurthermore, we use a price increment point (pip) bid-ask spread definition to\ngroup different foreign exchange pairs and analyze the impact of the spread in\nthe price response functions. We find that large pip spreads have a stronger\nimpact on the response. This is similar to what has been found in stock\nmarkets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 19:37:41 GMT"}, {"version": "v2", "created": "Mon, 26 Jul 2021 11:38:59 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Londono", "Juan Camilo Henao", ""], ["Guhr", "Thomas", ""]]}, {"id": "2104.13947", "submitter": "Nathan Provost", "authors": "Nathan Thomas Provost", "title": "Modelling Net Loan Loss with Bayesian and Frequentist Regression\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We create two distinct nonlinear regression models relating net loan loss (as\nan outcome) to several other financial and sociological quantities. We consider\nthese data for the time interval between April 1st 2011 and April 1st 2020. We\nalso include temporal quantities (month and year) in our model to improve\naccuracy. One model follows the frequentist paradigm for nonlinear regression,\nwhile the other follows the Bayesian paradigm. By using the two methods, we\nobtain a rounded understanding of the relationship between net loan losses and\nour given financial, sociological, and temporal variables, improving our\nability to make financial predictions regarding the profitability of loan\nallocation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 20:46:51 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Provost", "Nathan Thomas", ""]]}, {"id": "2104.13948", "submitter": "Ekaterina Zolotareva", "authors": "Ekaterina Zolotareva", "title": "Applying Convolutional Neural Networks for Stock Market Trends\n  Identification", "comments": "22 pages, 8 figures, 6 tables. This paper is the full text of the\n  research, presented at the 20th International Conference on Artificial\n  Intelligence and Soft Computing Web System (ICAISC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we apply a specific type ANNs - convolutional neural networks\n(CNNs) - to the problem of finding start and endpoints of trends, which are the\noptimal points for entering and leaving the market. We aim to explore long-term\ntrends, which last several months, not days. The key distinction of our model\nis that its labels are fully based on expert opinion data. Despite the various\nmodels based solely on stock price data, some market experts still argue that\ntraders are able to see hidden opportunities. The labelling was done via the\nGUI interface, which means that the experts worked directly with images, not\nnumerical data. This fact makes CNN the natural choice of algorithm. The\nproposed framework requires the sequential interaction of three CNN submodels,\nwhich identify the presence of a changepoint in a window, locate it and finally\nrecognize the type of new tendency - upward, downward or flat. These submodels\nhave certain pitfalls, therefore the calibration of their hyperparameters is\nthe main direction of further research. The research addresses such issues as\nimbalanced datasets and contradicting labels, as well as the need for specific\nquality metrics to keep up with practical applicability. This paper is the full\ntext of the research, presented at the 20th International Conference on\nArtificial Intelligence and Soft Computing Web System (ICAISC 2021)\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:43:29 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Zolotareva", "Ekaterina", ""]]}, {"id": "2104.14204", "submitter": "Micha{\\l} Narajewski", "authors": "Micha{\\l} Narajewski, Florian Ziel", "title": "Optimal bidding on hourly and quarter-hourly day-ahead electricity price\n  auctions: trading large volumes of power with market impact and transaction\n  costs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.MF q-fin.PM q-fin.TR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity exchanges offer several trading possibilities for market\nparticipants: starting with futures products through the spot market consisting\nof the auction and continuous part, and ending with the balancing market. This\nvariety of choice creates a new question for traders - when to trade to\nmaximize the gain. This problem is not trivial especially for trading larger\nvolumes as the market participants should also consider their own price impact.\nThe following paper raises this issue considering two markets: the hourly EPEX\nDay-Ahead Auction and the quarter-hourly EPEX Intraday Auction. We consider a\nrealistic setting which includes a forecasting study and a suitable evaluation.\nFor a meaningful optimization many price scenarios are considered that we\nobtain using bootstrap with models that are well-known and researched in the\nelectricity price forecasting literature. The own market impact is predicted by\nmimicking the demand or supply shift in the respectful auction curves. A number\nof trading strategies is considered, e.g. minimization of the trading costs,\nrisk neutral or risk averse agents. Additionally, we provide theoretical\nresults for risk neutral agents. Especially we show when the optimal trading\npath coincides with the solution that minimizes transaction costs. The\napplication study is conducted using the German market data, but the presented\nmethods can be easily utilized with other two auction-based markets. They could\nbe also generalized to other market types, what is discussed in the paper as\nwell. The empirical results show that market participants could increase their\ngains significantly compared to simple benchmark strategies.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:52:18 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Narajewski", "Micha\u0142", ""], ["Ziel", "Florian", ""]]}, {"id": "2104.14412", "submitter": "Erniel Barrios", "authors": "Erniel B. Barrios, Paolo Victor T. Redondo", "title": "Nonparametric Test for Volatility in Clustered Multiple Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Contagion arising from clustering of multiple time series like those in the\nstock market indicators can further complicate the nature of volatility,\nrendering a parametric test (relying on asymptotic distribution) to suffer from\nissues on size and power. We propose a test on volatility based on the\nbootstrap method for multiple time series, intended to account for possible\npresence of contagion effect. While the test is fairly robust to distributional\nassumptions, it depends on the nature of volatility. The test is correctly\nsized even in cases where the time series are almost nonstationary. The test is\nalso powerful specially when the time series are stationary in mean and that\nvolatility are contained only in fewer clusters. We illustrate the method in\nglobal stock prices data.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 01:35:14 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Barrios", "Erniel B.", ""], ["Redondo", "Paolo Victor T.", ""]]}]