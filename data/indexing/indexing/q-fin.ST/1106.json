[{"id": "1106.0039", "submitter": "Mauro Politi", "authors": "Mauro Politi, Nicolas Millot, Anirban Chakraborti", "title": "The near-extreme density of intraday log-returns", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2011.05.029", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extreme event statistics plays a very important role in the theory and\npractice of time series analysis. The reassembly of classical theoretical\nresults is often undermined by non-stationarity and dependence between\nincrements. Furthermore, the convergence to the limit distributions can be\nslow, requiring a huge amount of records to obtain significant statistics, and\nthus limiting its practical applications. Focussing, instead, on the closely\nrelated density of \"near-extremes\" -- the distance between a record and the\nmaximal value -- can render the statistical methods to be more suitable in the\npractical applications and/or validations of models. We apply this recently\nproposed method in the empirical validation of an adapted financial market\nmodel of the intraday market fluctuations.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2011 21:54:57 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Politi", "Mauro", ""], ["Millot", "Nicolas", ""], ["Chakraborti", "Anirban", ""]]}, {"id": "1106.0390", "submitter": "Jaroslaw Kwapien", "authors": "Stanislaw Drozdz, Jaroslaw Kwapien, Andreas A. Ioannides", "title": "Asymmetric random matrices: What do we need them for?", "comments": null, "journal-ref": "Acta Phys. Pol. B 42, 987-999 (2011)", "doi": "10.5506/APhysPolB.42.987", "report-no": null, "categories": "physics.data-an cs.CE q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems are typically represented by large ensembles of observations.\nCorrelation matrices provide an efficient formal framework to extract\ninformation from such multivariate ensembles and identify in a quantifiable way\npatterns of activity that are reproducible with statistically significant\nfrequency compared to a reference chance probability, usually provided by\nrandom matrices as fundamental reference. The character of the problem and\nespecially the symmetries involved must guide the choice of random matrices to\nbe used for the definition of a baseline reference. For standard correlation\nmatrices this is the Wishart ensemble of symmetric random matrices. The real\nworld complexity however often shows asymmetric information flows and therefore\nmore general correlation matrices are required to adequately capture the\nasymmetry. Here we first summarize the relevant theoretical concepts. We then\npresent some examples of human brain activity where asymmetric time-lagged\ncorrelations are evident and hence highlight the need for further theoretical\ndevelopments.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2011 09:11:21 GMT"}], "update_date": "2011-06-03", "authors_parsed": [["Drozdz", "Stanislaw", ""], ["Kwapien", "Jaroslaw", ""], ["Ioannides", "Andreas A.", ""]]}, {"id": "1106.2685", "submitter": "Aleksejus Kononovicius", "authors": "Aleksejus Kononovicius and Vygintas Gontis", "title": "Agent based reasoning for the non-linear stochastic models of long-range\n  memory", "comments": "10 pages, 3 figures", "journal-ref": "Physica A 391 (2012), pp. 1309-1314", "doi": "10.1016/j.physa.2011.08.061", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Kirman's model by introducing variable event time scale. The\nproposed flexible time scale is equivalent to the variable trading activity\nobserved in financial markets. Stochastic version of the extended Kirman's\nagent based model is compared to the non-linear stochastic models of long-range\nmemory in financial markets. Agent based model providing matching macroscopic\ndescription serves as a microscopic reasoning of the earlier proposed\nstochastic model exhibiting power law statistics.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 12:10:24 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2011 06:42:04 GMT"}], "update_date": "2011-12-23", "authors_parsed": [["Kononovicius", "Aleksejus", ""], ["Gontis", "Vygintas", ""]]}, {"id": "1106.3016", "submitter": "Remy Chicheportiche", "authors": "Remy Chicheportiche and Jean-Philippe Bouchaud", "title": "Goodness-of-Fit tests with Dependent Observations", "comments": "26 pages", "journal-ref": "J. Stat. Mech. (2011) P09003", "doi": "10.1088/1742-5468/2011/09/P09003", "report-no": null, "categories": "q-fin.ST cond-mat.stat-mech stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the Kolmogorov-Smirnov and Cram\\'er-von Mises goodness-of-fit\n(GoF) tests and propose a generalisation to identically distributed, but\ndependent univariate random variables. We show that the dependence leads to a\nreduction of the \"effective\" number of independent observations. The\ngeneralised GoF tests are not distribution-free but rather depend on all the\nlagged bivariate copulas. These objects, that we call \"self-copulas\", encode\nall the non-linear temporal dependences. We introduce a specific, log-normal\nmodel for these self-copulas, for which a number of analytical results are\nderived. An application to financial time series is provided. As is well known,\nthe dependence is to be long-ranged in this case, a finding that we confirm\nusing self-copulas. As a consequence, the acceptance rates for GoF tests are\nsubstantially higher than if the returns were iid random variables.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jun 2011 16:22:48 GMT"}, {"version": "v2", "created": "Wed, 3 Aug 2011 08:24:50 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Chicheportiche", "Remy", ""], ["Bouchaud", "Jean-Philippe", ""]]}, {"id": "1106.3915", "submitter": "Song Song", "authors": "Song Song and Peter J. Bickel", "title": "Large Vector Auto Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One popular approach for nonstructural economic and financial forecasting is\nto include a large number of economic and financial variables, which has been\nshown to lead to significant improvements for forecasting, for example, by the\ndynamic factor models. A challenging issue is to determine which variables and\n(their) lags are relevant, especially when there is a mixture of serial\ncorrelation (temporal dynamics), high dimensional (spatial) dependence\nstructure and moderate sample size (relative to dimensionality and lags). To\nthis end, an \\textit{integrated} solution that addresses these three challenges\nsimultaneously is appealing. We study the large vector auto regressions here\nwith three types of estimates. We treat each variable's own lags different from\nother variables' lags, distinguish various lags over time, and is able to\nselect the variables and lags simultaneously. We first show the consequences of\nusing Lasso type estimate directly for time series without considering the\ntemporal dependence. In contrast, our proposed method can still produce an\nestimate as efficient as an \\textit{oracle} under such scenarios. The tuning\nparameters are chosen via a data driven \"rolling scheme\" method to optimize the\nforecasting performance. A macroeconomic and financial forecasting problem is\nconsidered to illustrate its superiority over existing estimators.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2011 14:24:08 GMT"}], "update_date": "2011-06-21", "authors_parsed": [["Song", "Song", ""], ["Bickel", "Peter J.", ""]]}, {"id": "1106.3921", "submitter": "Song Song", "authors": "Song Song", "title": "Dynamic Large Spatial Covariance Matrix Estimation in Application to\n  Semiparametric Model Construction via Variable Clustering: the SCE approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.RM q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To better understand the spatial structure of large panels of economic and\nfinancial time series and provide a guideline for constructing semiparametric\nmodels, this paper first considers estimating a large spatial covariance matrix\nof the generalized $m$-dependent and $\\beta$-mixing time series (with $J$\nvariables and $T$ observations) by hard thresholding regularization as long as\n${{\\log J \\, \\cx^*(\\ct)}}/{T} = \\Co(1)$ (the former scheme with some time\ndependence measure $\\cx^*(\\ct)$) or $\\log J /{T} = \\Co(1)$ (the latter scheme\nwith some upper bounded mixing coefficient). We quantify the interplay between\nthe estimators' consistency rate and the time dependence level, discuss an\nintuitive resampling scheme for threshold selection, and also prove a general\ncross-validation result justifying this. Given a consistently estimated\ncovariance (correlation) matrix, by utilizing its natural links with graphical\nmodels and semiparametrics, after \"screening\" the (explanatory) variables, we\nimplement a novel forward (and backward) label permutation procedure to cluster\nthe \"relevant\" variables and construct the corresponding semiparametric model,\nwhich is further estimated by the groupwise dimension reduction method with\nsign constraints. We call this the SCE (screen - cluster - estimate) approach\nfor modeling high dimensional data with complex spatial structure. Finally we\napply this method to study the spatial structure of large panels of economic\nand financial time series and find the proper semiparametric structure for\nestimating the consumer price index (CPI) to illustrate its superiority over\nthe linear models.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2011 14:35:52 GMT"}, {"version": "v2", "created": "Thu, 23 Jun 2011 06:08:31 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Song", "Song", ""]]}, {"id": "1106.4710", "submitter": "Gleb Oshanin", "authors": "G. Oshanin, Yu. Holovatch and G. Schehr", "title": "Proportionate vs disproportionate distribution of wealth of two\n  individuals in a tempered Paretian ensemble", "comments": "9 pages, 8 figures, to appear in Physica A", "journal-ref": "Physica A 390, 4340--4346 (2011)", "doi": "10.1016/j.physa.2011.06.067", "report-no": null, "categories": "q-fin.GN math.PR math.ST physics.data-an q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distribution P(\\omega) of the random variable \\omega = x_1/(x_1\n+ x_2), where x_1 and x_2 are the wealths of two individuals selected at random\nfrom the same tempered Paretian ensemble characterized by the distribution\n\\Psi(x) \\sim \\phi(x)/x^{1 + \\alpha}, where \\alpha > 0 is the Pareto index and\n$\\phi(x)$ is the cut-off function. We consider two forms of \\phi(x): a bounded\nfunction \\phi(x) = 1 for L \\leq x \\leq H, and zero otherwise, and a smooth\nexponential function \\phi(x) = \\exp(-L/x - x/H). In both cases \\Psi(x) has\nmoments of arbitrary order.\n  We show that, for \\alpha > 1, P(\\omega) always has a unimodal form and is\npeaked at \\omega = 1/2, so that most probably x_1 \\approx x_2. For 0 < \\alpha <\n1 we observe a more complicated behavior which depends on the value of \\delta =\nL/H. In particular, for \\delta < \\delta_c - a certain threshold value -\nP(\\omega) has a three-modal (for a bounded \\phi(x)) and a bimodal M-shape (for\nan exponential \\phi(x)) form which signifies that in such ensembles the wealths\nx_1 and x_2 are disproportionately different.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2011 13:14:55 GMT"}], "update_date": "2012-07-24", "authors_parsed": [["Oshanin", "G.", ""], ["Holovatch", "Yu.", ""], ["Schehr", "G.", ""]]}, {"id": "1106.4957", "submitter": "Rosario Bartiromo Dr", "authors": "Rosario Bartiromo", "title": "Maximum entropy distribution of stock price fluctuations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The principle of absence of arbitrage opportunities allows obtaining the\ndistribution of stock price fluctuations by maximizing its information entropy.\nThis leads to a physical description of the underlying dynamics as a random\nwalk characterized by a stochastic diffusion coefficient and constrained to a\ngiven value of the expected volatility, taking in this way into account the\ninformation provided by the existence of an option market. This model is\nvalidated by a comprehensive comparison with observed distributions of both\nprice return and diffusion coefficient. Expected volatility is the only\nparameter in the model and can be obtained by analysing option prices. We give\nan analytic formulation of the probability density function for price returns\nwhich can be used to extract expected volatility from stock option data. This\ndistribution is of high practical interest since it should be preferred to a\nGaussian when dealing with the problem of pricing derivative financial\ncontracts.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 12:55:42 GMT"}, {"version": "v2", "created": "Fri, 11 May 2012 15:32:58 GMT"}, {"version": "v3", "created": "Mon, 14 May 2012 13:02:49 GMT"}, {"version": "v4", "created": "Wed, 30 Oct 2013 14:12:16 GMT"}], "update_date": "2013-10-31", "authors_parsed": [["Bartiromo", "Rosario", ""]]}, {"id": "1106.5913", "submitter": "Petr Jizba", "authors": "Petr Jizba, Hagen Kleinert, Mohammad Shefaat", "title": "Renyi's information transfer between financial time series", "comments": "35 pages, 16 figure, RevTeX4, revised version with minor changes,\n  accepted to Physica A", "journal-ref": "Physica A 391 (2012) 2971-2989", "doi": "10.1016/j.physa.2011.12.064", "report-no": null, "categories": "q-fin.ST cond-mat.stat-mech", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we quantify the statistical coherence between financial time\nseries by means of the Renyi entropy. With the help of Campbell's coding\ntheorem we show that the Renyi entropy selectively emphasizes only certain\nsectors of the underlying empirical distribution while strongly suppressing\nothers. This accentuation is controlled with Renyi's parameter q. To tackle the\nissue of the information flow between time series we formulate the concept of\nRenyi's transfer entropy as a measure of information that is transferred only\nbetween certain parts of underlying distributions. This is particularly\npertinent in financial time series where the knowledge of marginal events such\nas spikes or sudden jumps is of a crucial importance. We apply the Renyian\ninformation flow to stock market time series from 11 world stock indices as\nsampled at a daily rate in the time period 02.01.1990 - 31.12.2009.\nCorresponding heat maps and net information flows are represented graphically.\nA detailed discussion of the transfer entropy between the DAX and S&P500\nindices based on minute tick data gathered in the period from 02.04.2008 to\n11.09.2009 is also provided. Our analysis shows that the bivariate information\nflow between world markets is strongly asymmetric with a distinct information\nsurplus flowing from the Asia-Pacific region to both European and US markets.\nAn important yet less dramatic excess of information also flows from Europe to\nthe US. This is particularly clearly seen from a careful analysis of Renyi\ninformation flow between the DAX and S&P500 indices.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 11:45:57 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2012 22:46:23 GMT"}, {"version": "v3", "created": "Fri, 27 Jan 2012 08:58:09 GMT"}], "update_date": "2012-02-22", "authors_parsed": [["Jizba", "Petr", ""], ["Kleinert", "Hagen", ""], ["Shefaat", "Mohammad", ""]]}]