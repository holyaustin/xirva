[{"id": "2006.00123", "submitter": "Dhagash Mehta", "authors": "Dhagash Mehta, Dhruv Desai, Jithin Pradeep", "title": "Machine Learning Fund Categorizations", "comments": "8 pages, 2-column format, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG q-fin.CP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the surge in popularity of mutual funds (including exchange-traded\nfunds (ETFs)) as a diversified financial investment, a vast variety of mutual\nfunds from various investment management firms and diversification strategies\nhave become available in the market. Identifying similar mutual funds among\nsuch a wide landscape of mutual funds has become more important than ever\nbecause of many applications ranging from sales and marketing to portfolio\nreplication, portfolio diversification and tax loss harvesting. The current\nbest method is data-vendor provided categorization which usually relies on\ncuration by human experts with the help of available data. In this work, we\nestablish that an industry wide well-regarded categorization system is\nlearnable using machine learning and largely reproducible, and in turn\nconstructing a truly data-driven categorization. We discuss the intellectual\nchallenges in learning this man-made system, our results and their\nimplications.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 23:26:14 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Mehta", "Dhagash", ""], ["Desai", "Dhruv", ""], ["Pradeep", "Jithin", ""]]}, {"id": "2006.00158", "submitter": "Yasushi Ota", "authors": "Daiki Maki and Yasushi Ota", "title": "The impacts of asymmetry on modeling and forecasting realized volatility\n  in Japanese stock markets", "comments": "27 pages, 4 figures and 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the impacts of asymmetry on the modeling and\nforecasting of realized volatility in the Japanese futures and spot stock\nmarkets. We employ heterogeneous autoregressive (HAR) models allowing for three\ntypes of asymmetry: positive and negative realized semivariance (RSV),\nasymmetric jumps, and leverage effects. The estimation results show that\nleverage effects clearly influence the modeling of realized volatility models.\nLeverage effects exist for both the spot and futures markets in the Nikkei 225.\nAlthough realized semivariance aids better modeling, the estimations of RSV\nmodels depend on whether these models have leverage effects. Asymmetric jump\ncomponents do not have a clear influence on realized volatility models. While\nleverage effects and realized semivariance also improve the out-of-sample\nforecast performance of volatility models, asymmetric jumps are not useful for\npredictive ability. The empirical results of this study indicate that\nasymmetric information, in particular, leverage effects and realized\nsemivariance, yield better modeling and more accurate forecast performance.\nAccordingly, asymmetric information should be included when we model and\nforecast the realized volatility of Japanese stock markets.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2020 03:26:35 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Maki", "Daiki", ""], ["Ota", "Yasushi", ""]]}, {"id": "2006.00596", "submitter": "Vygintas Gontis", "authors": "Vygintas Gontis", "title": "Long-range memory test by the burst and inter-burst duration\n  distribution", "comments": "18 pages, 8 figures", "journal-ref": "J. Stat. Mech. (2020) 093406", "doi": "10.1088/1742-5468/abb4db", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is empirically established that order flow in the financial markets is\npositively auto-correlated and can serve as an example of a social system with\nlong-range memory. Nevertheless, widely used long-range memory estimators give\nvarying values of the Hurst exponent. We propose the burst and inter-burst\nduration statistical analysis as one more test of long-range memory and\nimplement it with the limit order book data comparing it with other widely used\nestimators. This method gives a more reliable evaluation of the Hurst exponent\nindependent of the stock in consideration or time definition used. Results\nstrengthen the expectation that burst and inter-burst duration analysis can\nserve as a better method to investigate the property of long-range memory.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 19:59:42 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2020 08:16:57 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Gontis", "Vygintas", ""]]}, {"id": "2006.02077", "submitter": "Nicklas Werge", "authors": "Nicklas Werge (LPSM), Olivier Wintenberger (LPSM)", "title": "AdaVol: An Adaptive Recursive Volatility Prediction Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quasi-Maximum Likelihood (QML) procedures are theoretically appealing and\nwidely used for statistical inference. While there are extensive references on\nQML estimation in batch settings, it has attracted little attention in\nstreaming settings until recently. An investigation of the convergence\nproperties of the QML procedure in a general conditionally heteroscedastic time\nseries model is conducted, and the classical batch optimization routines\nextended to the framework of streaming and large-scale problems. An adaptive\nrecursive estimation routine for GARCH models named AdaVol is presented. The\nAdaVol procedure relies on stochastic approximations combined with the\ntechnique of Variance Targeting Estimation (VTE). This recursive method has\ncomputationally efficient properties, while VTE alleviates some convergence\ndifficulties encountered by the usual QML estimation due to a lack of\nconvexity. Empirical results demonstrate a favorable trade-off between AdaVol's\nstability and the ability to adapt to time-varying estimates for real-life\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 07:28:31 GMT"}, {"version": "v2", "created": "Fri, 9 Oct 2020 13:11:49 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2021 15:22:43 GMT"}, {"version": "v4", "created": "Sun, 17 Jan 2021 09:38:02 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Werge", "Nicklas", "", "LPSM"], ["Wintenberger", "Olivier", "", "LPSM"]]}, {"id": "2006.02467", "submitter": "Javad Shaabani", "authors": "Javad Shaabani and Ali Akbar Jafari", "title": "A New Look to Three-Factor Fama-French Regression Model using Sample\n  Innovations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fama-French model is widely used in assessing the portfolio's performance\ncompared to market returns. In Fama-French models, all factors are time-series\ndata. The cross-sectional data are slightly different from the time series\ndata. A distinct problem with time-series regressions is that R-squared in time\nseries regressions is usually very high, especially compared with typical\nR-squared for cross-sectional data. The high value of R-squared may cause\nmisinterpretation that the regression model fits the observed data well, and\nthe variance in the dependent variable is explained well by the independent\nvariables. Thus, to do regression analysis, and overcome with the serial\ndependence and volatility clustering, we use standard econometrics time series\nmodels to derive sample innovations. In this study, we revisit and validate the\nFama-French models in two different ways: using the factors and asset returns\nin the Fama-French model and considering the sample innovations in the\nFama-French model instead of studying the factors. Comparing the two methods\nconsidered in this study, we suggest the Fama-French model should be considered\nwith heavy tail distributions as the tail behavior is relevant in Fama-French\nmodels, including financial data, and the QQ plot does not validate that the\nchoice of the normal distribution as the theoretical distribution for the noise\nin the model.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2020 18:24:09 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Shaabani", "Javad", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "2006.03014", "submitter": "Ioannis Anagnostou", "authors": "Ioannis Anagnostou, Tiziano Squartini, Drona Kandhai, Diego\n  Garlaschelli", "title": "Uncovering the mesoscale structure of the credit default swap market to\n  improve portfolio risk modelling", "comments": "Quantitative Finance (2021)", "journal-ref": null, "doi": "10.1080/14697688.2021.1890807", "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most challenging aspects in the analysis and modelling of\nfinancial markets, including Credit Default Swap (CDS) markets, is the presence\nof an emergent, intermediate level of structure standing in between the\nmicroscopic dynamics of individual financial entities and the macroscopic\ndynamics of the market as a whole. This elusive, mesoscopic level of\norganisation is often sought for via factor models that ultimately decompose\nthe market according to geographic regions and economic industries. However, at\na more general level the presence of mesoscopic structure might be revealed in\nan entirely data-driven approach, looking for a modular and possibly\nhierarchical organisation of the empirical correlation matrix between financial\ntime series. The crucial ingredient in such an approach is the definition of an\nappropriate null model for the correlation matrix. Recent research showed that\ncommunity detection techniques developed for networks become intrinsically\nbiased when applied to correlation matrices. For this reason, a method based on\nRandom Matrix Theory has been developed, which identifies the optimal\nhierarchical decomposition of the system into internally correlated and\nmutually anti-correlated communities. Building upon this technique, here we\nresolve the mesoscopic structure of the CDS market and identify groups of\nissuers that cannot be traced back to standard industry/region taxonomies,\nthereby being inaccessible to standard factor models. We use this decomposition\nto introduce a novel default risk model that is shown to outperform more\ntraditional alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2020 17:02:52 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 12:18:05 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Anagnostou", "Ioannis", ""], ["Squartini", "Tiziano", ""], ["Kandhai", "Drona", ""], ["Garlaschelli", "Diego", ""]]}, {"id": "2006.03458", "submitter": "Fabrizio Cipollini", "authors": "Alessandra Amendola, Vincenzo Candila, Fabrizio Cipollini, Giampiero\n  M. Gallo", "title": "Doubly Multiplicative Error Models with Long- and Short-run Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We suggest the Doubly Multiplicative Error class of models (DMEM) for\nmodeling and forecasting realized volatility, which combines two components\naccommodating low-, respectively, high-frequency features in the data. We\nderive the theoretical properties of the Maximum Likelihood and Generalized\nMethod of Moments estimators. Two such models are then proposed, the\nComponent-MEM, which uses daily data for both components, and the MEM-MIDAS,\nwhich exploits the logic of MIxed-DAta Sampling (MIDAS). The empirical\napplication involves the S&P 500, NASDAQ, FTSE 100 and Hang Seng indices:\nirrespective of the market, both DMEM's outperform the HAR and other relevant\nGARCH-type models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2020 14:03:48 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Amendola", "Alessandra", ""], ["Candila", "Vincenzo", ""], ["Cipollini", "Fabrizio", ""], ["Gallo", "Giampiero M.", ""]]}, {"id": "2006.03686", "submitter": "Yun-Cheng Tsai", "authors": "Jun-Hao Chen and Samuel Yen-Chi Chen and Yun-Cheng Tsai and\n  Chih-Shiang Shur", "title": "Adversarial Robustness of Deep Convolutional Candlestick Learner", "comments": "arXiv admin note: text overlap with arXiv:2005.06731", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CR cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has been applied extensively in a wide range of fields.\nHowever, it has been shown that DL models are susceptible to a certain kinds of\nperturbations called \\emph{adversarial attacks}. To fully unlock the power of\nDL in critical fields such as financial trading, it is necessary to address\nsuch issues. In this paper, we present a method of constructing perturbed\nexamples and use these examples to boost the robustness of the model. Our\nalgorithm increases the stability of DL models for candlestick classification\nwith respect to perturbations in the input data.\n", "versions": [{"version": "v1", "created": "Fri, 29 May 2020 02:58:04 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Chen", "Jun-Hao", ""], ["Chen", "Samuel Yen-Chi", ""], ["Tsai", "Yun-Cheng", ""], ["Shur", "Chih-Shiang", ""]]}, {"id": "2006.04212", "submitter": "Junyi Li", "authors": "Junyi Li, Xitong Wang, Yaoyang Lin, Arunesh Sinha, Micheal P. Wellman", "title": "Generating Realistic Stock Market Order Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to generate realistic and high-fidelity stock market\ndata based on generative adversarial networks (GANs). Our Stock-GAN model\nemploys a conditional Wasserstein GAN to capture history dependence of orders.\nThe generator design includes specially crafted aspects including components\nthat approximate the market's auction mechanism, augmenting the order history\nwith order-book constructions to improve the generation task. We perform an\nablation study to verify the usefulness of aspects of our network structure. We\nprovide a mathematical characterization of distribution learned by the\ngenerator. We also propose statistics to measure the quality of generated\norders. We test our approach with synthetic and actual market data, compare to\nmany baseline generative models, and find the generated data to be close to\nreal data.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2020 17:32:42 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Li", "Junyi", ""], ["Wang", "Xitong", ""], ["Lin", "Yaoyang", ""], ["Sinha", "Arunesh", ""], ["Wellman", "Micheal P.", ""]]}, {"id": "2006.04727", "submitter": "Florian Krach", "authors": "Calypso Herrera, Florian Krach, Josef Teichmann", "title": "Neural Jump Ordinary Differential Equations: Consistent Continuous-Time\n  Prediction and Filtering", "comments": null, "journal-ref": "International Conference on Learning Representations (2021)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR q-fin.CP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinations of neural ODEs with recurrent neural networks (RNN), like\nGRU-ODE-Bayes or ODE-RNN are well suited to model irregularly observed time\nseries. While those models outperform existing discrete-time approaches, no\ntheoretical guarantees for their predictive capabilities are available.\nAssuming that the irregularly-sampled time series data originates from a\ncontinuous stochastic process, the $L^2$-optimal online prediction is the\nconditional expectation given the currently available information. We introduce\nthe Neural Jump ODE (NJ-ODE) that provides a data-driven approach to learn,\ncontinuously in time, the conditional expectation of a stochastic process. Our\napproach models the conditional expectation between two observations with a\nneural ODE and jumps whenever a new observation is made. We define a novel\ntraining framework, which allows us to prove theoretical guarantees for the\nfirst time. In particular, we show that the output of our model converges to\nthe $L^2$-optimal prediction. This can be interpreted as solution to a special\nfiltering problem. We provide experiments showing that the theoretical results\nalso hold empirically. Moreover, we experimentally show that our model\noutperforms the baselines in more complex learning tasks and give comparisons\non real-world datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2020 16:34:51 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2020 06:46:28 GMT"}, {"version": "v3", "created": "Wed, 9 Dec 2020 13:36:20 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2021 12:54:38 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Herrera", "Calypso", ""], ["Krach", "Florian", ""], ["Teichmann", "Josef", ""]]}, {"id": "2006.05204", "submitter": "Dmitry Rokhlin B.", "authors": "Dmitry B. Rokhlin", "title": "Relative utility bounds for empirically optimal portfolios", "comments": "20 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a single-period portfolio selection problem for an investor,\nmaximizing the expected ratio of the portfolio utility and the utility of a\nbest asset taken in hindsight. The decision rules are based on the history of\nstock returns with unknown distribution. Assuming that the utility function is\nLipschitz or H\\\"{o}lder continuous (the concavity is not required), we obtain\nhigh probability utility bounds under the sole assumption that the returns are\nindependent and identically distributed. These bounds depend only on the\nutility function, the number of assets and the number of observations. For\nconcave utilities similar bounds are obtained for the portfolios produced by\nthe exponentiated gradient method. Also we use statistical experiments to study\nrisk and generalization properties of empirically optimal portfolios. Herein we\nconsider a model with one risky asset and a dataset, containing the stock\nprices from NYSE.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2020 12:03:38 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Rokhlin", "Dmitry B.", ""]]}, {"id": "2006.06237", "submitter": "Ingo Hoffmann", "authors": "Tim Schmitz and Ingo Hoffmann", "title": "Re-evaluating cryptocurrencies' contribution to portfolio\n  diversification -- A portfolio analysis with special focus on German\n  investors", "comments": "59 pages, 20 figures. JEL Classification: C12, C13, C32, E22, G11", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.GN q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate whether mixing cryptocurrencies to a German\ninvestor portfolio improves portfolio diversification. We analyse this research\nquestion by applying a (mean variance) portfolio analysis using a toolbox\nconsisting of (i) the comparison of descriptive statistics, (ii) graphical\nmethods and (iii) econometric spanning tests. In contrast to most of the former\nstudies we use a (broad) customized, Equally-Weighted Cryptocurrency Index\n(EWCI) to capture the average development of a whole ex ante defined\ncryptocurrency universe and to mitigate possible survivorship biases in the\ndata. According to Glas/Poddig (2018), this bias could have led to misleading\nresults in some already existing studies. We find that cryptocurrencies can\nimprove portfolio diversification in a few of the analyzed windows from our\ndataset (consisting of weekly observations from 2014-01-01 to 2019-05-31).\nHowever, we cannot confirm this pattern as the normal case. By including\ncryptocurrencies in their portfolios, investors predominantly cannot reach a\nsignificantly higher efficient frontier. These results also hold, if the\nnon-normality of cryptocurrency returns is considered. Moreover, we control for\nchanges of the results, if transaction costs/illiquidities on the\ncryptocurrency market are additionally considered.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2020 07:43:00 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 04:38:54 GMT"}], "update_date": "2020-08-07", "authors_parsed": [["Schmitz", "Tim", ""], ["Hoffmann", "Ingo", ""]]}, {"id": "2006.07847", "submitter": "Christof Schmidhuber", "authors": "Christof Schmidhuber", "title": "Trends, Reversion, and Critical Phenomena in Financial Markets", "comments": "28 pages, 6 figures, minor corrections and additional references in\n  revised version", "journal-ref": null, "doi": "10.1016/j.physa.2020.125642", "report-no": null, "categories": "q-fin.ST cond-mat.stat-mech hep-th q-fin.PM q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial markets across all asset classes are known to exhibit trends. These\ntrends have been exploited by traders for decades. Here, we empirically measure\nwhen trends revert, based on 30 years of daily futures prices for equity\nindices, interest rates, currencies and commodities. We find that trends tend\nto revert once they reach a critical level of statistical significance. Based\non polynomial regression, we carefully measure this critical level. We find\nthat it is universal across asset classes and has a universal scaling behavior,\nas the trend's time horizon runs from a few days to several years. The\ncorresponding regression coefficients are small, but statistically highly\nsignificant, as confirmed by bootstrapping and out-of-sample testing. Our\nresults signal to investors when to exit a trend. They also reveal how markets\nhave become more efficient over the decades. Moreover, they point towards a\npotential deep analogy between financial markets and critical phenomena: our\nanalysis supports the conjecture that financial markets can be modeled as\nstatistical mechanical ensembles of Buy/Sell orders near critical points. In\nthis analogy, the trend strength plays the role of an order parameter, whose\ndynamcis is described by a Langevin equation with a quartic potential.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2020 09:12:05 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2020 21:49:35 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2020 14:52:36 GMT"}, {"version": "v4", "created": "Fri, 11 Dec 2020 12:22:12 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Schmidhuber", "Christof", ""]]}, {"id": "2006.09154", "submitter": "Zu-Guo Yu", "authors": "Bao-Gen Li, Dian-Yi Ling, Zu-Guo Yu", "title": "Multifractal temporally weighted detrended partial cross-correlation\n  analysis to quantify intrinsic power-law cross-correlation of two\n  non-stationary time series affected by common external factors", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.physa.2021.125920", "report-no": null, "categories": "physics.soc-ph physics.data-an q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When common factors strongly influence two cross-correlated time series\nrecorded in complex natural and social systems, the results will be biased if\nwe use multifractal detrended cross-correlation analysis (MF-DXA) without\nconsidering these common factors. Based on multifractal temporally weighted\ndetrended cross-correlation analysis (MF-TWXDFA) proposed by our group and\nmultifractal partial cross-correlation analysis (MF-DPXA) proposed by Qian et\nal., we propose a new method---multifractal temporally weighted detrended\npartial cross-correlation analysis (MF-TWDPCCA) to quantify intrinsic power-law\ncross-correlation of two non-stationary time series affected by common external\nfactors in this paper. We use MF-TWDPCCA to characterize the intrinsic\ncross-correlations between the two simultaneously recorded time series by\nremoving the effects of other potential time series. To test the performance of\nMF-TWDPCCA, we apply it, MF-TWXDFA and MF-DPXA on simulated series. Numerical\ntests on artificially simulated series demonstrate that MF-TWDPCCA can\naccurately detect the intrinsic cross-correlations for two simultaneously\nrecorded series. To further show the utility of MF-TWDPCCA, we apply it on time\nseries from stock markets and find that there exists significantly multifractal\npower-law cross-correlation between stock returns. A new partial\ncross-correlation coefficient is defined to quantify the level of intrinsic\ncross-correlation between two time series.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2020 13:05:29 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Li", "Bao-Gen", ""], ["Ling", "Dian-Yi", ""], ["Yu", "Zu-Guo", ""]]}, {"id": "2006.09723", "submitter": "Pranava Madhyastha", "authors": "Karolina Sowinska and Pranava Madhyastha", "title": "A Tweet-based Dataset for Company-Level Stock Return Prediction", "comments": "Dataset available here:\n  https://github.com/ImperialNLP/stockreturnpred", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.SI q-fin.ST", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Public opinion influences events, especially related to stock market\nmovement, in which a subtle hint can influence the local outcome of the market.\nIn this paper, we present a dataset that allows for company-level analysis of\ntweet based impact on one-, two-, three-, and seven-day stock returns. Our\ndataset consists of 862, 231 labelled instances from twitter in English, we\nalso release a cleaned subset of 85, 176 labelled instances to the community.\nWe also provide baselines using standard machine learning algorithms and a\nmulti-view learning based approach that makes use of different types of\nfeatures. Our dataset, scripts and models are publicly available at:\nhttps://github.com/ImperialNLP/stockreturnpred.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2020 08:55:11 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Sowinska", "Karolina", ""], ["Madhyastha", "Pranava", ""]]}, {"id": "2006.10245", "submitter": "David Frazier", "authors": "Veronika Czellar, David T. Frazier and Eric Renault", "title": "Approximate Maximum Likelihood for Complex Structural Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect Inference (I-I) is a popular technique for estimating complex\nparametric models whose likelihood function is intractable, however, the\nstatistical efficiency of I-I estimation is questionable. While the efficient\nmethod of moments, Gallant and Tauchen (1996), promises efficiency, the price\nto pay for this efficiency is a loss of parsimony and thereby a potential lack\nof robustness to model misspecification. This stands in contrast to simpler I-I\nestimation strategies, which are known to display less sensitivity to model\nmisspecification precisely due to their focus on specific elements of the\nunderlying structural model. In this research, we propose a new\nsimulation-based approach that maintains the parsimony of I-I estimation, which\nis often critical in empirical applications, but can also deliver estimators\nthat are nearly as efficient as maximum likelihood. This new approach is based\non using a constrained approximation to the structural model, which ensures\nidentification and can deliver estimators that are nearly efficient. We\ndemonstrate this approach through several examples, and show that this approach\ncan deliver estimators that are nearly as efficient as maximum likelihood, when\nfeasible, but can be employed in many situations where maximum likelihood is\ninfeasible.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 02:53:53 GMT"}], "update_date": "2020-06-19", "authors_parsed": [["Czellar", "Veronika", ""], ["Frazier", "David T.", ""], ["Renault", "Eric", ""]]}, {"id": "2006.11088", "submitter": "Alexander McNeil", "authors": "Martin Bladt and Alexander J. McNeil", "title": "Time series copula models using d-vines and v-transforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An approach to modelling volatile financial return series using stationary\nd-vine copula processes combined with Lebesgue-measure-preserving\ntransformations known as v-transforms is proposed. By developing a method of\nstochastically inverting v-transforms, models are constructed that can describe\nboth stochastic volatility in the magnitude of price movements and serial\ncorrelation in their directions. In combination with parametric marginal\ndistributions it is shown that these models can rival and sometimes outperform\nwell-known models in the extended GARCH family.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 11:48:42 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2020 09:03:42 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 14:24:10 GMT"}, {"version": "v4", "created": "Tue, 13 Jul 2021 18:14:20 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Bladt", "Martin", ""], ["McNeil", "Alexander J.", ""]]}, {"id": "2006.11119", "submitter": "Hongwei Lin", "authors": "Chenkai Xu, Hongwei Lin, Xuansu Fang", "title": "Manifold Feature Index: A novel index based on high-dimensional data\n  simplification", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel stock index model, namely the manifold\nfeature(MF) index, to reflect the overall price activity of the entire stock\nmarket. Based on the theory of manifold learning, the researched stock dataset\nis assumed to be a low-dimensional manifold embedded in a higher-dimensional\nEuclidean space. After data preprocessing, its manifold structure and discrete\nLaplace-Beltrami operator(LBO) matrix are constructed. We propose a\nhigh-dimensional data feature detection method to detect feature points on the\neigenvectors of LBO, and the stocks corresponding to these feature points are\nconsidered as the constituent stocks of the MF index. Finally, the MF index is\ngenerated by a weighted formula using the price and market capitalization of\nthese constituents. The stock market studied in this research is the Shanghai\nStock Exchange(SSE). We propose four metrics to compare the MF index series and\nthe SSE index series (SSE 50, SSE 100, SSE 150, SSE 180 and SSE 380). From the\nperspective of data approximation, the results demonstrate that our indexes are\ncloser to the stock market than the SSE index series. From the perspective of\nrisk premium, MF indexes have higher stability and lower risk.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2020 13:10:04 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Xu", "Chenkai", ""], ["Lin", "Hongwei", ""], ["Fang", "Xuansu", ""]]}, {"id": "2006.12426", "submitter": "Stefano Giani", "authors": "Jonathan Readshaw and Stefano Giani", "title": "Using Company Specific Headlines and Convolutional Neural Networks to\n  Predict Stock Fluctuations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a Convolutional Neural Network (CNN) for the prediction of\nnext-day stock fluctuations using company-specific news headlines. Experiments\nto evaluate model performance using various configurations of word-embeddings\nand convolutional filter widths are reported. The total number of convolutional\nfilters used is far fewer than is common, reducing the dimensionality of the\ntask without loss of accuracy. Furthermore, multiple hidden layers with\ndecreasing dimensionality are employed. A classification accuracy of 61.7\\% is\nachieved using pre-learned embeddings, that are fine-tuned during training to\nrepresent the specific context of this task. Multiple filter widths are also\nimplemented to detect different length phrases that are key for classification.\nTrading simulations are conducted using the presented classification results.\nInitial investments are more than tripled over a 838 day testing period using\nthe optimal classification configuration and a simple trading strategy. Two\nnovel methods are presented to reduce the risk of the trading simulations.\nAdjustment of the sigmoid class threshold and re-labelling headlines using\nmultiple classes form the basis of these methods. A combination of these\napproaches is found to more than double the Average Trade Profit (ATP) achieved\nduring baseline simulations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2020 16:53:26 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Readshaw", "Jonathan", ""], ["Giani", "Stefano", ""]]}, {"id": "2006.14473", "submitter": "S M Raju", "authors": "S M Raju and Ali Mohammad Tarif", "title": "Real-Time Prediction of BITCOIN Price using Machine Learning Techniques\n  and Public Sentiment Analysis", "comments": "14 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bitcoin is the first digital decentralized cryptocurrency that has shown a\nsignificant increase in market capitalization in recent years. The objective of\nthis paper is to determine the predictable price direction of Bitcoin in USD by\nmachine learning techniques and sentiment analysis. Twitter and Reddit have\nattracted a great deal of attention from researchers to study public sentiment.\nWe have applied sentiment analysis and supervised machine learning principles\nto the extracted tweets from Twitter and Reddit posts, and we analyze the\ncorrelation between bitcoin price movements and sentiments in tweets. We\nexplored several algorithms of machine learning using supervised learning to\ndevelop a prediction model and provide informative analysis of future market\nprices. Due to the difficulty of evaluating the exact nature of a Time\nSeries(ARIMA) model, it is often very difficult to produce appropriate\nforecasts. Then we continue to implement Recurrent Neural Networks (RNN) with\nlong short-term memory cells (LSTM). Thus, we analyzed the time series model\nprediction of bitcoin prices with greater efficiency using long short-term\nmemory (LSTM) techniques and compared the predictability of bitcoin price and\nsentiment analysis of bitcoin tweets to the standard method (ARIMA). The RMSE\n(Root-mean-square error) of LSTM are 198.448 (single feature) and 197.515\n(multi-feature) whereas the ARIMA model RMSE is 209.263 which shows that LSTM\nwith multi feature shows the more accurate result.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2020 15:40:11 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["Raju", "S M", ""], ["Tarif", "Ali Mohammad", ""]]}, {"id": "2006.14498", "submitter": "Blanka Horvath", "authors": "Hans B\\\"uhler, Blanka Horvath, Terry Lyons, Imanol Perez Arribas, and\n  Ben Wood", "title": "A Data-driven Market Simulator for Small Data Environments", "comments": "27 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG q-fin.CP q-fin.MF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network based data-driven market simulation unveils a new and flexible\nway of modelling financial time series without imposing assumptions on the\nunderlying stochastic dynamics. Though in this sense generative market\nsimulation is model-free, the concrete modelling choices are nevertheless\ndecisive for the features of the simulated paths. We give a brief overview of\ncurrently used generative modelling approaches and performance evaluation\nmetrics for financial time series, and address some of the challenges to\nachieve good results in the latter. We also contrast some classical approaches\nof market simulation with simulation based on generative modelling and\nhighlight some advantages and pitfalls of the new approach. While most\ngenerative models tend to rely on large amounts of training data, we present\nhere a generative model that works reliably in environments where the amount of\navailable training data is notoriously small. Furthermore, we show how a rough\npaths perspective combined with a parsimonious Variational Autoencoder\nframework provides a powerful way for encoding and evaluating financial time\nseries in such environments where available training data is scarce. Finally,\nwe also propose a suitable performance evaluation metric for financial time\nseries and discuss some connections of our Market Generator to deep hedging.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2020 14:04:21 GMT"}], "update_date": "2020-06-26", "authors_parsed": [["B\u00fchler", "Hans", ""], ["Horvath", "Blanka", ""], ["Lyons", "Terry", ""], ["Arribas", "Imanol Perez", ""], ["Wood", "Ben", ""]]}, {"id": "2006.14499", "submitter": "Rupam Bhattacharyya", "authors": "Indrajit Banerjee, Atul Kumar, Rupam Bhattacharyya", "title": "Examining the Effect of COVID-19 on Foreign Exchange Rate and Stock\n  Market -- An Applied Insight into the Variable Effects of Lockdown on Indian\n  Economy", "comments": "8 Figures and Supplementary Document included - Total 72 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since March 25, 2020, India had been under a nation-wide lockdown announced\nas a response to the spread of SARS-CoV-2 and COVID-19 and has resorted to a\nprocess of 'unlocking' the lockdown over the past couple of months. This work\nattempts to examine the effect of novel coronavirus 2019 (COVID-19) and its\nresulting disease, the COVID-19, on the foreign exchange rates and stock market\nperformances of India using secondary data over a span of 112 days spanning\nbetween March 11 and June 30, 2020. The study explores whether the causal\nrelationships and directions among the growth rate of confirmed cases\n(GROWTHC), exchange rate (GEX) and SENSEX value (GSENSEX) are remaining the\nsame across different pre and post-lockdown phases, attempting to capture any\npotential changes over time via the vector autoregressive (VAR) models. A\npositive correlation is found between the growth rate of confirmed cases and\nthe growth rate of exchange rate, and a negative correlation between the growth\nrate of confirmed cases and the growth rate of SENSEX value. However, on\napplying a vector autoregressive (VAR) model, it is observed that an increase\nin the confirmed COVID-19 cases causes no significant change in the values of\nthe exchange rate and SENSEX index. The result varies if the analysis is split\nacross different time periods - before lockdown, the four phases of lockdown,\nand the first phase of unlock. Nuanced and sensible interpretations of the\nnumeric results indicate significant variability across time in terms of the\nrelation between the variables of interest. The detailed knowledge about the\nvarying patterns of dependence could potentially help the policy makers and\ninvestors of India in order to develop their strategies to cope up with the\nsituation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2020 18:40:05 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 01:05:44 GMT"}, {"version": "v3", "created": "Wed, 19 Aug 2020 16:50:12 GMT"}, {"version": "v4", "created": "Wed, 30 Sep 2020 21:36:18 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Banerjee", "Indrajit", ""], ["Kumar", "Atul", ""], ["Bhattacharyya", "Rupam", ""]]}, {"id": "2006.14510", "submitter": "Jakub Marecek", "authors": "Daniel J. Egger, Claudio Gambella, Jakub Marecek, Scott McFaddin,\n  Martin Mevissen, Rudy Raymond, Andrea Simonetto, Stefan Woerner, Elena\n  Yndurain", "title": "Quantum Computing for Finance: State of the Art and Future Prospects", "comments": "24 pages", "journal-ref": "IEEE Transactions on Quantum Engineering, vol. 1, pp. 1-24, 2020,\n  Art no. 3101724", "doi": "10.1109/TQE.2020.3030314", "report-no": null, "categories": "quant-ph q-fin.ST", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This article outlines our point of view regarding the applicability,\nstate-of-the-art, and potential of quantum computing for problems in finance.\nWe provide an introduction to quantum computing as well as a survey on problem\nclasses in finance that are computationally challenging classically and for\nwhich quantum computing algorithms are promising. In the main part, we describe\nin detail quantum algorithms for specific applications arising in financial\nservices, such as those involving simulation, optimization, and machine\nlearning problems. In addition, we include demonstrations of quantum algorithms\non IBM Quantum back-ends and discuss the potential benefits of quantum\nalgorithms for problems in financial services. We conclude with a summary of\ntechnical challenges and future prospects.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2020 16:02:05 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2020 18:10:06 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2021 10:34:36 GMT"}], "update_date": "2021-01-29", "authors_parsed": [["Egger", "Daniel J.", ""], ["Gambella", "Claudio", ""], ["Marecek", "Jakub", ""], ["McFaddin", "Scott", ""], ["Mevissen", "Martin", ""], ["Raymond", "Rudy", ""], ["Simonetto", "Andrea", ""], ["Woerner", "Stefan", ""], ["Yndurain", "Elena", ""]]}, {"id": "2006.15214", "submitter": "Ahmed Elsawah", "authors": "Zhongjun Wang, Mengye Sun, A. M. Elsawah", "title": "Improving MF-DFA model with applications in precious metals market", "comments": "23 pages, 17 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the aggravation of the global economic crisis and inflation, the\nprecious metals with safe-haven function have become more popular. An improved\nMF-DFA method is proposed to analyze price fluctuations of the precious metals\nmarket. Based on the widely used multifractal detrended fluctuation analysis\nmethod (MF-DFA), we compare these two methods and find that the Bi-OSW-MF-DFA\nmethod possesses better efficiency. This article analyzes the degree of\nmultifractality between spot gold market and spot silver market as well as\ntheir risks. From the numerical results and figures, it is found that two\nelements constitute the contributions in the formation of multifractality in\ntime series and the risk of the spot silver market is higher than that of the\nspot gold market. This attempt could lead to a better understanding of\ncomplicated precious metals market.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2020 21:07:39 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Wang", "Zhongjun", ""], ["Sun", "Mengye", ""], ["Elsawah", "A. M.", ""]]}, {"id": "2006.16383", "submitter": "Eduardo Ramos", "authors": "E. Ramos-P\\'erez, P.J. Alonso-Gonz\\'alez, J.J. N\\'u\\~nez-Vel\\'azquez", "title": "Forecasting volatility with a stacked model based on a hybridized\n  Artificial Neural Network", "comments": "22 pages, 7 tables, 1 Figure. Published in Expert Systems with\n  Applications, Volume 129, 1 September 2019, Pages 1-9", "journal-ref": "Expert Systems with Applications, Volume 129, 1 September 2019,\n  Pages 1-9", "doi": "10.1016/j.eswa.2019.03.046", "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An appropriate calibration and forecasting of volatility and market risk are\nsome of the main challenges faced by companies that have to manage the\nuncertainty inherent to their investments or funding operations such as banks,\npension funds or insurance companies. This has become even more evident after\nthe 2007-2008 Financial Crisis, when the forecasting models assessing the\nmarket risk and volatility failed. Since then, a significant number of\ntheoretical developments and methodologies have appeared to improve the\naccuracy of the volatility forecasts and market risk assessments. Following\nthis line of thinking, this paper introduces a model based on using a set of\nMachine Learning techniques, such as Gradient Descent Boosting, Random Forest,\nSupport Vector Machine and Artificial Neural Network, where those algorithms\nare stacked to predict S&P500 volatility. The results suggest that our\nconstruction outperforms other habitual models on the ability to forecast the\nlevel of volatility, leading to a more accurate assessment of the market risk.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2020 21:01:24 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 18:28:29 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Ramos-P\u00e9rez", "E.", ""], ["Alonso-Gonz\u00e1lez", "P. J.", ""], ["N\u00fa\u00f1ez-Vel\u00e1zquez", "J. J.", ""]]}]