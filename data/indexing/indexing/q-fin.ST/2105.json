[{"id": "2105.01380", "submitter": "David Thesmar", "authors": "Antoine Falck, Adam Rej, David Thesmar", "title": "Why and how systematic strategies decay", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose ex-ante characteristics that predict the drop in\nrisk-adjusted performance out-of-sample for a large set of stock anomalies\npublished in finance and accounting academic journals. Our set of predictors is\ngenerated by hypotheses of OOS decay put forward by McLean and Pontiff (2016):\narbitrage capital flowing into newly published strategies and in-sample\noverfitting linked to multiple hypothesis testing. The year of publication\nalone - compatible with both hypotheses - explains 30% of the variance of\nSharpe decay across factors: Every year, the Sharpe decay of newly-published\nfactors increases by 5ppt. The other important variables are directly related\nto overfitting: the number of operations required to calculate the signal and\ntwo measures of sensitivity of in-sample Sharpe to outliers together add\nanother 15% of explanatory power. Some arbitrage-related variables are\nstatistically significant, but their predictive power is marginal.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 09:20:25 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Falck", "Antoine", ""], ["Rej", "Adam", ""], ["Thesmar", "David", ""]]}, {"id": "2105.02057", "submitter": "Vygintas Gontis", "authors": "Vygintas Gontis", "title": "Order flow in the financial markets from the perspective of the\n  Fractional L\\'{e}vy stable motion", "comments": "13 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cond-mat.stat-mech", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is a challenging task to identify the best possible models based on given\nempirical data of real stochastic time series. Though the financial markets\nprovide us with a vast amount of empirical data, the best model selection is\nstill a big challenge for researchers. The widely used long-range memory and\nself-similarity estimators give varying values of the parameters as these\nestimators themselves are developed for the specific models of time series.\nHere we investigate the order disbalance time series constructed from the limit\norder book data of the financial markets under fractional L\\'{e}vy stable\nmotion assumption. Our results suggest that previous findings of persistence in\norder flow are related to the power-law distribution of order sizes. Still,\norders have stable estimates of anti-correlation for the 18 randomly selected\nstocks, when Absolute value and Higuchi's estimators are implemented. The burst\nduration analysis based on the first passage problem of time series and\nimplemented in this research gives different estimates of the Hurst parameter\nmore consistent with the uncorrelated increment cases.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 13:51:54 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Gontis", "Vygintas", ""]]}, {"id": "2105.02728", "submitter": "Gerard de Melo", "authors": "Tolga Buz, Gerard de Melo", "title": "Should You Take Investment Advice From WallStreetBets? A Data-Driven\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reddit's WallStreetBets (WSB) community has come to prominence in light of\nits notable role in affecting the stock prices of what are now referred to as\nmeme stocks. Yet very little is known about the reliability of the highly\nspeculative investment advice disseminated on WSB. This paper analyses WSB data\nspanning from January 2019 to April 2021 in order to assess how successful an\ninvestment strategy relying on the community's recommendations could have been.\nWe detect buy and sell advice and identify the community's most popular stocks,\nbased on which we define a WSB portfolio. Our evaluation shows that this\nportfolio has grown approx. 200% over the last three years and approx. 480%\nover the last year, significantly outperforming the S&P500. The average\nshort-term accuracy of buy and sell signals, in contrast, is not found to be\nsignificantly better than randomly or equally distributed buy decisions within\nthe same time frame. However, we present a technique for estimating whether\nposts are proactive as opposed to reactive and show that by focusing on a\nsubset of more promising buy signals, a trader could have made investments\nyielding higher returns than the broader market or the strategy of trusting all\nposted buy signals. Lastly, the analysis is also conducted specifically for the\nperiod before 2021 in order to factor out the effects of the GameStop hype of\nJanuary 2021 - the results confirm the conclusions and suggest that the 2021\nhype merely amplified pre-existing characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 14:47:03 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Buz", "Tolga", ""], ["de Melo", "Gerard", ""]]}, {"id": "2105.02785", "submitter": "Navid Mottaghi", "authors": "Navid Mottaghi and Sara Farhangdoost", "title": "Stock Price Forecasting in Presence of Covid-19 Pandemic and Evaluating\n  Performances of Machine Learning Models for Time-Series Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the heightened volatility in stock prices during the Covid-19 pandemic,\nthe need for price forecasting has become more critical. We investigated the\nforecast performance of four models including Long-Short Term Memory, XGBoost,\nAutoregression, and Last Value on stock prices of Facebook, Amazon, Tesla,\nGoogle, and Apple in COVID-19 pandemic time to understand the accuracy and\npredictability of the models in this highly volatile time region. To train the\nmodels, the data of all stocks are split into train and test datasets. The test\ndataset starts from January 2020 to April 2021 which covers the COVID-19\npandemic period. The results show that the Autoregression and Last value models\nhave higher accuracy in predicting the stock prices because of the strong\ncorrelation between the previous day and the next day's price value.\nAdditionally, the results suggest that the machine learning models (Long-Short\nTerm Memory and XGBoost) are not performing as well as Autoregression models\nwhen the market experiences high volatility.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 14:55:57 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Mottaghi", "Navid", ""], ["Farhangdoost", "Sara", ""]]}, {"id": "2105.04131", "submitter": "Geoffrey Ducournau", "authors": "Geoffrey Ducournau", "title": "Symbol Dynamics, Information theory and Complexity of Economic time\n  series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to examine the predictability and the complexity characteristics\nof the Standard&Poor500 dynamics behaviors in a coarse-grained way using the\nsymbolic dynamics method and under the prism of the Information theory through\nthe concept of entropy and uncertainty. We believe that experimental\nmeasurement of entropy as a way of examining the complexity of a system is more\nrelevant than more common tests of universality in the transition to chaos\nbecause it does not make any prior prejudices on the underlying causes\nassociated with the system dynamics, whether deterministic or stochastic. We\nregard the studied economic time series as being complex and propose to express\nit in terms of the amount of information this last is producing on different\ntime scales and according to various scaling parameters.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:19:00 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ducournau", "Geoffrey", ""]]}, {"id": "2105.04171", "submitter": "Geoffrey Ducournau", "authors": "Geoffrey Ducournau", "title": "Bayesian inference and superstatistics to describe long memory processes\n  of financial time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the standardized features of financial data is that log-returns are\nuncorrelated, but absolute log-returns or their squares namely the fluctuating\nvolatility are correlated and is characterized by heavy tailed in the sense\nthat some moment of the absolute log-returns is infinite and typically\nnon-Gaussian [20]. And this last characteristic change accordantly to different\ntimescales. We propose to model this long-memory phenomenon by superstatistical\ndynamics and provide a Bayesian Inference methodology drawing on\nMetropolis-Hasting random walk sampling to determine which superstatistics\namong inverse-Gamma and log-Normal describe the best log-returns complexity on\ndifferent timescales, from high to low frequency. We show that on smaller\ntimescales (minutes) even though the Inverse-Gamma superstatistics works the\nbest, the log-Normal model remains very reliable and suitable to fit the\nabsolute log-returns probability density distribution with strong capacity of\ndescribing heavy tails and power law decays. On larger timescales (daily), we\nshow in terms of Bayes factor that the inverse-Gamma superstatistics is\npreferred to the log-Normal model. We also show evidence of a transition of\nstatistics from power law decay on small timescales to exponential decay on\nlarge scale with less heavy tails meaning that on larger time scales the\nfluctuating volatility tend to be memoryless, consequently superstatistics\nbecomes less relevant.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 08:04:41 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ducournau", "Geoffrey", ""]]}, {"id": "2105.06584", "submitter": "Bruno Levy", "authors": "Bruno P. C. Levy, Hedibert F. Lopes", "title": "Dynamic Portfolio Allocation in High Dimensions using Sparse Risk\n  Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fast and flexible method to scale multivariate return volatility\npredictions up to high-dimensions using a dynamic risk factor model. Our\napproach increases parsimony via time-varying sparsity on factor loadings and\nis able to sequentially learn the use of constant or time-varying parameters\nand volatilities. We show in a dynamic portfolio allocation problem with 455\nstocks from the S&P 500 index that our dynamic risk factor model is able to\nproduce more stable and sparse predictions, achieving not just considerable\nportfolio performance improvements but also higher utility gains for the\nmean-variance investor compared to the traditional Wishart benchmark and the\npassive investment on the market index.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 23:12:49 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Levy", "Bruno P. C.", ""], ["Lopes", "Hedibert F.", ""]]}, {"id": "2105.08133", "submitter": "Tim Leung", "authors": "Tim Leung and Theodore Zhao", "title": "Adaptive Complementary Ensemble EMD and Energy-Frequency Spectra of\n  Cryptocurrency Prices", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the price dynamics of cryptocurrencies using adaptive complementary\nensemble empirical mode decomposition (ACE-EMD) and Hilbert spectral analysis.\nThis is a multiscale noise-assisted approach that decomposes any time series\ninto a number of intrinsic mode functions, along with the corresponding\ninstantaneous amplitudes and instantaneous frequencies. The decomposition is\nadaptive to the time-varying volatility of each cryptocurrency price evolution.\nDifferent combinations of modes allow us to reconstruct the time series using\ncomponents of different timescales. We then apply Hilbert spectral analysis to\ndefine and compute the instantaneous energy-frequency spectrum of each\ncryptocurrency to illustrate the properties of various timescales embedded in\nthe original time series.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 19:53:45 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Leung", "Tim", ""], ["Zhao", "Theodore", ""]]}, {"id": "2105.09140", "submitter": "Matthieu Garcin", "authors": "Matthieu Garcin", "title": "Forecasting with fractional Brownian motion: a financial perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.PM q-fin.ST q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fractional Brownian motion (fBm) extends the standard Brownian motion by\nintroducing some dependence between non-overlapping increments. Consequently,\nif one considers for example that log-prices follow an fBm, one can exploit the\nnon-Markovian nature of the fBm to forecast future states of the process and\nmake statistical arbitrages. We provide new insights into forecasting an fBm,\nby proposing theoretical formulas for accuracy metrics relevant to a systematic\ntrader, from the hit ratio to the expected gain and risk of a simple strategy.\nIn addition, we answer some key questions about optimizing trading strategies\nin the fBm framework: Which lagged increments of the fBm, observed in discrete\ntime, are to be considered? If the predicted increment is close to zero, up to\nwhich threshold is it more profitable not to invest? We also propose empirical\napplications on high-frequency FX rates, as well as on realized volatility\nseries, exploring the rough volatility concept in a forecasting perspective.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 13:59:52 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 08:26:11 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Garcin", "Matthieu", ""]]}, {"id": "2105.09473", "submitter": "Diakarya Barro Pr", "authors": "Dodo Natatou Moutari, Hassane Abba Mallam, Diakarya Barro, Bisso Saley", "title": "Dependence Modeling and Risk Assessment of a Financial Portfolio with\n  ARMA-APARCH-EVT models based on HACs", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study aims to widen the sphere of pratical applicability of the HAC\nmodel combined with the ARMA-APARCH volatility forecast model and the extreme\nvalues theory. A sequential process of modeling of the VaR of a portfolio based\non the ARMA-APARCH-EVT-HAC model was discussed. The empirical analysis\nconducted with data from international stock market indices clearly illustrates\nthe performance and accuracy of modeling based on HACs.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:30:19 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Moutari", "Dodo Natatou", ""], ["Mallam", "Hassane Abba", ""], ["Barro", "Diakarya", ""], ["Saley", "Bisso", ""]]}, {"id": "2105.10306", "submitter": "Honggao Cao", "authors": "Feng Zhang, Xi Wang and Honggao Cao", "title": "Turnover-Adjusted Information Ratio", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we study the behavior of information ratio (IR) as determined\nby the fundamental law of active investment management. We extend the classic\nrelationship between IR and its two determinants (i.e., information coefficient\nand investment \"breadth\") by explicitly and simultaneously taking into account\nthe volatility of IC and the cost from portfolio turnover. Through mathematical\nderivations and simulations, we show that - for both mean-variance and quintile\nportfolios - a turnover-adjusted IR is always lower than an IR that ignores the\ncost from turnover; more importantly, we find that, contrary to the implication\nfrom the fundamental low but consistent with available empirical evidence,\ninvestment managers may improve their investment performance or IR by\nlimiting/optimizing trade or portfolio turnover.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 22:12:40 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Zhang", "Feng", ""], ["Wang", "Xi", ""], ["Cao", "Honggao", ""]]}, {"id": "2105.10599", "submitter": "Diakarya Barro Pr", "authors": "Hassane Abba Mallam, Diakarya Barro, Yameogo WendKouni and Bisso Saley", "title": "Pricing multivariate european equity option using gaussian mixture\n  distributions and evt-based copulas", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.ST", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article, we present an approach which allows to take into account the\neffect of extreme values in the modeling of financial asset returns and in the\nvalorisation of associeted options. Specifically, the marginal distribution of\nassets returns is modeled by a mixture of two gaussiens distributions.\nMoreover, we model the joint dependence structure of the returns using an\nextremal copula which is suitable for our financial data. Applications are made\non the Atos and Dassault Systems actions of the CAC40 index. Monte-Carlo method\nis used to compute the values of some equity options: the call on maximum, the\ncall on minimum, the digital option and the spreads option with the basket\n(Atos, Dassault systems).\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 23:05:26 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Mallam", "Hassane Abba", ""], ["Barro", "Diakarya", ""], ["WendKouni", "Yameogo", ""], ["Saley", "Bisso", ""]]}, {"id": "2105.10866", "submitter": "Zeinab Rouhollahi", "authors": "Zeinab Rouhollahi", "title": "Towards Artificial Intelligence Enabled Financial Crime Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, financial institutes have been dealing with an increase in\nfinancial crimes. In this context, financial services firms started to improve\ntheir vigilance and use new technologies and approaches to identify and predict\nfinancial fraud and crime possibilities. This task is challenging as\ninstitutions need to upgrade their data and analytics capabilities to enable\nnew technologies such as Artificial Intelligence (AI) to predict and detect\nfinancial crimes. In this paper, we put a step towards AI-enabled financial\ncrime detection in general and money laundering detection in particular to\naddress this challenge. We study and analyse the recent works done in financial\ncrime detection and present a novel model to detect money laundering cases with\nminimum human intervention needs.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 06:57:25 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Rouhollahi", "Zeinab", ""]]}, {"id": "2105.10871", "submitter": "Theodore Zhao", "authors": "Tim Leung, Theodore Zhao", "title": "Financial Time Series Analysis and Forecasting with HHT Feature\n  Generation and Machine Learning", "comments": "28 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:2105.08133", "journal-ref": "Appl Stochastic Models Bus Ind. 2021; 1 - 24", "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the method of complementary ensemble empirical mode decomposition\n(CEEMD) and Hilbert-Huang transform (HHT) for analyzing nonstationary financial\ntime series. This noise-assisted approach decomposes any time series into a\nnumber of intrinsic mode functions, along with the corresponding instantaneous\namplitudes and instantaneous frequencies. Different combinations of modes allow\nus to reconstruct the time series using components of different timescales. We\nthen apply Hilbert spectral analysis to define and compute the associated\ninstantaneous energy-frequency spectrum to illustrate the properties of various\ntimescales embedded in the original time series. Using HHT, we generate a\ncollection of new features and integrate them into machine learning models,\nsuch as regression tree ensemble, support vector machine (SVM), and long\nshort-term memory (LSTM) neural network. Using empirical financial data, we\ncompare several HHT-enhanced machine learning models in terms of forecasting\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 07:26:52 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Leung", "Tim", ""], ["Zhao", "Theodore", ""]]}, {"id": "2105.11053", "submitter": "Sheng (Victor) Wang", "authors": "Samuel N. Cohen and Christoph Reisinger and Sheng Wang", "title": "Arbitrage-free neural-SDE market models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP math.PR q-fin.RM q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling joint dynamics of liquid vanilla options is crucial for\narbitrage-free pricing of illiquid derivatives and managing risks of option\ntrade books. This paper develops a nonparametric model for the European options\nbook respecting underlying financial constraints and while being practically\nimplementable. We derive a state space for prices which are free from static\n(or model-independent) arbitrage and study the inference problem where a model\nis learnt from discrete time series data of stock and option prices. We use\nneural networks as function approximators for the drift and diffusion of the\nmodelled SDE system, and impose constraints on the neural nets such that\nno-arbitrage conditions are preserved. In particular, we give methods to\ncalibrate \\textit{neural SDE} models which are guaranteed to satisfy a set of\nlinear inequalities. We validate our approach with numerical experiments using\ndata generated from a Heston stochastic local volatility model.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 00:53:10 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Cohen", "Samuel N.", ""], ["Reisinger", "Christoph", ""], ["Wang", "Sheng", ""]]}, {"id": "2105.11679", "submitter": "Susanna Manrubia Dr", "authors": "Dami\\'an H. Zanette and Susanna Manrubia", "title": "Fat Tails and Black Swans: Exact Results for Multiplicative Processes\n  with Resets", "comments": "10 pages, 6 figures", "journal-ref": "Chaos 30, 033104 (2020)", "doi": "10.1063/1.5141837", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of multiplicative processes which, added with stochastic\nreset events, give origin to stationary distributions with power-law tails --\nubiquitous in the statistics of social, economic, and ecological systems. Our\nmain goal is to provide a series of exact results on the dynamics and\nasymptotic behaviour of increasingly complex versions of a basic multiplicative\nprocess with resets, including discrete and continuous-time variants and\nseveral degrees of randomness in the parameters that control the process. In\nparticular, we show how the power-law distributions are built up as time\nelapses, how their moments behave with time, and how their stationary profiles\nbecome quantitatively determined by those parameters. Our discussion emphasizes\nthe connection with financial systems, but these stochastic processes are also\nexpected to be fruitful in modeling a wide variety of social and biological\nphenomena.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 05:35:30 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Zanette", "Dami\u00e1n H.", ""], ["Manrubia", "Susanna", ""]]}, {"id": "2105.13898", "submitter": "Jaydip Sen", "authors": "Jaydip Sen, Sidra Mehtab, Abhishek Dutta", "title": "Volatility Modeling of Stocks from Selected Sectors of the Indian\n  Economy Using GARCH", "comments": "This paper is the accepted version of our paper in the IEEE Asian\n  Conference on Innovation Technology (IEEE ASIANCON'2021) which will be\n  organized in Pune, INDIA during August 28 - 29, 2021. The paper consists of 8\n  pages and it contains 13 figures and 22 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volatility clustering is an important characteristic that has a significant\neffect on the behavior of stock markets. However, designing robust models for\naccurate prediction of future volatilities of stock prices is a very\nchallenging research problem. We present several volatility models based on\ngeneralized autoregressive conditional heteroscedasticity (GARCH) framework for\nmodeling the volatility of ten stocks listed in the national stock exchange\n(NSE) of India. The stocks are selected from the auto sector and the banking\nsector of the Indian economy, and they have a significant impact on the\nsectoral index of their respective sectors in the NSE. The historical stock\nprice records from Jan 1, 2010, to Apr 30, 2021, are scraped from the Yahoo\nFinance website using the DataReader API of the Pandas module in the Python\nprogramming language. The GARCH modules are built and fine-tuned on the\ntraining data and then tested on the out-of-sample data to evaluate the\nperformance of the models. The analysis of the results shows that asymmetric\nGARCH models yield more accurate forecasts on the future volatility of stocks.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 14:59:40 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sen", "Jaydip", ""], ["Mehtab", "Sidra", ""], ["Dutta", "Abhishek", ""]]}, {"id": "2105.14193", "submitter": "Larry Lacey", "authors": "Laurence F Lacey", "title": "Characterization of the probability and information entropy of a process\n  with an exponentially increasing sample space and its application to the\n  Broad Money Supply", "comments": "26 pages, 17 figures, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.EM math.PR stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There is a random variable (X) with a determined outcome (i.e., X = x0),\np(x0) = 1. Consider x0 to have a discrete uniform distribution over the integer\ninterval [1, s], where the size of the sample space (s) = 1, in the initial\nstate, such that p(x0) = 1. What is the probability of x0 and the associated\ninformation entropy (H), as s increases exponentially? If the sample space\nexpansion occurs at an exponential rate (rate constant = lambda) with time (t)\nand applying time scaling, such that T = lambda x t, gives: p(x0|T)=exp(-T) and\nH(T)=T. The characterization has also been extended to include exponential\nexpansion by means of simultaneous, independent processes, as well as the more\ngeneral multi-exponential case. The methodology was applied to the expansion of\nthe broad money supply of US$ over the period 2001-2019, as a real-world\nexample. At any given time, the information entropy is related to the rate at\nwhich the sample space is expanding. In the context of the expansion of the\nbroad money supply, the information entropy could be considered to be related\nto the \"velocity\" of the expansion of the money supply.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:31:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lacey", "Laurence F", ""]]}, {"id": "2105.14194", "submitter": "Nikolay Ivanov", "authors": "Nikolay Ivanov and Qiben Yan", "title": "Constraint-Based Inference of Heuristics for Foreign Exchange Trade\n  Model Optimization", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI cs.CE cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Foreign Exchange (Forex) is a large decentralized market, on which\ntrading analysis and algorithmic trading are popular. Research efforts have\nbeen focusing on proof of efficiency of certain technical indicators. We\ndemonstrate, however, that the values of indicator functions are not\nreproducible and often reduce the number of trade opportunities, compared to\nprice-action trading.\n  In this work, we develop two dataset-agnostic Forex trading heuristic\ntemplates with high rate of trading signals. In order to determine most optimal\nparameters for the given heuristic prototypes, we perform a machine learning\nsimulation of 10 years of Forex price data over three low-margin instruments\nand 6 different OHLC granularities. As a result, we develop a specific and\nreproducible list of most optimal trade parameters found for each\ninstrument-granularity pair, with 118 pips of average daily profit for the\noptimized configuration.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 00:36:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ivanov", "Nikolay", ""], ["Yan", "Qiben", ""]]}]