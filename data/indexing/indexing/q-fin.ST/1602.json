[{"id": "1602.00125", "submitter": "Wei-Xing Zhou", "authors": "Rui-Qi Han (ECUST), Wen-Jie Xie (ECUST), Xiong Xiong (TJU), Wei Zhang\n  (TJU), Wei-Xing Zhou (ECUST)", "title": "Market correlation structure changes around the Great Crash", "comments": "6 pages including 5 figures", "journal-ref": "Fluctuation and Noise Letters 16 (2), 1750018 (2017)", "doi": "10.1142/S0219477517500183", "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We perform a comparative analysis of the Chinese stock market around the\noccurrence of the 2008 crisis based on the random matrix analysis of\nhigh-frequency stock returns of 1228 stocks listed on the Shanghai and Shenzhen\nstock exchanges. Both raw correlation matrix and partial correlation matrix\nwith respect to the market index in two time periods of one year are\ninvestigated. We find that the Chinese stocks have stronger average correlation\nand partial correlation in 2008 than in 2007 and the average partial\ncorrelation is significantly weaker than the average correlation in each\nperiod. Accordingly, the largest eigenvalue of the correlation matrix is\nremarkably greater than that of the partial correlation matrix in each period.\nMoreover, each largest eigenvalue and its eigenvector reflect an evident market\neffect, while other deviating eigenvalues do not. We find no evidence that\ndeviating eigenvalues contain industrial sectorial information. Surprisingly,\nthe eigenvectors of the second largest eigenvalues in 2007 and of the third\nlargest eigenvalues in 2008 are able to distinguish the stocks from the two\nexchanges. We also find that the component magnitudes of the some largest\neigenvectors are proportional to the stocks' capitalizations.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jan 2016 14:44:56 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Han", "Rui-Qi", "", "ECUST"], ["Xie", "Wen-Jie", "", "ECUST"], ["Xiong", "Xiong", "", "TJU"], ["Zhang", "Wei", "", "TJU"], ["Zhou", "Wei-Xing", "", "ECUST"]]}, {"id": "1602.00629", "submitter": "Alessandro Stringhi", "authors": "Alessandro Stringhi, Silvia Figini", "title": "How to improve accuracy for DFA technique", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper extends the existing literature on empirical estimation of the\nconfidence intervals associated to the Detrended Fluctuation Analysis (DFA). We\nused Montecarlo simulation to evaluate the confidence intervals. Varying the\nparameters in DFA technique, we point out the relationship between those and\nthe standard deviation of H. The parameters considered are the finite time\nlength L, the number of divisors d used and the values of those. We found that\nall these parameters play a crucial role, determining the accuracy of the\nestimation of H.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2016 18:31:22 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Stringhi", "Alessandro", ""], ["Figini", "Silvia", ""]]}, {"id": "1602.00865", "submitter": "Johannes Rauch", "authors": "Johannes Rauch, Carol Alexander", "title": "Tail Risk Premia for Long-Term Equity Investors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PR q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the P&L on a particular class of swaps, representing variance and\nhigher moments for log returns, as estimators in our empirical study on the\nS&P500 that investigates the factors determining variance and higher-moment\nrisk premia. This class is the discretisation invariant sub-class of swaps with\nNeuberger's aggregating characteristics. Besides the market excess return,\nmomentum is the dominant driver for both skewness and kurtosis risk premia,\nwhich exhibit a highly significant negative correlation. By contrast, the\nvariance risk premium responds positively to size and negatively to growth, and\nthe correlation between variance and tail risk premia is relatively low\ncompared with previous research, particularly at high sampling frequencies.\nThese findings extend prior research on determinants of these risk premia.\nFurthermore, our meticulous data-construction methodology avoids unwanted\nartefacts which distort results.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2016 10:27:14 GMT"}], "update_date": "2016-02-03", "authors_parsed": [["Rauch", "Johannes", ""], ["Alexander", "Carol", ""]]}, {"id": "1602.01960", "submitter": "Emre Kahraman", "authors": "Emre Kahraman and Gazanfer \\\"Unal", "title": "Multiple Wavelet Coherency Analysis and Forecasting of Metal Prices", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assessment of co-movement among metals is crucial to better understand\nthe behaviors of the metal prices and the interactions with others that affect\nthe changes in prices. In this study, both Wavelet Analysis and VARMA (Vector\nAutoregressive Moving Average) models are utilized. First, Multiple Wavelet\nCoherence (MWC), where Wavelet Analysis is needed, is utilized to determine\ndynamic correlation time interval and scales. VARMA is then used for\nforecasting which results in reduced errors.\n  The daily prices of steel, aluminium, copper and zinc between 10.05.2010 and\n29.05.2014 are analyzed via wavelet analysis to highlight the interactions.\nResults uncover interesting dynamics between mentioned metals in the\ntime-frequency space. VARMA (1,1) model forecasting is carried out considering\nthe daily prices between 14.11.2011 and 16.11.2012 where the interactions are\nquite high and prediction errors are found quite limited with respect to\nARMA(1.1). It is shown that dynamic co-movement detection via four variables\nwavelet coherency analysis in the determination of VARMA time interval enables\nto improve forecasting power of ARMA by decreasing forecasting errors.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 09:22:12 GMT"}], "update_date": "2016-02-08", "authors_parsed": [["Kahraman", "Emre", ""], ["\u00dcnal", "Gazanfer", ""]]}, {"id": "1602.02185", "submitter": "Michael Ho", "authors": "Michael Ho, Jack Xin", "title": "Sparse Kalman Filtering Approaches to Covariance Estimation from High\n  Frequency Data in the Presence of Jumps", "comments": null, "journal-ref": "J. Math. Program. (2019)", "doi": "10.1007/s10107-019-01371-6", "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the covariance matrix of asset returns from high frequency data\nis complicated by asynchronous returns, market mi- crostructure noise and\njumps. One technique for addressing both asynchronous returns and market\nmicrostructure is the Kalman-EM (KEM) algorithm. However the KEM approach\nassumes log-normal prices and does not address jumps in the return process\nwhich can corrupt estimation of the covariance matrix.\n  In this paper we extend the KEM algorithm to price models that include jumps.\nWe propose two sparse Kalman filtering approaches to this problem. In the first\napproach we develop a Kalman Expectation Conditional Maximization (KECM)\nalgorithm to determine the un- known covariance as well as detecting the jumps.\nFor this algorithm we consider Laplace and the spike and slab jump models, both\nof which promote sparse estimates of the jumps. In the second method we take a\nBayesian approach and use Gibbs sampling to sample from the posterior\ndistribution of the covariance matrix under the spike and slab jump model.\nNumerical results using simulated data show that each of these approaches\nprovide for improved covariance estima- tion relative to the KEM method in a\nvariety of settings where jumps occur.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2016 23:00:32 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 23:42:08 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Ho", "Michael", ""], ["Xin", "Jack", ""]]}, {"id": "1602.03271", "submitter": "Omar Rojas", "authors": "Semei Coronado and Omar Rojas", "title": "A study of co-movements between oil price, stock index and exchange rate\n  under a cross-bicorrelation perspective: the case of Mexico", "comments": "14 pages, accepted to be published in the book Modelado de\n  Fen\\'omenos Econ\\'omicos y Financieros: Una Visi\\'on Contempor\\'anea, Vol. 1\n  (C.E. Castillo Ram\\'irez, F. L\\'opez Herrera and F. Venegas Mart\\'inez\n  (eds.))", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this chapter we studied the nonlinear co-movements between the Mexican\nCrude Oil price, the Mexican Stock Market Index and the USD/MXN Exchange Rate,\nfor the sample period from 1994 to date. We used a battery of nonlinear tests,\ncf. (Patterson & Ashley, 2000) and one multivariate test, in order to determine\nthe dynamic co-movement exerted from the oil prices to the stock and exchange\nrate markets. Such co-movement and time windows are exposed using the Brooks &\nHinich (1999) cross- bicorrelation statistical test. The effects of oil spills\non other markets have been studied from different angles and on several\nfinancial assets. In this study, we focus our attention on the detection, not\nonly of the correlations amongst markets but on the epochs in which such\nnonlinear dependence might occur. This is important in order to understand\nbetter, how the markets that drive the economy interact with each other. We\nhope to contribute to the literature with such findings, filling a gap in the\nemerging markets context, in particular, for the Mexican case.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2016 05:30:36 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Coronado", "Semei", ""], ["Rojas", "Omar", ""]]}, {"id": "1602.03944", "submitter": "Ioane Muni Toke", "authors": "Ioane Muni Toke and Nakahiro Yoshida", "title": "Modelling intensities of order flows in a limit order book", "comments": "30 pages, 15 figures, 4 tables", "journal-ref": "Quantitative Finance, 17(5), 683-701 (2017)", "doi": "10.1080/14697688.2016.1236210", "report-no": null, "categories": "q-fin.ST q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parametric model for the simulation of limit order books. We\nassume that limit orders, market orders and cancellations are submitted\naccording to point processes with state-dependent intensities. We propose new\nfunctional forms for these intensities, as well as new models for the placement\nof limit orders and cancellations. For cancellations, we introduce the concept\nof \"priority index\" to describe the selection of orders to be cancelled in the\norder book. Parameters of the model are estimated using likelihood\nmaximization. We illustrate the performance of the model by providing extensive\nsimulation results, with a comparison to empirical data and a standard Poisson\nreference.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2016 01:45:28 GMT"}], "update_date": "2019-07-15", "authors_parsed": [["Toke", "Ioane Muni", ""], ["Yoshida", "Nakahiro", ""]]}, {"id": "1602.05323", "submitter": "J\\\"orn Sass", "authors": "Vikram Krishnamurthy, Elisabeth Leoff, J\\\"orn Sass", "title": "Filterbased Stochastic Volatility in Continuous-Time Hidden Markov\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regime-switching models, in particular Hidden Markov Models (HMMs) where the\nswitching is driven by an unobservable Markov chain, are widely-used in\nfinancial applications, due to their tractability and good econometric\nproperties. In this work we consider HMMs in continuous time with both constant\nand switching volatility. In the continuous-time model with switching\nvolatility the underlying Markov chain could be observed due to this stochastic\nvolatility, and no estimation (filtering) of it is needed (in theory), while in\nthe discretized model or the model with constant volatility one has to filter\nfor the underlying Markov chain. The motivations for continuous-time models are\nexplicit computations in finance. To have a realistic model with unobservable\nMarkov chain in continuous time and good econometric properties we introduce a\nregime-switching model where the volatility depends on the filter for the\nunderlying chain and state the filtering equations. We prove an approximation\nresult for a fixed information filtration and further motivate the model by\nconsidering social learning arguments. We analyze its relation to the switching\nvolatility model and present a convergence result for the discretized model. We\nthen illustrate its econometric properties by considering numerical\nsimulations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 07:12:04 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Krishnamurthy", "Vikram", ""], ["Leoff", "Elisabeth", ""], ["Sass", "J\u00f6rn", ""]]}, {"id": "1602.05385", "submitter": "Ladislav Kristoufek", "authors": "Ladislav Kristoufek", "title": "Power-law cross-correlations estimation under heavy tails", "comments": "19 pages, 6 figures", "journal-ref": "Communications in Nonlinear Science and Numerical Simulation,\n  Volume 40, November 2016, Pages 163-172", "doi": "10.1016/j.cnsns.2016.04.010", "report-no": null, "categories": "q-fin.ST physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We examine the performance of six estimators of the power-law\ncross-correlations -- the detrended cross-correlation analysis, the detrending\nmoving-average cross-correlation analysis, the height cross-correlation\nanalysis, the averaged periodogram estimator, the cross-periodogram estimator\nand the local cross-Whittle estimator -- under heavy-tailed distributions. The\nselection of estimators allows to separate these into the time and frequency\ndomain estimators. By varying the characteristic exponent of the\n$\\alpha$-stable distributions which controls the tails behavior, we report\nseveral interesting findings. First, the frequency domain estimators are\npractically unaffected by heavy tails bias-wise. Second, the time domain\nestimators are upward biased for heavy tails but they have lower estimator\nvariance than the other group for short series. Third, specific estimators are\nmore appropriate depending on distributional properties and length of the\nanalyzed series. In addition, we provide a discussion of implications of these\nresults for empirical applications as well as theoretical explanations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 12:08:49 GMT"}, {"version": "v2", "created": "Wed, 20 Apr 2016 05:46:21 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Kristoufek", "Ladislav", ""]]}, {"id": "1602.05489", "submitter": "Jozef Barunik", "authors": "Jozef Barunik and Lukas Vacha", "title": "Do co-jumps impact correlations in currency markets?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify how co-jumps impact correlations in currency markets. To\ndisentangle the continuous part of quadratic covariation from co-jumps, and\nstudy the influence of co-jumps on correlations, we propose a new wavelet-based\nestimator. The proposed estimation framework is able to localize the co-jumps\nvery precisely through wavelet coefficients and identify statistically\nsignificant co-jumps. Empirical findings reveal the different behaviors of\nco-jumps during Asian, European and U.S. trading sessions. Importantly, we\ndocument that co-jumps significantly influence correlation in currency markets.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2016 17:13:28 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 10:19:47 GMT"}, {"version": "v3", "created": "Sun, 15 Oct 2017 08:27:22 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Barunik", "Jozef", ""], ["Vacha", "Lukas", ""]]}, {"id": "1602.05749", "submitter": "Stavros Stavroyiannis", "authors": "Stavros Stavroyiannis", "title": "Value-at-Risk and backtesting with the APARCH model and the standardized\n  Pearson type IV distribution", "comments": "21 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the efficiency of the Asymmetric Power ARCH (APARCH) model in the\ncase where the residuals follow the standardized Pearson type IV distribution.\nThe model is tested with a variety of loss functions and the efficiency is\nexamined via application of several statistical tests and risk measures. The\nresults indicate that the APARCH model with the standardized Pearson type IV\ndistribution is accurate, within the general financial risk modeling\nperspective, providing the financial analyst with an additional skewed\ndistribution for incorporation in the risk management tools.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 10:34:29 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Stavroyiannis", "Stavros", ""]]}, {"id": "1602.05858", "submitter": "Peng Huang", "authors": "Peng Huang, Tianxiang Wang", "title": "On the Profitability of Optimal Mean Reversion Trading Strategies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the profitability of optimal mean reversion trading strategies in\nthe US equity market. Different from regular pair trading practice, we apply\nmaximum likelihood method to construct the optimal static pairs trading\nportfolio that best fits the Ornstein-Uhlenbeck process, and rigorously\nestimate the parameters. Therefore, we ensure that our portfolios match the\nmean-reverting process before trading. We then generate contrarian trading\nsignals using the model parameters. We also optimize the thresholds and the\nlength of in-sample period by multiple tests. In nine good pair examples, we\ncan see that our pairs exhibit high Sharpe ratio (above 1.9) over the in-sample\nperiod and out-of-sample period. In particular, Crown Castle International\nCorp. (CCI) and HCP, Inc. (HCP) achieve a Sharpe ratio of 2.326 during\nin-sample period and a Sharpe ratio of 2.425 in out-of-sample test. Crown\nCastle International Corp. (CCI) and Realty Income Corporation (O) achieve a\nSharpe ratio of 2.405 and 2.903 respectively during in-sample period and\nout-of-sample period.\n", "versions": [{"version": "v1", "created": "Thu, 18 Feb 2016 16:15:31 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Huang", "Peng", ""], ["Wang", "Tianxiang", ""]]}, {"id": "1602.06186", "submitter": "Jakob S\\\"ohl", "authors": "Dirk Paulsen and Jakob S\\\"ohl", "title": "Noise Fit, Estimation Error and a Sharpe Information Criterion", "comments": "38 pages, 7 figures, 1 table", "journal-ref": "Quant. Finance 20(6) (2020) 1027-1043", "doi": "10.1080/14697688.2020.1718746", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the in-sample Sharpe ratio is obtained by optimizing over a\nk-dimensional parameter space, it is a biased estimator for what can be\nexpected on unseen data (out-of-sample). We derive (1) an unbiased estimator\nadjusting for both sources of bias: noise fit and estimation error. We then\nshow (2) how to use the adjusted Sharpe ratio as model selection criterion\nanalogously to the Akaike Information Criterion (AIC). Selecting a model with\nthe highest adjusted Sharpe ratio selects the model with the highest estimated\nout-of-sample Sharpe ratio in the same way as selection by AIC does for the\nlog-likelihood as measure of fit.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2016 15:40:12 GMT"}, {"version": "v2", "created": "Fri, 8 Sep 2017 15:27:53 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 12:54:26 GMT"}, {"version": "v4", "created": "Tue, 1 Oct 2019 17:20:47 GMT"}, {"version": "v5", "created": "Wed, 11 Dec 2019 12:39:18 GMT"}], "update_date": "2020-05-26", "authors_parsed": [["Paulsen", "Dirk", ""], ["S\u00f6hl", "Jakob", ""]]}, {"id": "1602.07599", "submitter": "Jacopo Corbetta", "authors": "Jacopo Corbetta and Ilaria Peri", "title": "Backtesting Lambda Value at Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new risk measure, the lambda value at risk (Lambda VaR), has been recently\nproposed from a theoretical point of view as a generalization of the value at\nrisk (VaR). The Lambda VaR appears attractive for its potential ability to\nsolve several problems of the VaR. In this paper we propose three nonparametric\nbacktesting methodologies for the Lambda VaR which exploit different features.\nTwo of these tests directly assess the correctness of the level of coverage\npredicted by the model. One of these tests is bilateral and provides an\nasymptotic result. A third test assess the accuracy of the Lambda VaR that\ndepends on the choice of the P&L distribution. However, this test requires the\nstorage of more information. Finally, we perform a backtesting exercise and we\ncompare our results with the ones from Hitaj and Peri (2015)\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 17:06:25 GMT"}, {"version": "v2", "created": "Sun, 12 Jun 2016 13:52:36 GMT"}, {"version": "v3", "created": "Fri, 10 Feb 2017 12:57:34 GMT"}, {"version": "v4", "created": "Fri, 2 Jun 2017 14:17:58 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Corbetta", "Jacopo", ""], ["Peri", "Ilaria", ""]]}, {"id": "1602.07663", "submitter": "Marcello Rambaldi", "authors": "Marcello Rambaldi, Emmanuel Bacry, Fabrizio Lillo", "title": "The role of volume in order book dynamics: a multivariate Hawkes process\n  analysis", "comments": null, "journal-ref": null, "doi": "10.1080/14697688.2016.1260759", "report-no": null, "categories": "q-fin.TR q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that multivariate Hawkes processes coupled with the nonparametric\nestimation procedure first proposed in Bacry and Muzy (2015) can be\nsuccessfully used to study complex interactions between the time of arrival of\norders and their size, observed in a limit order book market. We apply this\nmethodology to high-frequency order book data of futures traded at EUREX.\nSpecifically, we demonstrate how this approach is amenable not only to analyze\ninterplay between different order types (market orders, limit orders,\ncancellations) but also to include other relevant quantities, such as the order\nsize, into the analysis, showing also that simple models assuming the\nindependence between volume and time are not suitable to describe the data.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2016 20:22:04 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Rambaldi", "Marcello", ""], ["Bacry", "Emmanuel", ""], ["Lillo", "Fabrizio", ""]]}, {"id": "1602.08258", "submitter": "Guilherme Demos Mr.", "authors": "Vladimir Filimonov, Guilherme Demos and Didier Sornette", "title": "Modified Profile Likelihood Inference and Interval Forecast of the Burst\n  of Financial Bubbles", "comments": "40 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a detailed methodological study of the application of the modified\nprofile likelihood method for the calibration of nonlinear financial models\ncharacterised by a large number of parameters. We apply the general approach to\nthe Log-Periodic Power Law Singularity (LPPLS) model of financial bubbles. This\nmodel is particularly relevant because one of its parameters, the critical time\n$t_c$ signalling the burst of the bubble, is arguably the target of choice for\ndynamical risk management. However, previous calibrations of the LPPLS model\nhave shown that the estimation of $t_c$ is in general quite unstable. Here, we\nprovide a rigorous likelihood inference approach to determine $t_c$, which\ntakes into account the impact of the other nonlinear (so-called \"nuisance\")\nparameters for the correct adjustment of the uncertainty on $t_c$. This\nprovides a rigorous interval estimation for the critical time, rather than a\npoint estimation in previous approaches. As a bonus, the interval estimations\ncan also be obtained for the nuisance parameters ($m,\\omega$, damping), which\ncan be used to improve filtering of the calibration results. We show that the\nuse of the modified profile likelihood method dramatically reduces the number\nof local extrema by constructing much simpler smoother log-likelihood\nlandscapes. The remaining distinct solutions can be interpreted as genuine\nscenarios that unfold as the time of the analysis flows, which can be compared\ndirectly via their likelihood ratio. Finally, we develop a multi-scale profile\nlikelihood analysis to visualize the structure of the financial data at\ndifferent scales (typically from 100 to 750 days). We test the methodology\nsuccessfully on synthetic price time series and on three well-known historical\nfinancial bubbles.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2016 09:51:14 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Filimonov", "Vladimir", ""], ["Demos", "Guilherme", ""], ["Sornette", "Didier", ""]]}]