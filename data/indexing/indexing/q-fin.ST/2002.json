[{"id": "2002.00085", "submitter": "Andrew Papanicolaou", "authors": "Marco Avellaneda, Brian Healy, Andrew Papanicolaou, George\n  Papanicolaou", "title": "PCA for Implied Volatility Surfaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a useful tool when trying to construct\nfactor models from historical asset returns. For the implied volatilities of\nU.S. equities there is a PCA-based model with a principal eigenportfolio whose\nreturn time series lies close to that of an overarching market factor. The\nauthors show that this market factor is the index resulting from the daily\ncompounding of a weighted average of implied-volatility returns, with weights\nbased on the options' open interest (OI) and Vega. The authors also analyze the\nsingular vectors derived from the tensor structure of the implied volatilities\nof S&P500 constituents, and find evidence indicating that some type of OI and\nVega-weighted index should be one of at least two significant factors in this\nmarket.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 22:50:58 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Avellaneda", "Marco", ""], ["Healy", "Brian", ""], ["Papanicolaou", "Andrew", ""], ["Papanicolaou", "George", ""]]}, {"id": "2002.00724", "submitter": "Kei Nakagawa", "authors": "Katsuya Ito, Kei Nakagawa", "title": "NAPLES;Mining the lead-lag Relationship from Non-synchronous and\n  High-frequency Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In time-series analysis, the term \"lead-lag effect\" is used to describe a\ndelayed effect on a given time series caused by another time series. lead-lag\neffects are ubiquitous in practice and are specifically critical in formulating\ninvestment strategies in high-frequency trading. At present, there are three\nmajor challenges in analyzing the lead-lag effects. First, in practical\napplications, not all time series are observed synchronously. Second, the size\nof the relevant dataset and rate of change of the environment is increasingly\nfaster, and it is becoming more difficult to complete the computation within a\nparticular time limit. Third, some lead-lag effects are time-varying and only\nlast for a short period, and their delay lengths are often affected by external\nfactors. In this paper, we propose NAPLES (Negative And Positive lead-lag\nEStimator), a new statistical measure that resolves all these problems. Through\nexperiments on artificial and real datasets, we demonstrate that NAPLES has a\nstrong correlation with the actual lead-lag effects, including those triggered\nby significant macroeconomic announcements.\n", "versions": [{"version": "v1", "created": "Mon, 3 Feb 2020 13:39:41 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Ito", "Katsuya", ""], ["Nakagawa", "Kei", ""]]}, {"id": "2002.01800", "submitter": "Mehmet Caner", "authors": "Mehmet Caner, Marcelo Medeiros, and Gabriel Vasconcelos", "title": "Residual-Based Nodewise Regression in Factor Models with Ultra-High\n  Dimensions: Analysis of Mean-Variance Portfolio Efficiency and Estimation of\n  Out-of-Sample and Constrained Maximum Sharpe Ratios", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM econ.EM math.ST q-fin.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new theory for nodewise regression when the residuals from a\nfitted factor model are used to apply our results to the analysis of the\nmaximum Sharpe ratio when the number of assets in a portfolio is larger than\nits time span. We introduce a new hybrid model where factor models are combined\nwith feasible nodewise regression. Returns are generated from an increasing\nnumber of factors plus idiosyncratic components (errors). The precision matrix\nof the idiosyncratic terms is assumed to be sparse, but the respective\ncovariance matrix can be non-sparse. Since the nodewise regression is not\nfeasible due to the unknown nature of errors, we provide a\nfeasible-residual-based nodewise regression to estimate the precision matrix of\nerrors as a new method. Next, we show that the residual-based nodewise\nregression provides a consistent estimate for the precision matrix of errors.\nIn another new development, we also show that the precision matrix of returns\ncan be estimated consistently, even with an increasing number of factors.\nBenefiting from the consistency of the precision matrix estimate of returns, we\nshow that: (1) the portfolios in high dimensions are mean-variance efficient;\n(2) maximum out-of-sample Sharpe ratio estimator is consistent and the number\nof assets slows the convergence up to a logarithmic factor; (3) the maximum\nSharpe ratio estimator is consistent when the portfolio weights sum to one; and\n(4) the Sharpe ratio estimators are consistent in global minimum-variance and\nmean-variance portfolios.\n", "versions": [{"version": "v1", "created": "Wed, 5 Feb 2020 14:16:30 GMT"}, {"version": "v2", "created": "Mon, 29 Jun 2020 15:27:37 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 19:19:00 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Caner", "Mehmet", ""], ["Medeiros", "Marcelo", ""], ["Vasconcelos", "Gabriel", ""]]}, {"id": "2002.02008", "submitter": "Bryan Lim", "authors": "Bryan Lim, Stefan Zohren, Stephen Roberts", "title": "Detecting Changes in Asset Co-Movement Using the Autoencoder\n  Reconstruction Ratio", "comments": null, "journal-ref": "Risk 2020", "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG q-fin.PM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting changes in asset co-movements is of much importance to financial\npractitioners, with numerous risk management benefits arising from the timely\ndetection of breakdowns in historical correlations. In this article, we propose\na real-time indicator to detect temporary increases in asset co-movements, the\nAutoencoder Reconstruction Ratio, which measures how well a basket of asset\nreturns can be modelled using a lower-dimensional set of latent variables. The\nARR uses a deep sparse denoising autoencoder to perform the dimensionality\nreduction on the returns vector, which replaces the PCA approach of the\nstandard Absorption Ratio, and provides a better model for non-Gaussian\nreturns. Through a systemic risk application on forecasting on the CRSP US\nTotal Market Index, we show that lower ARR values coincide with higher\nvolatility and larger drawdowns, indicating that increased asset co-movement\ndoes correspond with periods of market weakness. We also demonstrate that\nshort-term (i.e. 5-min and 1-hour) predictors for realised volatility and\nmarket crashes can be improved by including additional ARR inputs.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 22:33:54 GMT"}, {"version": "v2", "created": "Sun, 27 Sep 2020 14:14:44 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Lim", "Bryan", ""], ["Zohren", "Stefan", ""], ["Roberts", "Stephen", ""]]}, {"id": "2002.02010", "submitter": "Xixi Li", "authors": "Yun Bai, Xixi Li, Hao Yu, and Suling Jia", "title": "Crude oil price forecasting incorporating news text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse and short news headlines can be arbitrary, noisy, and ambiguous,\nmaking it difficult for classic topic model LDA (latent Dirichlet allocation)\ndesigned for accommodating long text to discover knowledge from them.\nNonetheless, some of the existing research about text-based crude oil\nforecasting employs LDA to explore topics from news headlines, resulting in a\nmismatch between the short text and the topic model and further affecting the\nforecasting performance. Exploiting advanced and appropriate methods to\nconstruct high-quality features from news headlines becomes crucial in crude\noil forecasting. To tackle this issue, this paper introduces two novel\nindicators of topic and sentiment for the short and sparse text data. Empirical\nexperiments show that AdaBoost.RT with our proposed text indicators, with a\nmore comprehensive view and characterization of the short and sparse text data,\noutperforms the other benchmarks. Another significant merit is that our method\nalso yields good forecasting performance when applied to other futures\ncommodities.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 16:58:05 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2020 00:25:20 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 17:22:22 GMT"}, {"version": "v4", "created": "Thu, 1 Jul 2021 16:09:23 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Bai", "Yun", ""], ["Li", "Xixi", ""], ["Yu", "Hao", ""], ["Jia", "Suling", ""]]}, {"id": "2002.02011", "submitter": "Rising Odegua", "authors": "Rising Odegua", "title": "Predicting Bank Loan Default with Extreme Gradient Boosting", "comments": "5 pages, 3 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loan default prediction is one of the most important and critical problems\nfaced by banks and other financial institutions as it has a huge effect on\nprofit. Although many traditional methods exist for mining information about a\nloan application, most of these methods seem to be under-performing as there\nhave been reported increases in the number of bad loans. In this paper, we use\nan Extreme Gradient Boosting algorithm called XGBoost for loan default\nprediction. The prediction is based on a loan data from a leading bank taking\ninto consideration data sets from both the loan application and the demographic\nof the applicant. We also present important evaluation metrics such as\nAccuracy, Recall, precision, F1-Score and ROC area of the analysis. This paper\nprovides an effective basis for loan credit approval in order to identify risky\ncustomers from a large number of loan applications using predictive modeling.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 18:52:10 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Odegua", "Rising", ""]]}, {"id": "2002.02271", "submitter": "Dmitry Efimov", "authors": "Dmitry Efimov, Di Xu, Luyang Kong, Alexey Nefedov and Archana\n  Anandakrishnan", "title": "Using generative adversarial networks to synthesize artificial financial\n  datasets", "comments": null, "journal-ref": "Robust AI in FS 2019 : NeurIPS 2019 Workshop on Robust AI in\n  Financial Services: Data, Fairness, Explainability, Trustworthiness, and\n  Privacy, December 2019, Vancouver, Canada", "doi": null, "report-no": null, "categories": "cs.LG q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) became very popular for generation of\nrealistically looking images. In this paper, we propose to use GANs to\nsynthesize artificial financial data for research and benchmarking purposes. We\ntest this approach on three American Express datasets, and show that properly\ntrained GANs can replicate these datasets with high fidelity. For our\nexperiments, we define a novel type of GAN, and suggest methods for data\npreprocessing that allow good training and testing performance of GANs. We also\ndiscuss methods for evaluating the quality of generated data, and their\ncomparison with the original real data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Feb 2020 14:25:08 GMT"}], "update_date": "2020-02-07", "authors_parsed": [["Efimov", "Dmitry", ""], ["Xu", "Di", ""], ["Kong", "Luyang", ""], ["Nefedov", "Alexey", ""], ["Anandakrishnan", "Archana", ""]]}, {"id": "2002.03319", "submitter": "Diego Garlaschelli", "authors": "Marc van Kralingen, Diego Garlaschelli, Karolina Scholtus, Iman van\n  Lelyveld", "title": "Crowded trades, market clustering, and price instability", "comments": null, "journal-ref": "Entropy 23(3), 336 (2021)", "doi": "10.3390/e23030336", "report-no": "DNB Working Papers 668, Netherlands Central Bank, Research\n  Department", "categories": "q-fin.ST q-fin.RM q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowded trades by similarly trading peers influence the dynamics of asset\nprices, possibly creating systemic risk. We propose a market clustering measure\nusing granular trading data. For each stock the clustering measure captures the\ndegree of trading overlap among any two investors in that stock. We investigate\nthe effect of crowded trades on stock price stability and show that market\nclustering has a causal effect on the properties of the tails of the stock\nreturn distribution, particularly the positive tail, even after controlling for\ncommonly considered risk drivers. Reduced investor pool diversity could thus\nnegatively affect stock price stability.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 08:53:43 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["van Kralingen", "Marc", ""], ["Garlaschelli", "Diego", ""], ["Scholtus", "Karolina", ""], ["van Lelyveld", "Iman", ""]]}, {"id": "2002.04164", "submitter": "Giuseppe Brandi", "authors": "Giuseppe Brandi and T. Di Matteo", "title": "On the statistics of scaling exponents and the Multiscaling Value at\n  Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scaling and multiscaling financial time series have been widely studied in\nthe literature. The research on this topic is vast and still flourishing. One\nway to analyze the scaling properties of time series is through the estimation\nof their scaling exponents, that are recognized as being valuable measures to\ndiscriminate between random, persistent, and anti-persistent behaviors in these\ntime series. In the literature, several methods have been proposed to study the\nmultiscaling property. In this paper, we use the generalized Hurst exponent\n(GHE) tool and we propose a novel statistical procedure based on GHE which we\nname Relative Normalized and Standardized Generalized Hurst Exponent (RNSGHE).\nThis method is used to robustly estimate and test the multiscaling property\nand, together with a combination of t-tests and F-tests, serves to discriminate\nbetween real and spurious scaling. Furthermore, we introduce a new tool to\nestimate the optimal aggregation time used in our methodology which we name\nAutocororrelation Segmented Regression. We numerically validate this procedure\non simulated time series by using the Multifractal Random Walk (MRW) and we\nthen apply it to real financial data. We present results for times series with\nand without anomalies and we compute the bias that such anomalies introduce in\nthe measurement of the scaling exponents. We also show how the use of proper\nscaling and multiscaling can ameliorate the estimation of risk measures such as\nValue at Risk (VaR). Finally, we propose a methodology based on Monte Carlo\nsimulation, which we name Multiscaling Value at Risk (MSVaR), that takes into\naccount the statistical properties of multiscaling time series. We show that by\nusing this statistical procedure in combination with the robustly estimated\nmultiscaling exponents, the one year forecasted MSVaR mimics the VaR on the\nannual data for the majority of the stocks analyzed.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 01:50:42 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2020 15:49:11 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 19:59:07 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Brandi", "Giuseppe", ""], ["Di Matteo", "T.", ""]]}, {"id": "2002.05319", "submitter": "Oscar Espinosa", "authors": "Oscar Espinosa, Fabio Nieto", "title": "A study on the leverage effect on financial series using a TAR model: a\n  Bayesian approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research shows that under certain mathematical conditions, a threshold\nautoregressive model (TAR) can represent the leverage effect based on its\nconditional variance function. Furthermore, the analytical expressions for the\nthird and fourth moment of the TAR model are obtained when it is weakly\nstationary.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 03:00:52 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 03:02:51 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Espinosa", "Oscar", ""], ["Nieto", "Fabio", ""]]}, {"id": "2002.05697", "submitter": "C\\'esar A. Terrero-Escalante", "authors": "L\\'ester Alfonso, Danahe E. Garcia-Ramirez, Ricardo Mansilla, C\\'esar\n  A. Terrero-Escalante", "title": "Analysis of intra-day fluctuations in the Mexican financial market index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a statistical analysis of high frequency fluctuations of the\nIPC, the Mexican Stock Market Index, is presented. A sample of tick-to-tick\ndata covering the period from January 1999 to December 2002 was analyzed, as\nwell as several other sets obtained using temporal aggregation. Our results\nindicates that the highest frequency is not useful to understand the Mexican\nmarket because almost two thirds of the information corresponds to inactivity.\nFor the frequency where fluctuations start to be relevant, the IPC data does\nnot follows any alpha-stable distribution, including the Gaussian, perhaps\nbecause of the presence of autocorrelations. For a long range of\nlower-frequencies, but still in the intra-day regime, fluctuations can be\ndescribed as a truncated L\\'evy flight, while for frequencies above two-days, a\nGaussian distribution yields the best fit. Thought these results are consistent\nwith other previously reported for several markets, there are significant\ndifferences in the details of the corresponding descriptions.\n", "versions": [{"version": "v1", "created": "Thu, 13 Feb 2020 18:23:51 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Alfonso", "L\u00e9ster", ""], ["Garcia-Ramirez", "Danahe E.", ""], ["Mansilla", "Ricardo", ""], ["Terrero-Escalante", "C\u00e9sar A.", ""]]}, {"id": "2002.05784", "submitter": "Lior Sidi", "authors": "Lior Sidi", "title": "Improving S&P stock prediction with time series stock similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Stock market prediction with forecasting algorithms is a popular topic these\ndays where most of the forecasting algorithms train only on data collected on a\nparticular stock. In this paper, we enriched the stock data with related stocks\njust as a professional trader would have done to improve the stock prediction\nmodels. We tested five different similarities functions and found\nco-integration similarity to have the best improvement on the prediction model.\nWe evaluate the models on seven S&P stocks from various industries over five\nyears period. The prediction model we trained on similar stocks had\nsignificantly better results with 0.55 mean accuracy, and 19.782 profit compare\nto the state of the art model with an accuracy of 0.52 and profit of 6.6.\n", "versions": [{"version": "v1", "created": "Sat, 8 Feb 2020 14:13:45 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Sidi", "Lior", ""]]}, {"id": "2002.05786", "submitter": "Murat Ozbayoglu", "authors": "Ahmet Murat Ozbayoglu, Mehmet Ugur Gudelek, Omer Berat Sezer", "title": "Deep Learning for Financial Applications : A Survey", "comments": "13 Figures, 15 Tables, submitted to Applied Soft Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational intelligence in finance has been a very popular topic for both\nacademia and financial industry in the last few decades. Numerous studies have\nbeen published resulting in various models. Meanwhile, within the Machine\nLearning (ML) field, Deep Learning (DL) started getting a lot of attention\nrecently, mostly due to its outperformance over the classical models. Lots of\ndifferent implementations of DL exist today, and the broad interest is\ncontinuing. Finance is one particular area where DL models started getting\ntraction, however, the playfield is wide open, a lot of research opportunities\nstill exist. In this paper, we tried to provide a state-of-the-art snapshot of\nthe developed DL models for financial applications, as of today. We not only\ncategorized the works according to their intended subfield in finance but also\nanalyzed them based on their DL models. In addition, we also aimed at\nidentifying possible future implementations and highlighted the pathway for the\nongoing research within the field.\n", "versions": [{"version": "v1", "created": "Sun, 9 Feb 2020 14:34:56 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Ozbayoglu", "Ahmet Murat", ""], ["Gudelek", "Mehmet Ugur", ""], ["Sezer", "Omer Berat", ""]]}, {"id": "2002.05789", "submitter": "Felipe Tobar", "authors": "Taco de Wolff, Alejandro Cuevas, Felipe Tobar", "title": "Gaussian process imputation of multiple financial series", "comments": "Accepted at IEEE ICASSP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG eess.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Financial Signal Processing, multiple time series such as financial\nindicators, stock prices and exchange rates are strongly coupled due to their\ndependence on the latent state of the market and therefore they are required to\nbe jointly analysed. We focus on learning the relationships among financial\ntime series by modelling them through a multi-output Gaussian process (MOGP)\nwith expressive covariance functions. Learning these market dependencies among\nfinancial series is crucial for the imputation and prediction of financial\nobservations. The proposed model is validated experimentally on two real-world\nfinancial datasets for which their correlations across channels are analysed.\nWe compare our model against other MOGPs and the independent Gaussian process\non real financial data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Feb 2020 19:18:18 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["de Wolff", "Taco", ""], ["Cuevas", "Alejandro", ""], ["Tobar", "Felipe", ""]]}, {"id": "2002.06243", "submitter": "Kei Nakagawa", "authors": "Yusuke Uchiyama, Kei Nakagawa", "title": "TPLVM: Portfolio Construction by Student's $t$-process Latent Variable\n  Model", "comments": null, "journal-ref": null, "doi": "10.3390/math8030449", "report-no": null, "categories": "q-fin.PM cs.LG math.ST q-fin.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal asset allocation is a key topic in modern finance theory. To realize\nthe optimal asset allocation on investor's risk aversion, various portfolio\nconstruction methods have been proposed. Recently, the applications of machine\nlearning are rapidly growing in the area of finance. In this article, we\npropose the Student's $t$-process latent variable model (TPLVM) to describe\nnon-Gaussian fluctuations of financial timeseries by lower dimensional latent\nvariables. Subsequently, we apply the TPLVM to minimum-variance portfolio as an\nalternative of existing nonlinear factor models. To test the performance of the\nproposed portfolio, we construct minimum-variance portfolios of global stock\nmarket indices based on the TPLVM or Gaussian process latent variable model. By\ncomparing these portfolios, we confirm the proposed portfolio outperforms that\nof the existing Gaussian process latent variable model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 02:02:02 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Uchiyama", "Yusuke", ""], ["Nakagawa", "Kei", ""]]}, {"id": "2002.06405", "submitter": "Alexis Marchal", "authors": "Oksana Bashchenko and Alexis Marchal", "title": "Deep Learning for Asset Bubbles Detection", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.MF stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a methodology for detecting asset bubbles using a neural network.\nWe rely on the theory of local martingales in continuous-time and use a deep\nnetwork to estimate the diffusion coefficient of the price process more\naccurately than the current estimator, obtaining an improved detection of\nbubbles. We show the outperformance of our algorithm over the existing\nstatistical method in a laboratory created with simulated data. We then apply\nthe network classification to real data and build a zero net exposure trading\nstrategy that exploits the risky arbitrage emanating from the presence of\nbubbles in the US equity market from 2006 to 2008. The profitability of the\nstrategy provides an estimation of the economical magnitude of bubbles as well\nas support for the theoretical assumptions relied on.\n", "versions": [{"version": "v1", "created": "Sat, 15 Feb 2020 16:16:39 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Bashchenko", "Oksana", ""], ["Marchal", "Alexis", ""]]}, {"id": "2002.06878", "submitter": "Chi Chen", "authors": "Chi Chen, Li Zhao, Wei Cao, Jiang Bian and Chunxiao Xing", "title": "Trimming the Sail: A Second-order Learning Paradigm for Stock Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, machine learning methods have been widely used in stock prediction.\nTraditional approaches assume an identical data distribution, under which a\nlearned model on the training data is fixed and applied directly in the test\ndata. Although such assumption has made traditional machine learning techniques\nsucceed in many real-world tasks, the highly dynamic nature of the stock market\ninvalidates the strict assumption in stock prediction. To address this\nchallenge, we propose the second-order identical distribution assumption, where\nthe data distribution is assumed to be fluctuating over time with certain\npatterns. Based on such assumption, we develop a second-order learning paradigm\nwith multi-scale patterns. Extensive experiments on real-world Chinese stock\ndata demonstrate the effectiveness of our second-order learning paradigm in\nstock prediction.\n", "versions": [{"version": "v1", "created": "Mon, 17 Feb 2020 10:49:46 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Chen", "Chi", ""], ["Zhao", "Li", ""], ["Cao", "Wei", ""], ["Bian", "Jiang", ""], ["Xing", "Chunxiao", ""]]}, {"id": "2002.08849", "submitter": "Minjing Tao", "authors": "Wenjing Wang and Minjing Tao", "title": "Forecasting Realized Volatility Matrix With Copula-Based Models", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate volatility modeling and forecasting are crucial in financial\neconomics. This paper develops a copula-based approach to model and forecast\nrealized volatility matrices. The proposed copula-based time series models can\ncapture the hidden dependence structure of realized volatility matrices. Also,\nthis approach can automatically guarantee the positive definiteness of the\nforecasts through either Cholesky decomposition or matrix logarithm\ntransformation. In this paper we consider both multivariate and bivariate\ncopulas; the types of copulas include Student's t, Clayton and Gumbel copulas.\nIn an empirical application, we find that for one-day ahead volatility matrix\nforecasting, these copula-based models can achieve significant performance both\nin terms of statistical precision as well as creating economically\nmean-variance efficient portfolio. Among the copulas we considered, the\nmultivariate-t copula performs better in statistical precision, while\nbivariate-t copula has better economical performance.\n", "versions": [{"version": "v1", "created": "Thu, 20 Feb 2020 16:36:17 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Wang", "Wenjing", ""], ["Tao", "Minjing", ""]]}, {"id": "2002.09565", "submitter": "Micah Goldblum", "authors": "Micah Goldblum, Avi Schwarzschild, Ankit B. Patel, Tom Goldstein", "title": "Adversarial Attacks on Machine Learning Systems for High-Frequency\n  Trading", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CR q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic trading systems are often completely automated, and deep learning\nis increasingly receiving attention in this domain. Nonetheless, little is\nknown about the robustness properties of these models. We study valuation\nmodels for algorithmic trading from the perspective of adversarial machine\nlearning. We introduce new attacks specific to this domain with size\nconstraints that minimize attack costs. We further discuss how these attacks\ncan be used as an analysis tool to study and evaluate the robustness properties\nof financial models. Finally, we investigate the feasibility of realistic\nadversarial attacks in which an adversarial trader fools automated trading\nsystems into making inaccurate predictions.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 22:04:35 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 18:20:18 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 01:55:01 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Goldblum", "Micah", ""], ["Schwarzschild", "Avi", ""], ["Patel", "Ankit B.", ""], ["Goldstein", "Tom", ""]]}, {"id": "2002.09578", "submitter": "Xiaochun Meng", "authors": "Xiaochun Meng, James W. Taylor, Souhaib Ben Taieb, Siran Li", "title": "Scoring Functions for Multivariate Distributions and Level Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in predicting multivariate probability distributions is growing due\nto the increasing availability of rich datasets and computational developments.\nScoring functions enable the comparison of forecast accuracy, and can\npotentially be used for estimation. A scoring function for multivariate\ndistributions that has gained some popularity is the energy score. This is a\ngeneralization of the continuous ranked probability score (CRPS), which is\nwidely used for univariate distributions. A little-known, alternative\ngeneralization is the multivariate CRPS (MCRPS). We propose a theoretical\nframework for scoring functions for multivariate distributions, which\nencompasses the energy score and MCRPS, as well as the quadratic score, which\nhas also received little attention. We demonstrate how this framework can be\nused to generate new scores. For univariate distributions, it is\nwell-established that the CRPS can be expressed as the integral over a quantile\nscore. We show that, in a similar way, scoring functions for multivariate\ndistributions can be \"disintegrated\" to obtain scoring functions for level\nsets. Using this, we present scoring functions for different types of level\nset, including those for densities and cumulative distributions. To compute the\nscoring functions, we propose a simple numerical algorithm. We illustrate our\nproposals using simulated and stock returns data.\n", "versions": [{"version": "v1", "created": "Fri, 21 Feb 2020 23:56:14 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 23:46:22 GMT"}, {"version": "v3", "created": "Thu, 19 Mar 2020 00:43:56 GMT"}, {"version": "v4", "created": "Mon, 8 Jun 2020 15:06:46 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Meng", "Xiaochun", ""], ["Taylor", "James W.", ""], ["Taieb", "Souhaib Ben", ""], ["Li", "Siran", ""]]}, {"id": "2002.09656", "submitter": "Yifan Yang", "authors": "Yang Yifan, Guo Ju'e, Sun Shaolong, and Li Yixin", "title": "A new hybrid approach for crude oil price forecasting: Evidence from\n  multi-scale data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faced with the growing research towards crude oil price fluctuations\ninfluential factors following the accelerated development of Internet\ntechnology, accessible data such as Google search volume index are increasingly\nquantified and incorporated into forecasting approaches. In this paper, we\napply multi-scale data that including both GSVI data and traditional economic\ndata related to crude oil price as independent variables and propose a new\nhybrid approach for monthly crude oil price forecasting. This hybrid approach,\nbased on divide and conquer strategy, consists of K-means method, kernel\nprincipal component analysis and kernel extreme learning machine , where\nK-means method is adopted to divide input data into certain clusters, KPCA is\napplied to reduce dimension, and KELM is employed for final crude oil price\nforecasting. The empirical result can be analyzed from data and method levels.\nAt the data level, GSVI data perform better than economic data in level\nforecasting accuracy but with opposite performance in directional forecasting\naccuracy because of Herd Behavior, while hybrid data combined their advantages\nand obtain best forecasting performance in both level and directional accuracy.\nAt the method level, the approaches with K-means perform better than those\nwithout K-means, which demonstrates that divide and conquer strategy can\neffectively improve the forecasting performance.\n", "versions": [{"version": "v1", "created": "Sat, 22 Feb 2020 07:56:10 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Yifan", "Yang", ""], ["Ju'e", "Guo", ""], ["Shaolong", "Sun", ""], ["Yixin", "Li", ""]]}, {"id": "2002.09881", "submitter": "Taurai Muvunza", "authors": "Taurai Muvunza", "title": "An $\\alpha$-Stable Approach to Modelling Highly Speculative Assets and\n  Cryptocurrencies", "comments": "LaTeX MikTex 2.9 Texworks, 13 pages, 10 figures (in total). Paper\n  presented at The 4th PKU-NUS International Conference in Quantitative Finance\n  and Economics, 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.MF q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the behaviour of cryptocurrencies' return data. Using return\ndata for bitcoin, ethereum and ripple which account for over 70% of the\ncyrptocurrency market, we demonstrate that $\\alpha$-stable distribution models\nhighly speculative cryptocurrencies more robustly compared to other heavy\ntailed distributions that are used in financial econometrics. We find that the\nMaximum Likelihood Method proposed by DuMouchel (1971) produces estimates that\nfit the cryptocurrency return data much better than the quantile based approach\nof McCulloch (1986) and sample characteristic method by Koutrouvelis (1980).\nThe empirical results show that the leptokurtic feature presented in\ncryptocurrencies' return data can be captured by an ${\\alpha}$-stable\ndistribution. This papers covers predominant literature in cryptocurrencies and\nstable distributions.\n", "versions": [{"version": "v1", "created": "Sun, 23 Feb 2020 10:27:05 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Muvunza", "Taurai", ""]]}, {"id": "2002.10247", "submitter": "Manav Kaushik", "authors": "Manav Kaushik and A K Giri", "title": "Forecasting Foreign Exchange Rate: A Multivariate Comparative Analysis\n  between Traditional Econometric, Contemporary Machine Learning & Deep\n  Learning Techniques", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In todays global economy, accuracy in predicting macro-economic parameters\nsuch as the foreign the exchange rate or at least estimating the trend\ncorrectly is of key importance for any future investment. In recent times, the\nuse of computational intelligence-based techniques for forecasting\nmacroeconomic variables has been proven highly successful. This paper tries to\ncome up with a multivariate time series approach to forecast the exchange rate\n(USD/INR) while parallelly comparing the performance of three multivariate\nprediction modelling techniques: Vector Auto Regression (a Traditional\nEconometric Technique), Support Vector Machine (a Contemporary Machine Learning\nTechnique), and Recurrent Neural Networks (a Contemporary Deep Learning\nTechnique). We have used monthly historical data for several macroeconomic\nvariables from April 1994 to December 2018 for USA and India to predict USD-INR\nForeign Exchange Rate. The results clearly depict that contemporary techniques\nof SVM and RNN (Long Short-Term Memory) outperform the widely used traditional\nmethod of Auto Regression. The RNN model with Long Short-Term Memory (LSTM)\nprovides the maximum accuracy (97.83%) followed by SVM Model (97.17%) and VAR\nModel (96.31%). At last, we present a brief analysis of the correlation and\ninterdependencies of the variables used for forecasting.\n", "versions": [{"version": "v1", "created": "Wed, 19 Feb 2020 18:11:57 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Kaushik", "Manav", ""], ["Giri", "A K", ""]]}]