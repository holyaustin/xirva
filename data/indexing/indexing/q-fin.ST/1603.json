[{"id": "1603.01103", "submitter": "Roy Cerqueti", "authors": "Marcel Ausloos, Rosella Castellano and Roy Cerqueti", "title": "Regularities and Discrepancies of Credit Default Swaps: a Data Science\n  approach through Benford's Law", "comments": "20 pages, 6 tables, 1 figure, Chaos, Solitons and Fractals, 2016", "journal-ref": "Chaos, Solitons & Fractals 90 (2016) 8-17", "doi": "10.1016/j.chaos.2016.03.002", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we search whether the Benford's law is applicable to monitor\ndaily changes in sovereign Credit Default Swaps (CDS) quotes, which are\nacknowledged to be complex systems of economic content. This test is of\nparamount importance since the CDS of a country proxy its health and\nprobability to default, being associated to an insurance against the event of\nits default. We fit the Benford's law to the daily changes in sovereign CDS\nspreads for 13 European countries, - both inside and outside the European Union\nand European Monetary Union. Two different tenors for the sovereign CDS\ncontracts are considered: 5 yrs and 10 yrs, - the former being the reference\nand most liquid one. The time period under investigation is 2008-2015 which\nincludes the period of distress caused by the European sovereign debt crisis.\nMoreover, (i) an analysis over relevant sub-periods is carried out, (ii)\nseveral insights are provided also by implementing the tracking of the\nBenford's law over moving windows. The main test for checking the conformance\nto Benford's law is - as usual - the $\\chi^{2}$ test, whose values are\npresented and discussed for all cases. The analysis is further completed by\nelaborations based on Chebyshev's distance and Kullback and Leibler's\ndivergence. The results highlight differences by countries and tenors. In\nparticular, these results suggest that liquidity seems to be associated to\nhigher levels of distortion. Greece - representing a peculiar case - shows a\nvery different path with respect to the other European countries.\n", "versions": [{"version": "v1", "created": "Wed, 2 Mar 2016 13:18:32 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["Ausloos", "Marcel", ""], ["Castellano", "Rosella", ""], ["Cerqueti", "Roy", ""]]}, {"id": "1603.01308", "submitter": "Leopoldo Catania", "authors": "Leopoldo Catania", "title": "Dynamic Adaptive Mixture Models", "comments": "47 pages, 4 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new class of Dynamic Mixture Models (DAMMs) being\nable to sequentially adapt the mixture components as well as the mixture\ncomposition using information coming from the data. The information driven\nnature of the proposed class of models allows to exactly compute the full\nlikelihood and to avoid computer intensive simulation schemes. An extensive\nMonte Carlo experiment reveals that the new proposed model can accurately\napproximate the more complicated Stochastic Dynamic Mixture Model previously\nintroduced in the literature as well as other kind of models. The properties of\nthe new proposed class of models are discussed through the paper and an\napplication in financial econometrics is reported.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2016 22:50:57 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Catania", "Leopoldo", ""]]}, {"id": "1603.01397", "submitter": "Sunil Kumar Dr", "authors": "Sunil Kumar", "title": "Latent class analyisis for reliable measure of inflation expectation in\n  the indian public", "comments": "16 pages, 04 Tables and 03 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.GN q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main aim of this paper is to inspect the properties of survey based on\nhouseholds inflation expectations, conducted by Reserve Bank of India. It is\ntheorized that the respondents answers are exaggerated by extreme response\nbias. Latent class analysis has been hailed as a promising technique for\nstudying measurement errors in surveys, because the model produces estimates of\nthe error rates associated with a given question of the questionnaire. I have\nidentified a model with optimum performance and hence categorize the objective\nas well as reliable classifiers or otherwise.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 09:44:21 GMT"}], "update_date": "2016-03-07", "authors_parsed": [["Kumar", "Sunil", ""]]}, {"id": "1603.01580", "submitter": "Shanshan Wang", "authors": "Shanshan Wang, Rudi Sch\\\"afer and Thomas Guhr", "title": "Cross-response in correlated financial markets: individual stocks", "comments": "stock pairs, unaveraged, see arXiv:1510.03205", "journal-ref": "Eur. Phys. J. B (2016) 89: 105", "doi": "10.1140/epjb/e2016-60818-y", "report-no": null, "categories": "q-fin.ST q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous studies of the stock price response to trades focused on the\ndynamics of single stocks, i.e. they addressed the self-response. We\nempirically investigate the price response of one stock to the trades of other\nstocks in a correlated market, i.e. the cross-responses. How large is the\nimpact of one stock on others and vice versa? -- This impact of trades on the\nprice change across stocks appears to be transient instead of permanent as we\ndiscuss from the viewpoint of market efficiency. Furthermore, we compare the\nself-responses on different scales and the self- and cross-responses on the\nsame scale. We also find that the cross-correlation of the trade signs turns\nout to be a short-memory process.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 19:25:14 GMT"}, {"version": "v2", "created": "Mon, 25 Apr 2016 09:47:37 GMT"}], "update_date": "2016-04-26", "authors_parsed": [["Wang", "Shanshan", ""], ["Sch\u00e4fer", "Rudi", ""], ["Guhr", "Thomas", ""]]}, {"id": "1603.01586", "submitter": "Shanshan Wang", "authors": "Shanshan Wang, Rudi Sch\\\"afer and Thomas Guhr", "title": "Average cross-responses in correlated financial market", "comments": "stock pairs, averaged, see arXiv:1510.03205", "journal-ref": "Eur. Phys. J. B (2016) 89: 207", "doi": "10.1140/epjb/e2016-70137-0", "report-no": null, "categories": "q-fin.ST q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are non-vanishing price responses across different stocks in correlated\nfinancial markets. We further study this issue by performing different\naverages, which identify active and passive cross-responses. The two average\ncross-responses show different characteristic dependences on the time lag. The\npassive cross-response exhibits a shorter response period with sizeable\nvolatilities, while the corresponding period for the active cross-response is\nlonger. The average cross-responses for a given stock are evaluated either with\nrespect to the whole market or to different sectors. Using the response\nstrength, the influences of individual stocks are identified and discussed.\nMoreover, the various cross-responses as well as the average cross-responses\nare compared with the self-responses. In contrast, the short memory of trade\nsign cross-correlation for stock pairs, the sign cross-correlation has long\nmemory when averaged over different pairs of stocks.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2016 19:39:56 GMT"}, {"version": "v2", "created": "Fri, 23 Sep 2016 10:47:32 GMT"}], "update_date": "2016-09-26", "authors_parsed": [["Wang", "Shanshan", ""], ["Sch\u00e4fer", "Rudi", ""], ["Guhr", "Thomas", ""]]}, {"id": "1603.02615", "submitter": "Marcin Pitera", "authors": "Marcin Pitera and Thorsten Schmidt", "title": "Unbiased estimation of risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of risk measures recently gained a lot of attention, partly\nbecause of the backtesting issues of expected shortfall related to\nelicitability. In this work we shed a new and fundamental light on optimal\nestimation procedures of risk measures in terms of bias. We show that once the\nparameters of a model need to be estimated, one has to take additional care\nwhen estimating risks. The typical plug-in approach, for example, introduces a\nbias which leads to a systematic underestimation of risk. In this regard, we\nintroduce a novel notion of unbiasedness to the estimation of risk which is\nmotivated by economic principles. In general, the proposed concept does not\ncoincide with the well-known statistical notion of unbiasedness. We show that\nan appropriate bias correction is available for many well-known estimators. In\nparticular, we consider value-at-risk and expected shortfall (tail\nvalue-at-risk). In the special case of normal distributions, closed-formed\nsolutions for unbiased estimators can be obtained. We present a number of\nmotivating examples which show the outperformance of unbiased estimators in\nmany circumstances. The unbiasedness has a direct impact on backtesting and\ntherefore adds a further viewpoint to established statistical properties.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2016 18:21:35 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 12:30:46 GMT"}, {"version": "v3", "created": "Wed, 3 May 2017 16:51:17 GMT"}, {"version": "v4", "created": "Thu, 24 Aug 2017 16:01:15 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Pitera", "Marcin", ""], ["Schmidt", "Thorsten", ""]]}, {"id": "1603.02874", "submitter": "Aurelio Fernandez Bariviera", "authors": "Aurelio F. Bariviera, M. Belen Guercio, Lisana B. Martinez, Osvaldo A.\n  Rosso", "title": "Libor at crossroads: stochastic switching detection using information\n  theory quantifiers", "comments": "17 pages, 9 figures. arXiv admin note: text overlap with\n  arXiv:1508.04748, arXiv:1509.00217", "journal-ref": null, "doi": "10.1016/j.chaos.2016.02.009", "report-no": null, "categories": "q-fin.ST q-fin.CP q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the 28 time series of Libor rates, classified in seven\nmaturities and four currencies), during the last 14 years. The analysis was\nperformed using a novel technique in financial economics: the\nComplexity-Entropy Causality Plane. This planar representation allows the\ndiscrimination of different stochastic and chaotic regimes. Using a temporal\nanalysis based on moving windows, this paper unveals an abnormal movement of\nLibor time series arround the period of the 2007 financial crisis. This\nalteration in the stochastic dynamics of Libor is contemporary of what press\ncalled \"Libor scandal\", i.e. the manipulation of interest rates carried out by\nseveral prime banks. We argue that our methodology is suitable as a market\nwatch mechanism, as it makes visible the temporal redution in informational\nefficiency of the market.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2016 12:54:17 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Bariviera", "Aurelio F.", ""], ["Guercio", "M. Belen", ""], ["Martinez", "Lisana B.", ""], ["Rosso", "Osvaldo A.", ""]]}, {"id": "1603.04017", "submitter": "Gautier Marti", "authors": "Gautier Marti, S\\'ebastien Andler, Frank Nielsen, Philippe Donnat", "title": "Clustering Financial Time Series: How Long is Enough?", "comments": "Accepted at IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have used from 30 days to several years of daily returns as\nsource data for clustering financial time series based on their correlations.\nThis paper sets up a statistical framework to study the validity of such\npractices. We first show that clustering correlated random variables from their\nobserved values is statistically consistent. Then, we also give a first\nempirical answer to the much debated question: How long should the time series\nbe? If too short, the clusters found can be spurious; if too long, dynamics can\nbe smoothed out.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2016 10:47:00 GMT"}, {"version": "v2", "created": "Thu, 14 Apr 2016 20:03:41 GMT"}], "update_date": "2016-04-18", "authors_parsed": [["Marti", "Gautier", ""], ["Andler", "S\u00e9bastien", ""], ["Nielsen", "Frank", ""], ["Donnat", "Philippe", ""]]}, {"id": "1603.05700", "submitter": "Yoann Potiron", "authors": "Yoann Potiron, Per Mykland", "title": "Local Parametric Estimation in High Frequency Data", "comments": "67 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we give a general time-varying parameter model, where the\nmultidimensional parameter possibly includes jumps. The quantity of interest is\ndefined as the integrated value over time of the parameter process $\\Theta =\nT^{-1} \\int_0^T \\theta_t^* dt$. We provide a local parametric estimator (LPE)\nof $\\Theta$ and conditions under which we can show the central limit theorem.\nRoughly speaking those conditions correspond to some uniform limit theory in\nthe parametric version of the problem. The framework is restricted to the\nspecific convergence rate $n^{1/2}$. Several examples of LPE are studied:\nestimation of volatility, powers of volatility, volatility when incorporating\ntrading information and time-varying MA(1).\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2016 21:42:54 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 02:36:45 GMT"}, {"version": "v3", "created": "Fri, 10 Mar 2017 05:35:45 GMT"}, {"version": "v4", "created": "Tue, 21 Aug 2018 00:30:06 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Potiron", "Yoann", ""], ["Mykland", "Per", ""]]}, {"id": "1603.06202", "submitter": "Siddartha Ghoshal", "authors": "Sid Ghoshal, Stephen Roberts", "title": "Extracting Predictive Information from Heterogeneous Data Streams using\n  Gaussian Processes", "comments": "15 pages, 5 figures, accepted for publication in Algorithmic Finance", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial markets are notoriously complex environments, presenting vast\namounts of noisy, yet potentially informative data. We consider the problem of\nforecasting financial time series from a wide range of information sources\nusing online Gaussian Processes with Automatic Relevance Determination (ARD)\nkernels. We measure the performance gain, quantified in terms of Normalised\nRoot Mean Square Error (NRMSE), Median Absolute Deviation (MAD) and Pearson\ncorrelation, from fusing each of four separate data domains: time series\ntechnicals, sentiment analysis, options market data and broker recommendations.\nWe show evidence that ARD kernels produce meaningful feature rankings that help\nretain salient inputs and reduce input dimensionality, providing a framework\nfor sifting through financial complexity. We measure the performance gain from\nfusing each domain's heterogeneous data streams into a single probabilistic\nmodel. In particular our findings highlight the critical value of options data\nin mapping out the curvature of price space and inspire an intuitive, novel\ndirection for research in financial prediction.\n", "versions": [{"version": "v1", "created": "Sun, 20 Mar 2016 11:11:54 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 17:06:47 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Ghoshal", "Sid", ""], ["Roberts", "Stephen", ""]]}, {"id": "1603.07532", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb", "title": "A Short Note on P-Value Hacking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the expected values from p-value hacking as a choice of the\nminimum p-value among $m$ independents tests, which can be considerably lower\nthan the \"true\" p-value, even with a single trial, owing to the extreme\nskewness of the meta-distribution.\n  We first present an exact probability distribution (meta-distribution) for\np-values across ensembles of statistically identical phenomena. We derive the\ndistribution for small samples $2<n \\leq n^*\\approx 30$ as well as the limiting\none as the sample size $n$ becomes large. We also look at the properties of the\n\"power\" of a test through the distribution of its inverse for a given p-value\nand parametrization.\n  The formulas allow the investigation of the stability of the reproduction of\nresults and \"p-hacking\" and other aspects of meta-analysis.\n  P-values are shown to be extremely skewed and volatile, regardless of the\nsample size $n$, and vary greatly across repetitions of exactly same protocols\nunder identical stochastic copies of the phenomenon; such volatility makes the\nminimum $p$ value diverge significantly from the \"true\" one. Setting the power\nis shown to offer little remedy unless sample size is increased markedly or the\np-value is lowered by at least one order of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2016 11:37:23 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2016 11:32:12 GMT"}, {"version": "v3", "created": "Fri, 8 Apr 2016 01:29:21 GMT"}, {"version": "v4", "created": "Thu, 25 Jan 2018 20:15:34 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Taleb", "Nassim Nicholas", ""]]}, {"id": "1603.07822", "submitter": "Gautier Marti", "authors": "Gautier Marti, Frank Nielsen, Philippe Donnat, S\\'ebastien Andler", "title": "On clustering financial time series: a need for distances between\n  dependent random variables", "comments": "Work presented during a workshop on Information Geometry at the\n  International Centre for Mathematical Sciences, Edinburgh, UK", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following working document summarizes our work on the clustering of\nfinancial time series. It was written for a workshop on information geometry\nand its application for image and signal processing. This workshop brought\nseveral experts in pure and applied mathematics together with applied\nresearchers from medical imaging, radar signal processing and finance. The\nauthors belong to the latter group. This document was written as a long\nintroduction to further development of geometric tools in financial\napplications such as risk or portfolio analysis. Indeed, risk and portfolio\nanalysis essentially rely on covariance matrices. Besides that the Gaussian\nassumption is known to be inaccurate, covariance matrices are difficult to\nestimate from empirical data. To filter noise from the empirical estimate,\nMantegna proposed using hierarchical clustering. In this work, we first show\nthat this procedure is statistically consistent. Then, we propose to use\nclustering with a much broader application than the filtering of empirical\ncovariance matrices from the estimate correlation coefficients. To be able to\ndo that, we need to obtain distances between the financial time series that\nincorporate all the available information in these cross-dependent random\nprocesses.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2016 05:15:50 GMT"}], "update_date": "2016-03-28", "authors_parsed": [["Marti", "Gautier", ""], ["Nielsen", "Frank", ""], ["Donnat", "Philippe", ""], ["Andler", "S\u00e9bastien", ""]]}, {"id": "1603.08383", "submitter": "Elvis Oltean", "authors": "Elvis Oltean", "title": "Modelling income, wealth, and expenditure data by use of Econophysics", "comments": "arXiv admin note: text overlap with arXiv:0709.3662 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN physics.soc-ph q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper, we identify several distributions from Physics and\nstudy their applicability to phenomena such as distribution of income, wealth,\nand expenditure. Firstly, we apply logistic distribution to these data and we\nfind that it fits very well the annual data for the entire income interval\nincluding for upper income segment of population. Secondly, we apply\nFermi-Dirac distribution to these data. We seek to explain possible\ncorrelations and analogies between economic systems and statistical\nthermodynamics systems. We try to explain their behavior and properties when we\ncorrelate physical variables with macroeconomic aggregates and indicators. Then\nwe draw some analogies between parameters of the Fermi-Dirac distribution and\nmacroeconomic variables. Thirdly, as complex systems are modeled using\npolynomial distributions, we apply polynomials to the annual sets of data and\nwe find that it fits very well also the entire income interval. Fourthly, we\ndevelop a new methodology to approach dynamically the income, wealth, and\nexpenditure distribution similarly with dynamical complex systems. This\nmethodology was applied to different time intervals consisting of consecutive\nyears up to 35 years. Finally, we develop a mathematical model based on a\nHamiltonian that maximizes utility function applied to Ramsey model using\nFermi-Dirac and polynomial utility functions. We find some theoretical\nconnections with time preference theory. We apply these distributions to a\nlarge pool of data from countries with different levels of development, using\ndifferent methods for calculation of income, wealth, and expenditure.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2016 14:05:57 GMT"}], "update_date": "2016-03-29", "authors_parsed": [["Oltean", "Elvis", ""]]}]