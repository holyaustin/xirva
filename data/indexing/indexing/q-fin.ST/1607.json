[{"id": "1607.02093", "submitter": "Indranil Ghosh", "authors": "Tamal Datta Chaudhuri, Indranil Ghosh", "title": "Artificial Neural Network and Time Series Modeling Based Approach to\n  Forecasting the Exchange Rate in a Multivariate Framework", "comments": null, "journal-ref": "Journal of Insurance and Financial Management, Vol. 1, Issue 5,\n  PP. 92-123, 2016", "doi": null, "report-no": null, "categories": "q-fin.ST cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Any discussion on exchange rate movements and forecasting should include\nexplanatory variables from both the current account and the capital account of\nthe balance of payments. In this paper, we include such factors to forecast the\nvalue of the Indian rupee vis a vis the US Dollar. Further, factors reflecting\npolitical instability and lack of mechanism for enforcement of contracts that\ncan affect both direct foreign investment and also portfolio investment, have\nbeen incorporated. The explanatory variables chosen are the 3 month Rupee\nDollar futures exchange rate (FX4), NIFTY returns (NIFTYR), Dow Jones\nIndustrial Average returns (DJIAR), Hang Seng returns (HSR), DAX returns (DR),\ncrude oil price (COP), CBOE VIX (CV) and India VIX (IV). To forecast the\nexchange rate, we have used two different classes of frameworks namely,\nArtificial Neural Network (ANN) based models and Time Series Econometric\nmodels. Multilayer Feed Forward Neural Network (MLFFNN) and Nonlinear\nAutoregressive models with Exogenous Input (NARX) Neural Network are the\napproaches that we have used as ANN models. Generalized Autoregressive\nConditional Heteroskedastic (GARCH) and Exponential Generalized Autoregressive\nConditional Heteroskedastic (EGARCH) techniques are the ones that we have used\nas Time Series Econometric methods. Within our framework, our results indicate\nthat, although the two different approaches are quite efficient in forecasting\nthe exchange rate, MLFNN and NARX are the most efficient.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 06:17:36 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Chaudhuri", "Tamal Datta", ""], ["Ghosh", "Indranil", ""]]}, {"id": "1607.02470", "submitter": "Justin Sirignano", "authors": "Justin Sirignano, Apaar Sadhwani, and Kay Giesecke", "title": "Deep Learning for Mortgage Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a deep learning model of multi-period mortgage risk and use it to\nanalyze an unprecedented dataset of origination and monthly performance records\nfor over 120 million mortgages originated across the US between 1995 and 2014.\nOur estimators of term structures of conditional probabilities of prepayment,\nforeclosure and various states of delinquency incorporate the dynamics of a\nlarge number of loan-specific as well as macroeconomic variables down to the\nzip-code level. The estimators uncover the highly nonlinear nature of the\nrelationship between the variables and borrower behavior, especially\nprepayment. They also highlight the effects of local economic conditions on\nborrower behavior. State unemployment has the greatest explanatory power among\nall variables, offering strong evidence of the tight connection between housing\nfinance markets and the macroeconomy. The sensitivity of a borrower to changes\nin unemployment strongly depends upon current unemployment. It also\nsignificantly varies across the entire borrower population, which highlights\nthe interaction of unemployment and many other variables. These findings have\nimportant implications for mortgage-backed security investors, rating agencies,\nand housing finance policymakers.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 17:42:40 GMT"}, {"version": "v2", "created": "Sat, 10 Mar 2018 11:29:52 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Sirignano", "Justin", ""], ["Sadhwani", "Apaar", ""], ["Giesecke", "Kay", ""]]}, {"id": "1607.03205", "submitter": "Taisei Kaizoji", "authors": "Taisei Kaizoji and Michiko Miyano", "title": "Stock Market Market Crash of 2008: an empirical study of the deviation\n  of share prices from company fundamentals", "comments": "11 pages, 7 figures", "journal-ref": null, "doi": "10.1080/13504851.2018.1486004", "report-no": null, "categories": "q-fin.GN q-fin.PR q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this study is to investigate quantitatively whether share prices\ndeviated from company fundamentals in the stock market crash of 2008. For this\npurpose, we use a large database containing the balance sheets and share prices\nof 7,796 worldwide companies for the period 2004 through 2013. We develop a\npanel regression model using three financial indicators--dividends per share,\ncash flow per share, and book value per share--as explanatory variables for\nshare price. We then estimate individual company fundamentals for each year by\nremoving the time fixed effects from the two-way fixed effects model, which we\nidentified as the best of the panel regression models. One merit of our model\nis that we are able to extract unobservable factors of company fundamentals by\nusing the individual fixed effects.\n  Based on these results, we analyze the market anomaly quantitatively using\nthe divergence rate--the rate of the deviation of share price from a company's\nfundamentals. We find that share prices on average were overvalued in the\nperiod from 2005 to 2007, and were undervalued significantly in 2008, when the\nglobal financial crisis occurred. Share prices were equivalent to the\nfundamentals on average in the subsequent period. Our empirical results clearly\ndemonstrate that the worldwide stock market fluctuated excessively in the time\nperiod before and just after the global financial crisis of 2008.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 00:18:15 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Kaizoji", "Taisei", ""], ["Miyano", "Michiko", ""]]}, {"id": "1607.04883", "submitter": "Zurab Kakushadze", "authors": "Zura Kakushadze and Willie Yu", "title": "Statistical Industry Classification", "comments": "44 pages; trivial misprints corrected", "journal-ref": "Journal of Risk & Control 3(1) (2016) 17-65, Invited Editorial", "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give complete algorithms and source code for constructing (multilevel)\nstatistical industry classifications, including methods for fixing the number\nof clusters at each level (and the number of levels). Under the hood there are\nclustering algorithms (e.g., k-means). However, what should we cluster?\nCorrelations? Returns? The answer turns out to be neither and our backtests\nsuggest that these details make a sizable difference. We also give an algorithm\nand source code for building \"hybrid\" industry classifications by improving\noff-the-shelf \"fundamental\" industry classifications by applying our\nstatistical industry classification methods to them. The presentation is\nintended to be pedagogical and geared toward practical applications in\nquantitative trading.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 15:49:24 GMT"}, {"version": "v2", "created": "Mon, 23 Jan 2017 04:51:15 GMT"}, {"version": "v3", "created": "Tue, 24 Jan 2017 06:26:52 GMT"}, {"version": "v4", "created": "Sat, 29 Dec 2018 06:05:56 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Kakushadze", "Zura", ""], ["Yu", "Willie", ""]]}, {"id": "1607.05608", "submitter": "Erik Bartos", "authors": "Erik Barto\\v{s} and Richard Pin\\v{c}\\'ak", "title": "Identification of market trends with string and D2-brane maps", "comments": "10 pages, 8 figures, 3 tables", "journal-ref": "Physica A479 (2017) 57-70", "doi": "10.1016/j.physa.2017.03.014", "report-no": null, "categories": "q-fin.ST q-fin.CP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi dimensional string objects are introduced as a new alternative for\nan application of string models for time series forecasting in trading on\nfinancial markets. The objects are represented by open string with 2-endpoints\nand D2-brane, which are continuous enhancement of 1-endpoint open string model.\nWe show how new object properties can change the statistics of the predictors,\nwhich makes them the candidates for modeling a wide range of time series\nsystems. String angular momentum is proposed as another tool to analyze the\nstability of currency rates except the historical volatility. To show the\nreliability of our approach with application of string models for time series\nforecasting we present the results of real demo simulations for four currency\nexchange pairs.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 08:47:34 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Barto\u0161", "Erik", ""], ["Pin\u010d\u00e1k", "Richard", ""]]}, {"id": "1607.05660", "submitter": "Olgu Benli", "authors": "T. O. Benli", "title": "A Comparison of Nineteen Various Electricity Consumption Forecasting\n  Approaches and Practicing to Five Different Households in Turkey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accuracy of the household electricity consumption forecast is vital in\ntaking better cost effective and energy efficient decisions. In order to design\naccurate, proper and efficient forecasting model, characteristics of the series\nhave to been analyzed. The source of time series data comes from Online\nEnerjisa System, the system of electrical energy provider in capital of Turkey,\nwhich consumers can reach their latest two year period electricity\nconsumptions; in our study the period was May 2014 to May 2016. Various\ntechniques had been applied in order to analyze the data; classical\ndecomposition models; standard typed and also with the centering moving average\nmethod, regression equations, exponential smoothing models and ARIMA models. In\nour study, nine teen different approaches; all of these have at least\ndiversified aspects of methodology, had been compared and the best model for\nforecasting were decided by considering the smallest values of MAPE, MAD and\nMSD. As a first step we took the time period May 2014 to May 2016 and found\npredicted value for June 2016 with the best forecasting model. After finding\nthe best forecasting model and fitted value for June 2016, than validating\nprocess had been taken place; we made comparisons to see how well the real\nvalue of June 2016 and forecasted value for that specific period matched.\nAfterwards we made electrical consumption forecast for the following 3 months;\nJune-September 2016 for each of five households individually.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 19:10:53 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Benli", "T. O.", ""]]}, {"id": "1607.05831", "submitter": "Simon Clinet", "authors": "Simon Clinet and Yoann Potiron", "title": "Statistical inference for the doubly stochastic self-exciting process", "comments": "47 pages, 4 figures, 4 tables. Under revision for Bernoulli Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and show the existence of a Hawkes self-exciting point process\nwith exponentially-decreasing kernel and where parameters are time-varying. The\nquantity of interest is defined as the integrated parameter\n$T^{-1}\\int_0^T\\theta_t^*dt$, where $\\theta_t^*$ is the time-varying parameter,\nand we consider the high-frequency asymptotics. To estimate it na\\\"ively, we\nchop the data into several blocks, compute the maximum likelihood estimator\n(MLE) on each block, and take the average of the local estimates. The\nasymptotic bias explodes asymptotically, thus we provide a non-na\\\"ive\nestimator which is constructed as the na\\\"ive one when applying a first-order\nbias reduction to the local MLE. We show the associated central limit theorem.\nMonte Carlo simulations show the importance of the bias correction and that the\nmethod performs well in finite sample, whereas the empirical study discusses\nthe implementation in practice and documents the stochastic behavior of the\nparameters.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 06:27:40 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 19:14:47 GMT"}, {"version": "v3", "created": "Tue, 27 Jun 2017 01:14:06 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Clinet", "Simon", ""], ["Potiron", "Yoann", ""]]}, {"id": "1607.06158", "submitter": "Konstantinos Spiliopoulos", "authors": "Andrew Papanicolaou, Konstantinos Spiliopoulos", "title": "Dimension Reduction in Statistical Estimation of Partially Observed\n  Multiscale Processes", "comments": "SIAM Journal of Uncertainty Quantification, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-fin.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider partially observed multiscale diffusion models that are specified\nup to an unknown vector parameter. We establish for a very general class of\ntest functions that the filter of the original model converges to a filter of\nreduced dimension. Then, this result is used to justify statistical estimation\nfor the unknown parameters of interest based on the model of reduced dimension\nbut using the original available data. This allows to learn the unknown\nparameters of interest while working in lower dimensions, as opposed to working\nwith the original high dimensional system. Simulation studies support and\nillustrate the theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 00:14:31 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 20:06:55 GMT"}, {"version": "v3", "created": "Sun, 26 Nov 2017 18:02:11 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Papanicolaou", "Andrew", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1607.08214", "submitter": "Jozef Barunik", "authors": "Jozef Barunik and Evzen Kocenda and Lukas Vacha", "title": "Asymmetric volatility connectedness on forex markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how bad and good volatility propagate through forex markets, i.e., we\nprovide evidence for asymmetric volatility connectedness on forex markets.\nUsing high-frequency, intra-day data of the most actively traded currencies\nover 2007 - 2015 we document the dominating asymmetries in spillovers that are\ndue to bad rather than good volatility. We also show that negative spillovers\nare chiefly tied to the dragging sovereign debt crisis in Europe while positive\nspillovers are correlated with the subprime crisis, different monetary policies\namong key world central banks, and developments on commodities markets. It\nseems that a combination of monetary and real-economy events is behind the net\npositive asymmetries in volatility spillovers, while fiscal factors are linked\nwith net negative spillovers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 18:50:37 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Barunik", "Jozef", ""], ["Kocenda", "Evzen", ""], ["Vacha", "Lukas", ""]]}]