[{"id": "1904.00745", "submitter": "Luyang Chen", "authors": "Luyang Chen, Markus Pelger and Jason Zhu", "title": "Deep Learning in Asset Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use deep neural networks to estimate an asset pricing model for individual\nstock returns that takes advantage of the vast amount of conditioning\ninformation, while keeping a fully flexible form and accounting for\ntime-variation. The key innovations are to use the fundamental no-arbitrage\ncondition as criterion function, to construct the most informative test assets\nwith an adversarial approach and to extract the states of the economy from many\nmacroeconomic time series. Our asset pricing model outperforms out-of-sample\nall benchmark approaches in terms of Sharpe ratio, explained variation and\npricing errors and identifies the key factors that drive asset prices.\n", "versions": [{"version": "v1", "created": "Mon, 11 Mar 2019 01:37:47 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 18:50:27 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 21:40:25 GMT"}, {"version": "v4", "created": "Sat, 16 May 2020 14:20:53 GMT"}, {"version": "v5", "created": "Fri, 23 Jul 2021 22:26:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chen", "Luyang", ""], ["Pelger", "Markus", ""], ["Zhu", "Jason", ""]]}, {"id": "1904.00749", "submitter": "Roel Ceballos", "authors": "Novy Ann M. Etac and Roel F. Ceballos", "title": "Forecasting the Volatilities of Philippine Stock Exchange Composite\n  Index Using the Generalized Autoregressive Conditional Heteroskedasticity\n  Modeling", "comments": null, "journal-ref": "International Journal of Statistics and Economics, 19(3), 2018", "doi": null, "report-no": null, "categories": "q-fin.ST stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study was conducted to find an appropriate statistical model to forecast\nthe volatilities of PSEi using the model Generalized Autoregressive Conditional\nHeteroskedasticity (GARCH). Using the R software, the log returns of PSEi is\nmodeled using various ARIMA models and with the presence of heteroskedasticity,\nthe log returns was modeled using GARCH. Based on the analysis, GARCH models\nare the most appropriate to use for the log returns of PSEi. Among the selected\nGARCH models, GARCH (1,2) has the lowest AIC value and also has the highest LL\nvalue implying that GARCH (1,2) is the best model for the log returns of PSEi.\n", "versions": [{"version": "v1", "created": "Thu, 21 Feb 2019 10:42:27 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Etac", "Novy Ann M.", ""], ["Ceballos", "Roel F.", ""]]}, {"id": "1904.03488", "submitter": "Giuseppe Pernagallo Dr.", "authors": "Giuseppe Pernagallo and Benedetto Torrisi", "title": "Blindfolded monkeys or financial analysts: who is worth your money? New\n  evidence on informational inefficiencies in the U.S. stock market", "comments": null, "journal-ref": "Physica A: Statistical Mechanics and its Applications, 2019", "doi": "10.1016/j.physa.2019.122900", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficient market hypothesis has been considered one of the most\ncontroversial arguments in finance, with the academia divided between who\nclaims the impossibility of beating the market and who believes that it is\npossible to gain over the average profits. If the hypothesis holds, it means,\nas suggested by Burton Malkiel, that a blindfolded monkey selecting stocks by\nthrowing darts at a newspaper's financial pages could perform as well as a\nfinancial analyst, or even better. In this paper we use a novel approach, based\non confidence intervals for proportions, to assess the degree of inefficiency\nin the S&P 500 Index components concluding that several stocks are inefficient:\nwe estimated the proportion of inefficient stocks in the index to be between\n12.13% and 27.87%. This supports other studies proving that a financial\nanalyst, probably, is a better investor than a blindfolded monkey.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2019 16:38:24 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 08:28:01 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Pernagallo", "Giuseppe", ""], ["Torrisi", "Benedetto", ""]]}, {"id": "1904.03726", "submitter": "Giuseppe Pernagallo Dr.", "authors": "Giuseppe Pernagallo and Benedetto Torrisi", "title": "A Theory of Information overload applied to perfectly efficient\n  financial markets", "comments": "21 pages, 3 figures", "journal-ref": "Review of Behavioral Finance, 2020", "doi": "10.1108/RBF-07-2019-0088", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Before the massive spread of computer technology, information was far from\ncomplex. The development of technology shifted the paradigm: from individuals\nwho faced scarce and costly information to individuals who face massive amounts\nof information accessible at low costs. Nowadays we are living in the era of\nbig data and investors deal every day with a huge flow of information. In the\nspirit of the modern idea that economic agents have limited computational\ncapacity, we propose an original model using information overload to show how\ntoo much information could cause financial markets to depart from the\ntraditional assumption of informational efficiency. We show that when\ninformation tends to infinite, the efficient market hypothesis ceases to be\ntrue. This happens also for lower levels of information, when the use of the\nmaximum amount of information is not optimal for investors. The present work\ncan be a stimulus to consider more realistic economic models and it can be\nfurther deepened including other realistic features present in financial\nmarkets, such as information asymmetry or noise in the transmission of\ninformation.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2019 20:00:48 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Pernagallo", "Giuseppe", ""], ["Torrisi", "Benedetto", ""]]}, {"id": "1904.04951", "submitter": "Torsten Trimborn", "authors": "Maximilian Beikirch, Simon Cramer, Martin Frank, Philipp Otte, Emma\n  Pabich, Torsten Trimborn", "title": "Robust Mathematical Formulation and Probabilistic Description of\n  Agent-Based Computational Economic Market Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR econ.GN q-fin.EC q-fin.GN q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In science and especially in economics, agent-based modeling has become a\nwidely used modeling approach. These models are often formulated as a large\nsystem of difference equations. In this study, we discuss two aspects,\nnumerical modeling and the probabilistic description for two agent-based\ncomputational economic market models: the Levy-Levy-Solomon model and the\nFranke-Westerhoff model. We derive time-continuous formulations of both models,\nand in particular we discuss the impact of the time-scaling on the model\nbehavior for the Levy-Levy-Solomon model. For the Franke-Westerhoff model, we\nproof that a constraint required in the original model is not necessary for\nstability of the time-continuous model. It is shown that a semi-implicit\ndiscretization of the time-continuous system preserves this unconditional\nstability. In addition, this semi-implicit discretization can be computed at\ncost comparable to the original model. Furthermore, we discuss possible\nprobabilistic descriptions of time continuous agent-based computational\neconomic market models. Especially, we present the potential advantages of\nkinetic theory in order to derive mesoscopic desciptions of agent-based models.\nExemplified, we show two probabilistic descriptions of the Levy-Levy-Solomon\nand Franke-Westerhoff model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 00:05:45 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 08:05:26 GMT"}, {"version": "v3", "created": "Thu, 11 Mar 2021 22:33:43 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Beikirch", "Maximilian", ""], ["Cramer", "Simon", ""], ["Frank", "Martin", ""], ["Otte", "Philipp", ""], ["Pabich", "Emma", ""], ["Trimborn", "Torsten", ""]]}, {"id": "1904.05312", "submitter": "Angelos Alexopoulos Dr", "authors": "Angelos Alexopoulos, Petros Dellaportas, Omiros Papaspiliopoulos", "title": "Bayesian prediction of jumps in large panels of time series data", "comments": "49 pages, 27 figures, 4 tables", "journal-ref": null, "doi": "10.1214/21-BA1268", "report-no": null, "categories": "q-fin.ST stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We take a new look at the problem of disentangling the volatility and jumps\nprocesses of daily stock returns. We first provide a computational framework\nfor the univariate stochastic volatility model with Poisson-driven jumps that\noffers a competitive inference alternative to the existing tools. This\nmethodology is then extended to a large set of stocks for which we assume that\ntheir unobserved jump intensities co-evolve in time through a dynamic factor\nmodel. To evaluate the proposed modelling approach we conduct out-of-sample\nforecasts and we compare the posterior predictive distributions obtained from\nthe different models. We provide evidence that joint modelling of jumps\nimproves the predictive ability of the stochastic volatility models.\n", "versions": [{"version": "v1", "created": "Thu, 28 Mar 2019 22:59:32 GMT"}, {"version": "v2", "created": "Fri, 26 Jun 2020 07:57:45 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 15:24:56 GMT"}, {"version": "v4", "created": "Tue, 6 Apr 2021 11:29:49 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Alexopoulos", "Angelos", ""], ["Dellaportas", "Petros", ""], ["Papaspiliopoulos", "Omiros", ""]]}, {"id": "1904.05315", "submitter": "Amin Azari", "authors": "Amin Azari", "title": "Bitcoin Price Prediction: An ARIMA Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bitcoin is considered the most valuable currency in the world. Besides being\nhighly valuable, its value has also experienced a steep increase, from around 1\ndollar in 2010 to around 18000 in 2017. Then, in recent years, it has attracted\nconsiderable attention in a diverse set of fields, including economics and\ncomputer science. The former mainly focuses on studying how it affects the\nmarket, determining reasons behinds its price fluctuations, and predicting its\nfuture prices. The latter mainly focuses on its vulnerabilities, scalability,\nand other techno-crypto-economic issues. Here, we aim at revealing the\nusefulness of traditional autoregressive integrative moving average (ARIMA)\nmodel in predicting the future value of bitcoin by analyzing the price time\nseries in a 3-years-long time period. On the one hand, our empirical studies\nreveal that this simple scheme is efficient in sub-periods in which the\nbehavior of the time-series is almost unchanged, especially when it is used for\nshort-term prediction, e.g. 1-day. On the other hand, when we try to train the\nARIMA model to a 3-years-long period, during which the bitcoin price has\nexperienced different behaviors, or when we try to use it for a long-term\nprediction, we observe that it introduces large prediction errors. Especially,\nthe ARIMA model is unable to capture the sharp fluctuations in the price, e.g.\nthe volatility at the end of 2017. Then, it calls for more features to be\nextracted and used along with the price for a more accurate prediction of the\nprice. We have further investigated the bitcoin price prediction using an ARIMA\nmodel, trained over a large dataset, and a limited test window of the bitcoin\nprice, with length $w$, as inputs. Our study sheds lights on the interaction of\nthe prediction accuracy, choice of ($p,q,d$), and window size $w$.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2019 12:18:42 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Azari", "Amin", ""]]}, {"id": "1904.05317", "submitter": "Abhibasu Sen", "authors": "Abhibasu Sen, Prof. Karabi Dutta Chaudhury", "title": "On the Co-movement of Crude, Gold Prices and Stock Index in Indian\n  Market", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This non-linear relationship in the joint time-frequency domain has been\nstudied for the Indian National Stock Exchange (NSE) with the international\nGold price and WTI Crude Price being converted from Dollar to Indian National\nRupee based on that week's closing exchange rate. Though a good correlation was\nobtained during some period, but as a whole no such cointegration relation can\nbe found out. Using the \\textit{Discrete Wavelet Analysis}, the data was\ndecomposed and the presence of Granger Causal relations was tested.\nUnfortunately no significant relationships are being found. We then studied the\n\\textit{Wavelet Coherence} of the two pairs viz. NSE-Nifty \\& Gold and\nNSE-Nifty \\& Crude. For different frequencies, the coherence between the pairs\nhave been studied. At lower frequencies, some relatively good coherence have\nbeen found. In this paper, we report for the first time the co-movements\nbetween Crude Oil, Gold and Indian Stock Market Index using Wavelet Analysis\n(both Discrete and Continuous), a technique which is most sophisticated and\nrecent in market analysis. Thus for long term traders they can include gold\nand/or crude in their portfolio along with NSE-Nifty index in order to decrease\nthe risk(volatility) of the portfolio for Indian Market. But for short term\ntraders, it will not be effective, not to include all the three in their\nportfolio.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2019 04:47:02 GMT"}], "update_date": "2019-04-11", "authors_parsed": [["Sen", "Abhibasu", ""], ["Chaudhury", "Prof. Karabi Dutta", ""]]}, {"id": "1904.05384", "submitter": "Adamantios Ntakaris Mr", "authors": "Adamantios Ntakaris, Giorgio Mirone, Juho Kanniainen, Moncef Gabbouj,\n  Alexandros Iosifidis", "title": "Feature Engineering for Mid-Price Prediction with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mid-price movement prediction based on limit order book (LOB) data is a\nchallenging task due to the complexity and dynamics of the LOB. So far, there\nhave been very limited attempts for extracting relevant features based on LOB\ndata. In this paper, we address this problem by designing a new set of\nhandcrafted features and performing an extensive experimental evaluation on\nboth liquid and illiquid stocks. More specifically, we implement a new set of\neconometrical features that capture statistical properties of the underlying\nsecurities for the task of mid-price prediction. Moreover, we develop a new\nexperimental protocol for online learning that treats the task as a\nmulti-objective optimization problem and predicts i) the direction of the next\nprice movement and ii) the number of order book events that occur until the\nchange takes place. In order to predict the mid-price movement, the features\nare fed into nine different deep learning models based on multi-layer\nperceptrons (MLP), convolutional neural networks (CNN) and long short-term\nmemory (LSTM) neural networks. The performance of the proposed method is then\nevaluated on liquid and illiquid stocks, which are based on TotalView-ITCH US\nand Nordic stocks, respectively. For some stocks, results suggest that the\ncorrect choice of a feature set and a model can lead to the successful\nprediction of how long it takes to have a stock price movement.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2019 18:40:20 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 12:08:54 GMT"}, {"version": "v3", "created": "Fri, 7 Jun 2019 19:07:22 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Ntakaris", "Adamantios", ""], ["Mirone", "Giorgio", ""], ["Kanniainen", "Juho", ""], ["Gabbouj", "Moncef", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "1904.05931", "submitter": "Anshul Verma", "authors": "Anshul Verma and Pierpaolo Vivo and Tiziana Di Matteo", "title": "A memory-based method to select the number of relevant components in\n  Principal Component Analysis", "comments": "29 pages, published", "journal-ref": "Journal of Statistical Mechanics: Theory and Experiment, 2019", "doi": "10.1088/1742-5468/ab3bc4", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new data-driven method to select the optimal number of relevant\ncomponents in Principal Component Analysis (PCA). This new method applies to\ncorrelation matrices whose time autocorrelation function decays more slowly\nthan an exponential, giving rise to long memory effects. In comparison with\nother available methods present in the literature, our procedure does not rely\non subjective evaluations and is computationally inexpensive. The underlying\nbasic idea is to use a suitable factor model to analyse the residual memory\nafter sequentially removing more and more components, and stopping the process\nwhen the maximum amount of memory has been accounted for by the retained\ncomponents. We validate our methodology on both synthetic and real financial\ndata, and find in all cases a clear and computationally superior answer\nentirely compatible with available heuristic criteria, such as cumulative\nvariance and cross-validation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2019 19:18:46 GMT"}, {"version": "v2", "created": "Mon, 15 Apr 2019 16:50:00 GMT"}, {"version": "v3", "created": "Tue, 16 Apr 2019 18:16:18 GMT"}, {"version": "v4", "created": "Mon, 5 Aug 2019 13:23:26 GMT"}, {"version": "v5", "created": "Wed, 4 Sep 2019 19:56:47 GMT"}, {"version": "v6", "created": "Fri, 4 Oct 2019 10:29:27 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Verma", "Anshul", ""], ["Vivo", "Pierpaolo", ""], ["Di Matteo", "Tiziana", ""]]}, {"id": "1904.06007", "submitter": "Seyed Soheil Hosseini", "authors": "Seyed Soheil Hosseini, Nick Wormald, and Tianhai Tian", "title": "A Weight-based Information Filtration Algorithm for Stock-Correlation\n  Networks", "comments": "7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several algorithms have been proposed to filter information on a complete\ngraph of correlations across stocks to build a stock-correlation network. Among\nthem the planar maximally filtered graph (PMFG) algorithm uses $3n-6$ edges to\nbuild a graph whose features include a high frequency of small cliques and a\ngood clustering of stocks. We propose a new algorithm which we call\nproportional degree (PD) to filter information on the complete graph of\nnormalised mutual information (NMI) across stocks. Our results show that the PD\nalgorithm produces a network showing better homogeneity with respect to\ncliques, as compared to economic sectoral classification than its PMFG\ncounterpart. We also show that the partition of the PD network obtained through\nnormalised spectral clustering (NSC) agrees better with the NSC of the complete\ngraph than the corresponding one obtained from PMFG. Finally, we show that the\nclusters in the PD network are more robust with respect to the removal of\nrandom sets of edges than those in the PMFG network.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2019 02:16:45 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Hosseini", "Seyed Soheil", ""], ["Wormald", "Nick", ""], ["Tian", "Tianhai", ""]]}, {"id": "1904.08153", "submitter": "Th\\'eophile Griveau-Billion", "authors": "Th\\'eophile Griveau-Billion, Ben Calderhead", "title": "A Dynamic Bayesian Model for Interpretable Decompositions of Market\n  Behaviour", "comments": "59 pages, 14 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a heterogeneous simultaneous graphical dynamic linear model\n(H-SGDLM), which extends the standard SGDLM framework to incorporate a\nheterogeneous autoregressive realised volatility (HAR-RV) model. This novel\napproach creates a GPU-scalable multivariate volatility estimator, which\ndecomposes multiple time series into economically-meaningful variables to\nexplain the endogenous and exogenous factors driving the underlying\nvariability. This unique decomposition goes beyond the classic one step ahead\nprediction; indeed, we investigate inferences up to one month into the future\nusing stocks, FX futures and ETF futures, demonstrating its superior\nperformance according to accuracy of large moves, longer-term prediction and\nconsistency over time.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2019 09:30:42 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 08:46:38 GMT"}, {"version": "v3", "created": "Mon, 20 Jan 2020 15:09:10 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Griveau-Billion", "Th\u00e9ophile", ""], ["Calderhead", "Ben", ""]]}, {"id": "1904.09214", "submitter": "Tarcisio Rocha Filho", "authors": "Tarcisio M. Rocha Filho and Paulo M. M. Rocha", "title": "Inefficiency of the Brazilian Stock Market: the IBOVESPA Future\n  Contracts", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2019.123200", "report-no": null, "categories": "q-fin.CP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some indications of inefficiency of the Brazilian stock market\nbased on the existence of strong long-time cross-correlations with foreign\nmarkets and indices. Our results show a strong dependence on foreign markets\nindices as the S\\&P 500 and CAC 40, but not to the Shanghai SSE 180, indicating\nan intricate interdependence. We also show that the distribution of log-returns\nof the Brazilian BOVESPA index has a discrete fat tail in the time scale of a\nday, which is also a deviation of what is expected of an efficient equilibrated\nmarket. As a final argument of the inefficiency of the Brazilian stock market,\nwe use a neural network approach to forecast the direction of movement of the\nvalue of the IBOVESPA future contracts, with an accuracy allowing financial\nreturns over passive strategies.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2019 14:38:02 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Filho", "Tarcisio M. Rocha", ""], ["Rocha", "Paulo M. M.", ""]]}, {"id": "1904.09403", "submitter": "Akihiko Noda", "authors": "Akihiko Noda", "title": "On the Evolution of Cryptocurrency Market Efficiency", "comments": "10 pages, 3 figures, and 1 table", "journal-ref": null, "doi": "10.1080/13504851.2020.1758617", "report-no": null, "categories": "q-fin.ST q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study examines whether the efficiency of cryptocurrency markets (Bitcoin\nand Ethereum) evolve over time based on Lo's (2004) adaptive market hypothesis\n(AMH). In particular, we measure the degree of market efficiency using a\ngeneralized least squares-based time-varying model that does not depend on\nsample size, unlike previous studies that used conventional methods. The\nempirical results show that (1) the degree of market efficiency varies with\ntime in the markets, (2) Bitcoin's market efficiency level is higher than that\nof Ethereum over most periods, and (3) a market with high market liquidity has\nbeen evolving. We conclude that the results support the AMH for the most\nestablished cryptocurrency market.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2019 05:04:59 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 01:49:46 GMT"}, {"version": "v3", "created": "Tue, 7 Apr 2020 03:22:33 GMT"}, {"version": "v4", "created": "Tue, 5 May 2020 09:40:47 GMT"}, {"version": "v5", "created": "Wed, 6 May 2020 03:47:45 GMT"}, {"version": "v6", "created": "Tue, 7 Jul 2020 07:19:13 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Noda", "Akihiko", ""]]}, {"id": "1904.10182", "submitter": "Rituparna Sen", "authors": "Arnab Chakrabarti and Rituparna Sen", "title": "Copula estimation for nonsynchronous financial data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Copula is a powerful tool to model multivariate data. We propose the\nmodelling of intraday financial returns of multiple assets through copula. The\nproblem originates due to the asynchronous nature of intraday financial data.\nWe propose a consistent estimator of the correlation coefficient in case of\nElliptical copula and show that the plug-in copula estimator is uniformly\nconvergent. For non-elliptical copulas, we capture the dependence through\nKendall's Tau. We demonstrate underestimation of the copula parameter and use a\nquadratic model to propose an improved estimator. In simulations, the proposed\nestimator reduces the bias significantly for a general class of copulas. We\napply the proposed methods to real data of several stock prices.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2019 07:23:05 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 10:51:33 GMT"}], "update_date": "2020-09-16", "authors_parsed": [["Chakrabarti", "Arnab", ""], ["Sen", "Rituparna", ""]]}, {"id": "1904.11145", "submitter": "Ali Habibnia", "authors": "Ali Habibnia (1) and Esfandiar Maasoumi (2) ((1) Virginia Tech, (2)\n  Emory University)", "title": "Forecasting in Big Data Environments: an Adaptable and Automated\n  Shrinkage Estimation of Neural Networks (AAShNet)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers improved forecasting in possibly nonlinear dynamic\nsettings, with high-dimension predictors (\"big data\" environments). To overcome\nthe curse of dimensionality and manage data and model complexity, we examine\nshrinkage estimation of a back-propagation algorithm of a deep neural net with\nskip-layer connections. We expressly include both linear and nonlinear\ncomponents. This is a high-dimensional learning approach including both\nsparsity L1 and smoothness L2 penalties, allowing high-dimensionality and\nnonlinearity to be accommodated in one step. This approach selects significant\npredictors as well as the topology of the neural network. We estimate optimal\nvalues of shrinkage hyperparameters by incorporating a gradient-based\noptimization technique resulting in robust predictions with improved\nreproducibility. The latter has been an issue in some approaches. This is\nstatistically interpretable and unravels some network structure, commonly left\nto a black box. An additional advantage is that the nonlinear part tends to get\npruned if the underlying process is linear. In an application to forecasting\nequity returns, the proposed approach captures nonlinear dynamics between\nequities to enhance forecast performance. It offers an appreciable improvement\nover current univariate and multivariate models by RMSE and actual portfolio\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2019 03:43:02 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Habibnia", "Ali", ""], ["Maasoumi", "Esfandiar", ""]]}, {"id": "1904.12346", "submitter": "Tetsuya Takaishi", "authors": "Tetsuya Takaishi", "title": "Rough volatility of Bitcoin", "comments": "12 pages, 8 figures", "journal-ref": "Finance Research Letters 32 (2020) 101379", "doi": "10.1016/j.frl.2019.101379", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have found that the log-volatility of asset returns exhibit\nroughness. This study investigates roughness or the anti-persistence of Bitcoin\nvolatility. Using the multifractal detrended fluctuation analysis, we obtain\nthe generalized Hurst exponent of the log-volatility increments and find that\nthe generalized Hurst exponent is less than $1/2$, which indicates\nlog-volatility increments that are rough. Furthermore, we find that the\ngeneralized Hurst exponent is not constant. This observation indicates that the\nlog-volatility has multifractal property. Using shuffled time series of the\nlog-volatility increments, we infer that the source of multifractality partly\ncomes from the distributional property.\n", "versions": [{"version": "v1", "created": "Sun, 28 Apr 2019 17:06:17 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Takaishi", "Tetsuya", ""]]}, {"id": "1904.12526", "submitter": "Giuseppe Pernagallo Dr.", "authors": "Paolo Di Caro, Giuseppe Pernagallo, Antonino Damiano Rossello and\n  Benedetto Torrisi", "title": "Empirical facts characterizing banking crises: an analysis via binary\n  time series", "comments": "14 pages, 2 tables, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various works have already showed that common shocks and cross-country\nfinancial linkages caused the banking systems of several countries to be highly\ninterconnected with the result that during bad times, banking crises may arise\nsimultaneously in different countries. Our aim is to provide further evidence\non the topic using a dataset made by dichotomous banking crises time series for\n66 countries from 1800 to 2014. Via the use of heatmap matrices we show that\nseveral countries exhibit pairwise correlation, which means that banking crises\ntend to occur in the same year. Clustering analysis suggests that developed\ncountries (for the most European ones) are highly similar in terms of the path\nof events. An analysis of the events that followed the Great Depression and the\nGreat Recession shows that after the crisis of 2008, banking crises tend to\ncharacterize countries tied by financial links whereas before 2008 contagion\nseems to affect countries in the same geographical area. Clustering analysis\nshows also that after financial liberalization, crises affected countries with\nsimilar economic structures and growth. Further researches should enlighten the\norigin of these linkages investigating how the process of contagion eventually\nhappens.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2019 09:22:44 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Di Caro", "Paolo", ""], ["Pernagallo", "Giuseppe", ""], ["Rossello", "Antonino Damiano", ""], ["Torrisi", "Benedetto", ""]]}, {"id": "1904.13064", "submitter": "Aleksejus Kononovicius dr.", "authors": "Vygintas Gontis, Aleksejus Kononovicius", "title": "Bessel-like birth-death process", "comments": "11 pages, 3 figures", "journal-ref": "Physica A 540: 123119 (2020)", "doi": "10.1016/j.physa.2019.123119", "report-no": null, "categories": "physics.soc-ph q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider models of the population or opinion dynamics which result in the\nnon-linear stochastic differential equations (SDEs) exhibiting the spurious\nlong-range memory. In this context, the correspondence between the description\nof the birth-death processes as the continuous-time Markov chains and the\ncontinuous SDEs is of high importance for the alternatives of modeling. We\npropose and generalize the Bessel-like birth-death process having clear\nrepresentation by the SDEs. The new process helps to integrate the alternatives\nof description and to derive the equations for the probability density function\n(PDF) of the burst and inter-burst duration of the proposed continuous time\nbirth-death processes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2019 06:19:31 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 15:36:39 GMT"}], "update_date": "2019-10-28", "authors_parsed": [["Gontis", "Vygintas", ""], ["Kononovicius", "Aleksejus", ""]]}]