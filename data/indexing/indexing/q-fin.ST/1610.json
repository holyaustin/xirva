[{"id": "1610.00259", "submitter": "Rui Menezes", "authors": "Rui Menezes and Sonia Bentes", "title": "Hysteresis and Duration Dependence of Financial Crises in the US:\n  Evidence from 1871-2016", "comments": "59 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study analyses the duration dependence of events that trigger volatility\npersistence in stock markets. Such events, in our context, are monthly spells\nof contiguous price decline or negative returns for the S&P500 stock market\nindex over the last 145 years. Factors known to affect the duration of these\nspells are the magnitude or intensity of the price decline, long-term interest\nrates and economic recessions, among others. The result of interest is the\nconditional probability of ending a spell of consecutive months over which\nstock market returns remain negative. In this study, we rely on continuous time\nsurvival models in order to investigate this question. Several specifications\nwere attempted, some of which under the proportional hazards assumption and\nothers under the accelerated failure time assumption. The best fit of the\nvarious models endeavored was obtained for the log-normal distribution. This\ndistribution yields a non-monotonic hazard function that increases up to a\nmaximum and then decreases. The peak is achieved 2-3 months after the spells\nonset with a hazard of around 0.9 or higher; this hazard then decays\nasymptotically to zero. Spells duration increase during recessions, when\ninterest rate rises and when price declines are more intense. The main\nconclusion is that short spells of negative returns appear to be mainly\nfrictional while long spells become structural and trigger hysteresis effects\nafter an initial period of adjustment. Although in line with our expectations,\nthese results may be of some importance for policy-makers.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 11:08:10 GMT"}], "update_date": "2016-10-04", "authors_parsed": [["Menezes", "Rui", ""], ["Bentes", "Sonia", ""]]}, {"id": "1610.00332", "submitter": "Mikko Pakkanen", "authors": "Mikkel Bennedsen, Asger Lunde, Mikko S. Pakkanen", "title": "Decoupling the short- and long-term behavior of stochastic volatility", "comments": "48 pages, 7 figures, v3: major revision with increased emphasis on\n  noise-robust estimation, to appear in Journal of Financial Econometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.MF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of continuous-time models of the stochastic\nvolatility of asset prices. The models can simultaneously incorporate roughness\nand slowly decaying autocorrelations, including proper long memory, which are\ntwo stylized facts often found in volatility data. Our prime model is based on\nthe so-called Brownian semistationary process and we derive a number of\ntheoretical properties of this process, relevant to volatility modeling.\nApplying the models to realized volatility measures covering a vast panel of\nassets, we find evidence consistent with the hypothesis that time series of\nrealized measures of volatility are both rough and very persistent. Lastly, we\nillustrate the utility of the models in an extensive forecasting study; we find\nthat the models proposed in this paper outperform a wide array of benchmarks\nconsiderably, indicating that it pays off to exploit both roughness and\npersistence in volatility forecasting.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2016 18:38:48 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 10:26:54 GMT"}, {"version": "v3", "created": "Tue, 5 Jan 2021 15:42:44 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["Bennedsen", "Mikkel", ""], ["Lunde", "Asger", ""], ["Pakkanen", "Mikko S.", ""]]}, {"id": "1610.00795", "submitter": "Daniele Petrone", "authors": "Daniele Petrone and Vito Latora", "title": "A dynamic approach merging network theory and credit risk techniques to\n  assess systemic risk in financial networks", "comments": "8 pages, 5 figures, 1 table", "journal-ref": "Scientific Reports 8 (2018) 5561", "doi": "10.1038/s41598-018-23689-5", "report-no": null, "categories": "q-fin.CP q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interconnectedness of financial institutions affects instability and\ncredit crises. To quantify systemic risk we introduce here the PD model, a\ndynamic model that combines credit risk techniques with a contagion mechanism\non the network of exposures among banks. A potential loss distribution is\nobtained through a multi-period Monte Carlo simulation that considers the\nprobability of default (PD) of the banks and their tendency of defaulting in\nthe same time interval. A contagion process increases the PD of banks exposed\ntoward distressed counterparties. The systemic risk is measured by statistics\nof the loss distribution, while the contribution of each node is quantified by\nthe new measures PDRank and PDImpact. We illustrate how the model works on the\nnetwork of the European Global Systemically Important Banks. For a certain\nrange of the banks' capital and of their assets volatility, our results reveal\nthe emergence of a strong contagion regime where lower default correlation\nbetween banks corresponds to higher losses. This is the opposite of the\ndiversification benefits postulated by standard credit risk models used by\nbanks and regulators who could therefore underestimate the capital needed to\novercome a period of crisis, thereby contributing to the financial system\ninstability.\n", "versions": [{"version": "v1", "created": "Mon, 3 Oct 2016 23:40:44 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 12:34:28 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Petrone", "Daniele", ""], ["Latora", "Vito", ""]]}, {"id": "1610.01149", "submitter": "Wei-Xing Zhou", "authors": "Qing Cai, Hai-Chuan Xu and Wei-Xing Zhou (ECUST)", "title": "Taylor's Law of temporal fluctuation scaling in stock illiquidity", "comments": "14 Latex pages including 4 figures and 5 tables", "journal-ref": "Fluctuation and Noise Letters 15 (4), 1650029 (2016)", "doi": "10.1142/S0219477516500292", "report-no": null, "categories": "q-fin.ST q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taylor's law of temporal fluctuation scaling, variance $\\sim$ $a($mean$)^b$,\nis ubiquitous in natural and social sciences. We report for the first time\nconvincing evidence of a solid temporal fluctuation scaling law in stock\nilliquidity by investigating the mean-variance relationship of the\nhigh-frequency illiquidity of almost all stocks traded on the Shanghai Stock\nExchange (SHSE) and the Shenzhen Stock Exchange (SZSE) during the period from\n1999 to 2011. Taylor's law holds for A-share markets (SZSE Main Board, SZSE\nSmall & Mediate Enterprise Board, SZSE Second Board, and SHSE Main Board) and\nB-share markets (SZSE B-share and SHSE B-share). We find that the scaling\nexponent $b$ is greater than 2 for the A-share markets and less than 2 for the\nB-share markets. We further unveil that Taylor's law holds for stocks in 17\nindustry categories, in 28 industrial sectors and in 31 provinces and\ndirect-controlled municipalities with the majority of scaling exponents\n$b\\in(2,3)$. We also investigate the $\\Delta{t}$-min illiquidity and find that\nthe scaling exponent $b(\\Delta{t})$ increases logarithmically for small\n$\\Delta{t}$ values and decreases fast to a stable level.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2016 13:10:50 GMT"}], "update_date": "2016-10-07", "authors_parsed": [["Cai", "Qing", "", "ECUST"], ["Xu", "Hai-Chuan", "", "ECUST"], ["Zhou", "Wei-Xing", "", "ECUST"]]}, {"id": "1610.02863", "submitter": "Olivier Wintenberger", "authors": "F Blasques, P Gorgi, S Koopman (CREATES), O Wintenberger (University\n  of Copenhagen, LSTA)", "title": "Feasible Invertibility Conditions for Maximum Likelihood Estimation for\n  Observation-Driven Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Invertibility conditions for observation-driven time series models often fail\nto be guaranteed in empirical applications. As a result, the asymptotic theory\nof maximum likelihood and quasi-maximum likelihood estimators may be\ncompromised. We derive considerably weaker conditions that can be used in\npractice to ensure the consistency of the maximum likelihood estimator for a\nwide class of observation-driven time series models. Our consistency results\nhold for both correctly specified and misspecified models. The practical\nrelevance of the theory is highlighted in a set of empirical examples. We\nfurther obtain an asymptotic test and confidence bounds for the unfeasible \"\ntrue \" invertibility region of the parameter space.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2016 11:46:54 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Blasques", "F", "", "CREATES"], ["Gorgi", "P", "", "CREATES"], ["Koopman", "S", "", "CREATES"], ["Wintenberger", "O", "", "University\n  of Copenhagen, LSTA"]]}, {"id": "1610.04334", "submitter": "Akihiko Noda", "authors": "Mikio Ito, Akihiko Noda, Tatsuma Wada", "title": "Time-Varying Comovement of Foreign Exchange Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A time-varying cointegration model for foreign exchange rates is presented.\nUnlike previous studies, we allow the loading matrix in the vector error\ncorrection (VEC) model to be varying over time. Because the loading matrix in\nthe VEC model is associated with the speed at which deviations from the\nlong-run relationship disappear, we propose a new degree of market comovement\\\nbased on the time-varying loading matrix to measure the strength or robustness\nof the long-run relationship over time. Since exchange rates are determined by\nmacrovariables, cointegration among exchange rates implies those macroeconomic\nvariables share common stochastic trends. Therefore, the proposed degree\nmeasures the degree of market comovement. Our main finding is that the market\ncomovement has become stronger over the past quarter century, but the rate at\nwhich market comovement strengthens is decreasing with two major turning\npoints: one in 1995 and the other one in 2008.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2016 05:58:46 GMT"}], "update_date": "2016-10-17", "authors_parsed": [["Ito", "Mikio", ""], ["Noda", "Akihiko", ""], ["Wada", "Tatsuma", ""]]}, {"id": "1610.04760", "submitter": "Oliver Pfante", "authors": "Oliver Pfante and Nils Bertschinger", "title": "Uncertainty Estimates in the Heston Model via Fisher Information", "comments": "29 pages, 11 figures. The text overlap of the first version with\n  arXiv:1609.02108 by other authors has been removed. It occurred in the\n  introduction (page 2 line 22-37) of the first version. A part of the\n  introduction of arXiv:1609.02108 was incidentally adopted. The issue is\n  removed in the second version: the introduction is independent and\n  arXiv:1609.02108 is cited properly", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the information content of European option prices about volatility\nin terms of the Fisher information matrix. We assume that observed option\nprices are centred on the theoretical price provided by Heston's model\ndisturbed by additive Gaussian noise. We fit the likelihood function on the\ncomponents of the VIX, i.e., near- and next-term put and call options on the\nS&P 500 with more than 23 days and less than 37 days to expiration and\nnon-vanishing bid, and compute their Fisher information matrices from the\nGreeks in the Heston model. We find that option prices allow reliable estimates\nof volatility with negligible uncertainty as long as volatility is large\nenough. Interestingly, if volatility drops below a critical value, inferences\nfrom option prices become impossible because Vega, the derivative of a European\noption w.r.t. volatility, nearly vanishes.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2016 16:31:42 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2016 14:11:35 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Pfante", "Oliver", ""], ["Bertschinger", "Nils", ""]]}, {"id": "1610.05383", "submitter": "Marcello Rambaldi", "authors": "Marcello Rambaldi, Vladimir Filimonov, Fabrizio Lillo", "title": "Detection of intensity bursts using Hawkes processes: an application to\n  high frequency financial data", "comments": null, "journal-ref": "Phys. Rev. E 97, 032318 (2018)", "doi": "10.1103/PhysRevE.97.032318", "report-no": null, "categories": "q-fin.TR q-fin.CP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a stationary point process, an intensity burst is defined as a short\ntime period during which the number of counts is larger than the typical count\nrate. It might signal a local non-stationarity or the presence of an external\nperturbation to the system. In this paper we propose a novel procedure for the\ndetection of intensity bursts within the Hawkes process framework. By using a\nmodel selection scheme we show that our procedure can be used to detect\nintensity bursts when both their occurrence time and their total number is\nunknown. Moreover, the initial time of the burst can be determined with a\nprecision given by the typical inter-event time. We apply our methodology to\nthe mid-price change in FX markets showing that these bursts are frequent and\nthat only a relatively small fraction is associated to news arrival. We show\nlead-lag relations in intensity burst occurrence across different FX rates and\nwe discuss their relation with price jumps.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2016 23:41:28 GMT"}], "update_date": "2018-04-04", "authors_parsed": [["Rambaldi", "Marcello", ""], ["Filimonov", "Vladimir", ""], ["Lillo", "Fabrizio", ""]]}, {"id": "1610.05697", "submitter": "Pierluigi Vellucci", "authors": "Loretta Mastroeni and Pierluigi Vellucci", "title": "\"Butterfly Effect\" vs Chaos in Energy Futures Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we test for the sensitive dependence on initial conditions (the\nso called \"butterfly effect\") of energy futures time series (heating oil,\nnatural gas), and thus the determinism of those series. This paper is\ndistinguished from previous studies in the following points: first, we reread\nexistent works in the literature on energy markets, enlightening the role of\n\\emph{butterfly effect} in chaos definition (introduced by Devaney), using this\ndefinition to prevent us from misleading results about ostensible chaoticity of\nthe price series. Second, we test for the time series for sensitive dependence\non initial conditions, introducing a coefficient that describes the determinism\nrate of the series and that represents its reliability level (in percentage).\nThe introduction of this reliability level is motivated by the fact that time\nseries generated from stochastic systems also might show sensitive dependence\non initial conditions. According to this perspective, the maximum reliability\nlevel obtained here is too low to be able to ensure that there is strong\nevidence of sensitive The maximum reliability level obtained here was been\n$\\simeq 56\\% $, too low to ensure strong evidence of sensitive dependence on\ninitial conditions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Aug 2016 12:35:20 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Mastroeni", "Loretta", ""], ["Vellucci", "Pierluigi", ""]]}, {"id": "1610.07028", "submitter": "Jan Korbel", "authors": "Petr Jizba and Jan Korbel", "title": "Techniques for multifractal spectrum estimation in financial time series", "comments": "7 pages", "journal-ref": "Int J Des Nat Ecodyn 10(3), 2015, 261-266", "doi": "10.2495/DNE-V10-N3-261-266", "report-no": null, "categories": "q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multifractal analysis is one of the important approaches that enables us to\nmeasure the complexity of various data via the scaling properties. We compare\nthe most common techniques used for multifractal exponents estimation from both\ntheoretical and practical point of view. Particularly, we discuss the methods\nbased on estimation of R\\'enyi entropy, which provide a powerful tool\nespecially in presence of heavy-tailed data. To put some flesh on bare bones,\nall methods are compared on various real financial datasets, including daily\nand high-frequency data.\n", "versions": [{"version": "v1", "created": "Sat, 22 Oct 2016 10:10:46 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Jizba", "Petr", ""], ["Korbel", "Jan", ""]]}, {"id": "1610.07287", "submitter": "Ling-Yun He", "authors": "Shu-Peng Chen and Ling-Yun He", "title": "The asset price bubbles in emerging financial markets: a new statistical\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bubble is a controversial and important issue. Many methods which based\non the rational expectation have been proposed to detect the bubble. However,\nfor some developing countries, epically China, the asset markets are so young\nthat for many companies, there are no dividends and fundamental value, making\nit difficult (if not impossible) to measure the bubbles by existing methods.\nTherefore, we proposed a simple but effective statistical method and three\nstatistics (that is, C, U, V) to capture and quantify asset price bubbles,\nespecially in immature emerging markets. To present a clear example of the\napplication of this method to real world problems, we also applied our method\nto re-examine empirically the asset price bubble in some stock markets. Our\nmain contributions to current literature are as follows: firstly, this method\ndoes not rely on fundamental value, the discount rates and dividends, therefore\nis applicable to the immature markets without the sufficient data of such\nkinds, secondly, this new method allows us to examine different influences\n(herding behavior, abnormal fluctuation and composite influence) of bubble. Our\nnew statistical approach is, to the best of our knowledge, the only robust way\nin existing literature to to quantify the asset price bubble especially in\nemerging markets.\n", "versions": [{"version": "v1", "created": "Mon, 24 Oct 2016 06:04:44 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["Chen", "Shu-Peng", ""], ["He", "Ling-Yun", ""]]}, {"id": "1610.08104", "submitter": "Jo\\\"el Bun", "authors": "Jo\\\"el Bun, Jean-Philippe Bouchaud and Marc Potters", "title": "Cleaning large correlation matrices: tools from random matrix theory", "comments": "165 pages, article submitted to Physics Reports", "journal-ref": null, "doi": "10.1016/j.physrep.2016.10.005", "report-no": null, "categories": "cond-mat.stat-mech math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This review covers recent results concerning the estimation of large\ncovariance matrices using tools from Random Matrix Theory (RMT). We introduce\nseveral RMT methods and analytical techniques, such as the Replica formalism\nand Free Probability, with an emphasis on the Marchenko-Pastur equation that\nprovides information on the resolvent of multiplicatively corrupted noisy\nmatrices. Special care is devoted to the statistics of the eigenvectors of the\nempirical correlation matrix, which turn out to be crucial for many\napplications. We show in particular how these results can be used to build\nconsistent \"Rotationally Invariant\" estimators (RIE) for large correlation\nmatrices when there is no prior on the structure of the underlying process. The\nlast part of this review is dedicated to some real-world applications within\nfinancial markets as a case in point. We establish empirically the efficacy of\nthe RIE framework, which is found to be superior in this case to all previously\nproposed methods. The case of additively (rather than multiplicatively)\ncorrupted noisy matrices is also dealt with in a special Appendix. Several open\nproblems and interesting technical developments are discussed throughout the\npaper.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2016 21:53:28 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Bun", "Jo\u00ebl", ""], ["Bouchaud", "Jean-Philippe", ""], ["Potters", "Marc", ""]]}, {"id": "1610.08230", "submitter": "Zhi-Qiang Jiang", "authors": "Zhi-Qiang Jiang (ECUST, BU), Gang-Jin Wang (HNU, BU), Askery Canabarro\n  (BU, UFA), Boris Podobnik (ZSEM), Chi Xie (HNU), H. Eugene Stanley (BU),\n  Wei-Xing Zhou (ECUST)", "title": "Short term prediction of extreme returns based on the recurrence\n  interval analysis", "comments": "18 pages, 5 figues, 3 tables", "journal-ref": "Quantitative Finance 18 (3), 353-370 (2018)", "doi": "10.1080/14697688.2017.1373843", "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being able to predict the occurrence of extreme returns is important in\nfinancial risk management. Using the distribution of recurrence intervals---the\nwaiting time between consecutive extremes---we show that these extreme returns\nare predictable on the short term. Examining a range of different types of\nreturns and thresholds we find that recurrence intervals follow a\n$q$-exponential distribution, which we then use to theoretically derive the\nhazard probability $W(\\Delta t |t)$. Maximizing the usefulness of extreme\nforecasts to define an optimized hazard threshold, we indicates a financial\nextreme occurring within the next day when the hazard probability is greater\nthan the optimized threshold. Both in-sample tests and out-of-sample\npredictions indicate that these forecasts are more accurate than a benchmark\nthat ignores the predictive signals. This recurrence interval finding deepens\nour understanding of reoccurring extreme returns and can be applied to forecast\nextremes in risk management.\n", "versions": [{"version": "v1", "created": "Wed, 26 Oct 2016 08:49:33 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Jiang", "Zhi-Qiang", "", "ECUST, BU"], ["Wang", "Gang-Jin", "", "HNU, BU"], ["Canabarro", "Askery", "", "BU, UFA"], ["Podobnik", "Boris", "", "ZSEM"], ["Xie", "Chi", "", "HNU"], ["Stanley", "H. Eugene", "", "BU"], ["Zhou", "Wei-Xing", "", "ECUST"]]}, {"id": "1610.08414", "submitter": "Peter B. Lerner", "authors": "Peter B. Lerner", "title": "The Fellowship of LIBOR: A Study of Spurious Interbank Correlations by\n  the Method of Wigner-Ville Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The manipulation of LIBOR by a group of banks became one of the major blows\nto the remaining confidence in financial industry. Yet, despite an enormous\namount of popular literature on the subject, rigorous time-series studies are\nfew. In my paper, I discuss the following hypothesis. Namely, if we should\nassume for a statistical null, the quotes, which were submitted by the member\nbanks were true, the deviations from the LIBOR should have been entirely random\nbecause they were determined by idiosyncratic conditions by the member banks.\nThis hypothesis can be statistically verified. Serial correlations of the\nrates, which cannot be explained by the differences in credit qualities of the\nmember banks or the domicile Governments, were subjected to correlation tests.\nA new econometric method--the analysis of the Wigner-Ville function borrowed\nfrom quantum mechanics and signal processing--is used and explained for the\nstatistical interpretation of regression residuals.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 19:38:35 GMT"}, {"version": "v2", "created": "Fri, 3 Apr 2020 20:50:41 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Lerner", "Peter B.", ""]]}, {"id": "1610.08415", "submitter": "Olgu Benli", "authors": "T.O. Benli", "title": "A Comparison of Various Electricity Tariff Price Forecasting Techniques\n  in Turkey and Identifying the Impact of Time Series Periods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is very vital for suppliers and distributors to predict the deregulated\nelectricity prices for creating their bidding strategies in the competitive\nmarket area. Pre requirement of succeeding in this field, accurate and suitable\nelectricity tariff price forecasting tools are needed. In the presence of\neffective forecasting tools, taking the decisions of production, merchandising,\nmaintenance and investment with the aim of maximizing the profits and benefits\ncan be successively and effectively done. According to the electricity demand,\nthere are four various electricity tariffs pricing in Turkey; monochromic, day,\npeak and night. The objective is find the best suitable tool for predicting the\nfour pricing periods of electricity and produce short term forecasts (one year\nahead-monthly). Our approach based on finding the best model, which ensures the\nsmallest forecasting error measurements of: MAPE, MAD and MSD. We conduct a\ncomparison of various forecasting approaches in total accounts for nine teen,\nat least all of those have different aspects of methodology. Our beginning step\nwas doing forecasts for the year 2015. We validated and analyzed the\nperformance of our best model and made comparisons to see how well the\nhistorical values of 2015 and forecasted data for that specific period matched.\nResults show that given the time-series data, the recommended models provided\ngood forecasts. Second part of practice, we also include the year 2015, and\ncompute all the models with the time series of January 2011 to December 2015.\nAgain by choosing the best appropriate forecasting model, we conducted the\nforecast process and also analyze the impact of enhancing of time series\nperiods (January 2007 to December 2015) to model that we used for forecasting\nprocess.\n", "versions": [{"version": "v1", "created": "Thu, 18 Aug 2016 07:16:08 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["Benli", "T. O.", ""]]}, {"id": "1610.08416", "submitter": "Jaroslaw Kwapien", "authors": "Jaroslaw Kwapien, Pawel Oswiecimka, Marcin Forczek, Stanislaw Drozdz", "title": "Minimum spanning tree filtering of correlations for varying time scales\n  and size of fluctuations", "comments": null, "journal-ref": "Phys. Rev. E 95, 052313 (2017)", "doi": "10.1103/PhysRevE.95.052313", "report-no": null, "categories": "q-fin.ST physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a recently proposed $q$-dependent detrended cross-correlation\ncoefficient $\\rho_q$, we generalize the concept of minimum spanning tree (MST)\nby introducing a family of $q$-dependent minimum spanning trees ($q$MST) that\nare selective to cross-correlations between different fluctuation amplitudes\nand different time scales. They inherit this ability directly from the\ncoefficients $\\rho_q$ that are processed here to construct a distance matrix.\nConventional MST with detrending corresponds in this context to $q=2$. We apply\nthe $q$MSTs to sample empirical data from the stock market and discuss the\nresults. We show that the $q$MST graphs can complement $\\rho_q$ in\ndisentangling correlations that cannot be observed by the MST graphs based on\n$\\rho_{\\rm DCCA}$ and, therefore, they can be useful in many areas where the\nmultivariate cross-correlations are of interest. We apply our method to data\nfrom the stock market and obtain more information about correlation structure\nof the data than by using $q=2$ only. We show that two sets of signals that\ndiffer from each other statistically can give comparable trees for $q=2$, while\nonly by using the trees for $q \\ne 2$ we become able to distinguish between\nthese sets. We also show that a family of $q$MSTs for a range of $q$ express\nthe diversity of correlations in a manner resembling the multifractal analysis,\nwhere one computes a spectrum of the generalized fractal dimensions, the\ngeneralized Hurst exponents, or the multifractal singularity spectra: the more\ndiverse the correlations are, the more variable the tree topology is for\ndifferent $q$s. Our analysis exhibits that the stocks belonging to the same or\nsimilar industrial sectors are correlated via the fluctuations of moderate\namplitudes, while the largest fluctuations often happen to synchronize in those\nstocks that do not necessarily belong to the same industry.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2016 15:25:00 GMT"}, {"version": "v2", "created": "Thu, 4 May 2017 10:43:32 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Kwapien", "Jaroslaw", ""], ["Oswiecimka", "Pawel", ""], ["Forczek", "Marcin", ""], ["Drozdz", "Stanislaw", ""]]}, {"id": "1610.08921", "submitter": "Maciej Jagielski", "authors": "Maciej Jagielski, Ryszard Kutner, Didier Sornette", "title": "Theory of earthquakes interevent times applied to financial markets", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2017.04.115", "report-no": null, "categories": "q-fin.ST physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the probability density function (PDF) of waiting times between\nfinancial loss exceedances. The empirical PDFs are fitted with the self-excited\nHawkes conditional Poisson process with a long power law memory kernel. The\nHawkes process is the simplest extension of the Poisson process that takes into\naccount how past events influence the occurrence of future events. By analyzing\nthe empirical data for 15 different financial assets, we show that the\nformalism of the Hawkes process used for earthquakes can successfully model the\nPDF of interevent times between successive market losses.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 18:41:48 GMT"}, {"version": "v2", "created": "Fri, 28 Oct 2016 11:06:15 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Jagielski", "Maciej", ""], ["Kutner", "Ryszard", ""], ["Sornette", "Didier", ""]]}, {"id": "1610.09292", "submitter": "Nestor Parolya Jun.-Prof. Dr.", "authors": "Taras Bodnar, Ostap Okhrin, Nestor Parolya", "title": "Optimal Shrinkage Estimator for High-Dimensional Mean Vector", "comments": "20 pages, UPDATE2: revised version of the manuscript accepted for\n  publication in Journal of Multivariate Analysis", "journal-ref": null, "doi": "10.1016/j.jmva.2018.07.004", "report-no": null, "categories": "math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive the optimal linear shrinkage estimator for the\nhigh-dimensional mean vector using random matrix theory. The results are\nobtained under the assumption that both the dimension $p$ and the sample size\n$n$ tend to infinity in such a way that $p/n \\to c\\in(0,\\infty)$. Under weak\nconditions imposed on the underlying data generating mechanism, we find the\nasymptotic equivalents to the optimal shrinkage intensities and estimate them\nconsistently. The proposed nonparametric estimator for the high-dimensional\nmean vector has a simple structure and is proven to minimize asymptotically,\nwith probability $1$, the quadratic loss when $c\\in(0,1)$. When $c\\in(1,\n\\infty)$ we modify the estimator by using a feasible estimator for the\nprecision covariance matrix. To this end, an exhaustive simulation study and an\napplication to real data are provided where the proposed estimator is compared\nwith known benchmarks from the literature. It turns out that the existing\nestimators of the mean vector, including the new proposal, converge to the\nsample mean vector when the true mean vector has an unbounded Euclidean norm.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2016 16:10:19 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 18:21:44 GMT"}, {"version": "v3", "created": "Sat, 14 Jul 2018 10:53:20 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Bodnar", "Taras", ""], ["Okhrin", "Ostap", ""], ["Parolya", "Nestor", ""]]}, {"id": "1610.09519", "submitter": "Wei-Xing Zhou", "authors": "Zhi-Qiang Jiang (ECUST, BU), Xing-Lu Gao (ECUST), Wei-Xing Zhou\n  (ECUST), H. Eugene Stanley (BU)", "title": "Multifractal cross wavelet analysis", "comments": null, "journal-ref": "Fractals 25 (6), 1750054 (2017)", "doi": "10.1142/S0218348X17500542", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems are composed of mutually interacting components and the\noutput values of these components are usually long-range cross-correlated. We\npropose a method to characterize the joint multifractal nature of such\nlong-range cross correlations based on wavelet analysis, termed multifractal\ncross wavelet analysis (MFXWT). We assess the performance of the MFXWT method\nby performing extensive numerical experiments on the dual binomial measures\nwith multifractal cross correlations and the bivariate fractional Brownian\nmotions (bFBMs) with monofractal cross correlations. For binomial multifractal\nmeasures, the empirical joint multifractality of MFXWT is found to be in\napproximate agreement with the theoretical formula. For bFBMs, MFXWT may\nprovide spurious multifractality because of the wide spanning range of the\nmultifractal spectrum. We also apply the MFXWT method to stock market indexes\nand uncover intriguing joint multifractal nature in pairs of index returns and\nvolatilities.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2016 14:50:27 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 09:13:37 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Jiang", "Zhi-Qiang", "", "ECUST, BU"], ["Gao", "Xing-Lu", "", "ECUST"], ["Zhou", "Wei-Xing", "", "ECUST"], ["Stanley", "H. Eugene", "", "BU"]]}, {"id": "1610.09812", "submitter": "Zhongxing Wang", "authors": "Zhongxing Wang, Yan Yan and Xiaosong Chen", "title": "Long-range Correlation and Market Segmentation in Bond Market", "comments": "19 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.physa.2017.04.066", "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper looks into the analysis of the long-range auto-correlations and\ncross-correlations in bond market. Based on Detrended Moving Average (DMA)\nmethod, empirical results present a clear evidence of long-range persistence\nthat exists in one year scale. The degree of long-range correlation related to\nmaturities has an upward tendency with a peak in short term. These findings\nconfirm the expectations of fractal market hypothesis (FMH). Furthermore, we\nhave developed a method based on a complex network to study the long-range\ncross-correlation structure and apply it to our data, and found a clear pattern\nof market segmentation in the long run. We also detected the nature of\nlong-range correlation in the sub-period 2007 to 2012 and 2011 to 2016. The\nresult from our research shows that long-range auto-correlations are decreasing\nin the recent years while long-range cross-correlations are strengthening.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 07:43:40 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Wang", "Zhongxing", ""], ["Yan", "Yan", ""], ["Chen", "Xiaosong", ""]]}]