[{"id": "2010.01031", "submitter": "Xiao Fan Liu", "authors": "Xiao Fan Liu, Xin-Jian Jiang, Si-Hao Liu, Chi Kong Tse", "title": "Knowledge Discovery in Cryptocurrency Transactions: A Survey", "comments": "60 pages, 217 refs, 6 tables, and 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.CY q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryptocurrencies gain trust in users by publicly disclosing the full creation\nand transaction history. In return, the transaction history faithfully records\nthe whole spectrum of cryptocurrency user behaviors. This article analyzes and\nsummarizes the existing research on knowledge discovery in the cryptocurrency\ntransactions using data mining techniques. Specifically, we classify the\nexisting research into three aspects, i.e., transaction tracings and blockchain\naddress linking, the analyses of collective user behaviors, and the study of\nindividual user behaviors. For each aspect, we present the problems, summarize\nthe methodologies, and discuss major findings in the literature. Furthermore,\nan enumeration of transaction data parsing and visualization tools and services\nis also provided. Finally, we outline several future directions in this\nresearch area, such as the current rapid development of Decentralized Finance\n(De-Fi) and digital fiat money.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 14:38:08 GMT"}], "update_date": "2020-10-05", "authors_parsed": [["Liu", "Xiao Fan", ""], ["Jiang", "Xin-Jian", ""], ["Liu", "Si-Hao", ""], ["Tse", "Chi Kong", ""]]}, {"id": "2010.01157", "submitter": "Miroslav Fil", "authors": "Miroslav Fil", "title": "Gold Standard Pairs Trading Rules: Are They Valid?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairs trading is a strategy based on exploiting mean reversion in prices of\nsecurities. It has been shown to generate significant excess returns, but its\nprofitability has dropped significantly in recent periods. We employ the most\ncommon distance and cointegration methods on US equities from 1990 to 2020\nincluding the Covid-19 crisis. The strategy overall fails to outperform the\nmarket benchmark even with hyperparameter tuning, but it performs very strongly\nduring bear markets. Furthermore, we demonstrate that market factors have a\nstrong relationship with the optimal parametrization for the strategy, and\nadjustments are appropriate for modern market conditions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Oct 2020 18:26:17 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Fil", "Miroslav", ""]]}, {"id": "2010.01197", "submitter": "Xing Wang", "authors": "Xing Wang, Yijun Wang, Bin Weng, Aleksandr Vinel", "title": "Stock2Vec: A Hybrid Deep Learning Framework for Stock Market Prediction\n  with Representation Learning and Temporal Convolutional Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have proposed to develop a global hybrid deep learning framework to\npredict the daily prices in the stock market. With representation learning, we\nderived an embedding called Stock2Vec, which gives us insight for the\nrelationship among different stocks, while the temporal convolutional layers\nare used for automatically capturing effective temporal patterns both within\nand across series. Evaluated on S&P 500, our hybrid framework integrates both\nadvantages and achieves better performance on the stock price prediction task\nthan several popular benchmarked models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Sep 2020 22:54:30 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Wang", "Xing", ""], ["Wang", "Yijun", ""], ["Weng", "Bin", ""], ["Vinel", "Aleksandr", ""]]}, {"id": "2010.01199", "submitter": "Caglar Tuncay", "authors": "Caglar Tuncay", "title": "Market laws", "comments": "12 pages, 12 figures. Another version may appear in Physica A, soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  More than one billion data sampled with different frequencies from several\nfinancial instruments were investigated with the aim of testing whether they\ninvolve power law. As a result, a known power law with the power exponent\naround -4 was detected in the empirical distributions of the relative returns.\nMoreover, a number of new power law behaviors with various power exponents were\nexplored in the same data. Further on, a model based on finite sums over\nnumerous Maxwell-Boltzmann type distribution functions with random\n(pseudorandom) multipliers in the exponent were proposed to deal with the\nempirical distributions involving power laws. The results indicate that the\nproposed model may be universal.\n", "versions": [{"version": "v1", "created": "Thu, 1 Oct 2020 14:25:51 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Tuncay", "Caglar", ""]]}, {"id": "2010.01241", "submitter": "Samuel Holt", "authors": "Rakshit Jha, Mattijs De Paepe, Samuel Holt, James West, Shaun Ng", "title": "Deep Learning for Digital Asset Limit Order Books", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that temporal CNNs accurately predict bitcoin spot price\nmovements from limit order book data. On a 2 second prediction time horizon we\nachieve 71\\% walk-forward accuracy on the popular cryptocurrency exchange\ncoinbase. Our model can be trained in less than a day on commodity GPUs which\ncould be installed into colocation centers allowing for model sync with\nexisting faster orderbook prediction models. We provide source code and data at\nhttps://github.com/Globe-Research/deep-orderbook.\n", "versions": [{"version": "v1", "created": "Sat, 3 Oct 2020 00:57:27 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Jha", "Rakshit", ""], ["De Paepe", "Mattijs", ""], ["Holt", "Samuel", ""], ["West", "James", ""], ["Ng", "Shaun", ""]]}, {"id": "2010.01996", "submitter": "Junfeng Hu", "authors": "Junfeng Hu, Xiaosa Li, Yuru Xu, Shaowu Wu, Bin Zheng", "title": "Evaluation of company investment value based on machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, company investment value evaluation models are established\nbased on comprehensive company information. After data mining and extracting a\nset of 436 feature parameters, an optimal subset of features is obtained by\ndimension reduction through tree-based feature selection, followed by the\n5-fold cross-validation using XGBoost and LightGBM models. The results show\nthat the Root-Mean-Square Error (RMSE) reached 3.098 and 3.059, respectively.\nIn order to further improve the stability and generalization capability,\nBayesian Ridge Regression has been used to train a stacking model based on the\nXGBoost and LightGBM models. The corresponding RMSE is up to 3.047. Finally,\nthe importance of different features to the LightGBM model is analysed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Sep 2020 11:37:09 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Hu", "Junfeng", ""], ["Li", "Xiaosa", ""], ["Xu", "Yuru", ""], ["Wu", "Shaowu", ""], ["Zheng", "Bin", ""]]}, {"id": "2010.04610", "submitter": "Mikko Pakkanen", "authors": "Anine E. Bolko, Kim Christensen, Mikko S. Pakkanen, Bezirgen Veliyev", "title": "Roughness in spot variance? A GMM approach for estimation of fractional\n  log-normal stochastic volatility models using realized measures", "comments": "42 pages, 2 figures, v2: updated numerical methods and other minor\n  improvements", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a generalized method of moments approach for joint\nestimation of the parameters of a fractional log-normal stochastic volatility\nmodel. We show that with an arbitrary Hurst exponent an estimator based on\nintegrated variance is consistent. Moreover, under stronger conditions we also\nderive a central limit theorem. These results stand even when integrated\nvariance is replaced with a realized measure of volatility calculated from\ndiscrete high-frequency data. However, in practice a realized estimator\ncontains sampling error, the effect of which is to skew the fractal coefficient\ntoward \"roughness\". We construct an analytical approach to control this error.\nIn a simulation study, we demonstrate convincing small sample properties of our\napproach based both on integrated and realized variance over the entire memory\nspectrum. We show that the bias correction attenuates any systematic deviance\nin the estimated parameters. Our procedure is applied to empirical\nhigh-frequency data from numerous leading equity indexes. With our robust\napproach the Hurst index is estimated around 0.05, confirming roughness in\nintegrated variance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Oct 2020 14:39:34 GMT"}, {"version": "v2", "created": "Sun, 18 Jul 2021 16:16:17 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Bolko", "Anine E.", ""], ["Christensen", "Kim", ""], ["Pakkanen", "Mikko S.", ""], ["Veliyev", "Bezirgen", ""]]}, {"id": "2010.05601", "submitter": "Conrad Beyers", "authors": "Arno Botha, Conrad Beyers, Pieter de Villiers", "title": "The loss optimisation of loan recovery decision times using forecast\n  cash flows", "comments": "29 pages (including appendix), 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM q-fin.CP q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A theoretical method is empirically illustrated in finding the best time to\nforsake a loan such that the overall credit loss is minimised. This is\npredicated by forecasting the future cash flows of a loan portfolio up to the\ncontractual term, as a remedy to the inherent right-censoring of real-world\n`incomplete' portfolios. Two techniques, a simple probabilistic model as well\nas an eight-state Markov chain, are used to forecast these cash flows\nindependently. We train both techniques from different segments within\nresidential mortgage data, provided by a large South African bank, as part of a\ncomparative experimental framework. As a result, the recovery decision's\nimplied timing is empirically illustrated as a multi-period optimisation\nproblem across uncertain cash flows and competing costs. Using a delinquency\nmeasure as a central criterion, our procedure helps to find a loss-optimal\nthreshold at which loan recovery should ideally occur for a given portfolio.\nFurthermore, both the portfolio's historical risk profile and forecasting\nthereof are shown to influence the timing of the recovery decision. This work\ncan therefore facilitate the revision of relevant bank policies or strategies\ntowards optimising the loan collections process, especially that of secured\nlending.\n", "versions": [{"version": "v1", "created": "Mon, 12 Oct 2020 11:12:39 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Botha", "Arno", ""], ["Beyers", "Conrad", ""], ["de Villiers", "Pieter", ""]]}, {"id": "2010.06227", "submitter": "Jonathan Berrisch", "authors": "Jonathan Berrisch, Florian Ziel", "title": "Modeling and Probababilistic Forecasting of Natural Gas Prices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-fin.RM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the problem of modeling and forecasting European\nDay-Ahead and Month-Ahead natural gas prices. For this, we propose two distinct\nprobabilistic models that can be utilized in risk- and portfolio management. We\nuse daily pricing data ranging from 2011 to 2020. Extensive descriptive data\nanalysis shows that both time series feature heavy tails, conditional\nheteroscedasticity, and show asymmetric behavior in their differences. We\npropose state-space time series models under skewed, heavy-tailed distribution\nto capture all stylized facts in the data. They include the impact of\nautocorrelation, seasonality, risk premia, temperature, storage levels, the\nprice of European Emission Allowances, and related fuel prices of oil, coal,\nand electricity. We provide a rigorous model diagnostic and interpret all model\ncomponents in detail. Additionally, we conduct a probabilistic forecasting\nstudy with significance test and compare the predictive performance against\nliterature benchmarks. The proposed Day-Ahead (Month-Ahead) model leads to a\n$13\\%$ ($9$\\%) reduction in out of sample CRPS compared to the best performing\nbenchmark model, mainly due to adequate modeling of the volatility and heavy\ntails.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 08:22:00 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Berrisch", "Jonathan", ""], ["Ziel", "Florian", ""]]}, {"id": "2010.06306", "submitter": "Morteza Nattagh Najafi", "authors": "Fatemeh Gharari, Karina Arias-Calluari, Fernando Alonso-Marroquin,\n  Morteza. N. Najafi", "title": "Local and Non-local Fractional Porous Media Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently it was observed that the probability distribution of the price\nreturn in S\\&P500 can be modeled by $q$-Gaussian distributions, where various\nphases (weak, strong super diffusion and normal diffusion) are separated by\ndifferent fitting parameters (Phys Rev. E 99, 062313, 2019). Here we analyze\nthe fractional extensions of the porous media equation and show that all of\nthem admit solutions in terms of generalized $q$-Gaussian functions. Three\nkinds of \"fractionalization\" are considered: \\textit{local}, referring to the\nsituation where the fractional derivatives for both space and time are local;\n\\textit{non-local}, where both space and time fractional derivatives are\nnon-local; and \\textit{mixed}, where one derivative is local, and another is\nnon-local. Although, for the \\textit{local} and \\textit{non-local} cases we\nfind $q$-Gaussian solutions , they differ in the number of free parameters.\nThis makes differences to the quality of fitting to the real data. We test the\nresults for the S\\&P 500 price return and found that the local and non-local\nschemes fit the data better than the classic porous media equation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Oct 2020 11:46:03 GMT"}], "update_date": "2020-10-14", "authors_parsed": [["Gharari", "Fatemeh", ""], ["Arias-Calluari", "Karina", ""], ["Alonso-Marroquin", "Fernando", ""], ["Najafi", "Morteza. N.", ""]]}, {"id": "2010.07289", "submitter": "Kriste Krstovski", "authors": "Paul Glasserman, Kriste Krstovski, Paul Laliberte, Harry Mamaysky", "title": "Choosing News Topics to Explain Stock Market Returns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze methods for selecting topics in news articles to explain stock\nreturns. We find, through empirical and theoretical results, that supervised\nLatent Dirichlet Allocation (sLDA) implemented through Gibbs sampling in a\nstochastic EM algorithm will often overfit returns to the detriment of the\ntopic model. We obtain better out-of-sample performance through a random search\nof plain LDA models. A branching procedure that reinforces effective topic\nassignments often performs best. We test methods on an archive of over 90,000\nnews articles about S&P 500 firms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 01:38:35 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Glasserman", "Paul", ""], ["Krstovski", "Kriste", ""], ["Laliberte", "Paul", ""], ["Mamaysky", "Harry", ""]]}, {"id": "2010.07402", "submitter": "Yeguang Chi", "authors": "Yeguang Chi, Wenyan Hao", "title": "A Horserace of Volatility Models for Cryptocurrency: Evidence from\n  Bitcoin Spot and Option Markets", "comments": "41 pages, 6 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We test various volatility models using the Bitcoin spot price series. Our\nmodels include HIST, EMA ARCH, GARCH, and EGARCH, models. Both of our\nin-sample-fit and out-of-sample-forecast results suggest that GARCH and EGARCH\nmodels perform much better than other models. Moreover, the EGARCH model's\nasymmetric term is positive and insignificant, which suggests that Bitcoin\nprices lack the asymmetric volatility response to past returns. Finally, we\nformulate an option trading strategy by exploiting the volatility spread\nbetween the GARCH volatility forecast and the option's implied volatility. We\nshow that a simple volatility-spread trading strategy with delta-hedging can\nyield robust profits.\n", "versions": [{"version": "v1", "created": "Sun, 4 Oct 2020 08:16:52 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Chi", "Yeguang", ""], ["Hao", "Wenyan", ""]]}, {"id": "2010.07404", "submitter": "Qi Zhao", "authors": "Qi Zhao", "title": "A Deep Learning Framework for Predicting Digital Asset Price Movement\n  from Trade-by-trade Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI cs.LG q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a deep learning framework based on Long Short-term Memory\nNetwork(LSTM) that predicts price movement of cryptocurrencies from\ntrade-by-trade data. The main focus of this study is on predicting short-term\nprice changes in a fixed time horizon from a looking back period. By carefully\ndesigning features and detailed searching for best hyper-parameters, the model\nis trained to achieve high performance on nearly a year of trade-by-trade data.\nThe optimal model delivers stable high performance(over 60% accuracy) on\nout-of-sample test periods. In a realistic trading simulation setting, the\nprediction made by the model could be easily monetized. Moreover, this study\nshows that the LSTM model could extract universal features from trade-by-trade\ndata, as the learned parameters well maintain their high performance on other\ncryptocurrency instruments that were not included in training data. This study\nexceeds existing researches in term of the scale and precision of data used, as\nwell as the high prediction accuracy achieved.\n", "versions": [{"version": "v1", "created": "Sun, 11 Oct 2020 10:42:02 GMT"}], "update_date": "2020-10-16", "authors_parsed": [["Zhao", "Qi", ""]]}, {"id": "2010.08028", "submitter": "Roberto Baviera", "authors": "Roberto Baviera", "title": "The measure of model risk in credit capital requirements", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit capital requirements in Internal Rating Based approaches require the\ncalibration of two key parameters: the probability of default and the\nloss-given-default. This letter considers the uncertainty about these two\nparameters and models this uncertainty in an elementary way: it shows how this\nestimation risk can be computed and properly taken into account in regulatory\ncapital.\n  We analyse two standard real datasets: one composed by all corporates rated\nby Moody's and one limited only to the speculative grade ones. We statistically\ntest model hypotheses on both marginal distributions and parameter dependency.\nWe compute the estimation risk impact and observe that parameter dependency\nraises substantially the tail risk in capital requirements. The results are\nstriking with a required increase in regulatory capital in the range\n$38\\%$-$66\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 15 Oct 2020 21:29:27 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Baviera", "Roberto", ""]]}, {"id": "2010.08259", "submitter": "Demetrio Lacava", "authors": "Demetrio Lacava and Giampiero M. Gallo and Edoardo Otranto", "title": "Unconventional Policies Effects on Stock Market Volatility: A MAP\n  Approach", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Taking the European Central Bank unconventional policies as a reference, we\nsuggest a class of Multiplicative Error Models (MEM) taylored to analyze the\nimpact such policies have on stock market volatility. The new set of models,\ncalled MEM with Asymmetry and Policy effects (MAP), keeps the base volatility\ndynamics separate from a component reproducing policy effects, with an increase\nin volatility on announcement days and a decrease unfolding implementation\neffects. When applied to four Eurozone markets, a Model Confidence Set approach\nfinds a significant improvement of the forecasting power of the proxy after the\nExpanded Asset Purchase Programme implementation; a multi--step ahead\nforecasting exercise estimates the duration of the effect, and, by shocking the\npolicy variable, we are able to quantify the reduction in volatility which is\nmore marked for debt--troubled countries.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:22:05 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 09:45:54 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Lacava", "Demetrio", ""], ["Gallo", "Giampiero M.", ""], ["Otranto", "Edoardo", ""]]}, {"id": "2010.08263", "submitter": "Qi Wu", "authors": "Xing Yan, Weizhong Zhang, Lin Ma, Wei Liu, Qi Wu", "title": "Parsimonious Quantile Regression of Financial Asset Tail Dynamics via\n  Sequential Learning", "comments": "NeurIPS 2018:1582-1592", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.LG q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parsimonious quantile regression framework to learn the dynamic\ntail behaviors of financial asset returns. Our model captures well both the\ntime-varying characteristic and the asymmetrical heavy-tail property of\nfinancial time series. It combines the merits of a popular sequential neural\nnetwork model, i.e., LSTM, with a novel parametric quantile function that we\nconstruct to represent the conditional distribution of asset returns. Our model\nalso captures individually the serial dependences of higher moments, rather\nthan just the volatility. Across a wide range of asset classes, the\nout-of-sample forecasts of conditional quantiles or VaR of our model outperform\nthe GARCH family. Further, the proposed approach does not suffer from the issue\nof quantile crossing, nor does it expose to the ill-posedness comparing to the\nparametric probability density function approach.\n", "versions": [{"version": "v1", "created": "Fri, 16 Oct 2020 09:35:52 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Yan", "Xing", ""], ["Zhang", "Weizhong", ""], ["Ma", "Lin", ""], ["Liu", "Wei", ""], ["Wu", "Qi", ""]]}, {"id": "2010.08400", "submitter": "Tahir Miriyev", "authors": "Tahir Miriyev, Alessandro Contu, Kevin Schafers, Ion Gabriel Ion", "title": "Hybrid Modelling Approaches for Forecasting Energy Spot Prices in EPEC\n  market", "comments": "European Consortium for Mathematics in Industry", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we considered several hybrid modelling approaches for\nforecasting energy spot prices in EPEC market. Hybridization is performed\nthrough combining a Naive model, Fourier analysis, ARMA and GARCH models, a\nmean-reversion and jump-diffusion model, and Recurrent Neural Networks (RNN).\nTraining data was given in terms of electricity prices for 2013-2014 years, and\ntest data as a year of 2015.\n", "versions": [{"version": "v1", "created": "Wed, 14 Oct 2020 12:45:53 GMT"}], "update_date": "2020-10-19", "authors_parsed": [["Miriyev", "Tahir", ""], ["Contu", "Alessandro", ""], ["Schafers", "Kevin", ""], ["Ion", "Ion Gabriel", ""]]}, {"id": "2010.08698", "submitter": "Dan Wang", "authors": "Dan Wang, Tianrui Wang, Ionu\\c{t} Florescu", "title": "Is Image Encoding Beneficial for Deep Learning in Finance? An Analysis\n  of Image Encoding Methods for the Application of Convolutional Neural\n  Networks in Finance", "comments": "12 pages, 7 figures, 13 tables in the main content. IEEE Internet of\n  Things Journal", "journal-ref": null, "doi": "10.1109/JIOT.2020.3030492", "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2012, SEC mandated all corporate filings for any company doing business in\nUS be entered into the Electronic Data Gathering, Analysis, and Retrieval\n(EDGAR) system. In this work we are investigating ways to analyze the data\navailable through EDGAR database. This may serve portfolio managers (pension\nfunds, mutual funds, insurance, hedge funds) to get automated insights into\ncompanies they invest in, to better manage their portfolios. The analysis is\nbased on Artificial Neural Networks applied to the data.} In particular, one of\nthe most popular machine learning methods, the Convolutional Neural Network\n(CNN) architecture, originally developed to interpret and classify images, is\nnow being used to interpret financial data. This work investigates the best way\nto input data collected from the SEC filings into a CNN architecture. We\nincorporate accounting principles and mathematical methods into the design of\nthree image encoding methods. Specifically, two methods are derived from\naccounting principles (Sequential Arrangement, Category Chunk Arrangement) and\none is using a purely mathematical technique (Hilbert Vector Arrangement). In\nthis work we analyze fundamental financial data as well as financial ratio data\nand study companies from the financial, healthcare and IT sectors in the United\nStates. We find that using imaging techniques to input data for CNN works\nbetter for financial ratio data but is not significantly better than simply\nusing the 1D input directly for fundamental data. We do not find the Hilbert\nVector Arrangement technique to be significantly better than other imaging\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 17 Oct 2020 02:14:39 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wang", "Dan", ""], ["Wang", "Tianrui", ""], ["Florescu", "Ionu\u0163", ""]]}, {"id": "2010.08890", "submitter": "Giuseppe Brandi", "authors": "Ioannis P. Antoniades, Giuseppe Brandi, L. G. Magafas, T. Di Matteo", "title": "The use of scaling properties to detect relevant changes in financial\n  time series: a new visual warning tool", "comments": null, "journal-ref": null, "doi": "10.1016/j.physa.2020.125561", "report-no": null, "categories": "q-fin.ST q-fin.RM", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The dynamical evolution of multiscaling in financial time series is\ninvestigated using time-dependent Generalized Hurst Exponents (GHE), $H_q$, for\nvarious values of the parameter $q$. Using $H_q$, we introduce a new visual\nmethodology to algorithmically detect critical changes in the scaling of the\nunderlying complex time-series. The methodology involves the degree of\nmultiscaling at a particular time instance, the multiscaling trend which is\ncalculated by the Change-Point Analysis method, and a rigorous evaluation of\nthe statistical significance of the results. Using this algorithm, we have\nidentified particular patterns in the temporal co-evolution of the different\n$H_q$ time-series. These GHE patterns, distinguish in a statistically robust\nway, not only between time periods of uniscaling and multiscaling, but also\namong different types of multiscaling: symmetric multiscaling (M) and\nasymmetric multiscaling (A). We apply the visual methodology to time-series\ncomprising of daily close prices of four stock market indices: two major ones\n(S\\&P~500 and NIKKEI) and two peripheral ones (Athens Stock Exchange general\nIndex and Bombay-SENSEX). Results show that multiscaling varies greatly with\ntime: time periods of strong multiscaling behavior and time periods of\nuniscaling behavior are interchanged while transitions from uniscaling to\nmultiscaling behavior occur before critical market events, such as stock market\nbubbles. Moreover, particular asymmetric multiscaling patterns appear during\ncritical stock market eras and provide useful information about market\nconditions. In particular, they can be used as 'fingerprints' of a turbulent\nmarket period as well as provide warning signals for an upcoming stock market\n'bubble'. The applied visual methodology also appears to distinguish between\nexogenous and endogenous stock market crises, based on the observed patterns\nbefore the actual events.\n", "versions": [{"version": "v1", "created": "Sun, 18 Oct 2020 00:01:33 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 10:19:28 GMT"}, {"version": "v3", "created": "Tue, 3 Nov 2020 17:14:52 GMT"}, {"version": "v4", "created": "Wed, 9 Dec 2020 10:47:50 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Antoniades", "Ioannis P.", ""], ["Brandi", "Giuseppe", ""], ["Magafas", "L. G.", ""], ["Di Matteo", "T.", ""]]}, {"id": "2010.09937", "submitter": "Marcin Pitera", "authors": "Marcin Pitera and Thorsten Schmidt", "title": "Unbiased estimation and backtesting of risk in the context of heavy\n  tails", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.ST q-fin.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the estimation of risk is an important question in the daily business\nof banks and insurances, many existing plug-in estimation procedures suffer\nfrom an unnecessary bias. This often leads to the underestimation of risk and\nnegatively impacts backtesting results, especially in small sample cases. In\nthis article we show that the link between estimation bias and backtesting can\nbe traced back to the dual relationship between risk measures and the\ncorresponding performance measures, and discuss this in reference to\nvalue-at-risk and expected shortfall frameworks. Motivated by this finding, we\npropose a new algorithm for bias correction and show how to apply it for\ngeneralized Pareto distributions. In particular, we consider value-at-risk and\nexpected shortfall plug-in estimators, and show that the application of our\nalgorithm leads to gain in efficiency when heavy tails exist in the data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Oct 2020 00:37:52 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 17:34:09 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pitera", "Marcin", ""], ["Schmidt", "Thorsten", ""]]}, {"id": "2010.12002", "submitter": "Nino Antulov-Fantulin", "authors": "Metod Jazbec, Barna P\\'asztor, Felix Faltings, Nino Antulov-Fantulin,\n  Petter N. Kolm", "title": "On the impact of publicly available news and information transfer to\n  financial markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG physics.soc-ph q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We quantify the propagation and absorption of large-scale publicly available\nnews articles from the World Wide Web to financial markets. To extract publicly\navailable information, we use the news archives from the Common Crawl, a\nnonprofit organization that crawls a large part of the web. We develop a\nprocessing pipeline to identify news articles associated with the constituent\ncompanies in the S\\&P 500 index, an equity market index that measures the stock\nperformance of U.S. companies. Using machine learning techniques, we extract\nsentiment scores from the Common Crawl News data and employ tools from\ninformation theory to quantify the information transfer from public news\narticles to the U.S. stock market. Furthermore, we analyze and quantify the\neconomic significance of the news-based information with a simple\nsentiment-based portfolio trading strategy. Our findings provides support for\nthat information in publicly available news on the World Wide Web has a\nstatistically and economically significant impact on events in financial\nmarkets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Oct 2020 19:33:20 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Jazbec", "Metod", ""], ["P\u00e1sztor", "Barna", ""], ["Faltings", "Felix", ""], ["Antulov-Fantulin", "Nino", ""], ["Kolm", "Petter N.", ""]]}, {"id": "2010.12270", "submitter": "Jun-Ichi Maskawa", "authors": "Jun-ichi Maskawa and Koji Kuroda", "title": "Model of continuous random cascade processes in financial markets", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article present a continuous cascade model of volatility formulated as a\nstochastic differential equation. Two independent Brownian motions are\nintroduced as random sources triggering the volatility cascade. One\nmultiplicatively combines with volatility; the other does so additively.\nAssuming that the latter acts perturbatively on the system, then the model\nparameters are estimated by application to an actual stock price time series.\nNumerical calculation of the Fokker--Planck equation derived from the\nstochastic differential equation is conducted using the estimated values of\nparameters. The results reproduce the pdf of the empirical volatility, the\nmultifractality of the time series, and other empirical facts.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 09:57:01 GMT"}], "update_date": "2020-10-26", "authors_parsed": [["Maskawa", "Jun-ichi", ""], ["Kuroda", "Koji", ""]]}, {"id": "2010.13245", "submitter": "Zhipu Zhou", "authors": "Zhipu Zhou and Alexander Shkolnik and Sang-Yun Oh", "title": "Endogenous Representation of Asset Returns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor modeling of asset returns has been a dominant practice in investment\nscience since the introduction of the Capital Asset Pricing Model (CAPM) and\nthe Arbitrage Pricing Theory (APT). The factors, which account for the\nsystematic risk, are either specified or interpreted to be exogenous. They\nexplain a significant portion of the risk in large portfolios. We propose a\nframework that asks how much of the risk, that we see in equity markets, may be\nexplained by the asset returns themselves. To answer this question, we\ndecompose the asset returns into an endogenous component and the remainder, and\nanalyze the properties of the resulting risk decomposition. Statistical methods\nto estimate this decomposition from data are provided along with empirical\ntests. Our results point to the possibility that most of the risk in equity\nmarkets may be explained by a sparse network of interacting assets (or their\nissuing firms). This sparse network can give the appearance of a set exogenous\nfactors where, in fact, there may be none. We illustrate our results with\nseveral case studies.\n", "versions": [{"version": "v1", "created": "Sun, 25 Oct 2020 22:44:13 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2020 15:09:53 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Zhou", "Zhipu", ""], ["Shkolnik", "Alexander", ""], ["Oh", "Sang-Yun", ""]]}, {"id": "2010.13892", "submitter": "Amir Mukeri", "authors": "Amir Mukeri, Habibullah Shaikh, Dr. D.P. Gaikwad", "title": "Financial Data Analysis Using Expert Bayesian Framework For Bankruptcy\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, bankruptcy forecasting has gained lot of attention from\nresearchers as well as practitioners in the field of financial risk management.\nFor bankruptcy prediction, various approaches proposed in the past and\ncurrently in practice relies on accounting ratios and using statistical\nmodeling or machine learning methods. These models have had varying degrees of\nsuccesses. Models such as Linear Discriminant Analysis or Artificial Neural\nNetwork employ discriminative classification techniques. They lack explicit\nprovision to include prior expert knowledge. In this paper, we propose another\nroute of generative modeling using Expert Bayesian framework. The biggest\nadvantage of the proposed framework is an explicit inclusion of expert judgment\nin the modeling process. Also the proposed methodology provides a way to\nquantify uncertainty in prediction. As a result the model built using Bayesian\nframework is highly flexible, interpretable and intuitive in nature. The\nproposed approach is well suited for highly regulated or safety critical\napplications such as in finance or in medical diagnosis. In such cases accuracy\nin the prediction is not the only concern for decision makers. Decision makers\nand other stakeholders are also interested in uncertainty in the prediction as\nwell as interpretability of the model. We empirically demonstrate these\nbenefits of proposed framework on real world dataset using Stan, a\nprobabilistic programming language. We found that the proposed model is either\ncomparable or superior to the other existing methods. Also resulting model has\nmuch less False Positive Rate compared to many existing state of the art\nmethods. The corresponding R code for the experiments is available at Github\nrepository.\n", "versions": [{"version": "v1", "created": "Mon, 19 Oct 2020 19:09:02 GMT"}, {"version": "v2", "created": "Fri, 30 Oct 2020 05:30:18 GMT"}], "update_date": "2020-11-02", "authors_parsed": [["Mukeri", "Amir", ""], ["Shaikh", "Habibullah", ""], ["Gaikwad", "Dr. D. P.", ""]]}, {"id": "2010.15105", "submitter": "Juan Camilo Henao Londono", "authors": "Juan C. Henao-Londono, Sebastian M. Krause and Thomas Guhr", "title": "Price response functions and spread impact in correlated financial\n  markets", "comments": "19 pages, 12 figures, 5 tables", "journal-ref": null, "doi": "10.1140/epjb/s10051-021-00077-z", "report-no": null, "categories": "q-fin.ST physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research on the response of stock prices to trading activity revealed\nlong lasting effects, even across stocks of different companies. These results\nimply non-Markovian effects in price formation and when trading many stocks at\nthe same time, in particular trading costs and price correlations. How the\nprice response is measured depends on data set and research focus. However, it\nis important to clarify, how the details of the price response definition\nmodify the results. Here, we evaluate different price response implementations\nfor the Trades and Quotes (TAQ) data set from the NASDAQ stock market and find\nthat the results are qualitatively the same for two different definitions of\ntime scale, but the response can vary by up to a factor of two. Further, we\nshow the key importance of the order between trade signs and returns,\ndisplaying the changes in the signal strength. Moreover, we confirm the\ndominating contribution of immediate price response directly after a trade, as\nwe find that delayed responses are suppressed. Finally, we test the impact of\nthe spread in the price response, detecting that large spreads have stronger\nimpact.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 17:45:32 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Henao-Londono", "Juan C.", ""], ["Krause", "Sebastian M.", ""], ["Guhr", "Thomas", ""]]}, {"id": "2010.15111", "submitter": "Elizabeth Fons", "authors": "Elizabeth Fons, Paula Dawson, Xiao-jun Zeng, John Keane and Alexandros\n  Iosifidis", "title": "Evaluating data augmentation for financial time series classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data augmentation methods in combination with deep neural networks have been\nused extensively in computer vision on classification tasks, achieving great\nsuccess; however, their use in time series classification is still at an early\nstage. This is even more so in the field of financial prediction, where data\ntends to be small, noisy and non-stationary. In this paper we evaluate several\naugmentation methods applied to stocks datasets using two state-of-the-art deep\nlearning models. The results show that several augmentation methods\nsignificantly improve financial performance when used in combination with a\ntrading strategy. For a relatively small dataset ($\\approx30K$ samples),\naugmentation methods achieve up to $400\\%$ improvement in risk adjusted return\nperformance; for a larger stock dataset ($\\approx300K$ samples), results show\nup to $40\\%$ improvement.\n", "versions": [{"version": "v1", "created": "Wed, 28 Oct 2020 17:53:57 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Fons", "Elizabeth", ""], ["Dawson", "Paula", ""], ["Zeng", "Xiao-jun", ""], ["Keane", "John", ""], ["Iosifidis", "Alexandros", ""]]}, {"id": "2010.15403", "submitter": "Marcin W\\k{a}torek", "authors": "Marcin W\\k{a}torek, Stanis{\\l}aw Dro\\.zd\\.z, Jaros{\\l}aw Kwapie\\'n,\n  Ludovico Minati, Pawe{\\l} O\\'swi\\k{e}cimka, Marek Stanuszek", "title": "Multiscale characteristics of the emerging global cryptocurrency market", "comments": null, "journal-ref": "Physics Reports 901 1-82 (2021)", "doi": "10.1016/j.physrep.2020.10.005", "report-no": null, "categories": "q-fin.ST cs.CE econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The review introduces the history of cryptocurrencies, offering a description\nof the blockchain technology behind them. Differences between cryptocurrencies\nand the exchanges on which they are traded have been shown. The central part\nsurveys the analysis of cryptocurrency price changes on various platforms. The\nstatistical properties of the fluctuations in the cryptocurrency market have\nbeen compared to the traditional markets. With the help of the latest\nstatistical physics methods the non-linear correlations and multiscale\ncharacteristics of the cryptocurrency market are analyzed. In the last part the\nco-evolution of the correlation structure among the 100 cryptocurrencies having\nthe largest capitalization is retraced. The detailed topology of cryptocurrency\nnetwork on the Binance platform from bitcoin perspective is also considered.\nFinally, an interesting observation on the Covid-19 pandemic impact on the\ncryptocurrency market is presented and discussed: recently we have witnessed a\n\"phase transition\" of the cryptocurrencies from being a hedge opportunity for\nthe investors fleeing the traditional markets to become a part of the global\nmarket that is substantially coupled to the traditional financial instruments\nlike the currencies, stocks, and commodities.\n  The main contribution is an extensive demonstration that structural\nself-organization in the cryptocurrency markets has caused the same to attain\ncomplexity characteristics that are nearly indistinguishable from the Forex\nmarket at the level of individual time-series. However, the cross-correlations\nbetween the exchange rates on cryptocurrency platforms differ from it. The\ncryptocurrency market is less synchronized and the information flows more\nslowly, which results in more frequent arbitrage opportunities. The methodology\nused in the review allows the latter to be detected, and lead-lag relationships\nto be discovered.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 07:56:01 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 20:32:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["W\u0105torek", "Marcin", ""], ["Dro\u017cd\u017c", "Stanis\u0142aw", ""], ["Kwapie\u0144", "Jaros\u0142aw", ""], ["Minati", "Ludovico", ""], ["O\u015bwi\u0119cimka", "Pawe\u0142", ""], ["Stanuszek", "Marek", ""]]}, {"id": "2010.15586", "submitter": "Xianchao Wu", "authors": "Xianchao Wu", "title": "Event-Driven Learning of Systematic Behaviours in Stock Markets", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is reported that financial news, especially financial events expressed in\nnews, provide information to investors' long/short decisions and influence the\nmovements of stock markets. Motivated by this, we leverage financial event\nstreams to train a classification neural network that detects latent\nevent-stock linkages and stock markets' systematic behaviours in the U.S. stock\nmarket. Our proposed pipeline includes (1) a combined event extraction method\nthat utilizes Open Information Extraction and neural co-reference resolution,\n(2) a BERT/ALBERT enhanced representation of events, and (3) an extended\nhierarchical attention network that includes attentions on event, news and\ntemporal levels. Our pipeline achieves significantly better accuracies and\nhigher simulated annualized returns than state-of-the-art models when being\napplied to predicting Standard\\&Poor 500, Dow Jones, Nasdaq indices and 10\nindividual stocks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Oct 2020 16:14:25 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Wu", "Xianchao", ""]]}, {"id": "2010.15611", "submitter": "Samuel Holt", "authors": "Faizaan Pervaiz, Christopher Goh, Ashley Pennington, Samuel Holt,\n  James West, Shaun Ng", "title": "Fear and Volatility in Digital Assets", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show Bitcoin implied volatility on a 5 minute time horizon is modestly\npredictable from price, volatility momentum and alternative data including\nsentiment and engagement. Lagged Bitcoin index price and volatility movements\ncontribute to the model alongside Google Trends with markets responding often\nseveral hours later. The code and datasets used in this paper can be found at\nhttps://github.com/Globe-Research/bitfear.\n", "versions": [{"version": "v1", "created": "Thu, 29 Oct 2020 14:01:07 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Pervaiz", "Faizaan", ""], ["Goh", "Christopher", ""], ["Pennington", "Ashley", ""], ["Holt", "Samuel", ""], ["West", "James", ""], ["Ng", "Shaun", ""]]}]