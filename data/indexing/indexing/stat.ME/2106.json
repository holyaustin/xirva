[{"id": "2106.00097", "submitter": "Helton Saulo", "authors": "Roberto Vila, Helton Saulo and Jamer Roldan", "title": "On some properties of the bimodal normal distribution and its bivariate\n  version", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we derive some novel properties of the bimodal normal\ndistribution. Some of its mathematical properties are examined. We provide a\nformal proof for the bimodality and assess identifiability. We then discuss the\nmaximum likelihood estimates as well as the existence of these estimates, and\nalso some asymptotic properties of the estimator of the parameter that controls\nthe bimodality. A bivariate version of the BN distribution is derived and some\ncharacteristics such as covariance and correlation are analyzed. We study\nstationarity and ergodicity and a triangular array central limit theorem.\nFinally, a Monte Carlo study is carried out for evaluating the performance of\nthe maximum likelihood estimates.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 20:52:07 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Vila", "Roberto", ""], ["Saulo", "Helton", ""], ["Roldan", "Jamer", ""]]}, {"id": "2106.00170", "submitter": "Isaac Gibbs", "authors": "Isaac Gibbs and Emmanuel Cand\\`es", "title": "Adaptive Conformal Inference Under Distribution Shift", "comments": "21 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop methods for forming prediction sets in an online setting where the\ndata generating distribution is allowed to vary over time in an unknown\nfashion. Our framework builds on ideas from conformal inference to provide a\ngeneral wrapper that can be combined with any black box method that produces\npoint predictions of the unseen label or estimated quantiles of its\ndistribution. While previous conformal inference methods rely on the assumption\nthat the data points are exchangeable, our adaptive approach provably achieves\nthe desired long-term coverage frequency irrespective of the true data\ngenerating process. We accomplish this by modelling the distribution shift as a\nlearning problem in a single parameter whose optimal value is varying over time\nand must be continuously re-estimated. We test our method, adaptive conformal\ninference, on two real world datasets and find that its predictions are robust\nto visible and significant distribution shifts.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 01:37:32 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 13:43:01 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Gibbs", "Isaac", ""], ["Cand\u00e8s", "Emmanuel", ""]]}, {"id": "2106.00225", "submitter": "Zhen Lin", "authors": "Zhen Lin, Shubhendu Trivedi, Jimeng Sun", "title": "Locally Valid and Discriminative Confidence Intervals for Deep Learning\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Crucial for building trust in deep learning models for critical real-world\napplications is efficient and theoretically sound uncertainty quantification, a\ntask that continues to be challenging. Useful uncertainty information is\nexpected to have two key properties: It should be valid (guaranteeing coverage)\nand discriminative (more uncertain when the expected risk is high). Moreover,\nwhen combined with deep learning (DL) methods, it should be scalable and affect\nthe DL model performance minimally. Most existing Bayesian methods lack\nfrequentist coverage guarantees and usually affect model performance. The few\navailable frequentist methods are rarely discriminative and/or violate coverage\nguarantees due to unrealistic assumptions. Moreover, many methods are expensive\nor require substantial modifications to the base neural network. Building upon\nrecent advances in conformal prediction and leveraging the classical idea of\nkernel regression, we propose Locally Valid and Discriminative confidence\nintervals (LVD), a simple, efficient and lightweight method to construct\ndiscriminative confidence intervals (CIs) for almost any DL model. With no\nassumptions on the data distribution, such CIs also offer finite-sample local\ncoverage guarantees (contrasted to the simpler marginal coverage). Using a\ndiverse set of datasets, we empirically verify that besides being the only\nlocally valid method, LVD also exceeds or matches the performance (including\ncoverage rate and prediction accuracy) of existing uncertainty quantification\nmethods, while offering additional benefits in scalability and flexibility.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 04:39:56 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Lin", "Zhen", ""], ["Trivedi", "Shubhendu", ""], ["Sun", "Jimeng", ""]]}, {"id": "2106.00456", "submitter": "Thanh Vinh Vo", "authors": "Thanh Vinh Vo, Trong Nghia Hoang, Young Lee, Tze-Yun Leong", "title": "Federated Estimation of Causal Effects from Observational Data", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern applications collect data that comes in federated spirit, with\ndata kept locally and undisclosed. Till date, most insight into the causal\ninference requires data to be stored in a central repository. We present a\nnovel framework for causal inference with federated data sources. We assess and\nintegrate local causal effects from different private data sources without\ncentralizing them. Then, the treatment effects on subjects from observational\ndata using a non-parametric reformulation of the classical potential outcomes\nframework is estimated. We model the potential outcomes as a random function\ndistributed by Gaussian processes, whose defining parameters can be efficiently\nlearned from multiple data sources, respecting privacy constraints. We\ndemonstrate the promise and efficiency of the proposed approach through a set\nof simulated and real-world benchmark examples.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 08:06:00 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Vo", "Thanh Vinh", ""], ["Hoang", "Trong Nghia", ""], ["Lee", "Young", ""], ["Leong", "Tze-Yun", ""]]}, {"id": "2106.00457", "submitter": "Andrea Gilardi", "authors": "Andrea Gilardi, Riccardo Borgoni and Jorge Mateu", "title": "A non-separable first-order spatio-temporal intensity for events on\n  linear networks: an application to ambulance interventions", "comments": "20 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The algorithms used for optimal management of ambulances require accurate\ndescription and prediction of the spatio-temporal evolution of emergency\ninterventions. In the last years, several authors have proposed sophisticated\nstatistical approaches to forecast the ambulance dispatches, typically\nmodelling the events as a point pattern occurring on a planar region.\nNevertheless, ambulance interventions can be more appropriately modelled as a\nrealisation of a point process occurring along a network of lines, such as a\nroad network. The constrained spatial domain raises specific challenges and\nunique methodological problems that cannot be ignored when developing a proper\nstatistical model. Hence, this paper proposes a spatiotemporal model to analyse\nthe ambulance interventions that occurred in the road network of Milan (Italy)\nfrom 2015 to 2017. We adopt a non-separable first-order intensity function with\nspatial and temporal terms. The temporal component is estimated\nsemi-parametrically using a Poisson regression model, while the spatial\ndimension is estimated nonparametrically using a network kernel function. A set\nof weights is included in the spatial term to capture space-time interactions,\ninducing non-separability in the intensity function. A series of maps and\ngraphical tests show that our approach successfully models the ambulance\ninterventions and captures the space-time patterns.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:07:42 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gilardi", "Andrea", ""], ["Borgoni", "Riccardo", ""], ["Mateu", "Jorge", ""]]}, {"id": "2106.00463", "submitter": "Ziyue Huang", "authors": "Ziyue Huang, Yuting Liang, Ke Yi", "title": "Instance-optimal Mean Estimation Under Differential Privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean estimation under differential privacy is a fundamental problem, but\nworst-case optimal mechanisms do not offer meaningful utility guarantees in\npractice when the global sensitivity is very large. Instead, various heuristics\nhave been proposed to reduce the error on real-world data that do not resemble\nthe worst-case instance. This paper takes a principled approach, yielding a\nmechanism that is instance-optimal in a strong sense. In addition to its\ntheoretical optimality, the mechanism is also simple and practical, and adapts\nto a variety of data characteristics without the need of parameter tuning. It\neasily extends to the local and shuffle model as well.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:15:50 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Huang", "Ziyue", ""], ["Liang", "Yuting", ""], ["Yi", "Ke", ""]]}, {"id": "2106.00492", "submitter": "Nicholas Gray", "authors": "Nicholas Gray and Scott Ferson", "title": "Logistic Regression Through the Veil of Imprecise Data", "comments": "21 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression is an important statistical tool for assessing the\nprobability of an outcome based upon some predictive variables. Standard\nmethods can only deal with precisely known data, however many datasets have\nuncertainties which traditional methods either reduce to a single point or\ncompletely disregarded. In this paper we show that it is possible to include\nthese uncertainties by considering an imprecise logistic regression model using\nthe set of possible models that can be obtained from values from within the\nintervals. This has the advantage of clearly expressing the epistemic\nuncertainty removed by traditional methods.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 13:51:46 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Gray", "Nicholas", ""], ["Ferson", "Scott", ""]]}, {"id": "2106.00616", "submitter": "Stanislav Nagy", "authors": "Petra Laketa and Stanislav Nagy", "title": "Halfspace depth for general measures: The ray basis theorem and its\n  consequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The halfspace depth is a prominent tool of nonparametric multivariate\nanalysis. The upper level sets of the depth, termed the trimmed regions of a\nmeasure, serve as a natural generalization of the quantiles and inter-quantile\nregions to higher-dimensional spaces. The smallest non-empty trimmed region,\ncoined the halfspace median of a measure, generalizes the median. We focus on\nthe (inverse) ray basis theorem for the halfspace depth, a crucial theoretical\nresult that characterizes the halfspace median by a covering property. First, a\nnovel elementary proof of that statement is provided, under minimal assumptions\non the underlying measure. The proof applies not only to the median, but also\nto other trimmed regions. Motivated by the technical development of the amended\nray basis theorem, we specify connections between the trimmed regions, floating\nbodies, and additional equi-affine convex sets related to the depth. As a\nconsequence, minimal conditions for the strict monotonicity of the depth are\nobtained. Applications to the computation of the depth and robust estimation\nare outlined.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 10:49:09 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Laketa", "Petra", ""], ["Nagy", "Stanislav", ""]]}, {"id": "2106.00762", "submitter": "Preetam Nandy", "authors": "Preetam Nandy, Divya Venugopalan, Chun Lo, Shaunak Chatterjee", "title": "A/B Testing for Recommender Systems in a Two-sided Marketplace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two-sided marketplaces are standard business models of many online platforms\n(e.g., Amazon, Facebook, LinkedIn), wherein the platforms have consumers,\nbuyers or content viewers on one side and producers, sellers or\ncontent-creators on the other. Consumer side measurement of the impact of a\ntreatment variant can be done via simple online A/B testing. \\textit{Producer\nside measurement is more challenging because the producer experience depends on\nthe treatment assignment of the consumers}. Existing approaches for producer\nside measurement are either based on graph cluster-based randomization or on\ncertain treatment propagation assumptions. The former approach results in\nlow-powered experiments as the producer-consumer network density increases and\nthe latter approach lacks a strict notion of error control. In this paper, we\npropose (i) a quantification of the quality of a producer side experiment, and\n(ii) a new experiment design mechanism that generates high quality experiments\nbased on this quantification. Our approach, called UniCoRn ({Uni}fying\n{Co}unterfactual {R}a{n}kings), provides explicit control over the quality of\nthe experiment and its computation cost. Further, we prove that our experiment\ndesign is optimal. Our approach is agnostic to the density of the\nproducer-consumer network and does not rely on any treatment propagation\nassumption. Moreover, unlike the existing approaches, we do not need to know\nthe underlying network in advance, making this widely applicable to the\nindustrial setting where the underlying network is unknown and challenging to\npredict a priori due to its dynamic nature. We use simulations to thoroughly\nvalidate our approach and compare it against existing methods. We also\nimplement UniCoRn in an edge recommendation application that serves tens of\nmillions of members and billions of edge recommendations daily.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:11:42 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Nandy", "Preetam", ""], ["Venugopalan", "Divya", ""], ["Lo", "Chun", ""], ["Chatterjee", "Shaunak", ""]]}, {"id": "2106.00788", "submitter": "Jason Poulos", "authors": "Jason Poulos, Andrea Albanese, Andrea Mercatanti, Fan Li", "title": "Retrospective causal inference via matrix completion, with an evaluation\n  of the effect of European integration on cross-border employment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM econ.GN q-fin.EC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method of retrospective counterfactual imputation in panel data\nsettings with later-treated and always-treated units, but no never-treated\nunits. We use the observed outcomes to impute the counterfactual outcomes of\nthe later-treated using a matrix completion estimator. We propose a novel\npropensity-score and elapsed-time weighting of the estimator's objective\nfunction to correct for differences in the observed covariate and unobserved\nfixed effects distributions, and elapsed time since treatment between groups.\nOur methodology is motivated by studying the effect of two milestones of\nEuropean integration -- the Free Movement of persons and the Schengen Agreement\n-- on the share of cross-border workers in sending border regions. We apply the\nproposed method to the European Labour Force Survey (ELFS) data and provide\nevidence that opening the border almost doubled the probability of working\nbeyond the border in Eastern European regions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 20:42:55 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Poulos", "Jason", ""], ["Albanese", "Andrea", ""], ["Mercatanti", "Andrea", ""], ["Li", "Fan", ""]]}, {"id": "2106.00797", "submitter": "Maxime Vono", "authors": "Maxime Vono, Vincent Plassier, Alain Durmus, Aymeric Dieuleveut, Eric\n  Moulines", "title": "QLSD: Quantised Langevin stochastic dynamics for Bayesian federated\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning aims at conducting inference when data are decentralised\nand locally stored on several clients, under two main constraints: data\nownership and communication overhead. In this paper, we address these issues\nunder the Bayesian paradigm. To this end, we propose a novel Markov chain Monte\nCarlo algorithm coined \\texttt{QLSD} built upon quantised versions of\nstochastic gradient Langevin dynamics. To improve performance in a big data\nregime, we introduce variance-reduced alternatives of our methodology referred\nto as \\texttt{QLSD}$^\\star$ and \\texttt{QLSD}$^{++}$. We provide both\nnon-asymptotic and asymptotic convergence guarantees for the proposed\nalgorithms and illustrate their benefits on several federated learning\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2021 21:08:54 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Vono", "Maxime", ""], ["Plassier", "Vincent", ""], ["Durmus", "Alain", ""], ["Dieuleveut", "Aymeric", ""], ["Moulines", "Eric", ""]]}, {"id": "2106.00914", "submitter": "Guannan Wang", "authors": "Guannan Wang and Jue Wang", "title": "On Selection of Semiparametric Spatial Regression Models", "comments": null, "journal-ref": "STAT, 2019", "doi": "10.1002/sta4.221", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the variable selection techniques for a class of\nsemiparametric spatial regression models which allow one to study the effects\nof explanatory variables in the presence of the spatial information. The\nspatial smoothing problem in the nonparametric part is tackled by means of\nbivariate splines over triangulation, which is able to deal efficiently with\ndata distributed over irregularly shaped regions. In addition, we develop a\nunified procedure for variable selection to identify significant covariates\nunder a double penalization framework, and we show that the penalized\nestimators enjoy the \"oracle\" property. The proposed method can simultaneously\nidentify non-zero spatially distributed covariates and solve the problem of\n\"leakage\" across complex domains of the functional spatial component. To\nestimate the standard deviations of the proposed estimators for the\ncoefficients, a sandwich formula is developed as well. In the end, Monte Carlo\nsimulation examples and a real data example are provided to illustrate the\nproposed methodology. All technical proofs are given in the supplementary\nmaterials.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 03:14:16 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wang", "Guannan", ""], ["Wang", "Jue", ""]]}, {"id": "2106.00939", "submitter": "Wenlu Tang", "authors": "Wenlu Tang, Yuanyuan Lin, Linlin Dai and Kani Chen", "title": "Combining case-control studies for identifiability and efficiency\n  improvement in logistic regression", "comments": "Yuanyuan Lin (E-mail: ylin@sta.cuhk.edu.hk), Associate Professor in\n  Department of Statistics, The Chinese University of Hong Kong, Hong Kong is\n  the corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can two separate case-control studies, one about Hepatitis disease and the\nother about Fibrosis, for example, be combined together? It would be hugely\nbeneficial if two or more separately conducted case-control studies, even for\nentirely irrelevant purposes, can be merged together with a unified analysis\nthat produces better statistical properties, e.g., more accurate estimation of\nparameters. In this paper, we show that, when using the popular logistic\nregression model, the combined/integrative analysis produces a more accurate\nestimation of the slope parameters than the single case-control study. It is\nknown that, in a single logistic case-control study, the intercept is not\nidentifiable, contrary to prospective studies. In combined case-control\nstudies, however, the intercepts are proved to be identifiable under mild\nconditions. The resulting maximum likelihood estimates of the intercepts and\nslopes are proved to be consistent and asymptotically normal, with asymptotic\nvariances achieving the semiparametric efficiency lower bound.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 04:54:51 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Tang", "Wenlu", ""], ["Lin", "Yuanyuan", ""], ["Dai", "Linlin", ""], ["Chen", "Kani", ""]]}, {"id": "2106.00996", "submitter": "Yang Liu", "authors": "Yang Liu, Robert J.B. Goudie", "title": "Generalized Geographically Weighted Regression Model within a\n  Modularized Bayesian Framework", "comments": "30 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Geographically weighted regression (GWR) models handle geographical\ndependence through a spatially varying coefficient model and have been widely\nused in applied science, but its Bayesian extension is unclear because it\ninvolves a weighted log-likelihood which does not imply a probability\ndistribution on data. We present a Bayesian GWR model and show that its essence\nis dealing with partial misspecification of the model. Current modularized\nBayesian inference methods accommodate partial misspecification from single\ncomponent of the model. We extend these methods to handle partial\nmisspecification in more than one component of the model, as required for our\nBayesian GWR model. Information from the various spatial locations is\nmanipulated via a geographically weighted kernel and the optimal manipulation\nis chosen according to a Kullback-Leibler (KL) divergence. We justify the model\nvia an information risk minimization approach and show the consistency of the\nproposed estimator in terms of a geographically weighted KL divergence.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 07:40:49 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Liu", "Yang", ""], ["Goudie", "Robert J. B.", ""]]}, {"id": "2106.01104", "submitter": "Shuhao Jiao", "authors": "Shuhao Jiao, Ron D. Frostig, and Hernando Ombao", "title": "Filtrated Common Functional Principal Components for Multivariate\n  Functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local field potentials (LFPs) are signals that measure electrical activity in\nlocalized cortical regions from multiple implanted tetrodes in the human or\nanimal brain. They can be treated as multivariate functional data (i.e., curves\nobserved at many tetrodes spread across a patch on the surface of the cortex).\nMost multivariate functional data contain both global features (which are\nshared in common to all curves) as well isolated features (common only to a\nsmall subset of curves). The goal is this paper is to develop a procedure for\ncapturing this common features. We propose a novel tree-structured functional\nprincipal component (filt-fPC) model through low-dimensional functional\nrepresentation, specifically via filtration. A popular approach to dimension\nreduction of functional data is functional principal components analysis\n(fPCA). Ordinary fPCA can only capture the major information of one population,\nbut fail to reveal the similarity of variation pattern of different groups,\nwhich is potentially related to functional connectivity of brain. One major\nadvantage of the proposed filt-fPC method is the ability to extracting\ncomponents that are common to multiple groups, and meanwhile preserves the\nidiosyncratic individual features of different groups, leading to a\nparsimonious and interpretable low dimensional representation of multivariate\nfunctional data. Another advantage is that the extracted functional principal\ncomponents satisfy the orthonormal property for each set, making filt-fPC\nscores easy to be obtained. The proposed filt-fPC method was employed to study\nthe impact of a shock (induced stroke) on the functional organization structure\nof the rat brain. Finally we point to further directions as this filtration\nidea can also be generalized to other functional statistical models, such as\nfunctional regression, classification and functional times series models.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:11:16 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Jiao", "Shuhao", ""], ["Frostig", "Ron D.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2106.01121", "submitter": "Motonobu Kanagawa", "authors": "Veit Wild, Motonobu Kanagawa, Dino Sejdinovic", "title": "Connections and Equivalences between the Nystr\\\"om Method and Sparse\n  Variational Gaussian Processes", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the connections between sparse approximation methods for\nmaking kernel methods and Gaussian processes (GPs) scalable to massive data,\nfocusing on the Nystr\\\"om method and the Sparse Variational Gaussian Processes\n(SVGP). While sparse approximation methods for GPs and kernel methods share\nsome algebraic similarities, the literature lacks a deep understanding of how\nand why they are related. This is a possible obstacle for the communications\nbetween the GP and kernel communities, making it difficult to transfer results\nfrom one side to the other. Our motivation is to remove this possible obstacle,\nby clarifying the connections between the sparse approximations for GPs and\nkernel methods. In this work, we study the two popular approaches, the\nNystr\\\"om and SVGP approximations, in the context of a regression problem, and\nestablish various connections and equivalences between them. In particular, we\nprovide an RKHS interpretation of the SVGP approximation, and show that the\nEvidence Lower Bound of the SVGP contains the objective function of the\nNystr\\\"om approximation, revealing the origin of the algebraic equivalence\nbetween the two approaches. We also study recently established convergence\nresults for the SVGP and how they are related to the approximation quality of\nthe Nystr\\\"om method.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 12:45:49 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Wild", "Veit", ""], ["Kanagawa", "Motonobu", ""], ["Sejdinovic", "Dino", ""]]}, {"id": "2106.01150", "submitter": "Yang Yang", "authors": "Yang Yang, Yanrong Yang, Han Lin Shang", "title": "Feature Extraction for Functional Time Series: Theory and Application to\n  NIR Spectroscopy Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a novel method to extract global and local features of functional\ntime series. The global features concerning the dominant modes of variation\nover the entire function domain, and local features of function variations over\nparticular short intervals within function domain, are both important in\nfunctional data analysis. Functional principal component analysis (FPCA),\nthough a key feature extraction tool, only focus on capturing the dominant\nglobal features, neglecting highly localized features. We introduce a FPCA-BTW\nmethod that initially extracts global features of functional data via FPCA, and\nthen extracts local features by block thresholding of wavelet (BTW)\ncoefficients. Using Monte Carlo simulations, along with an empirical\napplication on near-infrared spectroscopy data of wood panels, we illustrate\nthat the proposed method outperforms competing methods including FPCA and\nsparse FPCA in the estimation functional processes. Moreover, extracted local\nfeatures inheriting serial dependence of the original functional time series\ncontribute to more accurate forecasts. Finally, we develop asymptotic\nproperties of FPCA-BTW estimators, discovering the interaction between\nconvergence rates of global and local features.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 13:40:28 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Yang", "Yang", ""], ["Yang", "Yanrong", ""], ["Shang", "Han Lin", ""]]}, {"id": "2106.01214", "submitter": "Jack Jewson", "authors": "Jack Jewson and David Rossell", "title": "General Bayesian Loss Function Selection and the use of Improper Models", "comments": "Keywords: Loss functions; Improper models; General Bayes; Hyv\\\"arinen\n  score; Robust regression; Kernel density estimation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statisticians often face the choice between using probability models or a\nparadigm defined by minimising a loss function. Both approaches are useful and,\nif the loss can be re-cast into a proper probability model, there are many\ntools to decide which model or loss is more appropriate for the observed data,\nin the sense of explaining the data's nature. However, when the loss leads to\nan improper model, there are no principled ways to guide this choice. We\naddress this task by combining the Hyv\\\"arinen score, which naturally targets\ninfinitesimal relative probabilities, and general Bayesian updating, which\nprovides a unifying framework for inference on losses and models. Specifically\nwe propose the H-score, a general Bayesian selection criterion and prove that\nit consistently selects the (possibly improper) model closest to the\ndata-generating truth in Fisher's divergence. We also prove that an associated\nH-posterior consistently learns optimal hyper-parameters featuring in loss\nfunctions, including a challenging tempering parameter in generalised Bayesian\ninference. As salient examples, we consider robust regression and\nnon-parametric density estimation where popular loss functions define improper\nmodels for the data and hence cannot be dealt with using standard model\nselection tools. These examples illustrate advantages in robustness-efficiency\ntrade-offs and provide a Bayesian implementation for kernel density estimation,\nopening a new avenue for Bayesian non-parametrics.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 15:05:42 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Jewson", "Jack", ""], ["Rossell", "David", ""]]}, {"id": "2106.01315", "submitter": "Jing Ma", "authors": "Jing Ma, Yushun Dong, Zheng Huang, Daniel Mietchen, Jundong Li", "title": "Assessing the Causal Impact of COVID-19 Related Policies on Outbreak\n  Dynamics: A Case Study in the US", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To mitigate the spread of COVID-19 pandemic, decision-makers and public\nauthorities have announced various non-pharmaceutical policies. Analyzing the\ncausal impact of these policies in reducing the spread of COVID-19 is important\nfor future policy-making. The main challenge here is the existence of\nunobserved confounders (e.g., vigilance of residents). Besides, as the\nconfounders may be time-varying during COVID-19 (e.g., vigilance of residents\nchanges in the course of the pandemic), it is even more difficult to capture\nthem. In this paper, we study the problem of assessing the causal effects of\ndifferent COVID-19 related policies on the outbreak dynamics in different\ncounties at any given time period. To this end, we integrate data about\ndifferent COVID-19 related policies (treatment) and outbreak dynamics (outcome)\nfor different United States counties over time and analyze them with respect to\nvariables that can infer the confounders, including the covariates of different\ncounties, their relational information and historical information. Based on\nthese data, we develop a neural network based causal effect estimation\nframework which leverages above information in observational data and learns\nthe representations of time-varying (unobserved) confounders. In this way, it\nenables us to quantify the causal impact of policies at different\ngranularities, ranging from a category of policies with a certain goal to a\nspecific policy type in this category. Besides, experimental results also\nindicate the effectiveness of our proposed framework in capturing the\nconfounders for quantifying the causal impact of different policies. More\nspecifically, compared with several baseline methods, our framework captures\nthe outbreak dynamics more accurately, and our assessment of policies is more\nconsistent with existing epidemiological studies of COVID-19.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 00:40:24 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Ma", "Jing", ""], ["Dong", "Yushun", ""], ["Huang", "Zheng", ""], ["Mietchen", "Daniel", ""], ["Li", "Jundong", ""]]}, {"id": "2106.01431", "submitter": "Guannan Wang", "authors": "Shan Yu, Guannan Wang, Li Wang and Lijian Yang", "title": "Multivariate Spline Estimation and Inference for Image-On-Scalar\n  Regression", "comments": null, "journal-ref": null, "doi": "10.5705/ss.202019.0188", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent data analyses in biomedical imaging studies, we consider\na class of image-on-scalar regression models for imaging responses and scalar\npredictors. We propose using flexible multivariate splines over triangulations\nto handle the irregular domain of the objects of interest on the images, as\nwell as other characteristics of images. The proposed estimators of the\ncoefficient functions are proved to be root-n consistent and asymptotically\nnormal under some regularity conditions. We also provide a consistent and\ncomputationally efficient estimator of the covariance function. Asymptotic\npointwise confidence intervals and data-driven simultaneous confidence\ncorridors for the coefficient functions are constructed. Our method can\nsimultaneously estimate and make inferences on the coefficient functions while\nincorporating spatial heterogeneity and spatial correlation. A highly efficient\nand scalable estimation algorithm is developed. Monte Carlo simulation studies\nare conducted to examine the finite-sample performance of the proposed method,\nwhich is then applied to the spatially normalized positron emission tomography\ndata of the Alzheimer's Disease Neuroimaging Initiative.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 19:21:36 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Yu", "Shan", ""], ["Wang", "Guannan", ""], ["Wang", "Li", ""], ["Yang", "Lijian", ""]]}, {"id": "2106.01552", "submitter": "Luyao Lin", "authors": "Luyao Lin, Derek Bingham, Floor Broekgaarden, Ilya Mandel", "title": "Uncertainty Quantification of a Computer Model for Binary Black Hole\n  Formation", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM astro-ph.HE stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a fast and parallelizable method based on Gaussian Processes\n(GPs) is introduced to emulate computer models that simulate the formation of\nbinary black holes (BBHs) through the evolution of pairs of massive stars. Two\nobstacles that arise in this application are the a priori unknown conditions of\nBBH formation and the large scale of the simulation data. We address them by\nproposing a local emulator which combines a GP classifier and a GP regression\nmodel. The resulting emulator can also be utilized in planning future computer\nsimulations through a proposed criterion for sequential design. By propagating\nuncertainties of simulation input through the emulator, we are able to obtain\nthe distribution of BBH properties under the distribution of physical\nparameters.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 02:35:25 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Lin", "Luyao", ""], ["Bingham", "Derek", ""], ["Broekgaarden", "Floor", ""], ["Mandel", "Ilya", ""]]}, {"id": "2106.01574", "submitter": "Yongshi Deng", "authors": "Yongshi Deng, Thomas Lumley", "title": "Multiple Imputation Through XGBoost", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple imputation is increasingly used in dealing with missing data. While\nsome conventional multiple imputation approaches are well studied and have\nshown empirical validity, they entail limitations in processing large datasets\nwith complex data structures. Their imputation performances usually rely on\nexpert knowledge of the inherent relations among variables. In addition, these\nstandard approaches tend to be computationally inefficient for medium and large\ndatasets. In this paper, we propose a scalable multiple imputation framework\nmixgb, which is based on XGBoost, bootstrapping and predictive mean matching.\nXGBoost, one of the fastest implementations of gradient boosted trees, is able\nto automatically retain interactions and non-linear relations in a dataset\nwhile achieving high computational efficiency. With the aid of bootstrapping\nand predictive mean matching, we show that our approach obtains less biased\nestimates and reflects appropriate imputation variability. The proposed\nframework is implemented in an R package misle. Supplementary materials for\nthis article are available online.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 03:43:11 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Deng", "Yongshi", ""], ["Lumley", "Thomas", ""]]}, {"id": "2106.01584", "submitter": "Hoon Hwangbo", "authors": "Di Bo, Hoon Hwangbo, Vinit Sharma, Corey Arndt, Stephanie C. TerMaath", "title": "A Subspace-based Approach for Dimensionality Reduction and Important\n  Variable Selection", "comments": "16 pages, 4 figures, will be submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An analysis of high dimensional data can offer a detailed description of a\nsystem but is often challenged by the curse of dimensionality. General\ndimensionality reduction techniques can alleviate such difficulty by extracting\na few important features, but they are limited due to the lack of\ninterpretability and connectivity to actual decision making associated with\neach physical variable. Important variable selection techniques, as an\nalternative, can maintain the interpretability, but they often involve a greedy\nsearch that is susceptible to failure in capturing important interactions. This\nresearch proposes a new method that produces subspaces, reduced-dimensional\nphysical spaces, based on a randomized search and forms an ensemble of models\nfor critical subspaces. When applied to high-dimensional data collected from a\ncomposite metal development process, the proposed method shows its superiority\nin prediction and important variable selection.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 04:10:34 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Bo", "Di", ""], ["Hwangbo", "Hoon", ""], ["Sharma", "Vinit", ""], ["Arndt", "Corey", ""], ["TerMaath", "Stephanie C.", ""]]}, {"id": "2106.01636", "submitter": "Sreedevi E P", "authors": "Sankaran P. G., Ashlin Mathew P. M., and Sreedevi E. P", "title": "Cause specific rate functions for panel count data with multiple modes\n  of recurrence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Panel count data arise from longitudinal studies on recurrent events where\neach subject is observed only at discrete time points. If recurrent events of\nseveral types are possible, we obtain panel count data with multiple modes of\nrecurrence. Such data is commonly encountered in medical studies, reliability\nexperiments as well as in sociological studies. In this article, we present\ncause specific rate functions for the analysis of panel count data with\nmultiple modes of recurrence and develop nonparametric estimation procedures\nfor the same. We derive empirical estimators for the cause specific rate\nfunctions and also propose a smoothed version of the same estimators using\nkernel estimation method. Asymptotic properties of the proposed estimators are\nstudied. A simulation study is conducted to assess the performance of the\nproposed estimators in finite samples. The practical utility of the proposed\nmethod is demonstrated using a real life data arising from skin cancer\nchemoprevention trial given in Sun and Zhao (2013).\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 07:12:01 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["G.", "Sankaran P.", ""], ["M.", "Ashlin Mathew P.", ""], ["P", "Sreedevi E.", ""]]}, {"id": "2106.01660", "submitter": "Botao Hao", "authors": "Tor Lattimore, Botao Hao", "title": "Bandit Phase Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study a bandit version of phase retrieval where the learner chooses\nactions $(A_t)_{t=1}^n$ in the $d$-dimensional unit ball and the expected\nreward is $\\langle A_t, \\theta_\\star\\rangle^2$ where $\\theta_\\star \\in \\mathbb\nR^d$ is an unknown parameter vector. We prove that the minimax cumulative\nregret in this problem is $\\smash{\\tilde \\Theta(d \\sqrt{n})}$, which improves\non the best known bounds by a factor of $\\smash{\\sqrt{d}}$. We also show that\nthe minimax simple regret is $\\smash{\\tilde \\Theta(d / \\sqrt{n})}$ and that\nthis is only achievable by an adaptive algorithm. Our analysis shows that an\napparently convincing heuristic for guessing lower bounds can be misleading and\nthat uniform bounds on the information ratio for information-directed sampling\nare not sufficient for optimal regret.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 08:04:33 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 15:52:52 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Lattimore", "Tor", ""], ["Hao", "Botao", ""]]}, {"id": "2106.01712", "submitter": "Jonas Wallin", "authors": "David Bolin and Jonas Wallin", "title": "Efficient methods for Gaussian Markov random fields under sparse linear\n  constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for inference and simulation of linearly constrained Gaussian Markov\nRandom Fields (GMRF) are computationally prohibitive when the number of\nconstraints is large. In some cases, such as for intrinsic GMRFs, they may even\nbe unfeasible. We propose a new class of methods to overcome these challenges\nin the common case of sparse constraints, where one has a large number of\nconstraints and each only involves a few elements. Our methods rely on a basis\ntransformation into blocks of constrained versus non-constrained subspaces, and\nwe show that the methods greatly outperform existing alternatives in terms of\ncomputational cost. By combining the proposed methods with the stochastic\npartial differential equation approach for Gaussian random fields, we also show\nhow to formulate Gaussian process regression with linear constraints in a GMRF\nsetting to reduce computational cost. This is illustrated in two applications\nwith simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 09:31:12 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Bolin", "David", ""], ["Wallin", "Jonas", ""]]}, {"id": "2106.01792", "submitter": "Jacopo Diquigiovanni", "authors": "Jacopo Diquigiovanni, Matteo Fontana, Simone Vantini", "title": "Conformal Prediction Bands for Multivariate Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by the pressing request of methods able to create prediction sets\nin a general regression framework for a multivariate functional response and\npushed by new methodological advancements in non-parametric prediction for\nfunctional data, we propose a set of conformal predictors that produce\nfinite-sample either valid or exact multivariate simultaneous prediction bands\nunder the mild assumption of exchangeable regression pairs. The fact that the\nprediction bands can be built around any regression estimator and that can be\neasily found in closed form yields a very widely usable method, which is fairly\nstraightforward to implement. In addition, we first introduce and then describe\na specific conformal predictor that guarantees an asymptotic result in terms of\nefficiency and inducing prediction bands able to modulate their width based on\nthe local behavior and magnitude of the functional data. The method is\ninvestigated and analyzed through a simulation study and a real-world\napplication in the field of urban mobility.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 12:25:49 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Diquigiovanni", "Jacopo", ""], ["Fontana", "Matteo", ""], ["Vantini", "Simone", ""]]}, {"id": "2106.01814", "submitter": "Christopher Barrie", "authors": "Roberto Cerina, Christopher Barrie, Neil Ketchley, and Aaron Zelin", "title": "Explaining Recruitment to Extremism: A Bayesian Contaminated Case\n  Control Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Who joins extremist movements? Answering this question poses considerable\nmethodological challenges. Survey techniques are practically infeasible and\nselective samples provide no counterfactual. Assigning recruits to contextual\nunits provides one solution, but is vulnerable to problems of ecological\ninference. In this article, we take inspiration from epidemiology and the\nprotest literature and elaborate a technique to combine survey and ecological\napproaches. The rare events, multilevel Bayesian contaminated case-control\ndesign we propose accounts for individual-level and contextual factors, as well\nas spatial autocorrelation in the incidence of recruitment. We validate our\napproach by matching a sample of Islamic State (ISIS) fighters from nine\nMuslim-majority countries with representative population surveys enumerated\nshortly before recruits joined the movement. We find that high status\nindividuals in their early twenties who had university education were more\nlikely to join ISIS. We find more mixed evidence for relative deprivation.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 13:07:59 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 09:40:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Cerina", "Roberto", ""], ["Barrie", "Christopher", ""], ["Ketchley", "Neil", ""], ["Zelin", "Aaron", ""]]}, {"id": "2106.01821", "submitter": "Stephen Walker", "authors": "Stephen G Walker", "title": "A New Measure of Overlap: An Alternative to the p--value", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we present a new measure for the overlap of two density\nfunctions which provides motivation and interpretation currently lacking with\nbenchmark measures based on the proportion of similar response, also known as\nthe overlap coefficient. We use this new measure to present an alternative to\nthe $p$--value as a guide to the choice of treatment in a comparative trial;\nwhere a current treatment and a new treatment are undergoing investigation. We\nshow that it is possible to reject the null hypothesis; i.e. the new treatment\nis significantly different in response to the old treatment, while the proposed\nnew summary for the same experiment indicates that as low as one in ten\nindividuals subject to the new treatment behave differently to individuals on\nthe old one.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 13:16:39 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 12:00:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Walker", "Stephen G", ""]]}, {"id": "2106.01858", "submitter": "Anders L{\\o}land", "authors": "Dag Tj{\\o}stheim and Martin Jullum and Anders L{\\o}land", "title": "Statistical embedding: Beyond principal components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an intense recent activity in embedding of very high\ndimensional and nonlinear data structures, much of it in the data science and\nmachine learning literature. We survey this activity in four parts. In the\nfirst part we cover nonlinear methods such as principal curves,\nmultidimensional scaling, local linear methods, ISOMAP, graph based methods and\nkernel based methods. The second part is concerned with topological embedding\nmethods, in particular mapping topological properties into persistence\ndiagrams. Another type of data sets with a tremendous growth is very\nhigh-dimensional network data. The task considered in part three is how to\nembed such data in a vector space of moderate dimension to make the data\namenable to traditional techniques such as cluster and classification\ntechniques. The final part of the survey deals with embedding in\n$\\mathbb{R}^2$, which is visualization. Three methods are presented: $t$-SNE,\nUMAP and LargeVis based on methods in parts one, two and three, respectively.\nThe methods are illustrated and compared on two simulated data sets; one\nconsisting of a triple of noisy Ranunculoid curves, and one consisting of\nnetworks of increasing complexity and with two types of nodes.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:01:21 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Tj\u00f8stheim", "Dag", ""], ["Jullum", "Martin", ""], ["L\u00f8land", "Anders", ""]]}, {"id": "2106.01906", "submitter": "Jingyu He", "authors": "Jingyu He, Nicholas Polson, Jianeng Xu", "title": "Bayesian Inference for Gamma Models", "comments": "Duplicate submission of arXiv:1905.12141 Please check\n  arXiv:1905.12141 for future update", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We use the theory of normal variance-mean mixtures to derive a data\naugmentation scheme for models that include gamma functions. Our methodology\napplies to many situations in statistics and machine learning, including\nMultinomial-Dirichlet distributions, Negative binomial regression,\nPoisson-Gamma hierarchical models, Extreme value models, to name but a few. All\nof those models include a gamma function which does not admit a natural\nconjugate prior distribution providing a significant challenge to inference and\nprediction. To provide a data augmentation strategy, we construct and develop\nthe theory of the class of Exponential Reciprocal Gamma distributions. This\nallows scalable EM and MCMC algorithms to be developed. We illustrate our\nmethodology on a number of examples, including gamma shape inference, negative\nbinomial regression and Dirichlet allocation. Finally, we conclude with\ndirections for future research.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 14:58:39 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 14:48:53 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["He", "Jingyu", ""], ["Polson", "Nicholas", ""], ["Xu", "Jianeng", ""]]}, {"id": "2106.01997", "submitter": "Marlena Bannick", "authors": "Fei Gao (1 and 2), Marlena S. Bannick (3) ((1) Vaccine and Infectious\n  Disease Division, Fred Hutchinson Cancer Research Center, Seattle, WA, (2)\n  Public Health Sciences Division, Fred Hutchinson Cancer Research Center,\n  Seattle, WA, (3) Department of Biostatistics, University of Washington,\n  Seattle, WA)", "title": "Statistical Considerations for Cross-Sectional HIV Incidence Estimation\n  Based on Recency Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal cohorts to determine the incidence of HIV infection are\nlogistically challenging, so researchers have sought alternative strategies.\nRecency test methods use biomarker profiles of HIV-infected subjects in a\ncross-sectional sample to infer whether they are \"recently\" infected and to\nestimate incidence in the population. Two main estimators have been used in\npractice: one that assumes a recency test is perfectly specific, and another\nthat allows for false-recent results. To date, these commonly used estimators\nhave not been rigorously studied with respect to their assumptions and\nstatistical properties. In this paper, we present a theoretical framework with\nwhich to understand these estimators and interrogate their assumptions, and\nperform a simulation study to assess the performance of these estimators under\nrealistic HIV epidemiological dynamics. We conclude with recommendations for\nthe use of these estimators in practice and a discussion of future\nmethodological developments to improve HIV incidence estimation via recency\ntest.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:16:25 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Gao", "Fei", "", "1 and 2"], ["Bannick", "Marlena S.", ""]]}, {"id": "2106.02029", "submitter": "Ruohan Zhan", "authors": "Ruohan Zhan, Vitor Hadad, David A. Hirshberg, and Susan Athey", "title": "Off-Policy Evaluation via Adaptive Weighting with Data from Contextual\n  Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has become increasingly common for data to be collected adaptively, for\nexample using contextual bandits. Historical data of this type can be used to\nevaluate other treatment assignment policies to guide future innovation or\nexperiments. However, policy evaluation is challenging if the target policy\ndiffers from the one used to collect data, and popular estimators, including\ndoubly robust (DR) estimators, can be plagued by bias, excessive variance, or\nboth. In particular, when the pattern of treatment assignment in the collected\ndata looks little like the pattern generated by the policy to be evaluated, the\nimportance weights used in DR estimators explode, leading to excessive\nvariance.\n  In this paper, we improve the DR estimator by adaptively weighting\nobservations to control its variance. We show that a t-statistic based on our\nimproved estimator is asymptotically normal under certain conditions, allowing\nus to form confidence intervals and test hypotheses. Using synthetic data and\npublic benchmarks, we provide empirical evidence for our estimator's improved\naccuracy and inferential properties relative to existing alternatives.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 17:54:44 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 17:56:15 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhan", "Ruohan", ""], ["Hadad", "Vitor", ""], ["Hirshberg", "David A.", ""], ["Athey", "Susan", ""]]}, {"id": "2106.02127", "submitter": "Antik Chakraborty", "authors": "Antik Chakraborty, Rihui Ou, David B. Dunson", "title": "Bayesian inference on high-dimensional multivariate binary data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It has become increasingly common to collect high-dimensional binary data;\nfor example, with the emergence of new sampling techniques in ecology. In\nsmaller dimensions, multivariate probit (MVP) models are routinely used for\ninferences. However, algorithms for fitting such models face issues in scaling\nup to high dimensions due to the intractability of the likelihood, involving an\nintegral over a multivariate normal distribution having no analytic form.\nAlthough a variety of algorithms have been proposed to approximate this\nintractable integral, these approaches are difficult to implement and/or\ninaccurate in high dimensions. We propose a two-stage Bayesian approach for\ninference on model parameters while taking care of the uncertainty propagation\nbetween the stages. We use the special structure of latent Gaussian models to\nreduce the highly expensive computation involved in joint parameter estimation\nto focus inference on marginal distributions of model parameters. This\nessentially makes the method embarrassingly parallel for both stages. We\nillustrate performance in simulations and applications to joint species\ndistribution modeling in ecology.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 20:53:41 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chakraborty", "Antik", ""], ["Ou", "Rihui", ""], ["Dunson", "David B.", ""]]}, {"id": "2106.02175", "submitter": "Haoyue Wang", "authors": "Rahul Mazumder, Haoyue Wang", "title": "Linear regression with partially mismatched data: local search with\n  theoretical guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is a fundamental modeling tool in statistics and related\nfields. In this paper, we study an important variant of linear regression in\nwhich the predictor-response pairs are partially mismatched. We use an\noptimization formulation to simultaneously learn the underlying regression\ncoefficients and the permutation corresponding to the mismatches. The\ncombinatorial structure of the problem leads to computational challenges. We\npropose and study a simple greedy local search algorithm for this optimization\nproblem that enjoys strong theoretical guarantees and appealing computational\nperformance. We prove that under a suitable scaling of the number of mismatched\npairs compared to the number of samples and features, and certain assumptions\non problem data; our local search algorithm converges to a nearly-optimal\nsolution at a linear rate. In particular, in the noiseless case, our algorithm\nconverges to the global optimal solution with a linear convergence rate. We\nalso propose an approximate local search step that allows us to scale our\napproach to much larger instances. We conduct numerical experiments to gather\nfurther insights into our theoretical results and show promising performance\ngains compared to existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2021 23:32:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Mazumder", "Rahul", ""], ["Wang", "Haoyue", ""]]}, {"id": "2106.02290", "submitter": "Sourav Chatterjee", "authors": "Sohom Bhattacharya, Sourav Chatterjee", "title": "Matrix completion with data-dependent missingness probabilities", "comments": "24 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of completing a large matrix with lots of missing entries has\nreceived widespread attention in the last couple of decades. Two popular\napproaches to the matrix completion problem are based on singular value\nthresholding and nuclear norm minimization. Most of the past works on this\nsubject assume that there is a single number $p$ such that each entry of the\nmatrix is available independently with probability $p$ and missing otherwise.\nThis assumption may not be realistic for many applications. In this work, we\nreplace it with the assumption that the probability that an entry is available\nis an unknown function $f$ of the entry itself. For example, if the entry is\nthe rating given to a movie by a viewer, then it seems plausible that high\nvalue entries have greater probability of being available than low value\nentries. We propose two new estimators, based on singular value thresholding\nand nuclear norm minimization, to recover the matrix under this assumption. The\nestimators are shown to be consistent under a low rank assumption. We also\nprovide a consistent estimator of the unknown function $f$.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 07:07:14 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bhattacharya", "Sohom", ""], ["Chatterjee", "Sourav", ""]]}, {"id": "2106.02364", "submitter": "Jakob Dambon", "authors": "Jakob A. Dambon, Fabio Sigrist, Reinhard Furrer", "title": "varycoef: An R Package for Gaussian Process-based Spatially Varying\n  Coefficient Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian processes (GPs) are well-known tools for modeling dependent data\nwith applications in spatial statistics, time series analysis, or econometrics.\nIn this article, we present the R package varycoef that implements estimation,\nprediction, and variable selection of linear models with spatially varying\ncoefficients (SVC) defined by GPs, so called GP-based SVC models. Such models\noffer a high degree of flexibility while being relatively easy to interpret.\nUsing varycoef, we show versatile applications of (spatially) varying\ncoefficient models on spatial and time series data. This includes model and\ncoefficient estimation with predictions and variable selection. The package\nuses state-of-the-art computational statistics techniques like parallelization,\nmodel-based optimization, and covariance tapering. This allows the user to work\nwith (S)VC models in a computationally efficient manner, i.e., model estimation\non large data sets is possible in a feasible amount of time.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 09:23:04 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Dambon", "Jakob A.", ""], ["Sigrist", "Fabio", ""], ["Furrer", "Reinhard", ""]]}, {"id": "2106.02447", "submitter": "Christina Nie{\\ss}l", "authors": "Christina Nie{\\ss}l (1), Moritz Herrmann (2), Chiara Wiedemann (1),\n  Giuseppe Casalicchio (2), Anne-Laure Boulesteix (1) ((1) Institute for\n  Medical Information Processing, Biometry and Epidemiology, LMU Munich,\n  Germany, (2) Department of Statistics, LMU Munich, Germany)", "title": "Over-optimism in benchmark studies and the multiplicity of design and\n  analysis options when interpreting their results", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the need for neutral benchmark studies that focus on the\ncomparison of methods from computational sciences has been increasingly\nrecognised by the scientific community. While general advice on the design and\nanalysis of neutral benchmark studies can be found in recent literature,\ncertain amounts of flexibility always exist. This includes the choice of data\nsets and performance measures, the handling of missing performance values and\nthe way the performance values are aggregated over the data sets. As a\nconsequence of this flexibility, researchers may be concerned about how their\nchoices affect the results or, in the worst case, may be tempted to engage in\nquestionable research practices (e.g. the selective reporting of results or the\npost-hoc modification of design or analysis components) to fit their\nexpectations or hopes. To raise awareness for this issue, we use an example\nbenchmark study to illustrate how variable benchmark results can be when all\npossible combinations of a range of design and analysis options are considered.\nWe then demonstrate how the impact of each choice on the results can be\nassessed using multidimensional unfolding. In conclusion, based on previous\nliterature and on our illustrative example, we claim that the multiplicity of\ndesign and analysis options combined with questionable research practices lead\nto biased interpretations of benchmark results and to over-optimistic\nconclusions. This issue should be considered by computational researchers when\ndesigning and analysing their benchmark studies and by the scientific community\nin general in an effort towards more reliable benchmark results.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 12:54:16 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Nie\u00dfl", "Christina", ""], ["Herrmann", "Moritz", ""], ["Wiedemann", "Chiara", ""], ["Casalicchio", "Giuseppe", ""], ["Boulesteix", "Anne-Laure", ""]]}, {"id": "2106.02453", "submitter": "Larry Lacey", "authors": "Laurence Lacey", "title": "On the nature of time in time-dependent expansionary processes", "comments": "18 pages, 4 figures, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.stat-mech math.PR stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  For an expansionary process, the size of the expansion space will increase.\nIf the expansionary process is time-dependent, time (t) will increase as a\nfunction of the increase in the size of the expansion space. A statistical\ninformation entropy methodology was used to investigate the properties of\ntime-dependent expansionary processes both in theory and through examples. The\nprimary objective of this paper was to investigate whether there is a universal\nmeasure of time (T) and how it relates to process related time (t), that is\nspecific to any given time-dependent expansionary process. It was found that\nfor such time-dependent processes, time (t) can be rescaled to time (T) such\nthat, T and the information entropy (H(T)) of the expansionary process are the\nsame, and directly related to the increase in the size of the expansion space.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2021 09:39:37 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Lacey", "Laurence", ""]]}, {"id": "2106.02475", "submitter": "Ruoyu Wang", "authors": "Ruoyu Wang, Miaomiao Su, Qihua Wang", "title": "Distributed nonparametric regression imputation for missing response\n  problems with large-scale data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric regression imputation is commonly used in missing data\nanalysis. However, it suffers from the \"curse of dimension\". The problem can be\nalleviated by the explosive sample size in the era of big data, while the\nlarge-scale data size presents some challenges on the storage of data and the\ncalculation of estimators. These challenges make the classical nonparametric\nregression imputation methods no longer applicable. This motivates us to\ndevelop two distributed nonparametric imputation methods. One is based on\nkernel smoothing and the other is based on the sieve method. The kernel based\ndistributed imputation method has extremely low communication cost and the\nsieve based distributed imputation method can accommodate more local machines.\nIn order to illustrate the proposed imputation methods, response mean\nestimation is considered. Two distributed nonparametric imputation estimators\nare proposed for the response mean, which are proved to be asymptotically\nnormal with asymptotic variances achieving the semiparametric efficiency bound.\nThe proposed methods are evaluated through simulation studies and are\nillustrated by a real data analysis.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 13:29:35 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wang", "Ruoyu", ""], ["Su", "Miaomiao", ""], ["Wang", "Qihua", ""]]}, {"id": "2106.02482", "submitter": "Jason Steffener", "authors": "Jason Steffener", "title": "Power of Mediation Effects Using Bootstrap Resampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Mediation analyses are a statistical tool for testing the hypothesis about\nhow the relationship between two variables may be direct or indirect via a\nthird variable. Assessing statistical significance has been an area of active\nresearch; however, assessment of statistical power has been hampered by the\nlack of closed form calculations and the need for substantial amounts of\ncomputational simulations. The current work provides a detailed explanation of\nimplementing large scale simulation procedures within a shared computing\ncluster environment. In addition, all results and code for implementing these\nprocedures is publicly available. The resulting power analyses compare the\neffects of sample size and strength and direction of the relationships between\nthe three variables. Comparisons of three confidence interval calculation\nmethods demonstrated that the bias-corrected method is optimal and requires\napproximately ten less participants than the percentile method to achieve\nequivalent power. Differing strengths of distal and proximal effects were\ncompared and did not differentially affect the power to detect mediation\neffects. Suppression effects were explored and demonstrate that in the presence\nof no observed relationship between two variables, entrance of the mediating\nvariable into the model can reveal a suppressed relationship. The power to\ndetect suppression effects is similar to unsuppressed mediation. These results\nand their methods provide important information about the power of mediation\nmodels for study planning. Of greater importance is that the methods lay the\ngroundwork for assessment of statistical power of more complicated models\ninvolving multiple mediators and moderators.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 13:41:14 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Steffener", "Jason", ""]]}, {"id": "2106.02521", "submitter": "Marc Chadeau-Hyam", "authors": "Barbara Bodinier, Sarah Filippi, Therese Haugdahl Nost, Julien Chiquet\n  and Marc Chadeau-Hyam", "title": "Automated calibration for stability selection in penalised regression\n  and graphical models: a multi-OMICs network application exploring the\n  molecular response to tobacco smoking", "comments": "Main paper 21 pages, SI: 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stability selection represents an attractive approach to identify sparse sets\nof features jointly associated with an outcome in high-dimensional contexts. We\nintroduce an automated calibration procedure via maximisation of an in-house\nstability score and accommodating a priori-known block structure (e.g.\nmulti-OMIC) data. It applies to (LASSO) penalised regression and graphical\nmodels. Simulations show our approach outperforms non-stability-based and\nstability selection approaches using the original calibration. Application to\nmulti-block graphical LASSO on real (epigenetic and transcriptomic) data from\nthe Norwegian Women and Cancer study reveals a central/credible and novel\ncross-OMIC role of the LRRN3 in the biological response to smoking.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 14:44:55 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Bodinier", "Barbara", ""], ["Filippi", "Sarah", ""], ["Nost", "Therese Haugdahl", ""], ["Chiquet", "Julien", ""], ["Chadeau-Hyam", "Marc", ""]]}, {"id": "2106.02590", "submitter": "J\\'er\\^ome-Alexis Chevalier", "authors": "J\\'er\\^ome-Alexis Chevalier, Tuan-Binh Nguyen, Bertrand Thirion,\n  Joseph Salmon", "title": "Spatially relaxed inference on high-dimensional linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the inference problem for high-dimensional linear models, when\ncovariates have an underlying spatial organization reflected in their\ncorrelation. A typical example of such a setting is high-resolution imaging, in\nwhich neighboring pixels are usually very similar. Accurate point and\nconfidence intervals estimation is not possible in this context with many more\ncovariates than samples, furthermore with high correlation between covariates.\nThis calls for a reformulation of the statistical inference problem, that takes\ninto account the underlying spatial structure: if covariates are locally\ncorrelated, it is acceptable to detect them up to a given spatial uncertainty.\nWe thus propose to rely on the $\\delta$-FWER, that is the probability of making\na false discovery at a distance greater than $\\delta$ from any true positive.\nWith this target measure in mind, we study the properties of ensembled\nclustered inference algorithms which combine three techniques: spatially\nconstrained clustering, statistical inference, and ensembling to aggregate\nseveral clustered inference solutions. We show that ensembled clustered\ninference algorithms control the $\\delta$-FWER under standard assumptions for\n$\\delta$ equal to the largest cluster diameter. We complement the theoretical\nanalysis with empirical results, demonstrating accurate $\\delta$-FWER control\nand decent power achieved by such inference algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:37:19 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Chevalier", "J\u00e9r\u00f4me-Alexis", ""], ["Nguyen", "Tuan-Binh", ""], ["Thirion", "Bertrand", ""], ["Salmon", "Joseph", ""]]}, {"id": "2106.02600", "submitter": "Song Wei", "authors": "Song Wei, Yao Xie, Christopher S. Josef, Rishikesan Kamaleswaran", "title": "Inferring Granger Causality from Irregularly Sampled Time Series", "comments": "33 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous, automated surveillance systems that incorporate machine learning\nmodels are becoming increasingly more common in healthcare environments. These\nmodels can capture temporally dependent changes across multiple patient\nvariables and can enhance a clinician's situational awareness by providing an\nearly warning alarm of an impending adverse event such as sepsis. However, most\ncommonly used methods, e.g., XGBoost, fail to provide an interpretable\nmechanism for understanding why a model produced a sepsis alarm at a given\ntime. The black-box nature of many models is a severe limitation as it prevents\nclinicians from independently corroborating those physiologic features that\nhave contributed to the sepsis alarm. To overcome this limitation, we propose a\ngeneralized linear model (GLM) approach to fit a Granger causal graph based on\nthe physiology of several major sepsis-associated derangements (SADs). We adopt\na recently developed stochastic monotone variational inequality-based estimator\ncoupled with forwarding feature selection to learn the graph structure from\nboth continuous and discrete-valued as well as regularly and irregularly\nsampled time series. Most importantly, we develop a non-asymptotic upper bound\non the estimation error for any monotone link function in the GLM. We conduct\nreal-data experiments and demonstrate that our proposed method can achieve\ncomparable performance to popular and powerful prediction methods such as\nXGBoost while simultaneously maintaining a high level of interpretability.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 16:59:24 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Wei", "Song", ""], ["Xie", "Yao", ""], ["Josef", "Christopher S.", ""], ["Kamaleswaran", "Rishikesan", ""]]}, {"id": "2106.02693", "submitter": "Peter Gr\\\"unwald", "authors": "Rosanne Turner, Alexander Ly, Peter Gr\\\"unwald", "title": "Safe Tests and Always-Valid Confidence Intervals for contingency tables\n  and beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop E variables for testing whether two data streams come from the\nsame source or not, and more generally, whether the difference between the\nsources is larger than some minimal effect size. These E variables lead to\ntests that remain safe, i.e. keep their Type-I error guarantees, under flexible\nsampling scenarios such as optional stopping and continuation. We also develop\nthe corresponding always-valid confidence intervals. In special cases our E\nvariables also have an optimal `growth' property under the alternative. We\nillustrate the generic construction through the special case of 2x2 contingency\ntables, where we also allow for the incorporation of different restrictions on\na composite alternative. Comparison to p-value analysis in simulations and a\nreal-world example show that E variables, through their flexibility, often\nallow for early stopping of data collection, thereby retaining similar power as\nclassical methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 20:12:13 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Turner", "Rosanne", ""], ["Ly", "Alexander", ""], ["Gr\u00fcnwald", "Peter", ""]]}, {"id": "2106.02718", "submitter": "Guannan Wang", "authors": "Yueying Wang, Guannan Wang, Li Wang and R. Todd Ogden", "title": "Simultaneous Confidence Corridors for Mean Functions in Functional Data\n  Analysis of Imaging Data", "comments": null, "journal-ref": "Biometrics, 2020", "doi": "10.1111/biom.13156", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent work involving the analysis of biomedical imaging data,\nwe present a novel procedure for constructing simultaneous confidence corridors\nfor the mean of imaging data. We propose to use flexible bivariate splines over\ntriangulations to handle irregular domain of the images that is common in brain\nimaging studies and in other biomedical imaging applications. The proposed\nspline estimators of the mean functions are shown to be consistent and\nasymptotically normal under some regularity conditions. We also provide a\ncomputationally efficient estimator of the covariance function and derive its\nuniform consistency. The procedure is also extended to the two-sample case in\nwhich we focus on comparing the mean functions from two populations of imaging\ndata. Through Monte Carlo simulation studies we examine the finite-sample\nperformance of the proposed method. Finally, the proposed method is applied to\nanalyze brain Positron Emission Tomography (PET) data in two different studies.\nOne dataset used in preparation of this article was obtained from the\nAlzheimer's Disease Neuroimaging Initiative (ADNI) database.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 20:59:09 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Yueying", ""], ["Wang", "Guannan", ""], ["Wang", "Li", ""], ["Ogden", "R. Todd", ""]]}, {"id": "2106.02724", "submitter": "Samyak Rajanala", "authors": "Samyak Rajanala and Julia A. Palacios", "title": "Statistical summaries of unlabelled evolutionary trees and ranked\n  hierarchical clustering trees", "comments": "46 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rooted and ranked binary trees are mathematical objects of great importance\nused to model hierarchical data and evolutionary relationships with\napplications in many fields including evolutionary biology and genetic\nepidemiology. Bayesian phylogenetic inference usually explore the posterior\ndistribution of trees via Markov Chain Monte Carlo methods, however assessing\nuncertainty and summarizing distributions or samples of such trees remains\nchallenging. While labelled phylogenetic trees have been extensively studied,\nrelatively less literature exists for unlabelled trees which are increasingly\nuseful, for example when one seeks to summarize samples of trees obtained with\ndifferent methods, or from different samples and environments, and wishes to\nassess stability and generalizability of these summaries. In our paper, we\nexploit recently proposed distance metrics of unlabelled ranked binary trees\nand unlabelled ranked genealogies (equipped with branch lengths) to define the\nFrechet mean and variance as summaries of these tree distributions. We provide\nan efficient combinatorial optimization algorithm for computing the Frechet\nmean from a sample of or distribution on unlabelled ranked tree shapes and\nunlabelled ranked genealogies. We show the applicability of our summary\nstatistics for studying popular tree distributions and for comparing the\nSARS-CoV-2 evolutionary trees across different locations during the COVID-19\nepidemic in 2020.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 21:23:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rajanala", "Samyak", ""], ["Palacios", "Julia A.", ""]]}, {"id": "2106.02727", "submitter": "Fabian Mies", "authors": "Stefan Bedbur and Fabian Mies", "title": "Confidence bands for exponential distribution functions under\n  progressive type-II censoring", "comments": "AOM. This article has been accepted for publication in Journal of\n  Statistical Computation and Simulation, published by Taylor & Francis", "journal-ref": null, "doi": "10.1080/00949655.2021.1931211", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a progressively type-II censored sample from the exponential\ndistribution with unknown location and scale parameter, confidence bands are\nproposed for the underlying distribution function by using confidence regions\nfor the parameters and Kolmogorov-Smirnov type statistics. Simple explicit\nrepresentations for the boundaries and for the coverage probabilities of the\nconfidence bands are analytically derived, and the performance of the bands is\ncompared in terms of band width and area by means of a data example. As a\nby-product, a novel confidence region for the location-scale parameter is\nobtained. Extensions of the results to related models for ordered data, such as\nsequential order statistics, as well as to other underlying location-scale\nfamilies of distributions are discussed.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 21:25:58 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bedbur", "Stefan", ""], ["Mies", "Fabian", ""]]}, {"id": "2106.02741", "submitter": "Pengfei Li", "authors": "Meng Yuan, Pengfei Li and Changbao Wu", "title": "Semiparametric inference on Gini indices of two semicontinuous\n  populations under density ratio models", "comments": "49 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gini index is a popular inequality measure with many applications in\nsocial and economic studies. This paper studies semiparametric inference on the\nGini indices of two semicontinuous populations. We characterize the\ndistribution of each semicontinuous population by a mixture of a discrete point\nmass at zero and a continuous skewed positive component. A semiparametric\ndensity ratio model is then employed to link the positive components of the two\ndistributions. We propose the maximum empirical likelihood estimators of the\ntwo Gini indices and their difference, and further investigate the asymptotic\nproperties of the proposed estimators. The asymptotic results enable us to\nconstruct confidence intervals and perform hypothesis tests for the two Gini\nindices and their difference. We show that the proposed estimators are more\nefficient than the existing fully nonparametric estimators. The proposed\nestimators and the asymptotic results are also applicable to cases without\nexcessive zero values. Simulation studies show the superiority of our proposed\nmethod over existing methods. Two real-data applications are presented using\nthe proposed methods.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2021 22:17:51 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Yuan", "Meng", ""], ["Li", "Pengfei", ""], ["Wu", "Changbao", ""]]}, {"id": "2106.02760", "submitter": "David B. Dahl", "authors": "David B. Dahl, Jacob Andros, J. Brandon Carter", "title": "Cluster Analysis via Random Partition Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Hierarchical and k-medoids clustering are deterministic clustering algorithms\nbased on pairwise distances. Using these same pairwise distances, we propose a\nnovel stochastic clustering method based on random partition distributions. We\ncall our method CaviarPD, for cluster analysis via random partition\ndistributions. CaviarPD first samples clusterings from a random partition\ndistribution and then finds the best cluster estimate based on these samples\nusing algorithms to minimize an expected loss. We compare CaviarPD with\nhierarchical and k-medoids clustering through eight case studies. Cluster\nestimates based on our method are competitive with those of hierarchical and\nk-medoids clustering. They also do not require the subjective choice of the\nlinkage method necessary for hierarchical clustering. Furthermore, our\ndistribution-based procedure provides an intuitive graphical representation to\nassess clustering uncertainty.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 00:23:35 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Dahl", "David B.", ""], ["Andros", "Jacob", ""], ["Carter", "J. Brandon", ""]]}, {"id": "2106.02803", "submitter": "Tianxi Li", "authors": "Tianxi Li, Can M. Le", "title": "Network Estimation by Mixing: Adaptivity and More", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks analysis has been commonly used to study the interactions between\nunits of complex systems. One problem of particular interest is learning the\nnetwork's underlying connection pattern given a single and noisy instantiation.\nWhile many methods have been proposed to address this problem in recent years,\nthey usually assume that the true model belongs to a known class, which is not\nverifiable in most real-world applications. Consequently, network modeling\nbased on these methods either suffers from model misspecification or relies on\nadditional model selection procedures that are not well understood in theory\nand can potentially be unstable in practice. To address this difficulty, we\npropose a mixing strategy that leverages available arbitrary models to improve\ntheir individual performances. The proposed method is computationally efficient\nand almost tuning-free; thus, it can be used as an off-the-shelf method for\nnetwork modeling. We show that the proposed method performs equally well as the\noracle estimate when the true model is included as individual candidates. More\nimportantly, the method remains robust and outperforms all current estimates\neven when the models are misspecified. Extensive simulation examples are used\nto verify the advantage of the proposed mixing method. Evaluation of link\nprediction performance on 385 real-world networks from six domains also\ndemonstrates the universal competitiveness of the mixing method across multiple\ndomains.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 05:17:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Tianxi", ""], ["Le", "Can M.", ""]]}, {"id": "2106.02909", "submitter": "Camila P. E. De Souza Dr.", "authors": "Zahra A. Shirazi, Jo\\~ao Pedro A. R. da Silva and Camila P. E. de\n  Souza", "title": "Parameter Estimation for Grouped Data Using EM and MCEM Algorithms", "comments": "26 pages, 6 tables and 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the confidentiality of data and information is of great importance\nfor many companies and organizations. For this reason, they may prefer not to\nrelease exact data, but instead to grant researchers access to approximate\ndata. For example, rather than providing the exact income of their clients,\nthey may only provide researchers with grouped data, that is, the number of\nclients falling in each of a set of non-overlapping income intervals. The\nchallenge is to estimate the mean and variance structure of the hidden\nungrouped data based on the observed grouped data. To tackle this problem, this\nwork considers the exact observed data likelihood and applies the\nExpectation-Maximization (EM) and Monte-Carlo EM (MCEM) algorithms for cases\nwhere the hidden data follow a univariate, bivariate, or multivariate normal\ndistribution. The results are then compared with the case of ignoring the\ngrouping and applying regular maximum likelihood. The well-known Galton data\nand simulated datasets are used to evaluate the properties of the proposed EM\nand MCEM algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2021 14:40:19 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Shirazi", "Zahra A.", ""], ["da Silva", "Jo\u00e3o Pedro A. R.", ""], ["de Souza", "Camila P. E.", ""]]}, {"id": "2106.03022", "submitter": "Fang Han", "authors": "Zhen Miao, Weihao Kong, Ramya Korlakai Vinayak, Wei Sun, and Fang Han", "title": "Fisher-Pitman permutation tests based on nonparametric Poisson mixtures\n  with application to single cell genomics", "comments": "52 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the theoretical and empirical performance of\nFisher-Pitman-type permutation tests for assessing the equality of unknown\nPoisson mixture distributions. Building on nonparametric maximum likelihood\nestimators (NPMLEs) of the mixing distribution, these tests are theoretically\nshown to be able to adapt to complicated unspecified structures of count data\nand also consistent against their corresponding ANOVA-type alternatives; the\nlatter is a result in parallel to classic claims made by Robinson (Robinson,\n1973). The studied methods are then applied to a single-cell RNA-seq data\nobtained from different cell types from brain samples of autism subjects and\nhealthy controls; empirically, they unveil genes that are differentially\nexpressed between autism and control subjects yet are missed using common\ntests. For justifying their use, rate optimality of NPMLEs is also established\nin settings similar to nonparametric Gaussian (Wu and Yang, 2020a) and binomial\nmixtures (Tian et al., 2017; Vinayak et al., 2019).\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 03:31:53 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Miao", "Zhen", ""], ["Kong", "Weihao", ""], ["Vinayak", "Ramya Korlakai", ""], ["Sun", "Wei", ""], ["Han", "Fang", ""]]}, {"id": "2106.03023", "submitter": "Ioannis Papageorgiou", "authors": "Ioannis Papageorgiou, Ioannis Kontoyiannis", "title": "Hierarchical Bayesian Mixture Models for Time Series Using Context Trees\n  as State Space Partitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general Bayesian framework is introduced for mixture modelling and\ninference with real-valued time series. At the top level, the state space is\npartitioned via the choice of a discrete context tree, so that the resulting\npartition depends on the values of some of the most recent samples. At the\nbottom level, a different model is associated with each region of the\npartition. This defines a very rich and flexible class of mixture models, for\nwhich we provide algorithms that allow for efficient, exact Bayesian inference.\nIn particular, we show that the maximum a posteriori probability (MAP) model\n(including the relevant MAP context tree partition) can be precisely\nidentified, along with its exact posterior probability. The utility of this\ngeneral framework is illustrated in detail when a different autoregressive (AR)\nmodel is used in each state-space region, resulting in a mixture-of-AR model\nclass. The performance of the associated algorithmic tools is demonstrated in\nthe problems of model selection and forecasting on both simulated and\nreal-world data, where they are found to provide results as good or better than\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 03:46:49 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Papageorgiou", "Ioannis", ""], ["Kontoyiannis", "Ioannis", ""]]}, {"id": "2106.03024", "submitter": "Jaime Roquero Gimenez", "authors": "Jaime Roquero Gimenez and Dominik Rothenh\\\"ausler", "title": "Causal aggregation: estimation and inference of causal effects by\n  constraint-based data fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Randomized experiments are the gold standard for causal inference. In\nexperiments, usually one variable is manipulated and its effect is measured on\nan outcome. However, practitioners may also be interested in the effect on a\nfixed target variable of simultaneous interventions on multiple covariates. We\npropose a novel method that allows to estimate the effect of joint\ninterventions using data from different experiments in which only very few\nvariables are manipulated. If the joint causal effect is linear, the proposed\nmethod can be used for estimation and inference of joint causal effects, and we\ncharacterize conditions for identifiability. The proposed method allows to\ncombine data sets arising from randomized experiments as well as observational\ndata sets for which IV assumptions or unconfoundedness hold: we indicate how to\nleverage all the available causal information to efficiently estimate the\ncausal effects in the overidentified setting. If the dimension of the covariate\nvector is large, we may have data from experiments on every covariate, but only\na few samples per randomized covariate. Under a sparsity assumption, we derive\nan estimator of the causal effects in this high-dimensional scenario. In\naddition, we show how to deal with the case where a lack of experimental\nconstraints prevents direct estimation of the causal effects. When the joint\ncausal effects are non-linear, we characterize conditions under which\nidentifiability holds, and propose a non-linear causal aggregation methodology\nfor experimental data sets similar to the gradient boosting algorithm where in\neach iteration we combine weak learners trained on different datasets using\nonly unconfounded samples. We demonstrate the effectiveness of the proposed\nmethod on simulated and semi-synthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 04:09:12 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Gimenez", "Jaime Roquero", ""], ["Rothenh\u00e4usler", "Dominik", ""]]}, {"id": "2106.03072", "submitter": "Andrea Cremaschi", "authors": "Andrea Cremaschi, Raffele Argiento, Maria De Iorio, Cai Shirong, Yap\n  Seng Chong, Michael J. Meaney and Michelle Z. L. Kee", "title": "Seemingly Unrelated Multi-State processes: a Bayesian semiparametric\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Many applications in medical statistics as well as in other fields can be\ndescribed by transitions between multiple states (e.g. from health to disease)\nexperienced by individuals over time. In this context, multi-state models are a\npopular statistical technique, in particular when the exact transition times\nare not observed. The key quantities of interest are the transition rates,\ncapturing the instantaneous risk of moving from one state to another. The main\ncontribution of this work is to propose a joint semiparametric model for\nseveral possibly related multi-state processes (Seemingly Unrelated\nMulti-State, SUMS, processes), assuming a Markov structure for the transitions\nover time. The dependence between different processes is captured by specifying\na joint random effect distribution on the transition rates of each process. We\nassume a flexible random effect distribution, which allows for clustering of\nthe individuals, overdispersion and outliers. Moreover, we employ a graph\nstructure to describe the dependence among processes, exploiting tools from the\nGaussian Graphical model literature. It is also possible to include covariate\neffects. We use our approach to model disease progression in mental health.\nPosterior inference is performed through a specially devised MCMC algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 09:22:32 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Cremaschi", "Andrea", ""], ["Argiento", "Raffele", ""], ["De Iorio", "Maria", ""], ["Shirong", "Cai", ""], ["Chong", "Yap Seng", ""], ["Meaney", "Michael J.", ""], ["Kee", "Michelle Z. L.", ""]]}, {"id": "2106.03218", "submitter": "Chenchen Ma", "authors": "Chenchen Ma and Gongjun Xu", "title": "Hypothesis Testing for Hierarchical Structures in Cognitive Diagnosis\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive Diagnosis Models (CDMs) are a special family of discrete latent\nvariable models widely used in educational, psychological and social sciences.\nIn many applications of CDMs, certain hierarchical structures among the latent\nattributes are assumed by researchers to characterize their dependence\nstructure. Specifically, a directed acyclic graph is used to specify\nhierarchical constraints on the allowable configurations of the discrete latent\nattributes. In this paper, we consider the important yet unaddressed problem of\ntesting the existence of latent hierarchical structures in CDMs. We first\nintroduce the concept of testability of hierarchical structures in CDMs and\npresent sufficient conditions. Then we study the asymptotic behaviors of the\nlikelihood ratio test (LRT) statistic, which is widely used for testing nested\nmodels. Due to the irregularity of the problem, the asymptotic distribution of\nLRT becomes nonstandard and tends to provide unsatisfactory finite sample\nperformance under practical conditions. We provide statistical insights on such\nfailures, and propose to use parametric bootstrap to perform the testing. We\nalso demonstrate the effectiveness and superiority of parametric bootstrap for\ntesting the latent hierarchies over non-parametric bootstrap and the na\\\"ive\nChi-squared test through comprehensive simulations and an educational\nassessment dataset.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 19:40:28 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ma", "Chenchen", ""], ["Xu", "Gongjun", ""]]}, {"id": "2106.03244", "submitter": "Lu Xia", "authors": "Lu Xia, Bin Nan and Yi Li", "title": "Statistical Inference for Cox Proportional Hazards Models with a\n  Diverging Number of Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For statistical inference on regression models with a diverging number of\ncovariates, the existing literature typically makes sparsity assumptions on the\ninverse of the Fisher information matrix. Such assumptions, however, are often\nviolated under Cox proportion hazards models, leading to biased estimates with\nunder-coverage confidence intervals. We propose a modified debiased lasso\napproach, which solves a series of quadratic programming problems to\napproximate the inverse information matrix without posing sparse matrix\nassumptions. We establish asymptotic results for the estimated regression\ncoefficients when the dimension of covariates diverges with the sample size. As\ndemonstrated by extensive simulations, our proposed method provides consistent\nestimates and confidence intervals with nominal coverage probabilities. The\nutility of the method is further demonstrated by assessing the effects of\ngenetic markers on patients' overall survival with the Boston Lung Cancer\nSurvival Cohort, a large-scale epidemiology study investigating mechanisms\nunderlying the lung cancer.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 20:54:44 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xia", "Lu", ""], ["Nan", "Bin", ""], ["Li", "Yi", ""]]}, {"id": "2106.03252", "submitter": "Federico Castelletti", "authors": "Federico Castelletti and Guido Consonni", "title": "Bayesian graphical modelling for heterogeneous causal effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our motivation stems from current medical research aiming at personalized\ntreatment using a molecular-based approach. The goal is to develop a more\nprecise and targeted decision making process, relative to traditional\ntreatments based primarily on clinical diagnoses. A challenge we address is\nevaluating treatment effects for individuals affected by Glioblastoma (GBM), a\nbrain cancer where targeted therapy is essential to improve patients'\nprospects. Specifically, we consider the pathway associated to cytokine\nTGF-beta, whose abnormal signalling activity has been found to be linked to the\nprogression of GBM and other tumors. We analyze treatment effects within a\ncausal framework represented by a Directed Acyclic Graph (DAG) model, whose\nvertices are the variables belonging to the TGF-beta pathway. A major obstacle\nin implementing the above program is represented by individual heterogeneity,\nimplying that patients will respond differently to the same therapy. We address\nthis issue through an infinite mixture of Gaussian DAG-models where both the\ngraphical structure as well as the allied model parameters are regarded as\nuncertain. Our procedure determines a clustering structure of the units\nreflecting the underlying heterogeneity, and produces subject-specific causal\neffects through Bayesian model averaging across a variety of model features.\nWhen applied to the GBM dataset, it reveals that regulation of TGF-beta\nproteins produces heterogeneous effects, represented by clusters of patients\npotentially benefiting from selective interventions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 21:19:19 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Castelletti", "Federico", ""], ["Consonni", "Guido", ""]]}, {"id": "2106.03263", "submitter": "Maciej Ber\\k{e}sewicz", "authors": "Maciej Ber\\k{e}sewicz and Herman Cherniaiev and Robert Pater", "title": "Estimating the number of entities with vacancies using administrative\n  and online data", "comments": "70 pages, 4 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP econ.GN q-fin.EC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we describe a study aimed at estimating job vacancy\nstatistics, in particular the number of entities with at least one vacancy. To\nachieve this goal, we propose an alternative approach to the methodology\nexploiting survey data, which is based solely on data from administrative\nregisters and online sources and relies on dual system estimation (DSE).\n  As these sources do not cover the whole reference population and the number\nof units appearing in all datasets is small, we have developed a DSE approach\nfor negatively dependent sources based on a recent work by Chatterjee and\nBhuyan (2020). To achieve the main goal we conducted a thorough data cleaning\nprocedure in order to remove out-of-scope units, identify entities from the\ntarget population, and link them by identifiers to minimize linkage errors. We\nverified the effectiveness and sensitivity of the proposed estimator in\nsimulation studies.\n  From a practical point of view, our results show that the current vacancy\nsurvey in Poland underestimates the number of entities with at least one\nvacancy by about 10-15%. The main reasons for this discrepancy are non-sampling\nerrors due to non-response and under-reporting, which is identified by\ncomparing survey data with administrative data.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 22:19:02 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ber\u0119sewicz", "Maciej", ""], ["Cherniaiev", "Herman", ""], ["Pater", "Robert", ""]]}, {"id": "2106.03322", "submitter": "Edwin Ng", "authors": "Edwin Ng, Zhishi Wang, Athena Dai", "title": "Bayesian Time Varying Coefficient Model with Applications to Marketing\n  Mix Modeling", "comments": "7. figures 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Both Bayesian and varying coefficient models are very useful tools in\npractice as they can be used to model parameter heterogeneity in a\ngeneralizable way. Motivated by the need of enhancing Marketing Mix Modeling at\nUber, we propose a Bayesian Time Varying Coefficient model, equipped with a\nhierarchical Bayesian structure. This model is different from other time\nvarying coefficient models in the sense that the coefficients are weighted over\na set of local latent variables following certain probabilistic distributions.\nStochastic Variational Inference is used to approximate the posteriors of\nlatent variables and dynamic coefficients. The proposed model also helps\naddress many challenges faced by traditional MMM approaches. We used\nsimulations as well as real world marketing datasets to demonstrate our model\nsuperior performance in terms of both accuracy and interpretability.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 03:38:29 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 16:32:28 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Ng", "Edwin", ""], ["Wang", "Zhishi", ""], ["Dai", "Athena", ""]]}, {"id": "2106.03334", "submitter": "Yong He", "authors": "Hao Chen, Ying Guo, Yong He, Dong Liu, Lei Liu, Xiao-Hua Zhou", "title": "Joint Learning of Multiple Differential Networks with fMRI data for\n  Brain Connectivity Alteration Detection", "comments": "arXiv admin note: text overlap with arXiv:2005.08457 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we focus on the problem of joint learning of multiple\ndifferential networks with function Magnetic Resonance Imaging (fMRI) data sets\nfrom multiple research centers. As the research centers may use different\nscanners and imaging parameters, joint learning of differential networks with\nfMRI data from different centers may reflect the underlying mechanism of\nneurological diseases from different perspectives while capturing the common\nstructures. We transform the task as a penalized logistic regression problem,\nand exploit sparse group Minimax Concave Penalty (gMCP) to induce common\nstructures among multiple differential networks and the sparse structures of\neach differential network. To further enhance the empirical performance, we\ndevelop an ensemble-learning procedure. We conduct thorough simulation study to\nassess the finite-sample performance of the proposed method and compare with\nstate-of-the-art alternatives. We apply the proposed method to analyze fMRI\ndatasets related with Attention Deficit Hyperactivity Disorder from various\nresearch centers. The identified common hub nodes and differential interaction\npatterns coincides with the existing experimental studies.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 04:36:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chen", "Hao", ""], ["Guo", "Ying", ""], ["He", "Yong", ""], ["Liu", "Dong", ""], ["Liu", "Lei", ""], ["Zhou", "Xiao-Hua", ""]]}, {"id": "2106.03344", "submitter": "Fei Xue", "authors": "Fei Xue, Rong Ma, Hongzhe Li", "title": "Semi-Supervised Statistical Inference for High-Dimensional Linear\n  Regression with Blockwise Missing Data", "comments": "39 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blockwise missing data occurs frequently when we integrate multisource or\nmultimodality data where different sources or modalities contain complementary\ninformation. In this paper, we consider a high-dimensional linear regression\nmodel with blockwise missing covariates and a partially observed response\nvariable. Under this semi-supervised framework, we propose a computationally\nefficient estimator for the regression coefficient vector based on carefully\nconstructed unbiased estimating equations and a multiple blockwise imputation\nprocedure, and obtain its rates of convergence. Furthermore, building upon an\ninnovative semi-supervised projected estimating equation technique that\nintrinsically achieves bias-correction of the initial estimator, we propose\nnearly unbiased estimators for the individual regression coefficients that are\nasymptotically normally distributed under mild conditions. By carefully\nanalyzing these debiased estimators, asymptotically valid confidence intervals\nand statistical tests about each regression coefficient are constructed.\nNumerical studies and application analysis of the Alzheimer's Disease\nNeuroimaging Initiative data show that the proposed method performs better and\nbenefits more from unsupervised samples than existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 05:12:42 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xue", "Fei", ""], ["Ma", "Rong", ""], ["Li", "Hongzhe", ""]]}, {"id": "2106.03372", "submitter": "Rafael Wei{\\ss}bach", "authors": "L. Radloff, R. Weissbach, C. Reinke, G. Doblhammer", "title": "A consistent nonparametric test of the effect of dementia duration on\n  mortality", "comments": "51 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A continuous-time multi-state history is semi-Markovian, if an intensity to\nmigrate from one state into another, depends on the duration in the first\nstate. Such duration can be formalised as covariate, entering the intensity\nprocess of the transition counts. We derive the integrated intensity process,\nprove its predictability and the martingale property of the residual. In\nparticular, we verify the usual conditions for the respective filtration. As a\nconsequence, according to Nielsen and Linton (1995), a kernel estimator of the\ntransition intensity, including the duration dependence, converges point-wise\nat a slow rate, compared to the Markovian kernel estimator, i.e when ignoring\ndependence. By using the rate discrepancy, we follow Gozalo (1993) and show\nthat the (properly scaled) maximal difference of the two kernel estimators on a\nrandom grid of points is asymptotically chi-square-1-distributed. As a data\nexample, for a sample of 130,000 German women observed over a period of nine\nyears, we model the mortality after dementia onset, potentially dependent on\nthe disease duration. As usual, the models under both hypotheses need to be\nenlarged to allow for independent right-censoring. We find a significant effect\nof dementia duration, nearly independent of the bandwidth.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 06:54:06 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Radloff", "L.", ""], ["Weissbach", "R.", ""], ["Reinke", "C.", ""], ["Doblhammer", "G.", ""]]}, {"id": "2106.03533", "submitter": "Peter Craigmile", "authors": "Shreyan Ganguly and Peter F. Craigmile", "title": "Modeling Nonstationary Time Series using Locally Stationary Basis\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Methods of estimation and forecasting for stationary models are well known in\nclassical time series analysis. However, stationarity is an idealization which,\nin practice, can at best hold as an approximation, but for many time series may\nbe an unrealistic assumption. We define a class of locally stationary processes\nwhich can lead to more accurate uncertainty quantification over making an\ninvalid assumption of stationarity. This class of processes assumes the model\nparameters to be time-varying and parameterizes them in terms of a\ntransformation of basis functions that ensures that the processes are locally\nstationary. We develop methods and theory for parameter estimation in this\nclass of models, and propose a test that allow us to examine certain departures\nfrom stationarity. We assess our methods using simulation studies and apply\nthese techniques to the analysis of an electroencephalogram time series.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 11:46:29 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ganguly", "Shreyan", ""], ["Craigmile", "Peter F.", ""]]}, {"id": "2106.03591", "submitter": "Kexuan Li", "authors": "Kexuan Li, Fangfang Wang, Ruiqi Liu, Fan Yang, Zuofeng Shang", "title": "Calibrating multi-dimensional complex ODE from noisy data via deep\n  neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary differential equations (ODEs) are widely used to model complex\ndynamics that arises in biology, chemistry, engineering, finance, physics, etc.\nCalibration of a complicated ODE system using noisy data is generally very\ndifficult. In this work, we propose a two-stage nonparametric approach to\naddress this problem. We first extract the de-noised data and their higher\norder derivatives using boundary kernel method, and then feed them into a\nsparsely connected deep neural network with ReLU activation function. Our\nmethod is able to recover the ODE system without being subject to the curse of\ndimensionality and complicated ODE structure. When the ODE possesses a general\nmodular structure, with each modular component involving only a few input\nvariables, and the network architecture is properly chosen, our method is\nproven to be consistent. Theoretical properties are corroborated by an\nextensive simulation study that demonstrates the validity and effectiveness of\nthe proposed method. Finally, we use our method to simultaneously characterize\nthe growth rate of Covid-19 infection cases from 50 states of the USA.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 13:17:16 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Kexuan", ""], ["Wang", "Fangfang", ""], ["Liu", "Ruiqi", ""], ["Yang", "Fan", ""], ["Shang", "Zuofeng", ""]]}, {"id": "2106.03646", "submitter": "Xiaohao Cai", "authors": "Xiaohao Cai, Jason D. McEwen, Marcelo Pereyra", "title": "High-dimensional Bayesian model selection by proximal nested sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imaging methods often rely on Bayesian statistical inference strategies to\nsolve difficult imaging problems. Applying Bayesian methodology to imaging\nrequires the specification of a likelihood function and a prior distribution,\nwhich define the Bayesian statistical model from which the posterior\ndistribution of the image is derived. Specifying a suitable model for a\nspecific application can be very challenging, particularly when there is no\nreliable ground truth data available. Bayesian model selection provides a\nframework for selecting the most appropriate model directly from the observed\ndata, without reference to ground truth data. However, Bayesian model selection\nrequires the computation of the marginal likelihood (Bayesian evidence), which\nis computationally challenging, prohibiting its use in high-dimensional imaging\nproblems. In this work we present the proximal nested sampling methodology to\nobjectively compare alternative Bayesian imaging models, without reference to\nground truth data. The methodology is based on nested sampling, a Monte Carlo\napproach specialised for model comparison, and exploits proximal Markov chain\nMonte Carlo techniques to scale efficiently to large problems and to tackle\nmodels that are log-concave and not necessarily smooth (e.g., involving L1 or\ntotal-variation priors). The proposed approach can be applied computationally\nto problems of dimension O(10^6) and beyond, making it suitable for\nhigh-dimensional inverse imaging problems. It is validated on large Gaussian\nmodels, for which the likelihood is available analytically, and subsequently\nillustrated on a range of imaging problems where it is used to analyse\ndifferent choices for the sparsifying dictionary and measurement model.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 14:18:45 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Cai", "Xiaohao", ""], ["McEwen", "Jason D.", ""], ["Pereyra", "Marcelo", ""]]}, {"id": "2106.03713", "submitter": "Jan Vandenbroucke", "authors": "Jan P Vandenbroucke, Elizabeth B Brickley, Christina M.J.E.\n  Vandenbroucke-Grauls, Neil Pearce", "title": "The evolving usefulness of the Test-Negative Design in studying risk\n  factors for COVID-19 due to changes in testing policy", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is a short extension of our previous paper [arXiv:2004.06033]\nabout the use of the Test-Negative design to study risk factors for COVID-19\n[See: PubMed and ArXiv reference below] Reason for the extension is that the\nconditions under which people refer themselves for testing have greatly\nchanged: originally, in most countries priority was given to people with\nsymptoms, but nowadays people without symptoms are also tested for different\nreasons, e.g., during contact tracing, or to be allowed on an (international)\nflight. Interestingly, this opens new possibilities to separately investigate\nrisk factors for infection and risk factors for becoming diseased. To use this\nnew situation to best effect, one has to think carefully about how to elucidate\nthe different reasons for testing and what analyses one might do with the\ndifferent groups.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 15:24:17 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Vandenbroucke", "Jan P", ""], ["Brickley", "Elizabeth B", ""], ["Vandenbroucke-Grauls", "Christina M. J. E.", ""], ["Pearce", "Neil", ""]]}, {"id": "2106.03737", "submitter": "Isa Marques", "authors": "Isa Marques, Thomas Kneib, Nadja Klein", "title": "A multivariate Gaussian random field prior against spatial confounding", "comments": "Submitted to Environmetrics and currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial models are used in a variety research areas, such as environmental\nsciences, epidemiology, or physics. A common phenomenon in many spatial\nregression models is spatial confounding. This phenomenon takes place when\nspatially indexed covariates modeling the mean of the response are correlated\nwith the spatial random effect. As a result, estimates for regression\ncoefficients of the covariates can be severely biased and interpretation of\nthese is no longer valid. Recent literature has shown that typical solutions\nfor reducing spatial confounding can lead to misleading and counterintuitive\nresults. In this paper, we develop a computationally efficient spatial model in\na Bayesian framework integrating novel prior structure that reduces spatial\nconfounding. Starting from the univariate case, we extend our prior structure\nto case of multiple spatially confounded covariates. In a simulation study, we\nshow that our novel model flexibly detects and reduces spatial confounding in\nspatial datasets, and it performs better than typically used methods such as\nrestricted spatial regression. These results are promising for any applied\nresearcher who wishes to interpret covariate effects in spatial regression\nmodels. As a real data illustration, we study the effect of elevation and\ntemperature on the mean of daily precipitation in Germany.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:01:45 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Marques", "Isa", ""], ["Kneib", "Thomas", ""], ["Klein", "Nadja", ""]]}, {"id": "2106.03742", "submitter": "Jeffrey N\\\"af", "authors": "Loris Michel, Jeffrey N\\\"af, Meta-Lina Spohn, Nicolai Meinshausen", "title": "Proper Scoring Rules for Missing Value Imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Given the prevalence of missing data in modern statistical research, a broad\nrange of methods is available for any given imputation task. How does one\nchoose the `best' method in a given application? The standard approach is to\nselect some observations, set their status to missing, and compare prediction\naccuracy of the methods under consideration for these observations. Besides\nhaving to somewhat artificially mask additional observations, a shortcoming of\nthis approach is that the optimal imputation in this scheme chooses the\nconditional mean if predictive accuracy is measured with RMSE. In contrast, we\nwould like to rank highest methods that can sample from the true conditional\ndistribution. In this paper, we develop a principled and easy-to-use evaluation\nmethod for missing value imputation under the missing completely at random\n(MCAR) assumption. The approach is applicable for discrete and continuous data\nand works on incomplete data sets, without having to leave out additional\nobservations for evaluation. Moreover, it favors imputation methods that\nreproduce the original data distribution. We show empirically on a range of\ndata sets and imputation methods that our score consistently ranks true data\nhigh(est) and is able to avoid pitfalls usually associated with performance\nmeasures such as RMSE. Finally, we provide an R-package with an implementation\nof our method.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 16:07:03 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Michel", "Loris", ""], ["N\u00e4f", "Jeffrey", ""], ["Spohn", "Meta-Lina", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "2106.03811", "submitter": "Antonio Forcina", "authors": "Antonio Forcina and Francesco Bartolucci", "title": "Estimating the size of a closed population by modeling latent and\n  observed heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper describe a new class of capture-recapture models for closed\npopulations when individual covariates are available. The novelty consists in\ncombining a latent class model for capture probabilities where the marginal\nweights and the conditional distributions given the latent may depend on\ncovariates, with a model for the marginal distribution of the available\ncovariates. In addition, a general formulation for the conditional\ndistributions given the latent and covariates which allows serial dependence is\nprovided. A Fisher scoring algorithm for maximum likelihood estimation is\npresented, asymptotic results are derived, and a procedure for constructing\nlikelihood based confidence intervals for the population total is presented.\nTwo examples with real data are used to illustrate the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 17:19:56 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 19:18:28 GMT"}, {"version": "v3", "created": "Sun, 25 Jul 2021 16:05:37 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Forcina", "Antonio", ""], ["Bartolucci", "Francesco", ""]]}, {"id": "2106.03990", "submitter": "Mauricio Girardi-Schappo", "authors": "Vinicius Lima, Fernanda Jaiara Dellajustina, Renan O. Shimoura,\n  Mauricio Girardi-Schappo, Nilton L. Kamiji, Rodrigo F. O. Pena, Antonio C.\n  Roque", "title": "Granger causality in the frequency domain: derivation and applications", "comments": "21 pages, 10 figures", "journal-ref": "Rev. Bras. Ensino F\\'is. 42: e20200007 (2020)", "doi": "10.1590/1806-9126-RBEF-2020-0007", "report-no": null, "categories": "physics.data-an physics.bio-ph q-bio.QM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physicists are starting to work in areas where noisy signal analysis is\nrequired. In these fields, such as Economics, Neuroscience, and Physics, the\nnotion of causality should be interpreted as a statistical measure. We\nintroduce to the lay reader the Granger causality between two time series and\nillustrate ways of calculating it: a signal $X$ ``Granger-causes'' a signal $Y$\nif the observation of the past of $X$ increases the predictability of the\nfuture of $Y$ when compared to the same prediction done with the past of $Y$\nalone. In other words, for Granger causality between two quantities it suffices\nthat information extracted from the past of one of them improves the forecast\nof the future of the other, even in the absence of any physical mechanism of\ninteraction. We present derivations of the Granger causality measure in the\ntime and frequency domains and give numerical examples using a non-parametric\nestimation method in the frequency domain. Parametric methods are addressed in\nthe Appendix. We discuss the limitations and applications of this method and\nother alternatives to measure causality.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2021 22:18:36 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Lima", "Vinicius", ""], ["Dellajustina", "Fernanda Jaiara", ""], ["Shimoura", "Renan O.", ""], ["Girardi-Schappo", "Mauricio", ""], ["Kamiji", "Nilton L.", ""], ["Pena", "Rodrigo F. O.", ""], ["Roque", "Antonio C.", ""]]}, {"id": "2106.04106", "submitter": "Jianqiao Wang", "authors": "Jianqiao Wang, Sai Li and Hongzhe Li", "title": "A Unified Approach to Robust Inference for Genetic Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies (GWAS) have identified thousands of genetic\nvariants associated with complex traits. Many complex traits are found to have\nshared genetic etiology. Genetic covariance is defined as the underlying\ncovariance of genetic values and can be used to measure the shared genetic\narchitecture. The data of two outcomes may be collected from the same group or\ndifferent groups of individuals and the outcomes can be of different types or\ncollected based on different study designs. This paper proposes a unified\napproach to robust estimation and inference for genetic covariance of general\noutcomes that may be associated with genetic variants nonlinearly. We provide\nthe asymptotic properties of the proposed estimator and show that our proposal\nis robust under certain model mis-specification. Our method under linear\nworking models provides robust inference for the narrow-sense genetic\ncovariance, even when both linear models are mis-specified. Various numerical\nexperiments are performed to support the theoretical results. Our method is\napplied to an outbred mice GWAS data set to study the overlapping genetic\neffects between the behavioral and physiological phenotypes. The real data\nresults demonstrate the robustness of the proposed method and reveal\ninteresting genetic covariance among different mice developmental traits.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 05:11:06 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wang", "Jianqiao", ""], ["Li", "Sai", ""], ["Li", "Hongzhe", ""]]}, {"id": "2106.04118", "submitter": "Matteo Sesia", "authors": "Shuangning Li, Matteo Sesia, Yaniv Romano, Emmanuel Cand\\`es, Chiara\n  Sabatti", "title": "Searching for consistent associations with a multi-environment knockoff\n  filter", "comments": "41 pages, 21 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a method based on model-X knockoffs to find conditional\nassociations that are consistent across diverse environments, controlling the\nfalse discovery rate. The motivation for this problem is that large data sets\nmay contain numerous associations that are statistically significant and yet\nmisleading, as they are induced by confounders or sampling imperfections.\nHowever, associations consistently replicated under different conditions may be\nmore interesting. In fact, consistency sometimes provably leads to valid causal\ninferences even if conditional associations do not. While the proposed method\nis flexible and can be deployed in a wide range of applications, this paper\nhighlights its relevance to genome-wide association studies, in which\nconsistency across populations with diverse ancestries mitigates confounding\ndue to unmeasured variants. The effectiveness of this approach is demonstrated\nby simulations and applications to the UK Biobank data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 06:04:27 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Li", "Shuangning", ""], ["Sesia", "Matteo", ""], ["Romano", "Yaniv", ""], ["Cand\u00e8s", "Emmanuel", ""], ["Sabatti", "Chiara", ""]]}, {"id": "2106.04142", "submitter": "Khandoker Akib Mohammad", "authors": "Khandoker Akib Mohammad, Yuichi Hirose, Yuan Yao and Budhi Surya", "title": "Efficient Estimation For The Joint Model of Survival and Longitudinal\n  Data", "comments": "15 pages. arXiv admin note: substantial text overlap with\n  arXiv:1905.08424", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In survival studies it is important to record the values of key longitudinal\ncovariates until the occurrence of event of a subject. For this reason, it is\nessential to study the association between longitudinal and time-to-event\noutcomes using the joint model. In this paper, profile likelihood approach has\nbeen used to estimate the cumulative hazard and regression parameters from the\njoint model of survival and longitudinal data. Moreover, we show the asymptotic\nnormality of the profile likelihood estimator via asymptotic expansion of the\nprofile likelihood and obtain the explicit form of the variance estimator. The\nestimators of the regression parameters from the joint model are shown to be\nsemi-parametric efficient.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 07:00:13 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Mohammad", "Khandoker Akib", ""], ["Hirose", "Yuichi", ""], ["Yao", "Yuan", ""], ["Surya", "Budhi", ""]]}, {"id": "2106.04170", "submitter": "Tiangang Cui", "authors": "Tiangang Cui and Sergey Dolgov and Olivier Zahm", "title": "Conditional Deep Inverse Rosenblatt Transports", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel offline-online method to mitigate the computational burden\nof the characterization of conditional beliefs in statistical learning. In the\noffline phase, the proposed method learns the joint law of the belief random\nvariables and the observational random variables in the tensor-train (TT)\nformat. In the online phase, it utilizes the resulting order-preserving\nconditional transport map to issue real-time characterization of the\nconditional beliefs given new observed information. Compared with the\nstate-of-the-art normalizing flows techniques, the proposed method relies on\nfunction approximation and is equipped with thorough performance analysis. This\nalso allows us to further extend the capability of transport maps in\nchallenging problems with high-dimensional observations and high-dimensional\nbelief variables. On the one hand, we present novel heuristics to reorder\nand/or reparametrize the variables to enhance the approximation power of TT. On\nthe other, we integrate the TT-based transport maps and the parameter\nreordering/reparametrization into layered compositions to further improve the\nperformance of the resulting transport maps. We demonstrate the efficiency of\nthe proposed method on various statistical learning tasks in ordinary\ndifferential equations (ODEs) and partial differential equations (PDEs).\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 08:23:11 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Cui", "Tiangang", ""], ["Dolgov", "Sergey", ""], ["Zahm", "Olivier", ""]]}, {"id": "2106.04237", "submitter": "Martin Huber", "authors": "Yu-Chin Hsu, Martin Huber, Ying-Ying Lee, Chu-An Liu", "title": "Testing Monotonicity of Mean Potential Outcomes in a Continuous\n  Treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While most treatment evaluations focus on binary interventions, a growing\nliterature also considers continuously distributed treatments, e.g. hours spent\nin a training program to assess its effect on labor market outcomes. In this\npaper, we propose a Cram\\'er-von Mises-type test for testing whether the mean\npotential outcome given a specific treatment has a weakly monotonic\nrelationship with the treatment dose under a weak unconfoundedness assumption.\nThis appears interesting for testing shape restrictions, e.g. whether\nincreasing the treatment dose always has a non-negative effect, no matter what\nthe baseline level of treatment is. We formally show that the proposed test\ncontrols asymptotic size and is consistent against any fixed alternative. These\ntheoretical findings are supported by the method's finite sample behavior in\nour Monte-Carlo simulations. As an empirical illustration, we apply our test to\nthe Job Corps study and reject a weakly monotonic relationship between the\ntreatment (hours in academic and vocational training) and labor market outcomes\nlike earnings or employment.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 10:33:09 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Hsu", "Yu-Chin", ""], ["Huber", "Martin", ""], ["Lee", "Ying-Ying", ""], ["Liu", "Chu-An", ""]]}, {"id": "2106.04253", "submitter": "Yi Zhou", "authors": "Yi Zhou, Ao Huang and Satoshi Hattori", "title": "A likelihood based sensitivity analysis for publication bias on summary\n  ROC in meta-analysis of diagnostic test accuracy", "comments": "32 pages, 4 figure, 5 tables in main text, 5 tables in supplementary\n  file", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In meta-analysis of diagnostic test accuracy, summary receiver operating\ncharacteristic (SROC) is a recommended method to summarize the discriminant\ncapacity of a diagnostic test in the presence of study-specific cutoff values\nand the area under the SROC (SAUC) gives the aggregate measure of test\naccuracy. SROC or SAUC can be estimated by bivariate modelling of pairs of\nsensitivity and specificity over the primary diagnostic studies. However,\npublication bias is a major threat to the validity of estimates in\nmeta-analysis. To address this issue, we propose to adopt sensitivity analysis\nto make an objective inference for the impact of publication bias on SROC or\nSAUC. We extend Copas likelihood based sensitivity analysis to the bivariate\nnormal model used for meta-analysis of diagnostic test accuracy to evaluate how\nmuch SROC or SAUC would change with different selection probabilities under\nseveral selective publication mechanisms dependent on sensitivity and/or\nspecificity. The selection probability is modelled by a selection function on\n$t$-type statistic for the linear combination of logit-transformed sensitivity\nand specificity, allowing the selective publication of each study to be\ninfluenced by the cutoff-dependent $p$-value for sensitivity, specificity, or\ndiagnostic odds ratio. By embedding the selection function into the bivariate\nnormal model, the conditional likelihood is proposed and the bias-corrected\nSROC or SAUC can be estimated by maximizing the likelihood. We illustrate the\nproposed sensitivity analysis by reanalyzing a meta-analysis of test accuracy\nfor intravascular device related infection. Simulation studies are conducted to\ninvestigate the performance of proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 11:23:46 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Zhou", "Yi", ""], ["Huang", "Ao", ""], ["Hattori", "Satoshi", ""]]}, {"id": "2106.04271", "submitter": "Tyler McCormick", "authors": "Mengjie Pan, Tyler H. McCormick, Bailey K. Fosdick", "title": "Inference for Network Regression Models with Community Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Network regression models, where the outcome comprises the valued edge in a\nnetwork and the predictors are actor or dyad-level covariates, are used\nextensively in the social and biological sciences. Valid inference relies on\naccurately modeling the residual dependencies among the relations. Frequently\nhomogeneity assumptions are placed on the errors which are commonly incorrect\nand ignore critical, natural clustering of the actors. In this work, we present\na novel regression modeling framework that models the errors as resulting from\na community-based dependence structure and exploits the subsequent\nexchangeability properties of the error distribution to obtain parsimonious\nstandard errors for regression parameters.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 12:04:31 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Pan", "Mengjie", ""], ["McCormick", "Tyler H.", ""], ["Fosdick", "Bailey K.", ""]]}, {"id": "2106.04304", "submitter": "Beth Ann Griffin PhD", "authors": "Beth Ann Griffin, Megan S. Schuler, Joseph Pane, Stephen W. Patrick,\n  Rosanna Smart, Bradley D. Stein, Geoffrey Grimm, Elizabeth A. Stuart", "title": "Methodological considerations for estimating policy effects in the\n  context of co-occurring policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective. Understanding how best to estimate state-level policy effects is\nimportant, and several unanswered questions remain, particularly about optimal\nmethods for disentangling the effects of concurrently implemented policies. In\nthis paper, we examined the impact of co-occurring policies on the performance\nof commonly used models in state policy evaluations.\n  Data Sources. Outcome of interest (annual state-specific opioid mortality\nrate per 100,000) was obtained from 1999-2016 National Vital Statistics System\n(NVSS) Multiple Cause of Death mortality files.\n  Study Design. We utilized Monte Carlo simulations to assess the effect of\nconcurrent policy enactment on the evaluation of state-level policies.\nSimulation conditions varied effect sizes of the co-occurring policies as well\nas the length of time between enactment dates of the co-occurring policies,\namong other factors.\n  Data Collection. Longitudinal annual state-level data over 18 years from 50\nstates.\n  Principal Findings. Our results demonstrated high relative bias (>85%) will\narise when confounding co-occurring policies are omitted from the analytic\nmodel and the co-occuring policies are enacted in rapid succession. Moreover,\nour findings indicated that controlling for all co-occurring policies will\neffectively mitigate the threat of confounding bias; however, effect estimates\nmay be relatively imprecise, with larger variance estimates when co-occurring\npolicies were enacted in near succession of each other. We also found that the\nrequired length of time between co-occurring policies necessary to obtain\nrobust policy estimates varied across model specifications, being generally\nshorter for autoregressive (AR) models compared to difference-in-differences\n(DID) models.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 13:04:14 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Griffin", "Beth Ann", ""], ["Schuler", "Megan S.", ""], ["Pane", "Joseph", ""], ["Patrick", "Stephen W.", ""], ["Smart", "Rosanna", ""], ["Stein", "Bradley D.", ""], ["Grimm", "Geoffrey", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "2106.04416", "submitter": "Manuele Leonelli", "authors": "Manuele Leonelli and Gherardo Varando", "title": "Context-Specific Causal Discovery for Categorical Data Using Staged\n  Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal discovery algorithms aims at untangling complex causal relationships\nusing observational data only. Here, we introduce new causal discovery\nalgorithms based on staged tree models, which can represent complex and\nnon-symmetric causal effects. To demonstrate the efficacy of our algorithms, we\nintroduce a new distance, inspired by the widely used structural interventional\ndistance, to quantify the closeness between two staged trees in terms of their\ncorresponding causal inference statements. A simulation study highlights the\nefficacy of staged trees in uncovering complex, asymmetric causal relationship\nfrom data and a real-world data application illustrates their use in a\npractical causal analysis.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:46:15 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Leonelli", "Manuele", ""], ["Varando", "Gherardo", ""]]}, {"id": "2106.04424", "submitter": "Vincent Audigier", "authors": "Vincent Audigier, Nd\\`eye Niang, Matthieu Resche-Rigon", "title": "Clustering with missing data: which imputation model for which cluster\n  analysis method?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple imputation (MI) is a popular method for dealing with missing values.\nOne main advantage of MI is to separate the imputation phase and the analysis\none. However, both are related since they are based on distribution assumptions\nthat have to be consistent. This point is well known as congeniality.\n  In this paper, we discuss congeniality for clustering on continuous data.\nFirst, we theoretically highlight how two joint modeling (JM) MI methods (JM-GL\nand JM-DP) are congenial with various clustering methods. Then, we propose a\nnew fully conditional specification (FCS) MI method with the same theoretical\nproperties as JM-GL. Finally, we extend this FCS MI method to account for more\ncomplex distributions. Based on an extensive simulation study, all MI methods\nare compared for various cluster analysis methods (k-means, k-medoids, mixture\nmodel, hierarchical clustering).\n  This study highlights the partition accuracy is improved when the imputation\nmodel accounts for clustered individuals. From this point of view, standard MI\nmethods ignoring such a structure should be avoided. JM-GL and JM-DP should be\nrecommended when data are distributed according to a gaussian mixture model,\nwhile FCS methods outperform JM ones on more complex data.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 14:53:01 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Audigier", "Vincent", ""], ["Niang", "Nd\u00e8ye", ""], ["Resche-Rigon", "Matthieu", ""]]}, {"id": "2106.04433", "submitter": "Alexander Wimbush", "authors": "Alexander Wimbush, Nicholas Gray, Scott Ferson", "title": "Singhing with Confidence: Visualising the Performance of Confidence\n  Structures", "comments": "12 Pages Textx, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Confidence intervals are an established means of portraying uncertainty about\nan inferred parameter and can be generated through the use of confidence\ndistributions. For a confidence distribution to be ideal, it must maintain\nfrequentist coverage of the true parameter. This can be represented for a\nprecise distribution by adherence to a cumulative unit uniform distribution,\nreferred to here as a Singh plot. This manuscript extends this to imprecise\nconfidence structures with bounds around the uniform distribution, and\ndescribes how deviations convey information regarding the characteristics of\nconfidence structures designed for inference and prediction. This quick visual\nrepresentation, in a manner similar to ROC curves, aids the development of\nrobust structures and methods that make use of confidence. A demonstration of\nthe utility of Singh plots is provided with an assessment of the coverage of\nthe ProUCL Chebyshev upper confidence limit estimator for the mean of an\nunknown distribution.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 15:09:57 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wimbush", "Alexander", ""], ["Gray", "Nicholas", ""], ["Ferson", "Scott", ""]]}, {"id": "2106.04503", "submitter": "Demetrios Papakostas", "authors": "Demetrios Papakostas, P.Richard Hahn, Jared Murray, Frank Zhou, and\n  Joseph Gerakos", "title": "Do forecasts of bankruptcy cause bankruptcy? A machine learning\n  sensitivity analysis", "comments": "26 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely speculated that auditors' public forecasts of bankruptcy are, at\nleast in part, self-fulfilling prophecies in the sense that they might actually\ncause bankruptcies that would not have otherwise occurred. This conjecture is\nhard to prove, however, because the strong association between bankruptcies and\nbankruptcy forecasts could simply indicate that auditors are skillful\nforecasters with unique access to highly predictive covariates. In this paper,\nwe investigate the causal effect of bankruptcy forecasts on bankruptcy using\nnonparametric sensitivity analysis. We contrast our analysis with two\nalternative approaches: a linear bivariate probit model with an endogenous\nregressor, and a recently developed bound on risk ratios called E-values.\nAdditionally, our machine learning approach incorporates a monotonicity\nconstraint corresponding to the assumption that bankruptcy forecasts do not\nmake bankruptcies less likely. Finally, a tree-based posterior summary of the\ntreatment effect estimates allows us to explore which observable firm\ncharacteristics moderate the inducement effect.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 16:43:55 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Papakostas", "Demetrios", ""], ["Hahn", "P. Richard", ""], ["Murray", "Jared", ""], ["Zhou", "Frank", ""], ["Gerakos", "Joseph", ""]]}, {"id": "2106.04654", "submitter": "Chunyi Zhao", "authors": "Chunyi Zhao, Athanasios Kottas", "title": "Modelling for Poisson process intensities over irregular spatial domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop nonparametric Bayesian modelling approaches for Poisson processes,\nusing weighted combinations of structured beta densities to represent the point\nprocess intensity function. For a regular spatial domain, such as the unit\nsquare, the model construction implies a Bernstein-Dirichlet prior for the\nPoisson process density, which supports general inference for point process\nfunctionals. The key contribution of the methodology is two classes of flexible\nand computationally efficient models for spatial Poisson process intensities\nover irregular domains. We address the choice or estimation of the number of\nbeta basis densities, and develop methods for prior specification and posterior\nsimulation for full inference about functionals of the point process. The\nmethodology is illustrated with both synthetic and real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 19:35:46 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhao", "Chunyi", ""], ["Kottas", "Athanasios", ""]]}, {"id": "2106.04741", "submitter": "Dar Gilboa", "authors": "Dar Gilboa, Ari Pakman, Thibault Vatter", "title": "Marginalizable Density Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Probability density models based on deep networks have achieved remarkable\nsuccess in modeling complex high-dimensional datasets. However, unlike kernel\ndensity estimators, modern neural models do not yield marginals or conditionals\nin closed form, as these quantities require the evaluation of seldom tractable\nintegrals. In this work, we present the Marginalizable Density Model\nApproximator (MDMA), a novel deep network architecture which provides closed\nform expressions for the probabilities, marginals and conditionals of any\nsubset of the variables. The MDMA learns deep scalar representations for each\nindividual variable and combines them via learned hierarchical tensor\ndecompositions into a tractable yet expressive CDF, from which marginals and\nconditional densities are easily obtained. We illustrate the advantage of exact\nmarginalizability in several tasks that are out of reach of previous deep\nnetwork-based density estimation models, such as estimating mutual information\nbetween arbitrary subsets of variables, inferring causality by testing for\nconditional independence, and inference with missing data without the need for\ndata imputation, outperforming state-of-the-art models on these tasks. The\nmodel also allows for parallelized sampling with only a logarithmic dependence\nof the time complexity on the number of variables.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 23:54:48 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Gilboa", "Dar", ""], ["Pakman", "Ari", ""], ["Vatter", "Thibault", ""]]}, {"id": "2106.04862", "submitter": "Boyao Zhang", "authors": "Boyao Zhang, Colin Griesbach, Cora Kim, Nadia M\\\"uller-Voggel,\n  Elisabeth Bergherr", "title": "Bayesian Boosting for Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting methods are widely used in statistical learning to deal with\nhigh-dimensional data due to their variable selection feature. However, those\nmethods lack straightforward ways to construct estimators for the precision of\nthe parameters such as variance or confidence interval, which can be achieved\nby conventional statistical methods like Bayesian inference. In this paper, we\npropose a new inference method \"BayesBoost\" that combines boosting and Bayesian\nfor linear mixed models to make the uncertainty estimation for the random\neffects possible on the one hand. On the other hand, the new method overcomes\nthe shortcomings of Bayesian inference in giving precise and unambiguous\nguidelines for the selection of covariates by benefiting from boosting\ntechniques. The implementation of Bayesian inference leads to the randomness of\nmodel selection criteria like the conditional AIC (cAIC), so we also propose a\ncAIC-based model selection criteria that focus on the stabilized regions\ninstead of the global minimum. The effectiveness of the new approach can be\nobserved via simulation and in a data example from the field of neurophysiology\nfocussing on the mechanisms in the brain while listening to unpleasant sounds.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 07:40:00 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Zhang", "Boyao", ""], ["Griesbach", "Colin", ""], ["Kim", "Cora", ""], ["M\u00fcller-Voggel", "Nadia", ""], ["Bergherr", "Elisabeth", ""]]}, {"id": "2106.04869", "submitter": "Xin Liu", "authors": "Xin Liu, Liwen Zhang, Zhen Zhang", "title": "Ultra High Dimensional Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Structural breaks have been commonly seen in applications. Specifically for\ndetection of change points in time, research gap still remains on the setting\nin ultra high dimension, where the covariates may bear spurious correlations.\nIn this paper, we propose a two-stage approach to detect change points in ultra\nhigh dimension, by firstly proposing the dynamic titled current correlation\nscreening method to reduce the input dimension, and then detecting possible\nchange points in the framework of group variable selection. Not only the\nspurious correlation between ultra-high dimensional covariates is taken into\nconsideration in variable screening, but non-convex penalties are studied in\nchange point detection in the ultra high dimension. Asymptotic properties are\nderived to guarantee the asymptotic consistency of the selection procedure, and\nthe numerical investigations show the promising performance of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 07:50:23 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Liu", "Xin", ""], ["Zhang", "Liwen", ""], ["Zhang", "Zhen", ""]]}, {"id": "2106.05024", "submitter": "Paul Goldsmith-Pinkham", "authors": "Paul Goldsmith-Pinkham, Peter Hull, and Michal Koles\\'ar", "title": "On Estimating Multiple Treatment Effects with Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the causal interpretation of regressions on multiple dependent\ntreatments and flexible controls. Such regressions are often used to analyze\nrandomized control trials with multiple intervention arms, and to estimate\ninstitutional quality (e.g. teacher value-added) with observational data. We\nshow that, unlike with a single binary treatment, these regressions do not\ngenerally estimate convex averages of causal effects-even when the treatments\nare conditionally randomly assigned and the controls fully address omitted\nvariables bias. We discuss different solutions to this issue, and propose as a\nsolution anew class of efficient estimators of weighted average treatment\neffects.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:33:59 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Goldsmith-Pinkham", "Paul", ""], ["Hull", "Peter", ""], ["Koles\u00e1r", "Michal", ""]]}, {"id": "2106.05031", "submitter": "Shosei Sakaguchi", "authors": "Shosei Sakaguchi", "title": "Estimation of Optimal Dynamic Treatment Assignment Rules under Policy\n  Constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper studies statistical decisions for dynamic treatment assignment\nproblems. Many policies involve dynamics in their treatment assignments where\ntreatments are sequentially assigned to individuals across multiple stages and\nthe effect of treatment at each stage is usually heterogeneous with respect to\nthe prior treatments, past outcomes, and observed covariates. We consider\nestimating an optimal dynamic treatment rule that guides the optimal treatment\nassignment for each individual at each stage based on the individual's history.\nThis paper proposes an empirical welfare maximization approach in a dynamic\nframework. The approach estimates the optimal dynamic treatment rule from panel\ndata taken from an experimental or quasi-experimental study. The paper proposes\ntwo estimation methods: one solves the treatment assignment problem at each\nstage through backward induction, and the other solves the whole dynamic\ntreatment assignment problem simultaneously across all stages. We derive\nfinite-sample upper bounds on the worst-case average welfare-regrets for the\nproposed methods and show $n^{-1/2}$-minimax convergence rates. We also modify\nthe simultaneous estimation method to incorporate intertemporal budget/capacity\nconstraints.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 12:42:53 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 17:14:06 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Sakaguchi", "Shosei", ""]]}, {"id": "2106.05067", "submitter": "Marco Mingione", "authors": "Marco Mingione and Pierfrancesco Alaimo Di Loro and Alessio Farcomeni\n  and Fabio Divino and Gianfranco Lovison and Giovanna Jona Lasinio and\n  Antonello Maruotti", "title": "Spatial modelling of COVID-19 incident cases using Richards' curve: an\n  application to the Italian regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce an extended generalised logistic growth model for discrete\noutcomes, in which a network structure can be specified to deal with spatial\ndependence and time dependence is dealt with using an Auto-Regressive approach.\nA major challenge concerns the specification of the network structure, crucial\nto consistently estimate the canonical parameters of the generalised logistic\ncurve, e.g. peak time and height. Parameters are estimated under the Bayesian\nframework, using the {\\texttt{ Stan}} probabilistic programming language. The\nproposed approach is motivated by the analysis of the first and second wave of\nCOVID-19 in Italy, i.e. from February 2020 to July 2020 and from July 2020 to\nDecember 2020, respectively. We analyse data at the regional level and,\ninterestingly enough, prove that substantial spatial and temporal dependence\noccurred in both waves, although strong restrictive measures were implemented\nduring the first wave. Accurate predictions are obtained, improving those of\nthe model where independence across regions is assumed.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 13:38:24 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Mingione", "Marco", ""], ["Di Loro", "Pierfrancesco Alaimo", ""], ["Farcomeni", "Alessio", ""], ["Divino", "Fabio", ""], ["Lovison", "Gianfranco", ""], ["Lasinio", "Giovanna Jona", ""], ["Maruotti", "Antonello", ""]]}, {"id": "2106.05074", "submitter": "Limor Gultchin", "authors": "Limor Gultchin, David S. Watson, Matt J. Kusner, Ricardo Silva", "title": "Operationalizing Complex Causes: A Pragmatic View of Mediation", "comments": null, "journal-ref": "International Conference on Machine Learning 2021", "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We examine the problem of causal response estimation for complex objects\n(e.g., text, images, genomics). In this setting, classical \\emph{atomic}\ninterventions are often not available (e.g., changes to characters, pixels, DNA\nbase-pairs). Instead, we only have access to indirect or \\emph{crude}\ninterventions (e.g., enrolling in a writing program, modifying a scene,\napplying a gene therapy). In this work, we formalize this problem and provide\nan initial solution. Given a collection of candidate mediators, we propose (a)\na two-step method for predicting the causal responses of crude interventions;\nand (b) a testing procedure to identify mediators of crude interventions. We\ndemonstrate, on a range of simulated and real-world-inspired examples, that our\napproach allows us to efficiently estimate the effect of crude interventions\nwith limited data from new treatment regimes.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 13:52:38 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 17:55:53 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Gultchin", "Limor", ""], ["Watson", "David S.", ""], ["Kusner", "Matt J.", ""], ["Silva", "Ricardo", ""]]}, {"id": "2106.05092", "submitter": "David Degras", "authors": "David Degras, Chee-Ming Ting, Hernando Ombao", "title": "Markov-Switching State-Space Models with Applications to Neuroimaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  State-space models (SSM) with Markov switching offer a powerful framework for\ndetecting multiple regimes in time series, analyzing mutual dependence and\ndynamics within regimes, and asserting transitions between regimes. These\nmodels however present considerable computational challenges due to the\nexponential number of possible regime sequences to account for. In addition,\nhigh dimensionality of time series can hinder likelihood-based inference. This\npaper proposes novel statistical methods for Markov-switching SSMs using\nmaximum likelihood estimation, Expectation-Maximization (EM), and parametric\nbootstrap. We develop solutions for initializing the EM algorithm, accelerating\nconvergence, and conducting inference that are ideally suited to massive\nspatio-temporal data such as brain signals. We evaluate these methods in\nsimulations and present applications to EEG studies of epilepsy and of motor\nimagery. All proposed methods are implemented in a MATLAB toolbox available at\nhttps://github.com/ddegras/switch-ssm.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:12:27 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Degras", "David", ""], ["Ting", "Chee-Ming", ""], ["Ombao", "Hernando", ""]]}, {"id": "2106.05116", "submitter": "Jarret Petrillo", "authors": "Jarret Petrillo", "title": "Verification and Validation of Log-Periodic Power Law Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and implement a nonlinear Verification and Validation (V&V)\nmethodology to test two fitting procedures for the log-periodic power law model\n(LPPL), a model that has diverse applications across data analysis, but known\nestimation issues. Prior studies have focused on ex-post analyses of rare\nevents: Earthquakes, glacial break-off events, and financial crashes. Or, on\nnon-dynamical simulations such as additive noise or resampling. Our results\nreject an estimation scheme that pre-conditions observed data by fitting and\nremoving an exponential trend. We validate a subordinated algorithm, and\nconfirm that it passes Feigenbaum's criticism, which articulates a broad hurdle\nfor ex-post statistical learning from rare events.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 14:48:04 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Petrillo", "Jarret", ""]]}, {"id": "2106.05172", "submitter": "Bradley Price", "authors": "Ben Sherwood and Bradley S. Price", "title": "On the Use of Minimum Penalties in Statistical Learning", "comments": "35 pages 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern multivariate machine learning and statistical methodologies estimate\nparameters of interest while leveraging prior knowledge of the association\nbetween outcome variables. The methods that do allow for estimation of\nrelationships do so typically through an error covariance matrix in\nmultivariate regression which does not scale to other types of models. In this\narticle we proposed the MinPEN framework to simultaneously estimate regression\ncoefficients associated with the multivariate regression model and the\nrelationships between outcome variables using mild assumptions. The MinPen\nframework utilizes a novel penalty based on the minimum function to exploit\ndetected relationships between responses. An iterative algorithm that\ngeneralizes current state of the art methods is proposed as a solution to the\nnon-convex optimization that is required to obtain estimates. Theoretical\nresults such as high dimensional convergence rates, model selection\nconsistency, and a framework for post selection inference are provided. We\nextend the proposed MinPen framework to other exponential family loss\nfunctions, with a specific focus on multiple binomial responses. Tuning\nparameter selection is also addressed. Finally, simulations and two data\nexamples are presented to show the finite sample properties of this framework.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:15:46 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Sherwood", "Ben", ""], ["Price", "Bradley S.", ""]]}, {"id": "2106.05204", "submitter": "Yili Hong", "authors": "Khaled F. Bedair and Yili Hong and Hussein R. Al-Khalidi", "title": "Copula-Frailty Models for Recurrent Event Data Based on Monte Carlo EM\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-type recurrent events are often encountered in medical applications\nwhen two or more different event types could repeatedly occur over an\nobservation period. For example, patients may experience recurrences of\nmulti-type nonmelanoma skin cancers in a clinical trial for skin cancer\nprevention. The aims in those applications are to characterize features of the\nmarginal processes, evaluate covariate effects, and quantify both the\nwithin-subject recurrence dependence and the dependence among different event\ntypes. We use copula-frailty models to analyze correlated recurrent events of\ndifferent types. Parameter estimation and inference are carried out by using a\nMonte Carlo expectation-maximization (MCEM) algorithm, which can handle a\nrelatively large (i.e., three or more) number of event types. Performances of\nthe proposed methods are evaluated via extensive simulation studies. The\ndeveloped methods are used to model the recurrences of skin cancer with\ndifferent types.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:46:35 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Bedair", "Khaled F.", ""], ["Hong", "Yili", ""], ["Al-Khalidi", "Hussein R.", ""]]}, {"id": "2106.05219", "submitter": "Zhendong Huang", "authors": "Zhendong Huang and Davide Ferrari", "title": "Fast construction of optimal composite likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A composite likelihood is a combination of low-dimensional likelihood objects\nuseful in applications where the data have complex structure. Although\ncomposite likelihood construction is a crucial aspect influencing both\ncomputing and statistical properties of the resulting estimator, currently\nthere does not seem to exist a universal rule to combine low-dimensional\nlikelihood objects that is statistically justified and fast in execution. This\npaper develops a methodology to select and combine the most informative\nlow-dimensional likelihoods from a large set of candidates while carrying out\nparameter estimation. The new procedure minimizes the distance between\ncomposite likelihood and full likelihood scores subject to a constraint\nrepresenting the afforded computing cost. The selected composite likelihood is\nsparse in the sense that it contains a relatively small number of informative\nsub-likelihoods while the noisy terms are dropped. The resulting estimator is\nfound to have asymptotic variance close to that of the minimum-variance\nestimator constructed using all the low-dimensional likelihoods.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:09:17 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Huang", "Zhendong", ""], ["Ferrari", "Davide", ""]]}, {"id": "2106.05241", "submitter": "Fabian Falck", "authors": "Fabian Falck, Haoting Zhang, Matthew Willetts, George Nicholson,\n  Christopher Yau, Christopher C Holmes", "title": "Multi-Facet Clustering Variational Autoencoders", "comments": "main text: 15 pages, appendices: 33 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Work in deep clustering focuses on finding a single partition of data.\nHowever, high-dimensional data, such as images, typically feature multiple\ninteresting characteristics one could cluster over. For example, images of\nobjects against a background could be clustered over the shape of the object\nand separately by the colour of the background. In this paper, we introduce\nMulti-Facet Clustering Variational Autoencoders (MFCVAE), a novel class of\nvariational autoencoders with a hierarchy of latent variables, each with a\nMixture-of-Gaussians prior, that learns multiple clusterings simultaneously,\nand is trained fully unsupervised and end-to-end. MFCVAE uses a\nprogressively-trained ladder architecture which leads to highly stable\nperformance. We provide novel theoretical results for optimising the ELBO\nanalytically with respect to the categorical variational posterior\ndistribution, and corrects earlier influential theoretical work. On image\nbenchmarks, we demonstrate that our approach separates out and clusters over\ndifferent aspects of the data in a disentangled manner. We also show other\nadvantages of our model: the compositionality of its latent space and that it\nprovides controlled generation of samples.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 17:36:38 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Falck", "Fabian", ""], ["Zhang", "Haoting", ""], ["Willetts", "Matthew", ""], ["Nicholson", "George", ""], ["Yau", "Christopher", ""], ["Holmes", "Christopher C", ""]]}, {"id": "2106.05403", "submitter": "Richard Warr", "authors": "Richard L. Warr, David B. Dahl, Jeremy M. Meyer, Arthur Lui", "title": "The Attraction Indian Buffet Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose the attraction Indian buffet distribution (AIBD), a distribution\nfor binary feature matrices influenced by pairwise similarity information.\nBinary feature matrices are used in Bayesian models to uncover latent variables\n(i.e., features) that explain observed data. The Indian buffet process (IBP) is\na popular exchangeable prior distribution for latent feature matrices. In the\npresence of additional information, however, the exchangeability assumption is\nnot reasonable or desirable. The AIBD can incorporate pairwise similarity\ninformation, yet it preserves many properties of the IBP, including the\ndistribution of the total number of features. Thus, much of the interpretation\nand intuition that one has for the IBP directly carries over to the AIBD. A\ntemperature parameter controls the degree to which the similarity information\naffects feature-sharing between observations. Unlike other nonexchangeable\ndistributions for feature allocations, the probability mass function of the\nAIBD has a tractable normalizing constant, making posterior inference on\nhyperparameters straight-forward using standard MCMC methods. A novel posterior\nsampling algorithm is proposed for the IBP and the AIBD. We demonstrate the\nfeasibility of the AIBD as a prior distribution in feature allocation models\nand compare the performance of competing methods in simulations and an\napplication.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 21:39:34 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 16:43:16 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Warr", "Richard L.", ""], ["Dahl", "David B.", ""], ["Meyer", "Jeremy M.", ""], ["Lui", "Arthur", ""]]}, {"id": "2106.05454", "submitter": "Wencan Zhu", "authors": "Wencan Zhu, Eric Adjakossa, C\\'eline L\\'evy-Leduc and Nils Tern\\`es", "title": "Sign Consistency of the Generalized Elastic Net Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel variable selection approach in the\nframework of high-dimensional linear models where the columns of the design\nmatrix are highly correlated. It consists in rewriting the initial\nhigh-dimensional linear model to remove the correlation between the columns of\nthe design matrix and in applying a generalized Elastic Net criterion since it\ncan be seen as an extension of the generalized Lasso. The properties of our\napproach called gEN (generalized Elastic Net) are investigated both from a\ntheoretical and a numerical point of view. More precisely, we provide a new\ncondition called GIC (Generalized Irrepresentable Condition) which generalizes\nthe EIC (Elastic Net Irrepresentable Condition) of Jia and Yu (2010) under\nwhich we prove that our estimator can recover the positions of the null and non\nnull entries of the coefficients when the sample size tends to infinity. We\nalso assess the performance of our methodology using synthetic data and compare\nit with alternative approaches. Our numerical experiments show that our\napproach improves the variable selection performance in many cases.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 01:52:37 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Zhu", "Wencan", ""], ["Adjakossa", "Eric", ""], ["L\u00e9vy-Leduc", "C\u00e9line", ""], ["Tern\u00e8s", "Nils", ""]]}, {"id": "2106.05588", "submitter": "Jeroen Hoogland", "authors": "J Hoogland, J IntHout, M Belias, MM Rovers, RD Riley, FE Harrell Jr,\n  KGM Moons, TPA Debray, JB Reitsma", "title": "A tutorial on individualized treatment effect prediction from randomized\n  trials with a binary endpoint", "comments": "27 pages, 5 figures, 1 ancillary file", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized trials typically estimate average relative treatment effects, but\ndecisions on the benefit of a treatment are possibly better informed by more\nindividualized predictions of the absolute treatment effect. In case of a\nbinary outcome, these predictions of absolute individualized treatment effect\nrequire knowledge of the individual's risk without treatment and incorporation\nof a possibly differential treatment effect (i.e. varying with patient\ncharacteristics). In this paper we lay out the causal structure of\nindividualized treatment effect in terms of potential outcomes and describe the\nrequired assumptions that underlie a causal interpretation of its prediction.\nSubsequently, we describe regression models and model estimation techniques\nthat can be used to move from average to more individualized treatment effect\npredictions. We focus mainly on logistic regression-based methods that are both\nwell-known and naturally provide the required probabilistic estimates. We\nincorporate key components from both causal inference and prediction research\nto arrive at individualized treatment effect predictions. While the separate\ncomponents are well known, their successful amalgamation is very much an\nongoing field of research. We cut the problem down to its essentials in the\nsetting of a randomized trial, discuss the importance of a clear definition of\nthe estimand of interest, provide insight into the required assumptions, and\ngive guidance with respect to modeling and estimation options. Simulated data\nillustrates the potential of different modeling options across scenarios that\nvary both average treatment effect and treatment effect heterogeneity. Two\napplied examples illustrate individualized treatment effect prediction in\nrandomized trial data.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 08:44:27 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Hoogland", "J", ""], ["IntHout", "J", ""], ["Belias", "M", ""], ["Rovers", "MM", ""], ["Riley", "RD", ""], ["Harrell", "FE", "Jr"], ["Moons", "KGM", ""], ["Debray", "TPA", ""], ["Reitsma", "JB", ""]]}, {"id": "2106.05694", "submitter": "David Strieder", "authors": "David Strieder, Tobias Freidling, Stefan Haffner and Mathias Drton", "title": "Confidence in Causal Discovery with Linear Causal Models", "comments": "Accepted for the 37th conference on Uncertainty in Artificial\n  Intelligence (UAI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural causal models postulate noisy functional relations among a set of\ninteracting variables. The causal structure underlying each such model is\nnaturally represented by a directed graph whose edges indicate for each\nvariable which other variables it causally depends upon. Under a number of\ndifferent model assumptions, it has been shown that this causal graph and, thus\nalso, causal effects are identifiable from mere observational data. For these\nmodels, practical algorithms have been devised to learn the graph. Moreover,\nwhen the graph is known, standard techniques may be used to give estimates and\nconfidence intervals for causal effects. We argue, however, that a two-step\nmethod that first learns a graph and then treats the graph as known yields\nconfidence intervals that are overly optimistic and can drastically fail to\naccount for the uncertain causal structure. To address this issue we lay out a\nframework based on test inversion that allows us to give confidence regions for\ntotal causal effects that capture both sources of uncertainty: causal structure\nand numerical size of nonzero effects. Our ideas are developed in the context\nof bivariate linear causal models with homoscedastic errors, but as we\nexemplify they are generalizable to larger systems as well as other settings\nsuch as, in particular, linear non-Gaussian models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 12:27:41 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Strieder", "David", ""], ["Freidling", "Tobias", ""], ["Haffner", "Stefan", ""], ["Drton", "Mathias", ""]]}, {"id": "2106.05797", "submitter": "Mike Li", "authors": "Paul Glasserman, Mike Li", "title": "Linear Classifiers Under Infinite Imbalance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-fin.RM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study the behavior of linear discriminant functions for binary\nclassification in the infinite-imbalance limit, where the sample size of one\nclass grows without bound while the sample size of the other remains fixed. The\ncoefficients of the classifier minimize an expected loss specified through a\nweight function. We show that for a broad class of weight functions, the\nintercept diverges but the rest of the coefficient vector has a finite limit\nunder infinite imbalance, extending prior work on logistic regression. The\nlimit depends on the left tail of the weight function, for which we distinguish\nthree cases: bounded, asymptotically polynomial, and asymptotically\nexponential. The limiting coefficient vectors reflect robustness or\nconservatism properties in the sense that they optimize against certain\nworst-case alternatives. In the bounded and polynomial cases, the limit is\nequivalent to an implicit choice of upsampling distribution for the minority\nclass. We apply these ideas in a credit risk setting, with particular emphasis\non performance in the high-sensitivity and high-specificity regions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:01:54 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Glasserman", "Paul", ""], ["Li", "Mike", ""]]}, {"id": "2106.05820", "submitter": "Murray Pollock", "authors": "Paul A. Jenkins, Murray Pollock and Gareth O. Roberts", "title": "Bayesian semi-parametric inference for diffusion processes using splines", "comments": "21 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a semi-parametric method to simultaneously infer both the drift\nand volatility functions of a discretely observed scalar diffusion. We\nintroduce spline bases to represent these functions and develop a Markov chain\nMonte Carlo algorithm to infer, a posteriori, the coefficients of these\nfunctions in the spline basis. A key innovation is that we use spline bases to\nmodel transformed versions of the drift and volatility functions rather than\nthe functions themselves. The output of the algorithm is a posterior sample of\nplausible drift and volatility functions that are not constrained to any\nparticular parametric family. The flexibility of this approach provides\npractitioners a powerful investigative tool, allowing them to posit parametric\nmodels to better capture the underlying dynamics of their processes of\ninterest. We illustrate the versatility of our method by applying it to\nchallenging datasets from finance, paleoclimatology, and astrophysics. In view\nof the parametric diffusion models widely employed in the literature for those\nexamples, some of our results are surprising since they call into question some\naspects of these models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:37:38 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Jenkins", "Paul A.", ""], ["Pollock", "Murray", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "2106.05824", "submitter": "Bruno Sudret", "authors": "P.-R. Wagner, S. Marelli, I. Papaioannou, D. Straub, B. Sudret", "title": "Rare event estimation using stochastic spectral embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": "RSUQ-2021-003", "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the probability of rare failure events is an essential step in the\nreliability assessment of engineering systems. Computing this failure\nprobability for complex non-linear systems is challenging, and has recently\nspurred the development of active-learning reliability methods. These methods\napproximate the limit-state function (LSF) using surrogate models trained with\na sequentially enriched set of model evaluations. A recently proposed method\ncalled stochastic spectral embedding (SSE) aims to improve the local\napproximation accuracy of global, spectral surrogate modelling techniques by\nsequentially embedding local residual expansions in subdomains of the input\nspace. In this work we apply SSE to the LSF, giving rise to a stochastic\nspectral embedding-based reliability (SSER) method. The resulting partition of\nthe input space decomposes the failure probability into a set of\neasy-to-compute domain-wise failure probabilities. We propose a set of\nmodifications that tailor the algorithm to efficiently solve rare event\nestimation problems. These modifications include specialized refinement domain\nselection, partitioning and enrichment strategies. We showcase the algorithm\nperformance on four benchmark problems of various dimensionality and complexity\nin the LSF.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 16:10:33 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wagner", "P. -R.", ""], ["Marelli", "S.", ""], ["Papaioannou", "I.", ""], ["Straub", "D.", ""], ["Sudret", "B.", ""]]}, {"id": "2106.05828", "submitter": "Housen Li", "authors": "Markus Haltmeier and Housen Li and Axel Munk", "title": "A Variational View on Statistical Multiscale Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a unifying view on various statistical estimation techniques\nincluding penalization, variational and thresholding methods. These estimators\nwill be analyzed in the context of statistical linear inverse problems\nincluding nonparametric and change point regression, and high dimensional\nlinear models as examples. Our approach reveals many seemingly unrelated\nestimation schemes as special instances of a general class of variational\nmultiscale estimators, named MIND (MultIscale Nemirovskii--Dantzig). These\nestimators result from minimizing certain regularization functionals under\nconvex constraints that can be seen as multiple statistical tests for local\nhypotheses.\n  For computational purposes, we recast MIND in terms of simpler unconstraint\noptimization problems via Lagrangian penalization as well as Fenchel duality.\nPerformance of several MINDs is demonstrated on numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 15:47:16 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Haltmeier", "Markus", ""], ["Li", "Housen", ""], ["Munk", "Axel", ""]]}, {"id": "2106.05833", "submitter": "Luc Pronzato", "authors": "Amaya Nogales G\\'omez, Luc Pronzato, Maria-Jo\\~ao Rendas", "title": "Incremental space-filling design based on coverings and spacings:\n  improving upon low discrepancy sequences", "comments": "28 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper addresses the problem of defining families of ordered sequences\n$\\{x_i\\}_{i\\in N}$ of elements of a compact subset $X$ of $R^d$ whose prefixes\n$X_n=\\{x_i\\}_{i=1}^{n}$, for all orders $n$, have good space-filling properties\nas measured by the dispersion (covering radius) criterion. Our ultimate aim is\nthe definition of incremental algorithms that generate sequences $X_n$ with\nsmall optimality gap, i.e., with a small increase in the maximum distance\nbetween points of $X$ and the elements of $X_n$ with respect to the optimal\nsolution $X_n^\\star$. The paper is a first step in this direction, presenting\nincremental design algorithms with proven optimality bound for one-parameter\nfamilies of criteria based on coverings and spacings that both converge to\ndispersion for large values of their parameter. The examples presented show\nthat the covering-based method outperforms state-of-the-art competitors,\nincluding coffee-house, suggesting that it inherits from its guaranteed 50\\%\noptimality gap.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 09:30:40 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["G\u00f3mez", "Amaya Nogales", ""], ["Pronzato", "Luc", ""], ["Rendas", "Maria-Jo\u00e3o", ""]]}, {"id": "2106.05834", "submitter": "Christophe Geissler", "authors": "Olivier Sorba, C Geissler", "title": "Online Bayesian inference for multiple changepoints and risk assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of the present study is to detect abrupt trend changes in the mean of\na multidimensional sequential signal. Directly inspired by papers of Fernhead\nand Liu ([4] and [5]), this work describes the signal in a hierarchical manner\n: the change dates of a time segmentation process trigger the renewal of a\npiece-wise constant emission law. Bayesian posterior information on the change\ndates and emission parameters is obtained. These estimations can be revised\nonline, i.e. as new data arrive. This paper proposes explicit formulations\ncorresponding to various emission laws, as well as a generalization to the case\nwhere only partially observed data are available. Practical applications\ninclude the returns of partially observed multi-asset investment strategies,\nwhen only scant prior knowledge of the movers of the returns is at hand,\nlimited to some statistical assumptions. This situation is different from the\nstudy of trend changes in the returns of individual assets, where fundamental\nexogenous information (news, earnings announcements, controversies, etc.) can\nbe used.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:09:09 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Sorba", "Olivier", ""], ["Geissler", "C", ""]]}, {"id": "2106.05838", "submitter": "Jingyi Zhang", "authors": "Cheng Meng, Yuan Ke, Jingyi Zhang, Mengrui Zhang, Wenxuan Zhong, Ping\n  Ma", "title": "Large-scale optimal transport map estimation using projection pursuit", "comments": null, "journal-ref": "Meng, C. \"Large-scale optimal transport map estimation using\n  projection pursuit.\" NeurIPS 2019 (2019); Ke, Y. \"Large-scale optimal\n  transport map estimation using projection pursuit.\" NeurIPS 2019 (2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the estimation of large-scale optimal transport maps\n(OTM), which is a well-known challenging problem owing to the curse of\ndimensionality. Existing literature approximates the large-scale OTM by a\nseries of one-dimensional OTM problems through iterative random projection.\nSuch methods, however, suffer from slow or none convergence in practice due to\nthe nature of randomly selected projection directions. Instead, we propose an\nestimation method of large-scale OTM by combining the idea of projection\npursuit regression and sufficient dimension reduction. The proposed method,\nnamed projection pursuit Monge map (PPMM), adaptively selects the most\n``informative'' projection direction in each iteration. We theoretically show\nthe proposed dimension reduction method can consistently estimate the most\n``informative'' projection direction in each iteration. Furthermore, the PPMM\nalgorithm weakly convergences to the target large-scale OTM in a reasonable\nnumber of steps. Empirically, PPMM is computationally easy and converges fast.\nWe assess its finite sample performance through the applications of Wasserstein\ndistance estimation and generative models.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 03:53:41 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Meng", "Cheng", ""], ["Ke", "Yuan", ""], ["Zhang", "Jingyi", ""], ["Zhang", "Mengrui", ""], ["Zhong", "Wenxuan", ""], ["Ma", "Ping", ""]]}, {"id": "2106.05840", "submitter": "Mian Adnan", "authors": "Mian Arif Shams Adnan, H. M. Miraz Mahmud", "title": "A Bagging and Boosting Based Convexly Combined Optimum Mixture\n  Probabilistic Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.AP stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Unlike previous studies on mixture distributions, a bagging and boosting\nbased convexly combined mixture probabilistic model has been suggested. This\nmodel is a result of iteratively searching for obtaining the optimum\nprobabilistic model that provides the maximum p value.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2021 04:20:00 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Adnan", "Mian Arif Shams", ""], ["Mahmud", "H. M. Miraz", ""]]}, {"id": "2106.05850", "submitter": "Jiayi Wang", "authors": "Jiayi Wang, Raymond K. W. Wong, Xiaojun Mao, Kwun Chuen Gary Chan", "title": "Matrix Completion with Model-free Weighting", "comments": "Proceedings of the 38th International Conference on Machine Learning,\n  PMLR 139, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel method for matrix completion under general\nnon-uniform missing structures. By controlling an upper bound of a novel\nbalancing error, we construct weights that can actively adjust for the\nnon-uniformity in the empirical risk without explicitly modeling the\nobservation probabilities, and can be computed efficiently via convex\noptimization. The recovered matrix based on the proposed weighted empirical\nrisk enjoys appealing theoretical guarantees. In particular, the proposed\nmethod achieves a stronger guarantee than existing work in terms of the scaling\nwith respect to the observation probabilities, under asymptotically\nheterogeneous missing settings (where entry-wise observation probabilities can\nbe of different orders). These settings can be regarded as a better theoretical\nmodel of missing patterns with highly varying probabilities. We also provide a\nnew minimax lower bound under a class of heterogeneous settings. Numerical\nexperiments are also provided to demonstrate the effectiveness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2021 06:28:20 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wang", "Jiayi", ""], ["Wong", "Raymond K. W.", ""], ["Mao", "Xiaojun", ""], ["Chan", "Kwun Chuen Gary", ""]]}, {"id": "2106.05859", "submitter": "Justin Wong", "authors": "Justin Wong and Dominik Damjakob", "title": "A Meta Learning Approach to Discerning Causal Graph Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We explore the usage of meta-learning to derive the causal direction between\nvariables by optimizing over a measure of distribution simplicity. We\nincorporate a stochastic graph representation which includes latent variables\nand allows for more generalizability and graph structure expression. Our model\nis able to learn causal direction indicators for complex graph structures\ndespite effects of latent confounders. Further, we explore robustness of our\nmethod with respect to violations of our distributional assumptions and data\nscarcity. Our model is particularly robust to modest data scarcity, but is less\nrobust to distributional changes. By interpreting the model predictions as\nstochastic events, we propose a simple ensemble method classifier to reduce the\noutcome variability as an average of biased events. This methodology\ndemonstrates ability to infer the existence as well as the direction of a\ncausal relationship between data distributions.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2021 22:44:44 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Wong", "Justin", ""], ["Damjakob", "Dominik", ""]]}, {"id": "2106.05906", "submitter": "Daniel Phillips", "authors": "M. A. Connell, I. Billig, D. R. Phillips", "title": "Does Bayesian Model Averaging improve polynomial extrapolations? Two toy\n  problems as tests", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME nucl-th physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We assess the accuracy of Bayesian polynomial extrapolations from small\nparameter values, x, to large values of x. We consider a set of polynomials of\nfixed order, intended as a proxy for a fixed-order effective field theory (EFT)\ndescription of data. We employ Bayesian Model Averaging (BMA) to combine\nresults from different order polynomials (EFT orders). Our study considers two\n\"toy problems\" where the underlying function used to generate data sets is\nknown. We use Bayesian parameter estimation to extract the polynomial\ncoefficients that describe these data at low x. A \"naturalness\" prior is\nimposed on the coefficients, so that they are O(1). We Bayesian-Model-Average\ndifferent polynomial degrees by weighting each according to its Bayesian\nevidence and compare the predictive performance of this Bayesian Model Average\nwith that of the individual polynomials. The credibility intervals on the BMA\nforecast have the stated coverage properties more consistently than does the\nhighest evidence polynomial, though BMA does not necessarily outperform every\npolynomial.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 16:47:24 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Connell", "M. A.", ""], ["Billig", "I.", ""], ["Phillips", "D. R.", ""]]}, {"id": "2106.06137", "submitter": "Edwin Fong", "authors": "Edwin Fong, Chris Holmes", "title": "Conformal Bayesian Computation", "comments": "19 pages, 4 figures, 12 tables; added references and fixed typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop scalable methods for producing conformal Bayesian predictive\nintervals with finite sample calibration guarantees. Bayesian posterior\npredictive distributions, $p(y \\mid x)$, characterize subjective beliefs on\noutcomes of interest, $y$, conditional on predictors, $x$. Bayesian prediction\nis well-calibrated when the model is true, but the predictive intervals may\nexhibit poor empirical coverage when the model is misspecified, under the so\ncalled ${\\cal{M}}$-open perspective. In contrast, conformal inference provides\nfinite sample frequentist guarantees on predictive confidence intervals without\nthe requirement of model fidelity. Using 'add-one-in' importance sampling, we\nshow that conformal Bayesian predictive intervals are efficiently obtained from\nre-weighted posterior samples of model parameters. Our approach contrasts with\nexisting conformal methods that require expensive refitting of models or\ndata-splitting to achieve computational efficiency. We demonstrate the utility\non a range of examples including extensions to partially exchangeable settings\nsuch as hierarchical models.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 02:49:12 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 12:16:04 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Fong", "Edwin", ""], ["Holmes", "Chris", ""]]}, {"id": "2106.06300", "submitter": "Maxime Vono", "authors": "Vincent Plassier, Maxime Vono, Alain Durmus and Eric Moulines", "title": "DG-LMC: A Turn-key and Scalable Synchronous Distributed MCMC Algorithm\n  via Langevin Monte Carlo within Gibbs", "comments": "77 pages. Accepted for publication at ICML 2021, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Performing reliable Bayesian inference on a big data scale is becoming a\nkeystone in the modern era of machine learning. A workhorse class of methods to\nachieve this task are Markov chain Monte Carlo (MCMC) algorithms and their\ndesign to handle distributed datasets has been the subject of many works.\nHowever, existing methods are not completely either reliable or computationally\nefficient. In this paper, we propose to fill this gap in the case where the\ndataset is partitioned and stored on computing nodes within a cluster under a\nmaster/slaves architecture. We derive a user-friendly centralised distributed\nMCMC algorithm with provable scaling in high-dimensional settings. We\nillustrate the relevance of the proposed methodology on both synthetic and real\ndata experiments.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 10:37:14 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 14:21:51 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Plassier", "Vincent", ""], ["Vono", "Maxime", ""], ["Durmus", "Alain", ""], ["Moulines", "Eric", ""]]}, {"id": "2106.06316", "submitter": "Anders Huitfeldt", "authors": "Anders Huitfeldt, Matthew P. Fox, Rhian M. Daniel, Asbj{\\o}rn\n  Hr\\'objartsson, Eleanor J. Murray", "title": "Shall we count the living or the dead?", "comments": "Minor update (added references and discussion of earlier work which\n  was previously unknown to us)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the 1958 paper \"Shall we count the living or the dead\", Mindel C. Sheps\nproposed a principled solution to the familiar problem of asymmetry of the\nrelative risk. We provide causal models to clarify the scope and limitations of\nSheps' line of reasoning, and show that her preferred variant of the relative\nrisk will be stable between patient groups under certain biologically\ninterpretable conditions. Such stability is useful when findings from an\nintervention study must be generalized to support clinical decisions in\npatients whose risk profile differs from the participants in the study. We show\nthat Sheps' approach is consistent with a substantial body of psychological and\nphilosophical research on how human reasoners carry causal information from one\ncontext to another.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 11:21:19 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 07:51:04 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Huitfeldt", "Anders", ""], ["Fox", "Matthew P.", ""], ["Daniel", "Rhian M.", ""], ["Hr\u00f3bjartsson", "Asbj\u00f8rn", ""], ["Murray", "Eleanor J.", ""]]}, {"id": "2106.06348", "submitter": "Saint-Clair Chabert-Liddell", "authors": "Saint-Clair Chabert-Liddell, Pierre Barbillon, Sophie Donnet", "title": "Impact of the mesoscale structure of a bipartite ecological interaction\n  network on its robustness through a probabilistic modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The robustness of an ecological network quantifies the resilience of the\necosystem it represents to species loss. It corresponds to the proportion of\nspecies that are disconnected from the rest of the network when extinctions\noccur sequentially. Classically, the robustness is calculated for a given\nnetwork, from the simulation of a large number of extinction sequences. The\nlink between network structure and robustness remains an open question. Setting\na joint probabilistic model on the network and the extinction sequences allows\nto analyze this relation.\n  Bipartite stochastic block models have proven their ability to model\nbipartite networks e.g. plant-pollinator networks: species are divided into\nblocks and interaction probabilities are determined by the blocks of\nmembership. Analytical expressions of the expectation and variance of\nrobustness are obtained under this model, for different distributions of\nprimary extinction sequences. The impact of the network structure on the\nrobustness is analyzed through a set of properties and numerical illustrations.\nThe analysis of a collection of bipartite ecological networks allows us to\ncompare the empirical approach to our probabilistic approach, and illustrates\nthe relevance of the latter when it comes to computing the robustness of a\npartially observed or incompletely sampled network.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 12:42:21 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Chabert-Liddell", "Saint-Clair", ""], ["Barbillon", "Pierre", ""], ["Donnet", "Sophie", ""]]}, {"id": "2106.06368", "submitter": "Sreedevi E P", "authors": "Sudheesh K. Kattumannil and Sreedevi E.P", "title": "A new goodness of fit test for uniform distribution with censored\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using fixed point characterization, we develop a new goodness of fit test for\nuniform distribution. We also discuss how the right censored observations can\nbe incorporated in the proposed test procedure. We study the asymptotic\nproperties of the proposed test statistics. A Monte Carlo simulation is carried\nout to evaluate the finite sample performance of the tests. We illustrate the\ntest procedures using real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 13:16:02 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Kattumannil", "Sudheesh K.", ""], ["P", "Sreedevi E.", ""]]}, {"id": "2106.06375", "submitter": "Kisung You", "authors": "Kisung You", "title": "Parameter Estimation and Model-Based Clustering with Spherical Normal\n  Distribution on the Unit Hypersphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In directional statistics, the von Mises-Fisher (vMF) distribution is one of\nthe most basic and popular probability distributions for data on the unit\nhypersphere. Recently, the spherical normal (SN) distribution was proposed as\nan intrinsic counterpart to the vMF distribution by replacing the standard\nEuclidean norm with the great-circle distance, which is the shortest path\njoining two points on the unit sphere. We propose numerical approaches for\nparameter estimation since there are no analytic formula available. We consider\nthe estimation problems in a general setting where non-negative weights are\nassigned to observations. This leads to a more interesting contribution for\nmodel-based clustering on the unit hypersphere by finite mixture model with SN\ndistributions. We validate efficiency of optimization-based estimation\nprocedures and effectiveness of SN mixture model using simulated and real data\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 13:28:11 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["You", "Kisung", ""]]}, {"id": "2106.06515", "submitter": "Zhiyuan Lin", "authors": "Zhiyuan Lin, Hao Sheng, Sharad Goel", "title": "Probability Paths and the Structure of Predictions over Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In settings ranging from weather forecasts to political prognostications to\nfinancial projections, probability estimates of future binary outcomes often\nevolve over time. For example, the estimated likelihood of rain on a specific\nday changes by the hour as new information becomes available. Given a\ncollection of such probability paths, we introduce a Bayesian framework --\nwhich we call the Gaussian latent information martingale, or GLIM -- for\nmodeling the structure of dynamic predictions over time. Suppose, for example,\nthat the likelihood of rain in a week is 50%, and consider two hypothetical\nscenarios. In the first, one expects the forecast is equally likely to become\neither 25% or 75% tomorrow; in the second, one expects the forecast to stay\nconstant for the next several days. A time-sensitive decision-maker might\nselect a course of action immediately in the latter scenario, but may postpone\ntheir decision in the former, knowing that new information is imminent. We\nmodel these trajectories by assuming predictions update according to a latent\nprocess of information flow, which is inferred from historical data. In\ncontrast to general methods for time series analysis, this approach preserves\nthe martingale structure of probability paths and better quantifies future\nuncertainties around probability paths. We show that GLIM outperforms three\npopular baseline methods, producing better estimated posterior probability path\ndistributions measured by three different metrics. By elucidating the dynamic\nstructure of predictions over time, we hope to help individuals make more\ninformed choices.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 17:18:05 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Lin", "Zhiyuan", ""], ["Sheng", "Hao", ""], ["Goel", "Sharad", ""]]}, {"id": "2106.06568", "submitter": "Adam Loy", "authors": "Adam Loy and Jenna Korobova", "title": "Bootstrapping Clustered Data in R using lmeresampler", "comments": "15 pages, 3 figures, 2 tables,", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear mixed-effects models are commonly used to analyze clustered data\nstructures. There are numerous packages to fit these models in R and conduct\nlikelihood-based inference. The implementation of resampling-based procedures\nfor inference are more limited. In this paper, we introduce the lmeresampler\npackage for bootstrapping nested linear mixed-effects models fit via lme4 or\nnlme. Bootstrap estimation allows for bias correction, adjusted standard errors\nand confidence intervals for small samples sizes and when distributional\nassumptions break down. We will also illustrate how bootstrap resampling can be\nused to diagnose this model class. In addition, lmeresampler makes it easy to\nconstruct interval estimates of functions of model parameters.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 18:39:09 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Loy", "Adam", ""], ["Korobova", "Jenna", ""]]}, {"id": "2106.06602", "submitter": "Ted Westling", "authors": "Ted Westling, Alex Luedtke, Peter Gilbert, and Marco Carone", "title": "Inference for treatment-specific survival curves using machine learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the absence of data from a randomized trial, researchers often aim to use\nobservational data to draw causal inference about the effect of a treatment on\na time-to-event outcome. In this context, interest often focuses on the\ntreatment-specific survival curves; that is, the survival curves were the\nentire population under study to be assigned to receive the treatment or not.\nUnder certain causal conditions, including that all confounders of the\ntreatment-outcome relationship are observed, the treatment-specific survival\ncan be identified with a covariate-adjusted survival function. Several\nestimators of this function have been proposed, including estimators based on\noutcome regression, inverse probability weighting, and doubly robust\nestimators. In this article, we propose a new cross-fitted doubly-robust\nestimator that incorporates data-adaptive (e.g. machine learning) estimators of\nthe conditional survival functions. We establish conditions on the nuisance\nestimators under which our estimator is consistent and asymptotically linear,\nboth pointwise and uniformly in time. We also propose a novel ensemble learner\nfor combining multiple candidate estimators of the conditional survival\nestimators. Notably, our methods and results accommodate events occurring in\ndiscrete or continuous time (or both). We investigate the practical performance\nof our methods using numerical studies and an application to the effect of a\nsurgical treatment to prevent metastases of parotid carcinoma on mortality.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 20:34:04 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Westling", "Ted", ""], ["Luedtke", "Alex", ""], ["Gilbert", "Peter", ""], ["Carone", "Marco", ""]]}, {"id": "2106.06608", "submitter": "Nhat Ho", "authors": "Nhat Ho, Stephen G. Walker", "title": "Statistical Analysis from the Fourier Integral Theorem", "comments": "20 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Taking the Fourier integral theorem as our starting point, in this paper we\nfocus on natural Monte Carlo and fully nonparametric estimators of multivariate\ndistributions and conditional distribution functions. We do this without the\nneed for any estimated covariance matrix or dependence structure between\nvariables. These aspects arise immediately from the integral theorem. Being\nable to model multivariate data sets using conditional distribution functions\nwe can study a number of problems, such as prediction for Markov processes,\nestimation of mixing distribution functions which depend on covariates, and\ngeneral multivariate data. Estimators are explicit Monte Carlo based and\nrequire no recursive or iterative algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2021 20:44:54 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ho", "Nhat", ""], ["Walker", "Stephen G.", ""]]}, {"id": "2106.06669", "submitter": "Daniel Spencer", "authors": "Daniel Spencer, Yu (Ryan) Yue, David Bolin, Sarah Ryan, Amanda F.\n  Mejia", "title": "Spatial Bayesian GLM on the cortical surface produces reliable task\n  activations in individuals and groups", "comments": "37 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The general linear model (GLM) is a popular and convenient tool for\nestimating the functional brain response and identifying areas of significant\nactivation during a task or stimulus. However, the classical GLM is based on a\nmassive univariate approach that does not leverage the similarity of activation\npatterns among neighboring brain locations. As a result, it tends to produce\nnoisy estimates and be underpowered to detect significant activations,\nparticularly in individual subjects and small groups. The recently proposed\ncortical surface-based spatial Bayesian GLM leverages spatial dependencies\namong neighboring cortical vertices to produce smoother and more accurate\nestimates and areas of functional activation. The spatial Bayesian GLM can be\napplied to individual and group-level analysis. In this study, we assess the\nreliability and power of individual and group-average measures of task\nactivation produced via the surface-based spatial Bayesian GLM. We analyze\nmotor task data from 45 subjects in the Human Connectome Project (HCP) and HCP\nRetest datasets. We also extend the model to multi-session analysis and employ\nsubject-specific cortical surfaces rather than surfaces inflated to a sphere\nfor more accurate distance-based modeling. Results show that the surface-based\nspatial Bayesian GLM produces highly reliable activations in individual\nsubjects and is powerful enough to detect trait-like functional topologies.\nAdditionally, spatial Bayesian modeling enhances reliability of group-level\nanalysis even in moderately sized samples (n=45). Notably, the power of the\nspatial Bayesian GLM to detect activations above a scientifically meaningful\neffect size is nearly invariant to sample size, exhibiting high power even in\nsmall samples (n=10). The spatial Bayesian GLM is computationally efficient in\nindividuals and groups and is convenient to use with the BayesfMRI R package.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 02:42:51 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 02:06:54 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Spencer", "Daniel", "", "Ryan"], ["Yu", "", "", "Ryan"], ["Yue", "", ""], ["Bolin", "David", ""], ["Ryan", "Sarah", ""], ["Mejia", "Amanda F.", ""]]}, {"id": "2106.06787", "submitter": "Hwanwoo Kim", "authors": "John Harlim, Shixiao Jiang, Hwanwoo Kim, Daniel Sanz-Alonso", "title": "Graph-based Prior and Forward Models for Inverse Problems on Manifolds\n  with Boundaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops manifold learning techniques for the numerical solution\nof PDE-constrained Bayesian inverse problems on manifolds with boundaries. We\nintroduce graphical Mat\\'ern-type Gaussian field priors that enable flexible\nmodeling near the boundaries, representing boundary values by superposition of\nharmonic functions with appropriate Dirichlet boundary conditions. We also\ninvestigate the graph-based approximation of forward models from PDE parameters\nto observed quantities. In the construction of graph-based prior and forward\nmodels, we leverage the ghost point diffusion map algorithm to approximate\nsecond-order elliptic operators with classical boundary conditions. Numerical\nresults validate our graph-based approach and demonstrate the need to design\nprior covariance models that account for boundary conditions.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 14:41:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Harlim", "John", ""], ["Jiang", "Shixiao", ""], ["Kim", "Hwanwoo", ""], ["Sanz-Alonso", "Daniel", ""]]}, {"id": "2106.06835", "submitter": "Tian Gu", "authors": "Tian Gu, Jeremy M.G. Taylor and Bhramar Mukherjee", "title": "Regression inference for multiple populations by integrating\n  summary-level data using stacked imputations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There is a growing need for flexible general frameworks that integrate\nindividual-level data with external summary information for improved\nstatistical inference. This paper proposes an imputation-based methodology\nwhere the goal is to fit an outcome regression model with all available\nvariables in the internal study while utilizing summary information from\nexternal models that may have used only a subset of the predictors. The method\nallows for heterogeneity of covariate effects across the external populations.\nThe proposed approach generates synthetic outcome data in each population, uses\nstacked multiple imputation to create a long dataset with complete covariate\ninformation, and finally analyzes the imputed data with weighted regression.\nThis flexible and unified approach attains the following four objectives: (i)\nincorporating supplementary information from a broad class of externally fitted\npredictive models or established risk calculators which could be based on\nparametric regression or machine learning methods, as long as the external\nmodel can generate outcome values given covariates; (ii) improving statistical\nefficiency of the estimated coefficients in the internal study; (iii) improving\npredictions by utilizing even partial information available from models that\nuses a subset of the full set of covariates used in the internal study; and\n(iv) providing valid statistical inference for the external population with\npotentially different covariate effects from the internal population.\nApplications include prostate cancer risk prediction models using novel\nbiomarkers that are measured only in the internal study.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 19:13:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Gu", "Tian", ""], ["Taylor", "Jeremy M. G.", ""], ["Mukherjee", "Bhramar", ""]]}, {"id": "2106.06865", "submitter": "Birbal Prasad", "authors": "Birbal Prasad and Xinzhong Li", "title": "Fused inverse-normal method for integrated differential expression\n  analysis of RNA-seq data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Use of next-generation sequencing technologies to transcriptomics (RNA-seq)\nfor gene expression profiling has found widespread application in studying\ndifferent biological conditions including cancers. However, RNA-seq experiments\nare still small sample size experiments due to the cost. Recently, an increased\nfocus has been on meta-analysis methods for integrated differential expression\nanalysis for exploration of potential biomarkers. In this study, we propose a\np-value combination method for meta-analysis of multiple related RNA-seq\nstudies that accounts for sample size of a study and direction of expression of\ngenes in individual studies. The proposed method generalizes the inverse-normal\nmethod without increase in computational complexity and does not pre- or\npost-hoc filter genes that have conflicting direction of expression in\ndifferent studies. Thus, the proposed method, as compared to the\ninverse-normal, has better potential for the discovery of differentially\nexpressed genes (DEGs) with potentially conflicting differential signals from\nmultiple studies related to disease. We demonstrated the use of the proposed\nmethod in detection of biologically relevant DEGs in glioblastoma (GBM), the\nmost aggressive brain cancer. Our approach notably enabled the identification\nof over-expression in GBM compared to healthy controls of the oncogene RAD51,\nwhich has recently been shown to be a target for inhibition to enhance\nradiosensitivity of GBM cells during treatment. Pathway analysis identified\nmultiple aberrant GBM related pathways as well as novel regulators such as\nTCF7L2 and MAPT as important upstream regulators in GBM. The proposed method\nprovides a way to establish differential expression status for genes with\nconflicting direction of expression in individual RNA-seq studies. Hence,\nleading to further exploration of them as potential biomarkers for the disease.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 21:29:46 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 22:23:15 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Prasad", "Birbal", ""], ["Li", "Xinzhong", ""]]}, {"id": "2106.06877", "submitter": "Aastha Khatiwada", "authors": "Aastha Khatiwada, Bethany J. Wolf, Ayse Selen Yilmaz, Paula S. Ramos,\n  Maciej Pietrzak, Andrew Lawson, Kelly J. Hunt, Hang J. Kim, Dongjun Chung", "title": "GPA-Tree: Statistical Approach for Functional-Annotation-Tree-Guided\n  Prioritization of GWAS Results", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Motivation: In spite of great success of genome-wide association studies\n(GWAS), multiple challenges still remain. First, complex traits are often\nassociated with many single nucleotide polymorphisms (SNPs), each with small or\nmoderate effect sizes. Second, our understanding of the functional mechanisms\nthrough which genetic variants are associated with complex traits is still\nlimited. To address these challenges, we propose GPA-Tree and it simultaneously\nimplements association mapping and identifies key combinations of functional\nannotations related to risk-associated SNPs by combining a decision tree\nalgorithm with a hierarchical modeling framework. Results: First, we\nimplemented simulation studies to evaluate the proposed GPA-Tree method and\ncompared its performance with existing statistical approaches. The results\nindicate that GPA-Tree outperforms existing statistical approaches in detecting\nrisk-associated SNPs and identifying the true combinations of functional\nannotations with high accuracy. Second, we applied GPA-Tree to a systemic lupus\nerythematosus (SLE) GWAS and functional annotation data including GenoSkyline\nand GenoSkylinePlus. The results from GPA-Tree highlight the dysregulation of\nblood immune cells, including but not limited to primary B, memory helper T,\nregulatory T, neutrophils and CD8+ memory T cells in SLE. These results\ndemonstrate that GPA-Tree can be a powerful tool that improves association\nmapping while facilitating understanding of the underlying genetic architecture\nof complex traits and potential mechanisms linking risk-associated SNPs with\ncomplex traits.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2021 22:44:39 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Khatiwada", "Aastha", ""], ["Wolf", "Bethany J.", ""], ["Yilmaz", "Ayse Selen", ""], ["Ramos", "Paula S.", ""], ["Pietrzak", "Maciej", ""], ["Lawson", "Andrew", ""], ["Hunt", "Kelly J.", ""], ["Kim", "Hang J.", ""], ["Chung", "Dongjun", ""]]}, {"id": "2106.06902", "submitter": "Shouto Yonekura", "authors": "Shouto Yonekura and Shonosuke Sugasawa", "title": "Adaptation of the Tuning Parameter in General Bayesian Inference with\n  Robust Divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a methodology for robust Bayesian estimation with robust\ndivergence (e.g., density power divergence or {\\gamma}-divergence), indexed by\na single tuning parameter. It is well known that the posterior density induced\nby robust divergence gives highly robust estimators against outliers if the\ntuning parameter is appropriately and carefully chosen. In a Bayesian\nframework, one way to find the optimal tuning parameter would be using evidence\n(marginal likelihood). However, we numerically illustrate that evidence induced\nby the density power divergence does not work to select the optimal tuning\nparameter since robust divergence is not regarded as a statistical model. To\novercome the problems, we treat the exponential of robust divergence as an\nunnormalized statistical model, and we estimate the tuning parameter via\nminimizing the Hyvarinen score. We also provide adaptive computational methods\nbased on sequential Monte Carlo (SMC) samplers, which enables us to obtain the\noptimal tuning parameter and samples from posterior distributions\nsimultaneously. The empirical performance of the proposed method through\nsimulations and an application to real data are also provided.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 03:25:37 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 17:25:21 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Yonekura", "Shouto", ""], ["Sugasawa", "Shonosuke", ""]]}, {"id": "2106.06918", "submitter": "Victor Patrangenaru", "authors": "Chen Shen, Vic Patrangenaru and Roland Moore", "title": "A Phylogenetic Trees Analysis of SARS-CoV-2", "comments": "22 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One regards spaces of trees as stratified spaces, to study distributions of\nphylogenetic trees. Stratified spaces with may have cycles, however spaces of\ntrees with a fixed number of leafs are contractible. Spaces of trees with three\nleafs, in particular, are spiders with three legs. One gives an elementary\nproof of the stickiness of intrinsic sample means on spiders. One also\nrepresents four leafs tree data in terms of an associated Petersen graph. One\napplies such ideas to analyze RNA sequences of SARS-CoV-2 from multiple\nsources, by building samples of trees and running nonparametric statistics for\nintrinsic means on tree spaces with three and four leafs. SARS-CoV-2 are also\nused to built trees with leaves consisting in addition to other related\ncoronaviruses.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 05:07:07 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 00:58:01 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Shen", "Chen", ""], ["Patrangenaru", "Vic", ""], ["Moore", "Roland", ""]]}, {"id": "2106.07096", "submitter": "Kenneth Harris", "authors": "Kenneth D. Harris", "title": "A test for partial correlation between repeatedly observed nonstationary\n  nonlinear timeseries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a family of statistical tests to measure partial correlation in\nvectorial timeseries. The test measures whether an observed timeseries Y can be\npredicted from a second series X, even after accounting for a third series Z\nwhich may correlate with X. It does not make any assumptions on the nature of\nthese timeseries, such as stationarity or linearity, but it does require that\nmultiple statistically independent recordings of the 3 series are available.\nIntuitively, the test works by asking if the series Y recorded on one\nexperiment can be better predicted from X recorded on the same experiment than\non a different experiment, after accounting for the prediction from Z recorded\non both experiments.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 21:35:10 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Harris", "Kenneth D.", ""]]}, {"id": "2106.07103", "submitter": "Liao Zhu", "authors": "Liao Zhu, Haoxuan Wu, Martin T. Wells", "title": "A News-based Machine Learning Model for Adaptive Asset Pricing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a new asset pricing model -- the News Embedding UMAP\nSelection (NEUS) model, to explain and predict the stock returns based on the\nfinancial news. Using a combination of various machine learning algorithms, we\nfirst derive a company embedding vector for each basis asset from the financial\nnews. Then we obtain a collection of the basis assets based on their company\nembedding. After that for each stock, we select the basis assets to explain and\npredict the stock return with high-dimensional statistical methods. The new\nmodel is shown to have a significantly better fitting and prediction power than\nthe Fama-French 5-factor model.\n", "versions": [{"version": "v1", "created": "Sun, 13 Jun 2021 22:38:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zhu", "Liao", ""], ["Wu", "Haoxuan", ""], ["Wells", "Martin T.", ""]]}, {"id": "2106.07138", "submitter": "Shulei Wang", "authors": "Shulei Wang", "title": "Self-Supervised Metric Learning in Multi-View Data: A Downstream Task\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised metric learning has been a successful approach for learning a\ndistance from an unlabeled dataset. The resulting distance is broadly useful\nfor improving various distance-based downstream tasks, even when no information\nfrom downstream tasks is utilized in the metric learning stage. To gain\ninsights into this approach, we develop a statistical framework to\ntheoretically study how self-supervised metric learning can benefit downstream\ntasks in the context of multi-view data. Under this framework, we show that the\ntarget distance of metric learning satisfies several desired properties for the\ndownstream tasks. On the other hand, our investigation suggests the target\ndistance can be further improved by moderating each direction's weights. In\naddition, our analysis precisely characterizes the improvement by\nself-supervised metric learning on four commonly used downstream tasks: sample\nidentification, two-sample testing, $k$-means clustering, and $k$-nearest\nneighbor classification. As a by-product, we propose a simple spectral method\nfor self-supervised metric learning, which is computationally efficient and\nminimax optimal for estimating target distance. Finally, numerical experiments\nare presented to support the theoretical results in the paper.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 02:34:33 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Wang", "Shulei", ""]]}, {"id": "2106.07377", "submitter": "Nick James", "authors": "Nick James and Max Menzies", "title": "A new measure to study erratic financial behaviors and time-varying\n  dynamics of equity markets", "comments": "Equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.GN q-fin.EC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new framework to quantify distance between finite\nsets with uncertainty present, where probability distributions determine the\nlocations of individual elements. Combining this with a Bayesian change point\ndetection algorithm, we produce a new measure of similarity between time series\nwith respect to their structural breaks. Next, we apply this to financial data\nto study the erratic behavior profiles of 19 countries and 11 sectors over the\npast 20 years. Then, we take a closer examination of individual equities and\ntheir behavior surrounding market crises, times when change points are\nconsistently observed. Combining new and existing methods, we study the\ndynamics of our collection of equities and highlight an increase in equity\nsimilarity in recent years, particularly during such crises. Finally, we show\nthat our methodology may provide a new outlook on diversification and\nrisk-reduction during times of extraordinary correlation between assets, where\ntraditional portfolio optimization algorithms encounter difficulties.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2021 00:22:51 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["James", "Nick", ""], ["Menzies", "Max", ""]]}, {"id": "2106.07401", "submitter": "Dylan Spicker", "authors": "Dylan Spicker, Michael P Wallace, Grace Y Yi", "title": "Generalizations to Corrections for the Effects of Measurement Error in\n  Approximately Consistent Methodologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measurement error is a pervasive issue which renders the results of an\nanalysis unreliable. The measurement error literature contains numerous\ncorrection techniques, which can be broadly divided into those which aim to\nproduce exactly consistent estimators, and those which are only approximately\nconsistent. While consistency is a desirable property, it is typically attained\nonly under specific model assumptions. Two approximately consistent techniques,\nregression calibration and simulation extrapolation, are used frequently in a\nwide variety of parametric and semiparametric settings. We generalize these\ncorrections, relaxing assumptions placed on replicate measurements. Under\nregularity conditions, the estimators are shown to be asymptotically normal,\nwith a sandwich estimator for the asymptotic variance. Through simulation, we\ndemonstrate the improved performance of our estimators, over the standard\ntechniques, when these assumptions are violated. We motivate these corrections\nusing the Framingham Heart Study, and apply our generalized techniques to an\nanalysis of these data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 13:02:20 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Spicker", "Dylan", ""], ["Wallace", "Michael P", ""], ["Yi", "Grace Y", ""]]}, {"id": "2106.07437", "submitter": "Marko Obradovi\\'c", "authors": "Bojana Milo\\v{s}evi\\'c, Ya.Yu. Nikitin, Marko Obradovi\\'c", "title": "Bahadur efficiency of EDF based normality tests when parameters are\n  estimated", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper some well-known tests based on empirical distribution functions\n(EDF) with estimated parameters for testing composite normality hypothesis are\nrevisited, and some new results on asymptotic properties are provided. In\nparticular, the approximate Bahadur slopes are obtained -- in the case of close\nalternatives -- for the EDF-based tests as well as the likelihood ratio test.\nThe local approximate efficiencies are calculated for several close\nalternatives. The obtained results could serve as a benchmark for evaluation of\nthe quality of recent and future normality tests.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 14:03:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Milo\u0161evi\u0107", "Bojana", ""], ["Nikitin", "Ya. Yu.", ""], ["Obradovi\u0107", "Marko", ""]]}, {"id": "2106.07587", "submitter": "Shunichiro Orihara", "authors": "Shunichiro Orihara", "title": "Quasi-Maximum Likelihood based Model Selection Procedures for Binary\n  Outcomes", "comments": "Keywords: Causal inference, Unmeasured covariates, Quasi-maximum\n  lilkelihood, Two-stage residual inclusion, Model selection, Consistency", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, I propose two model selection procedures based on a\nquasi-maximum likelihood estimator when there exist unmeasured covariates. I\nprove that a proposed BIC-type model selection procedure has model selection\nconsistency, and confirm these property through simulation datasets.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 16:45:15 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 05:33:30 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Orihara", "Shunichiro", ""]]}, {"id": "2106.07623", "submitter": "Ciaran Evans", "authors": "Ciaran Evans, Zara Y. Weinberg, Manojkumar A. Puthenveedu, Max G'Sell", "title": "Inference with generalizable classifier predictions", "comments": "26 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the problem of making statistical inference about a\npopulation that can only be identified through classifier predictions. The\nproblem is motivated by scientific studies in which human labels of a\npopulation are replaced by a classifier. For downstream analysis of the\npopulation based on classifier predictions to be sound, the predictions must\ngeneralize equally across experimental conditions. In this paper, we formalize\nthe task of statistical inference using classifier predictions, and propose\nbootstrap procedures to allow inference with a generalizable classifier. We\ndemonstrate the performance of our methods through extensive simulations and a\ncase study with live cell imaging data.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 17:28:03 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Evans", "Ciaran", ""], ["Weinberg", "Zara Y.", ""], ["Puthenveedu", "Manojkumar A.", ""], ["G'Sell", "Max", ""]]}, {"id": "2106.07636", "submitter": "Danica J. Sutherland", "authors": "Feng Liu and Wenkai Xu and Jie Lu and Danica J. Sutherland", "title": "Meta Two-Sample Testing: Learning Kernels for Testing with Limited Data", "comments": "Code is available from https://github.com/fengliu90/MetaTesting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern kernel-based two-sample tests have shown great success in\ndistinguishing complex, high-dimensional distributions with appropriate learned\nkernels. Previous work has demonstrated that this kernel learning procedure\nsucceeds, assuming a considerable number of observed samples from each\ndistribution. In realistic scenarios with very limited numbers of data samples,\nhowever, it can be challenging to identify a kernel powerful enough to\ndistinguish complex distributions. We address this issue by introducing the\nproblem of meta two-sample testing (M2ST), which aims to exploit (abundant)\nauxiliary data on related tasks to find an algorithm that can quickly identify\na powerful test on new target tasks. We propose two specific algorithms for\nthis task: a generic scheme which improves over baselines and amore tailored\napproach which performs even better. We provide both theoretical justification\nand empirical evidence that our proposed meta-testing schemes out-perform\nlearning kernel-based tests directly from scarce observations, and identify\nwhen such schemes will be successful.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 17:52:50 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Liu", "Feng", ""], ["Xu", "Wenkai", ""], ["Lu", "Jie", ""], ["Sutherland", "Danica J.", ""]]}, {"id": "2106.07695", "submitter": "Samir Khan", "authors": "Samir Khan, Johan Ugander", "title": "Adaptive normalization for IPW estimation", "comments": "32 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse probability weighting (IPW) is a general tool in survey sampling and\ncausal inference, used both in Horvitz-Thompson estimators, which normalize by\nthe sample size, and H\\'ajek/self-normalized estimators, which normalize by the\nsum of the inverse probability weights. In this work we study a family of IPW\nestimators, first proposed by Trotter and Tukey in the context of Monte Carlo\nproblems, that are normalized by an affine combination of these two terms. We\nshow how selecting an estimator from this family in a data-dependent way to\nminimize asymptotic variance leads to an iterative procedure that converges to\nan estimator with connections to regression control methods. We refer to this\nestimator as an adaptively normalized estimator. For mean estimation in survey\nsampling, this estimator has asymptotic variance that is never worse than the\nHorvitz--Thompson or H\\'ajek estimators, and is smaller except in edge cases.\nGoing further, we show that adaptive normalization can be used to propose\nimprovements of the augmented IPW (AIPW) estimator, average treatment effect\n(ATE) estimators, and policy learning objectives. Appealingly, these proposals\npreserve both the asymptotic efficiency of AIPW and the regret bounds for\npolicy learning with IPW objectives, and deliver consistent finite sample\nimprovements in simulations for all three of mean estimation, ATE estimation,\nand policy learning.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 18:22:29 GMT"}, {"version": "v2", "created": "Fri, 9 Jul 2021 18:59:12 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Khan", "Samir", ""], ["Ugander", "Johan", ""]]}, {"id": "2106.07717", "submitter": "Si Kai Lee", "authors": "Y. Samuel Wang, Si Kai Lee, Panos Toulis, Mladen Kolar", "title": "Robust Inference for High-Dimensional Linear Models via Residual\n  Randomization", "comments": null, "journal-ref": "International Conference on Machine Learning 2021", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a residual randomization procedure designed for robust Lasso-based\ninference in the high-dimensional setting. Compared to earlier work that\nfocuses on sub-Gaussian errors, the proposed procedure is designed to work\nrobustly in settings that also include heavy-tailed covariates and errors.\nMoreover, our procedure can be valid under clustered errors, which is important\nin practice, but has been largely overlooked by earlier work. Through extensive\nsimulations, we illustrate our method's wider range of applicability as\nsuggested by theory. In particular, we show that our method outperforms\nstate-of-art methods in challenging, yet more realistic, settings where the\ndistribution of covariates is heavy-tailed or the sample size is small, while\nit remains competitive in standard, ``well behaved\" settings previously studied\nin the literature.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2021 19:27:51 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Wang", "Y. Samuel", ""], ["Lee", "Si Kai", ""], ["Toulis", "Panos", ""], ["Kolar", "Mladen", ""]]}, {"id": "2106.07816", "submitter": "Anna Neufeld", "authors": "Anna C. Neufeld, Lucy L. Gao, Daniela M. Witten", "title": "Tree-Values: selective inference for regression trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider conducting inference on the output of the Classification and\nRegression Tree (CART) [Breiman et al., 1984] algorithm. A naive approach to\ninference that does not account for the fact that the tree was estimated from\nthe data will not achieve standard guarantees, such as Type 1 error rate\ncontrol and nominal coverage. Thus, we propose a selective inference framework\nfor conducting inference on a fitted CART tree. In a nutshell, we condition on\nthe fact that the tree was estimated from the data. We propose a test for the\ndifference in the mean response between a pair of terminal nodes that controls\nthe selective Type 1 error rate, and a confidence interval for the mean\nresponse within a single terminal node that attains the nominal selective\ncoverage. Efficient algorithms for computing the necessary conditioning sets\nare provided. We apply these methods in simulation and to a dataset involving\nthe association between portion control interventions and caloric intake.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 00:25:11 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Neufeld", "Anna C.", ""], ["Gao", "Lucy L.", ""], ["Witten", "Daniela M.", ""]]}, {"id": "2106.07914", "submitter": "Nikos Vlassis", "authors": "Nikos Vlassis, Ashok Chandrashekar, Fernando Amat Gil, Nathan Kallus", "title": "Control Variates for Slate Off-Policy Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy evaluation from batched contextual bandit\ndata with multidimensional actions, often termed slates. The problem is common\nto recommender systems and user-interface optimization, and it is particularly\nchallenging because of the combinatorially-sized action space. Swaminathan et\nal. (2017) have proposed the pseudoinverse (PI) estimator under the assumption\nthat the conditional mean rewards are additive in actions. Using control\nvariates, we consider a large class of unbiased estimators that includes as\nspecific cases the PI estimator and (asymptotically) its self-normalized\nvariant. By optimizing over this class, we obtain new estimators with risk\nimprovement guarantees over both the PI and self-normalized PI estimators.\nExperiments with real-world recommender data as well as synthetic data validate\nthese improvements in practice.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 06:59:53 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Vlassis", "Nikos", ""], ["Chandrashekar", "Ashok", ""], ["Gil", "Fernando Amat", ""], ["Kallus", "Nathan", ""]]}, {"id": "2106.08277", "submitter": "Jose Jimenez", "authors": "Jos\\'e L. Jim\\'enez and Haiyan Zheng", "title": "A Bayesian adaptive design for dual-agent phase I-II cancer clinical\n  trials combining efficacy data across stages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrated phase I-II clinical trial designs are efficient approaches to\naccelerate drug development. In cases where efficacy cannot be ascertained in a\nshort period of time, two-stage approaches are usually employed. When different\npatient populations are involved across stages, it is worth of discussion about\nthe use of efficacy data collected from both stages. In this paper, we focus on\na two-stage design that aims to estimate safe dose combinations with a certain\nlevel of efficacy. In stage I, conditional escalation with overdose control\n(EWOC) is used to allocate successive cohorts of patients. The maximum\ntolerated dose (MTD) curve is estimated based on a Bayesian dose-toxicity\nmodel. In stage II, we consider an adaptive allocation of patients to drug\ncombinations that have a high probability of being efficacious along the\nobtained MTD curve. A robust Bayesian hierarchical model is proposed to allow\nsharing of information on the efficacy parameters across stages assuming the\nrelated parameters are either exchangeable or nonexchangeable. Under the\nassumption of exchangeability, a random-effects distribution is specified for\nthe main effects parameters to capture uncertainty about the between-stage\ndifferences. The proposed methodology is assessed with extensive simulations\nmotivated by a real phase I-II drug combination trial using continuous doses.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 16:41:16 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Jim\u00e9nez", "Jos\u00e9 L.", ""], ["Zheng", "Haiyan", ""]]}, {"id": "2106.08281", "submitter": "Francesco Denti", "authors": "Francesco Denti, Ricardo Azevedo, Chelsie Lo, Damian Wheeler, Sunil P.\n  Gandhi, Michele Guindani, Babak Shahbaba", "title": "A Horseshoe Pit mixture model for Bayesian screening with an application\n  to light sheet fluorescence microscopy in brain imaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding parsimonious models through variable selection is a fundamental\nproblem in many areas of statistical inference. Here, we focus on Bayesian\nregression models, where variable selection can be implemented through a\nregularizing prior imposed on the distribution of the regression coefficients.\nIn the Bayesian literature, there are two main types of priors used to\naccomplish this goal: the spike-and-slab and the continuous scale mixtures of\nGaussians. The former is a discrete mixture of two distributions characterized\nby low and high variance. In the latter, a continuous prior is elicited on the\nscale of a zero-mean Gaussian distribution. In contrast to these existing\nmethods, we propose a new class of priors based on discrete mixture of\ncontinuous scale mixtures providing a more general framework for Bayesian\nvariable selection. To this end, we substitute the observation-specific local\nshrinkage parameters (typical of continuous mixtures) with mixture component\nshrinkage parameters. Our approach drastically reduces the number of parameters\nneeded and allows sharing information across the coefficients, improving the\nshrinkage effect. By using half-Cauchy distributions, this approach leads to a\ncluster-shrinkage version of the Horseshoe prior. We present the properties of\nour model and showcase its estimation and prediction performance in a\nsimulation study. We then recast the model in a multiple hypothesis testing\nframework and apply it to a neurological dataset obtained using a novel\nwhole-brain imaging technique.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 16:46:09 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Denti", "Francesco", ""], ["Azevedo", "Ricardo", ""], ["Lo", "Chelsie", ""], ["Wheeler", "Damian", ""], ["Gandhi", "Sunil P.", ""], ["Guindani", "Michele", ""], ["Shahbaba", "Babak", ""]]}, {"id": "2106.08297", "submitter": "Giovanna Nappo", "authors": "Rachele Foschi, Giovanna Nappo, Fabio L. Spizzichino", "title": "Diagonal sections of copulas, multivariate conditional hazard rates and\n  distributions of order statistics for minimally stable lifetimes", "comments": null, "journal-ref": null, "doi": null, "report-no": "Roma01.Math", "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a motivating problem, we aim to study some special aspects of the marginal\ndistributions of the order statistics for exchangeable and (more generally) for\nminimally stable non-negative random variables $T_{1},...,T_{r}$. In any case,\nwe assume that $T_{1},...,T_{r}$ are identically distributed, with a common\nsurvival function $\\overline{G}$ and their survival copula is denoted by $K$.\nThe diagonal's and subdiagonals' sections of $K$, along with $\\overline{G}$,\nare possible tools to describe the information needed to recover the laws of\norder statistics.\n  When attention is restricted to the absolutely continuous case, such a joint\ndistribution can be described in terms of the associated multivariate\nconditional hazard rate (m.c.h.r.) functions. We then study the distributions\nof the order statistics of $T_{1},...,T_{r}$ also in terms of the system of the\nm.c.h.r. functions. We compare and, in a sense, we combine the two different\napproaches in order to obtain different detailed formulas and to analyze some\nprobabilistic aspects for the distributions of interest. This study also leads\nus to compare the two cases of exchangeable and minimally stable variables both\nin terms of copulas and of m.c.h.r. functions. The paper concludes with the\nanalysis of two remarkable special cases of stochastic dependence, namely\nArchimedean copulas and load sharing models. This analysis will allow us to\nprovide some illustrative examples, and some discussion about peculiar aspects\nof our results.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 17:18:33 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Foschi", "Rachele", ""], ["Nappo", "Giovanna", ""], ["Spizzichino", "Fabio L.", ""]]}, {"id": "2106.08360", "submitter": "Yezheng Li", "authors": "Yezheng Li, Hongzhe Li, Yuanpei Cao", "title": "Multi-sample estimation of centered log-ratio matrix in microbiome\n  studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In microbiome studies, one of the ways of studying bacterial abundances is to\nestimate bacterial composition based on the sequencing read counts. Various\ntransformations are then applied to such compositional data for downstream\nstatistical analysis, among which the centered log-ratio (clr) transformation\nis most commonly used.\n  Due to limited sequencing depth and DNA dropouts, many rare bacterial taxa\nmight not be captured in the final sequencing reads, which results in many zero\ncounts. Naive composition estimation using count normalization leads to many\nzero proportions, which makes clr transformation infeasible. This paper\nproposes a multi-sample approach to estimation of the clr matrix directly in\norder to borrow information across samples and across species. Empirical\nresults from real datasets suggest that the clr matrix over multiple samples is\napproximately low rank, which motivates a regularized maximum likelihood\nestimation with a nuclear norm penalty. An efficient optimization algorithm\nusing the generalized accelerated proximal gradient is developed. Theoretical\nupper bounds of the estimation errors and of its corresponding singular\nsubspace errors are established. Simulation studies demonstrate that the\nproposed estimator outperforms the naive estimators. The method is analyzed on\nGut Microbiome dataset and the American Gut project.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 18:09:01 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Li", "Yezheng", ""], ["Li", "Hongzhe", ""], ["Cao", "Yuanpei", ""]]}, {"id": "2106.08460", "submitter": "Leying Guan", "authors": "Leying Guan", "title": "Localized Conformal Prediction: A Generalized Inference Framework for\n  Conformal Prediction", "comments": "This paper is based on the results on localized conformal prediction\n  under the i.i.d settings from arXiv:1908.08558, with strengthened theoretical\n  results, new and more efficient algorithms, and additional empirical studies.\n  50 pages; 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new inference framework called localized conformal prediction.\nIt generalizes the framework of conformal prediction and offers a\nsingle-test-sample adaptive construction by emphasizing a local region around\nit. Although there have been methods constructing heterogeneous prediction\nintervals for $Y$ by designing better conformal score functions, to our\nknowledge, this is the first work that introduces an adaptive nature to the\ninference framework itself. We prove that our proposal leads to an\nassumption-free and finite sample marginal coverage guarantee, as well as an\napproximate conditional coverage guarantee. Our proposal achieves asymptotic\nconditional coverage under suitable assumptions.\n  The localized conformal prediction can be combined with many existing works\nin conformal prediction, including different types of conformal score\nconstructions. We will demonstrate how to change from conformal prediction to\nlocalized conformal prediction in these related works and a potential gain via\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 22:10:43 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Guan", "Leying", ""]]}, {"id": "2106.08511", "submitter": "Xu Han", "authors": "Lijia Wang, Xu Han and Xin Tong", "title": "Skilled Mutual Fund Selection: False Discovery Control under Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Selecting skilled mutual funds through the multiple testing framework has\nreceived increasing attention from finance researchers and statisticians. The\nintercept $\\alpha$ of Carhart four-factor model is commonly used to measure the\ntrue performance of mutual funds, and positive $\\alpha$'s are considered as\nskilled. We observe that the standardized OLS estimates of $\\alpha$'s across\nthe funds possess strong dependence and nonnormality structures, indicating\nthat the conventional multiple testing methods are inadequate for selecting the\nskilled funds. We start from a decision theoretic perspective, and propose an\noptimal testing procedure to minimize a combination of false discovery rate and\nfalse non-discovery rate. Our proposed testing procedure is constructed based\non the probability of each fund not being skilled conditional on the\ninformation across all of the funds in our study. To model the distribution of\nthe information used for the testing procedure, we consider a mixture model\nunder dependence and propose a new method called ``approximate empirical Bayes\"\nto fit the parameters. Empirical studies show that our selected skilled funds\nhave superior long-term and short-term performance, e.g., our selection\nstrongly outperforms the S\\&P 500 index during the same period.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 01:44:21 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Wang", "Lijia", ""], ["Han", "Xu", ""], ["Tong", "Xin", ""]]}, {"id": "2106.08530", "submitter": "Tong Chen", "authors": "Tong Chen and Thomas Lumley", "title": "Optimal sampling for design-based estimators of regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-phase designs measure variables of interest on a subcohort where the\noutcome and covariates are readily available or cheap to collect on all\nindividuals in the cohort. Given limited resource availability, it is of\ninterest to find an optimal design that includes more informative individuals\nin the final sample. We explore the optimal designs and efficiencies for\nanalysis by design-based estimators. Generalized raking is an efficient\ndesign-based estimator that improves on the inverse-probability weighted (IPW)\nestimator by adjusting weights based on the auxiliary information. We derive a\nclosed-form solution of the optimal design for estimating regression\ncoefficients from generalized raking estimators. We compare it with the optimal\ndesign for analysis via the IPW estimator and other two-phase designs in\nmeasurement-error settings. We consider general two-phase designs where the\noutcome variable and variables of interest can be continuous or discrete. Our\nresults show that the optimal designs for analysis by the two design-based\nestimators can be very different. The optimal design for IPW estimation is\noptimal for analysis via the IPW estimator and typically gives near-optimal\nefficiency for generalized raking, though we show there is potential\nimprovement in some settings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 02:52:55 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Chen", "Tong", ""], ["Lumley", "Thomas", ""]]}, {"id": "2106.08540", "submitter": "Zishu Zhan", "authors": "Zishu Zhan, Xiangjie Li and Jingxiao Zhang", "title": "Projective Resampling Imputation Mean Estimation Method for Missing\n  Covariates Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data is a common problem in clinical data collection, which causes\ndifficulty in the statistical analysis of such data. To overcome problems\ncaused by incomplete data, we propose a new imputation method called projective\nresampling imputation mean estimation (PRIME), which can also address ``the\ncurse of dimensionality\" problem in imputation with less information loss. We\nuse various sample sizes, missing-data rates, covariate correlations, and noise\nlevels in simulation studies, and all results show that PRIME outperformes\nother methods such as iterative least-squares estimation (ILSE), maximum\nlikelihood (ML), and complete-case analysis (CC). Moreover, we conduct a study\nof influential factors in cardiac surgery-associated acute kidney injury\n(CSA-AKI), which show that our method performs better than the other models.\nFinally, we prove that PRIME has a consistent property under some regular\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 03:49:57 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Zhan", "Zishu", ""], ["Li", "Xiangjie", ""], ["Zhang", "Jingxiao", ""]]}, {"id": "2106.08881", "submitter": "Junhui Cai", "authors": "Junhui Cai, Xu Han, Ya'acov Ritov, Linda Zhao", "title": "Nonparametric Empirical Bayes Estimation and Testing for Sparse and\n  Heteroscedastic Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale modern data often involves estimation and testing for\nhigh-dimensional unknown parameters. It is desirable to identify the sparse\nsignals, ``the needles in the haystack'', with accuracy and false discovery\ncontrol. However, the unprecedented complexity and heterogeneity in modern data\nstructure require new machine learning tools to effectively exploit\ncommonalities and to robustly adjust for both sparsity and heterogeneity. In\naddition, estimates for high-dimensional parameters often lack uncertainty\nquantification. In this paper, we propose a novel Spike-and-Nonparametric\nmixture prior (SNP) -- a spike to promote the sparsity and a nonparametric\nstructure to capture signals. In contrast to the state-of-the-art methods, the\nproposed methods solve the estimation and testing problem at once with several\nmerits: 1) an accurate sparsity estimation; 2) point estimates with\nshrinkage/soft-thresholding property; 3) credible intervals for uncertainty\nquantification; 4) an optimal multiple testing procedure that controls false\ndiscovery rate. Our method exhibits promising empirical performance on both\nsimulated data and a gene expression case study.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 15:55:44 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Cai", "Junhui", ""], ["Han", "Xu", ""], ["Ritov", "Ya'acov", ""], ["Zhao", "Linda", ""]]}, {"id": "2106.08984", "submitter": "Michael Gallaugher Ph.D.", "authors": "Michael P.B. Gallaugher, Peter A. Tait, and Paul D. McNicholas", "title": "Four Skewed Tensor Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rise of the \"big data\" phenomenon in recent years, data is coming in\nmany different complex forms. One example of this is multi-way data that come\nin the form of higher-order tensors such as coloured images and movie clips.\nAlthough there has been a recent rise in models for looking at the simple case\nof three-way data in the form of matrices, there is a relative paucity of\nhigher-order tensor variate methods. The most common tensor distribution in the\nliterature is the tensor variate normal distribution; however, its use can be\nproblematic if the data exhibit skewness or outliers. Herein, we develop four\nskewed tensor variate distributions which to our knowledge are the first skewed\ntensor distributions to be proposed in the literature, and are able to\nparameterize both skewness and tail weight. Properties and parameter estimation\nare discussed, and real and simulated data are used for illustration.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 17:26:08 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["Tait", "Peter A.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "2106.09071", "submitter": "Xu Han", "authors": "Xu Han, Ethan X Fang, Cheng Yong Tang", "title": "Pre-processing with Orthogonal Decompositions for High-dimensional\n  Explanatory Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Strong correlations between explanatory variables are problematic for\nhigh-dimensional regularized regression methods. Due to the violation of the\nIrrepresentable Condition, the popular LASSO method may suffer from false\ninclusions of inactive variables. In this paper, we propose pre-processing with\northogonal decompositions (PROD) for the explanatory variables in\nhigh-dimensional regressions. The PROD procedure is constructed based upon a\ngeneric orthogonal decomposition of the design matrix. We demonstrate by two\nconcrete cases that the PROD approach can be effectively constructed for\nimproving the performance of high-dimensional penalized regression. Our\ntheoretical analysis reveals their properties and benefits for high-dimensional\npenalized linear regression with LASSO. Extensive numerical studies with\nsimulations and data analysis show the promising performance of the PROD.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 18:22:16 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Han", "Xu", ""], ["Fang", "Ethan X", ""], ["Tang", "Cheng Yong", ""]]}, {"id": "2106.09100", "submitter": "Jonathan Larson", "authors": "Jonathan Larson and Jukka-Pekka Onnela", "title": "Maximum likelihood estimation for mechanistic network models", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanistic network models specify the mechanisms by which networks grow and\nchange, allowing researchers to investigate complex systems using both\nsimulation and analytical techniques. Unfortunately, it is difficult to write\nlikelihoods for instances of graphs generated with mechanistic models because\nof a combinatorial explosion in outcomes of repeated applications of the\nmechanism. Thus it is near impossible to estimate the parameters using maximum\nlikelihood estimation. In this paper, we propose treating node sequence in a\ngrowing network model as an additional parameter, or as a missing random\nvariable, and maximizing over the resulting likelihood. We develop this\nframework in the context of a simple mechanistic network model, used to study\ngene duplication and divergence, and test a variety of algorithms for\nmaximizing the likelihood in simulated graphs. We also run the best-performing\nalgorithm on a human protein-protein interaction network and four non-human\nprotein-protein interaction networks. Although we focus on a specific\nmechanistic network model here, the proposed framework is more generally\napplicable to reversible models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 19:49:50 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Larson", "Jonathan", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "2106.09114", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal and Bohan Wu", "title": "Semiparametric count data regression for self-reported mental health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  \"For how many days during the past 30 days was your mental health not good?\"\nThe responses to this question measure self-reported mental health and can be\nlinked to important covariates in the National Health and Nutrition Examination\nSurvey (NHANES). However, these count variables present major distributional\nchallenges: the data are overdispersed, zero-inflated, bounded by 30, and\nheaped in five- and seven-day increments. To meet these challenges, we design a\nsemiparametric estimation and inference framework for count data regression.\nThe data-generating process is defined by simultaneously transforming and\nrounding (STAR) a latent Gaussian regression model. The transformation is\nestimated nonparametrically and the rounding operator ensures the correct\nsupport for the discrete and bounded data. Maximum likelihood estimators are\ncomputed using an EM algorithm that is compatible with any continuous data\nmodel estimable by least squares. STAR regression includes asymptotic\nhypothesis testing and confidence intervals, variable selection via information\ncriteria, and customized diagnostics. Simulation studies validate the utility\nof this framework. STAR is deployed to study the factors associated with\nself-reported mental health and demonstrates substantial improvements in\ngoodness-of-fit compared to existing count data regression models.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 20:38:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Kowal", "Daniel R.", ""], ["Wu", "Bohan", ""]]}, {"id": "2106.09115", "submitter": "Marcio Valk Valk .M", "authors": "Debora Zava Bello, Marcio Valk and Gabriela Bettella Cybis", "title": "Clustering inference in multiple groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inference in clustering is paramount to uncovering inherent group structure\nin data. Clustering methods which assess statistical significance have recently\ndrawn attention owing to their importance for the identification of patterns in\nhigh dimensional data with applications in many scientific fields. We present\nhere a U-statistics based approach, specially tailored for high-dimensional\ndata, that clusters the data into three groups while assessing the significance\nof such partitions. Because our approach stands on the U-statistics based\nclustering framework of the methods in R package uclust, it inherits its\ncharacteristics being a non-parametric method relying on very few assumptions\nabout the data, and thus can be applied to a wide range of dataset. Furthermore\nour method aims to be a more powerful tool to find the best partitions of the\ndata into three groups when that particular structure is present. In order to\ndo so, we first propose an extension of the test U-statistic and develop its\nasymptotic theory. Additionally we propose a ternary non-nested significance\nclustering method. Our approach is tested through multiple simulations and\nfound to have more statistical power than competing alternatives in all\nscenarios considered. Applications to peripheral blood mononuclear cells and to\nimage recognition shows the versatility of our proposal, presenting a superior\nperformance when compared with other approaches.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2021 20:38:14 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Bello", "Debora Zava", ""], ["Valk", "Marcio", ""], ["Cybis", "Gabriela Bettella", ""]]}, {"id": "2106.09327", "submitter": "Guillaume Dalle", "authors": "Guillaume Dalle (CERMICS), Yohann de Castro (ICJ, ECL)", "title": "Minimax Estimation of Partially-Observed Vector AutoRegressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand the behavior of large dynamical systems like transportation\nnetworks, one must often rely on measurements transmitted by a set of sensors,\nfor instance individual vehicles. Such measurements are likely to be incomplete\nand imprecise, which makes it hard to recover the underlying signal of\ninterest.Hoping to quantify this phenomenon, we study the properties of a\npartially-observed state-space model. In our setting, the latent state $X$\nfollows a high-dimensional Vector AutoRegressive process $X_t = \\theta X_{t-1}\n+ \\varepsilon_t$. Meanwhile, the observations $Y$ are given by a\nnoise-corrupted random sample from the state $Y_t = \\Pi_t X_t + \\eta_t$.\nSeveral random sampling mechanisms are studied, allowing us to investigate the\neffect of spatial and temporal correlations in the distribution of the sampling\nmatrices $\\Pi_t$.We first prove a lower bound on the minimax estimation error\nfor the transition matrix $\\theta$. We then describe a sparse estimator based\non the Dantzig selector and upper bound its non-asymptotic error, showing that\nit achieves the optimal convergence rate for most of our sampling mechanisms.\nNumerical experiments on simulated time series validate our theoretical\nfindings, while an application to open railway data highlights the relevance of\nthis model for public transport traffic analysis.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 08:46:53 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Dalle", "Guillaume", "", "CERMICS"], ["de Castro", "Yohann", "", "ICJ, ECL"]]}, {"id": "2106.09387", "submitter": "Feng Ruan", "authors": "Feng Ruan, Keli Liu, Michael I. Jordan", "title": "Taming Nonconvexity in Kernel Feature Selection---Favorable Properties\n  of the Laplace Kernel", "comments": "28 pages main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel-based feature selection is an important tool in nonparametric\nstatistics. Despite many practical applications of kernel-based feature\nselection, there is little statistical theory available to support the method.\nA core challenge is the objective function of the optimization problems used to\ndefine kernel-based feature selection are nonconvex. The literature has only\nstudied the statistical properties of the \\emph{global optima}, which is a\nmismatch, given that the gradient-based algorithms available for nonconvex\noptimization are only able to guarantee convergence to local minima. Studying\nthe full landscape associated with kernel-based methods, we show that feature\nselection objectives using the Laplace kernel (and other $\\ell_1$ kernels) come\nwith statistical guarantees that other kernels, including the ubiquitous\nGaussian kernel (or other $\\ell_2$ kernels) do not possess. Based on a sharp\ncharacterization of the gradient of the objective function, we show that\n$\\ell_1$ kernels eliminate unfavorable stationary points that appear when using\nan $\\ell_2$ kernel. Armed with this insight, we establish statistical\nguarantees for $\\ell_1$ kernel-based feature selection which do not require\nreaching the global minima. In particular, we establish model-selection\nconsistency of $\\ell_1$-kernel-based feature selection in recovering main\neffects and hierarchical interactions in the nonparametric setting with $n \\sim\n\\log p$ samples.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 11:05:48 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 05:15:48 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Ruan", "Feng", ""], ["Liu", "Keli", ""], ["Jordan", "Michael I.", ""]]}, {"id": "2106.09494", "submitter": "Jasper Yang", "authors": "Jasper B. Yang, Bryan E. Shepherd, Thomas Lumley, Pamela A. Shaw", "title": "Optimum Allocation for Adaptive Multi-Wave Sampling in R: The R Package\n  optimall", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package optimall offers a collection of functions that efficiently\nstreamline the design process of sampling in surveys ranging from simple to\ncomplex. The package's main functions allow users to interactively define and\nadjust strata cut points based on values or quantiles of auxiliary covariates,\nadaptively calculate the optimum number of samples to allocate to each stratum\nusing Neyman or Wright allocation, and select specific IDs to sample based on a\nstratified sampling design. Using real-life epidemiological study examples, we\ndemonstrate how optimall facilitates an efficient workflow for the design and\nimplementation of surveys in R. Although tailored towards multi-wave sampling\nunder two- or three-phase designs, the R package optimall may be useful for any\nsampling survey.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:38:57 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Yang", "Jasper B.", ""], ["Shepherd", "Bryan E.", ""], ["Lumley", "Thomas", ""], ["Shaw", "Pamela A.", ""]]}, {"id": "2106.09499", "submitter": "Walter Del Pozzo", "authors": "Alessandro Martini, Stefano Schmidt, Walter Del Pozzo", "title": "Maximum Entropy Spectral Analysis: a case study", "comments": "16 pages, 13 figure, submitted to A&A", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM physics.data-an", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Maximum Entropy Spectral Analysis (MESA) method, developed by Burg,\nprovides a powerful tool to perform spectral estimation of a time-series. The\nmethod relies on a Jaynes' maximum entropy principle and provides the means of\ninferring the spectrum of a stochastic process in terms of the coefficients of\nsome autoregressive process AR($p$) of order $p$. A closed form recursive\nsolution provides an estimate of the autoregressive coefficients as well as of\nthe order $p$ of the process. We provide a ready-to-use implementation of the\nalgorithm in the form of a python package \\texttt{memspectrum}. We characterize\nour implementation by performing a power spectral density analysis on synthetic\ndata (with known power spectral density) and we compare different criteria for\nstopping the recursion. Furthermore, we compare the performance of our code\nwith the ubiquitous Welch algorithm, using synthetic data generated from the\nreleased spectrum by the LIGO-Virgo collaboration. We find that, when compared\nto Welch's method, Burg's method provides a power spectral density (PSD)\nestimation with a systematically lower variance and bias. This is particularly\nmanifest in the case of a little number of data points, making Burg's method\nmost suitable to work in this regime.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 13:48:57 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Martini", "Alessandro", ""], ["Schmidt", "Stefano", ""], ["Del Pozzo", "Walter", ""]]}, {"id": "2106.09533", "submitter": "Graham Tierney", "authors": "Graham Tierney and Christopher Bail and Alexander Volfovsky", "title": "Author Clustering and Topic Estimation for Short Texts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysis of short text, such as social media posts, is extremely difficult\nbecause it relies on observing many document-level word co-occurrence pairs.\nBeyond topic distributions, a common downstream task of the modeling is\ngrouping the authors of these documents for subsequent analyses. Traditional\nmodels estimate the document groupings and identify user clusters with an\nindependent procedure. We propose a novel model that expands on the Latent\nDirichlet Allocation by modeling strong dependence among the words in the same\ndocument, with user-level topic distributions. We also simultaneously cluster\nusers, removing the need for post-hoc cluster estimation and improving topic\nestimation by shrinking noisy user-level topic distributions towards typical\nvalues. Our method performs as well as -- or better -- than traditional\napproaches to problems arising in short text, and we demonstrate its usefulness\non a dataset of tweets from United States Senators, recovering both meaningful\ntopics and clusters that reflect partisan ideology.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 20:55:55 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Tierney", "Graham", ""], ["Bail", "Christopher", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "2106.09597", "submitter": "David John", "authors": "David N. John, Livia Stohrer, Claudia Schillings, Michael Schick,\n  Vincent Heuveline", "title": "Hierarchical surrogate-based Approximate Bayesian Computation for an\n  electric motor test bench", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.OC physics.data-an stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Inferring parameter distributions of complex industrial systems from noisy\ntime series data requires methods to deal with the uncertainty of the\nunderlying data and the used simulation model. Bayesian inference is well\nsuited for these uncertain inverse problems. Standard methods used to identify\nuncertain parameters are Markov Chain Monte Carlo (MCMC) methods with explicit\nevaluation of a likelihood function. However, if the likelihood is very\ncomplex, such that its evaluation is computationally expensive, or even unknown\nin its explicit form, Approximate Bayesian Computation (ABC) methods provide a\npromising alternative. In this work both methods are first applied to\nartificially generated data and second on a real world problem, by using data\nof an electric motor test bench. We show that both methods are able to infer\nthe distribution of varying parameters with a Bayesian hierarchical approach.\nBut the proposed ABC method is computationally much more efficient in order to\nachieve results with similar accuracy. We suggest to use summary statistics in\norder to reduce the dimension of the data which significantly increases the\nefficiency of the algorithm. Further the simulation model is replaced by a\nPolynomial Chaos Expansion (PCE) surrogate to speed up model evaluations. We\nproof consistency for the proposed surrogate-based ABC method with summary\nstatistics under mild conditions on the (approximated) forward model.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 15:29:25 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["John", "David N.", ""], ["Stohrer", "Livia", ""], ["Schillings", "Claudia", ""], ["Schick", "Michael", ""], ["Heuveline", "Vincent", ""]]}, {"id": "2106.09632", "submitter": "Xu Han", "authors": "Xu Han, Sanat Sarkar, Shiyu Zhang", "title": "Large-Scale Multiple Testing for Matrix-Valued Data under Double\n  Dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  High-dimensional inference based on matrix-valued data has drawn increasing\nattention in modern statistical research, yet not much progress has been made\nin large-scale multiple testing specifically designed for analysing such data\nsets. Motivated by this, we consider in this article an electroencephalography\n(EEG) experiment that produces matrix-valued data and presents a scope of\ndeveloping novel matrix-valued data based multiple testing methods controlling\nfalse discoveries for hypotheses that are of importance in such an experiment.\nThe row-column cross-dependency of observations appearing in a matrix form,\nreferred to as double-dependency, is one of the main challenges in the\ndevelopment of such methods. We address it by assuming matrix normal\ndistribution for the observations at each of the independent matrix\ndata-points. This allows us to fully capture the underlying double-dependency\ninformed through the row- and column-covariance matrices and develop methods\nthat are potentially more powerful than the corresponding one (e.g., Fan and\nHan (2017)) obtained by vectorizing each data point and thus ignoring the\ndouble-dependency. We propose two methods to approximate the false discovery\nproportion with statistical accuracy. While one of these methods is a general\napproach under double-dependency, the other one provides more computational\nefficiency for higher dimensionality. Extensive numerical studies illustrate\nthe superior performance of the proposed methods over the principal factor\napproximation method of Fan and Han (2017). The proposed methods have been\nfurther applied to the aforementioned EEG data.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 16:18:20 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Han", "Xu", ""], ["Sarkar", "Sanat", ""], ["Zhang", "Shiyu", ""]]}, {"id": "2106.09633", "submitter": "Adam Lane", "authors": "Adam Lane", "title": "Optimal Relevant Subset Designs in Nonlinear Models", "comments": "25 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fisher (1934) argued that certain ancillary statistics form a relevant\nsubset, a subset of the sample space on which inference should be restricted,\nand showed that conditioning on their observed value reduces the dimension of\nthe data without a loss of information. The use of ancillary statistics in\npost-data inference has received significant attention; however, their role in\nthe design of the experiment has not been well characterized. Ancillary\nstatistics are unknown prior to data collection and as a result cannot be\nincorporated into the design a priori. However, if the data are observed\nsequentially then the ancillary statistics based on the data from the preceding\nobservations can be used to determine the design assignment for the current\nobservation. The main results of this work describe the benefits of\nincorporating ancillary statistics, specifically, the ancillary statistic that\nconstitutes a relevant subset, into an adaptive design.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 16:22:00 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lane", "Adam", ""]]}, {"id": "2106.09702", "submitter": "Tyler McCormick", "authors": "Shane Lubold and Bolun Liu and Tyler H. McCormick", "title": "Spectral goodness-of-fit tests for complete and partial network data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SI stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Networks describe the, often complex, relationships between individual\nactors. In this work, we address the question of how to determine whether a\nparametric model, such as a stochastic block model or latent space model, fits\na dataset well and will extrapolate to similar data. We use recent results in\nrandom matrix theory to derive a general goodness-of-fit test for dyadic data.\nWe show that our method, when applied to a specific model of interest, provides\nan straightforward, computationally fast way of selecting parameters in a\nnumber of commonly used network models. For example, we show how to select the\ndimension of the latent space in latent space models. Unlike other network\ngoodness-of-fit methods, our general approach does not require simulating from\na candidate parametric model, which can be cumbersome with large graphs, and\neliminates the need to choose a particular set of statistics on the graph for\ncomparison. It also allows us to perform goodness-of-fit tests on partial\nnetwork data, such as Aggregated Relational Data. We show with simulations that\nour method performs well in many situations of interest. We analyze several\nempirically relevant networks and show that our method leads to improved\ncommunity detection algorithms. R code to implement our method is available on\nGithub.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 17:56:30 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Lubold", "Shane", ""], ["Liu", "Bolun", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "2106.09762", "submitter": "Gianluca Detommaso", "authors": "Gianluca Detommaso, Michael Br\\\"uckner, Philip Schulz, Victor\n  Chernozhukov", "title": "Causal Bias Quantification for Continuous Treatment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we develop a novel characterization of marginal causal effect\nand causal bias in the continuous treatment setting. We show they can be\nexpressed as an expectation with respect to a conditional probability\ndistribution, which can be estimated via standard statistical and probabilistic\nmethods. All terms in the expectations can be computed via automatic\ndifferentiation, also for highly non-linear models. We further develop a new\ncomplete criterion for identifiability of causal effects via covariate\nadjustment, showing the bias equals zero if the criterion is met. We study the\neffectiveness of our framework in three different scenarios: linear models\nunder confounding, overcontrol and endogenous selection bias; a non-linear\nmodel where full identifiability cannot be achieved because of missing data; a\nsimulated medical study of statins and atherosclerotic cardiovascular disease.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 18:44:48 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Detommaso", "Gianluca", ""], ["Br\u00fcckner", "Michael", ""], ["Schulz", "Philip", ""], ["Chernozhukov", "Victor", ""]]}, {"id": "2106.09769", "submitter": "Mohamed Chaouch", "authors": "Mohamed Chaouch, Na\\^amane La\\\"ib", "title": "Generalized regression operator estimation for continuous time\n  functional data processes with missing at random response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we are interested in nonparametric kernel estimation of a\ngeneralized regression function, including conditional cumulative distribution\nand conditional quantile functions, based on an incomplete sample $(X_t, Y_t,\n\\zeta_t)_{t\\in \\mathbb{ R}^+}$ copies of a continuous-time stationary ergodic\nprocess $(X, Y, \\zeta)$. The predictor $X$ is valued in some\ninfinite-dimensional space, whereas the real-valued process $Y$ is observed\nwhen $\\zeta= 1$ and missing whenever $\\zeta = 0$. Pointwise and uniform\nconsistency (with rates) of these estimators as well as a central limit theorem\nare established. Conditional bias and asymptotic quadratic error are also\nprovided. Asymptotic and bootstrap-based confidence intervals for the\ngeneralized regression function are also discussed. A first simulation study is\nperformed to compare the discrete-time to the continuous-time estimations. A\nsecond simulation is also conducted to discuss the selection of the optimal\nsampling mesh in the continuous-time case. Finally, it is worth noting that our\nresults are stated under ergodic assumption without assuming any classical\nmixing conditions.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 18:56:59 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Chaouch", "Mohamed", ""], ["La\u00efb", "Na\u00e2mane", ""]]}, {"id": "2106.09845", "submitter": "Han Du", "authors": "Han Du and Peter M. Bentler", "title": "Distributionally Weighted Least Squares in Structural Equation Modeling", "comments": null, "journal-ref": null, "doi": "10.1037/met0000388", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real data analysis with structural equation modeling, data are unlikely to\nbe exactly normally distributed. If we ignore the non-normality reality, the\nparameter estimates, standard error estimates, and model fit statistics from\nnormal theory based methods such as maximum likelihood (ML) and normal theory\nbased generalized least squares estimation (GLS) are unreliable. On the other\nhand, the asymptotically distribution free (ADF) estimator does not rely on any\ndistribution assumption but cannot demonstrate its efficiency advantage with\nsmall and modest sample sizes. The methods which adopt misspecified loss\nfunctions including ridge GLS (RGLS) can provide better estimates and\ninferences than the normal theory based methods and the ADF estimator in some\ncases. We propose a distributionally-weighted least squares (DLS) estimator,\nand expect that it can perform better than the existing generalized least\nsquares, because it combines normal theory based and ADF based generalized\nleast squares estimation. Computer simulation results suggest that\nmodel-implied covariance based DLS (DLS_M) provided relatively accurate and\nefficient estimates in terms of RMSE. In addition, the empirical standard\nerrors, the relative biases of standard error estimates, and the Type I error\nrates of the Jiang-Yuan rank adjusted model fit test statistic (T_JY) in DLS_M\nwere competitive with the classical methods including ML, GLS, and RGLS. The\nperformance of DLS_M depends on its tuning parameter a. We illustrate how to\nimplement DLS_M and select the optimal a by a bootstrap procedure in a real\ndata example.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2021 23:17:25 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Du", "Han", ""], ["Bentler", "Peter M.", ""]]}, {"id": "2106.10057", "submitter": "Alexander Wolfgang Jung", "authors": "Alexander W. Jung and Moritz Gerstung", "title": "Bayesian Cox Regression for Population-scale Inference in Electronic\n  Health Records", "comments": "35 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cox model is an indispensable tool for time-to-event analysis,\nparticularly in biomedical research. However, medicine is undergoing a profound\ntransformation, generating data at an unprecedented scale, which opens new\nfrontiers to study and understand diseases. With the wealth of data collected,\nnew challenges for statistical inference arise, as datasets are often high\ndimensional, exhibit an increasing number of measurements at irregularly spaced\ntime points, and are simply too large to fit in memory. Many current\nimplementations for time-to-event analysis are ill-suited for these problems as\ninference is computationally demanding and requires access to the full data at\nonce. Here we propose a Bayesian version for the counting process\nrepresentation of Cox's partial likelihood for efficient inference on\nlarge-scale datasets with millions of data points and thousands of\ntime-dependent covariates. Through the combination of stochastic variational\ninference and a reweighting of the log-likelihood, we obtain an approximation\nfor the posterior distribution that factorizes over subsamples of the data,\nenabling the analysis in big data settings. Crucially, the method produces\nviable uncertainty estimates for large-scale and high-dimensional datasets. We\nshow the utility of our method through a simulation study and an application to\nmyocardial infarction in the UK Biobank.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 11:13:05 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Jung", "Alexander W.", ""], ["Gerstung", "Moritz", ""]]}, {"id": "2106.10107", "submitter": "Jean-Paul Fox", "authors": "Jean-Paul Fox and Wouter Smink", "title": "Assessing an Alternative for `Negative Variance Components': A Gentle\n  Introduction to Bayesian Covariance Structure Modelling for Negative\n  Associations Among Patients with Personalized Treatments", "comments": "4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The multilevel model (MLM) is the popular approach to describe dependences of\nhierarchically clustered observations. A main feature is the capability to\nestimate (cluster-specific) random effect parameters, while their distribution\ndescribes the variation across clusters. However, the MLM can only model\npositive associations among clustered observations, and it is not suitable for\nsmall sample sizes. The limitation of the MLM becomes apparent when estimation\nmethods produce negative estimates for random effect variances, which can be\nseen as an indication that observations are negatively correlated. A gentle\nintroduction to Bayesian Covariance Structure Modelling (BCSM) is given, which\nmakes it possible to model also negatively correlated observations. The BCSM\ndoes not model dependences through random (cluster-specific) effects, but\nthrough a covariance matrix. We show that this makes the BCSM particularly\nuseful for small data samples. We draw specific attention to detect effects of\na personalized intervention. The effect of a personalized treatment can differ\nacross individuals, and this can lead to negative associations among\nmeasurements of individuals who are treated by the same therapist. It is shown\nthat the BCSM enables the modeling of negative associations among clustered\nmeasurements and aids in the interpretation of negative clustering effects.\nThrough a simulation study and by analysis of a real data example, we discuss\nthe suitability of the BCSM for small data sets and for exploring effects of\nindividualized treatments, specifically when (standard) MLM software produces\nnegative or zero variance estimates.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 12:59:04 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Fox", "Jean-Paul", ""], ["Smink", "Wouter", ""]]}, {"id": "2106.10135", "submitter": "Liu Zhijun", "authors": "Liu Zhijun, Bai Zhidong, Hu Jiang, Song Haiyan", "title": "CLT for LSS of sample covariance matrices with unbounded dispersions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Under the high-dimensional setting that data dimension and sample size tend\nto infinity proportionally, we derive the central limit theorem (CLT) for\nlinear spectral statistics (LSS) of large-dimensional sample covariance matrix.\nDifferent from existing literature, our results do not require the assumption\nthat the population covariance matrices are bounded. Moreover, many common\nkernel functions in the real data such as logarithmic functions and polynomial\nfunctions are allowed in this paper. In our model, the number of spiked\neigenvalues can be fixed or tend to infinity. One salient feature of the\nasymptotic mean and covariance in our proposed central limit theorem is that it\nis related to the divergence order of the population spectral norm.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 13:56:32 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Zhijun", "Liu", ""], ["Zhidong", "Bai", ""], ["Jiang", "Hu", ""], ["Haiyan", "Song", ""]]}, {"id": "2106.10144", "submitter": "Jean-Paul Fox", "authors": "Jean-Paul Fox, Konrad Klotzke, Ahmet Salih Simsek", "title": "LNIRT: An R Package for Joint Modeling of Response Accuracy and Times", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In \\textit{computer-based testing} it has become standard to collect response\naccuracy (RA) and response times (RTs) for each test item. IRT models are used\nto measure a latent variable (e.g., ability, intelligence) using the RA\nobservations. The information in the RTs can help to improve routine operations\nin (educational) testing, and provide information about speed of working. In\nmodern applications, the joint models are needed to integrate RT information in\na test analysis. The R-package LNIRT supports fitting joint models through a\nuser-friendly setup which only requires specifying RA, RT data, and the total\nnumber of Gibbs sampling iterations. More detailed specifications of the\nanalysis are optional. The main results can be reported through the summary\nfunctions, but output can also be analysed with Markov chain Monte Carlo (MCMC)\noutput tools (i.e., coda, mcmcse). The main functionality of the LNIRT package\nis illustrated with two real data applications.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 14:18:25 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Fox", "Jean-Paul", ""], ["Klotzke", "Konrad", ""], ["Simsek", "Ahmet Salih", ""]]}, {"id": "2106.10171", "submitter": "Jean-Paul Fox", "authors": "Jean-Paul Fox, Konrad Klotzke, Duco Veen", "title": "Generalized Linear Randomized Response Modeling using GLMMRR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Randomized response (RR) designs are used to collect response data about\nsensitive behaviors (e.g., criminal behavior, sexual desires). The modeling of\nRR data is more complex, since it requires a description of the RR process. For\nthe class of generalized linear mixed models (GLMMs), the RR process can be\nrepresented by an adjusted link function, which relates the expected RR to the\nlinear predictor, for most common RR designs. The package GLMMRR includes\nmodified link functions for four different cumulative distributions (i.e.,\nlogistic, cumulative normal, gumbel, cauchy) for GLMs and GLMMs, where the\npackage lme4 facilitates ML and REML estimation. The mixed modeling framework\nin GLMMRR can be used to jointly analyse data collected under different designs\n(e.g., dual questioning, multilevel, mixed mode, repeated measurements designs,\nmultiple-group designs). The well-known features of the GLM and GLMM (package\nlme4) software are remained, while adding new model-fit tests, residual\nanalyses, and plot functions to give support to a profound RR data analysis.\nData of H\\\"{o}glinger and Jann (2018) and H\\\"{o}glinger, Jann, and Diekmann\n(2014) is used to illustrate the methodology and software.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 15:04:37 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Fox", "Jean-Paul", ""], ["Klotzke", "Konrad", ""], ["Veen", "Duco", ""]]}, {"id": "2106.10173", "submitter": "Kelly Ramsay", "authors": "Kelly Ramsay, Shojaeddin Chenouri", "title": "Robust nonparametric hypothesis tests for differences in the covariance\n  structure of functional data", "comments": "40 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a group of robust, nonparametric hypothesis tests which detect\ndifferences between the covariance operators of several populations of\nfunctional data. These tests, called FKWC tests, are based on functional data\ndepth ranks. These tests work well even when the data is heavy tailed, which is\nshown both in simulation and theoretically. These tests offer several other\nbenefits, they have a simple distribution under the null hypothesis, they are\ncomputationally cheap and they possess transformation invariance properties. We\nshow that under general alternative hypotheses these tests are consistent under\nmild, nonparametric assumptions. As a result of this work, we introduce a new\nfunctional depth function called L2-root depth which works well for the\npurposes of detecting differences in magnitude between covariance kernels. We\npresent an analysis of the FKWC test using L2-root depth under local\nalternatives. In simulation, when the true covariance kernels have strictly\npositive eigenvalues, we show that these tests have higher power than their\ncompetitors, while still maintaining their nominal size. We also provide a\nmethods for computing sample size and performing multiple comparisons.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 15:05:03 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Ramsay", "Kelly", ""], ["Chenouri", "Shojaeddin", ""]]}, {"id": "2106.10333", "submitter": "Audra McMillan", "authors": "Joerg Drechsler, Ira Globus-Harris, Audra McMillan, Jayshree Sarathy,\n  and Adam Smith", "title": "Non-parametric Differentially Private Confidence Intervals for the\n  Median", "comments": "44 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential privacy is a restriction on data processing algorithms that\nprovides strong confidentiality guarantees for individual records in the data.\nHowever, research on proper statistical inference, that is, research on\nproperly quantifying the uncertainty of the (noisy) sample estimate regarding\nthe true value in the population, is currently still limited. This paper\nproposes and evaluates several strategies to compute valid differentially\nprivate confidence intervals for the median. Instead of computing a\ndifferentially private point estimate and deriving its uncertainty, we directly\nestimate the interval bounds and discuss why this approach is superior if\nensuring privacy is important. We also illustrate that addressing both sources\nof uncertainty--the error from sampling and the error from protecting the\noutput--simultaneously should be preferred over simpler approaches that\nincorporate the uncertainty in a sequential fashion. We evaluate the\nperformance of the different algorithms under various parameter settings in\nextensive simulation studies and demonstrate how the findings could be applied\nin practical settings using data from the 1940 Decennial Census.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 19:45:37 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 18:05:54 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Drechsler", "Joerg", ""], ["Globus-Harris", "Ira", ""], ["McMillan", "Audra", ""], ["Sarathy", "Jayshree", ""], ["Smith", "Adam", ""]]}, {"id": "2106.10364", "submitter": "Chelsea Krantsevich", "authors": "Chelsea Krantsevich, P. Richard Hahn, Yi Zheng and Charles Katz", "title": "Bayesian decision theory for tree-based adaptive screening tests with an\n  application to youth delinquency", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Crime prevention strategies based on early intervention depend on accurate\nrisk assessment instruments for identifying high risk youth. It is important in\nthis context that the instruments be convenient to administer, which means, in\nparticular, that they must be reasonably brief; adaptive screening tests are\nuseful for this purpose. Although item response theory (IRT) bears a long and\nrich history in producing reliable adaptive tests, adaptive tests constructed\nusing classification and regression trees are becoming a popular alternative to\nthe traditional IRT approach for item selection. On the upside, unlike IRT,\ntree-based questionnaires require no real-time parameter estimation during\nadministration. On the downside, while item response theory provides robust\ncriteria for terminating the exam, the stopping criterion for a tree-based\nadaptive test (the maximum tree depth) is unclear. We present a Bayesian\ndecision theory approach for characterizing the trade-offs of administering\ntree-based questionnaires of different lengths. This formalism involves\nspecifying 1) a utility function measuring the goodness of the assessment; 2) a\ntarget population over which this utility should be maximized; 3) an action\nspace comprised of different-length assessments, populated via a tree-fitting\nalgorithm. Using this framework, we provide uncertainty estimates for the\ntrade-offs of shortening the exam, allowing practitioners to determine an\noptimal exam length in a principled way. The method is demonstrated through an\napplication to youth delinquency risk assessment in Honduras.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 21:40:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Krantsevich", "Chelsea", ""], ["Hahn", "P. Richard", ""], ["Zheng", "Yi", ""], ["Katz", "Charles", ""]]}, {"id": "2106.10383", "submitter": "Lorenzo Cappello", "authors": "Lorenzo Cappello and Oscar Hernan Madrid Padilla and Julia A. Palacios", "title": "Scalable Bayesian change point detection with spike and slab priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We study the use of spike and slab priors for consistent estimation of the\nnumber of change points and their locations. Leveraging recent results in the\nvariable selection literature, we show that an estimator based on spike and\nslab priors achieves optimal localization rate in the multiple offline change\npoint detection problem. Based on this estimator, we propose a Bayesian change\npoint detection method, which is one of the fastest Bayesian methodologies, and\nit is more robust to misspecification of the error terms than the competing\nmethods. We demonstrate through empirical work the good performance of our\napproach vis-a-vis some state-of-the-art benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 23:03:39 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Cappello", "Lorenzo", ""], ["Padilla", "Oscar Hernan Madrid", ""], ["Palacios", "Julia A.", ""]]}, {"id": "2106.10387", "submitter": "Ning Ning", "authors": "Ning Ning and Edward L. Ionides", "title": "Systemic Infinitesimal Over-dispersion on General Stochastic Graphical\n  Models", "comments": "47 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic models of interacting populations have crucial roles in scientific\nfields such as epidemiology and ecology, yet the standard approach to extending\nan ordinary differential equation model to a Markov chain does not have\nsufficient flexibility in the mean-variance relationship to match data (e.g.\n\\cite{bjornstad2001noisy}). A previous theory on time-homogeneous dynamics over\na single arrow by \\cite{breto2011compound} showed how gamma white noise could\nbe used to construct certain over-dispersed Markov chains, leading to widely\nused models (e.g. \\cite{breto2009time,he2010plug}). In this paper, we define\nsystemic infinitesimal over-dispersion, developing theory and methodology for\ngeneral time-inhomogeneous stochastic graphical models. Our approach, based on\nDirichlet noise, leads to a new class of Markov models over general direct\ngraphs. It is compatible with modern likelihood-based inference methodologies\n(e.g. \\cite{ionides2006inference,ionides2015inference,king2008inapparent}) and\ntherefore we can assess how well the new models fit data. We demonstrate our\nmethodology on a widely analyzed measles dataset, adding Dirichlet noise to a\nclassical SEIR (Susceptible-Exposed-Infected-Recovered) model. We find that the\nproposed methodology has higher log-likelihood than the gamma white noise\napproach, and the resulting parameter estimations provide new insights into the\nover-dispersion of this biological system.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 23:29:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ning", "Ning", ""], ["Ionides", "Edward L.", ""]]}, {"id": "2106.10398", "submitter": "Roberto Vila Gabriel", "authors": "Cira E. G. Otiniano, Roberto Vila, Pedro C. Brom, and Marcelo\n  Bourguignon", "title": "On the bimodal Gumbel model with application to environmental data", "comments": "23 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Gumbel model is a very popular statistical model due to its wide\napplicability for instance in the course of certain survival, environmental,\nfinancial or reliability studies. In this work, we have introduced a bimodal\ngeneralization of the Gumbel distribution that can be an alternative to model\nbimodal data. We derive the analytical shapes of the corresponding probability\ndensity function and the hazard rate function and provide graphical\nillustrations. Furthermore, We have discussed the properties of this density\nsuch as mode, bimodality, moment generating function and moments. Our results\nwere verified using the Markov chain Monte Carlo simulation method. The maximum\nlikelihood method is used for parameters estimation. Finally, we also carry out\nan application to real data that demonstrates the usefulness of the proposed\ndistribution.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 00:30:25 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Otiniano", "Cira E. G.", ""], ["Vila", "Roberto", ""], ["Brom", "Pedro C.", ""], ["Bourguignon", "Marcelo", ""]]}, {"id": "2106.10462", "submitter": "Roman Flury", "authors": "Roman Flury and Reinhard Furrer", "title": "Discussion on Competition for Spatial Statistics for Large Datasets", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": "10.1007/s13253-021-00461-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the experiences and results of the AppStatUZH team's participation\nin the comprehensive and unbiased comparison of different spatial\napproximations conducted in the Competition for Spatial Statistics for Large\nDatasets. In each of the different sub-competitions, we estimated parameters of\nthe covariance model based on a likelihood function and predicted missing\nobservations with simple kriging. We approximated the covariance model either\nwith covariance tapering or a compactly supported Wendland covariance function.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 09:45:34 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Flury", "Roman", ""], ["Furrer", "Reinhard", ""]]}, {"id": "2106.10477", "submitter": "Philipp Otto", "authors": "Philipp Otto and Wolfgang Schmid", "title": "Generalized Spatial and Spatiotemporal ARCH Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In time-series analyses, particularly for finance, generalized autoregressive\nconditional heteroscedasticity (GARCH) models are widely applied statistical\ntools for modelling volatility clusters (i.e., periods of increased or\ndecreased risk). In contrast, it has not been considered to be of critical\nimportance until now to model spatial dependence in the conditional second\nmoments. Only a few models have been proposed for modelling local clusters of\nincreased risks. In this paper, we introduce a novel spatial GARCH process in a\nunified spatial and spatiotemporal GARCH framework, which also covers all\npreviously proposed spatial ARCH models, exponential spatial GARCH, and\ntime-series GARCH models. In contrast to previous spatiotemporal and time\nseries models, this spatial GARCH allows for instantaneous spill-overs across\nall spatial units. For this common modelling framework, estimators are derived\nbased on a non-linear least-squares approach. Eventually, the use of the model\nis demonstrated by a Monte Carlo simulation study and by an empirical example\nthat focuses on real estate prices from 1995 to 2014 across the ZIP-Code areas\nof Berlin. A spatial autoregressive model is applied to the data to illustrate\nhow locally varying model uncertainties (e.g., due to latent regressors) can be\ncaptured by the spatial GARCH-type models.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 11:50:39 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Otto", "Philipp", ""], ["Schmid", "Wolfgang", ""]]}, {"id": "2106.10496", "submitter": "Anthony Davison C.", "authors": "Anthony C. Davison and Nancy Reid", "title": "The Tangent Exponential Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The likelihood function is central to both frequentist and Bayesian\nformulations of parametric statistical inference, and large-sample\napproximations to the sampling distributions of estimators and test statistics,\nand to posterior densities, are widely used in practice. Improved\napproximations have been widely studied and can provide highly accurate\ninferences when samples are small or there are many nuisance parameters. This\narticle reviews improved approximations based on the tangent exponential model\ndeveloped in a series of articles by D.~A.~S.~Fraser and co-workers, attempting\nto explain the theoretical basis of this model and to provide a guide to the\nassociated literature, including a partially-annotated bibliography.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 13:27:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Davison", "Anthony C.", ""], ["Reid", "Nancy", ""]]}, {"id": "2106.10503", "submitter": "Shonosuke Sugasawa", "authors": "Yasuyuki Hamura, Kaoru Irie and Shonosuke Sugasawa", "title": "Robust Hierarchical Modeling of Counts under Zero-inflation and Outliers", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count data with zero inflation and large outliers are ubiquitous in many\nscientific applications. However, the posterior analysis under a standard\nstatistical model such as Poisson or negative binomial distribution is\nsensitive to such contamination. This paper introduces a novel framework for\nBayesian modeling of counts robust to both zeros inflation and large outliers.\nIn doing so, we introduce the rescaled beta distribution and adopt it to absorb\nundesirable effects from zero and outlying counts. The proposed approach has\ntwo appealing features: the efficiency of the posterior computation via a\ncustom Gibbs sampling algorithm, and the theoretical posterior robustness,\nwhere the extreme outliers are automatically removed from the posterior\ndistribution. We demonstrate the usefulness of the proposed method through\nsimulation and real data applications.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 14:33:04 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hamura", "Yasuyuki", ""], ["Irie", "Kaoru", ""], ["Sugasawa", "Shonosuke", ""]]}, {"id": "2106.10539", "submitter": "Elan Ness-Cohn", "authors": "Elan Ness-Cohn and Rosemary Braun", "title": "Fasano-Franceschini Test: an Implementation of a 2-Dimensional\n  Kolmogorov-Smirnov test in R", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM physics.data-an q-bio.QM stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The univariate Kolmogorov-Smirnov (KS) test is a non-parametric statistical\ntest designed to assess whether a set of data is consistent with a given\nprobability distribution (or, in the two-sample case, whether the two samples\ncome from the same underlying distribution). The versatility of the KS test has\nmade it a cornerstone of statistical analysis and is commonly used across the\nscientific disciplines. However, the test proposed by Kolmogorov and Smirnov\ndoes not naturally extend to multidimensional distributions. Here, we present\nthe fasano.franceschini.test package, an R implementation of the 2-D KS\ntwo-sample test as defined by Fasano and Franceschini (Fasano and Franceschini\n1987). The fasano.franceschini.test package provides three improvements over\nthe current 2-D KS test on the Comprehensive R Archive Network (CRAN): (i) the\nFasano and Franceschini test has been shown to run in $O(n^2)$ versus the\nPeacock implementation which runs in $O(n^3)$; (ii) the package implements a\nprocedure for handling ties in the data; and (iii) the package implements a\nparallelized bootstrapping procedure for improved significance testing.\nUltimately, the fasano.franceschini.test package presents a robust statistical\ntest for analyzing random samples defined in 2-dimensions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 17:28:52 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ness-Cohn", "Elan", ""], ["Braun", "Rosemary", ""]]}, {"id": "2106.10571", "submitter": "Harrison Quick", "authors": "Guangzi Song, Loni Philip Tabb, and Harrison Quick", "title": "Geographic and Racial Disparities in the Incidence of Low Birthweight in\n  Pennsylvania", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Babies born with low and very low birthweights -- i.e., birthweights below\n2,500 and 1,500 grams, respectively -- have an increased risk of complications\ncompared to other babies, and the proportion of babies with a low birthweight\nis a common metric used when evaluating public health in a population. While\nmany factors increase the risk of a baby having a low birthweight, many can be\nlinked to the mother's socioeconomic status, which in turn contributes to large\nracial disparities in the incidence of low weight births. Here, we employ\nBayesian statistical models to analyze the proportion of babies with low\nbirthweight in Pennsylvania counties by race/ethnicity. Due to the small number\nof births -- and low weight births -- in many Pennsylvania counties when\nstratified by race/ethnicity, our methods must walk a fine line. On one hand,\nleveraging spatial structure can help improve the precision of our estimates.\nOn the other hand, we must be cautious to avoid letting the model overwhelm the\ninformation in the data and produce spurious conclusions. As such, we first\ndevelop a framework by which we can measure (and control) the informativeness\nof our spatial model. After demonstrating the properties of our framework via\nsimulation, we analyze the low birthweight data from Pennsylvania and examine\nthe extent to which the commonly used conditional autoregressive model can lead\nto oversmoothing. We then reanalyze the data using our proposed framework and\nhighlight its ability to detect (or not detect) evidence of racial disparities\nin the incidence of low birthweight.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 20:59:22 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Song", "Guangzi", ""], ["Tabb", "Loni Philip", ""], ["Quick", "Harrison", ""]]}, {"id": "2106.10577", "submitter": "Noah Greifer", "authors": "Noah Greifer, Elizabeth A. Stuart", "title": "Choosing the Estimand When Matching or Weighting in Observational\n  Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Matching and weighting methods for observational studies require the choice\nof an estimand, the causal effect with reference to a specific target\npopulation. Commonly used estimands include the average treatment effect in the\ntreated (ATT), the average treatment effect in the untreated (ATU), the average\ntreatment effect in the population (ATE), and the average treatment effect in\nthe overlap (i.e., equipoise population; ATO). Each estimand has its own\nassumptions, interpretation, and statistical methods that can be used to\nestimate it. This article provides guidance on selecting and interpreting an\nestimand to help medical researchers correctly implement statistical methods\nused to estimate causal effects in observational studies and to help audiences\ncorrectly interpret the results and limitations of these studies. The\ninterpretations of the estimands resulting from regression and instrumental\nvariable analyses are also discussed. Choosing an estimand carefully is\nessential for making valid inferences from the analysis of observational data\nand ensuring results are replicable and useful for practitioners.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 22:00:05 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Greifer", "Noah", ""], ["Stuart", "Elizabeth A.", ""]]}, {"id": "2106.10583", "submitter": "Pang Du", "authors": "Yunnan Xu, Pang Du, John Robertson and Ryan Senger", "title": "Sparse logistic regression on functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Motivated by a hemodialysis monitoring study, we propose a logistic model\nwith a functional predictor, called the Sparse Functional Logistic Regression\n(SFLR), where the corresponding coefficient function is {\\it locally sparse},\nthat is, it is completely zero on some subregions of its domain. The\ncoefficient function, together with the intercept parameter, are estimated\nthrough a doubly-penalized likelihood approach with a B-splines expansion. One\npenalty is for controlling the roughness of the coefficient function estimate\nand the other penalty, in the form of the $L_1$ norm, enforces the local\nsparsity. A Newton-Raphson procedure is designed for the optimization of the\npenalized likelihood. Our simulations show that SFLR is capable of generating a\nsmooth and reasonably good estimate of the coefficient function on the non-null\nregion(s) while recognizing the null region(s). Application of the method to\nthe Raman spectral data generated from the heomdialysis study pinpoint the\nwavenumber regions for identifying key chemicals contributing to the dialysis\nprogress.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 22:50:00 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Xu", "Yunnan", ""], ["Du", "Pang", ""], ["Robertson", "John", ""], ["Senger", "Ryan", ""]]}, {"id": "2106.10624", "submitter": "Zheng Chen", "authors": "Jingjing Lyu, Yawen Hou, Zheng Chen", "title": "Combined tests based on restricted mean time lost for competing risks\n  data", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "SBR-20-016", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competing risks data are common in medical studies, and the sub-distribution\nhazard (SDH) ratio is considered an appropriate measure. However, because the\nlimitations of hazard itself are not easy to interpret clinically and because\nthe SDH ratio is valid only under the proportional SDH assumption, this article\nintroduced an alternative index under competing risks, named restricted mean\ntime lost (RMTL). Several test procedures were also constructed based on RMTL.\nFirst, we introduced the definition and estimation of RMTL based on\nAalen-Johansen cumulative incidence functions. Then, we considered several\ncombined tests based on the SDH and the RMTL difference (RMTLd). The\nstatistical properties of the methods are evaluated using simulations and are\napplied to two examples. The type I errors of combined tests are close to the\nnominal level. All combined tests show acceptable power in all situations. In\nconclusion, RMTL can meaningfully summarize treatment effects for clinical\ndecision making, and three combined tests have robust power under various\nconditions, which can be considered for statistical inference in real data\nanalysis.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 05:06:12 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Lyu", "Jingjing", ""], ["Hou", "Yawen", ""], ["Chen", "Zheng", ""]]}, {"id": "2106.10625", "submitter": "Zheng Chen", "authors": "Zijing Yang, Hongji Wu, Yawen Hou, Hao Yuan, Zheng Chen", "title": "Dynamic prediction and analysis based on restricted mean survival time\n  in survival analysis with nonproportional hazards", "comments": "25 pages, 5 figures", "journal-ref": "Computer Methods and Programs in Biomedicine. 2021, 207: 106155", "doi": "10.1016/j.cmpb.2021.106155", "report-no": "207:106155", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the process of clinical diagnosis and treatment, the restricted mean\nsurvival time (RMST), which reflects the life expectancy of patients up to a\nspecified time, can be used as an appropriate outcome measure. However, the\nRMST only calculates the mean survival time of patients within a period of time\nafter the start of follow-up and may not accurately portray the change in a\npatient's life expectancy over time. The life expectancy can be adjusted for\nthe time the patient has already survived and defined as the conditional\nrestricted mean survival time (cRMST). A dynamic RMST model based on the cRMST\ncan be established by incorporating time-dependent covariates and covariates\nwith time-varying effects. We analysed data from a study of primary biliary\ncirrhosis (PBC) to illustrate the use of the dynamic RMST model. The predictive\nperformance was evaluated using the C-index and the prediction error. The\nproposed dynamic RMST model, which can explore the dynamic effects of\nprognostic factors on survival time, has better predictive performance than the\nRMST model. Three PBC patient examples were used to illustrate how the\npredicted cRMST changed at different prediction times during follow-up. The use\nof the dynamic RMST model based on the cRMST allows for optimization of\nevidence-based decision-making by updating personalized dynamic life expectancy\nfor patients.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 05:07:29 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Yang", "Zijing", ""], ["Wu", "Hongji", ""], ["Hou", "Yawen", ""], ["Yuan", "Hao", ""], ["Chen", "Zheng", ""]]}, {"id": "2106.10660", "submitter": "Yu Luo", "authors": "Yu Luo, David A. Stephens", "title": "Bayesian inference for continuous-time hidden Markov models with an\n  unknown number of states", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the modeling of data generated by a latent continuous-time Markov\njump process with a state space of finite but unknown dimensions. Typically in\nsuch models, the number of states has to be pre-specified, and Bayesian\ninference for a fixed number of states has not been studied until recently. In\naddition, although approaches to address the problem for discrete-time models\nhave been developed, no method has been successfully implemented for the\ncontinuous-time case. We focus on reversible jump Markov chain Monte Carlo\nwhich allows the trans-dimensional move among different numbers of states in\norder to perform Bayesian inference for the unknown number of states.\nSpecifically, we propose an efficient split-combine move which can facilitate\nthe exploration of the parameter space, and demonstrate that it can be\nimplemented effectively at scale. Subsequently, we extend this algorithm to the\ncontext of model-based clustering, allowing numbers of states and clusters both\ndetermined during the analysis. The model formulation, inference methodology,\nand associated algorithm are illustrated by simulation studies. Finally, We\napply this method to real data from a Canadian healthcare system in Quebec.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 09:13:55 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Luo", "Yu", ""], ["Stephens", "David A.", ""]]}, {"id": "2106.10720", "submitter": "Yunji Zhou", "authors": "Yunji Zhou, Elizabeth L. Turner, Ryan A. Simmons, Fan Li", "title": "Constrained randomization and statistical inference for multi-arm\n  parallel cluster randomized controlled trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster randomized controlled trials (cRCTs) are designed to evaluate\ninterventions delivered to groups of individuals. A practical limitation of\nsuch designs is that the number of available clusters may be small, resulting\nin an increased risk of baseline imbalance under simple randomization.\nConstrained randomization overcomes this issue by restricting the allocation to\na subset of randomization schemes where sufficient overall covariate balance\nacross comparison arms is achieved with respect to a pre-specified balance\nmetric. However, several aspects of constrained randomization for the design\nand analysis of multi-arm cRCTs have not been fully investigated. Motivated by\nan ongoing multi-arm cRCT, we provide a comprehensive evaluation of the\nstatistical properties of model-based and randomization-based tests under both\nsimple and constrained randomization designs in multi-arm cRCTs, with varying\ncombinations of design and analysis-based covariate adjustment strategies. In\nparticular, as randomization-based tests have not been extensively studied in\nmulti-arm cRCTs, we additionally develop most-powerful permutation tests under\nthe linear mixed model framework for our comparisons. Our results indicate that\nunder constrained randomization, both model-based and randomization-based\nanalyses could gain power while preserving nominal type I error rate, given\nproper analysis-based adjustment for the baseline covariates. The choice of\nbalance metrics and candidate set size and their implications on the testing of\nthe pairwise and global hypotheses are also discussed. Finally, we caution\nagainst the design and analysis of multi-arm cRCTs with an extremely small\nnumber of clusters, due to insufficient degrees of freedom and the tendency to\nobtain an overly restricted randomization space.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 16:15:00 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zhou", "Yunji", ""], ["Turner", "Elizabeth L.", ""], ["Simmons", "Ryan A.", ""], ["Li", "Fan", ""]]}, {"id": "2106.10765", "submitter": "Sundara Rajan Srinivasavaradhan", "authors": "Sundara Rajan Srinivasavaradhan, Pavlos Nikolopoulos, Christina\n  Fragouli and Suhas Diggavi", "title": "Dynamic group testing to control and monitor disease progression in a\n  population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.soc-ph q-bio.PE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the context of a pandemic like COVID-19, and until most people are\nvaccinated, proactive testing and interventions have been proved to be the only\nmeans to contain the disease spread. Recent academic work has offered\nsignificant evidence in this regard, but a critical question is still open: Can\nwe accurately identify all new infections that happen every day, without this\nbeing forbiddingly expensive, i.e., using only a fraction of the tests needed\nto test everyone everyday (complete testing)? Group testing offers a powerful\ntoolset for minimizing the number of tests, but it does not account for the\ntime dynamics behind the infections. Moreover, it typically assumes that people\nare infected independently, while infections are governed by community spread.\nEpidemiology, on the other hand, does explore time dynamics and community\ncorrelations through the well-established continuous-time SIR stochastic\nnetwork model, but the standard model does not incorporate discrete-time\ntesting and interventions. In this paper, we introduce a \"discrete-time SIR\nstochastic block model\" that also allows for group testing and interventions on\na daily basis. Our model can be regarded as a discrete version of the\ncontinuous-time SIR stochastic network model over a specific type of weighted\ngraph that captures the underlying community structure. We analyze that model\nw.r.t. the minimum number of group tests needed everyday to identify all\ninfections with vanishing error probability. We find that one can leverage the\nknowledge of the community and the model to inform nonadaptive group testing\nalgorithms that are order-optimal, and therefore achieve the same performance\nas complete testing using a much smaller number of tests.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jun 2021 22:22:30 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Srinivasavaradhan", "Sundara Rajan", ""], ["Nikolopoulos", "Pavlos", ""], ["Fragouli", "Christina", ""], ["Diggavi", "Suhas", ""]]}, {"id": "2106.10941", "submitter": "Karthik Bharath", "authors": "Shariq Mohammed, Sebastian Kurtek, Karthik Bharath, Arvind Rao and\n  Veerabhadran Baladandayuthapani", "title": "Tumor Radiogenomics with Bayesian Layered Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a statistical framework to integrate radiological magnetic\nresonance imaging (MRI) and genomic data to identify the underlying\nradiogenomic associations in lower grade gliomas (LGG). We devise a novel\nimaging phenotype by dividing the tumor region into concentric spherical layers\nthat mimics the tumor evolution process. MRI data within each layer is\nrepresented by voxel--intensity-based probability density functions which\ncapture the complete information about tumor heterogeneity. Under a\nRiemannian-geometric framework these densities are mapped to a vector of\nprincipal component scores which act as imaging phenotypes. Subsequently, we\nbuild Bayesian variable selection models for each layer with the imaging\nphenotypes as the response and the genomic markers as predictors. Our novel\nhierarchical prior formulation incorporates the interior-to-exterior structure\nof the layers, and the correlation between the genomic markers. We employ a\ncomputationally-efficient Expectation--Maximization-based strategy for\nestimation. Simulation studies demonstrate the superior performance of our\napproach compared to other approaches. With a focus on the cancer driver genes\nin LGG, we discuss some biologically relevant findings. Genes implicated with\nsurvival and oncogenesis are identified as being associated with the spherical\nlayers, which could potentially serve as early-stage diagnostic markers for\ndisease monitoring, prior to routine invasive approaches.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 09:24:40 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Mohammed", "Shariq", ""], ["Kurtek", "Sebastian", ""], ["Bharath", "Karthik", ""], ["Rao", "Arvind", ""], ["Baladandayuthapani", "Veerabhadran", ""]]}, {"id": "2106.10983", "submitter": "Pingfeng Xu", "authors": "Ping-Feng Xu, Lai-Xu Shang, Man-Lai Tang, Na Shan, Guoliang Tian", "title": "A generalized EMS algorithm for model selection with incomplete data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a so-called E-MS algorithm was developed for model selection in the\npresence of missing data. Specifically, it performs the Expectation step (E\nstep) and Model Selection step (MS step) alternately to find the minimum point\nof the observed generalized information criteria (GIC). In practice, it could\nbe numerically infeasible to perform the MS-step for high dimensional settings.\nIn this paper, we propose a more simple and feasible generalized EMS (GEMS)\nalgorithm which simply requires a decrease in the observed GIC in the MS-step\nand includes the original EMS algorithm as a special case. We obtain several\nnumerical convergence results of the GEMS algorithm under mild conditions. We\napply the proposed GEMS algorithm to Gaussian graphical model selection and\nvariable selection in generalized linear models and compare it with existing\ncompetitors via numerical experiments. We illustrate its application with three\nreal data sets.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 11:04:50 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Xu", "Ping-Feng", ""], ["Shang", "Lai-Xu", ""], ["Tang", "Man-Lai", ""], ["Shan", "Na", ""], ["Tian", "Guoliang", ""]]}, {"id": "2106.11009", "submitter": "Helene Charlotte Rytgaard", "authors": "Helene Charlotte Wiese Rytgaard, Frank Eriksson and Mark van der Laan", "title": "Estimation of time-specific intervention effects on continuously\n  distributed time-to-event outcomes by targeted maximum likelihood estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Targeted maximum likelihood estimation is a general methodology combining\nflexible ensemble learning and semiparametric efficiency theory in a two-step\nprocedure for estimation of causal parameters. Proposed targeted maximum\nlikelihood procedures for survival and competing risks analysis have so far\nfocused on events taken values in discrete time. We here present a targeted\nmaximum likelihood estimation procedure for event times that take values in R+.\nWe focuson the estimation of intervention-specific mean outcomes with\nstochastic interventions on a time-fixed treatment. For data-adaptive\nestimation of nuisance parameters, we propose a new flexible highly adaptive\nlasso estimation method for continuous-time intensities that can be implemented\nwith L1-penalized Poisson regression. In a simulation study the targeted\nmaximum likelihood estimator based on the highly adaptive lasso estimator\nproves to be unbiased and achieve proper coverage in agreement with the\nasymptotic theory and further displays efficiency improvements relative to a\nKaplan-Meier approach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 12:04:41 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Rytgaard", "Helene Charlotte Wiese", ""], ["Eriksson", "Frank", ""], ["van der Laan", "Mark", ""]]}, {"id": "2106.11043", "submitter": "Rihui Ou", "authors": "Rihui Ou, Deborshee Sen, David Dunson", "title": "Scalable Bayesian inference for time series via divide-and-conquer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bayesian computational algorithms tend to scale poorly as data size\nincreases. This had led to the development of divide-and-conquer-based\napproaches for scalable inference. These divide the data into subsets, perform\ninference for each subset in parallel, and then combine these inferences. While\nappealing theoretical properties and practical performance have been\ndemonstrated for independent observations, scalable inference for dependent\ndata remains challenging. In this work, we study the problem of Bayesian\ninference from very long time series. The literature in this area focuses\nmainly on approximate approaches that lack any theoretical guarantees and may\nprovide arbitrarily poor accuracy in practice. We propose a simple and scalable\ndivide-and-conquer method, and provide accuracy guarantees. Numerical\nsimulations and real data applications demonstrate the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 12:29:02 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 20:53:26 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Ou", "Rihui", ""], ["Sen", "Deborshee", ""], ["Dunson", "David", ""]]}, {"id": "2106.11104", "submitter": "Timo Dimitriadis", "authors": "Yannick Hoga, Timo Dimitriadis", "title": "On Testing Equal Conditional Predictive Ability Under Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Loss functions are widely used to compare several competing forecasts.\nHowever, forecast comparisons are often based on mismeasured proxy variables\nfor the true target. We introduce the concept of exact robustness to\nmeasurement error for loss functions and fully characterize this class of loss\nfunctions as the Bregman class. For such exactly robust loss functions,\nforecast loss differences are on average unaffected by the use of proxy\nvariables and, thus, inference on conditional predictive ability can be carried\nout as usual. Moreover, we show that more precise proxies give predictive\nability tests higher power in discriminating between competing forecasts.\nSimulations illustrate the different behavior of exactly robust and non-robust\nloss functions. An empirical application to US GDP growth rates demonstrates\nthat it is easier to discriminate between forecasts issued at different\nhorizons if a better proxy for GDP growth is used.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 13:39:22 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hoga", "Yannick", ""], ["Dimitriadis", "Timo", ""]]}, {"id": "2106.11180", "submitter": "Yibo Zeng", "authors": "Henry Lam, Yibo Zeng", "title": "Complexity-Free Generalization via Distributionally Robust Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Established approaches to obtain generalization bounds in data-driven\noptimization and machine learning mostly build on solutions from empirical risk\nminimization (ERM), which depend crucially on the functional complexity of the\nhypothesis class. In this paper, we present an alternate route to obtain these\nbounds on the solution from distributionally robust optimization (DRO), a\nrecent data-driven optimization framework based on worst-case analysis and the\nnotion of ambiguity set to capture statistical uncertainty. In contrast to the\nhypothesis class complexity in ERM, our DRO bounds depend on the ambiguity set\ngeometry and its compatibility with the true loss function. Notably, when using\nmaximum mean discrepancy as a DRO distance metric, our analysis implies, to the\nbest of our knowledge, the first generalization bound in the literature that\ndepends solely on the true loss function, entirely free of any complexity\nmeasures or bounds on the hypothesis class.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:19:52 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Lam", "Henry", ""], ["Zeng", "Yibo", ""]]}, {"id": "2106.11188", "submitter": "Riccardo Fogliato", "authors": "Riccardo Fogliato, Shamindra Shrotriya, Arun Kumar Kuchibhotla", "title": "maars: Tidy Inference under the 'Models as Approximations' Framework in\n  R", "comments": "The first two authors contributed equally to this work and are\n  ordered alphabetically", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression using ordinary least squares (OLS) is a critical part of\nevery statistician's toolkit. In R, this is elegantly implemented via lm() and\nits related functions. However, the statistical inference output from this\nsuite of functions is based on the assumption that the model is well specified.\nThis assumption is often unrealistic and at best satisfied approximately. In\nthe statistics and econometrics literature, this has long been recognized and a\nlarge body of work provides inference for OLS under more practical assumptions.\nThis can be seen as model-free inference. In this paper, we introduce our\npackage maars (\"models as approximations\") that aims at bringing research on\nmodel-free inference to R via a comprehensive workflow. The maars package\ndiffers from other packages that also implement variance estimation, such as\nsandwich, in three key ways. First, all functions in maars follow a consistent\ngrammar and return output in tidy format, with minimal deviation from the\ntypical lm() workflow. Second, maars contains several tools for inference\nincluding empirical, multiplier, residual bootstrap, and subsampling, for easy\ncomparison. Third, maars is developed with pedagogy in mind. For this, most of\nits functions explicitly return the assumptions under which the output is\nvalid. This key innovation makes maars useful in teaching inference under\nmisspecification and also a powerful tool for applied researchers. We hope our\ndefault feature of explicitly presenting assumptions will become a de facto\nstandard for most statistical modeling in R.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 15:27:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Fogliato", "Riccardo", ""], ["Shrotriya", "Shamindra", ""], ["Kuchibhotla", "Arun Kumar", ""]]}, {"id": "2106.11389", "submitter": "Jean Pauphilet", "authors": "Jean Pauphilet", "title": "Robust and Heterogenous Odds Ratio: Estimating Price Sensitivity for\n  Unbought Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Problem definition: Mining for heterogeneous responses to an intervention is\na crucial step for data-driven operations, for instance to personalize\ntreatment or pricing. We investigate how to estimate price sensitivity from\ntransaction-level data. In causal inference terms, we estimate heterogeneous\ntreatment effects when (a) the response to treatment (here, whether a customer\nbuys a product) is binary, and (b) treatment assignments are partially observed\n(here, full information is only available for purchased items).\nMethodology/Results: We propose a recursive partitioning procedure to estimate\nheterogeneous odds ratio, a widely used measure of treatment effect in medicine\nand social sciences. We integrate an adversarial imputation step to allow for\nrobust inference even in presence of partially observed treatment assignments.\nWe validate our methodology on synthetic data and apply it to three case\nstudies from political science, medicine, and revenue management. Managerial\nImplications: Our robust heterogeneous odds ratio estimation method is a simple\nand intuitive tool to quantify heterogeneity in patients or customers and\npersonalize interventions, while lifting a central limitation in many revenue\nmanagement data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 19:50:32 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Pauphilet", "Jean", ""]]}, {"id": "2106.11415", "submitter": "Shubhadeep Chakraborty", "authors": "Shubhadeep Chakraborty and Ali Shojaie", "title": "Nonparametric causal structure learning in high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The PC and FCI algorithms are popular constraint-based methods for learning\nthe structure of directed acyclic graphs (DAGs) in the absence and presence of\nlatent and selection variables, respectively. These algorithms (and their\norder-independent variants, PC-stable and FCI-stable) have been shown to be\nconsistent for learning sparse high-dimensional DAGs based on partial\ncorrelations. However, inferring conditional independences from partial\ncorrelations is valid if the data are jointly Gaussian or generated from a\nlinear structural equation model -- an assumption that may be violated in many\napplications. To broaden the scope of high-dimensional causal structure\nlearning, we propose nonparametric variants of the PC-stable and FCI-stable\nalgorithms that employ the conditional distance covariance (CdCov) to test for\nconditional independence relationships. As the key theoretical contribution, we\nprove that the high-dimensional consistency of the PC-stable and FCI-stable\nalgorithms carry over to general distributions over DAGs when we implement\nCdCov-based nonparametric tests for conditional independence. Numerical studies\ndemonstrate that our proposed algorithms perform nearly as good as the\nPC-stable and FCI-stable for Gaussian distributions, and offer advantages in\nnon-Gaussian graphical models.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2021 21:08:19 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Chakraborty", "Shubhadeep", ""], ["Shojaie", "Ali", ""]]}, {"id": "2106.11540", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Shouto Yonekura", "title": "On Selection Criteria for the Tuning Parameter in Robust Divergence", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While robust divergence such as density power divergence and\n$\\gamma$-divergence is helpful for robust statistical inference in the presence\nof outliers, the tuning parameter that controls the degree of robustness is\nchosen in a rule-of-thumb, which may lead to an inefficient inference. We here\npropose a selection criterion based on an asymptotic approximation of the\nHyvarinen score applied to an unnormalized model defined by robust divergence.\nThe proposed selection criterion only requires first and second-order partial\nderivatives of an assumed density function with respect to observations, which\ncan be easily computed regardless of the number of parameters. We demonstrate\nthe usefulness of the proposed method via numerical studies using normal\ndistributions and regularized linear regression.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 04:30:49 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Yonekura", "Shouto", ""]]}, {"id": "2106.11554", "submitter": "Andersen Chang", "authors": "Andersen Chang and Genevera I. Allen", "title": "Extreme Graphical Models with Applications to Functional Neuronal\n  Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With modern calcium imaging technology, the activities of thousands of\nneurons can be recorded simultaneously in vivo. These experiments can\npotentially provide new insights into functional connectivity, defined as the\nstatistical relationships between the spiking activity of neurons in the brain.\nAs a commonly used tool for estimating conditional dependencies in\nhigh-dimensional settings, graphical models are a natural choice for analyzing\ncalcium imaging data. However, raw neuronal activity recording data presents a\nunique challenge: the important information lies in the rare extreme value\nobservations that indicate neuronal firing, as opposed to the non-extreme\nobservations associated with inactivity. To address this issue, we develop a\nnovel class of graphical models, called the extreme graphical model, which\nfocuses on finding relationships between features with respect to the extreme\nvalues. Our model assumes the conditional distributions a subclass of the\ngeneralized normal or Subbotin distribution, and yields a form of a curved\nexponential family graphical model. We first derive the form of the joint\nmultivariate distribution of the extreme graphical model and show the\nconditions under which it is normalizable. We then demonstrate the model\nselection consistency of our estimation method. Lastly, we study the empirical\nperformance of the extreme graphical model through several simulation studies\nas well as through a real data example, in which we apply our method to a\nreal-world calcium imaging data set.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 06:16:58 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Chang", "Andersen", ""], ["Allen", "Genevera I.", ""]]}, {"id": "2106.11561", "submitter": "Johanna Meier", "authors": "Ziang Niu, Johanna Meier, Fran\\c{c}ois-Xavier Briol", "title": "Discrepancy-based Inference for Intractable Generative Models using\n  Quasi-Monte Carlo", "comments": "minor presentation changes and updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intractable generative models are models for which the likelihood is\nunavailable but sampling is possible. Most approaches to parameter inference in\nthis setting require the computation of some discrepancy between the data and\nthe generative model. This is for example the case for minimum distance\nestimation and approximate Bayesian computation. These approaches require\nsampling a high number of realisations from the model for different parameter\nvalues, which can be a significant challenge when simulating is an expensive\noperation. In this paper, we propose to enhance this approach by enforcing\n\"sample diversity\" in simulations of our models. This will be implemented\nthrough the use of quasi-Monte Carlo (QMC) point sets. Our key results are\nsample complexity bounds which demonstrate that, under smoothness conditions on\nthe generator, QMC can significantly reduce the number of samples required to\nobtain a given level of accuracy when using three of the most common\ndiscrepancies: the maximum mean discrepancy, the Wasserstein distance, and the\nSinkhorn divergence. This is complemented by a simulation study which\nhighlights that an improved accuracy is sometimes also possible in some\nsettings which are not covered by the theory.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 06:36:08 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 14:03:00 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Niu", "Ziang", ""], ["Meier", "Johanna", ""], ["Briol", "Fran\u00e7ois-Xavier", ""]]}, {"id": "2106.11617", "submitter": "Luca Scrucca", "authors": "Luca Scrucca", "title": "Modal clustering on PPGMMGA projection subspace", "comments": "10 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  PPGMMGA is a Projection Pursuit (PP) algorithm aimed at detecting and\nvisualizing clustering structures in multivariate data. The algorithm uses the\nnegentropy as PP index obtained by fitting Gaussian Mixture Models (GMMs) for\ndensity estimation, and then optimized using Genetic Algorithms (GAs). Since\nthe PPGMMGA algorithm is a dimension reduction technique specifically\nintroduced for visualization purposes, cluster memberships are not explicitly\nprovided. In this paper a modal clustering approach is proposed for estimating\nclusters of projected data points. In particular, a modal EM algorithm is\nemployed to estimate the modes corresponding to the local maxima in the\nprojection subspace of the underlying density estimated using parsimonious\nGMMs. Data points are then clustered according to the domain of attraction of\nthe identified modes. Simulated and real data are discussed to illustrate the\nproposed method and evaluate the clustering performance.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 08:57:44 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Scrucca", "Luca", ""]]}, {"id": "2106.11941", "submitter": "Luca Insolia", "authors": "Luca Insolia, Francesca Chiaromonte, Runze Li, Marco Riani", "title": "Doubly Robust Feature Selection with Mean and Variance Outlier Detection\n  and Oracle Properties", "comments": "35 pages, 9 figures (including supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general approach to handle data contaminations that might\ndisrupt the performance of feature selection and estimation procedures for\nhigh-dimensional linear models. Specifically, we consider the co-occurrence of\nmean-shift and variance-inflation outliers, which can be modeled as additional\nfixed and random components, respectively, and evaluated independently. Our\nproposal performs feature selection while detecting and down-weighting\nvariance-inflation outliers, detecting and excluding mean-shift outliers, and\nretaining non-outlying cases with full weights. Feature selection and\nmean-shift outlier detection are performed through a robust class of nonconcave\npenalization methods. Variance-inflation outlier detection is based on the\npenalization of the restricted posterior mode. The resulting approach satisfies\na robust oracle property for feature selection in the presence of data\ncontamination -- which allows the number of features to exponentially increase\nwith the sample size -- and detects truly outlying cases of each type with\nasymptotic probability one. This provides an optimal trade-off between a high\nbreakdown point and efficiency. Computationally efficient heuristic procedures\nare also presented. We illustrate the finite-sample performance of our proposal\nthrough an extensive simulation study and a real-world application.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 17:25:09 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Insolia", "Luca", ""], ["Chiaromonte", "Francesca", ""], ["Li", "Runze", ""], ["Riani", "Marco", ""]]}, {"id": "2106.12001", "submitter": "Nancy Reid", "authors": "Heather S. Battey and Nancy Reid", "title": "Inference in High-dimensional Linear Regression", "comments": "27 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop an approach to inference in a linear regression model when the\nnumber of potential explanatory variables is larger than the sample size. Our\napproach treats each regression coefficient in turn as the interest parameter,\nthe remaining coefficients being nuisance parameters, and seeks an optimal\ninterest-respecting transformation. The role of this transformation is to allow\na marginal least squares analysis for each variable, as in a factorial\nexperiment. One parameterization of the problem is found to be particularly\nconvenient, both computationally and mathematically. In particular, it permits\nan analytic solution to the optimal transformation problem, facilitating\ncomparison to other work. In contrast to regularized regression such as the\nlasso (Tibshirani, 1996) and its extensions, neither adjustment for selection,\nnor rescaling of the explanatory variables is needed, ensuring the physical\ninterpretation of regression coefficients is retained. We discuss the use of\nsuch confidence intervals as part of a broader set of inferential statements,\nso as to reflect uncertainty over the model as well as over the parameters. The\nconsiderations involved in extending the work to other regression models are\nbriefly discussed.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 18:19:10 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Battey", "Heather S.", ""], ["Reid", "Nancy", ""]]}, {"id": "2106.12105", "submitter": "Wenkai Xu", "authors": "Wenkai Xu", "title": "Generalised Kernel Stein Discrepancy(GKSD): A Unifying Approach for\n  Non-parametric Goodness-of-fit Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-parametric goodness-of-fit testing procedures based on kernel Stein\ndiscrepancies (KSD) are promising approaches to validate general unnormalised\ndistributions in various scenarios. Existing works have focused on studying\noptimal kernel choices to boost test performances. However, the Stein operators\nare generally non-unique, while different choices of Stein operators can also\nhave considerable effect on the test performances. In this work, we propose a\nunifying framework, the generalised kernel Stein discrepancy (GKSD), to\ntheoretically compare and interpret different Stein operators in performing the\nKSD-based goodness-of-fit tests. We derive explicitly that how the proposed\nGKSD framework generalises existing Stein operators and their corresponding\ntests. In addition, we show thatGKSD framework can be used as a guide to\ndevelop kernel-based non-parametric goodness-of-fit tests for complex new data\nscenarios, e.g. truncated distributions or compositional data. Experimental\nresults demonstrate that the proposed tests control type-I error well and\nachieve higher test power than existing approaches, including the test based on\nmaximum-mean-discrepancy (MMD).\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 00:44:31 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Xu", "Wenkai", ""]]}, {"id": "2106.12121", "submitter": "Ang Li", "authors": "Ang Li, Judea Pearl", "title": "Bounds on Causal Effects and Application to High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the problem of estimating causal effects when adjustment\nvariables in the back-door or front-door criterion are partially observed. For\nsuch scenarios, we derive bounds on the causal effects by solving two\nnon-linear optimization problems, and demonstrate that the bounds are\nsufficient. Using this optimization method, we propose a framework for\ndimensionality reduction that allows one to trade bias for estimation power,\nand demonstrate its performance using simulation studies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 01:47:38 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Li", "Ang", ""], ["Pearl", "Judea", ""]]}, {"id": "2106.12199", "submitter": "Prateek Jaiswal", "authors": "Prateek Jaiswal, Harsha Honnappa, Vinayak A. Rao", "title": "Bayesian Joint Chance Constrained Optimization: Approximations and\n  Statistical Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG math.OC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers data-driven chance-constrained stochastic optimization\nproblems in a Bayesian framework. Bayesian posteriors afford a principled\nmechanism to incorporate data and prior knowledge into stochastic optimization\nproblems. However, the computation of Bayesian posteriors is typically an\nintractable problem, and has spawned a large literature on approximate Bayesian\ncomputation. Here, in the context of chance-constrained optimization, we focus\non the question of statistical consistency (in an appropriate sense) of the\noptimal value, computed using an approximate posterior distribution. To this\nend, we rigorously prove a frequentist consistency result demonstrating the\nconvergence of the optimal value to the optimal value of a fixed, parameterized\nconstrained optimization problem. We augment this by also establishing a\nprobabilistic rate of convergence of the optimal value. We also prove the\nconvex feasibility of the approximate Bayesian stochastic optimization problem.\nFinally, we demonstrate the utility of our approach on an optimal staffing\nproblem for an M/M/c queueing model.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 07:11:39 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 17:01:49 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Jaiswal", "Prateek", ""], ["Honnappa", "Harsha", ""], ["Rao", "Vinayak A.", ""]]}, {"id": "2106.12262", "submitter": "David Frazier", "authors": "David T. Frazier, Ruben Loaiza-Maya and Gael M. Martin", "title": "A Note on the Accuracy of Variational Bayes in State Space Models:\n  Inference and Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using theoretical and numerical results, we document the accuracy of commonly\napplied variational Bayes methods across a broad range of state space models.\nThe results demonstrate that, in terms of accuracy on fixed parameters, there\nis a clear hierarchy in terms of the methods, with approaches that do not\napproximate the states yielding superior accuracy over methods that do. We also\ndocument numerically that the inferential discrepancies between the various\nmethods often yield only small discrepancies in predictive accuracy over small\nout-of-sample evaluation periods. Nevertheless, in certain settings, these\npredictive discrepancies can become marked over longer out-of-sample periods.\nThis finding indicates that the invariance of predictive results to inferential\ninaccuracy, which has been an oft-touted point made by practitioners seeking to\njustify the use of variational inference, is not ubiquitous and must be\nassessed on a case-by-case basis.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 09:39:30 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Frazier", "David T.", ""], ["Loaiza-Maya", "Ruben", ""], ["Martin", "Gael M.", ""]]}, {"id": "2106.12370", "submitter": "Jun Lu", "authors": "Lin Lu, Lu Jun and Li Weiyu", "title": "Online Updating Statistics for Heterogenous Updating Regressions via\n  Homogenization Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Under the environment of big data streams, it is a common situation where the\nvariable set of a model may change according to the condition of data streams.\nIn this paper, we propose a homogenization strategy to represent the\nheterogenous models that are gradually updated in the process of data streams.\nWith the homogenized representations, we can easily construct various online\nupdating statistics such as parameter estimation, residual sum of squares and\n$F$-statistic for the heterogenous updating regression models. The main\ndifference from the classical scenarios is that the artificial covariates in\nthe homogenized models are not identically distributed as the natural\ncovariates in the original models, consequently, the related theoretical\nproperties are distinct from the classical ones. The asymptotical properties of\nthe online updating statistics are established, which show that the new method\ncan achieve estimation efficiency and oracle property, without any constraint\non the number of data batches. The behavior of the method is further\nillustrated by various numerical examples from simulation experiments.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:08:09 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 14:02:06 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lu", "Lin", ""], ["Jun", "Lu", ""], ["Weiyu", "Li", ""]]}, {"id": "2106.12399", "submitter": "Damjan Manevski", "authors": "D. Manevski, H. Putter, M. Pohar Perme, E. F. Bonneville, J.\n  Schetelig, L. C. de Wreede", "title": "Integrating relative survival in multi-state models -- a non-parametric\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-state models provide an extension of the usual survival/event-history\nanalysis setting. In the medical domain, multi-state models give the\npossibility of further investigating intermediate events such as relapse and\nremission. In this work, a further extension is proposed using relative\nsurvival, where mortality due to population causes (i.e. non-disease-related\nmortality) is evaluated. The objective is to split all mortality in disease and\nnon-disease-related mortality, with and without intermediate events, in\ndatasets where cause of death is not recorded or is uncertain. To this end,\npopulation mortality tables are integrated into the estimation process, while\nusing the basic relative survival idea that the overall mortality hazard can be\nwritten as a sum of a population and an excess part. Hence, we propose an\nupgraded non-parametric approach to estimation, where population mortality is\ntaken into account. Precise definitions and suitable estimators are given for\nboth the transition hazards and probabilities. Variance estimating techniques\nand confidence intervals are introduced and the behaviour of the new method is\ninvestigated through simulations. The newly developed methodology is\nillustrated by the analysis of a cohort of patients followed after an\nallogeneic hematopoietic stem cell transplantation. The work is also\nimplemented in the R package mstate.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:41:22 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Manevski", "D.", ""], ["Putter", "H.", ""], ["Perme", "M. Pohar", ""], ["Bonneville", "E. F.", ""], ["Schetelig", "J.", ""], ["de Wreede", "L. C.", ""]]}, {"id": "2106.12408", "submitter": "Raj Agrawal", "authors": "Raj Agrawal and Tamara Broderick", "title": "The SKIM-FA Kernel: High-Dimensional Variable Selection and Nonlinear\n  Interaction Discovery in Linear Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific problems require identifying a small set of covariates that\nare associated with a target response and estimating their effects. Often,\nthese effects are nonlinear and include interactions, so linear and additive\nmethods can lead to poor estimation and variable selection. The Bayesian\nframework makes it straightforward to simultaneously express sparsity,\nnonlinearity, and interactions in a hierarchical model. But, as for the few\nother methods that handle this trifecta, inference is computationally\nintractable - with runtime at least quadratic in the number of covariates, and\noften worse. In the present work, we solve this computational bottleneck. We\nfirst show that suitable Bayesian models can be represented as Gaussian\nprocesses (GPs). We then demonstrate how a kernel trick can reduce computation\nwith these GPs to O(# covariates) time for both variable selection and\nestimation. Our resulting fit corresponds to a sparse orthogonal decomposition\nof the regression function in a Hilbert space (i.e., a functional ANOVA\ndecomposition), where interaction effects represent all variation that cannot\nbe explained by lower-order effects. On a variety of synthetic and real\ndatasets, our approach outperforms existing methods used for large,\nhigh-dimensional datasets while remaining competitive (or being orders of\nmagnitude faster) in runtime.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 13:53:36 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Agrawal", "Raj", ""], ["Broderick", "Tamara", ""]]}, {"id": "2106.12535", "submitter": "Valentina Zantedeschi Dr", "authors": "Valentina Zantedeschi, Paul Viallard, Emilie Morvant, R\\'emi Emonet,\n  Amaury Habrard, Pascal Germain, Benjamin Guedj", "title": "Learning Stochastic Majority Votes by Minimizing a PAC-Bayes\n  Generalization Bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate a stochastic counterpart of majority votes over finite\nensembles of classifiers, and study its generalization properties. While our\napproach holds for arbitrary distributions, we instantiate it with Dirichlet\ndistributions: this allows for a closed-form and differentiable expression for\nthe expected risk, which then turns the generalization bound into a tractable\ntraining objective. The resulting stochastic majority vote learning algorithm\nachieves state-of-the-art accuracy and benefits from (non-vacuous) tight\ngeneralization bounds, in a series of numerical experiments when compared to\ncompeting algorithms which also minimize PAC-Bayes objectives -- both with\nuninformed (data-independent) and informed (data-dependent) priors.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 16:57:23 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Zantedeschi", "Valentina", ""], ["Viallard", "Paul", ""], ["Morvant", "Emilie", ""], ["Emonet", "R\u00e9mi", ""], ["Habrard", "Amaury", ""], ["Germain", "Pascal", ""], ["Guedj", "Benjamin", ""]]}, {"id": "2106.12555", "submitter": "Joel Dyer", "authors": "Joel Dyer, Patrick Cannon, Sebastian M Schmon", "title": "Approximate Bayesian Computation with Path Signatures", "comments": "27 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation models of scientific interest often lack a tractable likelihood\nfunction, precluding standard likelihood-based statistical inference. A popular\nlikelihood-free method for inferring simulator parameters is approximate\nBayesian computation, where an approximate posterior is sampled by comparing\nsimulator output and observed data. However, effective measures of closeness\nbetween simulated and observed data are generally difficult to construct,\nparticularly for time series data which are often high-dimensional and\nstructurally complex. Existing approaches typically involve manually\nconstructing summary statistics, requiring substantial domain expertise and\nexperimentation, or rely on unrealistic assumptions such as iid data. Others\nare inappropriate in more complex settings like multivariate or irregularly\nsampled time series data. In this paper, we introduce the use of path\nsignatures as a natural candidate feature set for constructing distances\nbetween time series data for use in approximate Bayesian computation\nalgorithms. Our experiments show that such an approach can generate more\naccurate approximate Bayesian posteriors than existing techniques for time\nseries models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 17:25:43 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Dyer", "Joel", ""], ["Cannon", "Patrick", ""], ["Schmon", "Sebastian M", ""]]}, {"id": "2106.12571", "submitter": "Diakarya Barro Pr", "authors": "Diop Amadou, Barro Diakarya", "title": "Analysis of the evolution of agroclimatic risks in a context of climate\n  variability in the region of Segou in Mali", "comments": "25 pages, 31 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In the Sahel region the population depends largely on rain-fed agriculture.\nIn West Africa in particular, climate models turn to be unable to capture some\nbasic features of present-day climate variability. This study proposes a\ncontribution to the analysis of the evolution of agro-climatic risks in a\ncontext of climate variability. Some statistical tests are used on the main\nvariables of the rainy season to determine the trends and the variabilities are\ndescribed by the data series. Thus, the paper provides a statistical modeling\nof the different agro-climatic risks while the seasonal variability of\nagro-climatic parameters were analized as well as their inter annual\nvariability. The study identifies the probability distributions of agroclimatic\nrisks and the characterization of the rainy season was clarified.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 17:55:53 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Amadou", "Diop", ""], ["Diakarya", "Barro", ""]]}, {"id": "2106.12621", "submitter": "Hayden Helm", "authors": "Hayden S. Helm and Marah Abdin and Benjamin D. Pedigo and Shweti\n  Mahajan and Vince Lyzinski and Youngser Park and Amitabh Basu and\n  Piali~Choudhury and Christopher M. White and Weiwei Yang and Carey E. Priebe", "title": "Leveraging semantically similar queries for ranking via combining\n  representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.IR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern ranking problems, different and disparate representations of the\nitems to be ranked are often available. It is sensible, then, to try to combine\nthese representations to improve ranking. Indeed, learning to rank via\ncombining representations is both principled and practical for learning a\nranking function for a particular query. In extremely data-scarce settings,\nhowever, the amount of labeled data available for a particular query can lead\nto a highly variable and ineffective ranking function. One way to mitigate the\neffect of the small amount of data is to leverage information from semantically\nsimilar queries. Indeed, as we demonstrate in simulation settings and real data\nexamples, when semantically similar queries are available it is possible to\ngainfully use them when ranking with respect to a particular query. We describe\nand explore this phenomenon in the context of the bias-variance trade off and\napply it to the data-scarce settings of a Bing navigational graph and the\nDrosophila larva connectome.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 18:36:20 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Helm", "Hayden S.", ""], ["Abdin", "Marah", ""], ["Pedigo", "Benjamin D.", ""], ["Mahajan", "Shweti", ""], ["Lyzinski", "Vince", ""], ["Park", "Youngser", ""], ["Basu", "Amitabh", ""], ["Piali~Choudhury", "", ""], ["White", "Christopher M.", ""], ["Yang", "Weiwei", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2106.12652", "submitter": "Vojtech Kejzlar", "authors": "Vojtech Kejzlar and Shrijita Bhattacharya and Mookyong Son and\n  Tapabrata Maiti", "title": "Black Box Variational Bayes Model Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many decades now, Bayesian Model Averaging (BMA) has been a popular\nframework to systematically account for model uncertainty that arises in\nsituations when multiple competing models are available to describe the same or\nsimilar physical process. The implementation of this framework, however, comes\nwith multitude of practical challenges including posterior approximation via\nMarkov Chain Monte Carlo and numerical integration. We present a Variational\nBayes Inference approach to BMA as a viable alternative to the standard\nsolutions which avoids many of the aforementioned pitfalls. The proposed method\nis 'black box' in the sense that it can be readily applied to many models with\nlittle to no model-specific derivation. We illustrate the utility of our\nvariational approach on a suite of standard examples and discuss all the\nnecessary implementation details. Fully documented Python code with all the\nexamples is provided as well.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 20:40:27 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Kejzlar", "Vojtech", ""], ["Bhattacharya", "Shrijita", ""], ["Son", "Mookyong", ""], ["Maiti", "Tapabrata", ""]]}, {"id": "2106.12677", "submitter": "Judith Lok", "authors": "Judith J. Lok, Department of Mathematics and Statistics, Boston\n  University", "title": "Optimal estimation of coarse structural nested mean models with\n  application to initiating ART in HIV infected patients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coarse structural nested mean models are used to estimate treatment effects\nfrom longitudinal observational data. Coarse structural nested mean models lead\nto a large class of estimators. It turns out that estimates and standard errors\nmay differ considerably within this class. We prove that, under additional\nassumptions, there exists an explicit solution for the optimal estimator within\nthe class of coarse structural nested mean models. Moreover, we show that even\nif the additional assumptions do not hold, this optimal estimator is\ndoubly-robust: it is consistent and asymptotically normal not only if the model\nfor treatment initiation is correct, but also if a certain outcome-regression\nmodel is correct.\n  We compare the optimal estimator to some naive choices within the class of\ncoarse structural nested mean models in a simulation study. Furthermore, we\napply the optimal and naive estimators to study how the CD4 count increase due\nto one year of antiretroviral treatment (ART) depends on the time between HIV\ninfection and ART initiation in recently infected HIV infected patients. Both\nin the simulation study and in the application, the use of optimal estimators\nleads to substantial increases in precision.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2021 22:29:58 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Lok", "Judith J.", ""], ["Mathematics", "Department of", ""], ["Statistics", "", ""], ["University", "Boston", ""]]}, {"id": "2106.12719", "submitter": "Ran Dai", "authors": "Ran Dai and Cheng Zheng", "title": "Multiple Testing for Composite Null with FDR Control Guarantee", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  False discovery rate (FDR) controlling procedures provide important\nstatistical guarantees for reproducibility in signal identification experiments\nwith multiple hypotheses testing. In many recent applications, the same set of\ncandidate features are studied in multiple independent experiments. For\nexample, experiments repeated at different facilities and with different\ncohorts, and association studies with the same candidate features but different\noutcomes of interest. These studies provide us opportunities to identify\nsignals by considering the experiments jointly. We study the question of how to\nprovide reproducibility guarantees when we test composite null hypotheses on\nmultiple features. Specifically, we test the unions of the null hypotheses from\nmultiple experiments. We present a knockoff-based variable selection method to\nidentify mutual signals from multiple independent experiments, with a finite\nsample size FDR control guarantee. We demonstrate the performance of this\nmethod with numerical studies and applications in analyzing crime data and TCGA\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 01:43:04 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 19:58:05 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Dai", "Ran", ""], ["Zheng", "Cheng", ""]]}, {"id": "2106.12768", "submitter": "Jingru Zhang", "authors": "Jingru Zhang, Kathleen R. Merikangas, Hongzhe Li and Haochang Shou", "title": "Two-sample tests for repeated measurements of histogram objects with\n  applications to wearable device data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repeated observations have become increasingly common in biomedical research\nand longitudinal studies. For instance, wearable sensor devices are deployed to\ncontinuously track physiological and biological signals from each individual\nover multiple days. It remains of great interest to appropriately evaluate how\nthe daily distribution of biosignals might differ across disease groups and\ndemographics. Hence these data could be formulated as multivariate complex\nobject data such as probability densities, histograms, and observations on a\ntree. Traditional statistical methods would often fail to apply as they are\nsampled from an arbitrary non-Euclidean metric space. In this paper, we propose\nnovel non-parametric graph-based two-sample tests for object data with repeated\nmeasures. A set of test statistics are proposed to capture various possible\nalternatives. We derive their asymptotic null distributions under the\npermutation null. These tests exhibit substantial power improvements over the\nexisting methods while controlling the type I errors under finite samples as\nshown through simulation studies. The proposed tests are demonstrated to\nprovide additional insights on the location, inter- and intra-individual\nvariability of the daily physical activity distributions in a sample of studies\nfor mood disorders.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 05:01:39 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zhang", "Jingru", ""], ["Merikangas", "Kathleen R.", ""], ["Li", "Hongzhe", ""], ["Shou", "Haochang", ""]]}, {"id": "2106.12784", "submitter": "Gerhard Tutz", "authors": "Gerhard Tutz", "title": "Item Response Thresholds Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A comprehensive class of models is proposed that can be used for continuous,\nbinary, ordered categorical and count type responses. The difficulty of items\nis described by difficulty functions, which replace the item difficulty\nparameters that are typically used in item response models. They crucially\ndetermine the response distribution and make the models very flexible with\nregard to the range of distributions that are covered. The model class contains\nseveral widely used models as the binary Rasch model and the graded response\nmodel as special cases, allows for simplifications, and offers a distribution\nfree alternative to count type items. A major strength of the models is that\nthey can be used for mixed item formats, when different types of items are\ncombined to measure abilities or attitudes. It is an immediate consequence of\nthe comprehensive modeling approach that allows that difficulty functions\nautomatically adapt to the response distribution. Basic properties of the model\nclass are shown. Several real data sets are used to illustrate the flexibility\nof the models.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 06:29:08 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Tutz", "Gerhard", ""]]}, {"id": "2106.12844", "submitter": "Haeran Cho Dr", "authors": "Haeran Cho, Claudia Kirch", "title": "Bootstrap confidence intervals for multiple change points based on\n  moving sum procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the problem of quantifying uncertainty about the\nlocations of multiple change points. We first establish the asymptotic\ndistribution of the change point estimators obtained as the local maximisers of\nmoving sum statistics, where the limit distributions differ depending on\nwhether the corresponding size of changes is local, i.e. tends to zero as the\nsample size increases, or fixed. Then, we propose a bootstrap procedure for\nconfidence interval generation which adapts to the unknown size of changes and\nguarantees asymptotic validity both for local and fixed changes. Simulation\nstudies show good performance of the proposed bootstrap procedure, and we\nprovide some discussions about how it can be extended to serially dependent\nerrors.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 09:07:10 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Cho", "Haeran", ""], ["Kirch", "Claudia", ""]]}, {"id": "2106.12948", "submitter": "Youngjoo Cho", "authors": "Youngjoo Cho, Annette M. Molinaro, Chen Hu and Robert L. Strawderman", "title": "Regression Trees and Ensembles for Cumulative Incidence Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The use of cumulative incidence functions for characterizing the risk of one\ntype of event in the presence of others has become increasingly popular over\nthe past decade. The problems of modeling, estimation and inference have been\ntreated using parametric, nonparametric and semi-parametric methods. Efforts to\ndevelop suitable extensions of machine learning methods, such as regression\ntrees and related ensemble methods, have begun comparatively recently. In this\npaper, we propose a novel approach to estimating cumulative incidence curves in\na competing risks setting using regression trees and associated ensemble\nestimators. The proposed methods employ augmented estimators of the Brier score\nrisk as the primary basis for building and pruning trees, and lead to methods\nthat are easily implemented using existing R packages. Data from the Radiation\nTherapy Oncology Group (trial 9410) is used to illustrate these new methods.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 12:17:00 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Cho", "Youngjoo", ""], ["Molinaro", "Annette M.", ""], ["Hu", "Chen", ""], ["Strawderman", "Robert L.", ""]]}, {"id": "2106.13077", "submitter": "Rapha\\\"el de Fondeville", "authors": "Rapha\\\"el de Fondeville and Matthieu Wilhelm", "title": "Optimal sequential sampling design for environmental extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sihl river, located near the city of Zurich in Switzerland, is under\ncontinuous and tight surveillance as it flows directly under the city's main\nrailway station. To issue early warnings and conduct accurate risk\nquantification, a dense network of monitoring stations is necessary inside the\nriver basin. However, as of 2021 only three automatic stations are operated in\nthis region, naturally raising the question: how to extend this network for\noptimal monitoring of extreme rainfall events?\n  So far, existing methodologies for station network design have mostly focused\non maximizing interpolation accuracy or minimizing the uncertainty of some\nmodel's parameters estimates. In this work, we propose new principles inspired\nfrom extreme value theory for optimal monitoring of extreme events. For\nstationary processes, we study the theoretical properties of the induced\nsampling design that yields non-trivial point patterns resulting from a\ncompromise between a boundary effect and the maximization of inter-location\ndistances. For general applications, we propose a theoretically justified\nfunctional peak-over-threshold model and provide an algorithm for sequential\nstation selection. We then issue recommendations for possible extensions of the\nSihl river monitoring network, by efficiently leveraging both station and radar\nmeasurements available in this region.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 15:03:51 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["de Fondeville", "Rapha\u00ebl", ""], ["Wilhelm", "Matthieu", ""]]}, {"id": "2106.13319", "submitter": "Rui Yao", "authors": "Rui Yao, Shlomo Bekhor", "title": "A variational autoencoder approach for choice set generation and\n  implicit perception of alternatives in choice modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives the generalized extreme value (GEV) model with implicit\navailability/perception (IAP) of alternatives and proposes a variational\nautoencoder (VAE) approach for choice set generation and implicit perception of\nalternatives. Specifically, the cross-nested logit (CNL) model with IAP is\nderived as an example of IAP-GEV models. The VAE approach is adapted to model\nthe choice set generation process, in which the likelihood of perceiving chosen\nalternatives in the choice set is maximized. The VAE approach for route choice\nset generation is exemplified using a real dataset. IAP- CNL model estimated\nhas the best performance in terms of goodness-of-fit and prediction\nperformance, compared to multinomial logit models and conventional choice set\ngeneration methods.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2021 00:52:49 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Yao", "Rui", ""], ["Bekhor", "Shlomo", ""]]}, {"id": "2106.13379", "submitter": "Rui Meng", "authors": "Rui Meng, Kristofer Bouchard", "title": "Bayesian Inference in High-Dimensional Time-Serieswith the Orthogonal\n  Stochastic Linear Mixing Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern time-series datasets contain large numbers of output response\nvariables sampled for prolonged periods of time. For example, in neuroscience,\nthe activities of 100s-1000's of neurons are recorded during behaviors and in\nresponse to sensory stimuli. Multi-output Gaussian process models leverage the\nnonparametric nature of Gaussian processes to capture structure across multiple\noutputs. However, this class of models typically assumes that the correlations\nbetween the output response variables are invariant in the input space.\nStochastic linear mixing models (SLMM) assume the mixture coefficients depend\non input, making them more flexible and effective to capture complex output\ndependence. However, currently, the inference for SLMMs is intractable for\nlarge datasets, making them inapplicable to several modern time-series\nproblems. In this paper, we propose a new regression framework, the orthogonal\nstochastic linear mixing model (OSLMM) that introduces an orthogonal constraint\namongst the mixing coefficients. This constraint reduces the computational\nburden of inference while retaining the capability to handle complex output\ndependence. We provide Markov chain Monte Carlo inference procedures for both\nSLMM and OSLMM and demonstrate superior model scalability and reduced\nprediction error of OSLMM compared with state-of-the-art methods on several\nreal-world applications. In neurophysiology recordings, we use the inferred\nlatent functions for compact visualization of population responses to auditory\nstimuli, and demonstrate superior results compared to a competing method\n(GPFA). Together, these results demonstrate that OSLMM will be useful for the\nanalysis of diverse, large-scale time-series datasets.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 01:12:54 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Meng", "Rui", ""], ["Bouchard", "Kristofer", ""]]}, {"id": "2106.13390", "submitter": "Zheng Chen", "authors": "Hongji Wu, Hao Yuan, Zijing Yang, Yawen Hou, Zheng Chen", "title": "Implementation of an alternative method for assessing competing risks:\n  restricted mean time lost", "comments": "American Journal of Epidemiology, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical and epidemiological studies, hazard ratios are often applied to\ncompare treatment effects between two groups for survival data. For competing\nrisks data, the corresponding quantities of interest are cause-specific hazard\nratios (CHRs) and subdistribution hazard ratios (SHRs). However, they all have\nsome limitations related to model assumptions and clinical interpretation.\nTherefore, we introduce restricted mean time lost (RMTL) as an alternative that\nis easy to interpret in a competing risks framework. We propose a hypothetical\ntest and sample size estimator based on the difference in RMTL (RMTLd). The\nsimulation results show that the RMTLd test has robust statistical performance\n(both type I error and power). Meanwhile, the RMTLd-based sample size can\napproximately achieve the predefined power level. The results of two example\nanalyses also verify the performance of the RMTLd test. From the perspectives\nof clinical interpretation, application conditions and statistical performance,\nwe recommend that the RMTLd be reported with the HR when analyzing competing\nrisks data and that the RMTLd even be regarded as the primary outcome when the\nproportional hazard assumption fails.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 02:13:40 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wu", "Hongji", ""], ["Yuan", "Hao", ""], ["Yang", "Zijing", ""], ["Hou", "Yawen", ""], ["Chen", "Zheng", ""]]}, {"id": "2106.13431", "submitter": "Ziyu Ji", "authors": "Ziyu Ji and Julian Wolfson", "title": "A flexible Bayesian framework for individualized inference via dynamic\n  borrowing", "comments": "Submitted to Biostatistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosion in high-resolution data capture technologies in health has\nincreased interest in making inference about individual-level parameters. While\ntechnology may provide substantial data on a single individual, how best to use\nmultisource population data to improve individualized inference remains an open\nresearch question. One possible approach, the multisource exchangeability model\n(MEM), is a Bayesian method for integrating data from supplementary sources\ninto the analysis of a primary source. MEM was originally developed to improve\ninference for a single study by borrowing information from similar previous\nstudies; however, its computational burden grows exponentially with the number\nof supplementary sources, making it unsuitable for applications where hundreds\nor thousands of supplementary sources (i.e., individuals) could contribute to\ninference on a given individual. In this paper, we propose the data-driven MEM\n(dMEM), a two-stage approach that includes both source selection and clustering\nto enable the inclusion of an arbitrary number of sources to contribute to\nindividualized inference in a computationally tractable and data-efficient way.\nWe illustrate the application of dMEM to individual-level human behavior and\nmental well-being data collected via smartphones, where our approach increases\nindividual-level estimation precision by 84% compared with a standard\nno-borrowing method and outperforms recently-proposed competing methods in 80%\nof individuals.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 05:03:17 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Ji", "Ziyu", ""], ["Wolfson", "Julian", ""]]}, {"id": "2106.13501", "submitter": "Etienne Roquain", "authors": "David Mary and Etienne Roquain", "title": "Semi-supervised multiple testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important limitation of standard multiple testing procedures is that the\nnull distribution should be known. Here, we consider a null distribution-free\napproach for multiple testing in the following semi-supervised setting: the\nuser does not know the null distribution, but has at hand a single sample drawn\nfrom this null distribution. In practical situations, this null training sample\n(NTS) can come from previous experiments, from a part of the data under test,\nfrom specific simulations, or from a sampling process. In this work, we present\ntheoretical results that handle such a framework, with a focus on the false\ndiscovery rate (FDR) control and the Benjamini-Hochberg (BH) procedure. First,\nwe introduce a procedure providing strong FDR control. Second, we also give a\npower analysis for that procedure suggesting that the price to pay for ignoring\nthe null distribution is low when the NTS sample size $n$ is sufficiently large\nin front of the number of test $m$; namely $n\\gtrsim m/(\\max(1,k))$, where $k$\ndenotes the number of \"detectable\" alternatives. Third, to complete the\npicture, we also present a negative result that evidences an intrinsic\ntransition phase to the general semi-supervised multiple testing problem {and\nshows that the proposed method is optimal in the sense that its performance\nboundary follows this transition phase}. Our theoretical properties are\nsupported by numerical experiments, which also show that the delineated\nboundary is of correct order without further tuning any constant. Finally, we\ndemonstrate that our approach provides a theoretical ground for standard\npractice in astronomical data analysis, and in particular for the procedure\nproposed in \\cite{Origin2020} for galaxy detection.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 08:41:02 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Mary", "David", ""], ["Roquain", "Etienne", ""]]}, {"id": "2106.13508", "submitter": "Defeng Sun", "authors": "Qian LI, Binyan Jiang, Defeng Sun", "title": "MARS: A second-order reduction algorithm for high-dimensional sparse\n  precision matrices estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of the precision matrix (or inverse covariance matrix) is of great\nimportance in statistical data analysis. However, as the number of parameters\nscales quadratically with the dimension p, computation becomes very challenging\nwhen p is large. In this paper, we propose an adaptive sieving reduction\nalgorithm to generate a solution path for the estimation of precision matrices\nunder the $\\ell_1$ penalized D-trace loss, with each subproblem being solved by\na second-order algorithm. In each iteration of our algorithm, we are able to\ngreatly reduce the number of variables in the problem based on the\nKarush-Kuhn-Tucker (KKT) conditions and the sparse structure of the estimated\nprecision matrix in the previous iteration. As a result, our algorithm is\ncapable of handling datasets with very high dimensions that may go beyond the\ncapacity of the existing methods. Moreover, for the sub-problem in each\niteration, other than solving the primal problem directly, we develop a\nsemismooth Newton augmented Lagrangian algorithm with global linear convergence\non the dual problem to improve the efficiency. Theoretical properties of our\nproposed algorithm have been established. In particular, we show that the\nconvergence rate of our algorithm is asymptotically superlinear. The high\nefficiency and promising performance of our algorithm are illustrated via\nextensive simulation studies and real data applications, with comparison to\nseveral state-of-the-art solvers.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 09:00:10 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["LI", "Qian", ""], ["Jiang", "Binyan", ""], ["Sun", "Defeng", ""]]}, {"id": "2106.13564", "submitter": "Valentin Courgeau", "authors": "Valentin Courgeau and Almut E.D. Veraart", "title": "Extreme event propagation using counterfactual theory and vine copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Understanding multivariate extreme events play a crucial role in managing the\nrisks of complex systems since extremes are governed by their own mechanisms.\nConditional on a given variable exceeding a high threshold (e.g.\\ traffic\nintensity), knowing which high-impact quantities (e.g\\ air pollutant levels)\nare the most likely to be extreme in the future is key. This article\ninvestigates the contribution of marginal extreme events on future extreme\nevents of related quantities. We propose an Extreme Event Propagation framework\nto maximise counterfactual causation probabilities between a known cause and\nfuture high-impact quantities. Extreme value theory provides a tool for\nmodelling upper tails whilst vine copulas are a flexible device for capturing a\nlarge variety of joint extremal behaviours. We optimise for the probabilities\nof causation and apply our framework to a London road traffic and air\npollutants dataset. We replicate documented atmospheric mechanisms beyond\nlinear relationships. This provides a new tool for quantifying the propagation\nof extremes in a large variety of applications.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 11:22:17 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Courgeau", "Valentin", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "2106.13579", "submitter": "Louis Duvivier", "authors": "Louis Duvivier and R\\'emy Cazabet and C\\'eline Robardet", "title": "Graph model selection by edge probability sequential inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graphs are widely used for describing systems made up of many interacting\ncomponents and for understanding the structure of their interactions. Various\nstatistical models exist, which describe this structure as the result of a\ncombination of constraints and randomness. %Model selection techniques need to\nautomatically identify the best model, and the best set of parameters for a\ngiven graph. To do so, most authors rely on the minimum description length\nparadigm, and apply it to graphs by considering the entropy of probability\ndistributions defined on graph ensembles. In this paper, we introduce edge\nprobability sequential inference, a new approach to perform model selection,\nwhich relies on probability distributions on edge ensembles. From a theoretical\npoint of view, we show that this methodology provides a more consistent ground\nfor statistical inference with respect to existing techniques, due to the fact\nthat it relies on multiple realizations of the random variable. It also\nprovides better guarantees against overfitting, by making it possible to lower\nthe number of parameters of the model below the number of observations.\nExperimentally, we illustrate the benefits of this methodology in two\nsituations: to infer the partition of a stochastic blockmodel, and to identify\nthe most relevant model for a given graph between the stochastic blockmodel and\nthe configuration model.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 12:23:52 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Duvivier", "Louis", ""], ["Cazabet", "R\u00e9my", ""], ["Robardet", "C\u00e9line", ""]]}, {"id": "2106.13634", "submitter": "Heejung Shim", "authors": "Heejung Shim, Zhengrong Xing, Ester Pantaleo, Francesca Luca, Roger\n  Pique-Regi, Matthew Stephens", "title": "Multi-scale Poisson process approaches for differential expression\n  analysis of high-throughput sequencing data", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating and testing for differences in molecular phenotypes (e.g. gene\nexpression, chromatin accessibility, transcription factor binding) across\nconditions is an important part of understanding the molecular basis of gene\nregulation. These phenotypes are commonly measured using high-throughput\nsequencing assays (e.g., RNA-seq, ATAC-seq, ChIP-seq), which provide\nhigh-resolution count data that reflect how the phenotypes vary along the\ngenome. Multiple methods have been proposed to help exploit these\nhigh-resolution measurements for differential expression analysis. However,\nthey ignore the count nature of the data, instead using normal approximations\nthat work well only for data with large sample sizes or high counts. Here we\ndevelop count-based methods to address this problem. We model the data for each\nsample using an inhomogeneous Poisson process with spatially structured\nunderlying intensity function, and then, building on multi-scale models for the\nPoisson process, estimate and test for differences in the underlying intensity\nfunction across samples (or groups of samples). Using both simulation and real\nATAC-seq data we show that our method outperforms previous normal-based\nmethods, especially in situations with small sample sizes or low counts.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 13:36:28 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Shim", "Heejung", ""], ["Xing", "Zhengrong", ""], ["Pantaleo", "Ester", ""], ["Luca", "Francesca", ""], ["Pique-Regi", "Roger", ""], ["Stephens", "Matthew", ""]]}, {"id": "2106.13681", "submitter": "Haiyan Jiang", "authors": "Haiyan Jiang, Shuyu Li, Luwei Zhang, Haoyi Xiong, Dejing Dou", "title": "Robust Matrix Factorization with Grouping Effect", "comments": "22 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many techniques have been applied to matrix factorization (MF), they\nmay not fully exploit the feature structure. In this paper, we incorporate the\ngrouping effect into MF and propose a novel method called Robust Matrix\nFactorization with Grouping effect (GRMF). The grouping effect is a\ngeneralization of the sparsity effect, which conducts denoising by clustering\nsimilar values around multiple centers instead of just around 0. Compared with\nexisting algorithms, the proposed GRMF can automatically learn the grouping\nstructure and sparsity in MF without prior knowledge, by introducing a\nnaturally adjustable non-convex regularization to achieve simultaneous sparsity\nand grouping effect. Specifically, GRMF uses an efficient alternating\nminimization framework to perform MF, in which the original non-convex problem\nis first converted into a convex problem through Difference-of-Convex (DC)\nprogramming, and then solved by Alternating Direction Method of Multipliers\n(ADMM). In addition, GRMF can be easily extended to the Non-negative Matrix\nFactorization (NMF) settings. Extensive experiments have been conducted using\nreal-world data sets with outliers and contaminated noise, where the\nexperimental results show that GRMF has promoted performance and robustness,\ncompared to five benchmark algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:03:52 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 12:04:14 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Jiang", "Haiyan", ""], ["Li", "Shuyu", ""], ["Zhang", "Luwei", ""], ["Xiong", "Haoyi", ""], ["Dou", "Dejing", ""]]}, {"id": "2106.13685", "submitter": "Haiyan Jiang", "authors": "Haiyan Jiang, Shanshan Qin, Dejing Dou", "title": "Feature Grouping and Sparse Principal Component Analysis", "comments": "21 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Principal Component Analysis (SPCA) is widely used in data processing\nand dimension reduction; it uses the lasso to produce modified principal\ncomponents with sparse loadings for better interpretability. However, sparse\nPCA never considers an additional grouping structure where the loadings share\nsimilar coefficients (i.e., feature grouping), besides a special group with all\ncoefficients being zero (i.e., feature selection). In this paper, we propose a\nnovel method called Feature Grouping and Sparse Principal Component Analysis\n(FGSPCA) which allows the loadings to belong to disjoint homogeneous groups,\nwith sparsity as a special case. The proposed FGSPCA is a subspace learning\nmethod designed to simultaneously perform grouping pursuit and feature\nselection, by imposing a non-convex regularization with naturally adjustable\nsparsity and grouping effect. To solve the resulting non-convex optimization\nproblem, we propose an alternating algorithm that incorporates the\ndifference-of-convex programming, augmented Lagrange and coordinate descent\nmethods. Additionally, the experimental results on real data sets show that the\nproposed FGSPCA benefits from the grouping effect compared with methods without\ngrouping effect.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:08:39 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Jiang", "Haiyan", ""], ["Qin", "Shanshan", ""], ["Dou", "Dejing", ""]]}, {"id": "2106.13694", "submitter": "Keisuke Yano", "authors": "Yukito Iba and Keisuke Yano", "title": "Posterior Covariance Information Criterion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an information criterion, PCIC, for predictive evaluation based\non quasi-posterior distributions. It is regarded as a natural generalisation of\nthe widely applicable information criterion (WAIC) and can be computed via a\nsingle Markov chain Monte Carlo run. PCIC is useful in a variety of predictive\nsettings that are not well dealt with in WAIC, including weighted likelihood\ninference and quasi-Bayesian prediction\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 15:28:36 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 05:22:53 GMT"}, {"version": "v3", "created": "Thu, 15 Jul 2021 05:32:44 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Iba", "Yukito", ""], ["Yano", "Keisuke", ""]]}, {"id": "2106.13718", "submitter": "Onur Teymur", "authors": "Onur Teymur, Christopher N. Foley, Philip G. Breen, Toni Karvonen,\n  Chris. J. Oates", "title": "Black Box Probabilistic Numerics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic numerics casts numerical tasks, such the numerical solution of\ndifferential equations, as inference problems to be solved. One approach is to\nmodel the unknown quantity of interest as a random variable, and to constrain\nthis variable using data generated during the course of a traditional numerical\nmethod. However, data may be nonlinearly related to the quantity of interest,\nrendering the proper conditioning of random variables difficult and limiting\nthe range of numerical tasks that can be addressed. Instead, this paper\nproposes to construct probabilistic numerical methods based only on the final\noutput from a traditional method. A convergent sequence of approximations to\nthe quantity of interest constitute a dataset, from which the limiting quantity\nof interest can be extrapolated, in a probabilistic analogue of Richardson's\ndeferred approach to the limit. This black box approach (1) massively expands\nthe range of tasks to which probabilistic numerics can be applied, (2) inherits\nthe features and performance of state-of-the-art numerical methods, and (3)\nenables provably higher orders of convergence to be achieved. Applications are\npresented for nonlinear ordinary and partial differential equations, as well as\nfor eigenvalue problems-a setting for which no probabilistic numerical methods\nhave yet been developed.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2021 11:21:10 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Teymur", "Onur", ""], ["Foley", "Christopher N.", ""], ["Breen", "Philip G.", ""], ["Karvonen", "Toni", ""], ["Oates", "Chris. J.", ""]]}, {"id": "2106.13751", "submitter": "Louis Sharrock", "authors": "Louis Sharrock, Nikolas Kantas, Panos Parpas, Grigorios A. Pavliotis", "title": "Parameter Estimation for the McKean-Vlasov Stochastic Differential\n  Equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of parameter estimation for a\nstochastic McKean-Vlasov equation, and the associated system of weakly\ninteracting particles. We first establish consistency and asymptotic normality\nof the offline maximum likelihood estimator for the interacting particle system\nin the limit as the number of particles $N\\rightarrow\\infty$. We then propose\nan online estimator for the parameters of the McKean-Vlasov SDE, which evolves\naccording to a continuous-time stochastic gradient descent algorithm on the\nasymptotic log-likelihood of the interacting particle system. We prove that\nthis estimator converges in $\\mathbb{L}^1$ to the stationary points of the\nasymptotic log-likelihood of the McKean-Vlasov SDE in the joint limit as\n$N\\rightarrow\\infty$ and $t\\rightarrow\\infty$, under suitable assumptions which\nguarantee ergodicity and uniform-in-time propagation of chaos. We then\ndemonstrate, under the additional assumption of global strong concavity, that\nour estimator converges in $\\mathbb{L}^2$ to the unique maximiser of this\nasymptotic log-likelihood function, and establish an $\\mathbb{L}^2$ convergence\nrate. We also obtain analogous results under the assumption that, rather than\nobserving multiple trajectories of the interacting particle system, we instead\nobserve multiple independent replicates of the McKean-Vlasov SDE itself or,\nless realistically, a single sample path of the McKean-Vlasov SDE and its law.\nOur theoretical results are demonstrated via two numerical examples, a linear\nmean field model and a stochastic opinion dynamics model.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 16:40:51 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Sharrock", "Louis", ""], ["Kantas", "Nikolas", ""], ["Parpas", "Panos", ""], ["Pavliotis", "Grigorios A.", ""]]}, {"id": "2106.13827", "submitter": "Giacomo De Nicola", "authors": "Giacomo De Nicola, G\\\"oran Kauermann, Michael H\\\"ohle", "title": "On assessing excess mortality in Germany during the COVID-19 pandemic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Coronavirus disease 2019 (COVID-19) is associated with a very high number of\ncasualties in the general population. Assessing the exact magnitude of this\nnumber is a non-trivial problem, as relying only on officially reported\nCOVID-19 associated fatalities runs the risk of incurring in several kinds of\nbiases. One of the ways to approach the issue is to compare overall mortality\nduring the pandemic with expected mortality computed using the observed\nmortality figures of previous years. In this paper, we build on existing\nmethodology and propose two ways to compute expected as well as excess\nmortality, namely at the weekly and at the yearly level. Particular focus is\nput on the role of age, which plays a central part in both COVID-19-associated\nand overall mortality. We illustrate our methods by making use of\nage-stratified mortality data from the years 2016 to 2020 in Germany to compute\nage group-specific excess mortality during the COVID-19 pandemic in 2020.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 18:08:59 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["De Nicola", "Giacomo", ""], ["Kauermann", "G\u00f6ran", ""], ["H\u00f6hle", "Michael", ""]]}, {"id": "2106.13874", "submitter": "Steven Ellis", "authors": "Steven P. Ellis", "title": "Statistical Methods for the meta-analysis paper by Itzhaky et al", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes the statistical methods used in Itzhaky et al\n(\"Systematic Review and Meta-analysis: Twenty-six Years of Randomized Clinical\nTrials of Psychosocial Interventions to Reduce Suicide Risk in Adolescents\").\nThat paper is a meta-analysis of randomized controlled clinical trials testing\nmethods for preventing suicidal behavior and/or ideation in youth. Particularly\non the behavior side the meta-data are challenging to analyze.\n  This paper has two parts. The first is an informal discussion of the\nstatistical methods used. The second gives detailed mathematical derivations of\nsome formulas and methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2021 20:23:13 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ellis", "Steven P.", ""]]}, {"id": "2106.13925", "submitter": "He Jiang", "authors": "Ery Arias-Castro, He Jiang", "title": "Extending the Patra-Sen Approach to Estimating the Background Component\n  in a Two-Component Mixture Model", "comments": "34 pages, 11 figures, 17 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Patra and Sen (2016) consider a two-component mixture model, where one\ncomponent plays the role of background while the other plays the role of\nsignal, and propose to estimate the background component by simply \"maximizing\"\nits weight. While in their work the background component is a completely known\ndistribution, we extend their approach here to three emblematic settings: when\nthe background distribution is symmetric; when it is monotonic; and when it is\nlog-concave. In each setting, we derive estimators for the background\ncomponent, establish consistency, and provide a confidence band. While the\nestimation of a background component is straightforward when it is taken to be\nsymmetric or monotonic, when it is log-concave its estimation requires the\ncomputation of a largest concave minorant, which we implement using sequential\nquadratic programming. Compared to existing methods, our method has the\nadvantage of requiring much less prior knowledge on the background component,\nand is thus less prone to model misspecification. We illustrate this\nmethodology on a number of synthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 02:17:33 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Arias-Castro", "Ery", ""], ["Jiang", "He", ""]]}, {"id": "2106.13931", "submitter": "Hongyu Miao", "authors": "Han Feng, Xing Qiu, Hongyu Miao", "title": "Hypothesis Testing for Two Sample Comparison of Network Data", "comments": "40 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Network data is a major object data type that has been widely collected or\nderived from common sources such as brain imaging. Such data contains numeric,\ntopological, and geometrical information, and may be necessarily considered in\ncertain non-Euclidean space for appropriate statistical analysis. The\ndevelopment of statistical methodologies for network data is challenging and\ncurrently at its infancy; for instance, the non-Euclidean counterpart of basic\ntwo-sample tests for network data is scarce in literature. In this study, a\nnovel framework is presented for two independent sample comparison of networks.\nSpecifically, an approximation distance metric to quotient Euclidean distance\nis proposed, and then combined with network spectral distance to quantify the\nlocal and global dissimilarity of networks simultaneously. A permutational\nnon-Euclidean analysis of variance is adapted to the proposed distance metric\nfor the comparison of two independent groups of networks. Comprehensive\nsimulation studies and real applications are conducted to demonstrate the\nsuperior performance of our method over other alternatives. The asymptotic\nproperties of the proposed test are investigated and its high-dimensional\nextension is discussed as well.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 03:24:27 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Feng", "Han", ""], ["Qiu", "Xing", ""], ["Miao", "Hongyu", ""]]}, {"id": "2106.13946", "submitter": "Kazuharu Harada", "authors": "Kazuharu Harada and Hironori Fujisawa", "title": "Outlier-Resistant Estimators for Average Treatment Effect in Causal\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimators for causal quantities sometimes suffer from outliers. We\ninvestigate outlier-resistant estimation for the average treatment effect (ATE)\nunder challenging but realistic settings. We assume that the ratio of outliers\nis not necessarily small and that it can depend on covariates. We propose three\ntypes of estimators for the ATE, which combines the well-known inverse\nprobability weighting (IPW)/doubly robust (DR) estimators with the\ndensity-power weight. Under heterogeneous contamination, our methods can reduce\nthe bias caused by outliers. In particular, under homogeneous contamination,\nour estimators are approximately consistent with the true ATE. An\ninfluence-function-based analysis indicates that the adverse effect of outliers\nis negligible if the ratio of outliers is small even under heterogeneous\ncontamination. We also derived the asymptotic properties of our estimators. We\nevaluated the performance of our estimators through Monte-Carlo simulations and\nreal data analysis. The comparative methods, which estimate the median of the\npotential outcome, do not have enough outlier resistance. In experiments, our\nmethods outperformed the comparative methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 05:16:28 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Harada", "Kazuharu", ""], ["Fujisawa", "Hironori", ""]]}, {"id": "2106.14045", "submitter": "Ning Ning", "authors": "Ning Ning and Jinwen Qiu", "title": "The mbsts package: Multivariate Bayesian Structural Time Series Models\n  in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.MS stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate Bayesian structural time series (MBSTS) model\n\\citep{qiu2018multivariate,Jammalamadaka2019Predicting} as a generalized\nversion of many structural time series models, deals with inference and\nprediction for multiple correlated time series, where one also has the choice\nof using a different candidate pool of contemporaneous predictors for each\ntarget series. The MBSTS model has wide applications and is ideal for feature\nselection, time series forecasting, nowcasting, inferring causal impact, and\nothers. This paper demonstrates how to use the R package \\pkg{mbsts} for MBSTS\nmodeling, establishing a bridge between user-friendly and developer-friendly\nfunctions in package and the corresponding methodology. A simulated dataset and\nobject-oriented functions in the \\pkg{mbsts} package are explained in the way\nthat enables users to flexibly add or deduct some components, as well as to\nsimplify or complicate some settings.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 15:28:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ning", "Ning", ""], ["Qiu", "Jinwen", ""]]}, {"id": "2106.14063", "submitter": "Walter K Kremers", "authors": "Walter K Kremers", "title": "A general, simple, robust method to account for measurement error when\n  analyzing data with an internal validation subsample", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Background: Measurement errors in terms of quantification or classification\nfrequently occur in epidemiologic data and can strongly impact inference.\nMeasurement errors may occur when ascertaining, recording or extracting data.\nAlthough the effects of measurement errors can be severe and are well\ndescribed, simple straight forward general analytic solutions are not readily\navailable for statistical analysis and measurement error is frequently not\nacknowledged or accounted for. Generally, to account for measurement error\nrequires some data where we can observe the variables once with and once\nwithout error, to establish the relationship between the two. Methods: Here we\ndescribe a general method accounting for measurement error in outcome and/or\npredictor variables for the parametric regression setting when there is a\nvalidation subsample where variables are measured once with and once without\nerror. The method does not describe and thus does not depend on the particular\nrelation between the variables measured with and without error, and is\ngenerally robust to the type of measurement error, for example nondifferential,\ndifferential or Berkson errors. Results: Simulation studies show how the method\nreduces bias compared to models based upon variables measured with error alone\nand reduces variances compared to models based upon the variables measured\nwithout error in the validation subsample alone. Conclusion: The proposed\nestimator has favorable properties in terms of bias and variance, is easily\nderived empirically, and is robust to different types of measurement error.\nThis method should be a valuable tool in the analysis of data with measurement\nerror.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 16:52:42 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kremers", "Walter K", ""]]}, {"id": "2106.14077", "submitter": "Masahiro Kato", "authors": "Masahiro Kato and Kaito Ariu", "title": "The Role of Contextual Information in Best Arm Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG econ.EM math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the best-arm identification problem with fixed confidence when\ncontextual (covariate) information is available in stochastic bandits. Although\nwe can use contextual information in each round, we are interested in the\nmarginalized mean reward over the contextual distribution. Our goal is to\nidentify the best arm with a minimal number of samplings under a given value of\nthe error rate. We show the instance-specific sample complexity lower bounds\nfor the problem. Then, we propose a context-aware version of the\n\"Track-and-Stop\" strategy, wherein the proportion of the arm draws tracks the\nset of optimal allocations and prove that the expected number of arm draws\nmatches the lower bound asymptotically. We demonstrate that contextual\ninformation can be used to improve the efficiency of the identification of the\nbest marginalized mean reward compared with the results of Garivier & Kaufmann\n(2016). We experimentally confirm that context information contributes to\nfaster best-arm identification.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 18:39:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Kato", "Masahiro", ""], ["Ariu", "Kaito", ""]]}, {"id": "2106.14083", "submitter": "Michele Guindani", "authors": "Wei Zhang, Ivor Cribben, sonia Petrone, Michele Guindani", "title": "Bayesian Time-Varying Tensor Vector Autoregressive Models for Dynamic\n  Effective Connectivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent developments in functional magnetic resonance imaging (fMRI)\ninvestigate how some brain regions directly influence the activity of other\nregions of the brain {\\it dynamically} throughout the course of an experiment,\nnamely dynamic effective connectivity. Time-varying vector autoregressive\n(TV-VAR) models have been employed to draw inferencesfor this purpose, but they\nare very computationally intensive, since the number of parameters to be\nestimated increases quadratically with the number of time series. In this\npaper, we propose a computationally efficient Bayesian time-varying VAR\napproach for modeling high-dimensional time series. The proposed framework\nemploys a tensor decomposition for the VAR coefficient matrices at different\nlags. Dynamically varying connectivity patterns are captured by assuming that\nat any given time only a subset of components in the tensor decomposition is\nactive. Latent binary time series select the active components at each time via\na convenient Ising prior specification. The proposed prior structure encourages\nsparsity in the tensor structure and allows to ascertain model complexity\nthrough the posterior distribution. More specifically, sparsity-inducing priors\nare employed to allow for global-local shrinkage of the coefficients, to\ndetermine automatically the rank of the tensor decomposition and to guide the\nselection of the lags of the auto-regression. We show the performances of our\nmodel formulation via simulation studies and data from a real fMRI study\ninvolving a book reading experiment.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 20:14:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Zhang", "Wei", ""], ["Cribben", "Ivor", ""], ["Petrone", "sonia", ""], ["Guindani", "Michele", ""]]}, {"id": "2106.14085", "submitter": "Jianeng Xu", "authors": "Nicholas Polson, Vadim Sokolov, Jianeng Xu", "title": "Deep Learning Partial Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High dimensional data reduction techniques are provided by using partial\nleast squares within deep learning. Our framework provides a nonlinear\nextension of PLS together with a disciplined approach to feature selection and\narchitecture design in deep learning. This leads to a statistical\ninterpretation of deep learning that is tailor made for predictive problems. We\ncan use the tools of PLS, such as scree-plot, bi-plot to provide model\ndiagnostics. Posterior predictive uncertainty is available using MCMC methods\nat the last layer. Thus we achieve the best of both worlds: scalability and\nfast predictive rule construction together with uncertainty quantification. Our\nkey construct is to employ deep learning within PLS by predicting the output\nscores as a deep learner of the input scores. As with PLS our X-scores are\nconstructed using SVD and applied to both regression and classification\nproblems and are fast and scalable. Following Frank and Friedman 1993, we\nprovide a Bayesian shrinkage interpretation of our nonlinear predictor. We\nintroduce a variety of new partial least squares models: PLS-ReLU,\nPLS-Autoencoder, PLS-Trees and PLS-GP. To illustrate our methodology, we use\nsimulated examples and the analysis of preferences of orange juice and\npredicting wine quality as a function of input characteristics. We also\nillustrate Brillinger's estimation procedure to provide the feature selection\nand data dimension reduction. Finally, we conclude with directions for future\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 20:22:54 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Polson", "Nicholas", ""], ["Sokolov", "Vadim", ""], ["Xu", "Jianeng", ""]]}, {"id": "2106.14095", "submitter": "Maikol Sol\\'is", "authors": "Maikol Sol\\'is, Carlos Pasquier", "title": "Using relative weight analysis with residualization to detect relevant\n  nonlinear interaction effects in ordinary and logistic regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relative weight analysis is a classic tool to detect if one variable or\ninteraction in a model is relevant or not. In this paper, we will focus on the\nconstruction of relative weights for non-linear interactions using restricted\ncubic splines. Our aim is to provide an accessible method to analyze a\nmultivariate model and identify one subset with the most representative set of\nvariables. Furthermore, we developed a procedure treating control, fixed, free\nand interactions terms at the same time in the residual weight analysis. The\ninteractions are residualized properly against their main effects to keep their\ntrue effect in the model. We test this method with two simulated examples.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 21:20:07 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Sol\u00eds", "Maikol", ""], ["Pasquier", "Carlos", ""]]}, {"id": "2106.14109", "submitter": "Han Fu", "authors": "Han Fu (1), Shahrul Mt-Isa (2), Richard Baumgartner (3), William\n  Malbecq (2) ((1) The Ohio State University, (2) MSD, (3) Merck)", "title": "Parmsurv: a SAS Macro for Flexible Parametric Survival Analysis with\n  Long-Term Predictions", "comments": "15 pages, 1 figure, 10 tables, accepted by The Clinical Data Science\n  Conference - PHUSE US Connect 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health economic evaluations often require predictions of survival rates\nbeyond the follow-up period. Parametric survival models can be more convenient\nfor economic modelling than the Cox model. The generalized gamma (GG) and\ngeneralized F (GF) distributions are extensive families that contain almost all\ncommonly used distributions with various hazard shapes and arbitrary\ncomplexity. In this study, we present a new SAS macro for implementing a wide\nvariety of flexible parametric models including the GG and GF distributions and\ntheir special cases, as well as the Gompertz distribution. Proper custom\ndistributions are also supported. Different from existing SAS procedures, this\nmacro not only supports regression on the location parameter but also on\nancillary parameters, which greatly increases model flexibility. In addition,\nthe SAS macro supports weighted regression, stratified regression and robust\ninference. This study demonstrates with several examples how the SAS macro can\nbe used for flexible survival modeling and extrapolation.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jun 2021 23:24:19 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fu", "Han", "", "The Ohio State University"], ["Mt-Isa", "Shahrul", "", "MSD"], ["Baumgartner", "Richard", "", "Merck"], ["Malbecq", "William", "", "MSD"]]}, {"id": "2106.14145", "submitter": "Duncan Clark", "authors": "Duncan A. Clark and Mark S. Handcock", "title": "An Approach to Causal Inference over Stochastic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Claiming causal inferences in network settings necessitates careful\nconsideration of the often complex dependency between outcomes for actors. Of\nparticular importance are treatment spillover or outcome interference effects.\nWe consider causal inference when the actors are connected via an underlying\nnetwork structure. Our key contribution is a model for causality when the\nunderlying network is unobserved and the actor covariates evolve stochastically\nover time. We develop a joint model for the relational and covariate generating\nprocess that avoids restrictive separability assumptions and deterministic\nnetwork assumptions that do not hold in the majority of social network settings\nof interest. Our framework utilizes the highly general class of\nExponential-family Random Network models (ERNM) of which Markov Random Fields\n(MRF) and Exponential-family Random Graph models (ERGM) are special cases. We\npresent potential outcome based inference within a Bayesian framework, and\npropose a simple modification to the exchange algorithm to allow for sampling\nfrom ERNM posteriors. We present results of a simulation study demonstrating\nthe validity of the approach. Finally, we demonstrate the value of the\nframework in a case-study of smoking over time in the context of adolescent\nfriendship networks.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 05:09:57 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Clark", "Duncan A.", ""], ["Handcock", "Mark S.", ""]]}, {"id": "2106.14238", "submitter": "James Wilson", "authors": "James D. Wilson, Jihui Lee", "title": "Interpretable Network Representation Learning with Principal Component\n  Analysis", "comments": "33 pages. Submitted and currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of interpretable network representation learning for\nsamples of network-valued data. We propose the Principal Component Analysis for\nNetworks (PCAN) algorithm to identify statistically meaningful low-dimensional\nrepresentations of a network sample via subgraph count statistics. The PCAN\nprocedure provides an interpretable framework for which one can readily\nvisualize, explore, and formulate predictive models for network samples. We\nfurthermore introduce a fast sampling-based algorithm, sPCAN, which is\nsignificantly more computationally efficient than its counterpart, but still\nenjoys advantages of interpretability. We investigate the relationship between\nthese two methods and analyze their large-sample properties under the common\nregime where the sample of networks is a collection of kernel-based random\ngraphs. We show that under this regime, the embeddings of the sPCAN method\nenjoy a central limit theorem and moreover that the population level embeddings\nof PCAN and sPCAN are equivalent. We assess PCAN's ability to visualize,\ncluster, and classify observations in network samples arising in nature,\nincluding functional connectivity network samples and dynamic networks\ndescribing the political co-voting habits of the U.S. Senate. Our analyses\nreveal that our proposed algorithm provides informative and discriminatory\nfeatures describing the networks in each sample. The PCAN and sPCAN methods\nbuild on the current literature of network representation learning and set the\nstage for a new line of research in interpretable learning on network-valued\ndata. Publicly available software for the PCAN and sPCAN methods are available\nat https://www.github.com/jihuilee/.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 13:52:49 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wilson", "James D.", ""], ["Lee", "Jihui", ""]]}, {"id": "2106.14243", "submitter": "Guanhua Chen", "authors": "Rui Chen, Guanhua Chen, Menggang Yu", "title": "A Generalizability Score for Aggregate Causal Effect", "comments": "31 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scientists frequently generalize population level causal quantities such as\naverage treatment effect from a source population to a target population. When\nthe causal effects are heterogeneous, differences in subject characteristics\nbetween the source and target populations may make such a generalization\ndifficult and unreliable. Reweighting or regression can be used to adjust for\nsuch differences when generalizing. However, these methods typically suffer\nfrom large variance if there is limited covariate distribution overlap between\nthe two populations. We propose a generalizability score to address this issue.\nThe score can be used as a yardstick to select target subpopulations for\ngeneralization. A simplified version of the score avoids using any outcome\ninformation and thus can prevent deliberate biases associated with inadvertent\naccess to such information. Both simulation studies and real data analysis\ndemonstrate convincing results for such selection.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 14:25:07 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Chen", "Rui", ""], ["Chen", "Guanhua", ""], ["Yu", "Menggang", ""]]}, {"id": "2106.14255", "submitter": "Haim Bar", "authors": "Haim Bar and Martin T. Wells", "title": "On Graphical Models and Convex Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a mixture-model of beta distributions to identify significant\ncorrelations among $P$ predictors when $P$ is large. The method relies on\ntheorems in convex geometry, which we use to show how to control the error rate\nof edge detection in graphical models. Our `betaMix' method does not require\nany assumptions about the network structure, nor does it assume that the\nnetwork is sparse. The results in this article hold for a wide class of data\ngenerating distributions that include light-tailed and heavy-tailed spherically\nsymmetric distributions.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2021 15:18:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Bar", "Haim", ""], ["Wells", "Martin T.", ""]]}, {"id": "2106.14377", "submitter": "Subhankar Dutta", "authors": "Subhankar Dutta, Farha Sultana, Suchandan Kayal", "title": "Parametric Analysis of Gumbel Type-II Distribution under Step-stress\n  Life Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we focus on the parametric inference based on the Tampered\nRandom Variable (TRV) model for simple step-stress life testing (SSLT) using\nType-II censored data. The baseline lifetime of the experimental units under\nnormal stress conditions follows Gumbel Type-II distribution with $\\alpha$ and\n$\\lambda$ being the shape and scale parameters, respectively. Maximum\nlikelihood estimator (MLE) and Bayes estimator of the model parameters are\nderived based on Type-II censored samples. We obtain asymptotic intervals of\nthe unknown parameters using the observed Fisher information matrix. Bayes\nestimators are obtained using Markov Chain Monte Carlo (MCMC) method under\nsquared error loss function and LINEX loss function. We also construct highest\nposterior density (HPD) intervals of the unknown model parameters. Extensive\nsimulation studies are performed to investigate the finite sample properties of\nthe proposed estimators. Finally, the methods are illustrated with the analysis\nof a real data set.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 03:00:03 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Dutta", "Subhankar", ""], ["Sultana", "Farha", ""], ["Kayal", "Suchandan", ""]]}, {"id": "2106.14392", "submitter": "David Gunawan", "authors": "David Gunawan and Robert Kohn and David Nott", "title": "Flexible Variational Bayes based on a Copula of a Mixture of Normals", "comments": "39 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational Bayes methods approximate the posterior density by a family of\ntractable distributions and use optimisation to estimate the unknown parameters\nof the approximation. Variational approximation is useful when exact inference\nis intractable or very costly. Our article develops a flexible variational\napproximation based on a copula of a mixture of normals, which is implemented\nusing the natural gradient and a variance reduction method. The efficacy of the\napproach is illustrated by using simulated and real datasets to approximate\nmultimodal, skewed and heavy-tailed posterior distributions, including an\napplication to Bayesian deep feedforward neural network regression models. Each\nexample shows that the proposed variational approximation is much more accurate\nthan the corresponding Gaussian copula and a mixture of normals variational\napproximations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 04:50:31 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 10:49:46 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Gunawan", "David", ""], ["Kohn", "Robert", ""], ["Nott", "David", ""]]}, {"id": "2106.14399", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen and Jessica Bagnall-Guerreiro and Andrew T Jones", "title": "Universal inference with composite likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum composite likelihood estimation is a useful alternative to maximum\nlikelihood estimation when data arise from data generating processes (DGPs)\nthat do not admit tractable joint specification. We demonstrate that generic\ncomposite likelihoods consisting of marginal and conditional specifications\npermit the simple construction of composite likelihood ratio-like statistics\nfrom which finite-sample valid confidence sets and hypothesis tests can be\nconstructed. These statistics are universal in the sense that they can be\nconstructed from any estimator for the parameter of the underlying DGP. We\ndemonstrate our methodology via a simulation study using a pair of\nconditionally specified bivariate models.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 05:14:46 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Nguyen", "Hien D", ""], ["Bagnall-Guerreiro", "Jessica", ""], ["Jones", "Andrew T", ""]]}, {"id": "2106.14529", "submitter": "Johannes Buchner", "authors": "Johannes Buchner, Thomas Boller, David Bogensberger, Adam Malyali,\n  Kirpal Nandra, Joern Wilms, Tom Dwelly, Teng Liu", "title": "Systematic evaluation of variability detection methods for eROSITA", "comments": "Resubmitted version after a positive first referee report.\n  Variability analysis tools available\n  https://github.com/JohannesBuchner/bexvar/. 15 min Talk:\n  https://youtu.be/xBC1S9MTH4w. To appear on A&A, Special Issue: The Early Data\n  Release of eROSITA and Mikhail Pavlinsky ART-XC on the SRG Mission", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.HE astro-ph.IM stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The reliability of detecting source variability in sparsely and irregularly\nsampled X-ray light curves is investigated. This is motivated by the\nunprecedented survey capabilities of eROSITA onboard SRG, providing light\ncurves for many thousand sources in its final-depth equatorial deep field\nsurvey. Four methods for detecting variability are evaluated: excess variance,\namplitude maximum deviations, Bayesian blocks and a new Bayesian formulation of\nthe excess variance. We judge the false detection rate of variability based on\nsimulated Poisson light curves of constant sources, and calibrate significance\nthresholds. Simulations with flares injected favour the amplitude maximum\ndeviation as most sensitive at low false detections. Simulations with white and\nred stochastic source variability favour Bayesian methods. The results are\napplicable also for the million sources expected in eROSITA's all-sky survey.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 10:08:01 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Buchner", "Johannes", ""], ["Boller", "Thomas", ""], ["Bogensberger", "David", ""], ["Malyali", "Adam", ""], ["Nandra", "Kirpal", ""], ["Wilms", "Joern", ""], ["Dwelly", "Tom", ""], ["Liu", "Teng", ""]]}, {"id": "2106.14562", "submitter": "Pascal Fries", "authors": "Pascal Fries (1 and 2) and Eric Maris (2) ((1) Ernst Str\\\"ungmann\n  Institute for Neuroscience in Cooperation with Max Planck Society, Frankfurt,\n  Germany, (2) Donders Institute for Brain, Cognition and Behaviour, Radboud\n  University, Nijmegen, Netherlands)", "title": "What to do if N is two?", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of in-vivo neurophysiology currently uses statistical standards\nthat are based on tradition rather than formal analysis. Typically, data from\ntwo (or few) animals are pooled for one statistical test, or a significant test\nin a first animal is replicated in one (or few) further animals. The use of\nmore than one animal is widely believed to allow an inference on the\npopulation. Here, we explain that a useful inference on the population would\nrequire larger numbers and a different statistical approach. The field should\nconsider to perform studies at that standard, potentially through coordinated\nmulti-center efforts, for selected questions of exceptional importance. Yet,\nfor many questions, this is ethically and/or economically not justifiable. We\nexplain why in those studies with two (or few) animals, any useful inference is\nlimited to the sample of investigated animals, irrespective of whether it is\nbased on few animals, two animals or a single animal.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 10:32:55 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fries", "Pascal", "", "1 and 2"], ["Maris", "Eric", ""]]}, {"id": "2106.14599", "submitter": "Chuji Luo", "authors": "Chuji Luo, Michael J. Daniels", "title": "BNPqte: A Bayesian Nonparametric Approach to Causal Inference on\n  Quantiles in R", "comments": "44 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce the BNPqte R package which implements the\nBayesian nonparametric approach of Xu, Daniels and Winterstein (2018) for\nestimating quantile treatment effects in observational studies. This approach\nprovides flexible modeling of the distributions of potential outcomes, so it is\ncapable of capturing a variety of underlying relationships among the outcomes,\ntreatments and confounders and estimating multiple quantile treatment effects\nsimultaneously. Specifically, this approach uses a Bayesian additive regression\ntrees (BART) model to estimate the propensity score and a Dirichlet process\nmixture (DPM) of multivariate normals model to estimate the conditional\ndistribution of the potential outcome given the estimated propensity score. The\nBNPqte R package provides a fast implementation for this approach by designing\nefficient R functions for the DPM of multivariate normals model in joint and\nconditional density estimation. These R functions largely improve the\nefficiency of the DPM model in density estimation, compared to the popular\nDPpackage. BART-related R functions in the BNPqte R package are inherited from\nthe BART R package with two modifications on variable importance and split\nprobability. To maximize computational efficiency, the actual sampling and\ncomputation for each model are carried out in C++ code. The Armadillo C++\nlibrary is also used for fast linear algebra calculations.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 12:11:37 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Luo", "Chuji", ""], ["Daniels", "Michael J.", ""]]}, {"id": "2106.14630", "submitter": "Yue Gao", "authors": "Yue Gao, Garvesh Raskutti", "title": "Improved Prediction and Network Estimation Using the Monotone Single\n  Index Multi-variate Autoregressive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Network estimation from multi-variate point process or time series data is a\nproblem of fundamental importance. Prior work has focused on parametric\napproaches that require a known parametric model, which makes estimation\nprocedures less robust to model mis-specification, non-linearities and\nheterogeneities. In this paper, we develop a semi-parametric approach based on\nthe monotone single-index multi-variate autoregressive model (SIMAM) which\naddresses these challenges. We provide theoretical guarantees for dependent\ndata and an alternating projected gradient descent algorithm. Significantly we\ndo not explicitly assume mixing conditions on the process (although we do\nrequire conditions analogous to restricted strong convexity) and we achieve\nrates of the form $O(T^{-\\frac{1}{3}} \\sqrt{s\\log(TM)})$ (optimal in the\nindependent design case) where $s$ is the threshold for the maximum in-degree\nof the network that indicates the sparsity level, $M$ is the number of actors\nand $T$ is the number of time points. In addition, we demonstrate the superior\nperformance both on simulated data and two real data examples where our SIMAM\napproach out-performs state-of-the-art parametric methods both in terms of\nprediction and network estimation.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 12:32:29 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2021 02:00:22 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Gao", "Yue", ""], ["Raskutti", "Garvesh", ""]]}, {"id": "2106.14648", "submitter": "Lucile Ter-Minassian", "authors": "Sahra Ghalebikesabi, Lucile Ter-Minassian, Karla Diaz-Ordaz and Chris\n  Holmes", "title": "On Locality of Local Explanation Models", "comments": "Submitted to NeurIPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Shapley values provide model agnostic feature attributions for model outcome\nat a particular instance by simulating feature absence under a global\npopulation distribution. The use of a global population can lead to potentially\nmisleading results when local model behaviour is of interest. Hence we consider\nthe formulation of neighbourhood reference distributions that improve the local\ninterpretability of Shapley values. By doing so, we find that the\nNadaraya-Watson estimator, a well-studied kernel regressor, can be expressed as\na self-normalised importance sampling estimator. Empirically, we observe that\nNeighbourhood Shapley values identify meaningful sparse feature relevance\nattributions that provide insight into local model behaviour, complimenting\nconventional Shapley analysis. They also increase on-manifold explainability\nand robustness to the construction of adversarial classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2021 16:20:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ghalebikesabi", "Sahra", ""], ["Ter-Minassian", "Lucile", ""], ["Diaz-Ordaz", "Karla", ""], ["Holmes", "Chris", ""]]}, {"id": "2106.14857", "submitter": "Robert Lunde", "authors": "Robert Lunde, Purnamrita Sarkar, Rachel Ward", "title": "Bootstrapping the error of Oja's Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of quantifying uncertainty for the estimation error\nof the leading eigenvector from Oja's algorithm for streaming principal\ncomponent analysis, where the data are generated IID from some unknown\ndistribution. By combining classical tools from the U-statistics literature\nwith recent results on high-dimensional central limit theorems for quadratic\nforms of random vectors and concentration of matrix products, we establish a\n$\\chi^2$ approximation result for the $\\sin^2$ error between the population\neigenvector and the output of Oja's algorithm. Since estimating the covariance\nmatrix associated with the approximating distribution requires knowledge of\nunknown model parameters, we propose a multiplier bootstrap algorithm that may\nbe updated in an online manner. We establish conditions under which the\nbootstrap distribution is close to the corresponding sampling distribution with\nhigh probability, thereby establishing the bootstrap as a consistent\ninferential method in an appropriate asymptotic regime.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 17:27:26 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Lunde", "Robert", ""], ["Sarkar", "Purnamrita", ""], ["Ward", "Rachel", ""]]}, {"id": "2106.14981", "submitter": "Martin Jankowiak", "authors": "Martin Jankowiak", "title": "Fast Bayesian Variable Selection in Binomial and Negative Binomial\n  Regression", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian variable selection is a powerful tool for data analysis, as it\noffers a principled method for variable selection that accounts for prior\ninformation and uncertainty. However, wider adoption of Bayesian variable\nselection has been hampered by computational challenges, especially in\ndifficult regimes with a large number of covariates or non-conjugate\nlikelihoods. Generalized linear models for count data, which are prevalent in\nbiology, ecology, economics, and beyond, represent an important special case.\nHere we introduce an efficient MCMC scheme for variable selection in binomial\nand negative binomial regression that exploits Tempered Gibbs Sampling (Zanella\nand Roberts, 2019) and that includes logistic regression as a special case. In\nexperiments we demonstrate the effectiveness of our approach, including on\ncancer data with seventeen thousand covariates.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jun 2021 20:54:41 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Jankowiak", "Martin", ""]]}, {"id": "2106.15051", "submitter": "Zhuoqun Wang", "authors": "Zhuoqun Wang, Jialiang Mao, and Li Ma", "title": "Logistic-tree normal model for microbiome compositions", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a probabilistic model, called the \"logistic-tree normal\" (LTN),\nfor microbiome compositional data. The LTN marries two popular classes of\nmodels -- the logistic-normal (LN) and the Dirichlet-tree (DT) -- and inherits\nthe key benefits of both. LN models are flexible in characterizing rich\ncovariance structure among taxa but can be computationally prohibitive in face\nof high dimensionality (i.e., when the number of taxa is large) due to its lack\nof conjugacy to the multinomial sampling model. On the other hand, DT avoids\nthis issue by decomposing the multinomial sampling model into a collection of\nbinomials, one at each split of the phylogenetic tree of the taxa, and adopting\na conjugate beta model for each binomial probability, but at the same time the\nDT incurs restrictive covariance among the taxa. In contrast, the LTN model\ndecomposes the multinomial model into binomials as the DT does, but it jointly\nmodels the corresponding binomial probabilities using a (multivariate) LN\ndistribution instead of betas. It therefore allows rich covariance structures\nas the LN models, while the decomposition of the multinomial likelihood allows\nconjugacy to be restored through the P\\'olya-Gamma augmentation. Accordingly,\nBayesian inference on the LTN model can readily proceed by Gibbs sampling.\nMoreover, the multivariate Gaussian aspect of the model allows common\ntechniques for effective inference on high-dimensional data -- such as those\nbased on sparsity and low-rank assumptions in the covariance structure -- to be\nreadily incorporated. Depending on the goal of the analysis, the LTN model can\nbe used either as a standalone model or embedded into more sophisticated\nmodels. We demonstrate its use in estimating taxa covariance and in\nmixed-effects modeling. Finally, we carry out a case study using an LTN-based\nmixed-effects model to analyze a longitudinal dataset from the DIABIMMUNE\nproject.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 01:42:40 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wang", "Zhuoqun", ""], ["Mao", "Jialiang", ""], ["Ma", "Li", ""]]}, {"id": "2106.15074", "submitter": "Ye Wang", "authors": "Ye Wang", "title": "Causal Inference under Temporal and Spatial Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many social events and policies generate spillover effects in both time and\nspace. Their occurrence influences not only the outcomes of interest in the\nfuture, but also these outcomes in nearby areas. In this paper, we propose a\ndesign-based approach to estimate the direct and indirect/spillover treatment\neffects of any event or policy under the assumption of sequential ignorability,\nwhen both temporal and spatial interference are allowed to present. The\nproposed estimators are shown to be consistent and asymptotically Normal if the\ndegree of interference dependence does not grow too fast relative to the sample\nsize. The conventional difference-in-differences (DID) or two-way fixed effects\nmodel, nevertheless, leads to biased estimates in this scenario. We apply the\nmethod to examine the impact of Hong Kong's Umbrella Movement on the result of\nthe ensuing election and how an institutional reform affects real estate\nassessment in New York State.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 03:47:28 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Wang", "Ye", ""]]}, {"id": "2106.15076", "submitter": "Ye Wang", "authors": "Dana Burde, Joel Middleton, Cyrus Samii and Ye Wang", "title": "How to Account for Alternatives When Comparing Effects: Revisiting\n  'Bringing Education to Afghan Girls'", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses a \"principal strata\" approach to decompose treatment effects\nand interpret why a schooling intervention that yielded exceptional initial\neffects yielded substantially smaller effects in a replication years later. The\nspecific application is a set of 2008 and 2015 replications of an intervention\naiming to increase primary education for girls in rural Afghanistan. The\nintervention offers a new schooling option, and as such, its effects depend on\nhow individuals use alternatives that already exist. The principal strata\napproach accounts variation in use patterns when comparing effects across the\nreplications. Our findings show that even though the share of girls for whom\nthe intervention would be valuable dropped considerably in 2015 as compared to\n2008, the intervention was even more efficaciousness for those who continued to\nbenefit from it.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 03:53:53 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Burde", "Dana", ""], ["Middleton", "Joel", ""], ["Samii", "Cyrus", ""], ["Wang", "Ye", ""]]}, {"id": "2106.15081", "submitter": "Ye Wang", "authors": "Peter M. Aronow, Cyrus Samii, Jonathan Sullivan, and Ye Wang", "title": "Inference in Spatial Experiments with Interference using the\n  SpatialEffect Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents methods for analyzing spatial experiments when complex\nspillovers, displacement effects, and other types of \"interference\" are\npresent. We present a robust, design-based approach to analyzing effects in\nsuch settings. The design-based approach derives inferential properties for\ncausal effect estimators from known features of the experimental design, in a\nmanner analogous to inference in sample surveys. The methods presented here\ntarget a quantity of interest called the \"average marginalized response,\" which\nis equal to the average effect of activating a treatment at an intervention\npoint that is a given distance away, averaging ambient effects emanating from\nother intervention points. We provide a step-by-step tutorial based on the\nSpatialEffect package for R. We apply the methods to a randomized experiment on\npayments for community forest conservation in Uganda, showing how our methods\nreveal possibly substantial spatial spillovers that more conventional analyses\ncannot detect.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 04:09:39 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Aronow", "Peter M.", ""], ["Samii", "Cyrus", ""], ["Sullivan", "Jonathan", ""], ["Wang", "Ye", ""]]}, {"id": "2106.15323", "submitter": "Geraldine Jeckeln", "authors": "G\\'eraldine Jeckeln, Ying Hu, Jacqueline G. Cavazos, Amy N. Yates,\n  Carina A. Hahn, Larry Tang, P. Jonathon Phillips, Alice J. O'Toole", "title": "Face Identification Proficiency Test Designed Using Item Response Theory", "comments": "17 pages (including references), 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measures of face identification proficiency are essential to ensure accurate\nand consistent performance by professional forensic face examiners and others\nwho perform face identification tasks in applied scenarios. Current proficiency\ntests rely on static sets of stimulus items, and so, cannot be administered\nvalidly to the same individual multiple times. To create a proficiency test, a\nlarge number of items of \"known\" difficulty must be assembled. Multiple tests\nof equal difficulty can be constructed then using subsets of items. Here, we\nintroduce a proficiency test, the Triad Identity Matching (TIM) test, based on\nstimulus difficulty measures based on Item Response Theory (IRT). Participants\nview face-image \"triads\" (N=225) (two images of one identity and one image of a\ndifferent identity) and select the different identity. In Experiment 1,\nuniversity students (N=197) showed wide-ranging accuracy on the TIM test.\nFurthermore, IRT modeling demonstrated that the TIM test produces items of\nvarious difficulty levels. In Experiment 2, IRT-based item difficulty measures\nwere used to partition the TIM test into three equally \"easy\" and three equally\n\"difficult\" subsets. Simulation results indicated that the full set, as well as\ncurated subsets, of the TIM items yielded reliable estimates of subject\nability. In summary, the TIM test can provide a starting point for developing a\nframework that is flexible, calibrated, and adaptive to measure proficiency\nacross various ability levels (e.g., professionals or populations with face\nprocessing deficits)\n", "versions": [{"version": "v1", "created": "Tue, 22 Jun 2021 22:37:32 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 16:52:11 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Jeckeln", "G\u00e9raldine", ""], ["Hu", "Ying", ""], ["Cavazos", "Jacqueline G.", ""], ["Yates", "Amy N.", ""], ["Hahn", "Carina A.", ""], ["Tang", "Larry", ""], ["Phillips", "P. Jonathon", ""], ["O'Toole", "Alice J.", ""]]}, {"id": "2106.15327", "submitter": "Yoann Altmann", "authors": "Dan Yao and Stephen McLaughlin and Yoann Altmann", "title": "Patch-Based Image Restoration using Expectation Propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents a new Expectation Propagation (EP) framework for image\nrestoration using patch-based prior distributions. While Monte Carlo techniques\nare classically used to sample from intractable posterior distributions, they\ncan suffer from scalability issues in high-dimensional inference problems such\nas image restoration. To address this issue, EP is used here to approximate the\nposterior distributions using products of multivariate Gaussian densities.\nMoreover, imposing structural constraints on the covariance matrices of these\ndensities allows for greater scalability and distributed computation. While the\nmethod is naturally suited to handle additive Gaussian observation noise, it\ncan also be extended to non-Gaussian noise. Experiments conducted for\ndenoising, inpainting and deconvolution problems with Gaussian and Poisson\nnoise illustrate the potential benefits of such flexible approximate Bayesian\nmethod for uncertainty quantification in imaging problems, at a reduced\ncomputational cost compared to sampling techniques.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2021 10:45:15 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Yao", "Dan", ""], ["McLaughlin", "Stephen", ""], ["Altmann", "Yoann", ""]]}, {"id": "2106.15436", "submitter": "James Matuk", "authors": "James Matuk, Sebastian Kurtek, Karthik Bharath", "title": "Topological Data Analysis through alignment of Persistence Landscapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Persistence landscapes are functional summaries of persistence diagrams\ndesigned to enable analysis of the diagrams using tools from functional data\nanalysis. They comprise a collection of scalar functions such that birth and\ndeath times of topological features in persistence diagrams map to extrema of\nfunctions and intervals where they are non-zero. As a consequence, topological\ninformation is encoded in both amplitude and phase components of persistence\nlandscapes. Through functional data analysis of persistence landscapes under an\nelastic Riemannian metric, we show how meaningful statistical summaries of\npersistence landscapes (e.g., mean, dominant directions of variation) can be\nobtained by decoupling topological signal present in amplitude and phase\nvariations. The estimated phase functions are tied to the resolution parameter\nthat determines the filtration of simplicial complexes used to construct\npersistence diagrams. For a dataset obtained under scale and sampling\nvariabilities, the phase function prescribes an optimal rate of increase of the\nresolution parameter for enhancing the topological signal in a persistence\ndiagram. We demonstrate benefits of alignment through several simulation\nexamples and a real data example concerning structure of brain artery trees\nrepresented as 3D point clouds.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 14:14:47 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Matuk", "James", ""], ["Kurtek", "Sebastian", ""], ["Bharath", "Karthik", ""]]}, {"id": "2106.15480", "submitter": "Federico Camerlenghi", "authors": "Federico Camerlenghi, Stefano Favaro, Lorenzo Masoero, Tamara\n  Broderick", "title": "Scaled process priors for Bayesian nonparametric estimation of the\n  unseen genetic variation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in the estimation of the number of unseen\nfeatures, mostly driven by applications in biological sciences. A recent work\nbrought out the upside and the downside of the popular stable-Beta process\nprior, and generalizations thereof, in Bayesian nonparametric inference for the\nunseen-features problem: i) the downside lies in the limited use of the\nsampling information in the posterior distributions, which depend on the\nobservable sample only through the sample size; ii) the upside lies in the\nanalytical tractability and interpretability of the posterior distributions,\nwhich are simple Poisson distributions whose parameters are simple to compute,\nand depend on the sample size and the prior's parameter. In this paper, we\nintroduce and investigate an alternative nonparametric prior, referred to as\nthe stable-Beta scaled process prior, which is the first prior that allows to\nenrich the posterior distribution of the number of unseen features, through the\ninclusion of the sampling information on the number of distinct features in the\nobservable sample, while maintaining the same analytical tractability and\ninterpretability as the stable-Beta process prior. Our prior leads to a\nnegative Binomial posterior distribution, whose parameters depends on the\nsample size, the observed number of distinct features and the prior's\nparameter, providing estimates that are simple, linear in the sampling\ninformation and computationally efficient. We apply our approach to synthetic\nand real genetic data, showing that it outperforms parametric and nonparametric\ncompetitors in terms of estimation accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 15:04:28 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Camerlenghi", "Federico", ""], ["Favaro", "Stefano", ""], ["Masoero", "Lorenzo", ""], ["Broderick", "Tamara", ""]]}, {"id": "2106.15559", "submitter": "Marie Davidian", "authors": "Anastasios A. Tsiatis, Marie Davidian, Shannon T. Holloway", "title": "Estimation of the odds ratio in a proportional odds model with censored\n  time-lagged outcome in a randomized clinical trial", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many randomized clinical trials of therapeutics for COVID-19, the primary\noutcome is an ordinal, categorical variable for which the final category is\noften death, which can be ascertained at the time of occurence. For the\nremaining categories, determination of into which of these categories a\nparticipant's outcome falls cannot be made until some ascertainment time that\ncan be less than or equal to a pre-specified follow-up time. Interest focuses\non the odds ratio (active agent vs. control) under the assumption of a\nproportional odds model. Although at the final analysis the outcome will be\ndetermined for all subjects, at an interim analysis, the status of some\nparticipants may not yet be determined; accordingly, the outcome from these\nsubjects can be viewed as censored. A valid interim analysis can be based on\ndata only from those subjects with full follow up; however, this approach is\ninefficient, as it does not exploit additional information that may be\navailable on those who have not reached the follow-up time at the time of the\ninterim analysis. Appealing to the theory of semiparametrics, we propose an\nestimator for the odds ratio in a proportional odds model with censored,\ntime-lagged categorical outcome that incorporates such additional baseline and\ntime-dependent information and demonstrate that it can result in considerable\ngains in efficiency relative to simpler approaches.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 16:50:46 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 21:40:28 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Tsiatis", "Anastasios A.", ""], ["Davidian", "Marie", ""], ["Holloway", "Shannon T.", ""]]}, {"id": "2106.15675", "submitter": "Julia Lindberg", "authors": "Julia Lindberg, Carlos Am\\'endola, Jose Israel Rodriguez", "title": "Estimating Gaussian mixtures using sparse polynomial moment systems", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.AG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of moments is a statistical technique for density estimation that\nsolves a system of moment equations to estimate the parameters of an unknown\ndistribution. A fundamental question critical to understanding identifiability\nasks how many moment equations are needed to get finitely many solutions and\nhow many solutions there are. We answer this question for classes of Gaussian\nmixture models using the tools of polyhedral geometry. Using these results, we\npresent an algorithm that performs parameter recovery, and therefore density\nestimation, for high dimensional Gaussian mixture models that scales linearly\nin the dimension.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 18:43:36 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Lindberg", "Julia", ""], ["Am\u00e9ndola", "Carlos", ""], ["Rodriguez", "Jose Israel", ""]]}, {"id": "2106.15735", "submitter": "Guangning Xu", "authors": "Geng Deng, Guangning Xu, Qiang Fu, Xindong Wang and Jing Qin", "title": "Active-set algorithms based statistical inference for shape-restricted\n  generalized additive Cox regression models", "comments": "Updated with new latex template, 33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently the shape-restricted inference has gained popularity in statistical\nand econometric literature in order to relax the linear or quadratic covariate\neffect in regression analyses. The typical shape-restricted covariate effect\nincludes monotonic increasing, decreasing, convexity or concavity. In this\npaper, we introduce the shape-restricted inference to the celebrated Cox\nregression model (SR-Cox), in which the covariate response is modeled as\nshape-restricted additive functions. The SR-Cox regression approximates the\nshape-restricted functions using a spline basis expansion with data driven\nchoice of knots. The underlying minimization of negative log-likelihood\nfunction is formulated as a convex optimization problem, which is solved with\nan active-set optimization algorithm. The highlight of this algorithm is that\nit eliminates the superfluous knots automatically. When covariate effects\ninclude combinations of convex or concave terms with unknown forms and linear\nterms, the most interesting finding is that SR-Cox produces accurate linear\ncovariate effect estimates which are comparable to the maximum partial\nlikelihood estimates if indeed the forms are known. We conclude that concave or\nconvex SR-Cox models could significantly improve nonlinear covariate response\nrecovery and model goodness of fit.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 21:40:28 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 15:29:12 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Deng", "Geng", ""], ["Xu", "Guangning", ""], ["Fu", "Qiang", ""], ["Wang", "Xindong", ""], ["Qin", "Jing", ""]]}, {"id": "2106.15737", "submitter": "Laura Balzer PhD", "authors": "Laura B. Balzer, Mark van der Laan, James Ayieko, Moses Kamya, Gabriel\n  Chamie, Joshua Schwab, Diane V. Havlir, Maya L. Petersen", "title": "Two-Stage TMLE to Reduce Bias and Improve Efficiency in Cluster\n  Randomized Trials", "comments": "32 pages (16.5 pgs of main text); 1 figure; 3 main tables; 3 supp\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cluster randomized trials (CRTs) randomly assign an intervention to groups of\nindividuals (e.g., clinics or communities), and measure outcomes on individuals\nin those groups. While offering many advantages, this experimental design\nintroduces challenges that are only partially addressed by existing analytic\napproaches. First, outcomes are often missing for some individuals within\nclusters. Failing to appropriately adjust for differential outcome measurement\ncan result in biased estimates and inference. Second, CRTs often randomize\nlimited numbers of clusters, resulting in chance imbalances on baseline outcome\npredictors between arms. Failing to adaptively adjust for these imbalances and\nother predictive covariates can result in efficiency losses. To address these\nmethodological gaps, we propose and evaluate a novel two-stage targeted minimum\nloss-based estimator (TMLE) to adjust for baseline covariates in a manner that\noptimizes precision, after controlling for baseline and post-baseline causes of\nmissing outcomes. Finite sample simulations illustrate that our approach can\nnearly eliminate bias due to differential outcome measurement, while other\ncommon CRT estimators yield misleading results and inferences. Application to\nreal data from the SEARCH community randomized trial demonstrates the gains in\nefficiency afforded through adaptive adjustment for cluster-level covariates,\nafter controlling for missingness on individual-level outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 21:47:30 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Balzer", "Laura B.", ""], ["van der Laan", "Mark", ""], ["Ayieko", "James", ""], ["Kamya", "Moses", ""], ["Chamie", "Gabriel", ""], ["Schwab", "Joshua", ""], ["Havlir", "Diane V.", ""], ["Petersen", "Maya L.", ""]]}, {"id": "2106.15743", "submitter": "Chiao-Yu Yang", "authors": "Chiao-Yu Yang, Lihua Lei, Nhat Ho, and Will Fithian", "title": "BONuS: Multiple multivariate testing with a data-adaptivetest statistic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new adaptive empirical Bayes framework, the\nBag-Of-Null-Statistics (BONuS) procedure, for multiple testing where each\nhypothesis testing problem is itself multivariate or nonparametric. BONuS is an\nadaptive and interactive knockoff-type method that helps improve the testing\npower while controlling the false discovery rate (FDR), and is closely\nconnected to the \"counting knockoffs\" procedure analyzed in Weinstein et al.\n(2017). Contrary to procedures that start with a $p$-value for each hypothesis,\nour method analyzes the entire data set to adaptively estimate an optimal\n$p$-value transform based on an empirical Bayes model. Despite the extra\nadaptivity, our method controls FDR in finite samples even if the empirical\nBayes model is incorrect or the estimation is poor. An extension, the Double\nBONuS procedure, validates the empirical Bayes model to guard against power\nloss due to model misspecification.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2021 22:08:26 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 18:28:07 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Yang", "Chiao-Yu", ""], ["Lei", "Lihua", ""], ["Ho", "Nhat", ""], ["Fithian", "Will", ""]]}, {"id": "2106.15794", "submitter": "Lan Luo", "authors": "Lan Luo, Ling Zhou and Peter X.-K. Song", "title": "Real-Time Regression Analysis of Streaming Clustered Data With Possible\n  Abnormal Data Batches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper develops an incremental learning algorithm based on quadratic\ninference function (QIF) to analyze streaming datasets with correlated outcomes\nsuch as longitudinal data and clustered data. We propose a renewable QIF\n(RenewQIF) method within a paradigm of renewable estimation and incremental\ninference, in which parameter estimates are recursively renewed with current\ndata and summary statistics of historical data, but with no use of any\nhistorical subject-level raw data. We compare our renewable estimation method\nwith both offline QIF and offline generalized estimating equations (GEE)\napproach that process the entire cumulative subject-level data, and show\ntheoretically and numerically that our renewable procedure enjoys statistical\nand computational efficiency. We also propose an approach to diagnose the\nhomogeneity assumption of regression coefficients via a sequential\ngoodness-of-fit test as a screening procedure on occurrences of abnormal data\nbatches. We implement the proposed methodology by expanding existing Spark's\nLambda architecture for the operation of statistical inference and data quality\ndiagnosis. We illustrate the proposed methodology by extensive simulation\nstudies and an analysis of streaming car crash datasets from the National\nAutomotive Sampling System-Crashworthiness Data System (NASS CDS).\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 03:23:48 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Luo", "Lan", ""], ["Zhou", "Ling", ""], ["Song", "Peter X. -K.", ""]]}, {"id": "2106.15811", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Daisuke Murakami", "title": "Adaptively Robust Geographically Weighted Regression", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new robust geographically weighted regression method in the\npresence of outliers. We embed the standard geographically weighted regression\nin robust objective function based on $\\gamma$-divergence. A novel feature of\nthe proposed approach is that two tuning parameters that control robustness and\nspatial smoothness are automatically tuned in a data-dependent manner. Further,\nthe proposed method can produce robust standard error estimates of the robust\nestimator and give us a reasonable quantity for local outlier detection. We\ndemonstrate that the proposed method is superior to the existing robust version\nof geographically weighted regression through simulation and data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 04:59:43 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 03:58:49 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Murakami", "Daisuke", ""]]}, {"id": "2106.15812", "submitter": "Patrick Chao", "authors": "Patrick Chao, William Fithian", "title": "AdaPT-GMM: Powerful and robust covariate-assisted multiple testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new empirical Bayes method for covariate-assisted multiple\ntesting with false discovery rate (FDR) control, where we model the local false\ndiscovery rate for each hypothesis as a function of both its covariates and\np-value. Our method refines the adaptive p-value thresholding (AdaPT) procedure\nby generalizing its masking scheme to reduce the bias and variance of its false\ndiscovery proportion estimator, improving the power when the rejection set is\nsmall or some null p-values concentrate near 1. We also introduce a Gaussian\nmixture model for the conditional distribution of the test statistics given\ncovariates, modeling the mixing proportions with a generic user-specified\nclassifier, which we implement using a two-layer neural network. Like AdaPT,\nour method provably controls the FDR in finite samples even if the classifier\nor the Gaussian mixture model is misspecified. We show in extensive simulations\nand real data examples that our new method, which we call AdaPT-GMM,\nconsistently delivers high power relative to competing state-of-the-art\nmethods. In particular, it performs well in scenarios where AdaPT is\nunderpowered, and is especially well-suited for testing composite null\nhypothesis, such as whether the effect size exceeds a practical significance\nthreshold.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 05:06:18 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Chao", "Patrick", ""], ["Fithian", "William", ""]]}, {"id": "2106.15839", "submitter": "Siegfried H\\\"ormann", "authors": "Thomas Kuenzer, Siegfried H\\\"ormann, Piotr Kokoszka", "title": "Testing normality of spatially indexed functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a test of normality for spatially indexed functions. The\nassumption of normality is common in spatial statistics, yet no significance\ntests, or other means of assessment, have been available for functional data.\nThis paper aims at filling this gap in the case of functional observations on a\nspatial grid. Our test compares the moments of the spatial (frequency domain)\nprincipal component scores to those of a suitable Gaussian distribution.\nCritical values can be readily obtained from a chi-squared distribution. We\nprovide rigorous theoretical justification for a broad class of weakly\nstationary functional random fields. We perform simulation studies to assess\nthe the power of the test against various alternatives. An application to\nSurface Incoming Shortwave Radiation illustrates the practical value of this\nprocedure.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 06:52:01 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kuenzer", "Thomas", ""], ["H\u00f6rmann", "Siegfried", ""], ["Kokoszka", "Piotr", ""]]}, {"id": "2106.15847", "submitter": "Yinan Mao", "authors": "Yinan Mao and David J. Nott", "title": "Bayesian clustering using random effects models and predictive\n  projections", "comments": "27 pages, 16 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Linear mixed models are widely used for analyzing hierarchically structured\ndata involving missingness and unbalanced study designs. We consider a Bayesian\nclustering method that combines linear mixed models and predictive projections.\nFor each observation, we consider a predictive replicate in which only a subset\nof the random effects is shared between the observation and its replicate, with\nthe remainder being integrated out using the conditional prior. Predictive\nprojections are then defined in which the number of distinct values taken by\nthe shared random effects is finite, in order to obtain different clusters.\nIntegrating out some of the random effects acts as a noise filter, allowing the\nclustering to be focused on only certain chosen features of the data. The\nmethod is inspired by methods for Bayesian model checking, in which simulated\ndata replicates from a fitted model are used for model criticism by examining\ntheir similarity to the observed data in relevant ways. Here the predictive\nreplicates are used to define similarity between observations in relevant ways\nfor clustering. To illustrate the way our method reveals aspects of the data at\ndifferent scales, we consider fitting temporal trends in longitudinal data\nusing Fourier cosine bases with a random effect for each basis function, and\ndifferent clusterings defined by shared random effects for replicates of low or\nhigh frequency terms. The method is demonstrated in a series of real examples.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 07:01:03 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 16:14:41 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Mao", "Yinan", ""], ["Nott", "David J.", ""]]}, {"id": "2106.15865", "submitter": "Sujay Mukhoti", "authors": "Soham Ghosh and Sujay Mukhoti", "title": "Non-parametric generalised newsvendor model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In classical newsvendor model, piece-wise linear shortage and excess costs\nare balanced out to determine the optimal order quantity. However, for critical\nperishable commodities, severity of the costs may be much more than linear. In\nthis paper we discuss a generalisation of the newsvendor model with piece-wise\npolynomial cost functions to accommodate their severity. In addition, the\nstochastic demand has been assumed to follow a completely unknown probability\ndistribution. Subsequently, non-parametric estimator of the optimal order\nquantity has been developed from a random polynomial type estimating equation\nusing a random sample on demand. Strong consistency of the estimator has been\nproven when the true optimal order quantity is unique. The result has been\nextended to the case where multiple solutions for optimal order quantity are\navailable. Probability of existence of the estimated optimal order quantity has\nbeen studied through extensive simulation experiments. Simulation results\nindicate that the non-parametric method provides robust yet efficient estimator\nof the optimal order quantity in terms of mean square error.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 07:48:57 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ghosh", "Soham", ""], ["Mukhoti", "Sujay", ""]]}, {"id": "2106.15915", "submitter": "Marina Masioti", "authors": "Marina Masioti, Luke A. Prendergast, Amanda Shaker", "title": "On choosing optimal response transformations for dimension reduction", "comments": "20 pages, 6 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It has previously been shown that response transformations can be very\neffective in improving dimension reduction outcomes for a continuous response.\nThe choice of transformation used can make a big difference in the\nvisualization of the response versus the dimension reduced regressors. In this\narticle, we provide an automated approach for choosing parameters of\ntransformation functions to seek optimal results. A criterion based on an\ninfluence measure between dimension reduction spaces is utilized for choosing\nthe optimal parameter value of the transformation. Since influence measures can\nbe time-consuming for large data sets, two efficient criteria are also\nprovided. Given that a different transformation may be suitable for each\ndirection required to form the subspace, we also employ an iterative approach\nto choosing optimal parameter values. Several simulation studies and a real\ndata example highlight the effectiveness of the proposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 09:16:24 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Masioti", "Marina", ""], ["Prendergast", "Luke A.", ""], ["Shaker", "Amanda", ""]]}, {"id": "2106.15948", "submitter": "Silvia Pandolfi Dr", "authors": "Silvia Pandolfi, Francesco Bartolucci, Fulvia Pennoni", "title": "Maximum likelihood estimation of hidden Markov models for continuous\n  longitudinal data with missing responses and dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an inferential approach for maximum likelihood estimation of the\nhidden Markov models for continuous responses. We extend to the case of\nlongitudinal observations the finite mixture model of multivariate Gaussian\ndistributions with Missing At Random (MAR) outcomes, also accounting for\npossible dropout. The resulting hidden Markov model accounts for different\ntypes of missing pattern: (i) partially missing outcomes at a given time\noccasion; (ii) completely missing outcomes at a given time occasion\n(intermittent pattern); (iii) dropout before the end of the period of\nobservation (monotone pattern). The MAR assumption is formulated to deal with\nthe first two types of missingness, while to account for informative dropout we\nassume an extra absorbing state. Maximum likelihood estimation of the model\nparameters is based on an extended Expectation-Maximization algorithm relying\non suitable recursions. The proposal is illustrated by a Monte Carlo simulation\nstudy and an application based on historical data on primary biliary\ncholangitis.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 09:57:25 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Pandolfi", "Silvia", ""], ["Bartolucci", "Francesco", ""], ["Pennoni", "Fulvia", ""]]}, {"id": "2106.16022", "submitter": "Jamil Ownuk", "authors": "Jamil Ownuk and Ahmad Nezakati and Hossein Baghishani", "title": "Developing flexible classes of distributions to account for both\n  skewness and bimodality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop two novel approaches for constructing skewed and bimodal flexible\ndistributions that can effectively generalize classical symmetric\ndistributions. We illustrate the application of introduced techniques by\nextending normal, student-t, and Laplace distributions. We also study the\nproperties of the newly constructed distributions. The method of maximum\nlikelihood is proposed for estimating the model parameters. Furthermore, the\napplication of new distributions is represented using real-life data.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 12:39:44 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ownuk", "Jamil", ""], ["Nezakati", "Ahmad", ""], ["Baghishani", "Hossein", ""]]}, {"id": "2106.16042", "submitter": "Dong Xia", "authors": "Zhongyuan Lyu and Dong Xia and Yuan Zhang", "title": "Latent Space Model for Higher-order Networks and Generalized Tensor\n  Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a unified framework, formulated as general latent space models,\nto study complex higher-order network interactions among multiple entities. Our\nframework covers several popular models in recent network analysis literature,\nincluding mixture multi-layer latent space model and hypergraph latent space\nmodel. We formulate the relationship between the latent positions and the\nobserved data via a generalized multilinear kernel as the link function. While\nour model enjoys decent generality, its maximum likelihood parameter estimation\nis also convenient via a generalized tensor decomposition procedure.We propose\na novel algorithm using projected gradient descent on Grassmannians. We also\ndevelop original theoretical guarantees for our algorithm. First, we show its\nlinear convergence under mild conditions. Second, we establish finite-sample\nstatistical error rates of latent position estimation, determined by the signal\nstrength, degrees of freedom and the smoothness of link function, for both\ngeneral and specific latent space models. We demonstrate the effectiveness of\nour method on synthetic data. We also showcase the merit of our method on two\nreal-world datasets that are conventionally described by different specific\nmodels in producing meaningful and interpretable parameter estimations and\naccurate link prediction. We demonstrate the effectiveness of our method on\nsynthetic data. We also showcase the merit of our method on two real-world\ndatasets that are conventionally described by different specific models in\nproducing meaningful and interpretable parameter estimations and accurate link\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 13:11:17 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Lyu", "Zhongyuan", ""], ["Xia", "Dong", ""], ["Zhang", "Yuan", ""]]}, {"id": "2106.16094", "submitter": "Daisuke Murakami", "authors": "Tomoko Matsui, Nourddine Azzaoui, Daisuke Murakami", "title": "Analysis of COVID-19 evolution based on testing closeness of sequential\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A practical algorithm has been developed for closeness analysis of sequential\ndata that combines closeness testing with algorithms based on the Markov chain\ntester. It was applied to reported sequential data for COVID-19 to analyze the\nevolution of COVID-19 during a certain time period (week, month, etc.).\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 14:39:19 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Matsui", "Tomoko", ""], ["Azzaoui", "Nourddine", ""], ["Murakami", "Daisuke", ""]]}, {"id": "2106.16120", "submitter": "Leo Duan", "authors": "Leo L. Duan, David B. Dunson", "title": "Bayesian Spanning Tree: Estimating the Backbone of the Dependence Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multivariate data analysis, it is often important to estimate a graph\ncharacterizing dependence among (p) variables. A popular strategy uses the\nnon-zero entries in a (p\\times p) covariance or precision matrix, typically\nrequiring restrictive modeling assumptions for accurate graph recovery. To\nimprove model robustness, we instead focus on estimating the {\\em backbone} of\nthe dependence graph. We use a spanning tree likelihood, based on a minimalist\ngraphical model that is purposely overly-simplified. Taking a Bayesian\napproach, we place a prior on the space of trees and quantify uncertainty in\nthe graphical model. In both theory and experiments, we show that this model\ndoes not require the population graph to be a spanning tree or the covariance\nto satisfy assumptions beyond positive-definiteness. The model accurately\nrecovers the backbone of the population graph at a rate competitive with\nexisting approaches but with better robustness. We show combinatorial\nproperties of the spanning tree, which may be of independent interest, and\ndevelop an efficient Gibbs sampler for Bayesian inference. Analyzing\nelectroencephalography data using a Hidden Markov Model with each latent state\nmodeled by a spanning tree, we show that results are much more interpretable\ncompared with popular alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:19:10 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Duan", "Leo L.", ""], ["Dunson", "David B.", ""]]}, {"id": "2106.16124", "submitter": "Emmett Kendall", "authors": "Emmett Kendall, Brenden Beck, Joseph Antonelli", "title": "Robust inference for geographic regression discontinuity designs:\n  assessing the impact of police precincts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study variation in policing outcomes attributable to differential policing\npractices in New York City (NYC) using geographic regression discontinuity\ndesigns. By focusing on small geographic windows near police precinct\nboundaries we can estimate local average treatment effects of precincts on\narrest rates. The standard geographic regression discontinuity design relies on\ncontinuity assumptions of the potential outcome surface or a local\nrandomization assumption within a window around the boundary. While these\nassumptions are often thought to be more realistic than other assumptions used\nto infer causality from observational data, they can easily be violated in\nrealistic applications. We develop a novel and robust approach to testing\nwhether there are differences in policing outcomes that are caused by\ndifferences in police precincts across NYC. In particular, our test is robust\nto violations of the assumptions traditionally made in geographic regression\ndiscontinuity designs and is valid under much weaker assumptions. We use a\nunique form of resampling to identify new geographic boundaries that are known\nto have no treatment effect, which provides a valid estimate of our estimator's\nnull distribution even under violations of standard assumptions. We find that\nthis procedure gives substantially different results in the analysis of NYC\narrest rates than those that rely on standard assumptions, thereby providing\nmore robust estimates of the nature of the effect of police precincts on arrest\nrates in NYC.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:20:46 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Kendall", "Emmett", ""], ["Beck", "Brenden", ""], ["Antonelli", "Joseph", ""]]}, {"id": "2106.16149", "submitter": "Thomas Delerue", "authors": "Carsten Chong and Thomas Delerue and Guoying Li", "title": "Mixed semimartingales: Volatility estimation in the presence of\n  fractional noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR q-fin.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of estimating volatility for high-frequency data when\nthe observed process is the sum of a continuous It\\^o semimartingale and a\nnoise process that locally behaves like fractional Brownian motion with Hurst\nparameter H. The resulting class of processes, which we call mixed\nsemimartingales, generalizes the mixed fractional Brownian motion introduced by\nCheridito [Bernoulli 7 (2001) 913-934] to time-dependent and stochastic\nvolatility. Based on central limit theorems for variation functionals, we\nderive consistent estimators and asymptotic confidence intervals for H and the\nintegrated volatilities of both the semimartingale and the noise part, in all\ncases where these quantities are identifiable. When applied to recent stock\nprice data, we find strong empirical evidence for the presence of fractional\nnoise, with Hurst parameters H that vary considerably over time and between\nassets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2021 15:50:35 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Chong", "Carsten", ""], ["Delerue", "Thomas", ""], ["Li", "Guoying", ""]]}]