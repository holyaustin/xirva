[{"id": "1008.0055", "submitter": "Christian Sch\\\"afer", "authors": "Christian Sch\\\"afer (CREST, CEREMADE)", "title": "Parametric families on large binary spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of adaptive Monte Carlo algorithms, we cannot directly\ngenerate independent samples from the distribution of interest but use a proxy\nwhich we need to be close to the target. Generally, such a proxy distribution\nis a parametric family on the sampling spaces of the target distribution. For\ncontinuous sampling problems in high dimensions, we often use the multivariate\nnormal distribution as a proxy for we can easily parametrise it by its moments\nand quickly sample from it. Our objective is to construct similarly flexible\nparametric families on binary sampling spaces too large for exhaustive\nenumeration. The binary sampling problem is more difficult than its continuous\ncounterpart since the choice of a suitable proxy distribution is not obvious.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jul 2010 06:23:55 GMT"}, {"version": "v2", "created": "Fri, 18 Feb 2011 12:34:42 GMT"}, {"version": "v3", "created": "Mon, 21 Feb 2011 10:18:26 GMT"}, {"version": "v4", "created": "Thu, 24 Feb 2011 12:29:15 GMT"}, {"version": "v5", "created": "Mon, 14 Nov 2011 18:16:55 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Sch\u00e4fer", "Christian", "", "CREST, CEREMADE"]]}, {"id": "1008.0121", "submitter": "Roberto Casarin", "authors": "R. Casarin and L. Dalla Valle, F. Leisen", "title": "Bayesian Model Selection for Beta Autoregressive Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deal with Bayesian inference for Beta autoregressive processes. We\nrestrict our attention to the class of conditionally linear processes. These\nprocesses are particularly suitable for forecasting purposes, but are difficult\nto estimate due to the constraints on the parameter space. We provide a full\nBayesian approach to the estimation and include the parameter restrictions in\nthe inference problem by a suitable specification of the prior distributions.\nMoreover in a Bayesian framework parameter estimation and model choice can be\nsolved simultaneously. In particular we suggest a Markov-Chain Monte Carlo\n(MCMC) procedure based on a Metropolis-Hastings within Gibbs algorithm and\nsolve the model selection problem following a reversible jump MCMC approach.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jul 2010 20:30:32 GMT"}], "update_date": "2010-08-03", "authors_parsed": [["Casarin", "R.", ""], ["Valle", "L. Dalla", ""], ["Leisen", "F.", ""]]}, {"id": "1008.0149", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Balakrishnan B. Kannan, Ben Lasscock, Chris Mellen\n  and Simon Godsill", "title": "Bayesian Cointegrated Vector Autoregression models incorporating\n  Alpha-stable noise for inter-day price movements via Approximate Bayesian\n  Computation", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.CP stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a statistical model for pairs of traded assets, based on a\nCointegrated Vector Auto Regression (CVAR) Model. We extend standard CVAR\nmodels to incorporate estimation of model parameters in the presence of price\nseries level shifts which are not accurately modeled in the standard Gaussian\nerror correction model (ECM) framework. This involves developing a novel matrix\nvariate Bayesian CVAR mixture model comprised of Gaussian errors intra-day and\nAlpha-stable errors inter-day in the ECM framework. To achieve this we derive a\nnovel conjugate posterior model for the Scaled Mixtures of Normals (SMiN CVAR)\nrepresentation of Alpha-stable inter-day innovations. These results are\ngeneralized to asymmetric models for the innovation noise at inter-day\nboundaries allowing for skewed Alpha-stable models.\n  Our proposed model and sampling methodology is general, incorporating the\ncurrent literature on Gaussian models as a special subclass and also allowing\nfor price series level shifts either at random estimated time points or known a\npriori time points. We focus analysis on regularly observed non-Gaussian level\nshifts that can have significant effect on estimation performance in\nstatistical models failing to account for such level shifts, such as at the\nclose and open of markets. We compare the estimation accuracy of our model and\nestimation approach to standard frequentist and Bayesian procedures for CVAR\nmodels when non-Gaussian price series level shifts are present in the\nindividual series, such as inter-day boundaries. We fit a bi-variate\nAlpha-stable model to the inter-day jumps and model the effect of such jumps on\nestimation of matrix-variate CVAR model parameters using the likelihood based\nJohansen procedure and a Bayesian estimation. We illustrate our model and the\ncorresponding estimation procedures we develop on both synthetic and actual\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 1 Aug 2010 06:29:32 GMT"}], "update_date": "2010-08-03", "authors_parsed": [["Peters", "Gareth W.", ""], ["Kannan", "Balakrishnan B.", ""], ["Lasscock", "Ben", ""], ["Mellen", "Chris", ""], ["Godsill", "Simon", ""]]}, {"id": "1008.0415", "submitter": "Grace Wahba", "authors": "Xiwen Ma, Bin Dai, Ronald Klein, Barbara E.K. Klein, Kristine E. Lee\n  and Grace Wahba", "title": "Penalized Likelihood Regression in Reproducing Kernel Hilbert Spaces\n  with Randomized Covariate Data", "comments": "46 pages Missing data is a special case of the general theory here", "journal-ref": null, "doi": null, "report-no": "University of Wisconsin-Madison Statistics Department TR 1158", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical penalized likelihood regression problems deal with the case that\nthe independent variables data are known exactly. In practice, however, it is\ncommon to observe data with incomplete covariate information. We are concerned\nwith a fundamentally important case where some of the observations do not\nrepresent the exact covariate information, but only a probability distribution.\nIn this case, the maximum penalized likelihood method can be still applied to\nestimating the regression function. We first show that the maximum penalized\nlikelihood estimate exists under a mild condition. In the computation, we\npropose a dimension reduction technique to minimize the penalized likelihood\nand derive a GACV (Generalized Approximate Cross Validation) to choose the\nsmoothing parameter. Our methods are extended to handle more complicated\nincomplete data problems, such as, covariate measurement error and partially\nmissing covariates.\n", "versions": [{"version": "v1", "created": "Mon, 2 Aug 2010 21:46:11 GMT"}], "update_date": "2010-08-04", "authors_parsed": [["Ma", "Xiwen", ""], ["Dai", "Bin", ""], ["Klein", "Ronald", ""], ["Klein", "Barbara E. K.", ""], ["Lee", "Kristine E.", ""], ["Wahba", "Grace", ""]]}, {"id": "1008.1345", "submitter": "Lixing Zhu", "authors": "Lu Lin, Lixing Zhu and Yujie Gai", "title": "Adaptive post-Dantzig estimation and prediction for non-sparse \"large\n  $p$ and small $n$\" models", "comments": "37", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  For consistency (even oracle properties) of estimation and model prediction,\nalmost all existing methods of variable/feature selection critically depend on\nsparsity of models. However, for ``large $p$ and small $n$\" models sparsity\nassumption is hard to check and particularly, when this assumption is violated,\nthe consistency of all existing estimations is usually impossible because\nworking models selected by existing methods such as the LASSO and the Dantzig\nselector are usually biased. To attack this problem, we in this paper propose\nadaptive post-Dantzig estimation and model prediction. Here the adaptability\nmeans that the consistency based on the newly proposed method is adaptive to\nnon-sparsity of model, choice of shrinkage tuning parameter and dimension of\npredictor vector. The idea is that after a sub-model as a working model is\ndetermined by the Dantzig selector, we construct a globally unbiased sub-model\nby choosing suitable instrumental variables and nonparametric adjustment. The\nnew estimation of the parameters in the sub-model can be of the asymptotic\nnormality. The consistent estimator, together with the selected sub-model and\nadjusted model, improves model predictions. Simulation studies show that the\nnew approach has the significant improvement of estimation and prediction\naccuracies over the Gaussian Dantzig selector and other classical methods have.\n", "versions": [{"version": "v1", "created": "Sat, 7 Aug 2010 14:28:06 GMT"}], "update_date": "2010-08-10", "authors_parsed": [["Lin", "Lu", ""], ["Zhu", "Lixing", ""], ["Gai", "Yujie", ""]]}, {"id": "1008.1393", "submitter": "Zoltan Szabo", "authors": "Zoltan Szabo", "title": "Towards Nonstationary, Nonparametric Independent Process Analysis with\n  Unknown Source Component Dimensions", "comments": "9 pages, 3 figures", "journal-ref": "European Signal Processing Conference (EUSIPCO), 1718-1722, 2011", "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.DS math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to extend independent subspace analysis (ISA) to\nthe case of (i) nonparametric, not strictly stationary source dynamics and (ii)\nunknown source component dimensions. We make use of functional autoregressive\n(fAR) processes to model the temporal evolution of the hidden sources. An\nextension of the ISA separation principle--which states that the ISA problem\ncan be solved by traditional independent component analysis (ICA) and\nclustering of the ICA elements--is derived for the solution of the defined fAR\nindependent process analysis task (fAR-IPA): applying fAR identification we\nreduce the problem to ISA. A local averaging approach, the Nadaraya-Watson\nkernel regression technique is adapted to obtain strongly consistent fAR\nestimation. We extend the Amari-index to different dimensional components and\nillustrate the efficiency of the fAR-IPA approach by numerical examples.\n", "versions": [{"version": "v1", "created": "Sun, 8 Aug 2010 09:31:57 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Szabo", "Zoltan", ""]]}, {"id": "1008.1550", "submitter": "Daniel Saban\\'es Bov\\'e", "authors": "Daniel Saban\\'es Bov\\'e and Leonhard Held", "title": "Hyper-g Priors for Generalized Linear Models", "comments": "30 pages, 12 figures, poster contribution at ISBA 2010", "journal-ref": "Published in Bayesian Analysis (2011) volume 6, number 3, pages\n  387-410", "doi": "10.1214/11-BA615", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an extension of the classical Zellner's g-prior to generalized\nlinear models. The prior on the hyperparameter g is handled in a flexible way,\nso that any continuous proper hyperprior f(g) can be used, giving rise to a\nlarge class of hyper-g priors. Connections with the literature are described in\ndetail. A fast and accurate integrated Laplace approximation of the marginal\nlikelihood makes inference in large model spaces feasible. For posterior\nparameter estimation we propose an efficient and tuning-free\nMetropolis-Hastings sampler. The methodology is illustrated with variable\nselection and automatic covariate transformation in the Pima Indians diabetes\ndata set.\n", "versions": [{"version": "v1", "created": "Mon, 9 Aug 2010 17:11:17 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Bov\u00e9", "Daniel Saban\u00e9s", ""], ["Held", "Leonhard", ""]]}, {"id": "1008.1636", "submitter": "Andrew C. Thomas", "authors": "Andrew C. Thomas", "title": "Censoring Outdegree Compromises Inferences of Social Network Peer\n  Effects and Autocorrelation", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I examine the consequences of modelling contagious influence in a social\nnetwork with incomplete edge information, namely in the situation where each\nindividual may name a limited number of friends, so that extra outbound ties\nare censored. In particular, I consider a prototypical time series\nconfiguration where a property of the \"ego\" is affected in a causal fashion by\nthe properties of their \"alters\" at a previous time point, both in the total\nnumber of alters as well as the deviation from a central value. This is\nconsidered with three potential methods for naming one's friends: a strict\nupper limit on the number of declarations, a flexible limit, and an instruction\nwhere a person names a prespecified fraction of their friends. I find that one\nof two effects is present in the estimation of these effects: either that the\nsize of the effect is inflated in magnitude, or that the estimators instead are\ncentered about zero rather than related to the true effect. The degree of\nheterogeneity in friend count is one of the major factors into whether such an\nanalysis can be salvaged by post-hoc adjustments.\n", "versions": [{"version": "v1", "created": "Tue, 10 Aug 2010 05:41:07 GMT"}, {"version": "v2", "created": "Thu, 6 Jan 2011 18:55:20 GMT"}], "update_date": "2011-01-07", "authors_parsed": [["Thomas", "Andrew C.", ""]]}, {"id": "1008.1647", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "Gaussian Process Models for Nonparametric Functional Regression with\n  Functional Responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently nonparametric functional model with functional responses has been\nproposed within the functional reproducing kernel Hilbert spaces (fRKHS)\nframework. Motivated by its superior performance and also its limitations, we\npropose a Gaussian process model whose posterior mode coincide with the fRKHS\nestimator. The Bayesian approach has several advantages compared to its\npredecessor. Firstly, the multiple unknown parameters can be inferred together\nwith the regression function in a unified framework. Secondly, as a Bayesian\nmethod, the statistical inferences are straightforward through the posterior\ndistributions. We also use the predictive process models adapted from the\nspatial statistics literature to overcome the computational limitations, thus\nextending the applicability of this popular technique to a new problem.\nModifications of predictive process models are nevertheless critical in our\ncontext to obtain valid inferences. The numerical results presented demonstrate\nthe effectiveness of the modifications.\n", "versions": [{"version": "v1", "created": "Tue, 10 Aug 2010 08:30:14 GMT"}], "update_date": "2010-08-11", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "1008.1909", "submitter": "Djalel Eddine Meskaldji", "authors": "Djalel Eddine Meskaldji, Leila Cammoun, Patric Hagmann, Reto Meuli,\n  Jean Philippe Thiran and Stephan Morgenthaler", "title": "Efficient statistical analysis of large correlated multivariate\n  datasets: a case study on brain connectivity matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.NC stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroimaging, a large number of correlated tests are routinely performed\nto detect active voxels in single-subject experiments or to detect regions that\ndiffer between individuals belonging to different groups. In order to bound the\nprobability of a false discovery of pair-wise differences, a Bonferroni or\nother correction for multiplicity is necessary. These corrections greatly\nreduce the power of the comparisons which means that small signals\n(differences) remain hidden and therefore have been more or less successful\ndepending on the application. We introduce a method that improves the power of\na family of correlated statistical tests by reducing their number in an orderly\nfashion using our a-priori understanding of the problem . The tests are grouped\nby blocks that respect the data structure and only one or a few tests per group\nare performed. For each block we construct an appropriate summary statistic\nthat characterizes a meaningful feature of the block. The comparisons are based\non these summary statistics by a block-wise approach. We contrast this method\nwith the one based on the individual measures in terms of power. Finally, we\napply the method to compare brain connectivity matrices. Although the method is\nused in this study on the particular case of imaging, the proposed strategy can\nbe applied to a large variety of problems that involves multiple comparisons\nwhen the tests can be grouped according to attributes that depend on the\nspecific problem. Keywords and phrases: Multiple comparisons ; Family-wise\nerror rate; False discovery rate; Bonferroni procedure; Human brain\nconnectivity; Brain connectivity matrices.\n", "versions": [{"version": "v1", "created": "Wed, 11 Aug 2010 14:01:13 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Meskaldji", "Djalel Eddine", ""], ["Cammoun", "Leila", ""], ["Hagmann", "Patric", ""], ["Meuli", "Reto", ""], ["Thiran", "Jean Philippe", ""], ["Morgenthaler", "Stephan", ""]]}, {"id": "1008.1924", "submitter": "Armin Schwartzman", "authors": "Armin Schwartzman, Yulia Gavrilov, Robert J. Adler", "title": "Peak Detection as Multiple Testing", "comments": "37 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of detecting equal-shaped non-overlapping\nunimodal peaks in the presence of Gaussian ergodic stationary noise, where the\nnumber, location and heights of the peaks are unknown. A multiple testing\napproach is proposed in which, after kernel smoothing, the presence of a peak\nis tested at each observed local maximum. The procedure provides strong control\nof the family wise error rate and the false discovery rate asymptotically as\nboth the signal-to-noise ratio (SNR) and the search space get large, where the\nsearch space may grow exponentially as a function of SNR. Simulations assuming\na Gaussian peak shape and a Gaussian autocorrelation function show that desired\nerror levels are achieved for relatively low SNR and are robust to partial peak\noverlap. Simulations also show that detection power is maximized when the\nsmoothing bandwidth is close to the bandwidth of the signal peaks, akin to the\nwell-known matched filter theorem in signal processing. The procedure is\nillustrated in an analysis of electrical recordings of neuronal cell activity.\n", "versions": [{"version": "v1", "created": "Wed, 11 Aug 2010 14:52:33 GMT"}], "update_date": "2010-08-12", "authors_parsed": [["Schwartzman", "Armin", ""], ["Gavrilov", "Yulia", ""], ["Adler", "Robert J.", ""]]}, {"id": "1008.2028", "submitter": "Daphne Koller", "authors": "Suchi Saria, Daphne Koller and Anna Penn", "title": "Discovering shared and individual latent structure in multiple time\n  series", "comments": "Additional supplementary section in tex file", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a nonparametric Bayesian method for exploratory data\nanalysis and feature construction in continuous time series. Our method focuses\non understanding shared features in a set of time series that exhibit\nsignificant individual variability. Our method builds on the framework of\nlatent Diricihlet allocation (LDA) and its extension to hierarchical Dirichlet\nprocesses, which allows us to characterize each series as switching between\nlatent ``topics'', where each topic is characterized as a distribution over\n``words'' that specify the series dynamics. However, unlike standard\napplications of LDA, we discover the words as we learn the model. We apply this\nmodel to the task of tracking the physiological signals of premature infants;\nour model obtains clinically significant insights as well as useful features\nfor supervised learning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Aug 2010 00:41:23 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Saria", "Suchi", ""], ["Koller", "Daphne", ""], ["Penn", "Anna", ""]]}, {"id": "1008.2169", "submitter": "Peter Hoff", "authors": "Peter D. Hoff", "title": "Separable covariance arrays via the Tucker product, with applications to\n  multivariate relational data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern datasets are often in the form of matrices or arrays,potentially\nhaving correlations along each set of data indices. For example, data involving\nrepeated measurements of several variables over time may exhibit temporal\ncorrelation as well as correlation among the variables. A possible model for\nmatrix-valued data is the class of matrix normal distributions, which is\nparametrized by two covariance matrices, one for each index set of the data. In\nthis article we describe an extension of the matrix normal model to accommodate\nmultidimensional data arrays, or tensors. We generate a class of array normal\ndistributions by applying a group of multilinear transformations to an array of\nindependent standard normal random variables. The covariance structures of the\nresulting class take the form of outer products of dimension-specific\ncovariance matrices. We derive some properties of these covariance structures\nand the corresponding array normal distributions, discuss maximum likelihood\nand Bayesian estimation of covariance parameters and illustrate the model in an\nanalysis of multivariate longitudinal network data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Aug 2010 17:31:48 GMT"}], "update_date": "2010-08-13", "authors_parsed": [["Hoff", "Peter D.", ""]]}, {"id": "1008.2218", "submitter": "Christopher Paciorek", "authors": "Christopher J. Paciorek", "title": "Combining spatial information sources while accounting for systematic\n  errors in proxies", "comments": "5 figures, 2 tables", "journal-ref": "Journal of the Royal Statistical Society, Series C (Applied\n  Statistics) 61: 429-451 (2012)", "doi": "10.1111/j.1467-9876.2011.01035.x", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental research increasingly uses high-dimensional remote sensing and\nnumerical model output to help fill space-time gaps between traditional\nobservations. Such output is often a noisy proxy for the process of interest.\nThus one needs to separate and assess the signal and noise (often called\ndiscrepancy) in the proxy given complicated spatio-temporal dependencies. Here\nI extend a popular two-likelihood hierarchical model using a more flexible\nrepresentation for the discrepancy. I employ the little-used Markov random\nfield approximation to a thin plate spline, which can capture small-scale\ndiscrepancy in a computationally efficient manner while better modeling smooth\nprocesses than standard conditional auto-regressive models. The increased\nflexibility reduces identifiability, but the lack of identifiability is\ninherent in the scientific context. I model particulate matter air pollution\nusing satellite aerosol and atmospheric model output proxies. The estimated\ndiscrepancies occur at a variety of spatial scales, with small-scale\ndiscrepancy particularly important. The examples indicate little predictive\nimprovement over modeling the observations alone. Similarly, in simulations\nwith an informative proxy, the presence of discrepancy and resulting\nidentifiability issues prevent improvement in prediction. The results highlight\nbut do not resolve the critical question of how best to use proxy information\nwhile minimizing the potential for proxy-induced error.\n", "versions": [{"version": "v1", "created": "Thu, 12 Aug 2010 21:07:14 GMT"}, {"version": "v2", "created": "Tue, 31 Aug 2010 18:04:49 GMT"}, {"version": "v3", "created": "Tue, 13 Sep 2011 04:57:59 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Paciorek", "Christopher J.", ""]]}, {"id": "1008.2271", "submitter": "Heng Lian", "authors": "Heng Lian", "title": "Flexible Shrinkage Estimation in High-Dimensional Varying Coefficient\n  Models", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of simultaneous variable selection and constant\ncoefficient identification in high-dimensional varying coefficient models based\non B-spline basis expansion. Both objectives can be considered as some type of\nmodel selection problems and we show that they can be achieved by a double\nshrinkage strategy. We apply the adaptive group Lasso penalty in models\ninvolving a diverging number of covariates, which can be much larger than the\nsample size, but we assume the number of relevant variables is smaller than the\nsample size via model sparsity. Such so-called ultra-high dimensional settings\nare especially challenging in semiparametric models as we consider here and has\nnot been dealt with before. Under suitable conditions, we show that consistency\nin terms of both variable selection and constant coefficient identification can\nbe achieved, as well as the oracle property of the constant coefficients. Even\nin the case that the zero and constant coefficients are known a priori, our\nresults appear to be new in that it reduces to semivarying coefficient models\n(a.k.a. partially linear varying coefficient models) with a diverging number of\ncovariates. We also theoretically demonstrate the consistency of a\nsemiparametric BIC-type criterion in this high-dimensional context, extending\nseveral previous results. The finite sample behavior of the estimator is\nevaluated by some Monte Carlo studies.\n", "versions": [{"version": "v1", "created": "Fri, 13 Aug 2010 08:58:46 GMT"}], "update_date": "2010-08-16", "authors_parsed": [["Lian", "Heng", ""]]}, {"id": "1008.2565", "submitter": "Minas Gjoka", "authors": "Minas Gjoka and Carter T. Butts and Maciej Kurant and Athina\n  Markopoulou", "title": "Multigraph Sampling of Online Social Networks", "comments": "IEEE Journal on Selected Areas in Communications (JSAC), Special\n  Issue on Measurement of Internet Topologies, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.DS cs.SI physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art techniques for probability sampling of users of online\nsocial networks (OSNs) are based on random walks on a single social relation\n(typically friendship). While powerful, these methods rely on the social graph\nbeing fully connected. Furthermore, the mixing time of the sampling process\nstrongly depends on the characteristics of this graph. In this paper, we\nobserve that there often exist other relations between OSN users, such as\nmembership in the same group or participation in the same event. We propose to\nexploit the graphs these relations induce, by performing a random walk on their\nunion multigraph. We design a computationally efficient way to perform\nmultigraph sampling by randomly selecting the graph on which to walk at each\niteration. We demonstrate the benefits of our approach through (i) simulation\nin synthetic graphs, and (ii) measurements of Last.fm - an Internet website for\nmusic with social networking features. More specifically, we show that\nmultigraph sampling can obtain a representative sample and faster convergence,\neven when the individual graphs fail, i.e., are disconnected or highly\nclustered.\n", "versions": [{"version": "v1", "created": "Mon, 16 Aug 2010 02:01:02 GMT"}, {"version": "v2", "created": "Fri, 17 Jun 2011 01:58:00 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Gjoka", "Minas", ""], ["Butts", "Carter T.", ""], ["Kurant", "Maciej", ""], ["Markopoulou", "Athina", ""]]}, {"id": "1008.2743", "submitter": "Gautam Pendse", "authors": "Gautam V. Pendse", "title": "PMOG: The projected mixture of Gaussians model with application to blind\n  source separation", "comments": "46 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the mixtures of Gaussians (MOG) model to the projected mixture of\nGaussians (PMOG) model. In the PMOG model, we assume that q dimensional input\ndata points z_i are projected by a q dimensional vector w into 1-D variables\nu_i. The projected variables u_i are assumed to follow a 1-D MOG model. In the\nPMOG model, we maximize the likelihood of observing u_i to find both the model\nparameters for the 1-D MOG as well as the projection vector w. First, we derive\nan EM algorithm for estimating the PMOG model. Next, we show how the PMOG model\ncan be applied to the problem of blind source separation (BSS). In contrast to\nconventional BSS where an objective function based on an approximation to\ndifferential entropy is minimized, PMOG based BSS simply minimizes the\ndifferential entropy of projected sources by fitting a flexible MOG model in\nthe projected 1-D space while simultaneously optimizing the projection vector\nw. The advantage of PMOG over conventional BSS algorithms is the more flexible\nfitting of non-Gaussian source densities without assuming near-Gaussianity (as\nin conventional BSS) and still retaining computational feasibility.\n", "versions": [{"version": "v1", "created": "Mon, 16 Aug 2010 19:26:17 GMT"}], "update_date": "2010-08-17", "authors_parsed": [["Pendse", "Gautam V.", ""]]}, {"id": "1008.2870", "submitter": "Bent J{\\o}rgensen", "authors": "Ren\\'e Holst and Bent J{\\o}rgensen", "title": "Efficient and Robust Estimation for a Class of Generalized Linear\n  Longitudinal Mixed Models", "comments": "22 pages, 3 figures, IMADA preprint, uploaded 2010-08-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a versatile and computationally efficient estimating equation\nmethod for a class of hierarchical multiplicative generalized linear mixed\nmodels with additive dispersion components, based on explicit modelling of the\ncovariance structure. The class combines longitudinal and random effects models\nand retains a marginal as well as a conditional interpretation. The estimation\nprocedure combines that of generalized estimating equations for the regression\nwith residual maximum likelihood estimation for the association parameters.\nThis avoids the multidimensional integral of the conventional generalized\nlinear mixed models likelihood and allows an extension of the robust empirical\nsandwich estimator for use with both association and regression parameters. The\nmethod is applied to a set of otolith data, used for age determination of fish.\n", "versions": [{"version": "v1", "created": "Tue, 17 Aug 2010 11:50:59 GMT"}], "update_date": "2010-08-18", "authors_parsed": [["Holst", "Ren\u00e9", ""], ["J\u00f8rgensen", "Bent", ""]]}, {"id": "1008.2877", "submitter": "Dr. Wolfgang A. Rolke", "authors": "Wolfgang Rolke and Angel Lopez", "title": "A Test for Equality of Distributions in High Dimensions", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an astro-ph.IM hep-ex stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method which tests whether or not two datasets (one of which\ncould be Monte Carlo generated) might come from the same distribution. Our\nmethod works in arbitrarily high dimensions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Aug 2010 12:27:16 GMT"}], "update_date": "2010-08-18", "authors_parsed": [["Rolke", "Wolfgang", ""], ["Lopez", "Angel", ""]]}, {"id": "1008.3150", "submitter": "Grigory Temnov Dr", "authors": "L.B. Klebanov, A.V. Kakosyan, S.T. Rachev and G. Temnov", "title": "On a class of distributions stable under random summation", "comments": "12 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a family of distributions having a property of\nstability-under-addition, provided that the number $\\nu$ of added-up random\nvariables in the random sum is also a random variable. We call the\ncorresponding property a \\,$\\nu$-stability and investigate the situation with\nthe semigroup generated by the generating function of $\\nu$ is commutative.\nUsing results from the theory of iterations of analytic functions, we show that\nthe characteristic function of such a $\\nu$-stable distribution can be\nrepresented in terms of Chebyshev polynomials, and for the case of $\\nu$-normal\ndistribution, the resulting characteristic function corresponds to the\nhyperbolic secant distribution. We discuss some specific properties of the\nclass and present particular examples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Aug 2010 19:12:53 GMT"}], "update_date": "2010-08-19", "authors_parsed": [["Klebanov", "L. B.", ""], ["Kakosyan", "A. V.", ""], ["Rachev", "S. T.", ""], ["Temnov", "G.", ""]]}, {"id": "1008.3427", "submitter": "Ying Wang", "authors": "Hristo S. Sendov, Ying Wang, and Ricardas Zitikis", "title": "Log-supermodularity of weight functions and the loading monotonicity of\n  weighted insurance premiums", "comments": "17 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM math.FA math.PR q-fin.CP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is motivated by a problem concerning the monotonicity of insurance\npremiums with respect to their loading parameter: the larger the parameter, the\nlarger the insurance premium is expected to be. This property, usually called\nloading monotonicity, is satisfied by premiums that appear in the literature.\nThe increased interest in constructing new insurance premiums has raised a\nquestion as to what weight functions would produce loading-monotonic premiums.\nIn this paper we demonstrate a decisive role of log-supermodularity in\nanswering this question. As a consequence, we establish - at a stroke - the\nloading monotonicity of a number of well-known insurance premiums and offer a\nhost of further weight functions, and consequently of premiums, thus\nillustrating the power of the herein suggested methodology for constructing\nloading-monotonic insurance premiums.\n", "versions": [{"version": "v1", "created": "Fri, 20 Aug 2010 02:25:43 GMT"}], "update_date": "2010-08-23", "authors_parsed": [["Sendov", "Hristo S.", ""], ["Wang", "Ying", ""], ["Zitikis", "Ricardas", ""]]}, {"id": "1008.4203", "submitter": "Paul Kabaila", "authors": "Davide Farchione and Paul Kabaila", "title": "Variable-width confidence intervals in Gaussian regression and penalized\n  maximum likelihood estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hard thresholding, LASSO , adaptive LASSO and SCAD point estimators have been\nsuggested for use in the linear regression context when most of the components\nof the regression parameter vector are believed to be zero, a sparsity type of\nassumption. Potscher and Schneider, 2010, Electronic Journal of Statistics,\nhave considered the properties of fixed-width confidence intervals that include\none of these point estimators (for all possible data values). They consider a\nnormal linear regression model with orthogonal regressors and show that these\nconfidence intervals are longer than the standard confidence interval (based on\nthe maximum likelihood estimator) when the tuning parameter for these point\nestimators is chosen to lead to either conservative or consistent model\nselection. We extend this analysis to the case of variable-width confidence\nintervals that include one of these point estimators (for all possible data\nvalues). In consonance with these findings of Potscher and Schneider, we find\nthat these confidence intervals perform poorly by comparison with the standard\nconfidence interval, when the tuning parameter for these point estimators is\nchosen to lead to consistent model selection. However, when the tuning\nparameter for these point estimators is chosen to lead to conservative model\nselection, our conclusions differ from those of Potscher and Schneider. We\nconsider the variable-width confidence intervals of Farchione and Kabaila,\n2008, Statistics & Probability Letters, which have advantages over the standard\nconfidence interval in the context that there is a belief in a sparsity type of\nassumption. These variable-width confidence intervals are shown to include the\nhard thresholding, LASSO, adaptive LASSO and SCAD estimators (for all possible\ndata values) provided that the tuning parameters for these estimators are\nchosen to belong to an appropriate interval.\n", "versions": [{"version": "v1", "created": "Wed, 25 Aug 2010 05:15:24 GMT"}], "update_date": "2010-08-26", "authors_parsed": [["Farchione", "Davide", ""], ["Kabaila", "Paul", ""]]}, {"id": "1008.4316", "submitter": "Bodhisattva Sen", "authors": "Bodhisattva Sen, Moulinath Banerjee, George Michialidis", "title": "Threshold estimation based on a P-value framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use p-values as a discrepancy criterion for identifying the threshold\nvalue at which a regression function takes off from its baseline value -- a\nproblem that is motivated by applications in omics experiments, systems\nengineering, pharmacological dose-response studies and astronomy. In this\npaper, we study the problem in a controlled sampling setting, where multiple\nresponses, discrete or continuous, can be obtained at a number of different\ncovariate-levels. Our procedure involves testing the hypothesis that the\nregression function is at its baseline at each covariate value using the\nsampled responses at that value and then computing the p-value of the test. An\nestimate of the threshold is provided by fitting a stump, i.e., a piecewise\nconstant function with a single jump discontinuity, to the observed p-values,\nsince the corresponding p-values behave in markedly different ways on different\nsides of the threshold. The estimate is shown to be consistent, as both the\nnumber of covariate values and the number of responses sampled at each value\nbecome large, and its finite sample properties are studied through an extensive\nsimulation study. Our approach is computationally simple and can also be used\nto estimate the baseline value of the regression function. The procedure is\nillustrated on two motivating real data applications. Extensions to multiple\nthresholds are also briefly investigated.\n", "versions": [{"version": "v1", "created": "Wed, 25 Aug 2010 17:22:47 GMT"}], "update_date": "2010-08-26", "authors_parsed": [["Sen", "Bodhisattva", ""], ["Banerjee", "Moulinath", ""], ["Michialidis", "George", ""]]}, {"id": "1008.4373", "submitter": "Ritabrata Dutta", "authors": "Ritabrata Dutta, Jayanta K. Ghosh", "title": "Bayes Model Selection with Path Sampling: Factor Models and Other\n  Examples", "comments": "Published in at http://dx.doi.org/10.1214/12-STS403 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2013, Vol. 28, No. 1, 95-115", "doi": "10.1214/12-STS403", "report-no": "IMS-STS-STS403", "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a theorem justifying the regularity conditions which are needed for\nPath Sampling in Factor Models. We then show that the remaining ingredient,\nnamely, MCMC for calculating the integrand at each point in the path, may be\nseriously flawed, leading to wrong estimates of Bayes factors. We provide a new\nmethod of Path Sampling (with Small Change) that works much better than\nstandard Path Sampling in the sense of estimating the Bayes factor better and\nchoosing the correct model more often. When the more complex factor model is\ntrue, PS-SC is substantially more accurate. New MCMC diagnostics is provided\nfor these problems in support of our conclusions and recommendations. Some of\nour ideas for diagnostics and improvement in computation through small changes\nshould apply to other methods of computation of the Bayes factor for model\nselection.\n", "versions": [{"version": "v1", "created": "Wed, 25 Aug 2010 20:44:09 GMT"}, {"version": "v2", "created": "Tue, 31 Aug 2010 17:24:52 GMT"}, {"version": "v3", "created": "Sat, 2 Jul 2011 18:01:42 GMT"}, {"version": "v4", "created": "Thu, 21 Feb 2013 09:07:59 GMT"}], "update_date": "2013-02-22", "authors_parsed": [["Dutta", "Ritabrata", ""], ["Ghosh", "Jayanta K.", ""]]}, {"id": "1008.4504", "submitter": "Marie-Colette van Lieshout", "authors": "M.N.M. van Lieshout", "title": "A J-function for inhomogeneous point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new summary statistics for intensity-reweighted moment stationary\npoint processes that generalise the well known J-, empty space, and\nnearest-neighbour distance distribution functions, represent them in terms of\ngenerating functionals and conditional intensities, and relate them to the\ninhomogeneous reduced second moment function. Extensions to space time and\nmarked point processes are briefly discussed.\n", "versions": [{"version": "v1", "created": "Thu, 26 Aug 2010 13:17:15 GMT"}], "update_date": "2010-08-27", "authors_parsed": [["van Lieshout", "M. N. M.", ""]]}, {"id": "1008.5143", "submitter": "Lutz Mattner", "authors": "Lutz Mattner", "title": "Combining individually valid and conditionally i.i.d. P-variables", "comments": "13 pages. Various minor corrections and stylistic improvements. Data\n  displays added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given testing problem, let $U_1,...,U_n$ be individually valid and\nconditionally on the data i.i.d.\\ P-variables (often called P-values). For\nexample, the data could come in groups, and each $U_i$ could be based on\nsubsampling just one datum from each group in order to satisfy an independence\nassumption under the hypothesis. The problem is then to deterministically\ncombine the $U_i$ into a valid summary P-variable. Restricting here our\nattention to functions of a given order statistic $U_{k:n}$ of the $U_i$, we\ncompute the function $f_{n,k}$ which is smallest among all increasing functions\n$f$ such that $f(U_{k:n})$ is always a valid P-variable under the stated\nassumptions. Since $f_{n,k}(u)\\le 1\\wedge (\\frac {n}{k} u)$, with the right\nhand side being a good approximation for the left when $k$ is large, one may in\nparticular always take the minimum of 1 and twice the left sample median of the\ngiven P-variables.\n  We sketch the original application of the above in a recent study of\nassociations between various primate species by Astaras et al.\n", "versions": [{"version": "v1", "created": "Mon, 30 Aug 2010 19:22:29 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2011 16:18:31 GMT"}], "update_date": "2011-08-22", "authors_parsed": [["Mattner", "Lutz", ""]]}, {"id": "1008.5319", "submitter": "M{\\aa}ns Thulin", "authors": "M{\\aa}ns Thulin", "title": "On two simple tests for normality with high power", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The test statistics of two powerful tests for normality \\citep{lm1,mud2} are\nestimators of the correlation coefficient between certain sample moments. We\nderive new versions of the test statistics that are functions of the sample\nskewness and sample kurtosis. This sheds some light on the nature of these\ntests and leads to easier computations.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 14:01:59 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2011 12:29:42 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2011 06:20:05 GMT"}], "update_date": "2011-08-03", "authors_parsed": [["Thulin", "M\u00e5ns", ""]]}, {"id": "1008.5372", "submitter": "Yong Zhang", "authors": "Zhaosong Lu and Yong Zhang", "title": "Penalty Decomposition Methods for $L0$-Norm Minimization", "comments": "This paper has been withdrawn by the author because an updated\n  version has been resubmitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.CV cs.IT cs.LG cs.NA math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider general l0-norm minimization problems, that is, the\nproblems with l0-norm appearing in either objective function or constraint. In\nparticular, we first reformulate the l0-norm constrained problem as an\nequivalent rank minimization problem and then apply the penalty decomposition\n(PD) method proposed in [33] to solve the latter problem. By utilizing the\nspecial structures, we then transform all matrix operations of this method to\nvector operations and obtain a PD method that only involves vector operations.\nUnder some suitable assumptions, we establish that any accumulation point of\nthe sequence generated by the PD method satisfies a first-order optimality\ncondition that is generally stronger than one natural optimality condition. We\nfurther extend the PD method to solve the problem with the l0-norm appearing in\nobjective function. Finally, we test the performance of our PD methods by\napplying them to compressed sensing, sparse logistic regression and sparse\ninverse covariance selection. The computational results demonstrate that our\nmethods generally outperform the existing methods in terms of solution quality\nand/or speed.\n", "versions": [{"version": "v1", "created": "Tue, 31 Aug 2010 17:24:31 GMT"}, {"version": "v2", "created": "Fri, 11 May 2012 17:12:02 GMT"}], "update_date": "2012-05-14", "authors_parsed": [["Lu", "Zhaosong", ""], ["Zhang", "Yong", ""]]}]