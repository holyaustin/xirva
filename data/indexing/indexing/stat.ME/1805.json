[{"id": "1805.00057", "submitter": "Sokbae Lee", "authors": "Sokbae Lee, Bernard Salani\\'e", "title": "Identifying Effects of Multivalued Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivalued treatment models have typically been studied under restrictive\nassumptions: ordered choice, and more recently unordered monotonicity. We show\nhow treatment effects can be identified in a more general class of models that\nallows for multidimensional unobserved heterogeneity. Our results rely on two\nmain assumptions: treatment assignment must be a measurable function of\nthreshold-crossing rules, and enough continuous instruments must be available.\nWe illustrate our approach for several classes of models.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2018 18:43:25 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Lee", "Sokbae", ""], ["Salani\u00e9", "Bernard", ""]]}, {"id": "1805.00189", "submitter": "Zhen Li", "authors": "Zhen Li, Haiqin Chen, Tianli Li", "title": "Exploring the Accuracy of MIRT Scale Linking Procedures for Mixed-format\n  Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates the accuracy of Stocking-Lord scale linking\nprocedures for UIRT and MIRT models with common-item nonequivalent-group design\nfor mixed-format tests under two anchor conditions: MC-Only and MC-CR across\nthree different levels of format effects (FEs). Results provide recommendations\non the appropriateness of UIRT and two MIRT models when FEs present under each\nanchor scenario.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 05:11:59 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Li", "Zhen", ""], ["Chen", "Haiqin", ""], ["Li", "Tianli", ""]]}, {"id": "1805.00306", "submitter": "Sourish Das", "authors": "Sourish Das, Aritra Halder, Ananya Lahiri, Dipak K Dey", "title": "Modeling Risk and Return using Dirichlet Process Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we showed that the no-arbitrage condition holds if the market\nfollows the mixture of the geometric Brownian motion (GBM). The mixture of GBM\ncan incorporate heavy-tail behavior of the market. It automatically leads us to\nmodel the risk and return of multiple asset portfolios via the nonparametric\nBayesian method. We present a Dirichlet Process (DP) prior via an urn-scheme\nfor univariate modeling of the single asset return. This DP prior is presented\nin the spirit of dependent DP. We extend this approach to introduce a\nmultivariate distribution to model the return on multiple assets via an\nelliptical copula; which models the marginal distribution using the DP prior.\nWe compare different risk measures such as Value at Risk (VaR) and Conditional\nVaR (CVaR), also known as expected shortfall (ES) for the stock return data of\ntwo datasets. The first dataset contains the return of IBM, Intel and NASDAQ\nand the second dataset contains the return data of 51 stocks as part of the\nindex \"Nifty 50\" for Indian equity markets.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 13:08:09 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Das", "Sourish", ""], ["Halder", "Aritra", ""], ["Lahiri", "Ananya", ""], ["Dey", "Dipak K", ""]]}, {"id": "1805.00389", "submitter": "Magnus M\\\"unch", "authors": "Magnus M. M\\\"unch, Carel F.W. Peeters, Aad W. van der Vaart, Mark A.\n  van de Wiel", "title": "Adaptive group-regularized logistic elastic net regression", "comments": "19 pages, 5 figures, supplementary material available from first\n  author's personal website", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional data settings, additional information on the features is\noften available. Examples of such external information in omics research are:\n(a) p-values from a previous study, (b) a summary of prior information, and (c)\nomics annotation. The inclusion of this information in the analysis may enhance\nclassification performance and feature selection, but is not straightforward in\nthe standard regression setting. As a solution to this problem, we propose a\ngroup-regularized (logistic) elastic net regression method, where each penalty\nparameter corresponds to a group of features based on the external information.\nThe method, termed gren, makes use of the Bayesian formulation of logistic\nelastic net regression to estimate both the model and penalty parameters in an\napproximate empirical-variational Bayes framework. Simulations and an\napplication to a colon cancer microRNA study show that, if the partitioning of\nthe features is informative, classification performance and feature selection\nare indeed enhanced.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 15:21:48 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["M\u00fcnch", "Magnus M.", ""], ["Peeters", "Carel F. W.", ""], ["van der Vaart", "Aad W.", ""], ["van de Wiel", "Mark A.", ""]]}, {"id": "1805.00533", "submitter": "Ping Li", "authors": "Ping Li", "title": "Sign-Full Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of 1-bit (\"sign-sign\") random projections has been a popular tool\nfor efficient search and machine learning on large datasets. Given two $D$-dim\ndata vectors $u$, $v\\in\\mathbb{R}^D$, one can generate $x = \\sum_{i=1}^D u_i\nr_i$, and $y = \\sum_{i=1}^D v_i r_i$, where $r_i\\sim N(0,1)$ iid. The\n\"collision probability\" is ${Pr}\\left(sgn(x)=sgn(y)\\right) =\n1-\\frac{\\cos^{-1}\\rho}{\\pi}$, where $\\rho = \\rho(u,v)$ is the cosine\nsimilarity.\n  We develop \"sign-full\" random projections by estimating $\\rho$ from (e.g.,)\nthe expectation $E(sgn(x)y)=\\sqrt{\\frac{2}{\\pi}} \\rho$, which can be further\nsubstantially improved by normalizing $y$. For nonnegative data, we recommend\nan interesting estimator based on $E\\left(y_- 1_{x\\geq 0} + y_+ 1_{x<0}\\right)$\nand its normalized version. The recommended estimator almost matches the\naccuracy of the (computationally expensive) maximum likelihood estimator. At\nhigh similarity ($\\rho\\rightarrow1$), the asymptotic variance of recommended\nestimator is only $\\frac{4}{3\\pi} \\approx 0.4$ of the estimator for sign-sign\nprojections. At small $k$ and high similarity, the improvement would be even\nmuch more substantial.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2018 08:41:21 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1805.00541", "submitter": "Giacomo Zanella", "authors": "Giacomo Zanella and Gareth Roberts", "title": "Scalable Importance Tempering and Bayesian Variable Selection", "comments": "Online supplement not included", "journal-ref": "J. R. Statist. Soc. B (2019) 81, Part 3, pp. 489-517", "doi": "10.1111/rssb.12316", "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Monte Carlo algorithm to sample from high dimensional\nprobability distributions that combines Markov chain Monte Carlo and importance\nsampling. We provide a careful theoretical analysis, including guarantees on\nrobustness to high dimensionality, explicit comparison with standard Markov\nchain Monte Carlo methods and illustrations of the potential improvements in\nefficiency. Simple and concrete intuition is provided for when the novel scheme\nis expected to outperform standard schemes. When applied to Bayesian\nvariable-selection problems, the novel algorithm is orders of magnitude more\nefficient than available alternative sampling schemes and enables fast and\nreliable fully Bayesian inferences with tens of thousand regressors.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 20:15:53 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 09:05:02 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Zanella", "Giacomo", ""], ["Roberts", "Gareth", ""]]}, {"id": "1805.00550", "submitter": "Issa Dahabreh", "authors": "Issa J. Dahabreh, Sarah E. Robertson, Jon A. Steingrimsson, Elizabeth\n  A. Stuart, Miguel A. Hernan", "title": "Extending inferences from a randomized trial to a new target population", "comments": "changes in section 5", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When treatment effect modifiers influence the decision to participate in a\nrandomized trial, the average treatment effect in the population represented by\nthe randomized individuals will differ from the effect in other populations. In\nthis tutorial, we consider methods for extending causal inferences about\ntime-fixed treatments from a trial to a new target population of\nnon-participants, using data from a completed randomized trial and baseline\ncovariate data from a sample from the target population. We examine methods\nbased on modeling the expectation of the outcome, the probability of\nparticipation, or both (doubly robust). We compare the methods in a simulation\nstudy and show how they can be implemented in software. We apply the methods to\na randomized trial nested within a cohort of trial-eligible patients to compare\ncoronary artery surgery plus medical therapy versus medical therapy alone for\npatients with chronic coronary artery disease. We conclude by discussing issues\nthat arise when using the methods in applied analyses.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 21:04:24 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 21:00:18 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 10:14:59 GMT"}], "update_date": "2019-10-29", "authors_parsed": [["Dahabreh", "Issa J.", ""], ["Robertson", "Sarah E.", ""], ["Steingrimsson", "Jon A.", ""], ["Stuart", "Elizabeth A.", ""], ["Hernan", "Miguel A.", ""]]}, {"id": "1805.00555", "submitter": "Andrew Parnell", "authors": "John Haslett, Andrew Parnell, James Sweeney", "title": "A general framework for modelling zero inflation", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new framework for the modelling of count data exhibiting zero\ninflation (ZI). The main part of this framework includes a new and more general\nparameterisation for ZI models which naturally includes both over- and\nunder-inflation. It further sheds new theoretical light on modelling and\ninference and permits a simpler alternative, which we term as multiplicative,\nin contrast to the dominant mixture and hurdle models. Our approach gives the\nstatistician access to new types of ZI of which mixture and hurdle are special\ncases. We outline a simple parameterised modelling approach which can help to\ninfer both ZI type and degree and provide an underlying treatment that shows\nthat current ZI models are themselves typically within the exponential family,\nthus permitting much simpler theory, computation and classical inference. We\noutline some possibilities for a natural Bayesian framework for inference; and\na rich basis for work on correlated ZI counts.\n  The present paper is an incomplete report on the underlying theory. A later\nversion will include computational issues and provide further examples.\n", "versions": [{"version": "v1", "created": "Tue, 1 May 2018 21:17:45 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Haslett", "John", ""], ["Parnell", "Andrew", ""], ["Sweeney", "James", ""]]}, {"id": "1805.00649", "submitter": "David Gunawan", "authors": "David Gunawan, Robert Kohn, and Minh Ngoc Tran", "title": "Robust Particle Density Tempering for State Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density tempering (also called density annealing) for state space models is a\nsequential Monte Carlo (SMC) approach to Bayesian inference for general state\nmodels, that provides an alternative to MCMC. It moves a collection of\nparameters and latent states (which we call particles) through a number of\nstages, with each stage having its own target density. Initially, the particles\nare generated from a distribution that is easy to sample from, e.g. the prior;\nthe target density at the final stage is the posterior density of interest.\nTempering is usually carried out either in batch mode, involving all of the\ndata at each stage, or in sequential mode, where the tempering involves adding\nobservations at each stage; we call this data tempering. Our article proposes\ntwo innovations for particle based density tempering. First, data tempering is\nmade more robust to outliers and structural changes by adding batch tempering\nat each stage. Second, we propose generating the parameters and states at each\nstage using two Gibbs type Markov moves, where the parameters are generated\nconditional on the states and conversely. We explain how this allows the\ntempering to scale up in terms of the number parameters and states it can\nhandle. Most of the current literature uses a pseudo-marginal Markov move step\nwith the states integrated out and the parameters generated by a random walk\nproposal; this strategy is inefficient when the states or parameters are high\ndimensional. The article demonstrates the performance of the proposed methods\nusing univariate stochastic volatility models with outliers and structural\nbreaks and high dimensional factor stochastic volatility models having both a\nlarge number of parameters and a large number of latent state.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 07:01:39 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 08:23:21 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Gunawan", "David", ""], ["Kohn", "Robert", ""], ["Tran", "Minh Ngoc", ""]]}, {"id": "1805.00726", "submitter": "Kevin Wilson Dr", "authors": "Kevin J Wilson, Daniel A Henderson and John Quigley", "title": "Emulation of utility functions over a set of permutations: sequencing\n  reliability growth tasks", "comments": "Article accepted at Technometrics. The official journal version is\n  given here: https://doi.org/10.1080/00401706.2017.1377637", "journal-ref": null, "doi": "10.1080/00401706.2017.1377637", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian design of experiments problems in which we maximise the\nprior expectation of a utility function over a set of permutations, for example\nwhen sequencing a number of tasks to perform. When the number of tasks is large\nand the expected utility is expensive to compute, it may be unreasonable or\ninfeasible to evaluate the expected utility of all permutations. We propose an\napproach to emulate the expected utility using a surrogate function based on a\nparametric probabilistic model for permutations. The surrogate function is\nfitted by maximising the correlation with the expected utility over a set of\ntraining points. We propose a suitable transformation of the expected utility\nto improve the fit. We provide results linking the correlation between the two\nfunctions and the number of expected utility evaluations to undertake. The\napproach is applied to the sequencing of reliability growth tasks in the\ndevelopment of hardware systems, in which there is a large number of potential\ntasks to perform and engineers are interested in meeting a reliability target\nsubject to minimising costs and time. An illustrative example shows how the\napproach can be used and a simulation study demonstrates the performance of the\napproach more generally.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 10:57:04 GMT"}], "update_date": "2018-05-03", "authors_parsed": [["Wilson", "Kevin J", ""], ["Henderson", "Daniel A", ""], ["Quigley", "John", ""]]}, {"id": "1805.00753", "submitter": "Alexandra Suvorikova", "authors": "Francois Bachoc, Alexandra Suvorikova, David Ginsbourger, Jean-Michel\n  Loubes, Vladimir Spokoiny", "title": "Gaussian processes with multidimensional distribution inputs via optimal\n  transport and Hilbertian embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate Gaussian Processes indexed by multidimensional\ndistributions. While directly constructing radial positive definite kernels\nbased on the Wasserstein distance has been proven to be possible in the\nunidimensional case, such constructions do not extend well to the\nmultidimensional case as we illustrate here. To tackle the problem of defining\npositive definite kernels between multivariate distributions based on optimal\ntransport, we appeal instead to Hilbert space embeddings relying on optimal\ntransport maps to a reference distribution, that we suggest to take as a\nWasserstein barycenter. We characterize in turn radial positive definite\nkernels on Hilbert spaces, and show that the covariance parameters of virtually\nall parametric families of covariance functions are microergodic in the case of\n(infinite-dimensional) Hilbert spaces. We also investigate statistical\nproperties of our suggested positive definite kernels on multidimensional\ndistributions, with a focus on consistency when a population Wasserstein\nbarycenter is replaced by an empirical barycenter and additional explicit\nresults in the special case of Gaussian distributions. Finally, we study the\nGaussian process methodology based on our suggested positive definite kernels\nin regression problems with multidimensional distribution inputs, on simulation\ndata stemming both from synthetic examples and from a mechanical engineering\ntest case.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 11:59:34 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 18:52:37 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Bachoc", "Francois", ""], ["Suvorikova", "Alexandra", ""], ["Ginsbourger", "David", ""], ["Loubes", "Jean-Michel", ""], ["Spokoiny", "Vladimir", ""]]}, {"id": "1805.00829", "submitter": "Vivekananda Roy", "authors": "Vivekananda Roy and Evangelos Evangelou", "title": "Selection of proposal distributions for generalized importance sampling\n  estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard importance sampling (IS) estimator, generally does not work well\nin examples involving simultaneous inference on several targets as the\nimportance weights can take arbitrarily large values making the estimator\nhighly unstable. In such situations, alternative generalized IS estimators\ninvolving samples from multiple proposal distributions are preferred. Just like\nthe standard IS, the success of these multiple IS estimators crucially depends\non the choice of the proposal distributions. The selection of these proposal\ndistributions is the focus of this article. We propose three methods based on\n(i) a geometric space filling coverage criterion, (ii) a minimax variance\napproach, and (iii) a maximum entropy approach. The first two methods are\napplicable to any multi-proposal IS estimator, whereas the third approach is\ndescribed in the context of Doss's (2010) two-stage IS estimator. For the first\nmethod we propose a suitable measure of coverage based on the symmetric\nKullback-Leibler divergence, while the second and third approaches use\nestimates of asymptotic variances of Doss's (2010) IS estimator and Geyer's\n(1994) reverse logistic estimator, respectively. Thus, we provide consistent\nspectral variance estimators for these asymptotic variances. The proposed\nmethods for selecting proposal densities are illustrated using various detailed\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 14:08:37 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 16:26:40 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2019 11:26:43 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Roy", "Vivekananda", ""], ["Evangelou", "Evangelos", ""]]}, {"id": "1805.01010", "submitter": "Maitreyee Bose", "authors": "Maitreyee Bose, James S. Hodges, Sudipto Banerjee", "title": "Toward a diagnostic toolkit for linear models with Gaussian-process\n  distributed random effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are widely used as distributions of random effects\nin linear mixed models, which are fit using the restricted likelihood or the\nclosely-related Bayesian analysis. This article addresses two problems. First,\nwe propose tools for understanding how data determine estimates in these\nmodels, using a spectral basis approximation to the GP under which the\nrestricted likelihood is formally identical to the likelihood for a\ngamma-errors GLM with identity link. Second, to examine the data's support for\na covariate and to understand how adding that covariate moves variation in the\noutcome y out of the GP and error parts of the fit, we apply a linear-model\ndiagnostic, the added variable plot (AVP), both to the original observations\nand to projections of the data onto the spectral basis functions. The spectral-\nand observation-domain AVPs estimate the same coefficient for a covariate but\nemphasize low- and high-frequency data features respectively and thus highlight\nthe covariate's effect on the GP and error parts of the fit respectively. The\nspectral approximation applies to data observed on a regular grid; for data\nobserved at irregular locations, we propose smoothing the data to a grid before\napplying our methods. The methods are illustrated using the forest-biomass data\nof Finley et al.~(2008).\n", "versions": [{"version": "v1", "created": "Wed, 2 May 2018 20:35:48 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Bose", "Maitreyee", ""], ["Hodges", "James S.", ""], ["Banerjee", "Sudipto", ""]]}, {"id": "1805.01104", "submitter": "Jianeng Xu", "authors": "Guanhao Feng, Nicholas G. Polson and Jianeng Xu", "title": "Deep Learning in Characteristics-Sorted Factor Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study the characteristics-sorted factor model in asset pricing, we develop\na bottom-up approach with state-of-the-art deep learning optimization. With an\neconomic objective to minimize pricing errors, we train a non-reduced-form\nneural network using firm characteristics [inputs], and generate factors\n[intermediate features], to fit security returns [outputs]. Sorting securities\non firm characteristics provides a nonlinear activation to create long-short\nportfolio weights, as a hidden layer, from lag characteristics to realized\nreturns. Our model offers an alternative perspective for dimension reduction on\nfirm characteristics [inputs], rather than factors [intermediate features], and\nallows for both nonlinearity and interactions on inputs. Our empirical findings\nare twofold. We find robust statistical and economic evidence in out-of-sample\nportfolios and individual stock returns. To interpret our deep factors, we show\nhighly significant results in factor investing via the squared Sharpe ratio\ntest, as well as improvement in dissecting anomalies.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 03:55:43 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 10:35:30 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 21:51:12 GMT"}, {"version": "v4", "created": "Wed, 9 Oct 2019 17:19:49 GMT"}, {"version": "v5", "created": "Wed, 30 Oct 2019 20:14:38 GMT"}], "update_date": "2019-11-01", "authors_parsed": [["Feng", "Guanhao", ""], ["Polson", "Nicholas G.", ""], ["Xu", "Jianeng", ""]]}, {"id": "1805.01124", "submitter": "Hans-Peter Piepho", "authors": "Hans-Peter Piepho", "title": "A Coefficient of Determination (R2) for Linear Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extensions of linear models are very commonly used in the analysis of\nbiological data. Whereas goodness of fit measures such as the coefficient of\ndetermination (R2) or the adjusted R2 are well established for linear models,\nit is not obvious how such measures should be defined for generalized linear\nand mixed models. There are by now several proposals but no consensus has yet\nemerged as to the best unified approach in these settings. In particular, it is\nan open question how to best account for heteroscedasticity and for covariance\namong observations induced by random effects. This paper proposes a new\napproach that addresses this issue and is universally applicable. It is\nexemplified using three biological examples.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 05:48:54 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Piepho", "Hans-Peter", ""]]}, {"id": "1805.01417", "submitter": "Peter Rousseeuw", "authors": "Jakob Raymaekers and Peter J. Rousseeuw", "title": "A generalized spatial sign covariance matrix", "comments": null, "journal-ref": "Journal of Multivariate Analysis, 2019, Vol. 171, 94-111", "doi": "10.1016/j.jmva.2018.11.010", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known spatial sign covariance matrix (SSCM) carries out a radial\ntransform which moves all data points to a sphere, followed by computing the\nclassical covariance matrix of the transformed data. Its popularity stems from\nits robustness to outliers, fast computation, and applications to correlation\nand principal component analysis. In this paper we study more general radial\nfunctions. It is shown that the eigenvectors of the generalized SSCM are still\nconsistent and the ranks of the eigenvalues are preserved. The influence\nfunction of the resulting scatter matrix is derived, and it is shown that its\nbreakdown value is as high as that of the original SSCM. A simulation study\nindicates that the best results are obtained when the inner half of the data\npoints are not transformed and points lying far away are moved to the center.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 16:42:54 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 09:06:14 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Raymaekers", "Jakob", ""], ["Rousseeuw", "Peter J.", ""]]}, {"id": "1805.01500", "submitter": "Adji Bousso Dieng", "authors": "Adji B. Dieng, Rajesh Ranganath, Jaan Altosaar, David M. Blei", "title": "Noisin: Unbiased Regularization for Recurrent Neural Networks", "comments": "In Proceedings of the International Conference on Machine Learning,\n  2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) are powerful models of sequential data. They\nhave been successfully used in domains such as text and speech. However, RNNs\nare susceptible to overfitting; regularization is important. In this paper we\ndevelop Noisin, a new method for regularizing RNNs. Noisin injects random noise\ninto the hidden states of the RNN and then maximizes the corresponding marginal\nlikelihood of the data. We show how Noisin applies to any RNN and we study many\ndifferent types of noise. Noisin is unbiased--it preserves the underlying RNN\non average. We characterize how Noisin regularizes its RNN both theoretically\nand empirically. On language modeling benchmarks, Noisin improves over dropout\nby as much as 12.2% on the Penn Treebank and 9.4% on the Wikitext-2 dataset. We\nalso compared the state-of-the-art language model of Yang et al. 2017, both\nwith and without Noisin. On the Penn Treebank, the method with Noisin more\nquickly reaches state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 18:34:52 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 00:08:47 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Dieng", "Adji B.", ""], ["Ranganath", "Rajesh", ""], ["Altosaar", "Jaan", ""], ["Blei", "David M.", ""]]}, {"id": "1805.01575", "submitter": "Gregory Haber", "authors": "Gregory Haber and Yaakov Malinovsky", "title": "Efficient methods for the estimation of the multinomial parameter for\n  the two-trait group testing model", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of a single Bernoulli parameter using pooled sampling is among the\noldest problems in the group testing literature. To carry out such estimation,\nan array of efficient estimators have been introduced covering a wide range of\nsituations routinely encountered in applications. More recently, there has been\ngrowing interest in using group testing to simultaneously estimate the joint\nprobabilities of two correlated traits using a multinomial model.\nUnfortunately, basic estimation results, such as the maximum likelihood\nestimator (MLE), have not been adequately addressed in the literature for such\ncases. In this paper, we show that finding the MLE for this problem is\nequivalent to maximizing a multinomial likelihood with a restricted parameter\nspace. A solution using the EM algorithm is presented which is guaranteed to\nconverge to the global maximizer, even on the boundary of the parameter space.\nTwo additional closed form estimators are presented with the goal of minimizing\nthe bias and/or mean square error. The methods are illustrated by considering\nan application to the joint estimation of transmission prevalence for two\nstrains of the Potato virus Y by the aphid myzus persicae.\n", "versions": [{"version": "v1", "created": "Thu, 3 May 2018 23:38:21 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 11:23:25 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Haber", "Gregory", ""], ["Malinovsky", "Yaakov", ""]]}, {"id": "1805.01638", "submitter": "Kevin Jaunatre", "authors": "Ion Grama and Kevin Jaunatre", "title": "Estimation of Extreme Survival Probabilities with Cox Model", "comments": "31 pages, poster in SAfJR - Leiden 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an extension of the regular Cox's proportional hazards model which\nallows the estimation of the probabilities of rare events. It is known that\nwhen the data are heavily censored at the upper end of the survival\ndistribution, the estimation of the tail of the survival distribution is not\nreliable. To estimate the distribution beyond the last observed data, we\nsuppose that the survival data are in the domain of attraction of the Fr\\'echet\ndistribution conditionally to covariates. Under this condition, by the\nFisher-Tippett-Gnedenko theorem, the tail of the baseline distribution can be\nadjusted by a Pareto distribution with parameter $\\theta$ beyond a threshold\n$\\tau$. The survival distributions conditioned to the covariates are easily\ncomputed from the baseline. We also propose an aggregated estimate of the\nsurvival probabilities. A procedure allowing an automatic choice of the\nthreshold and an application on two data sets are given.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 07:51:49 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 14:30:51 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Grama", "Ion", ""], ["Jaunatre", "Kevin", ""]]}, {"id": "1805.01729", "submitter": "Ilja Klebanov", "authors": "Ilja Klebanov", "title": "Axiomatic Approach to Variable Kernel Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable kernel density estimation allows the approximation of a probability\ndensity by the mean of differently stretched and rotated kernels centered at\ngiven sampling points $y_n\\in\\mathbb{R}^d,\\ n=1,\\dots,N$. Up to now, the choice\nof the corresponding bandwidth matrices $h_n$ has relied mainly on asymptotic\narguments, like the minimization of the asymptotic mean integrated squared\nerror (AMISE), which work well for large numbers of sampling points. However,\nin practice, one is often confronted with small to moderately sized sample sets\nfar below the asymptotic regime, which highly restricts the usability of such\nmethods. As an alternative to this asymptotic reasoning we suggest an axiomatic\napproach which guarantees invariance of the density estimate under linear\ntransformations of the original density (and the sampling points) as well as\nunder splitting of the density into several `well-separated' parts. In order to\nstill ensure proper asymptotic behavior of the estimate, we \\emph{postulate}\nthe typical dependence $h_n\\propto N^{-1/(d+4)}$. Further, we derive a new\nbandwidths selection rule which satisfies these axioms and performs\nconsiderably better than conventional ones in an artificially intricate\ntwo-dimensional example as well as in a real life example.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 11:58:33 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Klebanov", "Ilja", ""]]}, {"id": "1805.01789", "submitter": "Xin Ma", "authors": "Xin Ma, Wenqing Wu, Bo Zeng, Yong Wang, Xinxing Wu", "title": "The conformable fractional grey system model", "comments": "28 pages, 8 figures", "journal-ref": null, "doi": "10.1016/j.isatra.2019.07.009", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fractional order grey models (FGM) have appealed considerable interest of\nresearch in recent years due to its higher effectiveness and flexibility than\nthe conventional grey models and other prediction models. However, the\ndefinitions of the fractional order accumulation (FOA) and difference (FOD) is\ncomputationally complex, which leads to difficulties for the theoretical\nanalysis and applications. In this paper, the new definition of the FOA are\nproposed based on the definitions of Conformable Fractional Derivative, which\nis called the Conformable Fractional Accumulation (CFA), along with its inverse\noperation, the Conformable Fractional Difference (CFD). Then the new\nConformable Fractional Grey Model (CFGM) based on CFA and CFD is introduced\nwith detailed modelling procedures. The feasibility and simplicity and the CFGM\nare shown in the numerical example. And the at last the comprehensive\nreal-world case studies of natural gas production forecasting in 11 countries\nare presented, and results show that the CFGM is much more effective than the\nexisting FGM model in the 165 subcases.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 13:59:59 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 00:29:20 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 15:24:49 GMT"}], "update_date": "2019-07-08", "authors_parsed": [["Ma", "Xin", ""], ["Wu", "Wenqing", ""], ["Zeng", "Bo", ""], ["Wang", "Yong", ""], ["Wu", "Xinxing", ""]]}, {"id": "1805.01852", "submitter": "David R\\\"ugamer", "authors": "David R\\\"ugamer, Sonja Greven", "title": "Inference for $L_2$-Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical inference framework for the component-wise\nfunctional gradient descent algorithm (CFGD) under normality assumption for\nmodel errors, also known as $L_2$-Boosting. The CFGD is one of the most\nversatile tools to analyze data, because it scales well to high-dimensional\ndata sets, allows for a very flexible definition of additive regression models\nand incorporates inbuilt variable selection. Due to the variable selection, we\nbuild on recent proposals for post-selection inference. However, the iterative\nnature of component-wise boosting, which can repeatedly select the same\ncomponent to update, necessitates adaptations and extensions to existing\napproaches. We propose tests and confidence intervals for linear, grouped and\npenalized additive model components selected by $L_2$-Boosting. Our concepts\nalso transfer to slow-learning algorithms more generally, and to other\nselection techniques which restrict the response space to more complex sets\nthan polyhedra. We apply our framework to an additive model for sales prices of\nresidential apartments and investigate the properties of our concepts in\nsimulation studies.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 16:51:38 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 10:00:44 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 14:44:10 GMT"}, {"version": "v4", "created": "Tue, 4 Jun 2019 20:17:26 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["R\u00fcgamer", "David", ""], ["Greven", "Sonja", ""]]}, {"id": "1805.01862", "submitter": "Patrick Laurie Davies Mr", "authors": "Laurie Davies", "title": "Lasso, knockoff and Gaussian covariates: a comparison", "comments": "The output of runcomp.R has been deleted. New versions of selvar.f\n  and selvar.R are provided which are faster than the old ones. The paper is\n  now 32 pages long", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given data $\\mathbf{y}$ and $k$ covariates $\\mathbf{x}_j$ one problem in\nlinear regression is to decide which if any of the covariates to include when\nregressing the dependent variable $\\mathbf{y}$ on the covariates\n$\\mathbf{x}_j$. In this paper three such methods, lasso, knockoff and Gaussian\ncovariates are compared using simulations and real data. The Gaussian covariate\nmethod is based on exact probabilities which are valid for all $\\mathbf{y}$ and\n$\\mathbf{x}_j$ making it model free. Moreover the probabilities agree with\nthose based on the F-distribution for the standard linear model with i.i.d.\nGaussian errors. It is conceptually, mathematically and algorithmically very\nsimple, it is very fast and makes no use of simulations. It outperforms lasso\nand knockoff in all respects by a considerable margin.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 17:12:22 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 15:25:27 GMT"}, {"version": "v3", "created": "Thu, 17 May 2018 12:54:44 GMT"}, {"version": "v4", "created": "Mon, 25 Mar 2019 13:49:45 GMT"}, {"version": "v5", "created": "Thu, 28 Mar 2019 15:39:49 GMT"}, {"version": "v6", "created": "Sat, 30 Mar 2019 16:55:34 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Davies", "Laurie", ""]]}, {"id": "1805.01864", "submitter": "Bochao Jia", "authors": "Bochao Jia", "title": "Mixture Envelope Model for Heterogeneous Genomics Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Envelope model also known as multivariate regression model was proposed to\nsolve the multiple response regression problems. It measures the linear\nassociation between predictors and multiple responses by using the minimal\nreducing subspace of the covariance matrix that accommodates the mean function.\nHowever, in many real applications, data may consist many unknown confounding\nfactors or they just come from different resources. Thus, there might be some\nheterogeneous dependency across the whole population and divide them into\ndifferent groups. For example, there exists several subtypes across the\npopulation with breast cancer with different gene interaction mechanisms for\neach subtype group. In this setting, constructing a single model using all\nobservations ignores the difference between groups while estimating multiple\nmodels for each group is infeasible due to the unknown group classification. To\ndeal with this problem, we proposed a mixture envelope model which construct a\ngroupwise model for heterogeneous data and simultaneously classify them into\ndifferent groups by an Imputation-Conditional Consistency (ICC) algorithm.\nSimulation results shows that our proposed method outperforms on both\nclassification and prediction than some existing methods. Finally, we apply our\nproposed method into breast cancer analysis to identify patients with\ninflammatory breast cancer subtype and evaluate the associations between\nmicro-RNAs and message RNAs gene expression.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 17:16:12 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Jia", "Bochao", ""]]}, {"id": "1805.01868", "submitter": "Jongbin Jung", "authors": "Jongbin Jung, Ravi Shroff, Avi Feller, Sharad Goel", "title": "Algorithmic Decision Making in the Presence of Unmeasured Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On a variety of complex decision-making tasks, from doctors prescribing\ntreatment to judges setting bail, machine learning algorithms have been shown\nto outperform expert human judgments. One complication, however, is that it is\noften difficult to anticipate the effects of algorithmic policies prior to\ndeployment, making the decision to adopt them risky. In particular, one\ngenerally cannot use historical data to directly observe what would have\nhappened had the actions recommended by the algorithm been taken. One standard\nstrategy is to model potential outcomes for alternative decisions assuming that\nthere are no unmeasured confounders (i.e., to assume ignorability). But if this\nignorability assumption is violated, the predicted and actual effects of an\nalgorithmic policy can diverge sharply. In this paper we present a flexible,\nBayesian approach to gauge the sensitivity of predicted policy outcomes to\nunmeasured confounders. We show that this policy evaluation problem is a\ngeneralization of estimating heterogeneous treatment effects in observational\nstudies, and so our methods can immediately be applied to that setting.\nFinally, we show, both theoretically and empirically, that under certain\nconditions it is possible to construct near-optimal algorithmic policies even\nwhen ignorability is violated. We demonstrate the efficacy of our methods on a\nlarge dataset of judicial actions, in which one must decide whether defendants\nawaiting trial should be required to pay bail or can be released without\npayment.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 17:29:14 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Jung", "Jongbin", ""], ["Shroff", "Ravi", ""], ["Feller", "Avi", ""], ["Goel", "Sharad", ""]]}, {"id": "1805.01886", "submitter": "Tra My Pham", "authors": "Tra My Pham, James R Carpenter, Tim P Morris, Angela M Wood, Irene\n  Petersen", "title": "Population-calibrated multiple imputation for a binary/categorical\n  covariate in categorical regression models", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8004", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiple imputation (MI) has become popular for analyses with missing data in\nmedical research. The standard implementation of MI is based on the assumption\nof data being missing at random (MAR). However, for missing data generated by\nmissing not at random (MNAR) mechanisms, MI performed assuming MAR might not be\nsatisfactory. For an incomplete variable in a given dataset, its corresponding\npopulation marginal distribution might also be available in an external data\nsource. We show how this information can be readily utilised in the imputation\nmodel to calibrate inference to the population, by incorporating an\nappropriately calculated offset termed the `calibrated-$\\delta$ adjustment'. We\ndescribe the derivation of this offset from the population distribution of the\nincomplete variable and show how in applications it can be used to closely (and\noften exactly) match the post-imputation distribution to the population level.\nThrough analytic and simulation studies, we show that our proposed\ncalibrated-$\\delta$ adjustment MI method can give the same inference as\nstandard MI when data are MAR, and can produce more accurate inference under\ntwo general MNAR missingness mechanisms. The method is used to impute missing\nethnicity data in a type 2 diabetes prevalence case study using UK primary care\nelectronic health records, where it results in scientifically relevant changes\nin inference for non-White ethnic groups compared to standard MI.\nCalibrated-$\\delta$ adjustment MI represents a pragmatic approach for utilising\navailable population-level information in a sensitivity analysis to explore\npotential departure from the MAR assumption.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 17:58:51 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Pham", "Tra My", ""], ["Carpenter", "James R", ""], ["Morris", "Tim P", ""], ["Wood", "Angela M", ""], ["Petersen", "Irene", ""]]}, {"id": "1805.01960", "submitter": "Joshua Brul\\'e", "authors": "Joshua Brul\\'e", "title": "Causal programming: inference with structural causal models as finding\n  instances of a relation", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a causal inference relation and causal programming as\ngeneral frameworks for causal inference with structural causal models. A tuple,\n$\\langle M, I, Q, F \\rangle$, is an instance of the relation if a formula, $F$,\ncomputes a causal query, $Q$, as a function of known population probabilities,\n$I$, in every model entailed by a set of model assumptions, $M$. Many problems\nin causal inference can be viewed as the problem of enumerating instances of\nthe relation that satisfy given criteria. This unifies a number of previously\nstudied problems, including causal effect identification, causal discovery and\nrecovery from selection bias. In addition, the relation supports formalizing\nnew problems in causal inference with structural causal models, such as the\nproblem of research design. Causal programming is proposed as a further\ngeneralization of causal inference as the problem of finding optimal instances\nof the relation, with respect to a cost function.\n", "versions": [{"version": "v1", "created": "Fri, 4 May 2018 23:14:36 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Brul\u00e9", "Joshua", ""]]}, {"id": "1805.02044", "submitter": "Monia Lupparelli", "authors": "Monia Lupparelli", "title": "Conditional and marginal relative risk parameters for a class of\n  recursive regression graph models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In linear regression modelling the distortion of effects after marginalizing\nover variables of the conditioning set has been widely studied in several\ncontexts. For Gaussian variables, the relationship between marginal and partial\nregression coefficients is well-established and the issue is often addressed as\na result of W. G. Cochran. Possible generalizations beyond the linear Gaussian\ncase have been developed, nevertheless the case of discrete variables is still\nchallenging, in particular in medical and social science settings. A\nmultivariate regression framework is proposed for binary data with regression\ncoefficients given by the logarithm of relative risks and a multivariate\nRelative Risk formula is derived to define the relationship between marginal\nand conditional relative risks. The method is illustrated through the analysis\nof the morphine data in order to assess the effect of preoperative oral\nmorphine administration on the postoperative pain relief.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 11:35:08 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Lupparelli", "Monia", ""]]}, {"id": "1805.02046", "submitter": "Yijun Zuo", "authors": "Yijun Zuo", "title": "On general notions of depth for regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth notions in location have fascinated tremendous attention in the\nliterature. In fact data depth and its applications remain one of the most\nactive research topics in statistics in the last two decades. Most favored\nnotions of depth in location include Tukey (1975) halfspace depth (HD), Liu\n(1990) simplicial depth, and projection depth (Stahel (1981) and Donoho (1982),\nLiu (1992), Zuo and Serfling (2000) (ZS00) and Zuo (2003)), among others.\n  Depth notions in regression have also been proposed, sporadically\nnevertheless. Regression depth (RD) of Rousseeuw and Hubert (1999) (RH99) is\nthe most famous one which is a direct extension of Tukey HD to regression.\nOthers include Carrizosa (1996) and the ones induced from Marrona and Yohai\n(1993) (MY93) proposed in this article. Is there any relationship between\nCarrizosa depth and RD of RH99? Do these depth notions possess desirable\nproperties? What are the desirable properties? Can existing notions really\nserve as depth functions in regression? These questions remain open.\n  Revealing the equivalence between Carrizosa depth and RD of RH99; expanding\nlocation depth evaluating criteria in ZS00 for regression depth notions;\nexamining the existing regression notions with respect to the gauges; and\nproposing the regression counterpart of the eminent projection depth in\nlocation are the four major objectives of the article.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 11:55:23 GMT"}, {"version": "v2", "created": "Sun, 24 Jun 2018 13:37:02 GMT"}, {"version": "v3", "created": "Wed, 26 Dec 2018 14:48:39 GMT"}, {"version": "v4", "created": "Tue, 25 Jun 2019 11:34:26 GMT"}, {"version": "v5", "created": "Mon, 6 Jan 2020 22:32:41 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Zuo", "Yijun", ""]]}, {"id": "1805.02075", "submitter": "Subhadeep Mukhopadhyay", "authors": "Subhadeep Mukhopadhyay", "title": "Decentralized Nonparametric Multiple Testing", "comments": "Revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a big data multiple testing task, where, due to storage and\ncomputational bottlenecks, one is given a very large collection of p-values by\nsplitting into manageable chunks and distributing over thousands of computer\nnodes. This paper is concerned with the following question: How can we find the\nfull data multiple testing solution by operating completely independently on\nindividual machines in parallel, without any data exchange between nodes? This\nversion of the problem tends naturally to arise in a wide range of\ndata-intensive science and industry applications whose methodological solution\nhas not appeared in the literature to date; therefore, we feel it is necessary\nto undertake such analysis. Based on the nonparametric functional statistical\nviewpoint of large-scale inference, started in Mukhopadhyay (2016), this paper\nfurnishes a new computing model that brings unexpected simplicity to the design\nof the algorithm which might otherwise seem daunting using classical approach\nand notations.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 15:57:04 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1805.02087", "submitter": "Eric Strobl", "authors": "Eric V. Strobl", "title": "A Constraint-Based Algorithm For Causal Discovery with Cycles, Latent\n  Variables and Selection Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal processes in nature may contain cycles, and real datasets may violate\ncausal sufficiency as well as contain selection bias. No constraint-based\ncausal discovery algorithm can currently handle cycles, latent variables and\nselection bias (CLS) simultaneously. I therefore introduce an algorithm called\nCyclic Causal Inference (CCI) that makes sound inferences with a conditional\nindependence oracle under CLS, provided that we can represent the cyclic causal\nprocess as a non-recursive linear structural equation model with independent\nerrors. Empirical results show that CCI outperforms CCD in the cyclic case as\nwell as rivals FCI and RFCI in the acyclic case.\n", "versions": [{"version": "v1", "created": "Sat, 5 May 2018 17:12:10 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Strobl", "Eric V.", ""]]}, {"id": "1805.02257", "submitter": "Lingrui Gan", "authors": "Lingrui Gan, Naveen N. Narisetty, Feng Liang", "title": "Bayesian Regularization for Graphical Models with Unequal Shrinkage", "comments": "To appear in Journal of the American Statistical Association (Theory\n  & Methods)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Bayesian framework for estimating a high-dimensional sparse\nprecision matrix, in which adaptive shrinkage and sparsity are induced by a\nmixture of Laplace priors. Besides discussing our formulation from the Bayesian\nstandpoint, we investigate the MAP (maximum a posteriori) estimator from a\npenalized likelihood perspective that gives rise to a new non-convex penalty\napproximating the $\\ell_0$ penalty. Optimal error rates for estimation\nconsistency in terms of various matrix norms along with selection consistency\nfor sparse structure recovery are shown for the unique MAP estimator under mild\nconditions. For fast and efficient computation, an EM algorithm is proposed to\ncompute the MAP estimator of the precision matrix and (approximate) posterior\nprobabilities on the edges of the underlying sparse structure. Through\nextensive simulation studies and a real application to a call center data, we\nhave demonstrated the fine performance of our method compared with existing\nalternatives.\n", "versions": [{"version": "v1", "created": "Sun, 6 May 2018 18:16:21 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 18:31:59 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Gan", "Lingrui", ""], ["Narisetty", "Naveen N.", ""], ["Liang", "Feng", ""]]}, {"id": "1805.02306", "submitter": "Ruoqing Zhu", "authors": "Jack Yutong Li, Ruoqing Zhu, Annie Qu, Han Ye, Zhankun Sun", "title": "Semi-orthogonal Non-negative Matrix Factorization with an Application in\n  Text Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergency Department (ED) crowding is a worldwide issue that affects the\nefficiency of hospital management and the quality of patient care. This occurs\nwhen the request for an admit ward-bed to receive a patient is delayed until an\nadmission decision is made by a doctor. To reduce the overcrowding and waiting\ntime of ED, we build a classifier to predict the disposition of patients using\nmanually-typed nurse notes collected during triage, thereby allowing hospital\nstaff to begin necessary preparation beforehand. However, these triage notes\ninvolve high dimensional, noisy, and also sparse text data which makes model\nfitting and interpretation difficult. To address this issue, we propose the\nsemi-orthogonal non-negative matrix factorization (SONMF) for both continuous\nand binary design matrices to first bi-cluster the patients and words into a\nreduced number of topics. The subjects can then be interpreted as a\nnon-subtractive linear combination of orthogonal basis topic vectors. These\ngenerated topic vectors provide the hospital with a direct understanding of the\ncause of admission. We show that by using a transformation of basis, the\nclassification accuracy can be further increased compared to the conventional\nbag-of-words model and alternative matrix factorization approaches. Through\nsimulated data experiments, we also demonstrate that the proposed method\noutperforms other non-negative matrix factorization (NMF) methods in terms of\nfactorization accuracy, rate of convergence, and degree of orthogonality.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 01:12:12 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 19:12:37 GMT"}, {"version": "v3", "created": "Thu, 4 Jul 2019 15:46:21 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Li", "Jack Yutong", ""], ["Zhu", "Ruoqing", ""], ["Qu", "Annie", ""], ["Ye", "Han", ""], ["Sun", "Zhankun", ""]]}, {"id": "1805.02407", "submitter": "Adam Lund", "authors": "Adam Lund, S{\\o}ren Wengel Mogensen, Niels Richard Hansen", "title": "Soft Maximin Estimation for Heterogeneous Array Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of a common signal across many recordings is difficult when\neach recording -- in addition to the signal -- contains large, unique variation\ncomponents. Maximin estimation has previously been proposed as a robust\nestimation method in the presence of heterogeneous noise.\n  We propose soft maximin estimation as a computationally attractive\nmethodology for estimating a common signal from heterogeneous data. The soft\nmaximin loss is introduced as an aggregation, controlled by a parameter\n$\\zeta>0$, of explained variances and the estimator is obtained by minimizing\nthe penalized soft maximin loss.\n  By establishing statistical and computational properties we argue that the\nsoft maximin method is a statistically sensibel and computationally attractive\nalternative to existing methods. In particular we demonstrate, on simulated and\nreal data, that the soft maximin estimator can outperform existing methods both\nin terms of predictive performance and run time. We also provide a time and\nmemory efficient implementation for data with array-tensor structure in the R\npackage SMMA available on CRAN.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 09:02:40 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 10:33:57 GMT"}, {"version": "v3", "created": "Tue, 22 Sep 2020 18:46:47 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Lund", "Adam", ""], ["Mogensen", "S\u00f8ren Wengel", ""], ["Hansen", "Niels Richard", ""]]}, {"id": "1805.02547", "submitter": "Bochao Jia", "authors": "Bochao Jia, Faming Liang", "title": "Learning Gene Regulatory Networks with High-Dimensional Heterogeneous\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian graphical model is a widely used tool for learning gene\nregulatory networks with high-dimensional gene expression data. Most existing\nmethods for Gaussian graphical models assume that the data are homogeneous,\ni.e., all samples are drawn from a single Gaussian distribution. However, for\nmany real problems, the data are heterogeneous, which may contain some\nsubgroups or come from different resources. This paper proposes to model the\nheterogeneous data using a mixture Gaussian graphical model, and apply the\nimputation-consistency algorithm, combining with the $\\psi$-learning algorithm,\nto estimate the parameters of the mixture model and cluster the samples to\ndifferent subgroups. An integrated Gaussian graphical network is learned across\nthe subgroups along with the iterations of the imputation-consistency\nalgorithm. The proposed method is compared with an existing method for learning\nmixture Gaussian graphical models as well as a few other methods developed for\nhomogeneous data, such as graphical Lasso, nodewise regression and\n$\\psi$-learning. The numerical results indicate superiority of the proposed\nmethod in all aspects of parameter estimation, cluster identification and\nnetwork construction. The numerical results also indicate generality of the\nproposed method: it can be applied to homogeneous data without significant\nharms.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 14:38:30 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Jia", "Bochao", ""], ["Liang", "Faming", ""]]}, {"id": "1805.02620", "submitter": "Bochao Jia", "authors": "Bochao Jia, Faming Liang and the TEDDY Study Group", "title": "Fast Bayesian Integrative Learning of Multiple Gene Regulatory Networks\n  for Type 1 Diabetes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need to study the molecular mechanism underlying Type 1\nDiabetes (T1D) with the gene expression data collected from both the patients\nand healthy controls at multiple time points, we propose an innovative method\nfor jointly estimating multiple dependent Gaussian graphical models. Compared\nto the existing methods, the proposed method has a few significant advantages.\nFirst, it includes a meta-analysis procedure to explicitly integrate\ninformation across distinct conditions. In contrast, the existing methods often\nintegrate information through prior distributions or penalty function, which is\nusually less efficient. Second, instead of working on original data, the\nBayesian step of the proposed method works on edge-wise scores, through which\nthe proposed method avoids to invert high-dimensional covariance matrices and\nthus can perform very fast. The edge-wise score forms an equivalent measure of\nthe partial correlation coefficient and thus provides a good summary for the\ngraph structure information contained in the data under each condition. Third,\nthe proposed method can provide an overall uncertainty measure for the edges\ndetected in multiple graphical models, while the existing methods only produce\na point estimate or are feasible for very small size problems. We prove\nconsistency of the proposed method under mild conditions and illustrate its\nperformance using simulated and real data examples. The numerical results\nindicate the superiority of the proposed method over the existing ones in both\nestimation accuracy and computational efficiency. Extension of the proposed\nmethod to joint estimation of multiple mixed graphical models is\nstraightforward.\n", "versions": [{"version": "v1", "created": "Mon, 7 May 2018 17:04:41 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 14:55:32 GMT"}, {"version": "v3", "created": "Fri, 7 Dec 2018 05:15:58 GMT"}], "update_date": "2018-12-10", "authors_parsed": [["Jia", "Bochao", ""], ["Liang", "Faming", ""], ["Group", "the TEDDY Study", ""]]}, {"id": "1805.02826", "submitter": "Yiqiao Zhong", "authors": "Jianqing Fan and Yiqiao Zhong", "title": "Optimal Subspace Estimation Using Overidentifying Vectors via\n  Generalized Method of Moments", "comments": "48 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical models seek relationship between variables via subspaces of\nreduced dimensions. For instance, in factor models, variables are roughly\ndistributed around a low dimensional subspace determined by the loading matrix;\nin mixed linear regression models, the coefficient vectors for different\nmixtures form a subspace that captures all regression functions; in multiple\nindex models, the effect of covariates is summarized by the effective dimension\nreduction space.\n  Such subspaces are typically unknown, and good estimates are crucial for data\nvisualization, dimension reduction, diagnostics and estimation of unknown\nparameters. Usually, we can estimate these subspaces by computing moments from\ndata. Often, there are many ways to estimate a subspace, by using moments of\ndifferent orders, transformed moments, etc. A natural question is: how can we\ncombine all these moment conditions and achieve optimality for subspace\nestimation?\n  In this paper, we formulate our problem as estimation of an unknown subspace\n$\\mathcal{S}$ of dimension $r$, given a set of overidentifying vectors $\\{\n\\mathrm{\\bf v}_\\ell \\}_{\\ell=1}^m$ (namely $m \\ge r$) that satisfy $\\mathbb{E}\n\\mathrm{\\bf v}_{\\ell} \\in \\mathcal{S}$ and have the form $$ \\mathrm{\\bf v}_\\ell\n= \\frac{1}{n} \\sum_{i=1}^n \\mathrm{\\bf f}_\\ell(\\mathbf{x}_i, y_i), $$ where\ndata are i.i.d. and each function $\\mathrm{\\bf f}_\\ell$ is known. By exploiting\ncertain covariance information related to $\\mathrm{\\bf v}_\\ell$, our estimator\nof $\\mathcal{S}$ uses an optimal weighting matrix and achieves the smallest\nasymptotic error, in terms of canonical angles. The analysis is based on the\ngeneralized method of moments that is tailored to our problem. Our method is\napplied to aforementioned models and distributed estimation of heterogeneous\ndatasets, and may be potentially extended to analyze matrix completion, neural\nnets, among others.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 04:14:25 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Fan", "Jianqing", ""], ["Zhong", "Yiqiao", ""]]}, {"id": "1805.02898", "submitter": "Ning Zhang", "authors": "N.Zhang", "title": "Simulation Study on Local Influence Diagnosis for Poisson Mixed-Effect\n  Linear Model", "comments": "Acknowledgement: Financial support from \"2017 Shanghai University\n  Students Innovation and Entrepreneurship Training Program Model School\",\n  Shanghai Education Commission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given that hierarchical count data in many fields are not\nNormally-distributed and include random effects, this paper extends the\nGeneralized Linear Mixed Models (GLMMs) into Poisson Mixed-Effect Linear Model\n(PMELM) and do numerical simulation experiments to verify the approach proposed\nby Rakhmawati et al. (2016) in detecting outliers. This paper produces random\ndata based on epilepsy longitudinal data in Thall and Vail (1990), use six ways\nto contaminate it and try to use code mentioned in supplementary materials in\nprevious research to detect the man-made outlier. Output shows that this method\nis effective sometimes but does not always work, this is probably because of\nthe limitation of coding or some other reasons. Even though the data set and\nlocal influence method has been researched and analyzed extensively in previous\npapers, this paper makes contributions in data visualization. Figures in this\npaper show the effect of each influencial component, which are clearer than the\noriginal output in R and SAS.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 08:39:35 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Zhang", "N.", ""]]}, {"id": "1805.02993", "submitter": "Jana Falt\\'ynkov\\'a", "authors": "Jana Svobodov\\'a", "title": "Bayesian models in geographic profiling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of geographic profiling and offer an approach to\nchoosing a suitable model for each offender. Based on the analysis of the\nexamined dataset, we divide offenders into several types with similar behavior.\nAccording to the spatial distribution of the offender's crime sites, each new\ncriminal is assigned to the corresponding group. Then we choose an appropriate\nmodel for the offender and using Bayesian methods we determine the posterior\ndistribution for the criminal's anchor point. Our models include\ndirectionality, similar to models of Mohler and Short (2012). Our approach also\nprovides a way to incorporate two possible situations into the model - when the\ncriminal is a resident or a non-resident. We test this methodology on a real\ndata set of offenders from Baltimore County and compare the results with\nRossmo's approach. Our approach leads to substantial improvement over Rossmo's\nmethod, especially in the presence of non-residents.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 13:12:26 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Svobodov\u00e1", "Jana", ""]]}, {"id": "1805.03240", "submitter": "Arkaprava Roy", "authors": "Arkaprava Roy, Brian J. Reich, Joseph Guinness, Russell T. Shinohara,\n  and Ana-Maria Staicu", "title": "Spatial shrinkage via the product independent Gaussian process prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of sparse signal detection on a spatial domain. We\npropose a novel approach to model continuous signals that are sparse and\npiecewise smooth as product of independent Gaussian processes (PING) with a\nsmooth covariance kernel. The smoothness of the PING process is ensured by the\nsmoothness of the covariance kernels of Gaussian components in the product, and\nsparsity is controlled by the number of components. The bivariate kurtosis of\nthe PING process shows more components in the product results in thicker tail\nand sharper peak at zero. The simulation results demonstrate the improvement in\nestimation using the PING prior over Gaussian process (GP) prior for different\nimage regressions. We apply our method to a longitudinal MRI dataset to detect\nthe regions that are affected by multiple sclerosis (MS) in the greatest\nmagnitude through an image-on-scalar regression model. Due to huge\ndimensionality of these images, we transform the data into the spectral domain\nand develop methods to conduct computation in this domain. In our MS imaging\nstudy, the estimates from the PING model are more informative than those from\nthe GP model.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 19:23:21 GMT"}, {"version": "v2", "created": "Sun, 2 Sep 2018 16:42:44 GMT"}, {"version": "v3", "created": "Sat, 26 Jan 2019 20:31:38 GMT"}, {"version": "v4", "created": "Sat, 6 Jun 2020 00:33:47 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Roy", "Arkaprava", ""], ["Reich", "Brian J.", ""], ["Guinness", "Joseph", ""], ["Shinohara", "Russell T.", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1805.03273", "submitter": "Alyssa Bilinski", "authors": "Alyssa Bilinski and Laura A. Hatfield", "title": "Nothing to see here? Non-inferiority approaches to parallel trends and\n  other model assumptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many causal models make assumptions of \"no difference\" or \"no effect.\" For\nexample, difference-in-differences (DID) assumes that there is no trend\ndifference between treatment and comparison groups' untreated potential\noutcomes (\"parallel trends\"). Tests of these assumptions typically assume a\nnull hypothesis that there is no violation. When researchers fail to reject the\nnull, they consider the assumption to hold. We argue this approach is incorrect\nand frequently misleading. These tests reverse the roles of Type I and Type II\nerror and have a high probability of missing assumption violations. Even when\npower is high, they may detect statistically significant violations too small\nto be of practical importance. We present test reformulations in a\nnon-inferiority framework that rule out violations of model assumptions that\nexceed some threshold. We then focus on the parallel trends assumption, for\nwhich we propose a \"one step up\" method: 1) reporting treatment effect\nestimates from a model with a more complex trend difference than is believed to\nbe the case and 2) testing that that the estimated treatment effect falls\nwithin a specified distance of the treatment effect from the simpler model. We\nshow that this reduces bias while also considering power, controlling\nmean-squared error. Our base model also aligns power to detect a treatment\neffect with power to rule out meaningful violations of parallel trends. We\napply our approach to 4 data sets used to analyze the Affordable Care Act's\ndependent coverage mandate and demonstrate that coverage gains may have been\nsmaller than previously estimated.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 20:40:51 GMT"}, {"version": "v2", "created": "Tue, 21 Aug 2018 20:22:55 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2019 04:58:23 GMT"}, {"version": "v4", "created": "Tue, 24 Dec 2019 09:45:11 GMT"}, {"version": "v5", "created": "Fri, 17 Jan 2020 00:31:41 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Bilinski", "Alyssa", ""], ["Hatfield", "Laura A.", ""]]}, {"id": "1805.03288", "submitter": "Robert Bassett", "authors": "Robert Bassett and James Sharpnack", "title": "Fused Density Estimation: Theory and Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a method for nonparametric density estimation on\ngeometric networks. We define fused density estimators as solutions to a total\nvariation regularized maximum-likelihood density estimation problem. We provide\ntheoretical support for fused density estimation by proving that the squared\nHellinger rate of convergence for the estimator achieves the minimax bound over\nunivariate densities of log-bounded variation. We reduce the original\nvariational formulation in order to transform it into a tractable,\nfinite-dimensional quadratic program. Because random variables on geometric\nnetworks are simple generalizations of the univariate case, this method also\nprovides a useful tool for univariate density estimation. Lastly, we apply this\nmethod and assess its performance on examples in the univariate and geometric\nnetwork setting. We compare the performance of different optimization\ntechniques to solve the problem, and use these results to inform\nrecommendations for the computation of fused density estimators.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 21:07:29 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 19:31:22 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Bassett", "Robert", ""], ["Sharpnack", "James", ""]]}, {"id": "1805.03309", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss, Joseph Guinness, Wenlong Gong, Daniel Zilber", "title": "Vecchia approximations of Gaussian-process predictions", "comments": null, "journal-ref": "Journal of Agricultural, Biological, and Environmental Statistics,\n  25(3), 383-414 (2020)", "doi": "10.1007/s13253-020-00401-7", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) are highly flexible function estimators used for\ngeospatial analysis, nonparametric regression, and machine learning, but they\nare computationally infeasible for large datasets. Vecchia approximations of\nGPs have been used to enable fast evaluation of the likelihood for parameter\ninference. Here, we study Vecchia approximations of spatial predictions at\nobserved and unobserved locations, including obtaining joint predictive\ndistributions at large sets of locations. We consider a general Vecchia\nframework for GP predictions, which contains some novel and some existing\nspecial cases. We study the accuracy and computational properties of these\napproaches theoretically and numerically, proving that our new methods exhibit\nlinear computational complexity in the total number of spatial locations. We\nshow that certain choices within the framework can have a strong effect on\nuncertainty quantification and computational cost, which leads to specific\nrecommendations on which methods are most suitable for various settings. We\nalso apply our methods to a satellite dataset of chlorophyll fluorescence,\nshowing that the new methods are faster or more accurate than existing methods,\nand reduce unrealistic artifacts in prediction maps.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 22:25:40 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 13:36:40 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 15:55:32 GMT"}, {"version": "v4", "created": "Thu, 14 May 2020 22:26:17 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Guinness", "Joseph", ""], ["Gong", "Wenlong", ""], ["Zilber", "Daniel", ""]]}, {"id": "1805.03316", "submitter": "Boris Beranger", "authors": "Boris Beranger, Simone A. Padoan, Yangfan Xu and Scott A. Sisson", "title": "Extremal properties of the univariate extended skew-normal distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the extremal properties of the highly flexible univariate\nextended skew-normal distribution. We derive the well-known Mills' inequalities\nand Mills' ratio for the extended skew-normal distribution and establish the\nasymptotic extreme-value distribution for the maximum of samples drawn from\nthis distribution.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 23:08:46 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 03:10:32 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Beranger", "Boris", ""], ["Padoan", "Simone A.", ""], ["Xu", "Yangfan", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1805.03317", "submitter": "Matias Quiroz", "authors": "David Gunawan, Khue-Dung Dang, Matias Quiroz, Robert Kohn, Minh-Ngoc\n  Tran", "title": "Subsampling Sequential Monte Carlo for Static Bayesian Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to speed up Sequential Monte Carlo (SMC) for Bayesian inference\nin large data problems by data subsampling. SMC sequentially updates a cloud of\nparticles through a sequence of distributions, beginning with a distribution\nthat is easy to sample from such as the prior and ending with the posterior\ndistribution. Each update of the particle cloud consists of three steps:\nreweighting, resampling, and moving. In the move step, each particle is moved\nusing a Markov kernel; this is typically the most computationally expensive\npart, particularly when the dataset is large. It is crucial to have an\nefficient move step to ensure particle diversity. Our article makes two\nimportant contributions. First, in order to speed up the SMC computation, we\nuse an approximately unbiased and efficient annealed likelihood estimator based\non data subsampling. The subsampling approach is more memory efficient than the\ncorresponding full data SMC, which is an advantage for parallel computation.\nSecond, we use a Metropolis within Gibbs kernel with two conditional updates. A\nHamiltonian Monte Carlo update makes distant moves for the model parameters,\nand a block pseudo-marginal proposal is used for the particles corresponding to\nthe auxiliary variables for the data subsampling. We demonstrate both the\nusefulness and limitations of the methodology for estimating four generalized\nlinear models and a generalized additive model with large datasets.\n", "versions": [{"version": "v1", "created": "Tue, 8 May 2018 23:17:01 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 08:12:12 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 10:36:25 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Gunawan", "David", ""], ["Dang", "Khue-Dung", ""], ["Quiroz", "Matias", ""], ["Kohn", "Robert", ""], ["Tran", "Minh-Ngoc", ""]]}, {"id": "1805.03336", "submitter": "Zifeng Zhao", "authors": "Zifeng Zhao, Peng Shi, Zhengjun Zhang", "title": "Modeling Multivariate Time Series with Copula-linked Univariate D-vines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel multivariate time series model named\nCopula-linked univariate D-vines (CuDvine), which enables the simultaneous\ncopula-based modeling of both temporal and cross-sectional dependence for\nmultivariate time series. To construct CuDvine, we first build a semiparametric\nunivariate D-vine time series model (uDvine) based on a D-vine. The uDvine\ngeneralizes the existing first-order copula-based Markov chain models to Markov\nchains of an arbitrary-order. Building upon uDvine, we construct CuDvine by\nlinking multiple uDvines via a parametric copula. As a simple and tractable\nmodel, CuDvine provides flexible models for marginal behavior and temporal\ndependence of time series, and can also incorporate sophisticated\ncross-sectional dependence such as time-varying and spatio-temporal dependence\nfor high-dimensional applications. Robust and computationally efficient\nprocedures, including a sequential model selection method and a two-stage MLE,\nare proposed for model estimation and inference, and their statistical\nproperties are investigated. Numerical experiments are conducted to demonstrate\nthe flexibility of CuDvine, and to examine the performance of the sequential\nmodel selection procedure and the two-stage MLE. Real data applications on the\nAustralian electricity price data demonstrate the superior performance of\nCuDvine to traditional multivariate time series models.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 01:01:52 GMT"}, {"version": "v2", "created": "Mon, 1 Oct 2018 20:30:50 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2020 02:05:27 GMT"}], "update_date": "2020-12-02", "authors_parsed": [["Zhao", "Zifeng", ""], ["Shi", "Peng", ""], ["Zhang", "Zhengjun", ""]]}, {"id": "1805.03353", "submitter": "Bingying Xie", "authors": "Bingying Xie and Jun Shao", "title": "Nonparametric Estimation of Conditional Expectation with Auxiliary\n  Information and Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric estimation of the conditional expectation $E(Y | U)$ of an\noutcome $Y$ given a covariate vector $U$ is of primary importance in many\nstatistical applications such as prediction and personalized medicine. In some\nproblems, there is an additional auxiliary variable $Z$ in the training dataset\nused to construct estimators, but $Z$ is not available for future prediction or\nselecting patient treatment in personalized medicine. For example, in the\ntraining dataset longitudinal outcomes are observed, but only the last outcome\n$Y$ is concerned in the future prediction or analysis. The longitudinal\noutcomes other than the last point is then the variable $Z$ that is observed\nand related with both $Y$ and $U$. Previous work on how to make use of $Z$ in\nthe estimation of $E(Y|U)$ mainly focused on using $Z$ in the construction of a\nlinear function of $U$ to reduce covariate dimension for better estimation.\nUsing $E(Y|U) = E\\{E(Y|U, Z)| U\\}$, we propose a two-step estimation of inner\nand outer expectations, respectively, with sufficient dimension reduction for\nkernel estimation in both steps. The information from $Z$ is utilized not only\nin dimension reduction, but also directly in the estimation. Because of the\nexistence of different ways for dimension reduction, we construct two\nestimators that may improve the estimator without using $Z$. The improvements\nare shown in the convergence rate of estimators as the sample size increases to\ninfinity as well as in the finite sample simulation performance. A real data\nanalysis about the selection of mammography intervention is presented for\nillustration.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 02:35:52 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Xie", "Bingying", ""], ["Shao", "Jun", ""]]}, {"id": "1805.03373", "submitter": "Ruoxuan Xiong", "authors": "Markus Pelger, Ruoxuan Xiong", "title": "Interpretable Sparse Proximate Factors for Large Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes sparse and easy-to-interpret proximate factors to\napproximate statistical latent factors. Latent factors in a large-dimensional\nfactor model can be estimated by principal component analysis (PCA), but are\nusually hard to interpret. We obtain proximate factors that are easier to\ninterpret by shrinking the PCA factor weights and setting them to zero except\nfor the largest absolute ones. We show that proximate factors constructed with\nonly 5-10% of the data are usually sufficient to almost perfectly replicate the\npopulation and PCA factors without actually assuming a sparse structure in the\nweights or loadings. Using extreme value theory we explain why sparse proximate\nfactors can be substitutes for non-sparse PCA factors. We derive analytical\nasymptotic bounds for the correlation of appropriately rotated proximate\nfactors with the population factors. These bounds provide guidance on how to\nconstruct the proximate factors. In simulations and empirical analyses of\nfinancial portfolio and macroeconomic data we illustrate that sparse proximate\nfactors are close substitutes for PCA factors with average correlations of\naround 97.5% while being interpretable.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 05:11:44 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 02:20:12 GMT"}, {"version": "v3", "created": "Tue, 2 Apr 2019 03:42:01 GMT"}, {"version": "v4", "created": "Sat, 1 Aug 2020 04:54:16 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Pelger", "Markus", ""], ["Xiong", "Ruoxuan", ""]]}, {"id": "1805.03433", "submitter": "Zaid Sawlan", "authors": "Ivo Babuska, Zaid Sawlan, Marco Scavino, Barna Szab\\'o, Ra\\'ul Tempone", "title": "Spatial Poisson processes for fatigue crack initiation", "comments": null, "journal-ref": null, "doi": "10.1016/j.cma.2018.11.007", "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a stochastic model for estimating the occurrence of\ncrack initiations on the surface of metallic specimens in fatigue problems that\ncan be applied to a general class of geometries. The stochastic model is based\non spatial Poisson processes with intensity function that combines stress-life\n(S-N) curves with averaged effective stress, $\\sigma_{{\\mathrm{eff}}}^{\\Delta}\n(\\mathbf{x})$, which is computed after solving numerically the linear\nelasticity equations on the specimen domains using finite element methods.\nHere, $\\Delta$ is a parameter that characterizes the size of the neighbors\ncovering the domain boundary. The averaged effective stress, parameterized by\n$\\Delta$, maps the stress tensor to a scalar field upon the specimen domain.\nData from fatigue experiments on notched and unnotched sheet specimens of\n75S-T6 aluminum alloys are used to calibrate the model parameters for the\nindividual data sets and for their combination. Bayesian and classical\napproaches are applied to estimate the survival-probability function for any\nspecimen tested under a prescribed fatigue experimental setup. Our proposed\nmodel can predict the initiation of cracks in specimens made from the same\nmaterial with new geometries.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 09:29:00 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 16:37:16 GMT"}], "update_date": "2018-12-26", "authors_parsed": [["Babuska", "Ivo", ""], ["Sawlan", "Zaid", ""], ["Scavino", "Marco", ""], ["Szab\u00f3", "Barna", ""], ["Tempone", "Ra\u00fal", ""]]}, {"id": "1805.03567", "submitter": "Louis Ellam", "authors": "L. Ellam, M. Girolami, G. A. Pavliotis and A. Wilson", "title": "Stochastic Modelling of Urban Structure", "comments": "http://dx.doi.org/10.1098/rspa.2017.0700", "journal-ref": "Ellam L, Girolami M, Pavliotis GA,Wilson A. 2018 Stochastic\n  modelling of urban structure. Proc. R. Soc. A 20170700", "doi": "10.1098/rspa.2017.0700", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The building of mathematical and computer models of cities has a long\nhistory. The core elements are models of flows (spatial interaction) and the\ndynamics of structural evolution. In this article, we develop a stochastic\nmodel of urban structure to formally account for uncertainty arising from less\npredictable events. Standard practice has been to calibrate the spatial\ninteraction models independently and to explore the dynamics through\nsimulation. We present two significant results that will be transformative for\nboth elements. First, we represent the structural variables through a single\npotential function and develop stochastic differential equations (SDEs) to\nmodel the evolution. Secondly, we show that the parameters of the spatial\ninteraction model can be estimated from the structure alone, independently of\nflow data, using the Bayesian inferential framework. The posterior distribution\nis doubly intractable and poses significant computational challenges that we\novercome using Markov chain Monte Carlo (MCMC) methods. We demonstrate our\nmethodology with a case study on the London retail system.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 14:55:33 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Ellam", "L.", ""], ["Girolami", "M.", ""], ["Pavliotis", "G. A.", ""], ["Wilson", "A.", ""]]}, {"id": "1805.03719", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul, Venkata K. Jandhyala and Stergios B. Fotopoulos", "title": "An efficient two step algorithm for high dimensional change point\n  regression models without grid search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two step algorithm based on $\\ell_1/\\ell_0$ regularization for\nthe detection and estimation of parameters of a high dimensional change point\nregression model and provide the corresponding rates of convergence for the\nchange point as well as the regression parameter estimates. Importantly, the\ncomputational cost of our estimator is only $2\\cdotp$Lasso$(n,p)$, where\nLasso$(n,p)$ represents the computational burden of one Lasso optimization in a\nmodel of size $(n,p)$. In comparison, existing grid search based approaches to\nthis problem require a computational cost of at least $n\\cdot {\\rm Lasso}(n,p)$\noptimizations. Additionally, the proposed method is shown to be able to\nconsistently detect the case of `no change', i.e., where no finite change point\nexists in the model. We work under a subgaussian random design where the\nunderlying assumptions in our study are milder than those currently assumed in\nthe high dimensional change point regression literature. We allow the true\nchange point parameter $\\tau_0$ to possibly move to the boundaries of its\nparametric space, and the jump size $\\|\\beta_0-\\gamma_0\\|_2$ to possibly\ndiverge as $n$ increases. We then characterize the corresponding effects on the\nrates of convergence of the change point and regression estimates. In\nparticular, we show that, while an increasing jump size may have a beneficial\neffect on the change point estimate, however the optimal rate of regression\nparameter estimates are preserved only upto a certain rate of the increasing\njump size. This behavior in the rate of regression parameter estimates is\nunique to high dimensional change point regression models only. Simulations are\nperformed to empirically evaluate performance of the proposed estimators. The\nmethodology is applied to community level socio-economic data of the U.S.,\ncollected from the 1990 U.S. census and other sources.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 20:19:25 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 18:08:53 GMT"}, {"version": "v3", "created": "Wed, 20 Jun 2018 02:48:03 GMT"}, {"version": "v4", "created": "Sat, 23 Jun 2018 02:14:00 GMT"}, {"version": "v5", "created": "Thu, 17 Jan 2019 18:40:51 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Kaul", "Abhishek", ""], ["Jandhyala", "Venkata K.", ""], ["Fotopoulos", "Stergios B.", ""]]}, {"id": "1805.03744", "submitter": "Luke Keele", "authors": "Hyunseung Kang and Luke Keele", "title": "Estimation Methods for Cluster Randomized Trials with Noncompliance: A\n  Study of A Biometric Smartcard Payment System in India", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many policy evaluations occur in settings where treatment is randomized at\nthe cluster level, and there is treatment noncompliance within each cluster.\nFor example, villages might be assigned to treatment and control, but residents\nin each village may choose to comply or not with their assigned treatment\nstatus. When noncompliance is present, the instrumental variables framework can\nbe used to identify and estimate causal effects. While a large literature\nexists on instrumental variables estimation methods, relatively little work has\nbeen focused on settings with clustered treatments. Here, we review extant\nmethods for instrumental variable estimation in clustered designs and derive\nboth the finite and asymptotic properties of these estimators. We prove that\nthe properties of current estimators depend on unrealistic assumptions. We then\ndevelop a new IV estimation method for cluster randomized trials and study its\nformal properties. We prove that our IV estimator allows for possible treatment\neffect heterogeneity that is correlated with cluster size and is robust to low\ncompliance rates within clusters. We evaluate these methods using simulations\nand apply them to data from a randomized intervention in India.\n", "versions": [{"version": "v1", "created": "Wed, 9 May 2018 22:13:33 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2019 15:11:48 GMT"}, {"version": "v3", "created": "Thu, 15 Aug 2019 00:12:02 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Kang", "Hyunseung", ""], ["Keele", "Luke", ""]]}, {"id": "1805.03807", "submitter": "Alessandro Casini", "authors": "Alessandro Casini and Pierre Perron", "title": "Structural Breaks in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter covers methodological issues related to estimation, testing and\ncomputation for models involving structural changes. Our aim is to review\ndevelopments as they relate to econometric applications based on linear models.\nSubstantial advances have been made to cover models at a level of generality\nthat allow a host of interesting practical applications. These include models\nwith general stationary regressors and errors that can exhibit temporal\ndependence and heteroskedasticity, models with trending variables and possible\nunit roots and cointegrated models, among others. Advances have been made\npertaining to computational aspects of constructing estimates, their limit\ndistributions, tests for structural changes, and methods to determine the\nnumber of changes present. A variety of topics are covered. The first part\nsummarizes and updates developments described in an earlier review, Perron\n(2006), with the exposition following heavily that of Perron (2008). Additions\nare included for recent developments: testing for common breaks, models with\nendogenous regressors (emphasizing that simply using least-squares is\npreferable over instrumental variables methods), quantile regressions, methods\nbased on Lasso, panel data models, testing for changes in forecast accuracy,\nfactors models and methods of inference based on a continuous records\nasymptotic framework. Our focus is on the so-called off-line methods whereby\none wants to retrospectively test for breaks in a given sample of data and form\nconfidence intervals about the break dates. The aim is to provide the readers\nwith an overview of methods that are of direct usefulness in practice as\nopposed to issues that are mostly of theoretical interest.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 04:18:10 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Casini", "Alessandro", ""], ["Perron", "Pierre", ""]]}, {"id": "1805.04010", "submitter": "Mika Meitz", "authors": "Mika Meitz, Daniel Preve, Pentti Saikkonen", "title": "A mixture autoregressive model based on Student's $t$-distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new mixture autoregressive model based on Student's $t$-distribution is\nproposed. A key feature of our model is that the conditional $t$-distributions\nof the component models are based on autoregressions that have multivariate\n$t$-distributions as their (low-dimensional) stationary distributions. That\nautoregressions with such stationary distributions exist is not immediate. Our\nformulation implies that the conditional mean of each component model is a\nlinear function of past observations and the conditional variance is also time\nvarying. Compared to previous mixture autoregressive models our model may\ntherefore be useful in applications where the data exhibits rather strong\nconditional heteroskedasticity. Our formulation also has the theoretical\nadvantage that conditions for stationarity and ergodicity are always met and\nthese properties are much more straightforward to establish than is common in\nnonlinear autoregressive models. An empirical example employing a realized\nkernel series based on S&P 500 high-frequency data shows that the proposed\nmodel performs well in volatility forecasting.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 14:58:58 GMT"}], "update_date": "2018-05-11", "authors_parsed": [["Meitz", "Mika", ""], ["Preve", "Daniel", ""], ["Saikkonen", "Pentti", ""]]}, {"id": "1805.04035", "submitter": "Yulong Lu", "authors": "Jianfeng Lu, Yulong Lu, James Nolen", "title": "Scaling limit of the Stein variational gradient descent: the mean field\n  regime", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an interacting particle system in $\\mathbf{R}^d$ motivated by Stein\nvariational gradient descent [Q. Liu and D. Wang, NIPS 2016], a deterministic\nalgorithm for sampling from a given probability density with unknown\nnormalization. We prove that in the large particle limit the empirical measure\nof the particle system converges to a solution of a non-local and nonlinear\nPDE. We also prove global existence, uniqueness and regularity of the solution\nto the limiting PDE. Finally, we prove that the solution to the PDE converges\nto the unique invariant solution in long time limit.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 16:02:53 GMT"}, {"version": "v2", "created": "Thu, 26 Jul 2018 16:25:46 GMT"}, {"version": "v3", "created": "Tue, 6 Nov 2018 18:27:15 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Lu", "Jianfeng", ""], ["Lu", "Yulong", ""], ["Nolen", "James", ""]]}, {"id": "1805.04164", "submitter": "Momiao Xiong", "authors": "Rong Jiao, Nan Lin, Zixin Hu, David A Bennett, Li Jin and Momiao Xiong", "title": "Bivariate Causal Discovery and its Applications to Gene Expression and\n  Imaging Data Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mainstream of research in genetics, epigenetics and imaging data analysis\nfocuses on statistical association or exploring statistical dependence between\nvariables. Despite their significant progresses in genetic research,\nunderstanding the etiology and mechanism of complex phenotypes remains elusive.\nUsing association analysis as a major analytical platform for the complex data\nanalysis is a key issue that hampers the theoretic development of genomic\nscience and its application in practice. Causal inference is an essential\ncomponent for the discovery of mechanical relationships among complex\nphenotypes. Many researchers suggest making the transition from association to\ncausation. Despite its fundamental role in science, engineering and\nbiomedicine, the traditional methods for causal inference require at least\nthree variables. However, quantitative genetic analysis such as QTL, eQTL,\nmQTL, and genomic-imaging data analysis requires exploring the causal\nrelationships between two variables. This paper will focus on bivariate causal\ndiscovery. We will introduce independence of cause and mechanism (ICM) as a\nbasic principle for causal inference, algorithmic information theory and\nadditive noise model (ANM) as major tools for bivariate causal discovery.\nLarge-scale simulations will be performed to evaluate the feasibility of the\nANM for bivariate causal discovery. To further evaluate their performance for\ncausal inference, the ANM will be applied to the construction of gene\nregulatory networks. Also, the ANM will be applied to trait-imaging data\nanalysis to illustrate three scenarios: presence of both causation and\nassociation, presence of association while absence of causation, and presence\nof causation, while lack of association between two variables.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 20:27:13 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Jiao", "Rong", ""], ["Lin", "Nan", ""], ["Hu", "Zixin", ""], ["Bennett", "David A", ""], ["Jin", "Li", ""], ["Xiong", "Momiao", ""]]}, {"id": "1805.04187", "submitter": "Larry Wasserman", "authors": "Isabella Verdinelli and Larry Wasserman", "title": "Analysis of a Mode Clustering Diagram", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mode-based clustering methods define clusters to be the basins of attraction\nof the modes of a density estimate. The most common version is mean shift clus-\ntering which uses a gradient ascent algorithm to find the basins. Rodriguez and\nLaio (2014) introduced a new method that is faster and simpler than mean shift\nclustering. Furthermore, they define a clustering diagram that provides a sim-\nple, two-dimensional summary of the mode clustering information. We study the\nstatistical properties of this diagram and we propose some improvements and\nextensions. In particular, we show a connection between the diagram and robust\nlinear regression.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 21:58:21 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Verdinelli", "Isabella", ""], ["Wasserman", "Larry", ""]]}, {"id": "1805.04203", "submitter": "Yang Tang", "authors": "Yang Tang and Paul D. McNicholas and Antonio Punzo", "title": "Robust Model-Based Clustering of Voting Records", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the possibility of discovering extreme voting patterns in the U.S.\nCongressional voting records by drawing ideas from the mixture of contaminated\nnormal distributions. A mixture of latent trait models via contaminated normal\ndistributions is proposed. We assume that the low dimensional continuous latent\nvariable comes from a contaminated normal distribution and, therefore, picks up\nextreme patterns in the observed binary data while clustering. We consider in\nparticular such model for the analysis of voting records. The model is applied\nto a U.S. Congressional Voting data set on 16 issues. Note this approach is the\nfirst instance within the literature of a mixture model handling binary data\nwith possible extreme patterns.\n", "versions": [{"version": "v1", "created": "Thu, 10 May 2018 23:10:49 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Tang", "Yang", ""], ["McNicholas", "Paul D.", ""], ["Punzo", "Antonio", ""]]}, {"id": "1805.04394", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen and Yohan Yee and Geoffrey J. McLachlan and Jason P.\n  Lerch", "title": "False discovery rate control under reduced precision computation for\n  analysis of neuroimaging data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mitigation of false positives is an important issue when conducting\nmultiple hypothesis testing. The most popular paradigm for false positives\nmitigation in high-dimensional applications is via the control of the false\ndiscovery rate (FDR). Multiple testing data from neuroimaging experiments can\nbe very large, and reduced precision storage of such data is often required.\nReduced precision computation is often a problem in the analysis of legacy data\nand data arising from legacy pipelines. We present a method for FDR control\nthat is applicable in cases where only p\\text{-values} or test statistics (with\ncommon and known null distribution) are available, and when those\np\\text{-values} or test statistics are encoded in a reduced precision format.\nOur method is based on an empirical-Bayes paradigm where the probit\ntransformation of the p\\text{-values} (called the z\\text{-scores}) are modeled\nas a two-component mixture of normal distributions. Due to the reduced\nprecision of the p\\text{-values} or test statistics, the usual approach for\nfitting mixture models may not be feasible. We instead use a binned-data\ntechnique, which can be proved to consistently estimate the z\\text{-score}\ndistribution parameters under mild correlation assumptions, as is often the\ncase in neuroimaging data. A simulation study shows that our methodology is\ncompetitive when compared with popular alternatives, especially with data in\nthe presence of misspecification. We demonstrate the applicability of our\nmethodology in practice via a brain imaging study of mice.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 13:40:59 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 07:03:31 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Nguyen", "Hien D.", ""], ["Yee", "Yohan", ""], ["McLachlan", "Geoffrey J.", ""], ["Lerch", "Jason P.", ""]]}, {"id": "1805.04421", "submitter": "Xin Zhang", "authors": "Yuqing Pan, Qing Mai, Xin Zhang", "title": "Covariate-Adjusted Tensor Classification in High-Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In contemporary scientific research, it is of great interest to predict a\ncategorical response based on a high-dimensional tensor (i.e. multi-dimensional\narray) and additional covariates. This mixture of different types of data leads\nto challenges in statistical analysis. Motivated by applications in science and\nengineering, we propose a comprehensive and interpretable discriminant analysis\nmodel, called CATCH model (in short for Covariate-Adjusted Tensor\nClassification in High-dimensions), which efficiently integrates the covariates\nand the tensor to predict the categorical outcome. The CATCH model jointly\nmodels the relationships among the covariates, the tensor predictor, and the\ncategorical response. More importantly, it preserves and utilizes the\nstructures of the data for maximum interpretability and optimal prediction. To\ntackle the new computational and statistical challenges arising from the\nintimidating tensor dimensions, we propose a penalized approach to select a\nsubset of tensor predictor entries that has direct discriminative effect after\nadjusting for covariates. We further develop an efficient algorithm that takes\nadvantage of the tensor structure. Theoretical results confirm that our method\nachieves variable selection consistency and optimal classification error, even\nwhen the tensor dimension is much larger than the sample size. The superior\nperformance of our method over existing methods is demonstrated in extensive\nsimulated and real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 14:31:46 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Pan", "Yuqing", ""], ["Mai", "Qing", ""], ["Zhang", "Xin", ""]]}, {"id": "1805.04584", "submitter": "Zhengwu Zhang", "authors": "Zhengwu Zhang and Eric Klassen and Anuj Srivastava", "title": "Robust Comparison of Kernel Densities on Spherical Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While spherical data arises in many contexts, including in directional\nstatistics, the current tools for density estimation and population comparison\non spheres are quite limited. Popular approaches for comparing populations (on\nEuclidean domains) mostly involvea two-step procedure: (1) estimate probability\ndensity functions (pdfs) from their respective samples, most commonly using the\nkernel density estimator, and, (2) compare pdfs using a metric such as the L2\nnorm. However, both the estimated pdfs and their differences depend heavily on\nthe chosen kernels, bandwidths, and sample sizes. Here we develop a framework\nfor comparing spherical populations that is robust to these choices.\nEssentially, we characterize pdfs on spherical domains by quantifying their\nsmoothness. Our framework uses a spectral representation, with densities\nrepresented by their coefficients with respect to the eigenfunctions of the\nLaplacian operator on a sphere. The change in smoothness, akin to using\ndifferent kernel bandwidths, is controlled by exponential decays in coefficient\nvalues. Then we derive a proper distance for comparing pdf coefficients while\nequalizing smoothness levels, negating influences of sample size and bandwidth.\nThis signifies a fair and meaningful comparisons of populations, despite vastly\ndifferent sample sizes, and leads to a robust and improved performance. We\ndemonstrate this framework using examples of variables on S1 and S2, and\nevaluate its performance using a number of simulations and real data\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 20:40:46 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Zhang", "Zhengwu", ""], ["Klassen", "Eric", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1805.04602", "submitter": "Wei Jiang", "authors": "Wei Jiang, Julie Josse, Marc Lavielle and TraumaBase Group", "title": "Logistic Regression with Missing Covariates -- Parameter Estimation,\n  Model Selection and Prediction within a Joint-Modeling Framework", "comments": "R package misaem https://CRAN.R-project.org/package=misaem, R\n  implementations https://github.com/wjiang94/miSAEM_logReg", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression is a common classification method in supervised learning.\nSurprisingly, there are very few solutions for performing logistic regression\nwith missing values in the covariates. We suggest a complete approach based on\na stochastic approximation version of the EM algorithm to do statistical\ninference with missing values including the estimation of the parameters and\ntheir variance, derivation of confidence intervals and a model selection\nprocedure. We also tackle the problem of prediction for new observations (on a\ntest set) with missing covariate data. The methodology is computationally\nefficient, and its good coverage and variable selection properties are\ndemonstrated in a simulation study where we contrast its performances to other\nmethods. For instance, the popular approach of multiple imputation by chained\nequations can lead to estimates that exhibit meaningfully greater biases than\nthe proposed approach. We then illustrate the method on a dataset of severely\ntraumatized patients from Paris hospitals to predict the occurrence of\nhemorrhagic shock, a leading cause of early preventable death in severe trauma\ncases. The aim is to consolidate the current red flag procedure, a binary alert\nidentifying patients with a high risk of severe hemorrhage. The methodology is\nimplemented in the R package misaem.\n", "versions": [{"version": "v1", "created": "Fri, 11 May 2018 21:54:21 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 06:23:41 GMT"}, {"version": "v3", "created": "Mon, 17 Dec 2018 09:35:31 GMT"}, {"version": "v4", "created": "Wed, 7 Aug 2019 19:45:44 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Jiang", "Wei", ""], ["Josse", "Julie", ""], ["Lavielle", "Marc", ""], ["Group", "TraumaBase", ""]]}, {"id": "1805.04667", "submitter": "Xi Chen", "authors": "Xi Chen, David Banks, Mike West", "title": "Bayesian Dynamic Modeling and Monitoring of Network Flows", "comments": "34 pages, 24 figures", "journal-ref": "Net Sci 7 (2019) 292-318", "doi": "10.1017/nws.2019.10", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of a motivating study of dynamic network flow data on a\nlarge-scale e-commerce web site, we develop Bayesian models for\non-line/sequential analysis for monitoring and adapting to changes reflected in\nnode-node traffic. For large-scale networks, we customize core Bayesian time\nseries analysis methods using dynamic generalized linear models (DGLMs). These\nare integrated into the context of multivariate networks using the concept of\ndecouple/recouple that was recently introduced in multivariate time series.\nThis method enables flexible dynamic modeling of flows on large-scale networks\nand exploitation of partial parallelization of analysis while maintaining\ncoherence with an over-arching multivariate dynamic flow model. This approach\nis anchored in a case-study on internet data, with flows of visitors to a\ncommercial news web site defining a long time series of node-node counts on\nover 56,000 node pairs. Central questions include characterizing inherent\nstochasticity in traffic patterns, understanding node-node interactions,\nadapting to dynamic changes in flows and allowing for sensitive monitoring to\nflag anomalies. The methodology of dynamic network DGLMs applies to many\ndynamic network flow studies.\n", "versions": [{"version": "v1", "created": "Sat, 12 May 2018 06:19:06 GMT"}, {"version": "v2", "created": "Sun, 14 Apr 2019 02:42:17 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Chen", "Xi", ""], ["Banks", "David", ""], ["West", "Mike", ""]]}, {"id": "1805.04899", "submitter": "Joseph Antonelli", "authors": "Joseph Antonelli, Georgia Papadogeorgou and Francesca Dominici", "title": "Causal Inference in high dimensions: A marriage between Bayesian\n  modeling and good frequentist properties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for estimating causal effects of binary and\ncontinuous treatments in high dimensions. We show how posterior distributions\nof treatment and outcome models can be used together with doubly robust\nestimators. We propose an approach to uncertainty quantification for the doubly\nrobust estimator which utilizes posterior distributions of model parameters and\n(1) results in good frequentist properties in small samples, (2) is based on a\nsingle MCMC, and (3) improves over frequentist measures of uncertainty which\nrely on asymptotic properties. We show that our proposed variance estimation\nstrategy is consistent when both models are correctly specified and that it is\nconservative in finite samples or when one or both models are misspecified. We\nconsider a flexible framework for modeling the treatment and outcome processes\nwithin the Bayesian paradigm that reduces model dependence, accommodates\nnonlinearity, and achieves dimension reduction of the covariate space. We\nillustrate the ability of the proposed approach to flexibly estimate causal\neffects in high dimensions and appropriately quantify uncertainty, and show\nthat it performs well relative to existing approaches. Finally, we estimate the\neffect of continuous environmental exposures on cholesterol and triglyceride\nlevels. An R package is available at github.com/jantonelli111/DoublyRobustHD.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 15:25:02 GMT"}, {"version": "v2", "created": "Thu, 17 May 2018 14:25:52 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 04:26:57 GMT"}, {"version": "v4", "created": "Sat, 15 Dec 2018 11:59:19 GMT"}, {"version": "v5", "created": "Mon, 23 Sep 2019 19:59:01 GMT"}, {"version": "v6", "created": "Sun, 7 Jun 2020 22:59:12 GMT"}, {"version": "v7", "created": "Fri, 2 Oct 2020 22:02:56 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Antonelli", "Joseph", ""], ["Papadogeorgou", "Georgia", ""], ["Dominici", "Francesca", ""]]}, {"id": "1805.04924", "submitter": "Payam Siyari", "authors": "Payam Siyari, Bistra Dilkina, Constantine Dovrolis", "title": "Emergence and Evolution of Hierarchical Structure in Complex Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.SI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that many complex systems, both in technology and nature,\nexhibit hierarchical modularity: smaller modules, each of them providing a\ncertain function, are used within larger modules that perform more complex\nfunctions. What is not well understood however is how this hierarchical\nstructure (which is fundamentally a network property) emerges, and how it\nevolves over time. We propose a modeling framework, referred to as Evo-Lexis,\nthat provides insight to some fundamental questions about evolving hierarchical\nsystems. Evo-Lexis models the most elementary modules of the system as symbols\n(\"sources\") and the modules at the highest level of the hierarchy as sequences\nof those symbols (\"targets\"). Evo-Lexis computes the optimized adjustment of a\ngiven hierarchy when the set of targets changes over time by additions and\nremovals (a process referred to as \"incremental design\"). In this paper we use\ncomputation modeling to show that:\n  - Low-cost and deep hierarchies emerge when the population of target\nsequences evolves through tinkering and mutation. - Strong selection on the\ncost of new candidate targets results in reuse of more complex (longer) nodes\nin an optimized hierarchy. - The bias towards reuse of complex nodes results in\nan \"hourglass architecture\" (i.e., few intermediate nodes that cover almost all\nsource-target paths). - With such bias, the core nodes are conserved for\nrelatively long time periods although still being vulnerable to major\ntransitions and punctuated equilibria. - Finally, we analyze the differences in\nterms of cost and structure between incrementally designed hierarchies and the\ncorresponding \"clean-slate\" hierarchies which result when the system is\ndesigned from scratch after a change.\n", "versions": [{"version": "v1", "created": "Sun, 13 May 2018 18:38:51 GMT"}, {"version": "v2", "created": "Sat, 4 Aug 2018 21:40:05 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Siyari", "Payam", ""], ["Dilkina", "Bistra", ""], ["Dovrolis", "Constantine", ""]]}, {"id": "1805.05002", "submitter": "Natalie Karavarsamis", "authors": "N. Karavarsamis, G. Guillera-Arroita, RM Huggins, B J T Morgan", "title": "How can the score test be consistent?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The score test statistic using the observed information is easy to compute\nnumerically. Its large sample distribution under the null hypothesis is well\nknown and is equivalent to that of the score test based on the expected\ninformation, the likelihood-ratio test and the Wald test. However, several\nauthors have noted that under the alternative this no longer holds and in\nparticular the statistic can take negative values. Here we examine the score\ntest using the observed information in the context of comparing two binomial\nproportions under imperfect detection, a common problem in ecology when\nstudying occurrence of species. We demonstrate through a combination of\nsimulations and theoretical analysis that a new modified rule which we propose\nthat rejects the null hypothesis when the observed score statistic is larger\nthan the usual chi-square cut-off or is negative has power that is mostly\ngreater to any other test. In addition consistency is largely restored. Our new\ntest is easy to use and inference is always possible.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 03:35:02 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 06:05:40 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Karavarsamis", "N.", ""], ["Guillera-Arroita", "G.", ""], ["Huggins", "RM", ""], ["Morgan", "B J T", ""]]}, {"id": "1805.05017", "submitter": "Kengo Nagashima", "authors": "Kengo Nagashima, Yasunori Sato, Hisashi Noma, Chikuma Hamada", "title": "An efficient and robust method for analyzing population pharmacokinetic\n  data in genome-wide pharmacogenomic studies: a generalized estimating\n  equation approach", "comments": "26 pages, 1 figure", "journal-ref": "Statistics in Medicine 2013; 32(27): 4838-4858", "doi": "10.1002/sim.5895", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powerful array-based single-nucleotide polymorphism--typing platforms have\nrecently heralded a new era in which genome-wide studies are conducted with\nincreasing frequency. A genetic polymorphism associated with population\npharmacokinetics (PK) is typically analyzed using nonlinear mixed-effect models\n(NLMM). Applying NLMM to large-scale data, such as those generated by\ngenome-wide studies, raises several issues related to the assumption of random\neffects, as follows: (i) Computation time: it takes a long time to compute the\nmarginal likelihood. (ii) Convergence of iterative calculation: an adaptive\nGauss-Hermite quadrature is generally used to estimate NLMM; however, iterative\ncalculations may not converge in complex models. (iii) Random-effects\nmisspecification leads to slightly inflated type-I error rates. As an\nalternative effective approach to resolving these issues, in this article we\npropose a generalized estimating equation (GEE) approach for analyzing\npopulation PK data. In general, GEE analysis does not account for\ninter-individual variability in PK parameters; therefore, the usual GEE\nestimators cannot be interpreted straightforwardly, and their validities have\nnot been justified. Here, we propose valid inference methods for using GEE even\nunder conditions of inter-individual variability, and provide theoretical\njustifications of the proposed GEE estimators for population PK data. In\nnumerical evaluations by simulations, the proposed GEE approach exhibited high\ncomputational speed and stability relative to the NLMM approach. Furthermore,\nthe NLMM analysis was sensitive to the misspecification of the random-effects\ndistribution, and the proposed GEE inference is valid for any distributional\nform. An illustration is provided using data from a genome-wide pharmacogenomic\nstudy of an anticancer drug.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 06:06:30 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Nagashima", "Kengo", ""], ["Sato", "Yasunori", ""], ["Noma", "Hisashi", ""], ["Hamada", "Chikuma", ""]]}, {"id": "1805.05054", "submitter": "Pierre Alquier", "authors": "Badr-Eddine Ch\\'erief-Abdellatif and Pierre Alquier", "title": "Consistency of Variational Bayes Inference for Estimation and Model\n  Selection in Mixtures", "comments": null, "journal-ref": "Electronic Journal of Statistics, 2018, vol. 12, no. 2, pp.\n  2995-3035", "doi": "10.1214/18-EJS1475", "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are widely used in Bayesian statistics and machine learning,\nin particular in computational biology, natural language processing and many\nother fields. Variational inference, a technique for approximating intractable\nposteriors thanks to optimization algorithms, is extremely popular in practice\nwhen dealing with complex models such as mixtures. The contribution of this\npaper is two-fold. First, we study the concentration of variational\napproximations of posteriors, which is still an open problem for general\nmixtures, and we derive consistency and rates of convergence. We also tackle\nthe problem of model selection for the number of components: we study the\napproach already used in practice, which consists in maximizing a numerical\ncriterion (the Evidence Lower Bound). We prove that this strategy indeed leads\nto strong oracle inequalities. We illustrate our theoretical results by\napplications to Gaussian and multinomial mixtures.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 08:15:48 GMT"}, {"version": "v2", "created": "Sun, 12 Aug 2018 09:50:12 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Ch\u00e9rief-Abdellatif", "Badr-Eddine", ""], ["Alquier", "Pierre", ""]]}, {"id": "1805.05133", "submitter": "Pascaline Descloux", "authors": "Pascaline Descloux and Sylvain Sardy", "title": "Model selection with lasso-zero: adding straw to the haystack to better\n  find needles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The high-dimensional linear model $y = X \\beta^0 + \\epsilon$ is considered\nand the focus is put on the problem of recovering the support $S^0$ of the\nsparse vector $\\beta^0.$ We introduce Lasso-Zero, a new $\\ell_1$-based\nestimator whose novelty resides in an \"overfit, then threshold\" paradigm and\nthe use of noise dictionaries concatenated to $X$ for overfitting the response.\nTo select the threshold, we employ the quantile universal threshold based on a\npivotal statistic that requires neither knowledge nor preliminary estimation of\nthe noise level. Numerical simulations show that Lasso-Zero performs well in\nterms of support recovery and provides an excellent trade-off between high true\npositive rate and low false discovery rate compared to competitors. Our\nmethodology is supported by theoretical results showing that when no noise\ndictionary is used, Lasso-Zero recovers the signs of $\\beta^0$ under weaker\nconditions on $X$ and $S^0$ than the Lasso and achieves sign consistency for\ncorrelated Gaussian designs. The use of noise dictionary improves the procedure\nfor low signals.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 12:03:13 GMT"}, {"version": "v2", "created": "Fri, 12 Apr 2019 14:33:42 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Descloux", "Pascaline", ""], ["Sardy", "Sylvain", ""]]}, {"id": "1805.05232", "submitter": "Mike West", "authors": "Lindsay Berry and Mike West", "title": "Bayesian forecasting of many count-valued time series", "comments": "26 pages, 10 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops forecasting methodology and application of new classes of\ndynamic models for time series of non-negative counts. Novel univariate models\nsynthesise dynamic generalized linear models for binary and conditionally\nPoisson time series, with dynamic random effects for over-dispersion. These\nmodels allow use of dynamic covariates in both binary and non-zero count\ncomponents. Sequential Bayesian analysis allows fast, parallel analysis of sets\nof decoupled time series. New multivariate models then enable information\nsharing in contexts when data at a more highly aggregated level provide more\nincisive inferences on shared patterns such as trends and seasonality. A novel\nmulti-scale approach-- one new example of the concept of decouple/recouple in\ntime series-- enables information sharing across series. This incorporates\ncross-series linkages while insulating parallel estimation of univariate\nmodels, hence enables scalability in the number of series. The major motivating\ncontext is supermarket sales forecasting. Detailed examples drawn from a case\nstudy in multi-step forecasting of sales of a number of related items showcase\nforecasting of multiple series, with discussion of forecast accuracy metrics\nand broader questions of probabilistic forecast accuracy assessment.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 15:33:58 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Berry", "Lindsay", ""], ["West", "Mike", ""]]}, {"id": "1805.05383", "submitter": "Jeremias Knoblauch", "authors": "Jeremias Knoblauch and Theodoros Damoulas", "title": "Spatio-temporal Bayesian On-line Changepoint Detection with Model\n  Selection", "comments": "10 pages, 7f figures, to appear in Proceedings of the 35th\n  International Conference on Machine Learning 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian On-line Changepoint Detection is extended to on-line model selection\nand non-stationary spatio-temporal processes. We propose spatially structured\nVector Autoregressions (VARs) for modelling the process between changepoints\n(CPs) and give an upper bound on the approximation error of such models. The\nresulting algorithm performs prediction, model selection and CP detection\non-line. Its time complexity is linear and its space complexity constant, and\nthus it is two orders of magnitudes faster than its closest competitor. In\naddition, it outperforms the state of the art for multivariate data.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 18:59:04 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 15:43:20 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Knoblauch", "Jeremias", ""], ["Damoulas", "Theodoros", ""]]}, {"id": "1805.05480", "submitter": "Rafael Izbicki", "authors": "Rafael Izbicki, Ann B. Lee, Taylor Pospisil", "title": "ABC-CDE: Towards Approximate Bayesian Computation with Complex\n  High-Dimensional Data and Limited Simulations", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 2019\n  (https://www.tandfonline.com/doi/abs/10.1080/10618600.2018.1546594)", "doi": "10.1080/10618600.2018.1546594", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) is typically used when the likelihood\nis either unavailable or intractable but where data can be simulated under\ndifferent parameter settings using a forward model. Despite the recent interest\nin ABC, high-dimensional data and costly simulations still remain a bottleneck\nin some applications. There is also no consensus as to how to best assess the\nperformance of such methods without knowing the true posterior. We show how a\nnonparametric conditional density estimation (CDE) framework, which we refer to\nas ABC-CDE, help address three nontrivial challenges in ABC: (i) how to\nefficiently estimate the posterior distribution with limited simulations and\ndifferent types of data, (ii) how to tune and compare the performance of ABC\nand related methods in estimating the posterior itself, rather than just\ncertain properties of the density, and (iii) how to efficiently choose among a\nlarge set of summary statistics based on a CDE surrogate loss. We provide\ntheoretical and empirical evidence that justify ABC-CDE procedures that {\\em\ndirectly} estimate and assess the posterior based on an initial ABC sample, and\nwe describe settings where standard ABC and regression-based approaches are\ninadequate.\n", "versions": [{"version": "v1", "created": "Mon, 14 May 2018 22:05:38 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 12:53:13 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Izbicki", "Rafael", ""], ["Lee", "Ann B.", ""], ["Pospisil", "Taylor", ""]]}, {"id": "1805.05606", "submitter": "Shota Gugushvili", "authors": "Shota Gugushvili, Frank van der Meulen, Moritz Schauer and Peter\n  Spreij", "title": "Nonparametric Bayesian volatility learning under microstructure noise", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at financial applications, we study the problem of learning the\nvolatility under market microstructure noise. Specifically, we consider noisy\ndiscrete time observations from a stochastic differential equation and develop\na novel computational method to learn the diffusion coefficient of the\nequation. We take a nonparametric Bayesian approach, where we model the\nvolatility function a priori as piecewise constant. Its prior is specified via\nthe inverse Gamma Markov chain. Sampling from the posterior is accomplished by\nincorporating the Forward Filtering Backward Simulation algorithm in the Gibbs\nsampler. Good performance of the method is demonstrated on two representative\nsynthetic data examples. Finally, we apply the method on the EUR/USD exchange\nrate dataset.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 07:32:18 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Gugushvili", "Shota", ""], ["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""], ["Spreij", "Peter", ""]]}, {"id": "1805.05756", "submitter": "Michael Friendly", "authors": "Michael Friendly and Matthew Sigal", "title": "Visualizing Tests for Equality of Covariance Matrices", "comments": "The American Statistician, in press (2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores a variety of topics related to the question of testing\nthe equality of covariance matrices in multivariate linear models, particularly\nin the MANOVA setting. The main focus is on graphical methods that can be used\nto address the evaluation of this assumption. We introduce some extensions of\ndata ellipsoids, hypothesis-error (HE) plots and canonical discriminant plots\nand demonstrate how they can be applied to the testing of equality of\ncovariance matrices. Further, a simple plot of the components of Box's M test\nis proposed that shows _how_ groups differ in covariance and also suggests\nother visualizations and alternative test statistics. These methods are\nimplemented and freely available in the **heplots** and **candisc** packages\nfor R. Examples from the paper are available in supplementary materials.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 13:36:05 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Friendly", "Michael", ""], ["Sigal", "Matthew", ""]]}, {"id": "1805.05795", "submitter": "Suzie Cro", "authors": "Suzie Cro, James R Carpenter, Michael G Kenward", "title": "Information-Anchored Sensitivity Analysis: Theory and Application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of longitudinal randomised controlled trials is frequently\ncomplicated because patients deviate from the protocol. Where such deviations\nare relevant for the estimand, we are typically required to make an untestable\nassumption about post-deviation behaviour in order to perform our primary\nanalysis and estimate the treatment effect. In such settings, it is now widely\nrecognised that we should follow this with sensitivity analyses to explore the\nrobustness of our inferences to alternative assumptions about post-deviation\nbehaviour. Although there has been a lot of work on how to conduct such\nsensitivity analyses, little attention has been given to the appropriate loss\nof information due to missing data within sensitivity analysis. We argue more\nattention needs to be given to this issue, showing it is quite possible for\nsensitivity analysis to decrease and increase the information about the\ntreatment effect. To address this critical issue, we introduce the concept of\ninformation-anchored sensitivity analysis. By this we mean sensitivity analysis\nin which the proportion of information about the treatment estimate lost due to\nmissing data is the same as the proportion of information about the treatment\nestimate lost due to missing data in the primary analysis. We argue this forms\na transparent, practical starting point for interpretation of sensitivity\nanalysis. We then derive results showing that, for longitudinal continuous\ndata, a broad class of controlled and reference-based sensitivity analyses\nperformed by multiple imputation are information-anchored. We illustrate the\ntheory with simulations and an analysis of a peer review trial, then discuss\nour work in the context of other recent work in this area. Our results give a\ntheoretical basis for the use of controlled multiple imputation procedures for\nsensitivity analysis.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 14:20:53 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Cro", "Suzie", ""], ["Carpenter", "James R", ""], ["Kenward", "Michael G", ""]]}, {"id": "1805.06035", "submitter": "Anders Ledberg", "authors": "Anders Ledberg", "title": "Confounding caused by causal-effect covariability", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Confounding seriously impairs our ability to learn about causal relations\nfrom observational data. Confounding can be defined as a statistical\nassociation between two variables due to inputs from a common source (the\nconfounder). For example, if $Z\\rightarrow Y$ and $Z\\rightarrow X$, then $X$\nand $Y$ will be statistically dependent, even if there are no causal\nconnections between the two. There are several approaches available to adjust\nfor confounding, i.e. to remove, or reduce, the association between two\nvariables due to the confounder. Common adjustment techniques include\nstratifying the analysis on the confounder, and including confounders as\ncovariates in regression models. Most adjustments rely on the assumption that\nthe causal effects of confounders, on different variables, do not co-vary. For\nexample, if the causal effect of $Z$ on $X$ and the causal effect of $Z$ on $Y$\nco-vary between observational units, a confounding effect remains after\nadjustment for $Z$. This causal-effect covariability and its consequences is\nthe topic of this paper.\n  Causal-effect covariability is first explicated using the framework of\nstructural causal models. Using this framework it is easy to show that\ncausal-effect covariability generally leads to confounding that cannot be\nadjusted for by standard methods. Evidence from data indicates that the\nconfounding introduced by causal-effect covariability might be a real concern\nin applied work.\n", "versions": [{"version": "v1", "created": "Tue, 15 May 2018 21:18:05 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Ledberg", "Anders", ""]]}, {"id": "1805.06099", "submitter": "Sam Brilleman", "authors": "Samuel L. Brilleman, Michael J. Crowther, Margarita Moreno-Betancur,\n  Jacqueline Buros Novik, James Dunyak, Nidal Al-Huniti, Robert Fox, Jeff\n  Hammerbacher, Rory Wolfe", "title": "Joint longitudinal and time-to-event models for multilevel hierarchical\n  data", "comments": "35 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint modelling of longitudinal and time-to-event data has received much\nattention recently. Increasingly, extensions to standard joint modelling\napproaches are being proposed to handle complex data structures commonly\nencountered in applied research. In this paper we propose a joint model for\nhierarchical longitudinal and time-to-event data. Our motivating application\nexplores the association between tumor burden and progression-free survival in\nnon-small cell lung cancer patients. We define tumor burden as a function of\nthe sizes of target lesions clustered within a patient. Since a patient may\nhave more than one lesion, and each lesion is tracked over time, the data have\na three-level hierarchical structure: repeated measurements taken at time\npoints (level 1) clustered within lesions (level 2) within patients (level 3).\nWe jointly model the lesion-specific longitudinal trajectories and\npatient-specific risk of death or disease progression by specifying novel\nassociation structures that combine information across lower level clusters\n(e.g. lesions) into patient-level summaries (e.g. tumor burden). We provide\nuser-friendly software for fitting the model under a Bayesian framework.\nLastly, we discuss alternative situations in which additional clustering\nfactor(s) occur at a level higher in the hierarchy than the patient-level,\nsince this has implications for the model formulation.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 02:35:56 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Brilleman", "Samuel L.", ""], ["Crowther", "Michael J.", ""], ["Moreno-Betancur", "Margarita", ""], ["Novik", "Jacqueline Buros", ""], ["Dunyak", "James", ""], ["Al-Huniti", "Nidal", ""], ["Fox", "Robert", ""], ["Hammerbacher", "Jeff", ""], ["Wolfe", "Rory", ""]]}, {"id": "1805.06178", "submitter": "Rhythm Grover", "authors": "Rhythm Grover, Debasis Kundu and Amit Mitra", "title": "Chirp-like model and its parameter estimation", "comments": "33 pages, 5 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a chirp-like signal model as an alternative to a chirp model and a\ngeneralisation of the sinusoidal model, which is a fundamental model in the\nstatistical signal processing literature. It is observed that the proposed\nmodel can be arbitrarily close to the chirp model. The propounded model is\nsimilar to a chirp model in the sense that here also the frequency changes\nlinearly with time. However, the parameter estimation of a chirp-like model is\nsimpler compared to a chirp model. In this paper, we consider the least squares\nand the sequential least squares estimation procedures and study the asymptotic\nproperties of these proposed estimators. These asymptotic results are\ncorroborated through simulation studies and analysis of four speech signal data\nsets have been performed to see the effectiveness of the proposed model, and\nthe results are quite encouraging.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 07:58:05 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Grover", "Rhythm", ""], ["Kundu", "Debasis", ""], ["Mitra", "Amit", ""]]}, {"id": "1805.06364", "submitter": "Gabriela Ciuperca", "authors": "Gabriela Ciuperca", "title": "Adaptive elastic-net selection in a quantile model with diverging number\n  of variable groups", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real applications of the linear model, the explanatory variables are very\noften naturally grouped, the most common example being the multivariate\nvariance analysis. In the present paper, a quantile model with structure group\nis considered, the number of groups can diverge with sample size. We introduce\nand study the adaptive elastic-net group estimator, for improving the parameter\nestimation accuracy. This method allows automatic selection, with a probability\nconverging to one, of significant groups and further the non zero parameter\nestimators are asymptotically normal. The convergence rate of the adaptive\nelastic-net group quantile estimator is also obtained, rate which depends on\nthe number of groups. In order to put the estimation method into practice, an\nalgorithm based on the subgradient method is proposed and implemented. The\nMonte Carlo simulations show that the adaptive elastic-net group quantile\nestimations are more accurate that other existing group estimations in the\nliterature. Moreover, the numerical study confirms the theoretical results and\nthe usefulness of the proposed estimation method.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 15:14:21 GMT"}, {"version": "v2", "created": "Tue, 27 Nov 2018 16:17:28 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2019 21:34:01 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Ciuperca", "Gabriela", ""]]}, {"id": "1805.06432", "submitter": "Pengfei Li", "authors": "Yilin Chen, Pengfei Li and Changbao Wu", "title": "Doubly Robust Inference with Non-probability Survey Samples", "comments": "25 pages, 5 tables, this paper has been submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a general framework for statistical inferences with\nnon-probability survey samples when relevant auxiliary information is available\nfrom a probability survey sample. We develop a rigorous procedure for\nestimating the propensity scores for units in the non-probability sample, and\nconstruct doubly robust estimators for the finite population mean. Variance\nestimation is discussed under the proposed framework. Results from simulation\nstudies show the robustness and the efficiency of our proposed estimators as\ncompared to existing methods. The proposed method is used to analyze a\nnon-probability survey sample collected by the Pew Research Center with\nauxiliary information from the Behavioral Risk Factor Surveillance System and\nthe Current Population Survey. Our results illustrate a general approach to\ninference with non-probability samples and highlight the importance and\nusefulness of auxiliary information from probability survey samples.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 17:15:12 GMT"}], "update_date": "2018-05-17", "authors_parsed": [["Chen", "Yilin", ""], ["Li", "Pengfei", ""], ["Wu", "Changbao", ""]]}, {"id": "1805.06488", "submitter": "Sungwook Kim", "authors": "Sungwook Kim, Michael P. Fay, and Michael A. Proschan", "title": "Valid and Approximately Valid Confidence Intervals for Current Status\n  Data", "comments": "42 pages, and 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new framework for creating point-wise confidence intervals for\nthe distribution of event times for current status data. Existing methods are\nbased on asymptotics. Our framework is based on binomial properties and\nmotivates confidence intervals that are very simple to apply and are valid,\ni.e., guarantee nominal coverage. Although these confidence intervals are\nnecessarily conservative for small sample sizes, asymptotically their coverage\nrate approaches the nominal one. This binomial framework also motivates\napproximately valid confidence intervals, and simulations show that these\napproximate intervals generally have coverage rates closer to the nominal level\nwith shorter length than existing intervals, including the likelihood\nratio-based confidence interval. Unlike previous asymptotic methods that\nrequire different asymptotic distributions for continuous or grid-based\nassessment, the binomial framework can be applied to either type of assessment\ndistribution.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 18:48:17 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Kim", "Sungwook", ""], ["Fay", "Michael P.", ""], ["Proschan", "Michael A.", ""]]}, {"id": "1805.06539", "submitter": "Tineke Blom", "authors": "Tineke Blom, Stephan Bongers, Joris M. Mooij", "title": "Beyond Structural Causal Models: Causal Constraints Models", "comments": "Published in Proceedings of the 35th Annual Conference on Uncertainty\n  in Artificial Intelligence (UAI-19)", "journal-ref": "Proceedings of the 35th Annual Conference on Uncertainty in\n  Artificial Intelligence, 2019", "doi": null, "report-no": null, "categories": "cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural Causal Models (SCMs) provide a popular causal modeling framework.\nIn this work, we show that SCMs are not flexible enough to give a complete\ncausal representation of dynamical systems at equilibrium. Instead, we propose\na generalization of the notion of an SCM, that we call Causal Constraints Model\n(CCM), and prove that CCMs do capture the causal semantics of such systems. We\nshow how CCMs can be constructed from differential equations and initial\nconditions and we illustrate our ideas further on a simple but ubiquitous\n(bio)chemical reaction. Our framework also allows to model functional laws,\nsuch as the ideal gas law, in a sensible and intuitive way.\n", "versions": [{"version": "v1", "created": "Wed, 16 May 2018 22:04:28 GMT"}, {"version": "v2", "created": "Fri, 17 May 2019 12:33:01 GMT"}, {"version": "v3", "created": "Tue, 6 Aug 2019 10:21:44 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Blom", "Tineke", ""], ["Bongers", "Stephan", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1805.06639", "submitter": "Ze Jin", "authors": "Ze Jin, David S. Matteson", "title": "Independent Component Analysis via Energy-based and Kernel-based Mutual\n  Dependence Measures", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply both distance-based (Jin and Matteson, 2017) and kernel-based\n(Pfister et al., 2016) mutual dependence measures to independent component\nanalysis (ICA), and generalize dCovICA (Matteson and Tsay, 2017) to MDMICA,\nminimizing empirical dependence measures as an objective function in both\ndeflation and parallel manners. Solving this minimization problem, we introduce\nLatin hypercube sampling (LHS) (McKay et al., 2000), and a global optimization\nmethod, Bayesian optimization (BO) (Mockus, 1994) to improve the initialization\nof the Newton-type local optimization method. The performance of MDMICA is\nevaluated in various simulation studies and an image data example. When the ICA\nmodel is correct, MDMICA achieves competitive results compared to existing\napproaches. When the ICA model is misspecified, the estimated independent\ncomponents are less mutually dependent than the observed components using\nMDMICA, while they are prone to be even more mutually dependent than the\nobserved components using other approaches.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 07:53:09 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Matteson", "David S.", ""]]}, {"id": "1805.06640", "submitter": "Ze Jin", "authors": "Ze Jin, Xiaohan Yan, David S. Matteson", "title": "Testing for Conditional Mean Independence with Covariates through\n  Martingale Difference Divergence", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a crucial problem in statistics is to decide whether additional variables\nare needed in a regression model. We propose a new multivariate test to\ninvestigate the conditional mean independence of Y given X conditioning on some\nknown effect Z, i.e., E(Y|X, Z) = E(Y|Z). Assuming that E(Y|Z) and Z are\nlinearly related, we reformulate an equivalent notion of conditional mean\nindependence through transformation, which is approximated in practice. We\napply the martingale difference divergence (Shao and Zhang, 2014) to measure\nconditional mean dependence, and show that the estimation error from\napproximation is negligible, as it has no impact on the asymptotic distribution\nof the test statistic under some regularity assumptions. The implementation of\nour test is demonstrated by both simulations and a financial data example.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 07:53:48 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Jin", "Ze", ""], ["Yan", "Xiaohan", ""], ["Matteson", "David S.", ""]]}, {"id": "1805.06714", "submitter": "Oliver Dukes", "authors": "Oliver Dukes, Vahe Avagyan and Stijn Vansteelandt", "title": "Doubly robust tests of exposure effects under high-dimensional\n  confounding", "comments": "53 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After variable selection, standard inferential procedures for regression\nparameters may not be uniformly valid; there is no finite-sample size at which\na standard test is guaranteed to approximately attain its nominal size. This\nproblem is exacerbated in high-dimensional settings, where variable selection\nbecomes unavoidable. This has prompted a flurry of activity in developing\nuniformly valid hypothesis tests for a low-dimensional regression parameter\n(e.g. the causal effect of an exposure A on an outcome Y) in high-dimensional\nmodels. So far there has been limited focus on model misspecification, although\nthis is inevitable in high-dimensional settings. We propose tests of the null\nthat are uniformly valid under sparsity conditions weaker than those typically\ninvoked in the literature, assuming working models for the exposure and outcome\nare both correctly specified. When one of the models is misspecified, by\namending the procedure for estimating the nuisance parameters, our tests\ncontinue to be valid; hence they are doubly robust. Our proposals are\nstraightforward to implement using existing software for penalized maximum\nlikelihood estimation and do not require sample-splitting. We illustrate them\nin simulations and an analysis of data obtained from the Ghent University\nIntensive Care Unit.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 11:57:31 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 13:06:05 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2019 10:38:48 GMT"}, {"version": "v4", "created": "Mon, 6 Jul 2020 08:21:59 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Dukes", "Oliver", ""], ["Avagyan", "Vahe", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "1805.06730", "submitter": "Debasis Kundu Professor", "authors": "N. Balakrishnan and D. Kundu", "title": "Birnbaum-Saunders Distribution: A Review of Models, Analysis and\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Birnbaum and Saunders introduced a two-parameter lifetime distribution to\nmodel fatigue life of a metal, subject to cyclic stress. Since then, extensive\nwork has been done on this model providing different interpretations,\nconstructions, generalizations, inferential methods, and extensions to\nbivariate, multivariate and matrix-variate cases. More than two hundred papers\nand one research monograph have already appeared describing all these aspects\nand developments. In this paper, we provide a detailed review of all these\ndevelopments and at the same time indicate several open problems that could be\nconsidered for further research.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 12:39:26 GMT"}], "update_date": "2018-05-18", "authors_parsed": [["Balakrishnan", "N.", ""], ["Kundu", "D.", ""]]}, {"id": "1805.06826", "submitter": "Yixin Wang", "authors": "Yixin Wang, David M. Blei", "title": "The Blessings of Multiple Causes", "comments": "72 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference from observational data often assumes \"ignorability,\" that\nall confounders are observed. This assumption is standard yet untestable.\nHowever, many scientific studies involve multiple causes, different variables\nwhose effects are simultaneously of interest. We propose the deconfounder, an\nalgorithm that combines unsupervised machine learning and predictive model\nchecking to perform causal inference in multiple-cause settings. The\ndeconfounder infers a latent variable as a substitute for unobserved\nconfounders and then uses that substitute to perform causal inference. We\ndevelop theory for the deconfounder, and show that it requires weaker\nassumptions than classical causal inference. We analyze its performance in\nthree types of studies: semi-simulated data around smoking and lung cancer,\nsemi-simulated data around genome-wide association studies, and a real dataset\nabout actors and movie revenue. The deconfounder provides a checkable approach\nto estimating closer-to-truth causal effects.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 15:39:17 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 16:24:28 GMT"}, {"version": "v3", "created": "Mon, 15 Apr 2019 03:37:55 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Wang", "Yixin", ""], ["Blei", "David M.", ""]]}, {"id": "1805.06855", "submitter": "Yinchu Zhu", "authors": "Yinchu Zhu", "title": "Learning non-smooth models: instrumental variable quantile regressions\n  and related problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes computationally efficient methods that can be used for\ninstrumental variable quantile regressions (IVQR) and related methods with\nstatistical guarantees. This is much needed when we investigate heterogenous\ntreatment effects since interactions between the endogenous treatment and\ncontrol variables lead to an increased number of endogenous covariates. We\nprove that the GMM formulation of IVQR is NP-hard and finding an approximate\nsolution is also NP-hard. Hence, solving the problem from a purely\ncomputational perspective seems unlikely. Instead, we aim to obtain an estimate\nthat has good statistical properties and is not necessarily the global solution\nof any optimization problem.\n  The proposal consists of employing $k$-step correction on an initial\nestimate. The initial estimate exploits the latest advances in mixed integer\nlinear programming and can be computed within seconds. One theoretical\ncontribution is that such initial estimators and Jacobian of the moment\ncondition used in the k-step correction need not be even consistent and merely\n$k=4\\log n$ fast iterations are needed to obtain an efficient estimator. The\noverall proposal scales well to handle extremely large sample sizes because\nlack of consistency requirement allows one to use a very small subsample to\nobtain the initial estimate and the k-step iterations on the full sample can be\nimplemented efficiently. Another contribution that is of independent interest\nis to propose a tuning-free estimation for the Jacobian matrix, whose\ndefinition nvolves conditional densities. This Jacobian estimator generalizes\nbootstrap quantile standard errors and can be efficiently computed via\nclosed-end solutions. We evaluate the performance of the proposal in\nsimulations and an empirical example on the heterogeneous treatment effect of\nJob Training Partnership Act.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 16:58:11 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 17:10:45 GMT"}, {"version": "v3", "created": "Wed, 28 Aug 2019 17:29:53 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 07:10:30 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Zhu", "Yinchu", ""]]}, {"id": "1805.06915", "submitter": "Martin Slawski", "authors": "Felicitas J. Detmer and Martin Slawski", "title": "A Note on Coding and Standardization of Categorical Variables in\n  (Sparse) Group Lasso Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorical regressor variables are usually handled by introducing a set of\nindicator variables, and imposing a linear constraint to ensure identifiability\nin the presence of an intercept, or equivalently, using one of various coding\nschemes. As proposed in Yuan and Lin [J. R. Statist. Soc. B, 68 (2006), 49-67],\nthe group lasso is a natural and computationally convenient approach to perform\nvariable selection in settings with categorical covariates. As pointed out by\nSimon and Tibshirani [Stat. Sin., 22 (2011), 983-1001], \"standardization\" by\nmeans of block-wise orthonormalization of column submatrices each corresponding\nto one group of variables can substantially boost performance. In this note, we\nstudy the aspect of standardization for the special case of categorical\npredictors in detail. The main result is that orthonormalization is not\nrequired; column-wise scaling of the design matrix followed by re-scaling and\ncentering of the coefficients is shown to have exactly the same effect. Similar\nreductions can be achieved in the case of interactions. The extension to the\nso-called sparse group lasso, which additionally promotes within-group\nsparsity, is considered as well. The importance of proper standardization is\nillustrated via extensive simulations.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 18:21:58 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Detmer", "Felicitas J.", ""], ["Slawski", "Martin", ""]]}, {"id": "1805.06970", "submitter": "Rong Ma", "authors": "Rong Ma, T. Tony Cai and Hongzhe Li", "title": "Global and Simultaneous Hypothesis Testing for High-Dimensional Logistic\n  Regression Models", "comments": "Typos corrected", "journal-ref": "Journal of the American Statistical Association (2019)", "doi": "10.1080/01621459.2019.1699421", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional logistic regression is widely used in analyzing data with\nbinary outcomes. In this paper, global testing and large-scale multiple testing\nfor the regression coefficients are considered in both single- and\ntwo-regression settings. A test statistic for testing the global null\nhypothesis is constructed using a generalized low-dimensional projection for\nbias correction and its asymptotic null distribution is derived. A lower bound\nfor the global testing is established, which shows that the proposed test is\nasymptotically minimax optimal over some sparsity range. For testing the\nindividual coefficients simultaneously, multiple testing procedures are\nproposed and shown to control the false discovery rate (FDR) and falsely\ndiscovered variables (FDV) asymptotically. Simulation studies are carried out\nto examine the numerical performance of the proposed tests and their\nsuperiority over existing methods. The testing procedures are also illustrated\nby analyzing a data set of a metabolomics study that investigates the\nassociation between fecal metabolites and pediatric Crohn's disease and the\neffects of treatment on such associations.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 21:11:34 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 19:32:51 GMT"}, {"version": "v3", "created": "Wed, 7 Aug 2019 23:22:57 GMT"}, {"version": "v4", "created": "Wed, 13 Nov 2019 22:05:12 GMT"}, {"version": "v5", "created": "Thu, 19 Nov 2020 19:08:45 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Ma", "Rong", ""], ["Cai", "T. Tony", ""], ["Li", "Hongzhe", ""]]}, {"id": "1805.07042", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla", "title": "Graphon estimation via nearest neighbor algorithm and 2D fused lasso\n  denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of methods for graphon estimation based on exploiting\nconnections with nonparametric regression. The idea is to construct an ordering\nof the nodes in the network, similar in spirit to Chan and Airoldi (2014).\nHowever, rather than only considering orderings based on the empirical degree\nas in Chan and Airoldi (2014), we use the nearest neighbor algorithm which is\nan approximating solution to the traveling salesman problem. This in turn can\nhandle general distances $\\hat{d}$ between the nodes, something that allows us\nto incorporate rich information of the network. Once an ordering is\nconstructed, we formulate a 2D grid graph denoising problem that we solve\nthrough fused lasso regularization. For particular choices of the metric\n$\\hat{d}$, we show that the corresponding two-step estimator can attain\ncompetitive rates when the true model is the stochastic block model, and when\nthe underlying graphon is piecewise H\\\"{o}lder or it has bounded variation.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 04:13:10 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 08:11:32 GMT"}, {"version": "v3", "created": "Mon, 10 Jun 2019 01:35:18 GMT"}, {"version": "v4", "created": "Tue, 18 Jun 2019 23:43:22 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""]]}, {"id": "1805.07086", "submitter": "Ngom Papa", "authors": "Macoumba Ndourand Mactar Ndaw and Papa Ngom", "title": "Relationship between the Bregman divergence and beta-divergence and\n  their Applications", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bregman divergence have been the subject of several studies. We do not go\nto do an exhaustive study of its subclasses, but propose a proof that shows\nthat the \\b{eta}-divergence are subclasses of the Bregman divergences. It is in\nthis order of idea that we will make a proposition of demonstration which shows\nthat the \\b{eta}-divergence are particular cases of the Bregman divergence. And\nalso we will propose algorithms and their applications to show the consistency\nof our approach. This is of interest for numerous applications since these\ndivergences are widely used for instant non-negative matrix factorization\n(NMF).\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 08:14:26 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Ndaw", "Macoumba Ndourand Mactar", ""], ["Ngom", "Papa", ""]]}, {"id": "1805.07088", "submitter": "Ngom Papa", "authors": "Papa Ngom, Freedath Djibril Moussa, Jean de Dieu Nkurunziza", "title": "Strongly Consistent of Kullback-Leibler Divergence Estimator and Tests\n  for Model Selection Based on a Bias Reduced Kernel Density Estimator", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the strong consistency of a bias reduced kernel\ndensity estimator and derive a strongly con- sistent Kullback-Leibler\ndivergence (KLD) estimator. As application, we formulate a goodness-of-fit test\nand an asymptotically standard normal test for model selection. The Monte Carlo\nsimulation show the effectiveness of the proposed estimation methods and\nstatistical tests.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 08:15:54 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Ngom", "Papa", ""], ["Moussa", "Freedath Djibril", ""], ["Nkurunziza", "Jean de Dieu", ""]]}, {"id": "1805.07092", "submitter": "Peter Zeidman", "authors": "Karl Friston, Thomas Parr, Peter Zeidman", "title": "Bayesian model reduction", "comments": "The manuscript has been thoroughly updated, including more detailed\n  explanations, additional derivations and three worked examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews recent developments in statistical structure learning;\nnamely, Bayesian model reduction. Bayesian model reduction is a method for\nrapidly computing the evidence and parameters of probabilistic models that\ndiffer only in their priors. In the setting of variational Bayes this has an\nanalytical solution, which finesses the problem of scoring large model spaces\nin model comparison or structure learning. In this technical note, we review\nBayesian model reduction and provide the relevant equations for several\ndiscrete and continuous probability distributions. We provide worked examples\nin the context of multivariate linear regression, Gaussian mixture models and\ndynamical systems (dynamic causal modelling). These examples are accompanied by\nthe Matlab scripts necessary to reproduce the results. Finally, we briefly\nreview recent applications in the fields of neuroimaging and neuroscience.\nSpecifically, we consider structure learning and hierarchical or empirical\nBayes that can be regarded as a metaphor for neurobiological processes like\nabductive reasoning.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 08:32:31 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2019 12:30:56 GMT"}], "update_date": "2019-10-15", "authors_parsed": [["Friston", "Karl", ""], ["Parr", "Thomas", ""], ["Zeidman", "Peter", ""]]}, {"id": "1805.07109", "submitter": "Junyang Wang", "authors": "Junyang Wang, Jon Cockayne, Chris Oates", "title": "On the Bayesian Solution of Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interpretation of numerical methods, such as finite difference methods\nfor differential equations, as point estimators allows for formal statistical\nquantification of the error due to discretisation in the numerical context.\nCompeting statistical paradigms can be considered and Bayesian probabilistic\nnumerical methods (PNMs) are obtained when Bayesian statistical principles are\ndeployed. Bayesian PNM are closed under composition, such that uncertainty due\nto different sources of discretisation can be jointly modelled and rigorously\npropagated. However, we argue that no strictly Bayesian PNM for the numerical\nsolution of ordinary differential equations (ODEs) have yet been developed. To\naddress this gap, we work at a foundational level, where a novel Bayesian PNM\nis proposed as a proof-of-concept. Our proposal is a synthesis of classical Lie\ngroup methods, to exploit the underlying structure of the gradient field, and\nnon-parametric regression in a transformed solution space for the ODE. The\nprocedure is presented in detail for first order ODEs and relies on a certain\ntechnical condition -- existence of a solvable Lie algebra -- being satisfied.\nNumerical illustrations are provided.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 09:25:54 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 11:28:23 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Wang", "Junyang", ""], ["Cockayne", "Jon", ""], ["Oates", "Chris", ""]]}, {"id": "1805.07147", "submitter": "Andrea Gabrio", "authors": "Andrea Gabrio, and Michael J. Daniels, and Gianluca Baio", "title": "A Bayesian Parametric Approach to Handle Missing Longitudinal Outcome\n  Data in Trial-Based Health Economic Evaluations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trial-based economic evaluations are typically performed on cross-sectional\nvariables, derived from the responses for only the completers in the study,\nusing methods that ignore the complexities of utility and cost data (e.g.\nskewness and spikes). We present an alternative and more efficient Bayesian\nparametric approach to handle missing longitudinal outcomes in economic\nevaluations, while accounting for the complexities of the data. We specify a\nflexible parametric model for the observed data and partially identify the\ndistribution of the missing data with partial identifying restrictions and\nsensitivity parameters. We explore alternative nonignorable scenarios through\ndifferent priors for the sensitivity parameters, calibrated on the observed\ndata. Our approach is motivated by, and applied to, data from a trial assessing\nthe cost-effectiveness of a new treatment for intellectual disability and\nchallenging behaviour.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 11:27:33 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Gabrio", "Andrea", ""], ["Daniels", "Michael J.", ""], ["Baio", "Gianluca", ""]]}, {"id": "1805.07267", "submitter": "Linda S. L. Tan", "authors": "Linda S. L. Tan", "title": "Use of model reparametrization to improve variational Bayes", "comments": null, "journal-ref": "JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL\n  METHODOLOGY, 2020", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose using model reparametrization to improve variational Bayes\ninference for hierarchical models whose variables can be classified as global\n(shared across observations) or local (observation specific). Posterior\ndependence between local and global variables is minimized by applying an\ninvertible affine transformation on the local variables. The functional form of\nthis transformation is deduced by approximating the posterior distribution of\neach local variable conditional on the global variables by a Gaussian density\nvia a second order Taylor expansion. Variational Bayes inference for the\nreparametrized model is then obtained using stochastic approximation. Our\napproach can be readily extended to large datasets via a divide and recombine\nstrategy. Using generalized linear mixed models, we demonstrate that\nreparametrized variational Bayes (RVB) provides improvements in both accuracy\nand convergence rate compared to state of the art Gaussian variational\napproximation methods.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 15:04:08 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 20:10:38 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2020 06:57:33 GMT"}], "update_date": "2021-01-28", "authors_parsed": [["Tan", "Linda S. L.", ""]]}, {"id": "1805.07272", "submitter": "Fabian Rathke", "authors": "Fabian Rathke, Christoph Schn\\\"orr", "title": "Fast Multivariate Log-Concave Density Estimation", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel computational approach to log-concave density estimation is proposed.\nPrevious approaches utilize the piecewise-affine parametrization of the density\ninduced by the given sample set. The number of parameters as well as non-smooth\nsubgradient-based convex optimization for determining the maximum likelihood\ndensity estimate cause long runtimes for dimensions $d \\geq 2$ and large sample\nsets. The presented approach is based on mildly non-convex smooth\napproximations of the objective function and \\textit{sparse}, adaptive\npiecewise-affine density parametrization. Established memory-efficient\nnumerical optimization techniques enable to process larger data sets for\ndimensions $d \\geq 2$. While there is no guarantee that the algorithm returns\nthe maximum likelihood estimate for every problem instance, we provide\ncomprehensive numerical evidence that it does yield near-optimal results after\nsignificantly shorter runtimes. For example, 10000 samples in $\\mathbb{R}^2$\nare processed in two seconds, rather than in $\\approx 14$ hours required by the\nprevious approach to terminate. For higher dimensions, density estimation\nbecomes tractable as well: Processing $10000$ samples in $\\mathbb{R}^6$\nrequires 35 minutes. The software is publicly available as CRAN R package\nfmlogcondens.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 15:13:21 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 17:41:34 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Rathke", "Fabian", ""], ["Schn\u00f6rr", "Christoph", ""]]}, {"id": "1805.07301", "submitter": "Zifeng Zhao", "authors": "Peng Shi and Zifeng Zhao", "title": "Predictive Modeling of Multivariate Longitudinal Insurance Claims Using\n  Pair Copula Construction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bundling feature of a nonlife insurance contract often leads to multiple\nlongitudinal measurements of an insurance risk. Assessing the association among\nthe evolution of the multivariate outcomes is critical to the operation of\nproperty-casualty insurers. One complication in the modeling process is the\nnon-continuousness of insurance risks.\n  Motivated by insurance applications, we propose a general framework for\nmodeling multivariate repeated measurements. The framework easily accommodates\ndifferent types of data, including continuous, discrete, as well as mixed\noutcomes. Specifically, the longitudinal observations of each response is\nseparately modeled using pair copula constructions with a D-vine structure. The\nmultiple D-vines are then joined by a multivariate copula. A sequential\napproach is employed for inference and its performance is investigated under a\nsimulated setting.\n  In the empirical analysis, we examine property risks in a government\nmulti-peril property insurance program. The proposed method is applied to both\npolicyholders' claim count and loss cost. The model is validated based on\nout-of-sample predictions.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 15:46:56 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Shi", "Peng", ""], ["Zhao", "Zifeng", ""]]}, {"id": "1805.07413", "submitter": "Maricela Cruz", "authors": "Maricela Cruz, Daniel L. Gillen, Miriam Bender, Hernando Ombao", "title": "Assessing Health Care Interventions via an Interrupted Time Series\n  Model: Study Power and Design Considerations", "comments": "41 pages, 6 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The delivery and assessment of quality health care is complex with many\ninteracting and interdependent components. In terms of research design and\nstatistical analysis, this complexity and interdependency makes it difficult to\nassess the true impact of interventions designed to improve patient health care\noutcomes. Interrupted time series (ITS) is a quasi-experimental design\ndeveloped for inferring the effectiveness of a health policy intervention while\naccounting for temporal dependence within a single system or unit. Current\nstandardized ITS methods do not simultaneously analyze data for several units,\nnor are there methods to test for the existence of a change point and to assess\nstatistical power for study planning purposes in this context. To address this\nlimitation we propose the `Robust Multiple ITS' (R-MITS) model, appropriate for\nmulti-unit ITS data, that allows for inference regarding the estimation of a\nglobal change point across units in the presence of a potentially lagged (or\nanticipatory) treatment effect. Under the R-MITS model, one can formally test\nfor the existence of a change point and estimate the time delay between the\nformal intervention implementation and the over-all-unit intervention effect.\nWe conducted empirical simulation studies to assess the type one error rate of\nthe testing procedure, power for detecting specified change-point alternatives,\nand accuracy of the proposed estimating methodology. R-MITS is illustrated by\nanalyzing patient satisfaction data from a hospital that implemented and\nevaluated a new care delivery model in multiple units.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 19:43:01 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 01:01:05 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Cruz", "Maricela", ""], ["Gillen", "Daniel L.", ""], ["Bender", "Miriam", ""], ["Ombao", "Hernando", ""]]}, {"id": "1805.07423", "submitter": "Mike Pereira", "authors": "Mike Pereira and Nicolas Desassis", "title": "Efficient simulation of Gaussian Markov random fields by Chebyshev\n  polynomial approximation", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": "10.1016/j.spasta.2019.100359", "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm to simulate Gaussian random vectors whose\nprecision matrix can be expressed as a polynomial of a sparse matrix. This\nsituation arises in particular when simulating Gaussian Markov random fields\nobtained by the discretization by finite elements of the solutions of some\nstochastic partial derivative equations. The proposed algorithm uses a\nChebyshev polynomial approximation to compute simulated vectors with a linear\ncomplexity. This method is asymptotically exact as the approximation order\ngrows. Criteria based on tests of the statistical properties of the produced\nvectors are derived to determine minimal orders of approximation.\n", "versions": [{"version": "v1", "created": "Thu, 17 May 2018 16:03:41 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 15:34:12 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Pereira", "Mike", ""], ["Desassis", "Nicolas", ""]]}, {"id": "1805.07575", "submitter": "Muhammad Naveed Tabassum", "authors": "Muhammad Naveed Tabassum and Esa Ollila", "title": "Sequential adaptive elastic net approach for single-snapshot source\n  localization", "comments": "12 pages, 5 figures, in the publication to the Journal of the\n  Acoustical Society of America", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.CV math.IT math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes efficient algorithms for accurate recovery of\ndirection-of-arrival (DoA) of sources from single-snapshot measurements using\ncompressed beamforming (CBF). In CBF, the conventional sensor array signal\nmodel is cast as an underdetermined complex-valued linear regression model and\nsparse signal recovery methods are used for solving the DoA finding problem. We\ndevelop a complex-valued pathwise weighted elastic net (c-PW-WEN) algorithm\nthat finds solutions at knots of penalty parameter values over a path (or grid)\nof EN tuning parameter values. c-PW-WEN also computes Lasso or weighted Lasso\nin its path. We then propose a sequential adaptive EN (SAEN) method that is\nbased on c-PW-WEN algorithm with adaptive weights that depend on the previous\nsolution. Extensive simulation studies illustrate that SAEN improves the\nprobability of exact recovery of true support compared to conventional sparse\nsignal recovery approaches such as Lasso, elastic net or orthogonal matching\npursuit in several challenging multiple target scenarios. The effectiveness of\nSAEN is more pronounced in the presence of high mutual coherence.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 11:57:54 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Tabassum", "Muhammad Naveed", ""], ["Ollila", "Esa", ""]]}, {"id": "1805.07580", "submitter": "Aleksey Polunchenko", "authors": "Aleksey S. Polunchenko and Andrey Pepelyshev", "title": "Analytic moment and Laplace transform formulae for the quasi-stationary\n  distribution of the Shiryaev diffusion on an interval", "comments": "24 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive analytic closed-form moment and Laplace transform formulae for the\nquasi-stationary distribution of the classical Shiryaev diffusion restricted to\nthe interval $[0,A]$ with absorption at a given $A>0$.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 12:17:51 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Polunchenko", "Aleksey S.", ""], ["Pepelyshev", "Andrey", ""]]}, {"id": "1805.07622", "submitter": "Vanda In\\'acio De Carvalho", "authors": "Vanda Inacio de Carvalho, Miguel de Carvalho, Adam Branscum", "title": "Bayesian Bootstrap Inference for the ROC Surface", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate diagnosis of disease is of great importance in clinical practice and\nmedical research. The receiver operating characteristic (ROC) surface is a\npopular tool for evaluating the discriminatory ability of continuous diagnostic\ntest outcomes when there exist three ordered disease classes (e.g., no disease,\nmild disease, advanced disease). We propose the Bayesian bootstrap, a fully\nnonparametric method, for conducting inference about the ROC surface and its\nfunctionals, such as the volume under the surface. The proposed method is based\non a simple, yet interesting, representation of the ROC surface in terms of\nplacement variables. Results from a simulation study demonstrate the ability of\nour method to successfully recover the true ROC surface and to produce valid\ninferences in a variety of complex scenarios. An application to data from the\nTrail Making Test to assess cognitive impairment in Parkinson's disease\npatients is provided.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 17:02:33 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["de Carvalho", "Vanda Inacio", ""], ["de Carvalho", "Miguel", ""], ["Branscum", "Adam", ""]]}, {"id": "1805.07656", "submitter": "Jennifer Starling", "authors": "Jennifer E. Starling, Jared S. Murray, Carlos M. Carvalho, Radek K.\n  Bukowski, and James G. Scott", "title": "BART with Targeted Smoothing: An analysis of patient-specific stillbirth\n  risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces BART with Targeted Smoothing, or tsBART, a new\nBayesian tree-based model for nonparametric regression. The goal of tsBART is\nto introduce smoothness over a single target covariate t, while not necessarily\nrequiring smoothness over other covariates x. TsBART is based on the Bayesian\nAdditive Regression Trees (BART) model, an ensemble of regression trees. TsBART\nextends BART by parameterizing each tree's terminal nodes with smooth functions\nof t, rather than independent scalars. Like BART, tsBART captures complex\nnonlinear relationships and interactions among the predictors. But unlike BART,\ntsBART guarantees that the response surface will be smooth in the target\ncovariate. This improves interpretability and helps regularize the estimate.\n  After introducing and benchmarking the tsBART model, we apply it to our\nmotivating example: pregnancy outcomes data from the National Center for Health\nStatistics. Our aim is to provide patient-specific estimates of stillbirth risk\nacross gestational age (t), based on maternal and fetal risk factors (x).\nObstetricians expect stillbirth risk to vary smoothly over gestational age, but\nnot necessarily over other covariates, and tsBART has been designed precisely\nto reflect this structural knowledge. The results of our analysis show the\nclear superiority of the tsBART model for quantifying stillbirth risk, thereby\nproviding patients and doctors with better information for managing the risk of\nperinatal mortality. All methods described here are implemented in the R\npackage tsbart.\n", "versions": [{"version": "v1", "created": "Sat, 19 May 2018 21:15:06 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 01:26:10 GMT"}, {"version": "v3", "created": "Fri, 22 Jun 2018 17:39:36 GMT"}, {"version": "v4", "created": "Fri, 17 Aug 2018 03:23:22 GMT"}, {"version": "v5", "created": "Fri, 14 Dec 2018 03:08:05 GMT"}, {"version": "v6", "created": "Wed, 2 Jan 2019 19:44:28 GMT"}, {"version": "v7", "created": "Mon, 3 Jun 2019 14:06:39 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Starling", "Jennifer E.", ""], ["Murray", "Jared S.", ""], ["Carvalho", "Carlos M.", ""], ["Bukowski", "Radek K.", ""], ["Scott", "James G.", ""]]}, {"id": "1805.07684", "submitter": "Sherri Rose", "authors": "Sherri Rose", "title": "Consistent Estimation of Propensity Score Functions with Oversampled\n  Exposed Subjects", "comments": "15 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observational cohort studies with oversampled exposed subjects are typically\nimplemented to understand the causal effect of a rare exposure. Because the\ndistribution of exposed subjects in the sample differs from the source\npopulation, estimation of a propensity score function (i.e., probability of\nexposure given baseline covariates) targets a nonparametrically nonidentifiable\nparameter. Consistent estimation of propensity score functions is an important\ncomponent of various causal inference estimators, including double robust\nmachine learning and inverse probability weighted estimators. This paper\ndevelops the use of the probability of exposure from the source population in a\nflexible computational implementation that can be used with any algorithm that\nallows observation weighting to produce consistent estimators of propensity\nscore functions. Simulation studies and a hypothetical health policy\nintervention data analysis demonstrate low empirical bias and variance for\nthese propensity score function estimators with observation weights in finite\nsamples.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 00:56:05 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 04:52:35 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Rose", "Sherri", ""]]}, {"id": "1805.07714", "submitter": "Debasis Kundu Professor", "authors": "Debasis Kundu", "title": "On a General Class of Discrete Bivariate Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a very general class of bivariate discrete\ndistributions. The basic idea is very simple. The marginals are obtained by\ntaking the random geometric sum of a baseline distribution function. The\nproposed class of distributions is a very flexible class of distributions in\nthe sense the marginals can take variety of shapes. It can be multimodal as\nwell as heavy tailed also. It can be both over dispersed as well as under\ndispersed. Moreover, the correlation can be of a wide range. We discuss\ndifferent properties of the proposes class of bivariate distributions. The\nproposed distribution has some interesting physical interpretations also.\nFurther, we consider two specific base line distributions namely; Poisson and\nnegative binomial distributions for illustrative purposes. Both of them are\ninfinitely divisible. The maximum likelihood estimators of the unknown\nparameters cannot be obtained in closed form. They can be obtained by solving\nthree and five dimensional non-linear optimizations problems, respectively. To\navoid that we propose to use the method of moment estimators and they can be\nobtained quite conveniently. The analyses of two real data sets have been\nperformed to show the effectiveness of the proposed class of models. Finally,\nwe discuss some open problems and conclude the paper.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 06:21:11 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Kundu", "Debasis", ""]]}, {"id": "1805.07800", "submitter": "Takumi Saegusa", "authors": "Takumi Saegusa", "title": "Large Sample Theory for Merged Data from Multiple Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop large sample theory for merged data from multiple sources. Main\nstatistical issues treated in this paper are (1) the same unit potentially\nappears in multiple datasets from overlapping data sources, (2) duplicated\nitems are not identified, and (3) a sample from the same data source is\ndependent due to sampling without replacement. We propose and study a new\nweighted empirical process and extend empirical process theory to a dependent\nand biased sample with duplication. Specifically, we establish the uniform law\nof large numbers and uniform central limit theorem over a class of functions\nalong with several empirical process results under conditions identical to\nthose in the i.i.d. setting. As applications, we study infinite-dimensional\nM-estimation and develop its consistency, rates of convergence, and asymptotic\nnormality. Our theoretical results are illustrated with simulation studies and\na real data example.\n", "versions": [{"version": "v1", "created": "Sun, 20 May 2018 17:18:15 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Saegusa", "Takumi", ""]]}, {"id": "1805.07970", "submitter": "Onur Teymur", "authors": "Onur Teymur, Han Cheng Lie, Tim Sullivan, Ben Calderhead", "title": "Implicit Probabilistic Integrators for ODEs", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018), Montr\\'eal, Canada", "journal-ref": "Advances in Neural Information Processing Systems 31 (2018) pp.\n  7244-7253", "doi": null, "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of implicit probabilistic integrators for initial value\nproblems (IVPs), taking as a starting point the multistep Adams-Moulton method.\nThe implicit construction allows for dynamic feedback from the forthcoming\ntime-step, in contrast to previous probabilistic integrators, all of which are\nbased on explicit methods. We begin with a concise survey of the\nrapidly-expanding field of probabilistic ODE solvers. We then introduce our\nmethod, which builds on and adapts the work of Conrad et al. (2016) and Teymur\net al. (2016), and provide a rigorous proof of its well-definedness and\nconvergence. We discuss the problem of the calibration of such integrators and\nsuggest one approach. We give an illustrative example highlighting the effect\nof the use of probabilistic integrators - including our new method - in the\nsetting of parameter inference within an inverse problem.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 10:17:31 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 13:33:40 GMT"}, {"version": "v3", "created": "Thu, 27 Dec 2018 14:34:57 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Teymur", "Onur", ""], ["Lie", "Han Cheng", ""], ["Sullivan", "Tim", ""], ["Calderhead", "Ben", ""]]}, {"id": "1805.07996", "submitter": "Marjan Cugmas", "authors": "Marjan Cugmas and Anu\\v{s}ka Ferligoj", "title": "Comparing Two Partitions of Non-Equal Sets of Units", "comments": null, "journal-ref": "Metodolo\\v{s}ki zvezki - Advances in Methodology and Statistics,\n  Vol. 15, No. 1, 2018, 1-21. Avaiable at\n  http://ibmi.mf.uni-lj.si/mz/2018/no-1/Cugmas2018.pdf", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rand (1971) proposed what has since become a well-known index for comparing\ntwo partitions obtained on the same set of units. The index takes a value on\nthe interval between 0 and 1, where a higher value indicates more similar\npartitions. Sometimes, e.g. when the units are observed in two time periods,\nthe splitting and merging of clusters should be considered differently,\naccording to the operationalization of the stability of clusters. The Rand\nIndex is symmetric in the sense that both the splitting and merging of clusters\nlower the value of the index. In such a non-symmetric case, one of the Wallace\nindexes (Wallace, 1983) can be used. Further, there are several cases when one\nwants to compare two partitions obtained on different sets of units, where the\nintersection of these sets of units is a non-empty set of units. In this\ninstance, the new units and units which leave the clusters from the first\npartition can be considered as a factor lowering the value of the index.\nTherefore, a modified Rand index is presented. Because the splitting and\nmerging of clusters have to be considered differently in some situations, an\nasymmetric modified Wallace Index is also proposed. For all presented indices,\nthe correction for chance is described, which allows different values of a\nselected index to be compared.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 11:38:23 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Cugmas", "Marjan", ""], ["Ferligoj", "Anu\u0161ka", ""]]}, {"id": "1805.08110", "submitter": "Francisco Javier Rubio", "authors": "Francisco J. Rubio, Laurent Remontet, Nicholas P. Jewell, Aur\\'elien\n  Belot", "title": "On a general structure for hazard-based regression models: an\n  application to population-based cancer research", "comments": "To appear in Statistical Methods in Medical Research. Supplementary\n  material and software available here:\n  https://sites.google.com/site/fjavierrubio67/home/papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proportional hazards model represents the most commonly assumed hazard\nstructure when analysing time to event data using regression models. We study a\ngeneral hazard structure which contains, as particular cases, proportional\nhazards, accelerated hazards, and accelerated failure time structures, as well\nas combinations of these. We propose an approach to apply these different\nhazard structures, based on a flexible parametric distribution (Exponentiated\nWeibull) for the baseline hazard. This distribution allows us to cover the\nbasic hazard shapes of interest in practice: constant, bathtub, increasing,\ndecreasing, and unimodal. In an extensive simulation study, we evaluate our\napproach in the context of excess hazard modelling, which is the main quantity\nof interest in descriptive cancer epidemiology. This study exhibits good\ninferential properties of the proposed model, as well as good performance when\nusing the Akaike Information Criterion for selecting the hazard structure. An\napplication on lung cancer data illustrates the usefulness of the proposed\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 15:06:50 GMT"}, {"version": "v2", "created": "Tue, 22 May 2018 22:22:09 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Rubio", "Francisco J.", ""], ["Remontet", "Laurent", ""], ["Jewell", "Nicholas P.", ""], ["Belot", "Aur\u00e9lien", ""]]}, {"id": "1805.08233", "submitter": "Michael Tzen", "authors": "Michael Tzen", "title": "Multilevel Models Allow Modular Specification of What and Where to\n  Regularize, Especially in Small Area Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Through the lense of multilevel model (MLM) specification and regularization,\nthis is a connect-the-dots introductory summary of Small Area Estimation, e.g.\nsmall group prediction informed by a complex sampling design. While a\ncomprehensive book is (Rao and Molina 2015), the goal of this paper is to get\ninterested researchers up to speed with some current developments. We first\nprovide historical context of two kinds of regularization: 1) the\nregularization 'within' the components of a predictor and 2) the regularization\n'between' outcome and predictor. We focus on the MLM framework as it allows the\nanalyst to flexibly control the targets of the regularization. The flexible\ncontrol is useful when analysts want to overcome shortcomings in design-based\nestimates. We'll describe the precision deficiencies (high variance) typical of\ndesign-based estimates of small groups. We then highlight an interesting MLM\nexample from (Chaudhuri and Ghosh 2011) that integrates both kinds of\nregularization (between and within). The key idea is to use the design-based\nvariance to control the amount of 'between' regularization and prior\ninformation to regularize the components 'within' a predictor. The goal is to\nlet the design-based estimate have authority (when precise) but defer to a\nmodel-based prediction when imprecise. We conclude by discussing optional\ncriteria to incorporate into a MLM prediction and possible entrypoints for\nextensions.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 18:04:53 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Tzen", "Michael", ""]]}, {"id": "1805.08275", "submitter": "Jorge Balat", "authors": "Jorge Balat, Sukjin Han", "title": "Multiple Treatments with Strategic Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an empirical framework to identify and estimate the effects of\ntreatments on outcomes of interest when the treatments are the result of\nstrategic interaction (e.g., bargaining, oligopolistic entry, peer effects). We\nconsider a model where agents play a discrete game with complete information\nwhose equilibrium actions (i.e., binary treatments) determine a post-game\noutcome in a nonseparable model with endogeneity. Due to the simultaneity in\nthe first stage, the model as a whole is incomplete and the selection process\nfails to exhibit the conventional monotonicity. Without imposing parametric\nrestrictions or large support assumptions, this poses challenges in recovering\ntreatment parameters. To address these challenges, we first establish a\nmonotonic pattern of the equilibria in the first-stage game in terms of the\nnumber of treatments selected. Based on this finding, we derive bounds on the\naverage treatment effects (ATEs) under nonparametric shape restrictions and the\nexistence of excluded exogenous variables. We show that instrument variation\nthat compensates strategic substitution helps solve the multiple equilibria\nproblem. We apply our method to data on airlines and air pollution in cities in\nthe U.S. We find that (i) the causal effect of each airline on pollution is\npositive, and (ii) the effect is increasing in the number of firms but at a\ndecreasing rate.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 20:05:36 GMT"}, {"version": "v2", "created": "Mon, 2 Sep 2019 22:19:55 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Balat", "Jorge", ""], ["Han", "Sukjin", ""]]}, {"id": "1805.08300", "submitter": "David Tyler", "authors": "David E. Tyler and Mengxi Yi", "title": "Lassoing Eigenvalues", "comments": "18 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The properties of penalized sample covariance matrices depend on the choice\nof the penalty function. In this paper, we introduce a class of non-smooth\npenalty functions for the sample covariance matrix, and demonstrate how this\nmethod results in a grouping of the estimated eigenvalues. We refer to this\nmethod as \"lassoing eigenvalues\" or as the \"elasso\".\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 21:49:26 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Tyler", "David E.", ""], ["Yi", "Mengxi", ""]]}, {"id": "1805.08304", "submitter": "Deborah Kunkel", "authors": "Deborah Kunkel and Mario Peruggia", "title": "Anchored Bayesian Gaussian Mixture Models", "comments": "65 pages, 11 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixtures are a flexible modeling tool for irregularly shaped densities\nand samples from heterogeneous populations. When modeling with mixtures using\nan exchangeable prior on the component features, the component labels are\narbitrary and are indistinguishable in posterior analysis. This makes it\nimpossible to attribute any meaningful interpretation to the marginal posterior\ndistributions of the component features. We propose a model in which a small\nnumber of observations are assumed to arise from some of the labeled component\ndensities. The resulting model is not exchangeable, allowing inference on the\ncomponent features without post-processing. Our method assigns meaning to the\ncomponent labels at the modeling stage and can be justified as a data-dependent\ninformative prior on the labelings. We show that our method produces\ninterpretable results, often (but not always) similar to those resulting from\nrelabeling algorithms, with the added benefit that the marginal inferences\noriginate directly from a well specified probability model rather than a post\nhoc manipulation. We provide asymptotic results leading to practical guidelines\nfor model selection that are motivated by maximizing prior information about\nthe class labels and demonstrate our method on real and simulated data.\n", "versions": [{"version": "v1", "created": "Mon, 21 May 2018 22:00:58 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 20:38:50 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2019 20:43:11 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 19:04:41 GMT"}], "update_date": "2020-07-10", "authors_parsed": [["Kunkel", "Deborah", ""], ["Peruggia", "Mario", ""]]}, {"id": "1805.08342", "submitter": "Jongha Jon Ryu", "authors": "J. Jon Ryu, Shouvik Ganguly, Young-Han Kim, Yung-Kyun Noh, Daniel D.\n  Lee", "title": "Nearest neighbor density functional estimation from inverse Laplace\n  transform", "comments": "52 pages, 4 figures. Submitted to the IEEE Transactions on\n  Information Theory. Minor fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to $L_2$-consistent estimation of a general density functional\nusing $k$-nearest neighbor distances is proposed, where the functional under\nconsideration is in the form of the expectation of some function $f$ of the\ndensities at each point. The estimator is designed to be asymptotically\nunbiased, using the convergence of the normalized volume of a $k$-nearest\nneighbor ball to a Gamma distribution in the large-sample limit, and naturally\ninvolves the inverse Laplace transform of a scaled version of the function $f.$\nSome instantiations of the proposed estimator recover existing $k$-nearest\nneighbor based estimators of Shannon and R\\'enyi entropies and\nKullback--Leibler and R\\'enyi divergences, and discover new consistent\nestimators for many other functionals such as logarithmic entropies and\ndivergences. The $L_2$-consistency of the proposed estimator is established for\na broad class of densities for general functionals, and the convergence rate in\nmean squared error is established as a function of the sample size for smooth,\nbounded densities.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 01:19:30 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 06:18:22 GMT"}, {"version": "v3", "created": "Fri, 14 Aug 2020 00:05:06 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Ryu", "J. Jon", ""], ["Ganguly", "Shouvik", ""], ["Kim", "Young-Han", ""], ["Noh", "Yung-Kyun", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1805.08423", "submitter": "Matt Wand Professor", "authors": "P. Hall, I.M. Johnstone, J.T. Ormerod, M.P. Wand and J.C.F. Yu", "title": "Fast and Accurate Binary Response Mixed Model Analysis via Expectation\n  Propagation", "comments": "35 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation propagation is a general prescription for approximation of\nintegrals in statistical inference problems. Its literature is mainly concerned\nwith Bayesian inference scenarios. However, expectation propagation can also be\nused to approximate integrals arising in frequentist statistical inference. We\nfocus on likelihood-based inference for binary response mixed models and show\nthat fast and accurate quadrature-free inference can be realized for the probit\nlink case with multivariate random effects and higher levels of nesting. The\napproach is supported by asymptotic theory in which expectation propagation is\nseen to provide consistent estimation of the exact likelihood surface.\nNumerical studies reveal the availability of fast, highly accurate and scalable\nmethodology for binary mixed model analysis.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 06:59:34 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Hall", "P.", ""], ["Johnstone", "I. M.", ""], ["Ormerod", "J. T.", ""], ["Wand", "M. P.", ""], ["Yu", "J. C. F.", ""]]}, {"id": "1805.08463", "submitter": "Ho Chung Leon Law", "authors": "Ho Chung Leon Law, Dino Sejdinovic, Ewan Cameron, Tim CD Lucas, Seth\n  Flaxman, Katherine Battle, Kenji Fukumizu", "title": "Variational Learning on Aggregate Outputs with Gaussian Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a typical supervised learning framework assumes that the inputs and the\noutputs are measured at the same levels of granularity, many applications,\nincluding global mapping of disease, only have access to outputs at a much\ncoarser level than that of the inputs. Aggregation of outputs makes\ngeneralization to new inputs much more difficult. We consider an approach to\nthis problem based on variational learning with a model of output aggregation\nand Gaussian processes, where aggregation leads to intractability of the\nstandard evidence lower bounds. We propose new bounds and tractable\napproximations, leading to improved prediction accuracy and scalability to\nlarge datasets, while explicitly taking uncertainty into account. We develop a\nframework which extends to several types of likelihoods, including the Poisson\nmodel for aggregated count data. We apply our framework to a challenging and\nimportant problem, the fine-scale spatial modelling of malaria incidence, with\nover 1 million observations.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 09:08:01 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Law", "Ho Chung Leon", ""], ["Sejdinovic", "Dino", ""], ["Cameron", "Ewan", ""], ["Lucas", "Tim CD", ""], ["Flaxman", "Seth", ""], ["Battle", "Katherine", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1805.08512", "submitter": "Olga Gorskikh", "authors": "Pekka Malo, Lauri Viitasaari, Olga Gorskikh, Pauliina Ilmonen", "title": "Non-parametric Structural Change Detection in Multivariate Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural change detection problems are often encountered in analytics and\neconometrics, where the performance of a model can be significantly affected by\nunforeseen changes in the underlying relationships. Although these problems\nhave a comparatively long history in statistics, the number of studies done in\nthe context of multivariate data under nonparametric settings is still small.\nIn this paper, we propose a consistent method for detecting multiple structural\nchanges in a system of related regressions over a large dimensional variable\nspace. In most applications, practitioners also do not have a priori\ninformation on the relevance of different variables, and therefore, both\nlocations of structural changes as well as the corresponding sparse regression\ncoefficients need to be estimated simultaneously. The method combines\nnonparametric energy distance minimization principle with penalized regression\ntechniques. After showing asymptotic consistency of the model, we compare the\nproposed approach with competing methods in a simulation study. As an example\nof a large scale application, we consider structural change point detection in\nthe context of news analytics during the recent financial crisis period.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 11:40:32 GMT"}, {"version": "v2", "created": "Tue, 28 May 2019 07:17:10 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Malo", "Pekka", ""], ["Viitasaari", "Lauri", ""], ["Gorskikh", "Olga", ""], ["Ilmonen", "Pauliina", ""]]}, {"id": "1805.08518", "submitter": "Justin Petrovich", "authors": "Justin Petrovich, Matthew Reimherr, and Carrie Daymont", "title": "Highly Irregular Functional Generalized Linear Regression with\n  Electronic Health Records", "comments": "5 figures, 17 tables (including supplementary material), 34 pages\n  (including supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a new approach, called MISFIT, for fitting generalized\nfunctional linear regression models with sparsely and irregularly sampled data.\nCurrent methods do not allow for consistent estimation unless one assumes that\nthe number of observed points per curve grows sufficiently quickly with the\nsample size. In contrast, MISFIT is based on a multiple imputation framework,\nwhich has the potential to produce consistent estimates without such an\nassumption. Just as importantly, it propagates the uncertainty of not having\ncompletely observed curves, allowing for a more accurate assessment of the\nuncertainty of parameter estimates, something that most methods currently\ncannot accomplish. This work is motivated by a longitudinal study on\nmacrocephaly, or atypically large head size, in which electronic medical\nrecords allow for the collection of a great deal of data. However, the sampling\nis highly variable from child to child. Using MISFIT we are able to clearly\ndemonstrate that the development of pathologic conditions related to\nmacrocephaly is associated with both the overall head circumference of the\nchildren as well as the velocity of their head growth.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 11:46:01 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 18:40:57 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Petrovich", "Justin", ""], ["Reimherr", "Matthew", ""], ["Daymont", "Carrie", ""]]}, {"id": "1805.08670", "submitter": "Colman Humphrey", "authors": "Colman Humphrey, Dan Swingley", "title": "Regression Analysis of Proportion Outcomes with Random Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A regression method for proportional, or fractional, data with mixed effects\nis outlined, designed for analysis of datasets in which the outcomes have\nsubstantial weight at the bounds. In such cases a normal approximation is\nparticularly unsuitable as it can result in incorrect inference. To resolve\nthis problem, we employ a logistic regression model and then apply a bootstrap\nmethod to correct conservative confidence intervals. This paper outlines the\ntheory of the method, and demonstrates its utility using simulated data.\nWorking code for the R platform is provided through the package glmmboot,\navailable on CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 15:44:08 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Humphrey", "Colman", ""], ["Swingley", "Dan", ""]]}, {"id": "1805.08719", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou", "title": "Parsimonious Bayesian deep networks", "comments": "NeurIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining Bayesian nonparametrics and a forward model selection strategy, we\nconstruct parsimonious Bayesian deep networks (PBDNs) that infer\ncapacity-regularized network architectures from the data and require neither\ncross-validation nor fine-tuning when training the model. One of the two\nessential components of a PBDN is the development of a special infinite-wide\nsingle-hidden-layer neural network, whose number of active hidden units can be\ninferred from the data. The other one is the construction of a greedy\nlayer-wise learning algorithm that uses a forward model selection criterion to\ndetermine when to stop adding another hidden layer. We develop both Gibbs\nsampling and stochastic gradient descent based maximum a posteriori inference\nfor PBDNs, providing state-of-the-art classification accuracy and interpretable\ndata subtypes near the decision boundaries, while maintaining low computational\ncomplexity for out-of-sample prediction.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 16:26:42 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 15:11:43 GMT"}, {"version": "v3", "created": "Tue, 1 Jan 2019 00:45:05 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Zhou", "Mingyuan", ""]]}, {"id": "1805.08765", "submitter": "Jos\\'e-Miguel Ponciano PhD", "authors": "Jose-Miguel Ponciano and Mark L Taper", "title": "Multi-model inference through projections in model space", "comments": "31 pages, 8 figures. Submitted to JRSSB", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Information criteria have had a profound impact on modern ecological science.\nThey allow researchers to estimate which probabilistic approximating models are\nclosest to the generating process. Unfortunately, information criterion\ncomparison does not tell how good the best model is. Nor do practitioners\nroutinely test the reliability (e.g. error rates) of information\ncriterion-based model selection. In this work, we show that these two\nshortcomings can be resolved by extending a key observation from Hirotugu\nAkaike's original work. Standard information criterion analysis considers only\nthe divergences of each model from the generating process. It is ignored that\nthere are also estimable divergence relationships amongst all of the\napproximating models. We then show that using both sets of divergences, a model\nspace can be constructed that includes an estimated location for the generating\nprocess. Thus, not only can an analyst determine which model is closest to the\ngenerating process, she/he can also determine how close to the generating\nprocess the best approximating model is. Properties of the generating process\nestimated from these projections are more accurate than those estimated by\nmodel averaging. The applications of our findings extend to all areas of\nscience where model selection through information criteria is done.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 17:50:45 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Ponciano", "Jose-Miguel", ""], ["Taper", "Mark L", ""]]}, {"id": "1805.08863", "submitter": "Charles Matthews", "authors": "Charles Matthews and Jonathan Weare", "title": "Langevin Markov Chain Monte Carlo with stochastic gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo sampling techniques have broad applications in machine learning,\nBayesian posterior inference, and parameter estimation. Often the target\ndistribution takes the form of a product distribution over a dataset with a\nlarge number of entries. For sampling schemes utilizing gradient information it\nis cheaper for the derivative to be approximated using a random small subset of\nthe data, introducing extra noise into the system. We present a new\ndiscretization scheme for underdamped Langevin dynamics when utilizing a\nstochastic (noisy) gradient. This scheme is shown to bias computed averages to\nsecond order in the stepsize while giving exact results in the special case of\nsampling a Gaussian distribution with a normally distributed stochastic\ngradient.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 20:54:44 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 22:22:57 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Matthews", "Charles", ""], ["Weare", "Jonathan", ""]]}, {"id": "1805.08883", "submitter": "Yaroslav Mukhin", "authors": "Yaroslav Mukhin", "title": "Sensitivity of Regular Estimators", "comments": "35 pages, 5 figures, includes appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies local asymptotic relationship between two scalar\nestimates. We define sensitivity of a target estimate to a control estimate to\nbe the directional derivative of the target functional with respect to the\ngradient direction of the control functional. Sensitivity according to the\ninformation metric on the model manifold is the asymptotic covariance of\nregular efficient estimators. Sensitivity according to a general policy metric\non the model manifold can be obtained from influence functions of regular\nefficient estimators. Policy sensitivity has a local counterfactual\ninterpretation, where the ceteris paribus change to a counterfactual\ndistribution is specified by the combination of a control parameter and a\nRiemannian metric on the model manifold.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 21:57:42 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Mukhin", "Yaroslav", ""]]}, {"id": "1805.08931", "submitter": "Roel Ceballos", "authors": "Roel F. Ceballos, Fe F. Largo", "title": "On The Estimation of the Hurst Exponent Using Adjusted Rescaled Range\n  Analysis, Detrended Fluctuation Analysis and Variance Time Plot: A Case of\n  Exponential Distribution", "comments": null, "journal-ref": "Imperial Journal of Interdisciplinary Research 2017 (Volume 3,\n  Issue 8, pp. 424-434)", "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hurst Exponent has been widely used in different fields as a measure of long\nrange dependence in time series. It has been studied in hydrology and\ngeophysics, economics and finance, and recently, it is still a hot topic in the\ndifferent areas of research involving DNA sequences, cardiac dynamics, internet\ntraffic, meteorology and geology. Various methods in the estimation of Hurst\nExponent have been proposed such as Adjusted Rescaled Range Analysis, Detrended\nFluctuation Analysis and Variance Time Plot Analysis. This study explored the\nefficiency of the three methods: Adjusted Rescaled Range Analysis, Detrended\nFluctuation Analysis and Variance Time Plot Analysis in the estimation of Hurst\nExponent when data are generated from an exponential distribution. In addition,\nthe efficiency of the three methods was compared in different sample sizes of\n128, 256, 512, 1024 and varying {\\lambda} parameter values of 0.1, 0.5, 1.5,\n3.0, 5.0 and 7.0. The estimation process for each of the methods using\ndifferent sample sizes and {\\lambda} parameter values were repeated for 100,\n500 and 1000 times to verify the consistency of the result. A Scilab Program\ncontaining different functions was developed for the study to aid in the\nsimulation process and calculation. The Adjusted Rescaled Range Analysis was\nthe most efficient method with the smallest Mean Square Error for all {\\lambda}\nparameter values and different sample sizes.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 01:48:00 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Ceballos", "Roel F.", ""], ["Largo", "Fe F.", ""]]}, {"id": "1805.09091", "submitter": "Sebastian Lerch", "authors": "Stephan Rasp and Sebastian Lerch", "title": "Neural networks for post-processing ensemble weather forecasts", "comments": null, "journal-ref": "Monthly Weather Review 2018, 146, 3885-3900", "doi": "10.1175/MWR-D-18-0187.1", "report-no": null, "categories": "stat.ML cs.LG physics.ao-ph stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble weather predictions require statistical post-processing of\nsystematic errors to obtain reliable and accurate probabilistic forecasts.\nTraditionally, this is accomplished with distributional regression models in\nwhich the parameters of a predictive distribution are estimated from a training\nperiod. We propose a flexible alternative based on neural networks that can\nincorporate nonlinear relationships between arbitrary predictor variables and\nforecast distribution parameters that are automatically learned in a\ndata-driven way rather than requiring pre-specified link functions. In a case\nstudy of 2-meter temperature forecasts at surface stations in Germany, the\nneural network approach significantly outperforms benchmark post-processing\nmethods while being computationally more affordable. Key components to this\nimprovement are the use of auxiliary predictor variables and station-specific\ninformation with the help of embeddings. Furthermore, the trained neural\nnetwork can be used to gain insight into the importance of meteorological\nvariables thereby challenging the notion of neural networks as uninterpretable\nblack boxes. Our approach can easily be extended to other statistical\npost-processing and forecasting problems. We anticipate that recent advances in\ndeep learning combined with the ever-increasing amounts of model and\nobservation data will transform the post-processing of numerical weather\nforecasts in the coming decade.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 12:30:28 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Rasp", "Stephan", ""], ["Lerch", "Sebastian", ""]]}, {"id": "1805.09175", "submitter": "Armin Rauschenberger", "authors": "Armin Rauschenberger, Renee X. Menezes, Mark A. van de Wiel, Natasja\n  M. van Schoor, and Marianne A. Jonker", "title": "Detecting SNPs with interactive effects on a quantitative trait", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we propose a test to detect effects of single nucleotide polymorphisms\n(SNPs) on a quantitative trait. Significant SNP-SNP interactions are more\ndifficult to detect than significant SNPs, partly due to the massive amount of\nSNP-SNP combinations. We propose to move away from testing interaction terms,\nand move towards testing whether an individual SNP is involved in any\ninteraction. This reduces the multiple testing burden to one test per SNP, and\nallows for interactions with unobserved factors. Analysing one SNP at a time,\nwe split the individuals into two groups, based on the number of minor alleles.\nIf the quantitative trait differs in mean between the two groups, the SNP has a\nmain effect. If the quantitative trait differs in distribution between some\nindividuals in one group and all other individuals, it possibly has an\ninteractive effect. We propose a mixture test to detect both types of effects.\nImplicitly, the membership probabilities may suggest potential interacting\nvariables. Analysing simulated and experimental data, we show that the proposed\ntest is statistically powerful, maintains the type I error rate, and detects\nmeaningful signals. The R package semisup is available from Bioconductor.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 13:51:43 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Rauschenberger", "Armin", ""], ["Menezes", "Renee X.", ""], ["van de Wiel", "Mark A.", ""], ["van Schoor", "Natasja M.", ""], ["Jonker", "Marianne A.", ""]]}, {"id": "1805.09392", "submitter": "Joshua Snoke", "authors": "Joshua Snoke and Aleksandra Slavkovi\\'c", "title": "pMSE Mechanism: Differentially Private Synthetic Data with Maximal\n  Distributional Similarity", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for the release of differentially private synthetic\ndatasets. In many contexts, data contain sensitive values which cannot be\nreleased in their original form in order to protect individuals' privacy.\nSynthetic data is a protection method that releases alternative values in place\nof the original ones, and differential privacy (DP) is a formal guarantee for\nquantifying the privacy loss. We propose a method that maximizes the\ndistributional similarity of the synthetic data relative to the original data\nusing a measure known as the pMSE, while guaranteeing epsilon-differential\nprivacy. Additionally, we relax common DP assumptions concerning the\ndistribution and boundedness of the original data. We prove theoretical results\nfor the privacy guarantee and provide simulations for the empirical failure\nrate of the theoretical results under typical computational limitations. We\nalso give simulations for the accuracy of linear regression coefficients\ngenerated from the synthetic data compared with the accuracy of\nnon-differentially private synthetic data and other differentially private\nmethods. Additionally, our theoretical results extend a prior result for the\nsensitivity of the Gini Index to include continuous predictors.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 19:23:04 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Snoke", "Joshua", ""], ["Slavkovi\u0107", "Aleksandra", ""]]}, {"id": "1805.09397", "submitter": "Sukjin Han", "authors": "Sukjin Han", "title": "Identification in Nonparametric Models for Dynamic Treatment Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a nonparametric model that represents how sequences of\noutcomes and treatment choices influence one another in a dynamic manner. In\nthis setting, we are interested in identifying the average outcome for\nindividuals in each period, had a particular treatment sequence been assigned.\nThe identification of this quantity allows us to identify the average treatment\neffects (ATE's) and the ATE's on transitions, as well as the optimal treatment\nregimes, namely, the regimes that maximize the (weighted) sum of the average\npotential outcomes, possibly less the cost of the treatments. The main\ncontribution of this paper is to relax the sequential randomization assumption\nwidely used in the biostatistics literature by introducing a flexible\nchoice-theoretic framework for a sequence of endogenous treatments. We show\nthat the parameters of interest are identified under each period's two-way\nexclusion restriction, i.e., with instruments excluded from the\noutcome-determining process and other exogenous variables excluded from the\ntreatment-selection process. We also consider partial identification in the\ncase where the latter variables are not available. Lastly, we extend our\nresults to a setting where treatments do not appear in every period.\n", "versions": [{"version": "v1", "created": "Wed, 23 May 2018 19:37:47 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 15:12:50 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 21:48:34 GMT"}], "update_date": "2019-01-16", "authors_parsed": [["Han", "Sukjin", ""]]}, {"id": "1805.09460", "submitter": "Yotam Hechtlinger", "authors": "Yotam Hechtlinger, Barnab\\'as P\\'oczos and Larry Wasserman", "title": "Cautious Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most classifiers operate by selecting the maximum of an estimate of the\nconditional distribution $p(y|x)$ where $x$ stands for the features of the\ninstance to be classified and $y$ denotes its label. This often results in a\n{\\em hubristic bias}: overconfidence in the assignment of a definite label.\nUsually, the observations are concentrated on a small volume but the classifier\nprovides definite predictions for the entire space. We propose constructing\nconformal prediction sets which contain a set of labels rather than a single\nlabel. These conformal prediction sets contain the true label with probability\n$1-\\alpha$. Our construction is based on $p(x|y)$ rather than $p(y|x)$ which\nresults in a classifier that is very cautious: it outputs the null set ---\nmeaning \"I don't know\" --- when the object does not resemble the training\nexamples. An important property of our approach is that adversarial attacks are\nlikely to be predicted as the null set or would also include the true label. We\ndemonstrate the performance on the ImageNet ILSVRC dataset and the CelebA and\nIMDB-Wiki facial datasets using high dimensional features obtained from state\nof the art convolutional neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 00:17:24 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 19:27:58 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Hechtlinger", "Yotam", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Wasserman", "Larry", ""]]}, {"id": "1805.09468", "submitter": "Abdolnasser Sadeghkhani", "authors": "Abdolnasser Sadeghkhani", "title": "Bayesian predictive densities as an interpretation of a class of\n  Skew--Student $t$ distributions with application to medical data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new Bayesian interpretation of a class of\nskew--Student $t$ distributions. We consider a hierarchical normal model with\nunknown covariance matrix and show that by imposing different restrictions on\nthe parameter space, corresponding Bayes predictive density estimators under\nKullback-Leibler loss function embrace some well-known skew--Student $t$\ndistributions. We show that obtained estimators perform better in terms of\nfrequentist risk function over regular Bayes predictive density estimators. We\napply our proposed methods to estimate future densities of medical data: the\nleg-length discrepancy and effect of exercise on the age at which a child\nstarts to walk.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 01:05:14 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Sadeghkhani", "Abdolnasser", ""]]}, {"id": "1805.09505", "submitter": "Ranjan Maitra", "authors": "Israel Almod\\'ovar-Rivera and Ranjan Maitra", "title": "Kernel-estimated Nonparametric Overlap-Based Syncytial Clustering", "comments": "32 pages, 22 figures, 9 tables: published in JMLR at:\n  http://jmlr.org/papers/v21/18-435.html", "journal-ref": "Journal of Machine Learning Research 21:122, 1-54 (2020)", "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonly-used clustering algorithms usually find ellipsoidal, spherical or\nother regular-structured clusters, but are more challenged when the underlying\ngroups lack formal structure or definition. Syncytial clustering is the name\nthat we introduce for methods that merge groups obtained from standard\nclustering algorithms in order to reveal complex group structure in the data.\nHere, we develop a distribution-free fully-automated syncytial clustering\nalgorithm that can be used with $k$-means and other algorithms. Our approach\nestimates the cumulative distribution function of the normed residuals from an\nappropriately fit $k$-groups model and calculates the estimated nonparametric\noverlap between each pair of clusters. Groups with high pairwise overlap are\nmerged as long as the estimated generalized overlap decreases. Our methodology\nis always a top performer in identifying groups with regular and irregular\nstructures in several datasets and can be applied to datasets with scatter or\nincomplete records. The approach is also used to identify the distinct kinds of\ngamma ray bursts in the Burst and Transient Source Experiment 4Br catalog and\nthe distinct kinds of activation in a functional Magnetic Resonance Imaging\nstudy.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 04:50:10 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 01:40:05 GMT"}, {"version": "v3", "created": "Tue, 28 Jan 2020 02:38:06 GMT"}, {"version": "v4", "created": "Sat, 9 May 2020 17:10:38 GMT"}, {"version": "v5", "created": "Tue, 28 Jul 2020 18:09:53 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Almod\u00f3var-Rivera", "Israel", ""], ["Maitra", "Ranjan", ""]]}, {"id": "1805.09579", "submitter": "Ross Towe", "authors": "Ross Towe, Jonathan Tawn, Rob Lamb and Chris Sherlock", "title": "Model-based inference of conditional extreme value distributions with\n  hydrological applications", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": "10.1002/env.2575", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate extreme value models are used to estimate joint risk in a number\nof applications, with a particular focus on environmental fields ranging from\nclimatology and hydrology to oceanography and seismic hazards. The\nsemi-parametric conditional extreme value model of Heffernan and Tawn (2004)\ninvolving a multivariate regression provides the most suitable of current\nstatistical models in terms of its flexibility to handle a range of extremal\ndependence classes. However, the standard inference for the joint distribution\nof the residuals of this model suffers from the curse of dimensionality since\nin a $d$-dimensional application it involves a $d-1$-dimensional non-parametric\ndensity estimator, which requires, for accuracy, a number points and\ncommensurate effort that is exponential in $d$. Furthermore, it does not allow\nfor any partially missing observations to be included and a previous proposal\nto address this is extremely computationally intensive, making its use\nprohibitive if the proportion of missing data is non-trivial. We propose to\nreplace the $d-1$-dimensional non-parametric density estimator with a\nmodel-based copula with univariate marginal densities estimated using kernel\nmethods. This approach provides statistically and computationally efficient\nestimates whatever the dimension, $d$ or the degree of missing data. Evidence\nis presented to show that the benefits of this approach substantially outweigh\npotential mis-specification errors. The methods are illustrated through the\nanalysis of UK river flow data at a network of 46 sites and assessing the\nrarity of the 2015 floods in north west England.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 09:51:53 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2019 13:25:04 GMT"}], "update_date": "2019-08-08", "authors_parsed": [["Towe", "Ross", ""], ["Tawn", "Jonathan", ""], ["Lamb", "Rob", ""], ["Sherlock", "Chris", ""]]}, {"id": "1805.09674", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "A D-vine copula mixed model for joint meta-analysis and comparison of\n  diagnostic tests", "comments": "arXiv admin note: substantial text overlap with arXiv:1506.03920,\n  arXiv:1502.07505", "journal-ref": "Statistical Methods in Medical Research, 2019, 28 (10-11),\n  3286-3300", "doi": "10.1177/0962280218796685", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a particular disease there may be two diagnostic tests developed, where\neach of the tests is subject to several studies. A quadrivariate generalized\nlinear mixed model (GLMM) has been recently proposed to joint meta-analyse and\ncompare two diagnostic tests. We propose a D-vine copula mixed model for joint\nmeta-analysis and comparison of two diagnostic tests. Our general model\nincludes the quadrivariate GLMM as a special case and can also operate on the\noriginal scale of sensitivities and specificities. The method allows the direct\ncalculation of sensitivity and specificity for each test, as well as, the\nparameters of the summary receiver operator characteristic (SROC) curve, along\nwith a comparison between the SROCs of each test. Our methodology is\ndemonstrated with an extensive simulation study and illustrated by\nmeta-analysing two examples where 2 tests for the diagnosis of a particular\ndisease are compared. Our study suggests that there can be an improvement on\nGLMM in fit to data since our model can also provide tail dependencies and\nasymmetries.\n", "versions": [{"version": "v1", "created": "Tue, 22 May 2018 19:49:41 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 17:15:43 GMT"}], "update_date": "2019-12-03", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1805.09700", "submitter": "Jozef Jakubik", "authors": "Jozef Jakubik", "title": "Convex method for selection of fixed effects in high-dimensional linear\n  mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of high-dimensional data is currently a popular field of research,\nthanks to many applications e.g. in genetics (DNA data in genomewide\nassociation studies), spectrometry or web analysis. At the same time, the type\nof problems that tend to arise in genetics can often be modelled using linear\nmixed models in conjunction with high-dimensional data because linear mixed\nmodels allow us to specify the covariance structure of the models. This enables\nus to capture relationships in data such as the population structure, family\nrelatedness, etc. In this paper we introduce two new convex methods for\nvariable selection in high-dimensional linear mixed models which, thanks to\nconvexity, can handle many more variables than existing non-convex methods.\nBoth methods are compared with existing methods and in the end we suggest an\napproach for a wider class of linear mixed models.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 14:41:03 GMT"}], "update_date": "2018-05-25", "authors_parsed": [["Jakubik", "Jozef", ""]]}, {"id": "1805.09736", "submitter": "Rachel Nethery", "authors": "Rachel C. Nethery, Fabrizia Mealli, and Francesca Dominici", "title": "Estimating Population Average Causal Effects in the Presence of\n  Non-Overlap: The Effect of Natural Gas Compressor Station Exposure on Cancer\n  Mortality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most causal inference studies rely on the assumption of overlap to estimate\npopulation or sample average causal effects. When data exhibit non-overlap,\nestimation of these estimands requires reliance on model specifications, due to\npoor data support. All existing methods to address non-overlap, such as\ntrimming or down-weighting data in regions of poor support, change the\nestimand. In environmental health research, where study results are often\nintended to influence policy, changes in the estimand can diminish the study's\nimpact, because estimates may not be representative of effects in the\npopulation of interest to policymakers. Researchers may be willing to make\nadditional, minimal modeling assumptions in order to preserve the ability to\nestimate population average causal effects. We seek to make two contributions\non this topic. First, we propose a flexible, data-driven definition of\npropensity score overlap and non-overlap regions. Second, we develop a novel\nBayesian framework to estimate population average causal effects with minor\nmodel dependence and appropriately large uncertainties in the presence of\nnon-overlap. In this approach, the tasks of estimating causal effects in the\noverlap and non-overlap regions are delegated to two distinct models, suited to\nthe degree of data support in each region. Tree ensembles are used to\nnon-parametrically estimate individual causal effects in the overlap region,\nwhere the data can speak for themselves. In the non-overlap region, where\ninsufficient data support means reliance on model specification is necessary,\nindividual causal effects are estimated by extrapolating trends from the\noverlap region via a spline model. The promising performance of our method is\ndemonstrated in simulations. Finally, we utilize our method to perform a novel\ninvestigation of the causal effect of natural gas compressor station exposure\non cancer outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 15:42:39 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 21:24:41 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Nethery", "Rachel C.", ""], ["Mealli", "Fabrizia", ""], ["Dominici", "Francesca", ""]]}, {"id": "1805.09838", "submitter": "Sasha Shirman", "authors": "Sasha Shirman, Henry D. I. Abarbanel", "title": "Strategic Monte Carlo Methods for State and Parameter Estimation in High\n  Dimensional Nonlinear Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical data assimilation one seeks the largest maximum of the\nconditional probability distribution $P(\\mathbf{X},\\mathbf{p}|\\mathbf{Y})$ of\nmodel states, $\\mathbf{X}$, and parameters,$\\mathbf{p}$, conditioned on\nobservations $\\mathbf{Y}$ through minimizing the `action', $A(\\mathbf{X}) =\n-\\log P(\\mathbf{X},\\mathbf{p}|\\mathbf{Y})$. This determines the dominant\ncontribution to the expected values of functions of $\\mathbf{X}$ but does not\ngive information about the structure of $P(\\mathbf{X},\\mathbf{p}|\\mathbf{Y})$\naway from the maximum. We introduce a Monte Carlo sampling method, called\nStrategic Monte Carlo (SMC) sampling, for estimating $P(\\mathbf{X},\n\\mathbf{p}|\\mathbf{Y})$ in the neighborhood of its largest maximum to remedy\nthis limitation. SMC begins with a systematic variational annealing (VA)\nprocedure for finding the smallest minimum of $A(\\mathbf{X})$. SMC generates\naccurate estimates for the mean, standard deviation and other higher moments of\n$P(\\mathbf{X},\\mathbf{p}|\\mathbf{Y})$. Additionally, the random search allows\nfor an understanding of any multimodal structure that may underly the dynamics\nof the problem. SMC generates a gaussian probability control term based on the\npaths determined by VA to minimize a cost function $A(\\mathbf{X},\\mathbf{p})$.\nThis probability is sampled during the Monte Carlo search of the cost function\nto constrain the search to high probability regions of the surface thus\nsubstantially reducing the time necessary to sufficiently explore the space.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 18:08:58 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Shirman", "Sasha", ""], ["Abarbanel", "Henry D. I.", ""]]}, {"id": "1805.09840", "submitter": "Pariya Behrouzi", "authors": "Pariya Behrouzi, Fentaw Abegaz, and Ernst C. Wit", "title": "Dynamic Chain Graph Models for Ordinal Time Series Data", "comments": "19 pages, 2 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces sparse dynamic chain graph models for network inference\nin high dimensional non-Gaussian time series data. The proposed method\nparametrized by a precision matrix that encodes the intra time-slice\nconditional independence among variables at a fixed time point, and an\nautoregressive coefficient that contains dynamic conditional independences\ninteractions among time series components across consecutive time steps. The\nproposed model is a Gaussian copula vector autoregressive model, which is used\nto model sparse interactions in a high-dimensional setting. Estimation is\nachieved via a penalized EM algorithm. In this paper, we use an efficient\ncoordinate descent algorithm to optimize the penalized log-likelihood with the\nsmoothly clipped absolute deviation penalty. We demonstrate our approach on\nsimulated and genomic datasets. The method is implemented in an R package\ntsnetwork.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 18:12:10 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Behrouzi", "Pariya", ""], ["Abegaz", "Fentaw", ""], ["Wit", "Ernst C.", ""]]}, {"id": "1805.09871", "submitter": "Dong Xia", "authors": "Dong Xia", "title": "Confidence Region of Singular Subspaces for Low-rank Matrix Regression", "comments": "typos are corrected and motivating examples are added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix regression refers to the instances of recovering a low-rank\nmatrix based on specially designed measurements and the corresponding noisy\noutcomes. In the last decade, numerous statistical methodologies have been\ndeveloped for efficiently recovering the unknown low-rank matrices. However, in\nsome applications, the unknown singular subspace is scientifically more\nimportant than the low-rank matrix itself. In this article, we revisit the\nlow-rank matrix regression model and introduce a two-step procedure to\nconstruct confidence regions of the singular subspace. The procedure involves\nthe de-biasing for the typical low-rank estimators after which we calculate the\nempirical singular vectors. We investigate the distribution of the joint\nprojection distance between the empirical singular subspace and the unknown\ntrue singular subspace. We specifically prove the asymptotical normality of the\njoint projection distance with data-dependent centering and normalization when\n$r^{3/2}(m_1+m_2)^{3/2}=o(n/\\log n)$ where $m_1, m_2$ denote the matrix row and\ncolumn sizes, $r$ is the rank and $n$ is the number of independent random\nmeasurements. Consequently, we propose data-dependent confidence regions of the\ntrue singular subspace which attains any pre-determined confidence level\nasymptotically. In addition, non-asymptotical convergence rates are also\nestablished. Numerical results are presented to demonstrate the merits of our\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 19:53:58 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 08:04:17 GMT"}, {"version": "v3", "created": "Wed, 23 Jan 2019 14:16:22 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Xia", "Dong", ""]]}, {"id": "1805.09873", "submitter": "Charles R Doss", "authors": "Charles R. Doss", "title": "Concave regression: value-constrained estimation and likelihood\n  ratio-based inference", "comments": "29 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a likelihood ratio statistic for forming hypothesis tests and\nconfidence intervals for a nonparametrically estimated univariate regression\nfunction, based on the shape restriction of concavity (alternatively,\nconvexity). Dealing with the likelihood ratio statistic requires studying an\nestimator satisfying a null hypothesis, that is, studying a concave\nleast-squares estimator satisfying a further equality constraint. We study this\nnull hypothesis least-squares estimator (NLSE) here, and use it to study our\nlikelihood ratio statistic. The NLSE is the solution to a convex program, and\nwe find a set of inequality and equality constraints that characterize the\nsolution. We also study a corresponding limiting version of the convex program\nbased on observing a Brownian motion with drift. The solution to the limit\nproblem is a stochastic process. We study the optimality conditions for the\nsolution to the limit problem and find that they match those we derived for the\nsolution to the finite sample problem. This allows us to show the limit\nstochastic process yields the limit distribution of the (finite sample) NLSE.\nWe conjecture that the likelihood ratio statistic is asymptotically pivotal,\nmeaning that it has a limit distribution with no nuisance parameters to be\nestimated, which makes it a very effective tool for this difficult inference\nproblem. We provide a partial proof of this conjecture, and we also provide\nsimulation evidence strongly supporting this conjecture.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 19:57:12 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 06:19:14 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Doss", "Charles R.", ""]]}, {"id": "1805.09876", "submitter": "Chuan Hong", "authors": "Chuan Hong, Georgia Salanti, Sally Morton, Richard Riley, Haitao Chu,\n  Stephen E. Kimmel, Yong Chen", "title": "Testing small study effects in multivariate meta-analysis", "comments": "20 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Small study effects occur when smaller studies show different, often larger,\ntreatment effects than large ones, which may threaten the validity of\nsystematic reviews and meta-analyses. The most well-known reasons for small\nstudy effects include publication bias, outcome reporting bias and clinical\nheterogeneity. Methods to account for small study effects in univariate\nmeta-analysis have been extensively studied. However, detecting small study\neffects in a multivariate meta-analysis setting remains an untouched research\narea. One of the complications is that different types of selection processes\ncan be involved in the reporting of multivariate outcomes. For example, some\nstudies may be completely unpublished while others may selectively report\nmultiple outcomes. In this paper, we propose a score test as an overall test of\nsmall study effects in multivariate meta-analysis. Two detailed case studies\nare given to demonstrate the advantage of the proposed test over various naive\napplications of univariate tests in practice. Through simulation studies, the\nproposed test is found to retain nominal Type I error with considerable power\nin moderate sample size settings. Finally, we also evaluate the concordance\nbetween the proposed test with the naive application of univariate tests by\nevaluating 44 systematic reviews with multiple outcomes from the Cochrane\nDatabase.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 20:00:44 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Hong", "Chuan", ""], ["Salanti", "Georgia", ""], ["Morton", "Sally", ""], ["Riley", "Richard", ""], ["Chu", "Haitao", ""], ["Kimmel", "Stephen E.", ""], ["Chen", "Yong", ""]]}, {"id": "1805.09902", "submitter": "Fabian Kr\\\"uger", "authors": "Fabian Kr\\\"uger and Johanna F. Ziegel", "title": "Generic Conditions for Forecast Dominance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have analyzed whether one forecast method dominates another\nunder a class of consistent scoring functions. While the existing literature\nfocuses on empirical tests of forecast dominance, little is known about the\ntheoretical conditions under which one forecast dominates another. To address\nthis question, we derive a new characterization of dominance among forecasts of\nthe mean functional. We present various scenarios under which dominance occurs.\nUnlike existing results, our results allow for the case that the forecasts'\nunderlying information sets are not nested, and allow for uncalibrated\nforecasts that suffer, e.g., from model misspecification or parameter\nestimation error. We illustrate the empirical relevance of our results via data\nexamples from finance and economics.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 21:13:31 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 08:58:35 GMT"}, {"version": "v3", "created": "Wed, 6 Mar 2019 12:32:13 GMT"}, {"version": "v4", "created": "Wed, 18 Dec 2019 10:38:27 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Kr\u00fcger", "Fabian", ""], ["Ziegel", "Johanna F.", ""]]}, {"id": "1805.09937", "submitter": "Tatsushi Oka", "authors": "Dukpa Kim, Tatsushi Oka, Francisco Estrada, Pierre Perron", "title": "Inference Related to Common Breaks in a Multivariate System with Joined\n  Segmented Trends with Applications to Global and Hemispheric Temperatures", "comments": "42 pages, 8 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What transpires from recent research is that temperatures and radiative\nforcing seem to be characterized by a linear trend with two changes in the rate\nof growth. The first occurs in the early 60s and indicates a very large\nincrease in the rate of growth of both temperature and radiative forcing\nseries. This was termed as the \"onset of sustained global warming\". The second\nis related to the more recent so-called hiatus period, which suggests that\ntemperatures and total radiative forcing have increased less rapidly since the\nmid-90s compared to the larger rate of increase from 1960 to 1990. There are\ntwo issues that remain unresolved. The first is whether the breaks in the slope\nof the trend functions of temperatures and radiative forcing are common. This\nis important because common breaks coupled with the basic science of climate\nchange would strongly suggest a causal effect from anthropogenic factors to\ntemperatures. The second issue relates to establishing formally via a proper\ntesting procedure that takes into account the noise in the series, whether\nthere was indeed a `hiatus period' for temperatures since the mid 90s. This is\nimportant because such a test would counter the widely held view that the\nhiatus is the product of natural internal variability. Our paper provides tests\nrelated to both issues. The results show that the breaks in temperatures and\nradiative forcing are common and that the hiatus is characterized by a\nsignificant decrease in their rate of growth. The statistical results are of\nindependent interest and applicable more generally.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 00:13:42 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Kim", "Dukpa", ""], ["Oka", "Tatsushi", ""], ["Estrada", "Francisco", ""], ["Perron", "Pierre", ""]]}, {"id": "1805.09978", "submitter": "James Sharpnack", "authors": "Shitong Wei, Oscar Hernan Madrid-Padilla, James Sharpnack", "title": "Distributed Cartesian Power Graph Segmentation for Graphon Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an extention of total variation denoising over images to over\nCartesian power graphs and its applications to estimating non-parametric\nnetwork models. The power graph fused lasso (PGFL) segments a matrix by\nexploiting a known graphical structure, $G$, over the rows and columns. Our\nmain results shows that for any connected graph, under subGaussian noise, the\nPGFL achieves the same mean-square error rate as 2D total variation denoising\nfor signals of bounded variation. We study the use of the PGFL for denoising an\nobserved network $H$, where we learn the graph $G$ as the $K$-nearest\nneighborhood graph of an estimated metric over the vertices. We provide\ntheoretical and empirical results for estimating graphons, a non-parametric\nexchangeable network model, and compare to the state of the art graphon\nestimation methods.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 04:27:58 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Wei", "Shitong", ""], ["Madrid-Padilla", "Oscar Hernan", ""], ["Sharpnack", "James", ""]]}, {"id": "1805.10040", "submitter": "Ingo Hoffmann", "authors": "Ingo Hoffmann and Christoph J. B\\\"orner", "title": "Body and Tail - Separating the distribution function by an efficient\n  tail-detecting procedure in risk management", "comments": "38 pages, 12 figures", "journal-ref": null, "doi": "10.21314/JOR.2020.447", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In risk management, tail risks are of crucial importance. The quality of a\ntail model, which is determined by data from an unknown distribution, depends\ncritically on the subset of data used to model the tail. Based on a suitably\nweighted mean square error, we present a method that can separate the required\nsubset. The selected data are used to determine the parameters of the tail\nmodel. Notably, no parameter specifications have to be made to apply the\nproposed procedure. Standard goodness of fit tests allow us to evaluate the\nquality of the fitted tail model. We apply the method to standard distributions\nthat are usually considered in the finance and insurance industries. In\naddition, for the MSCI World Index, we use historical data to identify the tail\nmodel and to compute the quantiles required for a risk assessment.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 08:52:38 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Hoffmann", "Ingo", ""], ["B\u00f6rner", "Christoph J.", ""]]}, {"id": "1805.10122", "submitter": "Shifeng Xiong Doc", "authors": "Shifeng Xiong", "title": "The Reconstruction Approach: From Interpolation to Regression", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an interpolation-based method, called the\nreconstruction approach, for nonparametric regression. Based on the fact that\ninterpolation usually has negligible errors compared to statistical estimation,\nthe reconstruction approach uses an interpolator to parameterize the regression\nfunction with its values at finite knots, and then estimates these values by\n(regularized) least squares. Some popular methods including kernel ridge\nregression can be viewed as its special cases. It is shown that, the\nreconstruction idea not only provides different angles to look into existing\nmethods, but also produces new effective experimental design and estimation\nmethods for nonparametric models. In particular, for some methods of complexity\nO(n3), where n is the sample size, this approach provides effective surrogates\nwith much less computational burden. This point makes it very suitable for\nlarge datasets.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 12:56:54 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 07:18:59 GMT"}, {"version": "v3", "created": "Wed, 27 Nov 2019 12:09:07 GMT"}], "update_date": "2019-11-28", "authors_parsed": [["Xiong", "Shifeng", ""]]}, {"id": "1805.10214", "submitter": "Maxime Rischard", "authors": "Maxime Rischard, Natesh Pillai, Karen A. McKinnon", "title": "Bias correction in daily maximum and minimum temperature measurements\n  through Gaussian process modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Global Historical Climatology Network-Daily database contains, among\nother variables, daily maximum and minimum temperatures from weather stations\naround the globe. It is long known that climatological summary statistics based\non daily temperature minima and maxima will not be accurate, if the bias due to\nthe time at which the observations were collected is not accounted for. Despite\nsome previous work, to our knowledge, there does not exist a satisfactory\nsolution to this important problem. In this paper, we carefully detail the\nproblem and develop a novel approach to address it. Our idea is to impute the\nhourly temperatures at the location of the measurements by borrowing\ninformation from the nearby stations that record hourly temperatures, which\nthen can be used to create accurate summaries of temperature extremes. The key\ndifficulty is that these imputations of the temperature curves must satisfy the\nconstraint of falling between the observed daily minima and maxima, and\nattaining those values at least once in a twenty-four hour period. We develop a\nspatiotemporal Gaussian process model for imputing the hourly measurements from\nthe nearby stations, and then develop a novel and easy to implement Markov\nChain Monte Carlo technique to sample from the posterior distribution\nsatisfying the above constraints. We validate our imputation model using hourly\ntemperature data from four meteorological stations in Iowa, of which one is\nhidden and the data replaced with daily minima and maxima, and show that the\nimputed temperatures recover the hidden temperatures well. We also demonstrate\nthat our model can exploit information contained in the data to infer the time\nof daily measurements.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 15:52:12 GMT"}, {"version": "v2", "created": "Tue, 29 May 2018 16:13:53 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Rischard", "Maxime", ""], ["Pillai", "Natesh", ""], ["McKinnon", "Karen A.", ""]]}, {"id": "1805.10229", "submitter": "Konstantinos Spiliopoulos", "authors": "Matthew R. Morse and Konstantinos Spiliopoulos", "title": "Importance sampling for slow-fast diffusions based on moderate\n  deviations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider systems of slow--fast diffusions with small noise in the slow\ncomponent. We construct provably logarithmic asymptotically optimal importance\nschemes for the estimation of rare events based on the moderate deviations\nprinciple. Using the subsolution approach we construct schemes and identify\nconditions under which the schemes will be asymptotically optimal. Moderate\ndeviations--based importance sampling offers a viable alternative to large\ndeviations importance sampling when the events are not too rare. In particular,\nin many cases of interest one can indeed construct the required change of\nmeasure in closed form, a task which is more complicated using the large\ndeviations--based importance sampling, especially when it comes to multiscale\ndynamically evolving processes. The presence of multiple scales and the fact\nthat we do not make any periodicity assumptions for the coefficients driving\nthe processes, complicates the design and the analysis of efficient importance\nsampling schemes. Simulation studies illustrate the theory.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 16:11:33 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2020 18:25:02 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Morse", "Matthew R.", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1805.10406", "submitter": "Simon Du", "authors": "Simon S. Du, Yining Wang, Sivaraman Balakrishnan, Pradeep Ravikumar,\n  Aarti Singh", "title": "Robust Nonparametric Regression under Huber's $\\epsilon$-contamination\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DS cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the non-parametric regression problem under Huber's\n$\\epsilon$-contamination model, in which an $\\epsilon$ fraction of observations\nare subject to arbitrary adversarial noise. We first show that a simple local\nbinning median step can effectively remove the adversary noise and this median\nestimator is minimax optimal up to absolute constants over the H\\\"{o}lder\nfunction class with smoothness parameters smaller than or equal to 1.\nFurthermore, when the underlying function has higher smoothness, we show that\nusing local binning median as pre-preprocessing step to remove the adversarial\nnoise, then we can apply any non-parametric estimator on top of the medians. In\nparticular we show local median binning followed by kernel smoothing and local\npolynomial regression achieve minimaxity over H\\\"{o}lder and Sobolev classes\nwith arbitrary smoothness parameters. Our main proof technique is a decoupled\nanalysis of adversary noise and stochastic noise, which can be potentially\napplied to other robust estimation problems. We also provide numerical results\nto verify the effectiveness of our proposed methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 00:39:12 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Du", "Simon S.", ""], ["Wang", "Yining", ""], ["Balakrishnan", "Sivaraman", ""], ["Ravikumar", "Pradeep", ""], ["Singh", "Aarti", ""]]}, {"id": "1805.10540", "submitter": "Agatha Rodrigues Mrs.", "authors": "Agatha Rodrigues and Carlos Alberto Pereira and Adriano Polpo", "title": "Reliability Estimation in Coherent Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually, methods evaluating system reliability require engineers to quantify\nthe reliability of each of the system components. For series and parallel\nsystems, there are some options to handle the estimation of each component's\nreliability. We will treat the reliability estimation of complex problems of\ntwo classes of coherent systems: series-parallel, and parallel-series. In both\nof the cases, the component reliabilities may be unknown. We will present\nestimators for reliability functions at all levels of the system (component and\nsystem reliabilities). Nonparametric Bayesian estimators of all\nsub-distribution and distribution functions are derived, and a Dirichlet\nmultivariate process as a prior distribution is presented. Parametric estimator\nof the component's reliability based on Weibull model is presented for any kind\nof system. Also, some ideas in systems with masked data are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 26 May 2018 21:15:20 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Rodrigues", "Agatha", ""], ["Pereira", "Carlos Alberto", ""], ["Polpo", "Adriano", ""]]}, {"id": "1805.10570", "submitter": "X. Jessie Jeng", "authors": "X. Jessie Jeng, Teng Zhang, and Jung-Ying Tzeng", "title": "Efficient Signal Inclusion With Genomic Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the challenge of efficiently capturing a high proportion\nof true signals for subsequent data analyses when sample sizes are relatively\nlimited with respect to data dimension. We propose the signal missing rate as a\nnew measure for false negative control to account for the variability of false\nnegative proportion. Novel data-adaptive procedures are developed to control\nsignal missing rate without incurring many unnecessary false positives under\ndependence. We justify the efficiency and adaptivity of the proposed methods\nvia theory and simulation. The proposed methods are applied to GWAS on human\nheight to effectively remove irrelevant SNPs while retaining a high proportion\nof relevant SNPs for subsequent polygenic analysis.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 02:34:16 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 01:29:31 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Jeng", "X. Jessie", ""], ["Zhang", "Teng", ""], ["Tzeng", "Jung-Ying", ""]]}, {"id": "1805.10594", "submitter": "Sharmodeep Bhattacharyya", "authors": "Sharmodeep Bhattacharyya and Shirshendu Chatterjee", "title": "Spectral Clustering for Multiple Sparse Networks: I", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although much of the focus of statistical works on networks has been on\nstatic networks, multiple networks are currently becoming more common among\nnetwork data sets. Usually, a number of network data sets, which share some\nform of connection between each other are known as multiple or multi-layer\nnetworks. We consider the problem of identifying the common community\nstructures for multiple networks. We consider extensions of the spectral\nclustering methods for the multiple sparse networks, and give theoretical\nguarantee that the spectral clustering methods produce consistent community\ndetection in case of both multiple stochastic block model and multiple\ndegree-corrected block models. The methods are shown to work under sufficiently\nmild conditions on the number of multiple networks to detect associative\ncommunity structures, even if all the individual networks are sparse and most\nof the individual networks are below community detectability threshold. We\nreinforce the validity of the theoretical results via simulations too.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 08:01:21 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Bhattacharyya", "Sharmodeep", ""], ["Chatterjee", "Shirshendu", ""]]}, {"id": "1805.10639", "submitter": "Joris Mulder", "authors": "Joris Mulder and Adrian E. Raftery", "title": "BIC extensions for order-constrained model selection", "comments": "25 pages, 4, figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Schwarz or Bayesian information criterion (BIC) is one of the most widely\nused tools for model comparison in social science research. The BIC however is\nnot suitable for evaluating models with order constraints on the parameters of\ninterest. This paper explores two extensions of the BIC for evaluating order\nconstrained models, one where a truncated unit information prior is used under\nthe order-constrained model, and the other where a truncated local unit\ninformation prior is used. The first prior is centered around the maximum\nlikelihood estimate and the latter prior is centered around a null value.\nSeveral analyses show that the order-constrained BIC based on the local unit\ninformation prior works better as an Occam's razor for evaluating\norder-constrained models and results in lower error probabilities. The\nmethodology based on the local unit information prior is implemented in the R\npackage `BICpack' which allows researchers to easily apply the method for\norder-constrained model selection. The usefulness of the methodology is\nillustrated using data from the European Values Study.\n", "versions": [{"version": "v1", "created": "Sun, 27 May 2018 15:32:45 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 13:35:22 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 18:37:05 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Mulder", "Joris", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1805.10742", "submitter": "Jinyuan Chang", "authors": "Jinyuan Chang and Song Xi Chen and Cheng Yong Tang and Tong Tong Wu", "title": "High-dimensional empirical likelihood inference", "comments": "The original title of this paper is \"High-dimensional statistical\n  inferences with over-identification: confidence set estimation and\n  specification test\"", "journal-ref": "Biometrika 2021, Vol. 108, No. 1, 127-147", "doi": "10.1093/biomet/asaa051", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional statistical inference with general estimating equations are\nchallenging and remain less explored. In this paper, we study two problems in\nthe area: confidence set estimation for multiple components of the model\nparameters, and model specifications test. For the first one, we propose to\nconstruct a new set of estimating equations such that the impact from\nestimating the high-dimensional nuisance parameters becomes asymptotically\nnegligible. The new construction enables us to estimate a valid confidence\nregion by empirical likelihood ratio. For the second one, we propose a test\nstatistic as the maximum of the marginal empirical likelihood ratios to\nquantify data evidence against the model specification. Our theory establishes\nthe validity of the proposed empirical likelihood approaches, accommodating\nover-identification and exponentially growing data dimensionality. The\nnumerical studies demonstrate promising performance and potential practical\nbenefits of the new methods.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 02:36:02 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 22:16:18 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Chang", "Jinyuan", ""], ["Chen", "Song Xi", ""], ["Tang", "Cheng Yong", ""], ["Wu", "Tong Tong", ""]]}, {"id": "1805.10854", "submitter": "Ingrid Hob{\\ae}k Haff", "authors": "Erik B{\\o}lviken and Ingrid Hob{\\ae}k Haff", "title": "One family, six distributions -- A flexible model for insurance claim\n  severity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of claim severity distributions with six parameters,\nthat has the standard two-parameter distributions, the log-normal, the\nlog-Gamma, the Weibull, the Gamma and the Pareto, as special cases. This\ndistribution is much more flexible than its special cases, and therefore more\nable to to capture important characteristics of claim severity data. Further,\nwe have investigated how increased parameter uncertainty due to a larger number\nof parameters affects the estimate of the reserve. This is done in a large\nsimulation study, where both the characteristics of the claim size\ndistributions and the sample size are varied. We have also tried our model on a\nset of motor insurance claims from a Norwegian insurance company. The results\nfrom the study show that as long as the amount of data is reasonable, the five-\nand six-parameter versions of our model provide very good estimates of both the\nquantiles of the claim severity distribution and the reserves, for claim size\ndistributions ranging from medium to very heavy tailed. However, when the\nsample size is small, our model appears to struggle with heavy-tailed data, but\nis still adequate for data with more moderate tails.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 10:29:44 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["B\u00f8lviken", "Erik", ""], ["Haff", "Ingrid Hob\u00e6k", ""]]}, {"id": "1805.10865", "submitter": "Xanthi Pedeli", "authors": "Xanthi Pedeli and Cristiano Varin", "title": "Pairwise likelihood estimation of latent autoregressive count models", "comments": "The final version of the paper has been published in Statistical\n  Methods in Medical Research", "journal-ref": null, "doi": "10.1177/0962280220924068", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent autoregressive models are useful time series models for the analysis\nof infectious disease data. Evaluation of the likelihood function of latent\nautoregressive models is intractable and its approximation through\nsimulation-based methods appears as a standard practice. Although simulation\nmethods may make the inferential problem feasible, they are often\ncomputationally intensive and the quality of the numerical approximation may be\ndifficult to assess. We consider instead a weighted pairwise likelihood\napproach and explore several computational and methodological aspects including\nestimation of robust standard errors and the role of numerical integration. The\nsuggested approach is illustrated using monthly data on invasive meningococcal\ndisease infection in Greece and Italy.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 11:13:35 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 12:53:17 GMT"}, {"version": "v3", "created": "Tue, 11 Feb 2020 08:13:39 GMT"}, {"version": "v4", "created": "Tue, 25 Feb 2020 08:18:44 GMT"}, {"version": "v5", "created": "Mon, 22 Jun 2020 07:03:20 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Pedeli", "Xanthi", ""], ["Varin", "Cristiano", ""]]}, {"id": "1805.10890", "submitter": "Christian R\\\"over", "authors": "Christian R\\\"over, Simon Wandel, Tim Friede", "title": "Model averaging for robust extrapolation in evidence synthesis", "comments": "18 pages, 7 figures, 5 tables", "journal-ref": "Statistics in Medicine, 38(4):674-694, 2019", "doi": "10.1002/sim.7991", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extrapolation from a source to a target, e.g., from adults to children, is a\npromising approach to utilizing external information when data are sparse. In\nthe context of meta-analysis, one is commonly faced with a small number of\nstudies, while potentially relevant additional information may also be\navailable. Here we describe a simple extrapolation strategy using heavy-tailed\nmixture priors for effect estimation in meta-analysis, which effectively\nresults in a model-averaging technique. The described method is robust in the\nsense that a potential prior-data conflict, i.e., a discrepancy between source\nand target data, is explicitly anticipated. The aim of this paper to develop a\nsolution for this particular application, to showcase the ease of\nimplementation by providing R code, and to demonstrate the robustness of the\ngeneral approach in simulations.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 12:38:04 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 11:46:36 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["R\u00f6ver", "Christian", ""], ["Wandel", "Simon", ""], ["Friede", "Tim", ""]]}, {"id": "1805.11126", "submitter": "Fekadu L. Bayisa Dr.", "authors": "Fekadu L. Bayisa, Xijia Liu, Anders Garpebring, and Jun Yu", "title": "Statistical Methods in Computed Tomography Image Estimation", "comments": null, "journal-ref": null, "doi": "10.1002/mp.13204", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: There is increasing interest in computed tomography (CT) image\nestimations from magnetic resonance (MR) images. The estimated CT images can be\nutilised for attenuation correction, patient positioning, and dose planning in\ndiagnostic and radiotherapy workflows. This study aims to introduce a novel\nstatistical learning approach for improving CT estimation from MR images and to\ncompare the performance of our method with the existing model based CT image\nestimation methods.\n  Methods: The statistical learning approach proposed here consists of two\nstages. At the training stage, prior knowledges about tissue-types from CT\nimages were used together with a Gaussian mixture model (GMM) to explore CT\nimage estimations from MR images. Since the prior knowledges are not available\nat the prediction stage, a classifier based on RUSBoost algorithm was trained\nto estimate the tissue-types from MR images. For a new patient, the trained\nclassifier and GMMs were used to predict CT image from MR images. The\nclassifier and GMMs were validated by using voxel level 10-fold\ncross-validation and patient-level leave-one-out cross-validation,\nrespectively.\n  Results: The proposed approach has outperformance in CT estimation quality in\ncomparison with the existing model based methods, especially on bone tissues.\nOur method improved CT image estimation by 5% and 23% on the whole brain and\nbone tissues, respectively.\n  Conclusions: Evaluation of our method shows that it is a promising method to\ngenerate CT image substitutes for the implementation of fully MR-based\nradiotherapy and PET/MRI applications.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 18:42:43 GMT"}, {"version": "v2", "created": "Sat, 30 Mar 2019 17:38:56 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Bayisa", "Fekadu L.", ""], ["Liu", "Xijia", ""], ["Garpebring", "Anders", ""], ["Yu", "Jun", ""]]}, {"id": "1805.11183", "submitter": "Mingyuan Zhou", "authors": "Mingzhang Yin and Mingyuan Zhou", "title": "Semi-Implicit Variational Inference", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-implicit variational inference (SIVI) is introduced to expand the\ncommonly used analytic variational distribution family, by mixing the\nvariational parameter with a flexible distribution. This mixing distribution\ncan assume any density function, explicit or not, as long as independent random\nsamples can be generated via reparameterization. Not only does SIVI expand the\nvariational family to incorporate highly flexible variational distributions,\nincluding implicit ones that have no analytic density functions, but also\nsandwiches the evidence lower bound (ELBO) between a lower bound and an upper\nbound, and further derives an asymptotically exact surrogate ELBO that is\namenable to optimization via stochastic gradient ascent. With a substantially\nexpanded variational family and a novel optimization algorithm, SIVI is shown\nto closely match the accuracy of MCMC in inferring the posterior in a variety\nof Bayesian inference tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 21:55:02 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Yin", "Mingzhang", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1805.11258", "submitter": "Filip Tronarp", "authors": "Filip Tronarp, Simo S\\\"arkk\\\"a", "title": "Iterative Statistical Linear Regression for Gaussian Smoothing in\n  Continuous-Time Non-linear Stochastic Dynamic Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers approximate smoothing for discretely observed non-linear\nstochastic differential equations. The problem is tackled by developing methods\nfor linearising stochastic differential equations with respect to an arbitrary\nGaussian process. Two methods are developed based on 1) taking the limit of\nstatistical linear regression of the discretised process and 2) minimising an\nupper bound to a cost functional. Their difference is manifested in the\ndiffusion of the approximate processes. This in turn gives novel derivations of\npre-existing Gaussian smoothers when Method 1 is used and a new class of\nGaussian smoothers when Method 2 is used. Furthermore, based on the\naforementioned development the iterative Gaussian smoothers in discrete-time\nare generalised to the continuous-time setting by iteratively re-linearising\nthe stochastic differential equation with respect to the current Gaussian\nprocess approximation to the smoothed process. The method is verified in two\nchallenging tracking problems, a reentry problem and a radar tracked\ncoordinated turn model with state dependent diffusion. The results show that\nthe method has better estimation accuracy than state-of-the-art smoothers.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 06:07:39 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2019 14:00:09 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Tronarp", "Filip", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1805.11414", "submitter": "Shogo Nakakita", "authors": "Shogo H. Nakakita, Masayuki Uchida", "title": "Inference for ergodic diffusions plus noise", "comments": "arXiv admin note: substantial text overlap with arXiv:1711.04462", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We research adaptive maximum likelihood-type estimation for an ergodic\ndiffusion process where the observation is contaminated by noise. This\nmethodology leads to the asymptotic independence of the estimators for the\nvariance of observation noise, the diffusion parameter and the drift one of the\nlatent diffusion process. Moreover, it can lessen the computational burden\ncompared to simultaneous maximum likelihood-type estimation. In addition to\nadaptive estimation, we propose a test to see if noise exists or not, and\nanalyse real data as the example such that data contains observation noise with\nstatistical significance.\n", "versions": [{"version": "v1", "created": "Mon, 28 May 2018 15:12:14 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Nakakita", "Shogo H.", ""], ["Uchida", "Masayuki", ""]]}, {"id": "1805.11505", "submitter": "Timothy Cannings", "authors": "Timothy I. Cannings, Yingying Fan and Richard J. Samworth", "title": "Classification with imperfect training labels", "comments": "44 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of imperfect training data labels on the performance of\nclassification methods. In a general setting, where the probability that an\nobservation in the training dataset is mislabelled may depend on both the\nfeature vector and the true label, we bound the excess risk of an arbitrary\nclassifier trained with imperfect labels in terms of its excess risk for\npredicting a noisy label. This reveals conditions under which a classifier\ntrained with imperfect labels remains consistent for classifying uncorrupted\ntest data points. Furthermore, under stronger conditions, we derive detailed\nasymptotic properties for the popular $k$-nearest neighbour ($k$nn), support\nvector machine (SVM) and linear discriminant analysis (LDA) classifiers. One\nconsequence of these results is that the knn and SVM classifiers are robust to\nimperfect training labels, in the sense that the rate of convergence of the\nexcess risks of these classifiers remains unchanged; in fact, our theoretical\nand empirical results even show that in some cases, imperfect labels may\nimprove the performance of these methods. On the other hand, the LDA classifier\nis shown to be typically inconsistent in the presence of label noise unless the\nprior probabilities of each class are equal. Our theoretical results are\nsupported by a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 29 May 2018 14:36:06 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 10:58:08 GMT"}, {"version": "v3", "created": "Mon, 6 May 2019 10:53:29 GMT"}], "update_date": "2019-05-07", "authors_parsed": [["Cannings", "Timothy I.", ""], ["Fan", "Yingying", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1805.11908", "submitter": "Marco Scutari", "authors": "Marco Scutari, Catharina Elisabeth Graafland, Jos\\'e Manuel\n  Guti\\'errez", "title": "Who Learns Better Bayesian Network Structures: Accuracy and Speed of\n  Structure Learning Algorithms", "comments": "27 pages, 8 figures", "journal-ref": "Proceedings of Machine Learning Research (72, PGM 2018), 416-427;\n  extended version in International Journal of Approximate Reasoning,\n  115:235-253", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three classes of algorithms to learn the structure of Bayesian networks from\ndata are common in the literature: constraint-based algorithms, which use\nconditional independence tests to learn the dependence structure of the data;\nscore-based algorithms, which use goodness-of-fit scores as objective functions\nto maximise; and hybrid algorithms that combine both approaches.\nConstraint-based and score-based algorithms have been shown to learn the same\nstructures when conditional independence and goodness of fit are both assessed\nusing entropy and the topological ordering of the network is known (Cowell,\n2001).\n  In this paper, we investigate how these three classes of algorithms perform\noutside the assumptions above in terms of speed and accuracy of network\nreconstruction for both discrete and Gaussian Bayesian networks. We approach\nthis question by recognising that structure learning is defined by the\ncombination of a statistical criterion and an algorithm that determines how the\ncriterion is applied to the data. Removing the confounding effect of different\nchoices for the statistical criterion, we find using both simulated and\nreal-world complex data that constraint-based algorithms are often less\naccurate than score-based algorithms, but are seldom faster (even at large\nsample sizes); and that hybrid algorithms are neither faster nor more accurate\nthan constraint-based algorithms. This suggests that commonly held beliefs on\nstructure learning in the literature are strongly influenced by the choice of\nparticular statistical criteria rather than just by the properties of the\nalgorithms themselves.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 11:42:44 GMT"}, {"version": "v2", "created": "Wed, 1 Aug 2018 10:00:45 GMT"}, {"version": "v3", "created": "Wed, 31 Jul 2019 14:36:11 GMT"}], "update_date": "2021-02-10", "authors_parsed": [["Scutari", "Marco", ""], ["Graafland", "Catharina Elisabeth", ""], ["Guti\u00e9rrez", "Jos\u00e9 Manuel", ""]]}, {"id": "1805.12179", "submitter": "Gabriela Cybis", "authors": "Marcio Valk and Gabriela Bettella Cybis", "title": "U-statistical inference for hierarchical clustering", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering methods are a valuable tool for the identification of patterns in\nhigh dimensional data with applications in many scientific problems. However,\nquantifying uncertainty in clustering is a challenging problem, particularly\nwhen dealing with High Dimension Low Sample Size (HDLSS) data. We develop here\na U-statistics based clustering approach that assesses statistical significance\nin clustering and is specifically tailored to HDLSS scenarios. These\nnon-parametric methods rely on very few assumptions about the data, and thus\ncan be applied to a wide range of datasets for which the euclidean distance\ncaptures relevant features. We propose two significance clustering algorithms,\na hierarchical method and a non-nested version. In order to do so, we first\npropose an extension of a relevant U-statistics and develop its asymptotic\ntheory. Our methods are tested through extensive simulations and found to be\nmore powerful than competing alternatives. They are further showcased in two\napplications ranging from genetics to image recognition problems.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 18:59:57 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Valk", "Marcio", ""], ["Cybis", "Gabriela Bettella", ""]]}, {"id": "1805.12201", "submitter": "Abhra Sarkar", "authors": "Abhra Sarkar, David B. Dunson", "title": "Bayesian Higher Order Hidden Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of flexible modeling of higher order hidden Markov\nmodels when the number of latent states and the nature of the serial\ndependence, including the true order, are unknown. We propose Bayesian\nnonparametric methodology based on tensor factorization techniques that can\ncharacterize any transition probability with a specified maximal order,\nallowing automated selection of the important lags and capturing higher order\ninteractions among the lags. Theoretical results provide insights into\nidentifiability of the emission distributions and asymptotic behavior of the\nposterior. We design efficient Markov chain Monte Carlo algorithms for\nposterior computation. In simulation experiments, the method vastly\noutperformed its first and higher order competitors not just in higher order\nsettings, but, remarkably, also in first order cases. Practical utility is\nillustrated using real world applications.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 19:59:26 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 18:55:51 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Sarkar", "Abhra", ""], ["Dunson", "David B.", ""]]}, {"id": "1805.12249", "submitter": "Martin Happ", "authors": "Martin Happ, Arne C. Bathke, Edgar Brunner", "title": "Optimal Sample Size Planning for the Wilcoxon-Mann-Whitney-Test", "comments": null, "journal-ref": "Statistics in Medicine (2018)", "doi": "10.1002/sim.7983", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many different proposed procedures for sample size planning for the\nWilcoxon-Mann-Whitney test at given type-I and type-II error rates $\\alpha$ and\n$\\beta$, respectively. Most methods assume very specific models or types of\ndata in order to simplify calculations (for example, ordered categorical or\nmetric data, location shift alternatives, etc.). We present a unified approach\nthat covers metric data with and without ties, count data, ordered categorical\ndata, and even dichotomous data. For that, we calculate the unknown theoretical\nquantities such as the variances under the null and relevant alternative\nhypothesis by considering the following `synthetic data' approach. We evaluate\ndata whose empirical distribution functions match with the theoretical\ndistribution functions involved in the computations of the unknown theoretical\nquantities. Then well-known relations for the ranks of the data are used for\nthe calculations.\n  In addition to computing the necessary sample size $N$ for a fixed allocation\nproportion $t = n_1/N$, where $n_1$ is the sample size in the first group and\n$N = n_1 + n_2$ is the total sample size, we provide an interval for the\noptimal allocation rate $t$ which minimizes the total sample size $N$. It turns\nout that for certain distributions, a balanced design is optimal. We give a\ncharacterization of these distributions. Furthermore we show that the optimal\nchoice of $t$ depends on the ratio of the two variances which determine the\nvariance of the Wilcoxon-Mann-Whitney statistic under the alternative. This is\ndifferent from an optimal sample size allocation in case of the normal\ndistribution model.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 22:27:34 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Happ", "Martin", ""], ["Bathke", "Arne C.", ""], ["Brunner", "Edgar", ""]]}, {"id": "1805.12253", "submitter": "Mahdi Imani", "authors": "Mahdi Imani, Roozbeh Dehghannasiri, Ulisses M. Braga-Neto, Edward R.\n  Dougherty", "title": "Sequential Experimental Design for Optimal Structural Intervention in\n  Gene Regulatory Networks Based on the Mean Objective Cost of Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.MN cs.SY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientists are attempting to use models of ever increasing complexity,\nespecially in medicine, where gene-based diseases such as cancer require better\nmodeling of cell regulation. Complex models suffer from uncertainty and\nexperiments are needed to reduce this uncertainty. Because experiments can be\ncostly and time-consuming it is desirable to determine experiments providing\nthe most useful information. If a sequence of experiments is to be performed,\nexperimental design is needed to determine the order. A classical approach is\nto maximally reduce the overall uncertainty in the model, meaning maximal\nentropy reduction. A recently proposed method takes into account both model\nuncertainty and the translational objective, for instance, optimal structural\nintervention in gene regulatory networks, where the aim is to alter the\nregulatory logic to maximally reduce the long-run likelihood of being in a\ncancerous state. The mean objective cost of uncertainty (MOCU) quantifies\nuncertainty based on the degree to which model uncertainty affects the\nobjective. Experimental design involves choosing the experiment that yields the\ngreatest reduction in MOCU. This paper introduces finite-horizon dynamic\nprogramming for MOCU-based sequential experimental design and compares it to\nthe greedy approach, which selects one experiment at a time without\nconsideration of the full horizon of experiments. A salient aspect of the paper\nis that it demonstrates the advantage of MOCU-based design over the widely used\nentropy-based design for both greedy and dynamic-programming strategies and\ninvestigates the effect of model conditions on the comparative performances.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 22:53:22 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Imani", "Mahdi", ""], ["Dehghannasiri", "Roozbeh", ""], ["Braga-Neto", "Ulisses M.", ""], ["Dougherty", "Edward R.", ""]]}, {"id": "1805.12256", "submitter": "Chanseok Park", "authors": "Chanseok Park", "title": "Note on the robustification of the Student $t$-test statistic using the\n  median and the median absolute deviation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this note, we propose a robustified analogue of the conventional Student\n$t$-test statistic. The proposed statistic is easy to implement and thus\npractically useful. We also show that it is a pivotal quantity and converges to\na standard normal distribution.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 23:06:40 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Park", "Chanseok", ""]]}]