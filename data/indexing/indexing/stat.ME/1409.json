[{"id": "1409.0074", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Benjamin Haaland", "title": "Speeding up neighborhood search in local Gaussian process prediction", "comments": "24 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent implementations of local approximate Gaussian process models have\npushed computational boundaries for non-linear, non-parametric prediction\nproblems, particularly when deployed as emulators for computer experiments.\nTheir flavor of spatially independent computation accommodates massive\nparallelization, meaning that they can handle designs two or more orders of\nmagnitude larger than previously. However, accomplishing that feat can still\nrequire massive supercomputing resources. Here we aim to ease that burden. We\nstudy how predictive variance is reduced as local designs are built up for\nprediction. We then observe how the exhaustive and discrete nature of an\nimportant search subroutine involved in building such local designs may be\noverly conservative. Rather, we suggest that searching the space radially,\ni.e., continuously along rays emanating from the predictive location of\ninterest, is a far thriftier alternative. Our empirical work demonstrates that\nray-based search yields predictors with accuracy comparable to exhaustive\nsearch, but in a fraction of the time - bringing a supercomputer implementation\nback onto the desktop.\n", "versions": [{"version": "v1", "created": "Sat, 30 Aug 2014 01:02:47 GMT"}, {"version": "v2", "created": "Mon, 5 Jan 2015 18:26:27 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Haaland", "Benjamin", ""]]}, {"id": "1409.0177", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Jamie L. Hanson, Jieping Ye, Richard J. Davidson, Seth\n  D. Pollak", "title": "Persistent Homology in Sparse Regression and its Application to Brain\n  Morphometry", "comments": "submitted to IEEE Transactions on Medical Imaging", "journal-ref": "IEEE Transactions on Medical Imaging 2015 34:1928-1939", "doi": "10.1109/TMI.2015.2416271", "report-no": null, "categories": "stat.ME cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse systems are usually parameterized by a tuning parameter that\ndetermines the sparsity of the system. How to choose the right tuning parameter\nis a fundamental and difficult problem in learning the sparse system. In this\npaper, by treating the the tuning parameter as an additional dimension,\npersistent homological structures over the parameter space is introduced and\nexplored. The structures are then further exploited in speeding up the\ncomputation using the proposed soft-thresholding technique. The topological\nstructures are further used as multivariate features in the tensor-based\nmorphometry (TBM) in characterizing white matter alterations in children who\nhave experienced severe early life stress and maltreatment. These analyses\nreveal that stress-exposed children exhibit more diffuse anatomical\norganization across the whole white matter region.\n", "versions": [{"version": "v1", "created": "Sun, 31 Aug 2014 02:15:33 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 04:35:11 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Chung", "Moo K.", ""], ["Hanson", "Jamie L.", ""], ["Ye", "Jieping", ""], ["Davidson", "Richard J.", ""], ["Pollak", "Seth D.", ""]]}, {"id": "1409.0217", "submitter": "Gillian Raab", "authors": "Gillian Raab, Beata Nowok, Chris Dibben", "title": "A simplified approach to generating synthetic data for disclosure\n  control", "comments": "This version has minor clarifications of the derivations in section 2", "journal-ref": "Journal of privacy and Confidentiality 3/4 2016\n  http://repository.cmu.edu/jpc/vol7/iss3/4", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe results on the creation and use of synthetic data that were\nderived in the context of a project to make synthetic extracts available for\nusers of the UK Longitudinal Studies. A critical review of existing methods of\ninference from large synthetic data sets is presented. We introduce new\nvariance estimates for use with large samples of completely synthesised data\nthat do not require them to be generated from the posterior predictive\ndistribution derived from the observed data. We make recommendations on how to\nsynthesise data based on these findings. An example of synthesising data from\nthe Scottish Longitudinal Study is included to illustrate our results.\n", "versions": [{"version": "v1", "created": "Sun, 31 Aug 2014 13:14:04 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2015 12:10:46 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2015 05:51:18 GMT"}, {"version": "v4", "created": "Tue, 14 Jul 2015 00:27:04 GMT"}, {"version": "v5", "created": "Sat, 22 Aug 2015 10:30:43 GMT"}, {"version": "v6", "created": "Sun, 20 Mar 2016 09:02:57 GMT"}, {"version": "v7", "created": "Wed, 30 Nov 2016 13:58:34 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Raab", "Gillian", ""], ["Nowok", "Beata", ""], ["Dibben", "Chris", ""]]}, {"id": "1409.0391", "submitter": "Zhou Jie", "authors": "Jie Zhou and Aiping Tang", "title": "Estimating Linear Mixed-effects State Space Model Based on Disturbance\n  Smoothing", "comments": "27 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We extend the linear mixed-effects state model to accommodate the correlated\nindividuals and investigate its parameter and state estimation based on\ndisturbance smoothing in this paper. For parameter estimation, EM and score\nbased algorithms are considered. Intermediate quantity of EM algorithm is\ninvestigated firstly from which the explicit recursive formulas for the\nmaximizer of the intermediate quantity are derived out for two given models. As\nfor score based algorithms, explicit formulas for the score vector are achieved\nfrom which it is shown that the maximum likelihood estimation is equivalent to\nmoment estimation. For state estimation we advocate it should be carried out\nwithout assuming the random effects being known in advance especially when the\nlongitudinal observations are sparse. To this end an algorithm named mixture\nKalman filter with kernel smoothing (MKF-KS) is proposed. Numerical studies are\ncarried out to investigate the proposed algorithms which validate the efficacy\nof the proposed inference approaches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 12:50:47 GMT"}, {"version": "v2", "created": "Tue, 2 Sep 2014 05:22:45 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Zhou", "Jie", ""], ["Tang", "Aiping", ""]]}, {"id": "1409.0506", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Ingrid Van Keilegom, Rosa M. Crujeiras,\n  Wenceslao Gonz\\'alez-Manteiga", "title": "Testing parametric models in linear-directional regression", "comments": "13 pages, 3 figures. Supplementary material: 22 pages, 9 figures, 3\n  tables", "journal-ref": "Scandinavian Journal of Statistics, 43(4):1178-1191, 2016", "doi": "10.1111/sjos.12236", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a goodness-of-fit test for parametric regression models\nwith scalar response and directional predictor, that is, a vector on a sphere\nof arbitrary dimension. The testing procedure is based on the weighted squared\ndistance between a smooth and a parametric regression estimator, where the\nsmooth regression estimator is obtained by a projected local approach.\nAsymptotic behavior of the test statistic under the null hypothesis and local\nalternatives is provided, jointly with a consistent bootstrap algorithm for\napplication in practice. A simulation study illustrates the performance of the\ntest in finite samples. The procedure is applied to test a linear model in text\nmining.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2014 19:01:12 GMT"}, {"version": "v2", "created": "Sun, 28 Sep 2014 03:19:22 GMT"}, {"version": "v3", "created": "Fri, 3 Apr 2015 15:08:59 GMT"}, {"version": "v4", "created": "Sun, 20 Sep 2020 23:46:01 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Van Keilegom", "Ingrid", ""], ["Crujeiras", "Rosa M.", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""]]}, {"id": "1409.0606", "submitter": "Sa\\\"id Moussaoui", "authors": "Cl\\'ement Gilavert and Sa\\\"id Moussaoui and J\\'er\\^ome Idier", "title": "Efficient Gaussian Sampling for Solving Large-Scale Inverse Problems\n  using MCMC Methods", "comments": "20 pages, 10 figures, under review for journal publication", "journal-ref": null, "doi": "10.1109/TSP.2014.2367457", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The resolution of many large-scale inverse problems using MCMC methods\nrequires a step of drawing samples from a high dimensional Gaussian\ndistribution. While direct Gaussian sampling techniques, such as those based on\nCholesky factorization, induce an excessive numerical complexity and memory\nrequirement, sequential coordinate sampling methods present a low rate of\nconvergence. Based on the reversible jump Markov chain framework, this paper\nproposes an efficient Gaussian sampling algorithm having a reduced computation\ncost and memory usage. The main feature of the algorithm is to perform an\napproximate resolution of a linear system with a truncation level adjusted\nusing a self-tuning adaptive scheme allowing to achieve the minimal computation\ncost. The connection between this algorithm and some existing strategies is\ndiscussed and its efficiency is illustrated on a linear inverse problem of\nimage resolution enhancement.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 04:23:20 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Gilavert", "Cl\u00e9ment", ""], ["Moussaoui", "Sa\u00efd", ""], ["Idier", "J\u00e9r\u00f4me", ""]]}, {"id": "1409.0643", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts", "title": "Entity Resolution with Empirically Motivated Priors", "comments": "30 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Databases often contain corrupted, degraded, and noisy data with duplicate\nentries across and within each database. Such problems arise in citations,\nmedical databases, genetics, human rights databases, and a variety of other\napplied settings. The target of statistical inference can be viewed as an\nunsupervised problem of determining the edges of a bipartite graph that links\nthe observed records to unobserved latent entities. Bayesian approaches provide\nattractive benefits, naturally providing uncertainty quantification via\nposterior probabilities. We propose a novel record linkage approach based on\nempirical Bayesian principles. Specifically, the empirical Bayesian--type step\nconsists of taking the empirical distribution function of the data as the prior\nfor the latent entities. This approach improves on the earlier HB approach not\nonly by avoiding the prior specification problem but also by allowing both\ncategorical and string-valued variables. Our extension to string-valued\nvariables also involves the proposal of a new probabilistic mechanism by which\nobserved record values for string fields can deviate from the values of their\nassociated latent entities. Categorical fields that deviate from their\ncorresponding true value are simply drawn from the empirical distribution\nfunction. We apply our proposed methodology to a simulated data set of German\nnames and an Italian household survey, showing our method performs favorably\ncompared to several standard methods in the literature. We also consider the\nrobustness of our methods to changes in the hyper-parameters.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 09:41:43 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 16:08:02 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Steorts", "Rebecca C.", ""]]}, {"id": "1409.0713", "submitter": "Ying Ding", "authors": "Ying Ding, Hui-Min Lin and Jason C. Hsu", "title": "Subgroup Mixable Inference in Personalized Medicine, with an Application\n  to Time-to-Event Outcomes", "comments": "5 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring treatment efficacy in mixture of subgroups from a randomized\nclinical trial is a fundamental problem in personalized medicine development,\nin deciding whether to treat the entire patient population or to target a\nsubgroup. We show that some commonly used efficacy measures are not suitable\nfor a mixture population. We also show that, while it is important to adjust\nfor imbalance in the data using least squares means (LSmeans) (not marginal\nmeans) estimation, the current practice of applying LSmeans to directly\nestimate the efficacy in a mixture population for any type of outcome is\ninappropriate. Proposing a new principle called {\\em subgroup mixable\nestimation}, we establish the logical relationship among parameters that\nrepresent efficacy and develop a general inference procedure to confidently\ninfer efficacy in subgroups and their mixtures. Using oncology studies with\ntime-to-event outcomes as an example, we show that Hazard Ratio is not suitable\nfor measuring efficacy in a mixture population, and provide alternative\nefficacy measures with a valid inference procedure.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 14:00:36 GMT"}], "update_date": "2014-09-03", "authors_parsed": [["Ding", "Ying", ""], ["Lin", "Hui-Min", ""], ["Hsu", "Jason C.", ""]]}, {"id": "1409.0743", "submitter": "Geir-Arne Fuglstad", "authors": "Geir-Arne Fuglstad, Daniel Simpson, Finn Lindgren and H{\\aa}vard Rue", "title": "Does non-stationary spatial data always require non-stationary random\n  fields?", "comments": "Minor change from previous version. arXiv admin note: text overlap\n  with arXiv:1306.0408", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A stationary spatial model is an idealization and we expect that the true\ndependence structures of physical phenomena are spatially varying, but how\nshould we handle this non-stationarity in practice? We study the challenges\ninvolved in applying a flexible non-stationary model to a dataset of annual\nprecipitation in the conterminous US, where exploratory data analysis shows\nstrong evidence of a non-stationary covariance structure.\n  The aim of this paper is to investigate the modelling pipeline once\nnon-stationarity has been detected in spatial data. We show that there is a\nreal danger of over-fitting the model and that careful modelling is necessary\nin order to properly account for varying second-order structure. In fact, the\nexample shows that sometimes non-stationary Gaussian random fields are not\nnecessary to model non-stationary spatial data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 15:05:14 GMT"}, {"version": "v2", "created": "Mon, 16 Feb 2015 15:09:44 GMT"}, {"version": "v3", "created": "Mon, 6 Jul 2015 09:34:07 GMT"}, {"version": "v4", "created": "Mon, 14 Sep 2015 11:55:14 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Fuglstad", "Geir-Arne", ""], ["Simpson", "Daniel", ""], ["Lindgren", "Finn", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1409.0895", "submitter": "Peng Ding", "authors": "Hua Chen, Peng Ding, Zhi Geng, and Xiao-Hua Zhou", "title": "Semiparametric Inference of the Complier Average Causal Effect with\n  Nonignorable Missing Outcomes", "comments": "Transactions on Intelligent Systems and Technology 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noncompliance and missing data often occur in randomized trials, which\ncomplicate the inference of causal effects. When both noncompliance and missing\ndata are present, previous papers proposed moment and maximum likelihood\nestimators for binary and normally distributed continuous outcomes under the\nlatent ignorable missing data mechanism. However, the latent ignorable missing\ndata mechanism may be violated in practice, because the missing data mechanism\nmay depend directly on the missing outcome itself. Under noncompliance and an\noutcome-dependent nonignorable missing data mechanism, previous studies showed\nthe identifiability of complier average causal effect for discrete outcomes. In\nthis paper, we study the semiparametric identifiability and estimation of\ncomplier average causal effect in randomized clinical trials with both\nall-or-none noncompliance and the outcome-dependent nonignorable missing\ncontinuous outcomes, and propose a two-step maximum likelihood estimator in\norder to eliminate the infinite dimensional nuisance parameter. Our method does\nnot need to specify a parametric form for the missing data mechanism. We also\nevaluate the finite sample property of our method via extensive simulation\nstudies and sensitivity analysis, with an application to a double-blinded\npsychiatric clinical trial.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 21:29:20 GMT"}], "update_date": "2014-09-04", "authors_parsed": [["Chen", "Hua", ""], ["Ding", "Peng", ""], ["Geng", "Zhi", ""], ["Zhou", "Xiao-Hua", ""]]}, {"id": "1409.0909", "submitter": "Earl Lawrence", "authors": "Kary Myers, Earl Lawrence, Michael Fugate, Claire McKay Bowen,\n  Lawrence Ticknor, Jon Woodring, Joanne Wendelberger and Jim Ahrens", "title": "Partitioning a Large Simulation as It Runs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  As computer simulations continue to grow in size and complexity, they present\na particularly challenging class of big data problems. Many application areas\nare moving toward exascale computing systems, systems that perform $10^{18}$\nFLOPS (FLoating-point Operations Per Second) --- a billion billion calculations\nper second. Simulations at this scale can generate output that exceeds both the\nstorage capacity and the bandwidth available for transfer to storage, making\npost-processing and analysis challenging. One approach is to embed some\nanalyses in the simulation while the simulation is running --- a strategy often\ncalled in situ analysis --- to reduce the need for transfer to storage. Another\nstrategy is to save only a reduced set of time steps rather than the full\nsimulation. Typically the selected time steps are evenly spaced, where the\nspacing can be defined by the budget for storage and transfer. This paper\ncombines both of these ideas to introduce an online in situ method for\nidentifying a reduced set of time steps of the simulation to save. Our approach\nsignificantly reduces the data transfer and storage requirements, and it\nprovides improved fidelity to the simulation to facilitate post-processing and\nreconstruction. We illustrate the method using a computer simulation that\nsupported NASA's 2009 Lunar Crater Observation and Sensing Satellite mission.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2014 22:40:50 GMT"}, {"version": "v2", "created": "Wed, 23 Sep 2015 23:32:20 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Myers", "Kary", ""], ["Lawrence", "Earl", ""], ["Fugate", "Michael", ""], ["Bowen", "Claire McKay", ""], ["Ticknor", "Lawrence", ""], ["Woodring", "Jon", ""], ["Wendelberger", "Joanne", ""], ["Ahrens", "Jim", ""]]}, {"id": "1409.1130", "submitter": "Roumen Varbanov", "authors": "Kelly McGinnity, Roumen Varbanov, Eric Chicken", "title": "Cross-Validated Wavelet Block Thresholding for Non-Gaussian Errors", "comments": "27 pages, 12 figures. The code included in the ancillary materials\n  was updated from the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wavelet thresholding generally assumes independent, identically distributed\nnormal errors when estimating functions in a nonparametric regression setting.\nVisuShrink and SureShrink are just two of the many common thresholding methods\nbased on this assumption. When the errors are not normally distributed,\nhowever, few methods have been proposed. A distribution-free method for\nthresholding wavelet coefficients in nonparametric regression is described,\nwhich unlike some other non-normal error thresholding methods, does not assume\nthe form of the non-normal distribution is known. Improvements are made to an\nexisting even-odd cross-validation method by employing block thresholding and\nlevel dependence. The efficiency of the proposed method on a variety of\nnon-normal errors, including comparisons to existing wavelet threshold\nestimators, is shown on both simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2014 15:40:24 GMT"}, {"version": "v2", "created": "Wed, 13 Jan 2016 19:57:16 GMT"}, {"version": "v3", "created": "Thu, 14 Jan 2016 16:15:56 GMT"}, {"version": "v4", "created": "Thu, 22 Sep 2016 19:02:47 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["McGinnity", "Kelly", ""], ["Varbanov", "Roumen", ""], ["Chicken", "Eric", ""]]}, {"id": "1409.1816", "submitter": "Alba Maria Franco Pereira", "authors": "A.M. Franco-Pereira, R.E. Lillo, J. Romo", "title": "Extremality measures and a rank test for functional data", "comments": "20pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical analysis of functional data is a growing need in many\nresearch areas. In particular, a robust methodology is important to study\ncurves, which are the output of experiments in applied statistics. In this\npaper we study some new definitions which reflect the \"extremality\" of a curve\nwith respect to a collection of functions, and provide natural orderings for\nsample curves. Their finite dimensional versions are computationally feasible\nand useful for studying high dimensional observations. Thus, these extreme\nmeasures are suitable for complex observations such as microarray data and\nimages. We show the applicability of these measures designing a rank test for\nfunctional data. This functional rank test shows different growth patterns for\nboys and girls when it is applied to children growth data.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2014 10:27:35 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Franco-Pereira", "A. M.", ""], ["Lillo", "R. E.", ""], ["Romo", "J.", ""]]}, {"id": "1409.1842", "submitter": "Robert Maidstone", "authors": "Robert Maidstone, Toby Hocking, Guillem Rigaill and Paul Fearnhead", "title": "On Optimal Multiple Changepoint Algorithms for Large Data", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing need for algorithms that can accurately detect\nchangepoints in long time-series, or equivalent, data. Many common approaches\nto detecting changepoints, for example based on penalised likelihood or minimum\ndescription length, can be formulated in terms of minimising a cost over\nsegmentations. Dynamic programming methods exist to solve this minimisation\nproblem exactly, but these tend to scale at least quadratically in the length\nof the time-series. Algorithms, such as Binary Segmentation, exist that have a\ncomputational cost that is close to linear in the length of the time-series,\nbut these are not guaranteed to find the optimal segmentation. Recently pruning\nideas have been suggested that can speed up the dynamic programming algorithms,\nwhilst still being guaranteed to find true minimum of the cost function. Here\nwe extend these pruning methods, and introduce two new algorithms for\nsegmenting data, FPOP and SNIP. Empirical results show that FPOP is\nsubstantially faster than existing dynamic programming methods, and unlike the\nexisting methods its computational efficiency is robust to the number of\nchangepoints in the data. We evaluate the method at detecting Copy Number\nVariations and observe that FPOP has a computational cost that is competitive\nwith that of Binary Segmentation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2014 15:44:34 GMT"}], "update_date": "2014-09-08", "authors_parsed": [["Maidstone", "Robert", ""], ["Hocking", "Toby", ""], ["Rigaill", "Guillem", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1409.2221", "submitter": "Zepu Zhang", "authors": "Zepu Zhang", "title": "Adaptive Anchored Inversion for Gaussian Random Fields Using Nonlinear\n  Data", "comments": "36 pages, 14 figures (some symbols in Figures 11, 12 were partially\n  damaged by arxiv processing)", "journal-ref": "Inverse Problems, 27 (2011) 125011 (31pp)", "doi": "10.1088/0266-5611/27/12/125011", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a broad and fundamental type of ''inverse problems'' in science, one\ninfers a spatially distributed physical attribute based on observations of\nprocesses that are controlled by the spatial attribute in question. The\ndata-generating field processes, known as ''forward processes'', are usually\nnonlinear with respect to the spatial attribute, and are often defined\nnon-analytically by a numerical model. The data often contain a large number of\nelements with significant inter-correlation. We propose a general statistical\nmethod to tackle this problem. The method is centered on a parameterization\ndevice called ''anchors'' and an iterative algorithm for deriving the\ndistribution of anchors conditional on the observed data. The algorithm draws\nupon techniques of importance sampling and multivariate kernel density\nestimation with weighted samples. Anchors are selected automatically; the\nselection evolves in iterations in a way that is tailored to important features\nof the attribute field. The method and the algorithm are general with respect\nto the scientific nature and technical details of the forward processes.\nConceptual and technical components render the method in contrast to standard\napproaches that are based on regularization or optimization. Some important\nfeatures of the proposed method are demonstrated by examples from the earth\nsciences, including groundwater flow, rainfall-runoff, and seismic tomography.\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 06:55:01 GMT"}], "update_date": "2014-09-09", "authors_parsed": [["Zhang", "Zepu", ""]]}, {"id": "1409.2437", "submitter": "George Karabatsos Ph.D.", "authors": "George Karabatsos", "title": "Fast Marginal Likelihood Estimation of the Ridge Parameter(s) in Ridge\n  Regression and Generalized Ridge Regression for Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike the ordinary least-squares (OLS) estimator for the linear model, a\nridge regression linear model provides coefficient estimates via shrinkage,\nusually with improved mean-square and prediction error. This is true especially\nwhen the observed design matrix is ill-conditioned or singular, either as a\nresult of highly-correlated covariates or the number of covariates exceeding\nthe sample size. This paper introduces novel and fast marginal maximum\nlikelihood (MML) algorithms for estimating the shrinkage parameter(s) for the\nBayesian ridge and power ridge regression models, and an automatic plug-in MML\nestimator for the Bayesian generalized ridge regression model. With the aid of\nthe singular value decomposition of the observed covariate design matrix, these\nMML estimation methods are quite fast even for data sets where either the\nsample size (n) or the number of covariates (p) is very large, and even when\np>n. On several real data sets varying widely in terms of n and p, the\ncomputation times of the MML estimation methods for the three ridge models,\nrespectively, are compared with the times of other methods for estimating the\nshrinkage parameter in ridge, LASSO and Elastic Net (EN) models, with the other\nmethods based on minimizing prediction error according to cross-validation or\ninformation criteria. Also, the ridge, LASSO, and EN models, and their\nassociated estimation methods, are compared in terms of prediction accuracy.\nFurthermore, a simulation study compares the ridge models under MML estimation,\nagainst the LASSO and EN models, in terms of their ability to differentiate\nbetween truly-significant covariates (i.e., with non-zero slope coefficients)\nand truly-insignificant covariates (with zero coefficients).\n", "versions": [{"version": "v1", "created": "Mon, 8 Sep 2014 17:35:27 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2015 19:12:42 GMT"}, {"version": "v3", "created": "Mon, 23 Feb 2015 23:27:43 GMT"}, {"version": "v4", "created": "Mon, 22 Jun 2015 21:27:25 GMT"}, {"version": "v5", "created": "Wed, 24 Jun 2015 02:05:50 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Karabatsos", "George", ""]]}, {"id": "1409.2599", "submitter": "Zepu Zhang", "authors": "Zepu Zhang", "title": "Iterative Posterior Inference for Bayesian Kriging", "comments": "18 pages, 6 figures", "journal-ref": "Stochastic Environmental Research and Risk Assessment, (2012)\n  26:913-923", "doi": "10.1007/s00477-011-0544-y", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for estimating the posterior distribution of a standard\ngeostatistical model. After choosing the model formulation and specifying a\nprior, we use normal mixture densities to approximate the posterior\ndistribution. The approximation is improved iteratively. Some difficulties in\nestimating the normal mixture densities, including determining tuning\nparameters concerning bandwidth and localization, are addressed. The method is\napplicable to other model formulations as long as all the parameters, or\ntransforms thereof, are defined on the whole real line, $(-\\infty, \\infty)$. Ad\nhoc treatments in the posterior inference such as imposing bounds on an\nunbounded parameter or discretizing a continuous parameter are avoided. The\nmethod is illustrated by two examples, one using digital elevation data and the\nother using historical soil moisture data. The examples in particular examine\nconvergence of the approximate posterior distributions in the iterations.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 05:31:50 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Zhang", "Zepu", ""]]}, {"id": "1409.2638", "submitter": "Nicolai Meinshausen", "authors": "Peter B\\\"uhlmann and Nicolai Meinshausen", "title": "Magging: maximin aggregation for inhomogeneous large-scale data", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale data analysis poses both statistical and computational problems\nwhich need to be addressed simultaneously. A solution is often straightforward\nif the data are homogeneous: one can use classical ideas of subsampling and\nmean aggregation to get a computationally efficient solution with acceptable\nstatistical accuracy, where the aggregation step simply averages the results\nobtained on distinct subsets of the data. However, if the data exhibit\ninhomogeneities (and typically they do), the same approach will be inadequate,\nas it will be unduly influenced by effects that are not persistent across all\nthe data due to, for example, outliers or time-varying effects. We show that a\ntweak to the aggregation step can produce an estimator of effects which are\ncommon to all data, and hence interesting for interpretation and often leading\nto better prediction than pooled effects.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 08:43:14 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["B\u00fchlmann", "Peter", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1409.2675", "submitter": "Arman Sabbaghi", "authors": "Arman Sabbaghi, Donald B. Rubin", "title": "Comments on the Neyman-Fisher Controversy and Its Consequences", "comments": "Published in at http://dx.doi.org/10.1214/13-STS454 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 267-284", "doi": "10.1214/13-STS454", "report-no": "IMS-STS-STS454", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neyman-Fisher controversy considered here originated with the 1935\npresentation of Jerzy Neyman's Statistical Problems in Agricultural\nExperimentation to the Royal Statistical Society. Neyman asserted that the\nstandard ANOVA F-test for randomized complete block designs is valid, whereas\nthe analogous test for Latin squares is invalid in the sense of detecting\ndifferentiation among the treatments, when none existed on average, more often\nthan desired (i.e., having a higher Type I error than advertised). However,\nNeyman's expressions for the expected mean residual sum of squares, for both\ndesigns, are generally incorrect. Furthermore, Neyman's belief that the Type I\nerror (when testing the null hypothesis of zero average treatment effects) is\nhigher than desired, whenever the expected mean treatment sum of squares is\ngreater than the expected mean residual sum of squares, is generally incorrect.\nSimple examples show that, without further assumptions on the potential\noutcomes, one cannot determine the Type I error of the F-test from expected\nsums of squares. Ultimately, we believe that the Neyman-Fisher controversy had\na deleterious impact on the development of statistics, with a major consequence\nbeing that potential outcomes were ignored in favor of linear models and\nclassical statistical procedures that are imprecise without applied contexts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 10:56:53 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Sabbaghi", "Arman", ""], ["Rubin", "Donald B.", ""]]}, {"id": "1409.2676", "submitter": "Max Hinne", "authors": "Max Hinne, Alex Lenkoski, Tom Heskes and Marcel van Gerven", "title": "Efficient sampling of Gaussian graphical models using conditional Bayes\n  factors", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian estimation of Gaussian graphical models has proven to be challenging\nbecause the conjugate prior distribution on the Gaussian precision matrix, the\nG-Wishart distribution, has a doubly intractable partition function. Recent\ndevelopments provide a direct way to sample from the G-Wishart distribution,\nwhich allows for more efficient algorithms for model selection than previously\npossible. Still, estimating Gaussian graphical models with more than a handful\nof variables remains a nearly infeasible task. Here, we propose two novel\nalgorithms that use the direct sampler to more efficiently approximate the\nposterior distribution of the Gaussian graphical model. The first algorithm\nuses conditional Bayes factors to compare models in a Metropolis-Hastings\nframework. The second algorithm is based on a continuous time Markov process.\nWe show that both algorithms are substantially faster than state-of-the-art\nalternatives. Finally, we show how the algorithms may be used to simultaneously\nestimate both structural and functional connectivity between subcortical brain\nregions using resting-state fMRI.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 10:57:23 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Hinne", "Max", ""], ["Lenkoski", "Alex", ""], ["Heskes", "Tom", ""], ["van Gerven", "Marcel", ""]]}, {"id": "1409.2677", "submitter": "Bradley Efron", "authors": "Bradley Efron", "title": "Two Modeling Strategies for Empirical Bayes Estimation", "comments": "Published in at http://dx.doi.org/10.1214/13-STS455 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 285-301", "doi": "10.1214/13-STS455", "report-no": "IMS-STS-STS455", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical Bayes methods use the data from parallel experiments, for instance,\nobservations $X_k\\sim\\mathcal{N}(\\Theta_k,1)$ for $k=1,2,\\ldots,N$, to estimate\nthe conditional distributions $\\Theta_k|X_k$. There are two main estimation\nstrategies: modeling on the $\\theta$ space, called \"$g$-modeling\" here, and\nmodeling on the $x$ space, called \"$f$-modeling.\" The two approaches are\ndescribed and compared. A series of computational formulas are developed to\nassess their frequentist accuracy. Several examples, both contrived and\ngenuine, show the strengths and limitations of the two strategies.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 10:58:33 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Efron", "Bradley", ""]]}, {"id": "1409.2692", "submitter": "Hans J. Haubold", "authors": "A.M. Mathai, H.J. Haubold", "title": "On a Generalized Entropy Measure Leading to the Pathway Model: with a\n  preliminary application to solar neutrino data", "comments": "19 pages, 4 figures, LaTeX. arXiv admin note: text overlap with\n  arXiv:cond-mat/0207536 by other authors", "journal-ref": "Entropy 15 (2013) 4011-4025", "doi": "10.3390/e15104011", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An entropy for the scalar variable case, parallel to Havrda-Charvat entropy\nwas introduced by the first author and the properties and its connection to\nTsallis non-extensive statistical mechanics and the Mathai pathway model were\nexamined by the authors in previous papers. In the current paper we extend the\nentropy to cover scalar case, multivariable case, and matrix variate case. Then\nthis measure is optimized under different types of restrictions and a number of\nmodels in the multivariable case and matrix variable case are obtained.\nConnections of these models to problems in statistical, physical, and\nengineering sciences are also pointed out. An application of the simplest case\nof the pathway model to the interpretation of solar neutrino data is provided.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2014 19:35:40 GMT"}], "update_date": "2014-09-11", "authors_parsed": [["Mathai", "A. M.", ""], ["Haubold", "H. J.", ""]]}, {"id": "1409.2709", "submitter": "Daniel Rudolf", "authors": "Krzysztof {\\L}atuszy\\'nski and Daniel Rudolf", "title": "Convergence of hybrid slice sampling via spectral gap", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the simple slice sampler has very robust convergence\nproperties, however the class of problems where it can be implemented is\nlimited. In contrast, we consider hybrid slice samplers which are easily\nimplementable and where another Markov chain approximately samples the uniform\ndistribution on each slice. Under appropriate assumptions on the Markov chain\non the slice we show a lower bound and an upper bound of the spectral gap of\nthe hybrid slice sampler in terms of the spectral gap of the simple slice\nsampler. An immediate consequence of this is that spectral gap and geometric\nergodicity of the hybrid slice sampler can be concluded from spectral gap and\ngeometric ergodicity of its simple version which is very well understood. These\nresults indicate that robustness properties of the simple slice sampler are\ninherited by (appropriately designed) easily implementable hybrid versions and\nprovide the first theoretical underpinning of their use in applications. We\napply the developed theory and analyse a number of specific algorithms such as\nthe stepping-out shrinkage slice sampling, hit-and-run slice sampling on very\ngeneral multidimensional targets and an easily implementable combination of\nboth procedures on fairly general and realistic multidimensional bimodal\ndensities.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 12:22:21 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["\u0141atuszy\u0144ski", "Krzysztof", ""], ["Rudolf", "Daniel", ""]]}, {"id": "1409.2768", "submitter": "Hans J. Haubold", "authors": "A.M. Mathai, H.J. Haubold", "title": "Stochastic processes via the pathway model", "comments": "15 pages, 7 figures, LaTeX", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After collecting data from observations or experiments, the next step is to\nbuild an appropriate mathematical or stochastic model to describe the data so\nthat further studies can be done with the help of the models. In this article,\nthe input-output type mechanism is considered first, where reaction, diffusion,\nreaction-diffusion, and production-destruction type physical situations can fit\nin. Then techniques are described to produce thicker or thinner tails (power\nlaw behavior) in stochastic models. Then the pathway idea is described where\none can switch to different functional forms of the probability density\nfunction) through a parameter called the pathway parameter.\n", "versions": [{"version": "v1", "created": "Sat, 6 Sep 2014 06:12:21 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Mathai", "A. M.", ""], ["Haubold", "H. J.", ""]]}, {"id": "1409.2833", "submitter": "Lina Thomas", "authors": "Lina D. Thomas, Victor Fossaluza and Anatoly Yambartsev", "title": "Building complex networks through classical and Bayesian statistics - a\n  comparison", "comments": "9 pages, 5 figures, conference Brazilian Meeting on Bayesian\n  Statistics 2012", "journal-ref": "AIP Conf. Proc. 1490, 323 (2012)", "doi": "10.1063/1.4759617", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research is about studying and comparing two different ways of building\ncomplex networks. The main goal of our study is to find an effective way to\nbuild networks, particularly when we have fewer observations than variables. We\nconstruct networks estimating the partial correlation coefficient on Classic\nStatistics (Inverse Method) and on Bayesian Statistics (Normal - Inverse\nWishart conjugate prior). In this current work, in order to solve the problem\nof having less observations than variables, we propose a new methodology called\nlocal partial correlation, which consists of selecting, for each pair of\nvariables, the other variables most correlated to the pair.We applied these\nmethods on simulated data and compared them through ROC curves. The most\nattractive result is that, even though it has high computational costs, to use\nBayesian inference on trees is better when we have less observations than\nvariables. In other cases, both approaches present satisfactory results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 18:27:41 GMT"}], "update_date": "2014-09-10", "authors_parsed": [["Thomas", "Lina D.", ""], ["Fossaluza", "Victor", ""], ["Yambartsev", "Anatoly", ""]]}, {"id": "1409.2984", "submitter": "Debashree Ray", "authors": "Debashree Ray, James S Pankow and Saonli Basu", "title": "USAT: A Unified Score-based Association Test for Multiple\n  Phenotype-Genotype Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide Association Studies (GWASs) for complex diseases often collect\ndata on multiple correlated endo-phenotypes. Multivariate analysis of these\ncorrelated phenotypes can improve the power to detect genetic variants.\nMultivariate analysis of variance (MANOVA) can perform such association\nanalysis at a GWAS level, but the behavior of MANOVA under different trait\nmodels has not been carefully investigated. In this paper, we show that MANOVA\nis generally very powerful for detecting association but there are situations,\nsuch as when a genetic variant is associated with all the traits, where MANOVA\nmay not have any detection power. We investigate the behavior of MANOVA, both\ntheoretically and using simulations, and derive the conditions where MANOVA\nloses power. Based on our findings, we propose a unified score-based test\nstatistic USAT that can perform better than MANOVA in such situations and\nnearly as well as MANOVA elsewhere. Our proposed test reports an approximate\nasymptotic p-value for association and is computationally very efficient to\nimplement at a GWAS level. We have studied through extensive simulation the\nperformance of USAT, MANOVA and other existing approaches and demonstrated the\nadvantage of using the USAT approach to detect association between a genetic\nvariant and multivariate phenotypes. We applied USAT to data from three\ncorrelated traits collected on 5,816 Caucasian individuals from the\nAtherosclerosis Risk in Communities (ARIC) Study and detected some interesting\nassociations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Sep 2014 08:30:56 GMT"}, {"version": "v2", "created": "Wed, 11 Mar 2015 02:08:45 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Ray", "Debashree", ""], ["Pankow", "James S", ""], ["Basu", "Saonli", ""]]}, {"id": "1409.3301", "submitter": "Shiqiong Huang", "authors": "Shiqiong Huang, Jiashun Jin and Zhigang Yao", "title": "Partial Correlation Screening for Estimating Large Precision Matrices,\n  with Applications to Classification", "comments": "47 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Partial Correlation Screening (PCS) as a new row-by-row approach\nto estimating a large precision matrix $\\Omega$. To estimate the $i$-th row of\n$\\Omega$, $1 \\leq i \\leq p$, PCS uses a Screen step and a Clean step. In the\nScreen step, PCS recruits a (small) subset of indices using a stage-wise\nalgorithm, where in each stage, the algorithm updates the set of recruited\nindices by adding the index $j$ that has the largest (in magnitude) empirical\npartial correlation with $i$. In the Clean step, PCS re-investigates all\nrecruited indices and use them to reconstruct the $i$-th row of $\\Omega$.\n  PCS is computationally efficient and modest in memory use: to estimate a row\nof $\\Omega$, it only needs a few rows (determined sequentially) of the\nempirical covariance matrix. This enables PCS to execute the estimation of a\nlarge precision matrix (e.g., $p=10K$) in a few minutes, and open doors to\nestimating much larger precision matrices.\n  We use PCS for classification. Higher Criticism Thresholding (HCT) is a\nrecent classifier that enjoys optimality, but to exploit its full potential in\npractice, one needs a good estimate of the precision matrix $\\Omega$. Combining\nHCT with any approach to estimating $\\Omega$ gives a new classifier: examples\ninclude HCT-PCS and HCT-glasso. We have applied HCT-PCS to two large microarray\ndata sets ($p = 8K$ and $10K$) for classification, where it not only\nsignificantly outperforms HCT-glasso, but also is competitive to the Support\nVector Machine (SVM) and Random Forest (RF). The results suggest that PCS gives\nmore useful estimates of $\\Omega$ than the glasso.\n  We set up a general theoretical framework and show that in a broad context,\nPCS fully recovers the support of $\\Omega$ and HCT-PCS yields optimal\nclassification behavior. Our proofs shed interesting light on the behavior of\nstage-wise procedures.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2014 02:43:37 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Huang", "Shiqiong", ""], ["Jin", "Jiashun", ""], ["Yao", "Zhigang", ""]]}, {"id": "1409.3531", "submitter": "John M. Chambers", "authors": "John M. Chambers", "title": "Object-Oriented Programming, Functional Programming and R", "comments": "Published in at http://dx.doi.org/10.1214/13-STS452 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 167-180", "doi": "10.1214/13-STS452", "report-no": "IMS-STS-STS452", "categories": "stat.ME cs.PL cs.SE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews some programming techniques in R that have proved useful,\nparticularly for substantial projects. These include several versions of\nobject-oriented programming, used in a large number of R packages. The review\ntries to clarify the origins and ideas behind the various versions, each of\nwhich is valuable in the appropriate context. R has also been strongly\ninfluenced by the ideas of functional programming and, in particular, by the\ndesire to combine functional with object oriented programming. To clarify how\nthis particular mix of ideas has turned out in the current R language and\nsupporting software, the paper will first review the basic ideas behind\nobject-oriented and functional programming, and then examine the evolution of R\nwith these ideas providing context. Functional programming supports\nwell-defined, defensible software giving reproducible results. Object-oriented\nprogramming is the mechanism par excellence for managing complexity while\nkeeping things simple for the user. The two paradigms have been valuable in\nsupporting major software for fitting models to data and numerous other\nstatistical applications. The paradigms have been adopted, and adapted,\ndistinctively in R. Functional programming motivates much of R but R does not\nenforce the paradigm. Object-oriented programming from a functional perspective\ndiffers from that used in non-functional languages, a distinction that needs to\nbe emphasized to avoid confusion. R initially replicated the S language from\nBell Labs, which in turn was strongly influenced by earlier program libraries.\nAt each stage, new ideas have been added, but the previous software continues\nto show its influence in the design as well. Outlining the evolution will\nfurther clarify why we currently have this somewhat unusual combination of\nideas.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2014 10:34:09 GMT"}], "update_date": "2014-09-12", "authors_parsed": [["Chambers", "John M.", ""]]}, {"id": "1409.3636", "submitter": "Wen-Yu Hua", "authors": "Wen-Yu Hua and Philip Reiss and Debashis Ghosh", "title": "Optimal Kernel Combination for Test of Independence against Local\n  Alternatives", "comments": "not working on this project anymore", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing the independence between two random variables $x$ and $y$ is an\nimportant problem in statistics and machine learning, where the kernel-based\ntests of independence is focused to address the study of dependence recently.\nThe advantage of the kernel framework rests on its flexibility in choice of\nkernel. The Hilbert-Schmidt Independence Criterion (HSIC) was shown to be\nequivalent to a class of tests, where the tests are based on different\ndistance-induced kernel pairs. In this work, we propose to select the optimal\nkernel pair by considering local alternatives, and evaluate the efficiency\nusing the quadratic time estimator of HSIC. The local alternative offers the\nadvantage that the measure of efficiency do not depend on a particular\nalternative, and only requires the knowledge of the asymptotic null\ndistribution of the test. We show in our experiments that the proposed strategy\nresults in higher power than other existing kernel selection approaches.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 02:29:54 GMT"}, {"version": "v2", "created": "Sun, 12 Apr 2015 07:18:08 GMT"}], "update_date": "2015-04-14", "authors_parsed": [["Hua", "Wen-Yu", ""], ["Reiss", "Philip", ""], ["Ghosh", "Debashis", ""]]}, {"id": "1409.3690", "submitter": "Monica Musio", "authors": "Valentina Mameli, Monica Musio and A.Philip Dawid", "title": "Comparisons of Hyv\\\"arinen and pairwise estimators in two simple linear\n  time series models", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to compare numerically the performance of two\nestimators based on Hyv\\\"arinen's local homogeneous scoring rule with that of\nthe full and the pairwise maximum likelihood estimators. In particular, two\ndifferent model settings, for which both full and pairwise maximum likelihood\nestimators can be obtained, have been considered: the first order\nautoregressive model (AR(1)) and the moving average model (MA(1)). Simulation\nstudies highlight very different behaviours for the Hyv\\\"arinen scoring rule\nestimators relative to the pairwise likelihood estimators in these two\nsettings.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 09:50:47 GMT"}], "update_date": "2014-09-15", "authors_parsed": [["Mameli", "Valentina", ""], ["Musio", "Monica", ""], ["Dawid", "A. Philip", ""]]}, {"id": "1409.3795", "submitter": "Michail Papathomas Dr", "authors": "Michail Papathomas", "title": "On the correspondence from Bayesian log-linear modelling to logistic\n  regression modelling with $g$-priors", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a set of categorical variables where at least one of them is binary.\nThe log-linear model that describes the counts in the resulting contingency\ntable implies a specific logistic regression model, with the binary variable as\nthe outcome. Within the Bayesian framework, the $g$-prior and mixtures of\n$g$-priors are commonly assigned to the parameters of a generalized linear\nmodel. We prove that assigning a $g$-prior (or a mixture of $g$-priors) to the\nparameters of a certain log-linear model designates a $g$-prior (or a mixture\nof $g$-priors) on the parameters of the corresponding logistic regression. By\nderiving an asymptotic result, and with numerical illustrations, we demonstrate\nthat when a $g$-prior is adopted, this correspondence extends to the posterior\ndistribution of the model parameters. Thus, it is valid to translate inferences\nfrom fitting a log-linear model to inferences within the logistic regression\nframework, with regard to the presence of main effects and interaction terms.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 17:13:03 GMT"}, {"version": "v2", "created": "Thu, 9 Oct 2014 13:19:45 GMT"}, {"version": "v3", "created": "Fri, 8 May 2015 09:23:27 GMT"}, {"version": "v4", "created": "Thu, 4 May 2017 17:22:35 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Papathomas", "Michail", ""]]}, {"id": "1409.3886", "submitter": "Bodhisattva Sen", "authors": "Rohit Kumar Patra, Bodhisattva Sen, Gabor Szekely", "title": "On a Nonparametric Notion of Residual and its Applications", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(X, \\mathbf{Z})$ be a continuous random vector in $\\mathbb{R} \\times\n\\mathbb{R}^d$, $d \\ge 1$. In this paper, we define the notion of a\nnonparametric residual of $X$ on $\\mathbf{Z}$ that is always independent of the\npredictor $\\mathbf{Z}$. We study its properties and show that the proposed\nnotion of residual matches with the usual residual (error) in a multivariate\nnormal regression model. Given a random vector $(X, Y, \\mathbf{Z})$ in\n$\\mathbb{R} \\times \\mathbb{R} \\times \\mathbb{R}^d$, we use this notion of\nresidual to show that the conditional independence between $X$ and $Y$, given\n$\\mathbf{Z}$, is equivalent to the mutual independence of the residuals (of $X$\non $\\mathbf{Z}$ and $Y$ on $\\mathbf{Z}$) and $\\mathbf{Z}$. This result is used\nto develop a test for conditional independence. We propose a bootstrap scheme\nto approximate the critical value of this test. We compare the proposed test,\nwhich is easily implementable, with some of the existing procedures through a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Fri, 12 Sep 2014 23:15:27 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 02:51:39 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Patra", "Rohit Kumar", ""], ["Sen", "Bodhisattva", ""], ["Szekely", "Gabor", ""]]}, {"id": "1409.4324", "submitter": "Andrea Mercatanti", "authors": "Andrea Mercatanti, Fan Li and Fabrizia Mealli", "title": "Improving Inference of Gaussian Mixtures Using Auxiliary Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expanding a lower-dimensional problem to a higher-dimensional space and then\nprojecting back is often beneficial. This article rigorously investigates this\nperspective in the context of finite mixture models, namely how to improve\ninference for mixture models by using auxiliary variables. Despite the large\nliterature in mixture models and several empirical examples, there is no\nprevious work that gives general theoretical justification for including\nauxiliary variables in mixture models, even for special cases. We provide a\ntheoretical basis for comparing inference for mixture multivariate models with\nthe corresponding inference for marginal univariate mixture models. Analytical\nresults for several special cases are established. We show that the probability\nof correctly allocating mixture memberships and the information number for the\nmeans of the primary outcome in a bivariate model with two Gaussian mixtures\nare generally larger than those in each univariate model. Simulations under a\nrange of scenarios, including misspecified models, are conducted to examine the\nimprovement. The method is illustrated by two real applications in ecology and\ncausal inference.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2014 16:51:40 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 20:14:02 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Mercatanti", "Andrea", ""], ["Li", "Fan", ""], ["Mealli", "Fabrizia", ""]]}, {"id": "1409.4512", "submitter": "Ali Mohammadian Mosammam", "authors": "A.M. Mosammam, J.T. Kent", "title": "Estimation and Testing for Covariance-Spectral Spatial-Temporal Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore a covariance spectral modelling strategy for\nspatial-temporal processes which involves a spectral approach for time but a\ncovariance approach for space.It facilitates the analysis of coherence between\nthe temporal frequency components at different spatial sites. Stein(2005)\ndeveloped a semi-parametric model within this framework.The purpose of this\npaper is to give a deeper insight into the properties of his model and to\ndevelop simple and more intuitive methods of estimation and testing. An example\nis given using the Irish wind speed data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 06:10:29 GMT"}], "update_date": "2014-09-17", "authors_parsed": [["Mosammam", "A. M.", ""], ["Kent", "J. T.", ""]]}, {"id": "1409.4696", "submitter": "Vishesh Karwa", "authors": "Vishesh Karwa, Aleksandra B. Slavkovi\\'c, Pavel Krivitsky", "title": "Differentially Private Exponential Random Graphs", "comments": "minor edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT cs.CR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose methods to release and analyze synthetic graphs in order to\nprotect privacy of individual relationships captured by the social network.\nProposed techniques aim at fitting and estimating a wide class of exponential\nrandom graph models (ERGMs) in a differentially private manner, and thus offer\nrigorous privacy guarantees. More specifically, we use the randomized response\nmechanism to release networks under $\\epsilon$-edge differential privacy. To\nmaintain utility for statistical inference, treating the original graph as\nmissing, we propose a way to use likelihood based inference and Markov chain\nMonte Carlo (MCMC) techniques to fit ERGMs to the produced synthetic networks.\nWe demonstrate the usefulness of the proposed techniques on a real data\nexample.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2014 16:47:47 GMT"}, {"version": "v2", "created": "Wed, 15 Apr 2015 04:10:44 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Karwa", "Vishesh", ""], ["Slavkovi\u0107", "Aleksandra B.", ""], ["Krivitsky", "Pavel", ""]]}, {"id": "1409.4978", "submitter": "Iain Johnston", "authors": "Ben P. Williams, Iain G. Johnston, Sarah Covshoff, Julian M. Hibberd", "title": "Phenotypic landscape inference reveals multiple evolutionary paths to\n  C$_4$ photosynthesis", "comments": null, "journal-ref": "eLife 2 e00961 (2013)", "doi": "10.7554/eLife.00961", "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  C$_4$ photosynthesis has independently evolved from the ancestral C$_3$\npathway in at least 60 plant lineages, but, as with other complex traits, how\nit evolved is unclear. Here we show that the polyphyletic appearance of C$_4$\nphotosynthesis is associated with diverse and flexible evolutionary paths that\ngroup into four major trajectories. We conducted a meta-analysis of 18 lineages\ncontaining species that use C$_3$, C$_4$, or intermediate C$_3$-C$_4$ forms of\nphotosynthesis to parameterise a 16-dimensional phenotypic landscape. We then\ndeveloped and experimentally verified a novel Bayesian approach based on a\nhidden Markov model that predicts how the C$_4$ phenotype evolved. The\nalternative evolutionary histories underlying the appearance of C$_4$\nphotosynthesis were determined by ancestral lineage and initial phenotypic\nalterations unrelated to photosynthesis. We conclude that the order of C$_4$\ntrait acquisition is flexible and driven by non-photosynthetic drivers. This\nflexibility will have facilitated the convergent evolution of this complex\ntrait.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 12:59:01 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Williams", "Ben P.", ""], ["Johnston", "Iain G.", ""], ["Covshoff", "Sarah", ""], ["Hibberd", "Julian M.", ""]]}, {"id": "1409.5009", "submitter": "Ming Yuan", "authors": "Luwan Zhang, Grace Wahba and Ming Yuan", "title": "Distance Shrinkage and Euclidean Embedding via Regularized Kernel\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recovering an Euclidean distance matrix from noisy observations is a\ncommon problem in practice, how well this could be done remains largely\nunknown. To fill in this void, we study a simple distance matrix estimate based\nupon the so-called regularized kernel estimate. We show that such an estimate\ncan be characterized as simply applying a constant amount of shrinkage to all\nobserved pairwise distances. This fact allows us to establish risk bounds for\nthe estimate implying that the true distances can be estimated consistently in\nan average sense as the number of objects increases. In addition, such a\ncharacterization suggests an efficient algorithm to compute the distance matrix\nestimator, as an alternative to the usual second order cone programming known\nnot to scale well for large problems. Numerical experiments and an application\nin visualizing the diversity of Vpu protein sequences from a recent HIV-1 study\nfurther demonstrate the practical merits of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2014 14:42:28 GMT"}], "update_date": "2014-09-18", "authors_parsed": [["Zhang", "Luwan", ""], ["Wahba", "Grace", ""], ["Yuan", "Ming", ""]]}, {"id": "1409.5178", "submitter": "Yu Nishiyama", "authors": "Yu Nishiyama and Motonobu Kanagawa and Arthur Gretton and Kenji\n  Fukumizu", "title": "Model-based Kernel Sum Rule: Kernel Bayesian Inference with\n  Probabilistic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel Bayesian inference is a principled approach to nonparametric inference\nin probabilistic graphical models, where probabilistic relationships between\nvariables are learned from data in a nonparametric manner. Various algorithms\nof kernel Bayesian inference have been developed by combining kernelized basic\nprobabilistic operations such as the kernel sum rule and kernel Bayes' rule.\nHowever, the current framework is fully nonparametric, and it does not allow a\nuser to flexibly combine nonparametric and model-based inferences. This is\ninefficient when there are good probabilistic models (or simulation models)\navailable for some parts of a graphical model; this is in particular true in\nscientific fields where \"models\" are the central topic of study. Our\ncontribution in this paper is to introduce a novel approach, termed the {\\em\nmodel-based kernel sum rule} (Mb-KSR), to combine a probabilistic model and\nkernel Bayesian inference. By combining the Mb-KSR with the existing kernelized\nprobabilistic rules, one can develop various algorithms for hybrid (i.e.,\nnonparametric and model-based) inferences. As an illustrative example, we\nconsider Bayesian filtering in a state space model, where typically there\nexists an accurate probabilistic model for the state transition process. We\npropose a novel filtering method that combines model-based inference for the\nstate transition process and data-driven, nonparametric inference for the\nobservation generating process. We empirically validate our approach with\nsynthetic and real-data experiments, the latter being the problem of\nvision-based mobile robot localization in robotics, which illustrates the\neffectiveness of the proposed hybrid approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 02:14:00 GMT"}, {"version": "v2", "created": "Thu, 5 Apr 2018 09:30:22 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 07:01:05 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Nishiyama", "Yu", ""], ["Kanagawa", "Motonobu", ""], ["Gretton", "Arthur", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1409.5391", "submitter": "Ashley Petersen", "authors": "Ashley Petersen, Daniela Witten, and Noah Simon", "title": "Fused Lasso Additive Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of predicting an outcome variable using $p$\ncovariates that are measured on $n$ independent observations, in the setting in\nwhich flexible and interpretable fits are desirable. We propose the fused lasso\nadditive model (FLAM), in which each additive function is estimated to be\npiecewise constant with a small number of adaptively-chosen knots. FLAM is the\nsolution to a convex optimization problem, for which a simple algorithm with\nguaranteed convergence to the global optimum is provided. FLAM is shown to be\nconsistent in high dimensions, and an unbiased estimator of its degrees of\nfreedom is proposed. We evaluate the performance of FLAM in a simulation study\nand on two data sets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 18:02:17 GMT"}], "update_date": "2014-09-19", "authors_parsed": [["Petersen", "Ashley", ""], ["Witten", "Daniela", ""], ["Simon", "Noah", ""]]}, {"id": "1409.5450", "submitter": "Amanda Mejia", "authors": "Amanda F. Mejia, Mary Beth Nebel, Haochang Shou, Ciprian M.\n  Crainiceanu, James J. Pekar, Stewart Mostofsky, Brian Caffo and Martin A.\n  Lindquist", "title": "Improving Reliability of Subject-Level Resting-State fMRI Parcellation\n  with Shrinkage Estimators", "comments": "body 21 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent interest in resting state functional magnetic resonance imaging\n(rsfMRI) lies in subdividing the human brain into anatomically and functionally\ndistinct regions of interest. For example, brain parcellation is often used for\ndefining the network nodes in connectivity studies. While inference has\ntraditionally been performed on group-level data, there is a growing interest\nin parcellating single subject data. However, this is difficult due to the low\nsignal-to-noise ratio of rsfMRI data, combined with typically short scan\nlengths. A large number of brain parcellation approaches employ clustering,\nwhich begins with a measure of similarity or distance between voxels. The goal\nof this work is to improve the reproducibility of single-subject parcellation\nusing shrinkage estimators of such measures, allowing the noisy\nsubject-specific estimator to \"borrow strength\" in a principled manner from a\nlarger population of subjects. We present several empirical Bayes shrinkage\nestimators and outline methods for shrinkage when multiple scans are not\navailable for each subject. We perform shrinkage on raw intervoxel correlation\nestimates and use both raw and shrinkage estimates to produce parcellations by\nperforming clustering on the voxels. Our proposed method is agnostic to the\nchoice of clustering method and can be used as a pre-processing step for any\nclustering algorithm. Using two datasets---a simulated dataset where the true\nparcellation is known and is subject-specific and a test-retest dataset\nconsisting of two 7-minute rsfMRI scans from 20 subjects---we show that\nparcellations produced from shrinkage correlation estimates have higher\nreliability and validity than those produced from raw estimates. Application to\ntest-retest data shows that using shrinkage estimators increases the\nreproducibility of subject-specific parcellations of the motor cortex by up to\n30%.\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2014 20:13:21 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 18:56:02 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Mejia", "Amanda F.", ""], ["Nebel", "Mary Beth", ""], ["Shou", "Haochang", ""], ["Crainiceanu", "Ciprian M.", ""], ["Pekar", "James J.", ""], ["Mostofsky", "Stewart", ""], ["Caffo", "Brian", ""], ["Lindquist", "Martin A.", ""]]}, {"id": "1409.5732", "submitter": "Lixing Zhu", "authors": "Junlong Zhao, Hongyu Zhao and Lixing Zhu", "title": "Estimation for ultra-high dimensional factor model: a pivotal variable\n  detection based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For factor model, the involved covariance matrix often has no row sparse\nstructure because the common factors may lead some variables to strongly\nassociate with many others. Under the ultra-high dimensional paradigm, this\nfeature causes existing methods for sparse covariance matrix in the literature\nnot directly applicable. In this paper, for general covariance matrix, a novel\napproach to detect these variables that is called the pivotal variables is\nsuggested. Then, two-stage estimation procedures are proposed to handle\nultra-high dimensionality in factor model. In these procedures, pivotal\nvariable detection is performed as a screening step and then existing\napproaches are applied to refine the working model. The estimation efficiency\ncan be promoted under weaker assumptions on the model structure. Simulations\nare conducted to examine the performance of the new method and a real dataset\nis analysed for illustration.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2014 17:20:17 GMT"}], "update_date": "2014-09-22", "authors_parsed": [["Zhao", "Junlong", ""], ["Zhao", "Hongyu", ""], ["Zhu", "Lixing", ""]]}, {"id": "1409.5914", "submitter": "Tsuyoshi Kunihama", "authors": "T. Kunihama, A. H. Herring, C. T. Halpern and D. B. Dunson", "title": "Nonparametric Bayes modeling with sample survey weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In population studies, it is standard to sample data via designs in which the\npopulation is divided into strata, with the different strata assigned different\nprobabilities of inclusion. Although there have been some proposals for\nincluding sample survey weights into Bayesian analyses, existing methods\nrequire complex models or ignore the stratified design underlying the survey\nweights. We propose a simple approach based on modeling the distribution of the\nselected sample as a mixture, with the mixture weights appropriately adjusted,\nwhile accounting for uncertainty in the adjustment. We focus for simplicity on\nDirichlet process mixtures but the proposed approach can be applied more\nbroadly. We sketch a simple Markov chain Monte Carlo algorithm for computation,\nand assess the approach via simulations and an application.\n", "versions": [{"version": "v1", "created": "Sat, 20 Sep 2014 20:07:18 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 20:36:36 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Kunihama", "T.", ""], ["Herring", "A. H.", ""], ["Halpern", "C. T.", ""], ["Dunson", "D. B.", ""]]}, {"id": "1409.6019", "submitter": "Antonio Punzo", "authors": "Antonio Punzo and Paul D. McNicholas", "title": "Robust Clustering in Regression Analysis via the Contaminated Gaussian\n  Cluster-Weighted Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian cluster-weighted model (CWM) is a mixture of regression models\nwith random covariates that allows for flexible clustering of a random vector\ncomposed of response variables and covariates. In each mixture component, it\nadopts a Gaussian distribution for both the covariates and the responses given\nthe covariates. To robustify the approach with respect to possible elliptical\nheavy tailed departures from normality, due to the presence of atypical\nobservations, the contaminated Gaussian CWM is here introduced. In addition to\nthe parameters of the Gaussian CWM, each mixture component of our contaminated\nCWM has a parameter controlling the proportion of outliers, one controlling the\nproportion of leverage points, one specifying the degree of contamination with\nrespect to the response variables, and one specifying the degree of\ncontamination with respect to the covariates. Crucially, these parameters do\nnot have to be specified a priori, adding flexibility to our approach.\nFurthermore, once the model is estimated and the observations are assigned to\nthe groups, a finer intra-group classification in typical points, outliers,\ngood leverage points, and bad leverage points - concepts of primary importance\nin robust regression analysis - can be directly obtained. Relations with other\nmixture-based contaminated models are analyzed, identifiability conditions are\nprovided, an expectation-conditional maximization algorithm is outlined for\nparameter estimation, and various implementation and operational issues are\ndiscussed. Properties of the estimators of the regression coefficients are\nevaluated through Monte Carlo experiments and compared to the estimators from\nthe Gaussian CWM. A sensitivity study is also conducted based on a real data\nset.\n", "versions": [{"version": "v1", "created": "Sun, 21 Sep 2014 17:31:35 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Punzo", "Antonio", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1409.6219", "submitter": "Christophe Ley", "authors": "Christophe Ley", "title": "Flexible modelling in statistics: past, present and future", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In times where more and more data become available and where the data exhibit\nrather complex structures (significant departure from symmetry, heavy or light\ntails), flexible modelling has become an essential task for statisticians as\nwell as researchers and practitioners from domains such as economics, finance\nor environmental sciences. This is reflected by the wealth of existing\nproposals for flexible distributions; well-known examples are Azzalini's\nskew-normal, Tukey's $g$-and-$h$, mixture and two-piece distributions, to cite\nbut these. My aim in the present paper is to provide an introduction to this\nresearch field, intended to be useful both for novices and professionals of the\ndomain. After a description of the research stream itself, I will narrate the\ngripping history of flexible modelling, starring emblematic heroes from the\npast such as Edgeworth and Pearson, then depict three of the most used flexible\nfamilies of distributions, and finally provide an outlook on future flexible\nmodelling research by posing challenging open questions.\n", "versions": [{"version": "v1", "created": "Mon, 22 Sep 2014 15:59:26 GMT"}], "update_date": "2014-09-23", "authors_parsed": [["Ley", "Christophe", ""]]}, {"id": "1409.6441", "submitter": "Edith Gabriel", "authors": "Edith Gabriel and Florent Bonneu and Pascal Monestiez and Joel\n  Chadoeuf", "title": "Predicting the intensity of partially observed data from a revisited\n  kriging for point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a stationary and isotropic spatial point process, whose a\nrealisation is observed within a large window. In order to predict its local\nintensity, we propose to define the first- and second-order characteristics of\na random field, defined as the regularized counting process, from the ones of\nthe point process and to interpolate the intensity by using a revisited kriging\nof the regularized process.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 08:17:34 GMT"}, {"version": "v2", "created": "Tue, 15 Dec 2015 09:55:11 GMT"}], "update_date": "2015-12-16", "authors_parsed": [["Gabriel", "Edith", ""], ["Bonneu", "Florent", ""], ["Monestiez", "Pascal", ""], ["Chadoeuf", "Joel", ""]]}, {"id": "1409.6444", "submitter": "Ladislav Kristoufek", "authors": "Ladislav Kristoufek", "title": "On the interplay between short and long term memory in the power-law\n  cross-correlations setting", "comments": "6 pages", "journal-ref": "Physica A: Statistical Mechanics and Its Applications 421, pp.\n  218-222 (2015)", "doi": "10.1016/j.physa.2014.11.040", "report-no": null, "categories": "stat.ME physics.data-an q-fin.ST", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We focus on emergence of the power-law cross-correlations from processes with\nboth short and long term memory properties. In the case of correlated\nerror-terms, the power-law decay of the cross-correlation function comes\nautomatically with the characteristics of separate processes. Bivariate Hurst\nexponent is then equal to an average of separate Hurst exponents of the\nanalyzed processes. Strength of short term memory has no effect on these\nasymptotic properties. Implications of these findings for the power-law\ncross-correlations concept are further discussed.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 08:21:06 GMT"}, {"version": "v2", "created": "Thu, 4 Dec 2014 21:15:37 GMT"}], "update_date": "2014-12-11", "authors_parsed": [["Kristoufek", "Ladislav", ""]]}, {"id": "1409.6498", "submitter": "Moo K. Chung", "authors": "Moo K. Chung, Anqi Qiu, Seongho Seo, Houri K. Vorperian", "title": "Unified Heat Kernel Regression for Diffusion, Kernel Smoothing and\n  Wavelets on Manifolds and Its Application to Mandible Growth Modeling in CT\n  Images", "comments": "Accepted in Medical Image Analysis", "journal-ref": "Medical Image Analysis 2015 22:63-76", "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel kernel regression framework for smoothing scalar surface\ndata using the Laplace-Beltrami eigenfunctions. Starting with the heat kernel\nconstructed from the eigenfunctions, we formulate a new bivariate kernel\nregression framework as a weighted eigenfunction expansion with the heat kernel\nas the weights. The new kernel regression is mathematically equivalent to\nisotropic heat diffusion, kernel smoothing and recently popular diffusion\nwavelets. Unlike many previous partial differential equation based approaches\ninvolving diffusion, our approach represents the solution of diffusion\nanalytically, reducing numerical inaccuracy and slow convergence. The numerical\nimplementation is validated on a unit sphere using spherical harmonics. As an\nillustration, we have applied the method in characterizing the localized growth\npattern of mandible surfaces obtained in CT images from subjects between ages 0\nand 20 years by regressing the length of displacement vectors with respect to\nthe template surface.\n", "versions": [{"version": "v1", "created": "Tue, 23 Sep 2014 11:45:19 GMT"}, {"version": "v2", "created": "Tue, 17 Feb 2015 11:21:50 GMT"}], "update_date": "2016-06-30", "authors_parsed": [["Chung", "Moo K.", ""], ["Qiu", "Anqi", ""], ["Seo", "Seongho", ""], ["Vorperian", "Houri K.", ""]]}, {"id": "1409.6981", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Unsupervised learning of regression mixture models with unknown number\n  of components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression mixture models are widely studied in statistics, machine learning\nand data analysis. Fitting regression mixtures is challenging and is usually\nperformed by maximum likelihood by using the expectation-maximization (EM)\nalgorithm. However, it is well-known that the initialization is crucial for EM.\nIf the initialization is inappropriately performed, the EM algorithm may lead\nto unsatisfactory results. The EM algorithm also requires the number of\nclusters to be given a priori; the problem of selecting the number of mixture\ncomponents requires using model selection criteria to choose one from a set of\npre-estimated candidate models. We propose a new fully unsupervised algorithm\nto learn regression mixture models with unknown number of components. The\ndeveloped unsupervised learning approach consists in a penalized maximum\nlikelihood estimation carried out by a robust expectation-maximization (EM)\nalgorithm for fitting polynomial, spline and B-spline regressions mixtures. The\nproposed learning approach is fully unsupervised: 1) it simultaneously infers\nthe model parameters and the optimal number of the regression mixture\ncomponents from the data as the learning proceeds, rather than in a two-fold\nscheme as in standard model-based clustering using afterward model selection\ncriteria, and 2) it does not require accurate initialization unlike the\nstandard EM for regression mixtures. The developed approach is applied to curve\nclustering problems. Numerical experiments on simulated data show that the\nproposed robust EM algorithm performs well and provides accurate results in\nterms of robustness with regard initialization and retrieving the optimal\npartition with the actual number of clusters. An application to real data in\nthe framework of functional data clustering, confirms the benefit of the\nproposed approach for practical applications.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 14:55:00 GMT"}], "update_date": "2014-09-25", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1409.7086", "submitter": "Sean Simpson", "authors": "Sean L. Simpson and Paul J. Laurienti", "title": "A Two-Part Mixed-Effects Modeling Framework For Analyzing Whole-Brain\n  Network Data", "comments": null, "journal-ref": "NeuroImage 113, 310-319, 2015", "doi": null, "report-no": null, "categories": "stat.AP q-bio.NC q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whole-brain network analyses remain the vanguard in neuroimaging research,\ncoming to prominence within the last decade. Network science approaches have\nfacilitated these analyses and allowed examining the brain as an integrated\nsystem. However, statistical methods for modeling and comparing groups of\nnetworks have lagged behind. Fusing multivariate statistical approaches with\nnetwork science presents the best path to develop these methods. Toward this\nend, we propose a two-part mixed-effects modeling framework that allows\nmodeling both the probability of a connection (presence/absence of an edge) and\nthe strength of a connection if it exists. Models within this framework enable\nquantifying the relationship between an outcome (e.g., disease status) and\nconnectivity patterns in the brain while reducing spurious correlations through\ninclusion of confounding covariates. They also enable prediction about an\noutcome based on connectivity structure and vice versa, simulating networks to\ngain a better understanding of normal ranges of topological variability, and\nthresholding networks leveraging group information. Thus, they provide a\ncomprehensive approach to studying system level brain properties to further our\nunderstanding of normal and abnormal brain function.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2014 20:17:08 GMT"}], "update_date": "2015-05-04", "authors_parsed": [["Simpson", "Sean L.", ""], ["Laurienti", "Paul J.", ""]]}, {"id": "1409.7158", "submitter": "Juhee Lee", "authors": "Juhee Lee, Peter Mueller, Subhajit Sengupta, Kamalakar Gulukota, Yuan\n  Ji", "title": "Bayesian Inference for Tumor Subclones Accounting for Sequencing and\n  Structural Variants", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tumor samples are heterogeneous. They consist of different subclones that are\ncharacterized by differences in DNA nucleotide sequences and copy numbers on\nmultiple loci. Heterogeneity can be measured through the identification of the\nsubclonal copy number and sequence at a selected set of loci. Understanding\nthat the accurate identification of variant allele fractions greatly depends on\na precise determination of copy numbers, we develop a Bayesian feature\nallocation model for jointly calling subclonal copy numbers and the\ncorresponding allele sequences for the same loci. The proposed method utilizes\nthree random matrices, L, Z and w to represent subclonal copy numbers (L),\nnumbers of subclonal variant alleles (Z) and cellular fractions of subclones in\nsamples (w), respectively. The unknown number of subclones implies a random\nnumber of columns for these matrices. We use next-generation sequencing data to\nestimate the subclonal structures through inference on these three matrices.\nUsing simulation studies and a real data analysis, we demonstrate how posterior\ninference on the subclonal structure is enhanced with the joint modeling of\nboth structure and sequencing variants on subclonal genomes. Software is\navailable at http://compgenome.org/BayClone2.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 05:22:17 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Lee", "Juhee", ""], ["Mueller", "Peter", ""], ["Sengupta", "Subhajit", ""], ["Gulukota", "Kamalakar", ""], ["Ji", "Yuan", ""]]}, {"id": "1409.7187", "submitter": "Arnoud den Boer", "authors": "Arnoud V. den Boer and Michel Mandjes", "title": "Convergence rates of Laplace-transform based estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating probabilities of the form\n$\\mathbb{P}(Y \\leq w)$, for a given value of $w$, in the situation that a\nsample of i.i.d.\\ observations $X_1, \\ldots, X_n$ of $X$ is available, and\nwhere we explicitly know a functional relation between the Laplace transforms\nof the non-negative random variables $X$ and $Y$. A plug-in estimator is\nconstructed by calculating the Laplace transform of the empirical distribution\nof the sample $X_1, \\ldots, X_n$, applying the functional relation to it, and\nthen (if possible) inverting the resulting Laplace transform and evaluating it\nin $w$. We show, under mild regularity conditions, that the resulting estimator\nis weakly consistent and has expected absolute estimation error $O(n^{-1/2}\n\\log(n+1))$. We illustrate our results by two examples: in the first we\nestimate the distribution of the workload in an M/G/1 queue from observations\nof the input in fixed time intervals, and in the second we identify the\ndistribution of the increments when observing a compound Poisson process at\nequidistant points in time (usually referred to as `decompounding').\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 08:56:53 GMT"}, {"version": "v2", "created": "Mon, 24 Aug 2015 07:06:21 GMT"}, {"version": "v3", "created": "Fri, 29 Jan 2016 11:09:27 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Boer", "Arnoud V. den", ""], ["Mandjes", "Michel", ""]]}, {"id": "1409.7419", "submitter": "Cl(\\'a)udia Silvestre", "authors": "Cl\\'audia Silvestre, Margarida G. M. S. Cardoso and M\\'ario A. T.\n  Figueiredo", "title": "Identifying the number of clusters in discrete mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on cluster analysis for categorical data continues to develop, with\nnew clustering algorithms being proposed. However, in this context, the\ndetermination of the number of clusters is rarely addressed. In this paper, we\npropose a new approach in which clustering of categorical data and the\nestimation of the number of clusters is carried out simultaneously. Assuming\nthat the data originate from a finite mixture of multinomial distributions, we\ndevelop a method to select the number of mixture components based on a minimum\nmessage length (MML) criterion and implement a new expectation-maximization\n(EM) algorithm to estimate all the model parameters. The proposed EM-MML\napproach, rather than selecting one among a set of pre-estimated candidate\nmodels (which requires running EM several times), seamlessly integrates\nestimation and model selection in a single algorithm. The performance of the\nproposed approach is compared with other well-known criteria (such as the\nBayesian information criterion-BIC), resorting to synthetic data and to two\nreal applications from the European Social Survey. The EM-MML computation time\nis a clear advantage of the proposed method. Also, the real data solutions are\nmuch more parsimonious than the solutions provided by competing methods, which\nreduces the risk of model order overestimation and increases interpretability.\n", "versions": [{"version": "v1", "created": "Thu, 25 Sep 2014 21:03:00 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Silvestre", "Cl\u00e1udia", ""], ["Cardoso", "Margarida G. M. S.", ""], ["Figueiredo", "M\u00e1rio A. T.", ""]]}, {"id": "1409.7458", "submitter": "Jiantao Jiao", "authors": "Jiantao Jiao, Kartik Venkat, Yanjun Han, Tsachy Weissman", "title": "Beyond Maximum Likelihood: from Theory to Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood is the most widely used statistical estimation technique.\nRecent work by the authors introduced a general methodology for the\nconstruction of estimators for functionals in parametric models, and\ndemonstrated improvements - both in theory and in practice - over the maximum\nlikelihood estimator (MLE), particularly in high dimensional scenarios\ninvolving parameter dimension comparable to or larger than the number of\nsamples. This approach to estimation, building on results from approximation\ntheory, is shown to yield minimax rate-optimal estimators for a wide class of\nfunctionals, implementable with modest computational requirements. In a\nnutshell, a message of this recent work is that, for a wide class of\nfunctionals, the performance of these essentially optimal estimators with $n$\nsamples is comparable to that of the MLE with $n \\ln n$ samples.\n  In the present paper, we highlight the applicability of the aforementioned\nmethodology to statistical problems beyond functional estimation, and show that\nit can yield substantial gains. For example, we demonstrate that for learning\ntree-structured graphical models, our approach achieves a significant reduction\nof the required data size compared with the classical Chow--Liu algorithm,\nwhich is an implementation of the MLE, to achieve the same accuracy. The key\nstep in improving the Chow--Liu algorithm is to replace the empirical mutual\ninformation with the estimator for mutual information proposed by the authors.\nFurther, applying the same replacement approach to classical Bayesian network\nclassification, the resulting classifiers uniformly outperform the previous\nclassifiers on 26 widely used datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 01:45:34 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Jiao", "Jiantao", ""], ["Venkat", "Kartik", ""], ["Han", "Yanjun", ""], ["Weissman", "Tsachy", ""]]}, {"id": "1409.7672", "submitter": "Dennis Leung", "authors": "Dennis Leung, Mathias Drton", "title": "Order-invariant prior specification in Bayesian factor analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In (exploratory) factor analysis, the loading matrix is identified only up to\northogonal rotation. For identifiability, one thus often takes the loading\nmatrix to be lower triangular with positive diagonal entries. In Bayesian\ninference, a standard practice is then to specify a prior under which the\nloadings are independent, the off-diagonal loadings are normally distributed,\nand the diagonal loadings follow a truncated normal distribution. This prior\nspecification, however, depends in an important way on how the variables and\nassociated rows of the loading matrix are ordered. We show how a minor\nmodification of the approach allows one to compute with the identifiable lower\ntriangular loading matrix but maintain invariance properties under reordering\nof the variables.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2014 19:18:43 GMT"}], "update_date": "2014-09-29", "authors_parsed": [["Leung", "Dennis", ""], ["Drton", "Mathias", ""]]}, {"id": "1409.7776", "submitter": "Wei Gao", "authors": "Wei Gao, Wicher Bergsma and Qiwei Yao", "title": "Estimation for Dynamic and Static Panel Probit Models with Large\n  Individual Effects", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For discrete panel data, the dynamic relationship between successive\nobservations is often of interest. We consider a dynamic probit model for short\npanel data. A problem with estimating the dynamic parameter of interest is that\nthe model contains a large number of nuisance parameters, one for each\nindividual. Heckman proposed to use maximum likelihood estimation of the\ndynamic parameter, which, however, does not perform well if the individual\neffects are large. We suggest new estimators for the dynamic parameter, based\non the assumption that the individual parameters are random and possibly large.\nTheoretical properties of our estimators are derived and a simulation study\nshows they have some advantages compared to Heckman's estimator.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2014 07:25:19 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Gao", "Wei", ""], ["Bergsma", "Wicher", ""], ["Yao", "Qiwei", ""]]}, {"id": "1409.7986", "submitter": "Daniel Paulin", "authors": "Benjamin M. Gyori and Daniel Paulin", "title": "Hypothesis testing for Markov chain Monte Carlo", "comments": "21 pages, 8 figures", "journal-ref": "Statistics and Computing, Springer. July 2015, pages 1-12", "doi": "10.1007/s11222-015-9594-1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing between hypotheses, when independent sampling is possible, is a well\ndeveloped subject. In this paper, we propose hypothesis tests that are\napplicable when the samples are obtained using Markov chain Monte Carlo. These\ntests are useful when one is interested in deciding whether the expected value\nof a certain quantity is above or below a given threshold. We show\nnon-asymptotic error bounds and bounds on the expected number of samples for\nthree types of tests, a fixed sample size test, a sequential test with\nindifference region, and a sequential test without indifference region. Our\ntests can lead to significant savings in sample size. We illustrate our results\non an example of Bayesian parameter inference involving an ODE model of a\nbiochemical pathway.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 03:08:09 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 09:42:29 GMT"}], "update_date": "2015-08-14", "authors_parsed": [["Gyori", "Benjamin M.", ""], ["Paulin", "Daniel", ""]]}, {"id": "1409.8185", "submitter": "Theodoros Tsiligkaridis", "authors": "Theodoros Tsiligkaridis, Keith W. Forsythe", "title": "Adaptive Low-Complexity Sequential Inference for Dirichlet Process\n  Mixture Models", "comments": "25 pages, To appear in Advances in Neural Information Processing\n  Systems (NIPS) 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a sequential low-complexity inference procedure for Dirichlet\nprocess mixtures of Gaussians for online clustering and parameter estimation\nwhen the number of clusters are unknown a-priori. We present an easily\ncomputable, closed form parametric expression for the conditional likelihood,\nin which hyperparameters are recursively updated as a function of the streaming\ndata assuming conjugate priors. Motivated by large-sample asymptotics, we\npropose a novel adaptive low-complexity design for the Dirichlet process\nconcentration parameter and show that the number of classes grow at most at a\nlogarithmic rate. We further prove that in the large-sample limit, the\nconditional likelihood and data predictive distribution become asymptotically\nGaussian. We demonstrate through experiments on synthetic and real data sets\nthat our approach is superior to other online state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 16:47:44 GMT"}, {"version": "v2", "created": "Thu, 5 Feb 2015 15:55:06 GMT"}, {"version": "v3", "created": "Fri, 11 Sep 2015 20:07:31 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Tsiligkaridis", "Theodoros", ""], ["Forsythe", "Keith W.", ""]]}, {"id": "1409.8198", "submitter": "Pratyaydipta Rudra", "authors": "Pratyaydipta Rudra and Fred A. Wright", "title": "A procedure to detect general association based on concentration of\n  ranks", "comments": "The Supplementary article can be found at the end of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern high-throughput applications, it is important to identify pairwise\nassociations between variables, and desirable to use methods that are powerful\nand sensitive to a variety of association relationships. We describe RankCover,\na new non-parametric association test for association between two variables\nthat measures the concentration of paired ranked points. Here `concentration'\nis quantified using a disk-covering statistic that is similar to those employed\nin spatial data analysis. Analysis of simulated datasets demonstrates that the\nmethod is robust and often powerful in comparison to competing general\nassociation tests. We illustrate RankCover in the analysis of several real\ndatasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2014 17:21:11 GMT"}], "update_date": "2014-09-30", "authors_parsed": [["Rudra", "Pratyaydipta", ""], ["Wright", "Fred A.", ""]]}, {"id": "1409.8363", "submitter": "Gael Martin Prof", "authors": "Gael M. Martin, Brendan P.M. McCabe, Worapree Maneesoonthorn and\n  Christian P. Robert", "title": "Approximate Bayesian Computation in State Space Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to inference in state space models is proposed, based on\napproximate Bayesian computation (ABC). ABC avoids evaluation of the likelihood\nfunction by matching observed summary statistics with statistics computed from\ndata simulated from the true process; exact inference being feasible only if\nthe statistics are sufficient. With finite sample sufficiency unattainable in\nthe state space setting, we seek asymptotic sufficiency via the maximum\nlikelihood estimator (MLE) of the parameters of an auxiliary model. We prove\nthat this auxiliary model-based approach achieves Bayesian consistency, and\nthat - in a precise limiting sense - the proximity to (asymptotic) sufficiency\nyielded by the MLE is replicated by the score. In multiple parameter settings a\nseparate treatment of scalar parameters, based on integrated likelihood\ntechniques, is advocated as a way of avoiding the curse of dimensionality. Some\nattention is given to a structure in which the state variable is driven by a\ncontinuous time process, with exact inference typically infeasible in this case\nas a result of intractable transitions. The ABC method is demonstrated using\nthe unscented Kalman filter as a fast and simple way of producing an\napproximation in this setting, with a stochastic volatility model for financial\nreturns used for illustration.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 02:11:25 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Martin", "Gael M.", ""], ["McCabe", "Brendan P. M.", ""], ["Maneesoonthorn", "Worapree", ""], ["Robert", "Christian P.", ""]]}, {"id": "1409.8415", "submitter": "Vincent Carey", "authors": "Vincent Carey, Dianne Cook", "title": "Four Papers on Contemporary Software Design Strategies for Statistical\n  Methodologists", "comments": "Published in at http://dx.doi.org/10.1214/14-STS481 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 2, 165-166", "doi": "10.1214/14-STS481", "report-no": "IMS-STS-STS481", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software design impacts much of statistical analysis and, as technology\nchanges, dramatically so in recent years, it is exciting to learn how\nstatistical software is adapting and changing. This leads to the collection of\npapers published here, written by John Chambers, Duncan Temple Lang, Michael\nLawrence, Martin Morgan, Yihui Xie, Heike Hofmann and Xiaoyue Cheng.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 07:58:34 GMT"}], "update_date": "2014-10-01", "authors_parsed": [["Carey", "Vincent", ""], ["Cook", "Dianne", ""]]}, {"id": "1409.8437", "submitter": "Ingo Steinwart", "authors": "Ingo Steinwart", "title": "Fully adaptive density-based clustering", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1331 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 5, 2132-2167", "doi": "10.1214/15-AOS1331", "report-no": "IMS-AOS-AOS1331", "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The clusters of a distribution are often defined by the connected components\nof a density level set. However, this definition depends on the user-specified\nlevel. We address this issue by proposing a simple, generic algorithm, which\nuses an almost arbitrary level set estimator to estimate the smallest level at\nwhich there are more than one connected components. In the case where this\nalgorithm is fed with histogram-based level set estimates, we provide a finite\nsample analysis, which is then used to show that the algorithm consistently\nestimates both the smallest level and the corresponding connected components.\nWe further establish rates of convergence for the two estimation problems, and\nlast but not least, we present a simple, yet adaptive strategy for determining\nthe width-parameter of the involved density estimator in a data-depending way.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 08:54:28 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 15:47:40 GMT"}, {"version": "v3", "created": "Fri, 23 Oct 2015 09:22:00 GMT"}, {"version": "v4", "created": "Wed, 28 Oct 2015 07:31:20 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Steinwart", "Ingo", ""]]}, {"id": "1409.8491", "submitter": "Felix Abramovich", "authors": "Felix Abramovich and Vadim Grinshtein", "title": "Model selection and minimax estimation in generalized linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider model selection in generalized linear models (GLM) for\nhigh-dimensional data and propose a wide class of model selection criteria\nbased on penalized maximum likelihood with a complexity penalty on the model\nsize. We derive a general nonasymptotic upper bound for the expected\nKullback-Leibler divergence between the true distribution of the data and that\ngenerated by a selected model, and establish the corresponding minimax lower\nbounds for sparse GLM. For the properly chosen (nonlinear) penalty, the\nresulting penalized maximum likelihood estimator is shown to be asymptotically\nminimax and adaptive to the unknown sparsity. We discuss also possible\nextensions of the proposed approach to model selection in GLM under additional\nstructural constraints and aggregation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 11:25:39 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2015 10:11:50 GMT"}, {"version": "v3", "created": "Wed, 30 Mar 2016 08:20:50 GMT"}], "update_date": "2016-03-31", "authors_parsed": [["Abramovich", "Felix", ""], ["Grinshtein", "Vadim", ""]]}, {"id": "1409.8502", "submitter": "Juho Kokkala", "authors": "Juho Kokkala and Simo S\\\"arkk\\\"a", "title": "Combining Particle MCMC with Rao-Blackwellized Monte Carlo Data\n  Association for Parameter Estimation in Multiple Target Tracking", "comments": "Revised version. 43 pages, 4 figures", "journal-ref": null, "doi": "10.1016/j.dsp.2015.04.004", "report-no": null, "categories": "stat.ME math.DS math.ST stat.AP stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider state and parameter estimation in multiple target tracking\nproblems with data association uncertainties and unknown number of targets. We\nshow how the problem can be recast into a conditionally linear Gaussian\nstate-space model with unknown parameters and present an algorithm for\ncomputationally efficient inference on the resulting model. The proposed\nalgorithm is based on combining the Rao-Blackwellized Monte Carlo data\nassociation algorithm with particle Markov chain Monte Carlo algorithms to\njointly estimate both parameters and data associations. Both particle marginal\nMetropolis-Hastings and particle Gibbs variants of particle MCMC are\nconsidered. We demonstrate the performance of the method both using simulated\ndata and in a real-data case study of using multiple target tracking to\nestimate the brown bear population in Finland.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 11:59:41 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 21:03:46 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Kokkala", "Juho", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1409.8544", "submitter": "Martin Scharpenberg", "authors": "Werner Brannath and Martin Scharpenberg", "title": "Interpretation of Linear Regression Coefficients under Mean Model\n  Miss-Specification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression is a frequently used tool in statistics, however, its\nvalidity and interpretability relies on strong model assumptions. While robust\nestimates of the coefficients' covariance extend the validity of hypothesis\ntests and confidence intervals, a clear interpretation of the coefficients is\nlacking if the mean structure of the model is miss-specified. We therefore\nsuggest a new intuitive and mathematical rigorous interpretation of the\ncoefficients that is independent from specific model assumptions. It relies on\na new population based measure of association. The idea is to quantify how much\nthe population mean of the dependent variable Y can be changed by changing the\ndistribution of the independent variable X. Restriction to linear functions for\nthe distributional changes in X provides the link to linear regression. It\nleads to a conservative approximation of the newly defined and generally\nnon-linear measure of association. The conservative linear approximation can\nthen be estimated by linear regression. We show how this interpretation can be\nextended to multiple regression and how far and in which sense it leads to an\nadjustment for confounding. We point to perspectives for new analysis\nstrategies and illustrate the utility and limitations of the new interpretation\nand strategies by examples and simulations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 13:44:37 GMT"}, {"version": "v2", "created": "Mon, 6 Oct 2014 11:08:32 GMT"}, {"version": "v3", "created": "Mon, 20 Oct 2014 07:34:11 GMT"}, {"version": "v4", "created": "Wed, 22 Apr 2015 05:29:12 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Brannath", "Werner", ""], ["Scharpenberg", "Martin", ""]]}, {"id": "1409.8565", "submitter": "Zongming Ma", "authors": "Chao Gao, Zongming Ma, Harrison H. Zhou", "title": "Sparse CCA: Adaptive Estimation and Computational Barriers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis is a classical technique for exploring the\nrelationship between two sets of variables. It has important applications in\nanalyzing high dimensional datasets originated from genomics, imaging and other\nfields. This paper considers adaptive minimax and computationally tractable\nestimation of leading sparse canonical coefficient vectors in high dimensions.\nFirst, we establish separate minimax estimation rates for canonical coefficient\nvectors of each set of random variables under no structural assumption on\nmarginal covariance matrices. Second, we propose a computationally feasible\nestimator to attain the optimal rates adaptively under an additional sample\nsize condition. Finally, we show that a sample size condition of this kind is\nneeded for any randomized polynomial-time estimator to be consistent, assuming\nhardness of certain instances of the Planted Clique detection problem. The\nresult is faithful to the Gaussian models used in the paper. As a byproduct, we\nobtain the first computational lower bounds for sparse PCA under the Gaussian\nsingle spiked covariance model.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 14:36:15 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 14:12:51 GMT"}, {"version": "v3", "created": "Fri, 9 Jan 2015 21:58:24 GMT"}, {"version": "v4", "created": "Mon, 4 Apr 2016 18:37:29 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Gao", "Chao", ""], ["Ma", "Zongming", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1409.8597", "submitter": "Luke Keele", "authors": "Luke Keele, Jose R. Zubizarreta", "title": "Optimal Multilevel Matching in Clustered Observational Studies: A Case\n  Study of the Effectiveness of Private Schools Under a Large-Scale Voucher\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distinctive feature of a clustered observational study is its multilevel or\nnested data structure arising from the assignment of treatment, in a non-random\nmanner, to groups or clusters of units or individuals. Examples are ubiquitous\nin the health and social sciences including patients in hospitals, employees in\nfirms, and students in schools. What is the optimal matching strategy in a\nclustered observational study? At first thought, one might start by matching\nclusters of individuals and then, within matched clusters, continue by matching\nindividuals. But as we discuss in this paper, the optimal strategy is the\nopposite: in typical applications, where the intracluster correlation is not\nperfect, it is best to first match individuals and, once all possible\ncombinations of matched individuals are known, then match clusters. In this\npaper we use dynamic and integer programming to implement this strategy and\nextend optimal matching methods to hierarchical and multilevel settings. Among\nother matched designs, our strategy can approximate a paired clustered\nrandomized study by finding the largest sample of matched pairs of treated and\ncontrol individuals within matched pairs of treated and control clusters that\nis balanced according to specifications given by the investigator. This\nstrategy directly balances covariates both at the cluster and individual levels\nand does not require estimating the propensity score, although the propensity\nscore can be balanced as an additional covariate. We illustrate our results\nwith a case study of the comparative effectiveness of public versus private\nvoucher schools in Chile, a question of intense policy debate in the country at\nthe present.\n", "versions": [{"version": "v1", "created": "Tue, 30 Sep 2014 15:31:22 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 20:57:17 GMT"}], "update_date": "2016-05-02", "authors_parsed": [["Keele", "Luke", ""], ["Zubizarreta", "Jose R.", ""]]}]