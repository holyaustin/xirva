[{"id": "1808.00212", "submitter": "Daniel W. Heck", "authors": "Daniel W. Heck, Morten Moshagen, Edgar Erdfelder", "title": "Model selection by minimum description length: Lower-bound sample sizes\n  for the Fisher information approximation", "comments": null, "journal-ref": "Journal of Mathematical Psychology (2014) 60, 29-34", "doi": "10.1016/j.jmp.2014.06.002", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher information approximation (FIA) is an implementation of the\nminimum description length principle for model selection. Unlike information\ncriteria such as AIC or BIC, it has the advantage of taking the functional form\nof a model into account. Unfortunately, FIA can be misleading in finite\nsamples, resulting in an inversion of the correct rank order of complexity\nterms for competing models in the worst case. As a remedy, we propose a\nlower-bound $N'$ for the sample size that suffices to preclude such errors. We\nillustrate the approach using three examples from the family of multinomial\nprocessing tree models.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 08:00:33 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Heck", "Daniel W.", ""], ["Moshagen", "Morten", ""], ["Erdfelder", "Edgar", ""]]}, {"id": "1808.00242", "submitter": "Dennis Dobler", "authors": "Dennis Dobler and Markus Pauly and Thomas H. Scheike", "title": "Wild Bootstrap based Confidence Bands for Multiplicative Hazards Models", "comments": "15 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new resampling-based approaches to construct asymptotically valid\ntime simultaneous confidence bands for cumulative hazard functions in\nmulti-state Cox models. In particular, we exemplify the methodology in detail\nfor the simple Cox model with time dependent covariates, where the data may be\nsubject to independent right-censoring or left-truncation. In extensive\nsimulations we investigate their finite sample behaviour. Finally, the methods\nare utilized to analyze an empirical example.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 09:34:29 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Dobler", "Dennis", ""], ["Pauly", "Markus", ""], ["Scheike", "Thomas H.", ""]]}, {"id": "1808.00419", "submitter": "Alessandro Gasparini", "authors": "Alessandro Gasparini, Keith R. Abrams, Jessica K. Barrett, Rupert W.\n  Major, Michael J. Sweeting, Nigel J. Brunskill, Michael J. Crowther", "title": "Mixed effects models for healthcare longitudinal data with an\n  informative visiting process: a Monte Carlo simulation study", "comments": null, "journal-ref": null, "doi": "10.1111/stan.12188", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic health records are being increasingly used in medical research to\nanswer more relevant and detailed clinical questions; however, they pose new\nand significant methodological challenges. For instance, observation times are\nlikely correlated with the underlying disease severity: patients with worse\nconditions utilise health care more and may have worse biomarker values\nrecorded. Traditional methods for analysing longitudinal data assume\nindependence between observation times and disease severity; yet, with\nhealthcare data such assumptions unlikely holds. Through Monte Carlo\nsimulation, we compare different analytical approaches proposed to account for\nan informative visiting process to assess whether they lead to unbiased\nresults. Furthermore, we formalise a joint model for the observation process\nand the longitudinal outcome within an extended joint modelling framework. We\nillustrate our results using data from a pragmatic trial on enhanced care for\nindividuals with chronic kidney disease, and we introduce user-friendly\nsoftware that can be used to fit the joint model for the observation process\nand a longitudinal outcome.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 17:01:29 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2019 15:58:02 GMT"}, {"version": "v3", "created": "Tue, 11 Jun 2019 12:46:41 GMT"}, {"version": "v4", "created": "Thu, 25 Jul 2019 14:44:27 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Gasparini", "Alessandro", ""], ["Abrams", "Keith R.", ""], ["Barrett", "Jessica K.", ""], ["Major", "Rupert W.", ""], ["Sweeting", "Michael J.", ""], ["Brunskill", "Nigel J.", ""], ["Crowther", "Michael J.", ""]]}, {"id": "1808.00424", "submitter": "Emeric Thibaud", "authors": "Samuel A. Morris, Brian J. Reich, Emeric Thibaud", "title": "Exploration and inference in spatial extremes using empirical basis\n  functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods for inference on spatial extremes of large datasets are\nyet to be developed. Motivated by standard dimension reduction techniques used\nin spatial statistics, we propose an approach based on empirical basis\nfunctions to explore and model spatial extremal dependence. Based on a low-rank\nmax-stable model we propose a data-driven approach to estimate meaningful basis\nfunctions using empirical pairwise extremal coefficients. These spatial\nempirical basis functions can be used to visualize the main trends in extremal\ndependence. In addition to exploratory analysis, we describe how these\nfunctions can be used in a Bayesian hierarchical model to model spatial\nextremes of large datasets. We illustrate our methods on extreme precipitations\nin eastern U.S.\n", "versions": [{"version": "v1", "created": "Wed, 1 Aug 2018 17:09:10 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Morris", "Samuel A.", ""], ["Reich", "Brian J.", ""], ["Thibaud", "Emeric", ""]]}, {"id": "1808.00647", "submitter": "Baisuo Jin", "authors": "Jialiang Li, Yaguang Li and Baisuo Jin", "title": "Multi-threshold Change Plane Model: Estimation Theory and Applications\n  in Subgroup Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a multi-threshold change plane regression model which naturally\npartitions the observed subjects into subgroups with different covariate\neffects. The underlying grouping variable is a linear function of covariates\nand thus multiple thresholds form parallel change planes in the covariate\nspace. We contribute a novel 2-stage approach to estimate the number of\nsubgroups, the location of thresholds and all other regression parameters. In\nthe first stage we adopt a group selection principle to consistently identify\nthe number of subgroups, while in the second stage change point locations and\nmodel parameter estimates are refined by a penalized induced smoothing\ntechnique. Our procedure allows sparse solutions for relatively moderate- or\nhigh-dimensional covariates. We further establish the asymptotic properties of\nour proposed estimators under appropriate technical conditions. We evaluate the\nperformance of the proposed methods by simulation studies and provide\nillustration using two medical data. Our proposal for subgroup identification\nmay lead to an immediate application in personalized medicine.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 03:03:36 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Li", "Jialiang", ""], ["Li", "Yaguang", ""], ["Jin", "Baisuo", ""]]}, {"id": "1808.00662", "submitter": "Xiuqi Li", "authors": "Xiuqi Li, Subhashis Ghosal", "title": "Bayesian Classification of Multiclass Functional Data", "comments": "26 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian approach to estimating parameters in multiclass\nfunctional models. Unordered multinomial probit, ordered multinomial probit and\nmultinomial logistic models are considered. We use finite random series priors\nbased on a suitable basis such as B-splines in these three multinomial models,\nand classify the functional data using the Bayes rule. We average over models\nbased on the marginal likelihood estimated from Markov Chain Monte Carlo (MCMC)\noutput. Posterior contraction rates for the three multinomial models are\ncomputed. We also consider Bayesian linear and quadratic discriminant analyses\non the multivariate data obtained by applying a functional principal component\ntechnique on the original functional data. A simulation study is conducted to\ncompare these methods on different types of data. We also apply these methods\nto a phoneme dataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 04:24:54 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Li", "Xiuqi", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1808.00723", "submitter": "Steven Hill", "authors": "Fan Wang, Sach Mukherjee, Sylvia Richardson, Steven M. Hill", "title": "High-dimensional regression in practice: an empirical study of\n  finite-sample prediction, variable selection and ranking", "comments": "This is a post-peer-review, pre-copyedit version of an article\n  published in Statistics and Computing. The final authenticated version is\n  available online (open access) at:\n  http://dx.doi.org/10.1007/s11222-019-09914-9", "journal-ref": "Statistics and Computing, 2019. Advance online publication", "doi": "10.1007/s11222-019-09914-9", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized likelihood approaches are widely used for high-dimensional\nregression. Although many methods have been proposed and the associated theory\nis now well-developed, the relative efficacy of different approaches in\nfinite-sample settings, as encountered in practice, remains incompletely\nunderstood. There is therefore a need for empirical investigations in this area\nthat can offer practical insight and guidance to users. In this paper we\npresent a large-scale comparison of penalized regression methods. We\ndistinguish between three related goals: prediction, variable selection and\nvariable ranking. Our results span more than 2,300 data-generating scenarios,\nincluding both synthetic and semi-synthetic data (real covariates and simulated\nresponses), allowing us to systematically consider the influence of various\nfactors (sample size, dimensionality, sparsity, signal strength and\nmulticollinearity). We consider several widely-used approaches (Lasso, Adaptive\nLasso, Elastic Net, Ridge Regression, SCAD, the Dantzig Selector and Stability\nSelection). We find considerable variation in performance between methods. Our\nresults support a `no panacea' view, with no unambiguous winner across all\nscenarios or goals, even in this restricted setting where all data align well\nwith the assumptions underlying the methods. The study allows us to make some\nrecommendations as to which approaches may be most (or least) suitable given\nthe goal and some data characteristics. Our empirical results complement\nexisting theory and provide a resource to compare methods across a range of\nscenarios and metrics.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 09:22:39 GMT"}, {"version": "v2", "created": "Tue, 28 Jan 2020 16:57:29 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Wang", "Fan", ""], ["Mukherjee", "Sach", ""], ["Richardson", "Sylvia", ""], ["Hill", "Steven M.", ""]]}, {"id": "1808.01217", "submitter": "Pamphile Roy", "authors": "Pamphile T. Roy, Sophie Ricci, B\\'en\\'edicte Cuenot and\n  Jean-Christophe Jouhaud", "title": "Sounding Spider: An Efficient Way for Representing Uncertainties in High\n  Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a visualization method for multidimensional data based\non: (i) Animated functional Hypothetical Outcome Plots (f-HOPs); (ii)\n3-dimensional Kiviat plot; and (iii) data sonification. In an Uncertainty\nQuantification (UQ) framework, such analysis coupled with standard statistical\nanalysis tools such as Probability Density Functions (PDF) can be used to\naugment the understanding of how the uncertainties in the numerical code inputs\ntranslate into uncertainties in the quantity of interest (QoI).\n  In contrast with static representation of most advanced techniques such as\nfunctional Highest Density Region (HDR) boxplot or functional boxplot, f-HOPs\nis a dynamic visualization that enables the practitioners to infer the dynamics\nof the physics and enables to see functional correlations that may exist. While\nthis technique only allows to represent the QoI, we propose a 3-dimensional\nversion of the Kiviat plot to encode all input parameters. This new\nvisualization takes advantage of information from f-HOPs through data\nsonification. All in all, this allows to analyse large datasets within a\nhigh-dimensional parameter space and a functional QoI in the same canvas. The\nproposed method is assessed and showed its benefits on two related\nenvironmental datasets.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 15:09:07 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Roy", "Pamphile T.", ""], ["Ricci", "Sophie", ""], ["Cuenot", "B\u00e9n\u00e9dicte", ""], ["Jouhaud", "Jean-Christophe", ""]]}, {"id": "1808.01236", "submitter": "Xiuqi Li", "authors": "Xiuqi Li, Subhashis Ghosal", "title": "Bayesian Change Point Detection for Functional Data", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian method to detect change points for functional data. We\nextract the features of a sequence of functional data by the discrete wavelet\ntransform (DWT), and treat each sequence of feature independently. We believe\nthere is potentially a change in each feature at possibly different time\npoints. The functional data evolves through such changes throughout the\nsequences of observations. The change point for this sequence of functional\ndata is the cumulative effect of changes in all features. We assign the\nfeatures with priors which incorporate the characteristic of the wavelet\ncoefficients. Then we compute the posterior distribution of change point for\neach sequence of feature, and define a matrix where each entry is a measure of\nsimilarity between two functional data in this sequence. We compute the ratio\nof the mean similarity between groups and within groups for all possible\npartitions, and the change point is where the ratio reaches the minimum. We\ndemonstrate this method using a dataset on climate change.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 15:56:14 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Li", "Xiuqi", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1808.01240", "submitter": "Valentina Raponi", "authors": "Lea Petrella and Valentina Raponi", "title": "Joint estimation of conditional quantiles in multivariate linear\n  regression models. An application to financial distress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a maximum-likelihood approach to jointly estimate\nmarginal conditional quantiles of multivariate response variables in a linear\nregression framework.\n  We consider a slight reparameterization of the Multivariate Asymmetric\nLaplace distribution proposed by Kotz et al (2001) and exploit its\nlocation-scale mixture representation to implement a new EM algorithm for\nestimating model parameters. The idea is to extend the link between the\nAsymmetric Laplace distribution and the well-known univariate quantile\nregression model to a multivariate context, i.e. when a multivariate dependent\nvariable is concerned. The approach accounts for association among multiple\nresponses and study how the relationship between responses and explanatory\nvariables can vary across different quantiles of the marginal conditional\ndistribution of the responses. A penalized version of the EM algorithm is also\npresented to tackle the problem of variable selection. The validity of our\napproach is analyzed in a simulation study, where we also provide evidence on\nthe efficiency gain of the proposed method compared to estimation obtained by\nseparate univariate quantile regressions. A real data application is finally\nproposed to study the main determinants of financial distress in a sample of\nItalian firms.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 16:00:41 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Petrella", "Lea", ""], ["Raponi", "Valentina", ""]]}, {"id": "1808.01332", "submitter": "Yingqi Zhao", "authors": "Ying-Qi Zhao and Ruoqing Zhu and Guanhua Chen and Yingye Zheng", "title": "Constructing Stabilized Dynamic Treatment Regimes for Censored Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stabilized dynamic treatment regimes are sequential decision rules for\nindividual patients that not only adaptive throughout the disease progression\nbut also remain consistent over time in format. The estimation of stabilized\ndynamic treatment regimes becomes more complicated when the clinical outcome of\ninterest is a survival time subject to censoring. To address this challenge, we\npropose two novel methods, censored shared-Q-learning and censored\nshared-O-learning. Both methods incorporate clinical preferences into a\nqualitative rule, where the parameters indexing the decision rules are shared\nacross different stages and estimated simultaneously. We use extensive\nsimulation studies to demonstrate the superior performance of the proposed\nmethods. The methods are further applied to the Framingham Study to derive\ntreatment rules for cardiovascular disease.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2018 20:03:47 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 06:14:36 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Zhao", "Ying-Qi", ""], ["Zhu", "Ruoqing", ""], ["Chen", "Guanhua", ""], ["Zheng", "Yingye", ""]]}, {"id": "1808.01408", "submitter": "Zhiqiang Tan", "authors": "Heng Shu and Zhiqiang Tan", "title": "Improved Estimation of Average Treatment Effects on the Treated: Local\n  Efficiency, Double Robustness, and Beyond", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of average treatment effects on the treated (ATT) is an important\ntopic of causal inference in econometrics and statistics. This problem seems to\nbe often treated as a simple modification or extension of that of estimating\noverall average treatment effects (ATE). However, the propensity score is no\nlonger ancillary for estimation of ATT, in contrast with estimation of ATE. In\nthis article, we review semiparametric theory for estimation of ATT and the use\nof efficient influence functionsto derive augmented inverse probability\nweighted (AIPW) estimators that are locally efficient and doubly robust.\nMoreover, we discuss improved estimation over AIPW by developing calibrated\nregression and likelihood estimators that are not only locally efficient and\ndoubly robust, but also intrinsically efficient in achieving smaller variances\nthan AIPW estimators when a propensity score model is correctly specified but\nan outcome regression model may be misspecified. Finally, we present two\nsimulation studies and an econometric application to demonstrate the advantage\nof the proposed methods when compared with existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 4 Aug 2018 01:58:17 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Shu", "Heng", ""], ["Tan", "Zhiqiang", ""]]}, {"id": "1808.01557", "submitter": "Yikai Wang", "authors": "Yikai Wang and Ying Guo", "title": "A hierarchical independent component analysis model for longitudinal\n  Neuroimaging studies", "comments": "37 pages, 13 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, longitudinal neuroimaging study has become increasingly\npopular in neuroscience research to investigate disease-related changes in\nbrain functions. In current neuroscience literature, one of the most commonly\nused tools to extract and characterize brain functional networks is independent\ncomponent analysis (ICA). However, existing ICA methods are not suited for\nmodelling repeatedly measured imaging data. In this paper, we propose a novel\nlongitudinal independent component model (L-ICA) which provides a formal\nmodeling framework for extending ICA to longitudinal studies. By incorporating\nsubject-specific random effects and visit-specific covariate effects, L-ICA is\nable to provide more accurate estimates of changes in brain functional networks\non both the population- and individual-level, borrow information across\nrepeated scans within the same subject to increase statistical power in\ndetecting covariate effects on the networks, and allow for model-based\nprediction for brain networks changes caused by disease progression, treatment\nor neurodevelopment. We develop a fully traceable exact EM algorithm to obtain\nmaximum likelihood estimates of L-ICA. We further develop a subspace-based\napproximate EM algorithm which greatly reduce the computation time while still\nretaining high accuracy. Moreover, we present a statistical testing procedure\nfor examining covariate effects on brain network changes. Simulation results\ndemonstrate the advantages of our proposed methods. We apply L-ICA to ADNI2\nstudy to investigate changes in brain functional networks in Alzheimer disease.\nResults from the L-ICA provide biologically insightful findings which are not\nrevealed using existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 03:55:40 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Wang", "Yikai", ""], ["Guo", "Ying", ""]]}, {"id": "1808.01647", "submitter": "Zhulin He", "authors": "Zhulin He", "title": "Inverse Conditional Probability Weighting with Clustered Data in Causal\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the average treatment causal effect in clustered data often\ninvolves dealing with unmeasured cluster-specific confounding variables. Such\nvariables may be correlated with the measured unit covariates and outcome. When\nthe correlations are ignored, the causal effect estimation can be biased. By\nutilizing sufficient statistics, we propose an inverse conditional probability\nweighting (ICPW) method, which is robust to both (i) the correlation between\nthe unmeasured cluster-specific confounding variable and the covariates and\n(ii) the correlation between the unmeasured cluster-specific confounding\nvariable and the outcome. Assumptions and conditions for the ICPW method are\npresented. We establish the asymptotic properties of the proposed estimators.\nSimulation studies and a case study are presented for illustration.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 16:37:04 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["He", "Zhulin", ""]]}, {"id": "1808.01665", "submitter": "Nicolas Brosse", "authors": "Nicolas Brosse, Alain Durmus, Sean Meyn, Eric Moulines and Anand\n  Radhakrishnan", "title": "Diffusion approximations and control variates for MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new methodology is presented for the construction of control variates to\nreduce the variance of additive functionals of Markov Chain Monte Carlo (MCMC)\nsamplers. Our control variates are definedthrough the minimization of the\nasymptotic variance of the Langevin diffusion over a family of functions, which\ncan be seen as a quadratic risk minimization procedure. The use of these\ncontrol variates is theoretically justified. We show that the asymptotic\nvariances of some well-known MCMC algorithms, including the Random Walk\nMetropolis and the (Metropolis) Unadjusted/Adjusted Langevin Algorithm, are\nclose to the asymptotic variance of the Langevin diffusion. Several examples of\nBayesian inference problems demonstrate that the corresponding reduction in the\nvariance is significant.\n", "versions": [{"version": "v1", "created": "Sun, 5 Aug 2018 18:16:54 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 09:25:46 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Brosse", "Nicolas", ""], ["Durmus", "Alain", ""], ["Meyn", "Sean", ""], ["Moulines", "Eric", ""], ["Radhakrishnan", "Anand", ""]]}, {"id": "1808.01749", "submitter": "Xu Gao", "authors": "Xu Gao, Weining Shen, Hernando Ombao", "title": "Regularized matrix data clustering and its application to image analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a regularized mixture probabilistic model to\ncluster matrix data and apply it to brain signals. The approach is able to\ncapture the sparsity (low rank, small/zero values) of the original signals by\nintroducing regularization terms into the likelihood function. Through a\nmodified EM algorithm, our method achieves the optimal solution with low\ncomputational cost. Theoretical results are also provided to establish the\nconsistency of the proposed estimators. Simulations show the advantages of the\nproposed method over other existing methods. We also apply the approach to two\nreal datasets from different experiments. Promising results imply that the\nproposed method successfully characterizes signals with different patterns\nwhile yielding insightful scientific interpretation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Aug 2018 07:07:21 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Gao", "Xu", ""], ["Shen", "Weining", ""], ["Ombao", "Hernando", ""]]}, {"id": "1808.02193", "submitter": "Huichen Zhu", "authors": "Huichen Zhu, Gen Li, Eric F. Lock", "title": "Generalized Integrative Principal Component Analysis for Multi-Type Data\n  with Block-Wise Missing Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional multi-source data are encountered in many fields. Despite\nrecent developments on the integrative dimension reduction of such data, most\nexisting methods cannot easily accommodate data of multiple types (e.g., binary\nor count-valued). Moreover, multi-source data often have block-wise missing\nstructure, i.e., data in one or more sources may be completely unobserved for a\nsample. The heterogeneous data types and presence of block-wise missing data\npose significant challenges to the integration of multi-source data and further\nstatistical analyses. In this paper, we develop a low-rank method, called\nGeneralized Integrative Principal Component Analysis (GIPCA), for the\nsimultaneous dimension reduction and imputation of multi-source block-wise\nmissing data, where different sources may have different data types. We also\ndevise an adapted BIC criterion for rank estimation. Comprehensive simulation\nstudies demonstrate the efficacy of the proposed method in terms of rank\nestimation, signal recovery, and missing data imputation. We apply GIPCA to a\nmortality study. We achieve accurate block-wise missing data imputation and\nidentify intriguing latent mortality rate patterns with sociological relevance.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 03:17:10 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Zhu", "Huichen", ""], ["Li", "Gen", ""], ["Lock", "Eric F.", ""]]}, {"id": "1808.02403", "submitter": "Kun Chen", "authors": "Zhe Sun, Wanli Xu, Xiaomei Cong, Gen Li and Kun Chen", "title": "Log-Contrast Regression with Functional Compositional Predictors:\n  Linking Preterm Infant's Gut Microbiome Trajectories to Neurobehavioral\n  Outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neonatal intensive care unit (NICU) experience is known to be one of the\nmost crucial factors that drive preterm infant's neurodevelopmental and health\noutcomes. It is hypothesized that stressful early life experience of very\npreterm neonate is imprinting gut microbiome by the regulation of the so-called\nbrain-gut axis, and consequently, certain microbiome markers are predictive of\nlater infant neurodevelopment. To investigate, a preterm infant study was\nconducted; infant fecal samples were collected during the infants' first month\nof postnatal age, resulting in functional compositional microbiome data, and\nneurobehavioral outcomes were measured when infants reached 36-38 weeks of\npost-menstrual age. To identify potential microbiome markers and estimate how\nthe trajectories of gut microbiome compositions during early postnatal stage\nimpact later neurobehavioral outcomes of the preterm infants, we innovate a\nsparse log-contrast regression with functional compositional predictors. The\nfunctional simplex structure is strictly preserved, and the functional\ncompositional predictors are allowed to have sparse, smoothly varying, and\naccumulating effects on the outcome through time. Through a pragmatic basis\nexpansion step, the problem boils down to a linearly constrained sparse group\nregression, for which we develop an efficient algorithm and obtain theoretical\nperformance guarantees. Our approach yields insightful results in the preterm\ninfant study. The identified microbiome markers and the estimated time dynamics\nof their impact on the neurobehavioral outcome shed light on the linkage\nbetween stress accumulation in early postnatal stage and neurodevelopmental\nprocess of infants.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 14:39:24 GMT"}, {"version": "v2", "created": "Wed, 9 Oct 2019 02:13:52 GMT"}], "update_date": "2019-10-10", "authors_parsed": [["Sun", "Zhe", ""], ["Xu", "Wanli", ""], ["Cong", "Xiaomei", ""], ["Li", "Gen", ""], ["Chen", "Kun", ""]]}, {"id": "1808.02430", "submitter": "Badong Chen", "authors": "Badong Chen and Rongjin Ma and Siyu Yu and Shaoyi Du and Jing Qin", "title": "Granger Causality Analysis Based on Quantized Minimum Error Entropy\n  Criterion", "comments": "5 pages, 2 figures, 3 tables", "journal-ref": null, "doi": "10.1109/LSP.2019.2890973", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression model (LRM) based on mean square error (MSE) criterion is\nwidely used in Granger causality analysis (GCA), which is the most commonly\nused method to detect the causality between a pair of time series. However,\nwhen signals are seriously contaminated by non-Gaussian noises, the LRM\ncoefficients will be inaccurately identified. This may cause the GCA to detect\na wrong causal relationship. Minimum error entropy (MEE) criterion can be used\nto replace the MSE criterion to deal with the non-Gaussian noises. But its\ncalculation requires a double summation operation, which brings computational\nbottlenecks to GCA especially when sizes of the signals are large. To address\nthe aforementioned problems, in this study we propose a new method called GCA\nbased on the quantized MEE (QMEE) criterion (GCA-QMEE), in which the QMEE\ncriterion is applied to identify the LRM coefficients and the quantized error\nentropy is used to calculate the causality indexes. Compared with the\ntraditional GCA, the proposed GCA-QMEE not only makes the results more\ndiscriminative, but also more robust. Its computational complexity is also not\nhigh because of the quantization operation. Illustrative examples on synthetic\nand EEG datasets are provided to verify the desirable performance and the\navailability of the GCA-QMEE.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 15:52:18 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Chen", "Badong", ""], ["Ma", "Rongjin", ""], ["Yu", "Siyu", ""], ["Du", "Shaoyi", ""], ["Qin", "Jing", ""]]}, {"id": "1808.02526", "submitter": "Ana Kenney", "authors": "Ana Kenney, Francesca Chiaromonte, Giovanni Felici", "title": "MIP-BOOST: Efficient and Effective $L_0$ Feature Selection for Linear\n  Regression", "comments": "This work has been presented at JSM 2018 (Vancouver, Canada), ISNPS\n  2018 (Salerno, Italy), and various other conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in mathematical programming have made Mixed Integer\nOptimization a competitive alternative to popular regularization methods for\nselecting features in regression problems. The approach exhibits unquestionable\nfoundational appeal and versatility, but also poses important challenges. Here\nwe propose MIP-BOOST, a revision of standard Mixed Integer Programming feature\nselection that reduces the computational burden of tuning the critical sparsity\nbound parameter and improves performance in the presence of feature\ncollinearity and of signals that vary in nature and strength. The final outcome\nis a more efficient and effective $L_0$ Feature Selection method for\napplications of realistic size and complexity, grounded on rigorous\ncross-validation tuning and exact optimization of the associated Mixed Integer\nProgram. Computational viability and improved performance in realistic\nscenarios is achieved through three independent but synergistic proposals.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 19:17:38 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 20:24:31 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 15:25:45 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Kenney", "Ana", ""], ["Chiaromonte", "Francesca", ""], ["Felici", "Giovanni", ""]]}, {"id": "1808.02560", "submitter": "Fabio Cuzzolin", "authors": "Fabio Cuzzolin", "title": "Belief likelihood function for generalised logistic regression", "comments": "10 pages, 3 figures; submitted to UAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of belief likelihood function of repeated trials is introduced,\nwhenever the uncertainty for individual trials is encoded by a belief measure\n(a finite random set). This generalises the traditional likelihood function,\nand provides a natural setting for belief inference from statistical data.\nFactorisation results are proven for the case in which conjunctive or\ndisjunctive combination are employed, leading to analytical expressions for the\nlower and upper likelihoods of `sharp' samples in the case of Bernoulli trials,\nand to the formulation of a generalised logistic regression framework.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2018 21:43:32 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 10:12:21 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Cuzzolin", "Fabio", ""]]}, {"id": "1808.02635", "submitter": "Jooyoung Jeon Dr", "authors": "Jooyoung Jeon, Anastasios Panagiotelis, Fotios Petropoulos", "title": "Reconciliation of probabilistic forecasts with an application to wind\n  power", "comments": null, "journal-ref": "European Journal of Operational Research (2019), 279, 364-379", "doi": "10.1016/j.ejor.2019.05.020", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New methods are proposed for adjusting probabilistic forecasts to ensure\ncoherence with the aggregation constraints inherent in temporal hierarchies.\nThe different approaches nested within this framework include methods that\nexploit information at all levels of the hierarchy as well as a novel method\nbased on cross-validation. The methods are evaluated using real data from two\nwind farms in Crete, an application where it is imperative for optimal\ndecisions related to grid operations and bidding strategies to be based on\ncoherent probabilistic forecasts of wind power. Empirical evidence is also\npresented showing that probabilistic forecast reconciliation improves the\naccuracy of both point forecasts and probabilistic forecasts.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 05:55:36 GMT"}], "update_date": "2020-08-18", "authors_parsed": [["Jeon", "Jooyoung", ""], ["Panagiotelis", "Anastasios", ""], ["Petropoulos", "Fotios", ""]]}, {"id": "1808.02671", "submitter": "Falong Tan", "authors": "Falong Tan, Xuejun Jiang, Xu Guo and Lixing Zhu", "title": "Testing heteroscedasticity for regression models based on projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a new test of heteroscedasticity for parametric\nregression models and partial linear regression models in high dimensional\nsettings. When the dimension of covariates is large, existing tests of\nheteroscedasticity perform badly due to the \\curse of dimensionality\". To\nattack this problem, we construct a test of heteroscedasticity by using a\nprojection-based empirical process. We study the asymptotic properties of the\ntest statistic under the null hypothesis and alternative hypotheses. It is\nshown that the test can detect local alternatives departure from the null\nhypothesis at the fastest possible rate in hypothesis testing. As the limiting\nnull distribution of the test statistic is not distribution free, we propose a\nresidual-based bootstrap. The validity of the bootstrap approximations is\ninvestigated. We present some simulation results to show the finite sample\nperformances of the test. Two real data analyses are conducted for\nillustration.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2018 08:47:50 GMT"}], "update_date": "2018-08-09", "authors_parsed": [["Tan", "Falong", ""], ["Jiang", "Xuejun", ""], ["Guo", "Xu", ""], ["Zhu", "Lixing", ""]]}, {"id": "1808.03201", "submitter": "Yaakov Malinovsky", "authors": "Yaakov Malinovsky, Gregory Haber, Paul S. Albert", "title": "An optimal design for hierarchical generalized group testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing an optimal strategy for hierarchical group testing is an important\nproblem for practitioners who are interested in disease screening with limited\nresources. For example, when screening for infectious diseases in large\npopulations, it is important to use algorithms that minimize the cost of\npotentially expensive assays. Black et al. (2015) described this as an\nintractable problem unless the number of individuals to screen is small. They\nproposed an approximation to an optimal strategy that is difficult to implement\nfor large population sizes. In this article, we develop an optimal design with\nrespect to the expected total number of tests that can be obtained using a\nnovel dynamic programming algorithm. We show that this algorithm is\nsubstantially more efficient than the approach proposed by Black et al. (2015).\nIn addition, we compare the two designs for imperfect tests. R code is provided\nfor the practitioner.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 15:37:14 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2019 11:41:57 GMT"}, {"version": "v3", "created": "Wed, 26 Feb 2020 18:03:21 GMT"}], "update_date": "2020-02-27", "authors_parsed": [["Malinovsky", "Yaakov", ""], ["Haber", "Gregory", ""], ["Albert", "Paul S.", ""]]}, {"id": "1808.03230", "submitter": "Oren Mangoubi", "authors": "Oren Mangoubi, Natesh S. Pillai, and Aaron Smith", "title": "Does Hamiltonian Monte Carlo mix faster than a random walk on multimodal\n  densities?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.LG stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo (HMC) is a very popular and generic collection of\nMarkov chain Monte Carlo (MCMC) algorithms. One explanation for the popularity\nof HMC algorithms is their excellent performance as the dimension $d$ of the\ntarget becomes large: under conditions that are satisfied for many common\nstatistical models, optimally-tuned HMC algorithms have a running time that\nscales like $d^{0.25}$. In stark contrast, the running time of the usual\nRandom-Walk Metropolis (RWM) algorithm, optimally tuned, scales like $d$. This\nsuperior scaling of the HMC algorithm with dimension is attributed to the fact\nthat it, unlike RWM, incorporates the gradient information in the proposal\ndistribution. In this paper, we investigate a different scaling question: does\nHMC beat RWM for highly $\\textit{multimodal}$ targets? We find that the answer\nis often $\\textit{no}$. We compute the spectral gaps for both the algorithms\nfor a specific class of multimodal target densities, and show that they are\nidentical. The key reason is that, within one mode, the gradient is effectively\nignorant about other modes, thus negating the advantage the HMC algorithm\nenjoys in unimodal targets. We also give heuristic arguments suggesting that\nthe above observation may hold quite generally. Our main tool for answering\nthis question is a novel simple formula for the conductance of HMC using\nLiouville's theorem. This result allows us to compute the spectral gap of HMC\nalgorithms, for both the classical HMC with isotropic momentum and the recent\nRiemannian HMC, for multimodal targets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 16:46:51 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 16:20:07 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Mangoubi", "Oren", ""], ["Pillai", "Natesh S.", ""], ["Smith", "Aaron", ""]]}, {"id": "1808.03364", "submitter": "Matthew Harding", "authors": "Matthew Harding and Carlos Lamarche", "title": "A Panel Quantile Approach to Attrition Bias in Big Data: Evidence from a\n  Randomized Experiment", "comments": "JEL: C21, C23, C25, C55. Keywords: Attrition; Big Data; Quantile\n  regression; Individual Effects; Time-of-Day Pricing", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a quantile regression estimator for panel data models\nwith individual heterogeneity and attrition. The method is motivated by the\nfact that attrition bias is often encountered in Big Data applications. For\nexample, many users sign-up for the latest program but few remain active users\nseveral months later, making the evaluation of such interventions inherently\nvery challenging. Building on earlier work by Hausman and Wise (1979), we\nprovide a simple identification strategy that leads to a two-step estimation\nprocedure. In the first step, the coefficients of interest in the selection\nequation are consistently estimated using parametric or nonparametric methods.\nIn the second step, standard panel quantile methods are employed on a subset of\nweighted observations. The estimator is computationally easy to implement in\nBig Data applications with a large number of subjects. We investigate the\nconditions under which the parameter estimator is asymptotically Gaussian and\nwe carry out a series of Monte Carlo simulations to investigate the finite\nsample properties of the estimator. Lastly, using a simulation exercise, we\napply the method to the evaluation of a recent Time-of-Day electricity pricing\nexperiment inspired by the work of Aigner and Hausman (1980).\n", "versions": [{"version": "v1", "created": "Thu, 9 Aug 2018 22:35:27 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Harding", "Matthew", ""], ["Lamarche", "Carlos", ""]]}, {"id": "1808.03447", "submitter": "Jairo Cugliari", "authors": "Mathias Bourel (IMERL), Jairo Cugliari (ERIC)", "title": "Bagging of Density Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we give new density estimators by averaging classical density\nestimators such as the histogram, the frequency polygon and the kernel density\nestimators obtained over different bootstrap samples of the original data. We\nprove the L 2-consistency of these new estimators and compare them to several\nsimilar approaches by extensive simulations. Based on them, we give also a way\nto construct non parametric pointwise confidence intervals for the target\ndensity.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 08:03:49 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 13:36:16 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Bourel", "Mathias", "", "IMERL"], ["Cugliari", "Jairo", "", "ERIC"]]}, {"id": "1808.03662", "submitter": "Luigi Antelmi", "authors": "Luigi Antelmi, Nicholas Ayache, Philippe Robert, Marco Lorenzi", "title": "Multi-Channel Stochastic Variational Inference for the Joint Analysis of\n  Heterogeneous Biomedical Data in Alzheimer's Disease", "comments": "accepted for presentation at MLCN 2018 workshop, in Conjunction with\n  MICCAI 2018, September 20, Granada, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The joint analysis of biomedical data in Alzheimer's Disease (AD) is\nimportant for better clinical diagnosis and to understand the relationship\nbetween biomarkers. However, jointly accounting for heterogeneous measures\nposes important challenges related to the modeling of the variability and the\ninterpretability of the results. These issues are here addressed by proposing a\nnovel multi-channel stochastic generative model. We assume that a latent\nvariable generates the data observed through different channels (e.g., clinical\nscores, imaging, ...) and describe an efficient way to estimate jointly the\ndistribution of both latent variable and data generative process. Experiments\non synthetic data show that the multi-channel formulation allows superior data\nreconstruction as opposed to the single channel one. Moreover, the derived\nlower bound of the model evidence represents a promising model selection\ncriterion. Experiments on AD data show that the model parameters can be used\nfor unsupervised patient stratification and for the joint interpretation of the\nheterogeneous observations. Because of its general and flexible formulation, we\nbelieve that the proposed method can find important applications as a general\ndata fusion technique.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 18:25:12 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Antelmi", "Luigi", ""], ["Ayache", "Nicholas", ""], ["Robert", "Philippe", ""], ["Lorenzi", "Marco", ""]]}, {"id": "1808.03692", "submitter": "Isabel Fulcher", "authors": "Isabel R. Fulcher, Xu Shi, and Eric J. Tchetgen Tchetgen", "title": "Estimation of natural indirect effects robust to unmeasured confounding\n  and mediator measurement error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of causal mediation analysis to evaluate the pathways by which an\nexposure affects an outcome is widespread in the social and biomedical\nsciences. Recent advances in this area have established formal conditions for\nidentification and estimation of natural direct and indirect effects. However,\nthese conditions typically involve stringent no unmeasured confounding\nassumptions and that the mediator has been measured without error. These\nassumptions may fail to hold in practice where mediation methods are often\napplied. The goal of this paper is two-fold. First, we show that the natural\nindirect effect can in fact be identified in the presence of unmeasured\nexposure-outcome confounding provided there is no additive interaction between\nthe mediator and unmeasured confounder(s). Second, we introduce a new estimator\nof the natural indirect effect that is robust to both classical measurement\nerror of the mediator and unmeasured confounding of both exposure-outcome and\nmediator-outcome relations under certain no interaction assumptions. We provide\nformal proofs and a simulation study to demonstrate our results.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 20:22:36 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Fulcher", "Isabel R.", ""], ["Shi", "Xu", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1808.03698", "submitter": "Marcelo Medeiros", "authors": "Yuri Fonseca, Marcelo Medeiros, Gabriel Vasconcelos, Alvaro Veiga", "title": "BooST: Boosting Smooth Trees for Partial Effect Estimation in Nonlinear\n  Regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new machine learning (ML) model for nonlinear\nregression called the Boosted Smooth Transition Regression Trees (BooST), which\nis a combination of boosting algorithms with smooth transition regression\ntrees. The main advantage of the BooST model is the estimation of the\nderivatives (partial effects) of very general nonlinear models. Therefore, the\nmodel can provide more interpretation about the mapping between the covariates\nand the dependent variable than other tree-based models, such as Random\nForests. We present several examples with both simulated and real data.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2018 20:37:52 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 00:48:14 GMT"}, {"version": "v3", "created": "Sat, 15 Dec 2018 13:41:37 GMT"}, {"version": "v4", "created": "Sun, 9 Jun 2019 14:37:04 GMT"}, {"version": "v5", "created": "Tue, 28 Jul 2020 00:27:36 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Fonseca", "Yuri", ""], ["Medeiros", "Marcelo", ""], ["Vasconcelos", "Gabriel", ""], ["Veiga", "Alvaro", ""]]}, {"id": "1808.03750", "submitter": "Takahiro Hoshino", "authors": "Keisuke Takahata and Takahiro Hoshino", "title": "Identification and Estimation of Heterogeneous Treatment Effects under\n  Non-compliance or Non-ignorable assignment", "comments": "The first version of the manuscript is found at\n  \\url{https://ideas.repec.org/p/keo/dpaper/2018-005.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide sufficient conditions for the identification of the heterogeneous\ntreatment effects, defined as the conditional expectation for the differences\nof potential outcomes given the untreated outcome, under the nonignorable\ntreatment condition and availability of the information on the marginal\ndistribution of the untreated outcome. These functions are useful both to\nidentify the average treatment effects (ATE) and to determine the treatment\nassignment policy. The identification holds in the following two general setups\nprevalent in applied studies: (i) a randomized controlled trial with one-sided\nnoncompliance and (ii) an observational study with nonignorable assignment with\nthe information on the marginal distribution of the untreated outcome or its\nsample moments. To handle the setup with many integrals and missing values, we\npropose a (quasi-)Bayesian estimation method for HTE and ATE and examine its\nproperties through simulation studies. We also apply the proposed method to the\ndataset obtained by the National Job Training Partnership Act Study.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 04:47:50 GMT"}, {"version": "v2", "created": "Sat, 12 Jan 2019 16:51:07 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Takahata", "Keisuke", ""], ["Hoshino", "Takahiro", ""]]}, {"id": "1808.03786", "submitter": "Zhiqiang Tan", "authors": "Heng Shu, Zhiqiang Tan", "title": "Improved Methods for Moment Restriction Models with Marginally\n  Incompatible Data Combination and an Application to Two-sample Instrumental\n  Variable Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining information from multiple samples is often needed in biomedical and\neconomic studies, but the differences between these samples must be\nappropriately taken into account in the analysis of the combined data. We study\nestimation for moment restriction models with data combination from two samples\nunder an ignorablility-type assumption but allowing for different marginal\ndistributions of common variables between the two samples. Suppose that an\noutcome regression model and a propensity score model are specified. By\nleveraging the semiparametric efficiency theory, we derive an augmented inverse\nprobability weighted (AIPW) estimator that is locally efficient and doubly\nrobust with respect to the outcome regression and propensity score models.\nFurthermore, we develop calibrated regression and likelihood estimators that\nare not only locally efficient and doubly robust, but also intrinsically\nefficient in achieving smaller variances than the AIPW estimator when the\npropensity score model is correctly specified but the outcome regression model\nmay be misspecified. As an important application, we study the two-sample\ninstrumental variable problem and derive the corresponding estimators while\nallowing for incompatible distributions of common variables between the two\nsamples. Finally, we provide a simulation study and an econometric application\non public housing projects to demonstrate the superior performance of our\nimproved estimators.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 10:30:41 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Shu", "Heng", ""], ["Tan", "Zhiqiang", ""]]}, {"id": "1808.03813", "submitter": "Nicholas Henderson", "authors": "Nicholas C. Henderson and Ravi Varadhan", "title": "Bayesian Bivariate Subgroup Analysis for Risk-Benefit Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgroup analysis is a frequently used tool for evaluating heterogeneity of\ntreatment effect and heterogeneity in treatment harm across observed baseline\npatient characteristics. While treatment efficacy and adverse event measures\nare often reported separately for each subgroup, analyzing their\nwithin-subgroup joint distribution is critical for better informed patient\ndecision-making. In this paper, we describe Bayesian models for performing a\nsubgroup analysis to compare the joint occurrence of a primary endpoint and an\nadverse event between two treatment arms. Our approaches emphasize estimation\nof heterogeneity in this joint distribution across subgroups, and our\napproaches directly accommodate subgroups with small numbers of observed\nprimary and adverse event combinations. In addition, we describe several ways\nin which our models may be used to generate interpretable summary measures of\nbenefit-risk tradeoffs for each subgroup. The methods described here are\nillustrated throughout using a large cardiovascular trial (N = 9,361)\ninvestigating the efficacy of an intervention for reducing systolic blood\npressure to a lower-than-usual target.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 14:44:14 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Henderson", "Nicholas C.", ""], ["Varadhan", "Ravi", ""]]}, {"id": "1808.03829", "submitter": "Moreno Bevilacqua", "authors": "M.Bevilacqua, C. Caama\\~no, C. Gaetan", "title": "On modelling positive continuous data with spatio-temporal dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we concentrate on an alternative modeling strategy for positive\ndata that exhibit spatial or spatio-temporal dependence. Specifically we\npropose to consider stochastic processes obtained trough a monotone\ntransformation of scaled version of $\\chi^2$ random processes. The latter are\nwell known in the specialized literature and originates by summing independent\ncopies of a squared Gaussian process. However their use as stochastic models\nand related inference have not been much considered.\n  Motivated by a spatio-temporal analysis of wind speed data from a network of\nmeteorological stations in the Netherlands, we exemplify our modeling strategy\nby means of a non-stationary process with Weibull marginal distributions. For\nthe proposed Weibull process we study the second-order and geometrical\nproperties and we provide analytic expressions for the bivariate distribution.\nSince the likelihood is intractable, even for relatively small data-set, we\nsuggest to adopt the pairwise likelihood as a tool for the inference. Moreover\nwe tackle the prediction problem and we propose a linear prediction. The\neffectiveness of our modeling strategy is illustrated through the analysis of\nthe aforementioned Netherland wind speed data that we supplement with a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Sat, 11 Aug 2018 16:23:49 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 12:08:50 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 17:24:16 GMT"}, {"version": "v4", "created": "Tue, 7 Apr 2020 15:12:45 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Bevilacqua", "M.", ""], ["Caama\u00f1o", "C.", ""], ["Gaetan", "C.", ""]]}, {"id": "1808.03889", "submitter": "Ziwei Zhu", "authors": "Jianqing Fan, Kaizheng Wang, Yiqiao Zhong, Ziwei Zhu", "title": "Robust high dimensional factor models with applications to statistical\n  machine learning", "comments": "41 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factor models are a class of powerful statistical models that have been\nwidely used to deal with dependent measurements that arise frequently from\nvarious applications from genomics and neuroscience to economics and finance.\nAs data are collected at an ever-growing scale, statistical machine learning\nfaces some new challenges: high dimensionality, strong dependence among\nobserved variables, heavy-tailed variables and heterogeneity. High-dimensional\nrobust factor analysis serves as a powerful toolkit to conquer these\nchallenges.\n  This paper gives a selective overview on recent advance on high-dimensional\nfactor models and their applications to statistics including Factor-Adjusted\nRobust Model selection (FarmSelect) and Factor-Adjusted Robust Multiple testing\n(FarmTest). We show that classical methods, especially principal component\nanalysis (PCA), can be tailored to many new problems and provide powerful tools\nfor statistical estimation and inference. We highlight PCA and its connections\nto matrix perturbation theory, robust statistics, random projection, false\ndiscovery rate, etc., and illustrate through several applications how insights\nfrom these fields yield solutions to modern challenges. We also present\nfar-reaching connections between factor models and popular statistical learning\nproblems, including network analysis and low-rank matrix recovery.\n", "versions": [{"version": "v1", "created": "Sun, 12 Aug 2018 04:34:24 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Fan", "Jianqing", ""], ["Wang", "Kaizheng", ""], ["Zhong", "Yiqiao", ""], ["Zhu", "Ziwei", ""]]}, {"id": "1808.04045", "submitter": "Chetkar Jha", "authors": "Chetkar Jha", "title": "A Nonparametric Bayesian Method for Clustering of High-Dimensional Mixed\n  Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is motivated from clustering problem in high-throughput mixed\ndatasets. Clustering of such datasets can provide much insight into biological\nassociations. An open problem in this context is to simultaneously cluster\nhigh-dimensional mixed dataset. This paper fills that gap and proposes a\nnonparametric Bayesian method called Gen-VariScan for biclustering of\nhigh-dimensional mixed dataset.\n  Gen-VariScan utilizes Generalized Linear Models (GLM), and latent variable\napproaches to integrate mixed dataset. We make use of Poisson Dirichlet Process\n(PDP) to identify a lower dimensional structure of mixed covariates. We show\nthat covariate co-cluster detection is aposteriori consistent, as the number of\nsubject and covariates grows. The advantage of Gen-VariScan is also\ndemonstrated through numerical simulation and data analysis. As a byproduct, we\nderive a working value approach to perform beta regression. Supplementary\nmaterials for this article are available online.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 02:20:14 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 07:49:43 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Jha", "Chetkar", ""]]}, {"id": "1808.04092", "submitter": "Florian Heinrichs", "authors": "Axel B\\\"ucher, Holger Dette, Florian Heinrichs", "title": "Detecting deviations from second-order stationarity in locally\n  stationary functional time series", "comments": "Key words: alpha-mixing, CUSUM-test, auto-covariance operator, block\n  multiplier bootstrap, change points", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A time-domain test for the assumption of second order stationarity of a\nfunctional time series is proposed. The test is based on combining individual\ncumulative sum tests which are designed to be sensitive to changes in the mean,\nvariance and autocovariance operators, respectively. The combination of their\ndependent $p$-values relies on a joint dependent block multiplier bootstrap of\nthe individual test statistics. Conditions under which the proposed combined\ntesting procedure is asymptotically valid under stationarity are provided. A\nprocedure is proposed to automatically choose the block length parameter needed\nfor the construction of the bootstrap. The finite-sample behavior of the\nproposed test is investigated in Monte Carlo experiments and an illustration on\na real data set is provided.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 07:57:27 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Dette", "Holger", ""], ["Heinrichs", "Florian", ""]]}, {"id": "1808.04139", "submitter": "Tapajit Dey", "authors": "Tapajit Dey, Audris Mockus", "title": "A Matching Based Theoretical Framework for Estimating Probability of\n  Causation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The concept of Probability of Causation (PC) is critically important in legal\ncontexts and can help in many other domains. While it has been around since\n1986, current operationalizations can obtain only the minimum and maximum\nvalues of PC, and do not apply for purely observational data. We present a\ntheoretical framework to estimate the distribution of PC from experimental and\nfrom purely observational data. We illustrate additional problems of the\nexisting operationalizations and show how our method can be used to address\nthem. We also provide two illustrative examples of how our method is used and\nhow factors like sample size or rarity of events can influence the distribution\nof PC. We hope this will make the concept of PC more widely usable in practice.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 10:28:05 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Dey", "Tapajit", ""], ["Mockus", "Audris", ""]]}, {"id": "1808.04233", "submitter": "Eric Benhamou", "authors": "Eric Benhamou", "title": "Connecting Sharpe ratio and Student t-statistic, and beyond", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sharpe ratio is widely used in asset management to compare and benchmark\nfunds and asset managers. It computes the ratio of the excess return over the\nstrategy standard deviation. However, the elements to compute the Sharpe ratio,\nnamely, the expected returns and the volatilities are unknown numbers and need\nto be estimated statistically. This means that the Sharpe ratio used by funds\nis subject to be error prone because of statistical estimation error. Lo\n(2002), Mertens (2002) derive explicit expressions for the statistical\ndistribution of the Sharpe ratio using standard asymptotic theory under several\nsets of assumptions (independent normally distributed - and identically\ndistributed returns). In this paper, we provide the exact distribution of the\nSharpe ratio for independent normally distributed return. In this case, the\nSharpe ratio statistic is up to a rescaling factor a non centered Student\ndistribution whose characteristics have been widely studied by statisticians.\nThe asymptotic behavior of our distribution provide the result of Lo (2002). We\nalso illustrate the fact that the empirical Sharpe ratio is asymptotically\noptimal in the sense that it achieves the Cramer Rao bound. We then study the\nempirical SR under AR(1) assumptions and investigate the effect of compounding\nperiod on the Sharpe (computing the annual Sharpe with monthly data for\ninstance). We finally provide general formula in this case of\nheteroscedasticity and autocorrelation.\n", "versions": [{"version": "v1", "created": "Thu, 2 Aug 2018 16:47:03 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 12:53:35 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 07:36:03 GMT"}, {"version": "v4", "created": "Tue, 14 May 2019 08:16:09 GMT"}], "update_date": "2019-05-15", "authors_parsed": [["Benhamou", "Eric", ""]]}, {"id": "1808.04246", "submitter": "Kolyan Ray", "authors": "Kolyan Ray and Aad van der Vaart", "title": "Semiparametric Bayesian causal inference", "comments": "54 pages", "journal-ref": "Ann. Statist. 48 (2020), 2999-3020", "doi": "10.1214/19-AOS1919", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a semiparametric Bayesian approach for estimating the mean\nresponse in a missing data model with binary outcomes and a nonparametrically\nmodelled propensity score. Equivalently we estimate the causal effect of a\ntreatment, correcting nonparametrically for confounding. We show that standard\nGaussian process priors satisfy a semiparametric Bernstein-von Mises theorem\nunder smoothness conditions. We further propose a novel propensity\nscore-dependent prior that provides efficient inference under strictly weaker\nconditions. We also show that it is theoretically preferable to model the\ncovariate distribution with a Dirichlet process or Bayesian bootstrap, rather\nthan modelling the covariate density using a Gaussian process prior.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 14:13:02 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 13:55:19 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Ray", "Kolyan", ""], ["van der Vaart", "Aad", ""]]}, {"id": "1808.04280", "submitter": "Erik-Sander Smits", "authors": "Erik-Sander Smits, Adam J. Pel, Michiel C.J. Bliemer, Bart van Arem", "title": "Generalized Multivariate Extreme Value Models for Explicit Route Choice\n  Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper analyses a class of route choice models with closed-form\nprobability expressions, namely, Generalized Multivariate Extreme Value (GMEV)\nmodels. A large group of these models emerge from different utility formulas\nthat combine systematic utility and random error terms. Twelve models are\ncaptured in a single discrete choice framework. The additive utility formula\nleads to the known logit family, being multinomial, path-size, paired\ncombinatorial and link-nested. For the multiplicative formulation only the\nmultinomial and path-size weibit models have been identified; this study also\nidentifies the paired combinatorial and link-nested variations, and generalizes\nthe path-size variant. Furthermore, a new traveller's decision rule based on\nthe multiplicative utility formula with a reference route is presented. Here\nthe traveller chooses exclusively based on the differences between routes. This\nleads to four new GMEV models. We assess the models qualitatively based on a\ngeneric structure of route utility with random foreseen travel times, for which\nwe empirically identify that the variance of utility should be different from\nthus far assumed for multinomial probit and logit-kernel models. The expected\ntravellers' behaviour and model-behaviour under simple network changes are\nanalysed. Furthermore, all models are estimated and validated on an\nillustrative network example with long distance and short distance\norigin-destination pairs. The new multiplicative models based on differences\noutperform the additive models in both tests.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 15:02:53 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Smits", "Erik-Sander", ""], ["Pel", "Adam J.", ""], ["Bliemer", "Michiel C. J.", ""], ["van Arem", "Bart", ""]]}, {"id": "1808.04312", "submitter": "Anne Presanis", "authors": "Daniela De Angelis, Anne M. Presanis", "title": "Analysing Multiple Epidemic Data Sources", "comments": "This manuscript is a preprint of a Chapter to appear in the \"Handbook\n  of Infectious Disease Data Analysis\", Held, L., Hens, N., O'Neill, P.D. and\n  Wallinga, J. (Eds.). Chapman & Hall/CRC, 2018. Please use the book for\n  possible citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidence-based knowledge of infectious disease burden, including prevalence,\nincidence, severity and transmission, in different population strata and\nlocations, and possibly in real time, is crucial to the planning and evaluation\nof public health policies. Direct observation of a disease process is rarely\npossible. However, latent characteristics of an epidemic and its evolution can\noften be inferred from the synthesis of indirect information from various\nroutine data sources, as well as expert opinion. The simultaneous synthesis of\nmultiple data sources, often conveniently carried out in a Bayesian framework,\nposes a number of statistical and computational challenges: the heterogeneity\nin type, relevance and granularity of the data, together with selection and\ninformative observation biases, lead to complex probabilistic models that are\ndifficult to build and fit, and challenging to criticize. Using motivating case\nstudies of influenza, this chapter illustrates the cycle of model development\nand criticism in the context of Bayesian evidence synthesis, highlighting the\nchallenges of complex model building, computationally efficient inference, and\nconflicting evidence.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 16:12:59 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["De Angelis", "Daniela", ""], ["Presanis", "Anne M.", ""]]}, {"id": "1808.04348", "submitter": "James S. Martin", "authors": "James S. Martin, David J. Murrell, Sofia C. Olhede", "title": "Multivariate Geometric Anisotropic Cox Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new modelling framework for multivariate anisotropic\nCox processes. Building on recent innovations in multivariate spatial\nstatistics, we propose a new family of multivariate anisotropic random fields\nand construct a family of anisotropic point processes from it. We give\nconditions that make the models valid, and we provide additional understanding\nof valid point process dependence. We also propose a likelihood-based inference\nmechanism for this type of process. Finally we illustrate the utility of the\nproposed modelling framework by analysing spatial ecological observations of\nplants and trees in the Barro Colorado Island study.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 17:52:43 GMT"}], "update_date": "2018-08-14", "authors_parsed": [["Martin", "James S.", ""], ["Murrell", "David J.", ""], ["Olhede", "Sofia C.", ""]]}, {"id": "1808.04401", "submitter": "Vladimir Minin", "authors": "James R. Faulkner, Andrew F. Magee, Beth Shapiro, Vladimir N. Minin", "title": "Horseshoe-based Bayesian nonparametric estimation of effective\n  population size trajectories", "comments": "36 pages, including supplementary information", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylodynamics is an area of population genetics that uses genetic sequence\ndata to estimate past population dynamics. Modern state-of-the-art Bayesian\nnonparametric methods for recovering population size trajectories of unknown\nform use either change-point models or Gaussian process priors. Change-point\nmodels suffer from computational issues when the number of change-points is\nunknown and needs to be estimated. Gaussian process-based methods lack local\nadaptivity and cannot accurately recover trajectories that exhibit features\nsuch as abrupt changes in trend or varying levels of smoothness. We propose a\nnovel, locally-adaptive approach to Bayesian nonparametric phylodynamic\ninference that has the flexibility to accommodate a large class of functional\nbehaviors. Local adaptivity results from modeling the log-transformed effective\npopulation size a priori as a horseshoe Markov random field, a recently\nproposed statistical model that blends together the best properties of the\nchange-point and Gaussian process modeling paradigms. We use simulated data to\nassess model performance, and find that our proposed method results in reduced\nbias and increased precision when compared to contemporary methods. We also use\nour models to reconstruct past changes in genetic diversity of human hepatitis\nC virus in Egypt and to estimate population size changes of ancient and modern\nsteppe bison. These analyses show that our new method captures features of the\npopulation size trajectories that were missed by the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 18:51:39 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 23:21:47 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Faulkner", "James R.", ""], ["Magee", "Andrew F.", ""], ["Shapiro", "Beth", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1808.04416", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Luke Keele, Rocio Titiunik, Gonzalo Vazquez-Bare", "title": "Extrapolating Treatment Effects in Multi-Cutoff Regression Discontinuity\n  Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In non-experimental settings, the Regression Discontinuity (RD) design is one\nof the most credible identification strategies for program evaluation and\ncausal inference. However, RD treatment effect estimands are necessarily local,\nmaking statistical methods for the extrapolation of these effects a key area\nfor development. We introduce a new method for extrapolation of RD effects that\nrelies on the presence of multiple cutoffs, and is therefore design-based. Our\napproach employs an easy-to-interpret identifying assumption that mimics the\nidea of \"common trends\" in difference-in-differences designs. We illustrate our\nmethods with data on a subsidized loan program on post-education attendance in\nColombia, and offer new evidence on program effects for students with test\nscores away from the cutoff that determined program eligibility.\n", "versions": [{"version": "v1", "created": "Mon, 13 Aug 2018 19:38:11 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2019 20:26:56 GMT"}, {"version": "v3", "created": "Wed, 1 Apr 2020 16:14:13 GMT"}], "update_date": "2020-04-02", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Keele", "Luke", ""], ["Titiunik", "Rocio", ""], ["Vazquez-Bare", "Gonzalo", ""]]}, {"id": "1808.04698", "submitter": "Mike West", "authors": "Lindsay R. Berry, Paul Helman and Mike West", "title": "Probabilistic forecasting of heterogeneous consumer transaction-sales\n  time series", "comments": "23 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new Bayesian methodology for consumer sales forecasting. With a\nfocus on multi-step ahead forecasting of daily sales of many supermarket items,\nwe adapt dynamic count mixture models to forecast individual customer\ntransactions, and introduce novel dynamic binary cascade models for predicting\ncounts of items per transaction. These transactions-sales models can\nincorporate time-varying trend, seasonal, price, promotion, random effects and\nother outlet-specific predictors for individual items. Sequential Bayesian\nanalysis involves fast, parallel filtering on sets of decoupled items and is\nadaptable across items that may exhibit widely varying characteristics. A\nmulti-scale approach enables information sharing across items with related\npatterns over time to improve prediction while maintaining scalability to many\nitems. A motivating case study in many-item, multi-period, multi-step ahead\nsupermarket sales forecasting provides examples that demonstrate improved\nforecast accuracy in multiple metrics, and illustrates the benefits of full\nprobabilistic models for forecast accuracy evaluation and comparison.\n  Keywords: Bayesian forecasting; decouple/recouple; dynamic binary cascade;\nforecast calibration; intermittent demand; multi-scale forecasting; predicting\nrare events; sales per transaction; supermarket sales forecasting\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 14:03:03 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2018 16:56:54 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Berry", "Lindsay R.", ""], ["Helman", "Paul", ""], ["West", "Mike", ""]]}, {"id": "1808.04753", "submitter": "Si Cheng", "authors": "Si Cheng (1), Daniel J. Eck (2), Forrest W. Crawford (3,4,5 and 6)\n  ((1) Department of Biostatistics, University of Washington, (2) Department of\n  Statistics, University of Illinois Urbana-Champaign, (3) Department of\n  Biostatistics, Yale School of Public Health, (4) Department of Statistics &\n  Data Science, Yale University, (5) Department of Ecology & Evolutionary\n  Biology, Yale University, (6) Yale School of Management)", "title": "Estimating the size of a hidden finite set: large-sample behavior of\n  estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A finite set is \"hidden\" if its elements are not directly enumerable or if\nits size cannot be ascertained via a deterministic query. In public health,\nepidemiology, demography, ecology and intelligence analysis, researchers have\ndeveloped a wide variety of indirect statistical approaches, under different\nmodels for sampling and observation, for estimating the size of a hidden set.\nSome methods make use of random sampling with known or estimable sampling\nprobabilities, and others make structural assumptions about relationships (e.g.\nordering or network information) between the elements that comprise the hidden\nset. In this review, we describe models and methods for learning about the size\nof a hidden finite set, with special attention to asymptotic properties of\nestimators. We study the properties of these methods under two asymptotic\nregimes, \"infill\" in which the number of fixed-size samples increases, but the\npopulation size remains constant, and \"outfill\" in which the sample size and\npopulation size grow together. Statistical properties under these two regimes\ncan be dramatically different.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 15:31:56 GMT"}, {"version": "v2", "created": "Tue, 15 Oct 2019 23:20:16 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Cheng", "Si", "", "3,4,5 and 6"], ["Eck", "Daniel J.", "", "3,4,5 and 6"], ["Crawford", "Forrest W.", "", "3,4,5 and 6"]]}, {"id": "1808.04773", "submitter": "Marzia Cremona", "authors": "Marzia A. Cremona and Francesca Chiaromonte", "title": "Probabilistic $K$-mean with local alignment for clustering and motif\n  discovery in functional data", "comments": "22 pages, 6 figures. This work has been presented at various\n  conferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new method to locally cluster curves and discover functional\nmotifs, i.e.~typical ``shapes'' that may recur several times along and across\nthe curves capturing important local characteristics. In order to identify\nthese shared curve portions, our method leverages ideas from functional data\nanalysis (joint clustering and alignment of curves), bioinformatics (local\nalignment through the extension of high similarity seeds) and fuzzy clustering\n(curves belonging to more than one cluster, if they contain more than one\ntypical ``shape''). It can employ various dissimilarity measures and\nincorporate derivatives in the discovery process, thus exploiting complex\nfacets of shapes. We demonstrate the performance of our method with an\nextensive simulation study, and show how it generalizes other clustering\nmethods for functional data. Finally, we provide real data applications to\nBerkeley growth data, Italian Covid-19 death curves and ``Omics'' data related\nto mutagenesis.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 16:11:15 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2020 16:01:12 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Cremona", "Marzia A.", ""], ["Chiaromonte", "Francesca", ""]]}, {"id": "1808.04780", "submitter": "Zhen Li", "authors": "Zhen Li, Lili Wu, Weilian Zhou and Sujit Ghosh", "title": "Multivariate Density Estimation with Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate density estimation is a popular technique in statistics with\nwide applications including regression models allowing for heteroskedasticity\nin conditional variances. The estimation problems become more challenging when\nobservations are missing in one or more variables of the multivariate vector. A\nflexible class of mixture of tensor products of kernel densities is proposed\nwhich allows for easy implementation of imputation methods using Gibbs sampling\nand shown to have superior performance compared to some of the exisiting\nimputation methods currently available in literature. Numerical illustrations\nare provided using several simulated data scenarios and applications to couple\nof case studies are also presented.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 16:17:40 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Li", "Zhen", ""], ["Wu", "Lili", ""], ["Zhou", "Weilian", ""], ["Ghosh", "Sujit", ""]]}, {"id": "1808.04906", "submitter": "Xu Shi", "authors": "Xu Shi and Wang Miao and Jennifer C. Nelson and Eric J. Tchetgen\n  Tchetgen", "title": "Multiply Robust Causal Inference with Double Negative Control Adjustment\n  for Categorical Unmeasured Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmeasured confounding is a threat to causal inference in observational\nstudies. In recent years, use of negative controls to mitigate unmeasured\nconfounding has gained increasing recognition and popularity. Negative controls\nhave a longstanding tradition in laboratory sciences and epidemiology to rule\nout non-causal explanations, although they have been used primarily for bias\ndetection. Recently, Miao et al. (2018) have described sufficient conditions\nunder which a pair of negative control exposure and outcome variables can be\nused to nonparametrically identify the average treatment effect (ATE) from\nobservational data subject to uncontrolled confounding. In this paper, we\nestablish nonparametric identification of the ATE under weaker conditions in\nthe case of categorical unmeasured confounding and negative control variables.\nWe also provide a general semiparametric framework for obtaining inferences\nabout the ATE while leveraging information about a possibly large number of\nmeasured covariates. In particular, we derive the semiparametric efficiency\nbound in the nonparametric model, and we propose multiply robust and locally\nefficient estimators when nonparametric estimation may not be feasible. We\nassess the finite sample performance of our methods in extensive simulation\nstudies. Finally, we illustrate our methods with an application to the\npostlicensure surveillance of vaccine safety among children.\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2018 21:48:13 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 22:08:19 GMT"}, {"version": "v3", "created": "Wed, 4 Sep 2019 14:52:08 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Shi", "Xu", ""], ["Miao", "Wang", ""], ["Nelson", "Jennifer C.", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1808.04945", "submitter": "Wang Miao", "authors": "Wang Miao, Xu Shi, and Eric Tchetgen Tchetgen", "title": "A Confounding Bridge Approach for Double Negative Control Inference on\n  Causal Effects", "comments": "Supplement and Sample Codes are included", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmeasured confounding is a key challenge for causal inference. Negative\ncontrol variables are widely available in observational studies. A negative\ncontrol outcome is associated with the confounder but not causally affected by\nthe exposure in view, and a negative control exposure is correlated with the\nprimary exposure or the confounder but does not causally affect the outcome of\ninterest. In this paper, we establish a framework to use them for unmeasured\nconfounding adjustment. We introduce a confounding bridge function that links\nthe potential outcome mean and the negative control outcome distribution, and\nwe incorporate a negative control exposure to identify the bridge function and\nthe average causal effect. Our approach can be used to repair an invalid\ninstrumental variable in case it is correlated with the unmeasured confounder.\nWe also extend our approach by allowing for a causal association between the\nprimary exposure and the control outcome. We illustrate our approach with\nsimulations and apply it to a study about the short-term effect of air\npollution. Although a standard analysis shows a significant acute effect of\nPM2.5 on mortality, our analysis indicates that this effect may be confounded,\nand after double negative control adjustment, the effect is attenuated toward\nzero.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 02:00:46 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 12:06:49 GMT"}, {"version": "v3", "created": "Fri, 18 Sep 2020 12:03:11 GMT"}], "update_date": "2020-09-21", "authors_parsed": [["Miao", "Wang", ""], ["Shi", "Xu", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "1808.05069", "submitter": "Sven Knoth", "authors": "Sven Knoth", "title": "The Steady-State Behavior of Multivariate Exponentially Weighted Moving\n  Average Control Charts", "comments": null, "journal-ref": null, "doi": "10.1080/07474946.2018.1554890", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate Exponentially Weighted Moving Average, MEWMA, charts are\npopular, handy and effective procedures to detect distributional changes in a\nstream of multivariate data. For doing appropriate performance analysis,\ndealing with the steady-state behavior of the MEWMA statistic is essential.\nGoing beyond early papers, we derive quite accurate approximations of the\nrespective steady-state densities of the MEWMA statistic. It turns out that\nthese densities could be rewritten as the product of two functions depending on\none argument only which allows feasible calculation. For proving the related\nstatements, the presentation of the non-central chisquare density deploying the\nconfluent hypergeometric limit function is applied. Using the new methods it\nwas found that for large dimensions, the steady-state behavior becomes\ndifferent to what one might expect from the univariate monitoring field. Based\non the integral equation driven methods, steady-state and worst-case average\nrun lengths are calculated with higher accuracy than before. Eventually,\noptimal MEWMA smoothing constants are derived for all considered measures.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 13:30:54 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Knoth", "Sven", ""]]}, {"id": "1808.05126", "submitter": "Satya Singh P", "authors": "Satya Prakash Singh and Pradeep Yadav", "title": "Optimal allocation of subjects in a cluster randomized trial with fixed\n  number of clusters when the ICCs or costs are heterogeneous over clusters", "comments": "There are some technical flaws in the proofs of theorems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intra-cluster correlation coefficient (ICC) plays an important role while\ndesigning the cluster randomized trials (CRTs). Often optimal CRTs are designed\nassuming that the magnitude of the ICC is constant across the clusters.\nHowever, this assumption is hardly satisfied. In some applications, the precise\ninformation about the cluster specific correlation is known in advance. In this\narticle, we propose an optimal design with non-constant ICC across the\nclusters. Also in many situations, the cost of sampling of an observation from\na particular cluster may differ from that of some other cluster. An optimal\ndesign in those scenarios is also obtained assuming unequal costs of sampling\nfrom different clusters. The theoretical findings are supplemented by thorough\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 15:20:25 GMT"}, {"version": "v2", "created": "Wed, 20 Feb 2019 17:25:00 GMT"}], "update_date": "2019-02-21", "authors_parsed": [["Singh", "Satya Prakash", ""], ["Yadav", "Pradeep", ""]]}, {"id": "1808.05185", "submitter": "Tin Lok James Ng", "authors": "Tin Lok James Ng, Thomas Brendan Murphy", "title": "Model-based clustering for random hypergraphs", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A probabilistic model for random hypergraphs is introduced to represent\nunary, binary and higher order interactions among objects in real-world\nproblems. This model is an extension of the Latent Class Analysis model, which\ncaptures clustering structures among objects. An EM (expectation maximization)\nalgorithm with MM (minorization maximization) steps is developed to perform\nparameter estimation while a cross validated likelihood approach is employed to\nperform model selection. The developed model is applied to three real-world\ndata sets where interesting results are obtained.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 17:04:36 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Ng", "Tin Lok James", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1808.05260", "submitter": "Derek Feng", "authors": "Derek Feng, Randolf Altmeyer, Derek Stafford, Nicholas A. Christakis\n  and Harrison H. Zhou", "title": "Testing for Balance in Social Networks", "comments": "Accepted to the Journal of the American Statistical Association", "journal-ref": null, "doi": "10.1080/01621459.2020.1764850", "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Friendship and antipathy exist in concert with one another in real social\nnetworks. Despite the role they play in social interactions, antagonistic ties\nare poorly understood and infrequently measured. One important theory of\nnegative ties that has received relatively little empirical evaluation is\nbalance theory, the codification of the adage `the enemy of my enemy is my\nfriend' and similar sayings. Unbalanced triangles are those with an odd number\nof negative ties, and the theory posits that such triangles are rare. To test\nfor balance, previous works have utilized a permutation test on the edge signs.\nThe flaw in this method, however, is that it assumes that negative and positive\nedges are interchangeable. In reality, they could not be more different. Here,\nwe propose a novel test of balance that accounts for this discrepancy and show\nthat our test is more accurate at detecting balance. Along the way, we prove\nasymptotic normality of the test statistic under our null model, which is of\nindependent interest. Our case study is a novel dataset of signed networks we\ncollected from 32 isolated, rural villages in Honduras. Contrary to previous\nresults, we find that there is only marginal evidence for balance in social tie\nformation in this setting.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 19:14:30 GMT"}, {"version": "v2", "created": "Tue, 26 May 2020 16:15:02 GMT"}], "update_date": "2020-05-27", "authors_parsed": [["Feng", "Derek", ""], ["Altmeyer", "Randolf", ""], ["Stafford", "Derek", ""], ["Christakis", "Nicholas A.", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1808.05291", "submitter": "Michael Hornstein", "authors": "Michael Hornstein, Shuheng Zhou, Kerby Shedden", "title": "Tensor models for linguistics pitch curve data of native speakers of\n  Afrikaans", "comments": "20 pages, 7 figures, 1 table in main paper; 51 pages, 48 figures, 2\n  tables in Appendix", "journal-ref": null, "doi": null, "report-no": "University of Michigan Technical Report 544", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use tensor analysis techniques for high-dimensional data to gain insight\ninto pitch curves, which play an important role in linguistics research. In\nparticular, we propose that demeaned phonetics pitch curve data can be modeled\nas having a Kronecker product inverse covariance structure with sparse factors\ncorresponding to words and time. Using data from a study of native Afrikaans\nspeakers, we show that by targeting conditional independence through a\ngraphical model, we reveal relationships associated with natural properties of\nwords as studied by linguists. We find that words with long vowels cluster\nbased on whether the vowel is pronounced at the front or back of the mouth, and\nwords with short vowels have strong edges associated with the initial\nconsonant.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 21:56:29 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Hornstein", "Michael", ""], ["Zhou", "Shuheng", ""], ["Shedden", "Kerby", ""]]}, {"id": "1808.05339", "submitter": "Fan Li", "authors": "Fan Li and Fan Li", "title": "Propensity Score Weighting for Causal Inference with Multiple Treatments", "comments": "30 pages, 3 figures, 5 tables", "journal-ref": "The Annals of Applied Statistics, 2019", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal or unconfounded descriptive comparisons between multiple groups are\ncommon in observational studies. Motivated from a racial disparity study in\nhealth services research, we propose a unified propensity score weighting\nframework, the balancing weights, for estimating causal effects with multiple\ntreatments. These weights incorporate the generalized propensity scores to\nbalance the weighted covariate distribution of each treatment group, all\nweighted toward a common pre-specified target population. The class of\nbalancing weights include several existing approaches such as the inverse\nprobability weights and trimming weights as special cases. Within this\nframework, we propose a set of target estimands based on linear contrasts. We\nfurther develop the generalized overlap weights, constructed as the product of\nthe inverse probability weights and the harmonic mean of the generalized\npropensity scores. The generalized overlap weighting scheme corresponds to the\ntarget population with the most overlap in covariates across the multiple\ntreatments. These weights are bounded and thus bypass the problem of extreme\npropensities. We show that the generalized overlap weights minimize the total\nasymptotic variance of the moment weighting estimators for the pairwise\ncontrasts within the class of balancing weights. We consider two balance check\ncriteria and propose a new sandwich variance estimator for estimating the\ncausal effects with generalized overlap weights. We apply these methods to\nstudy the racial disparities in medical expenditure between several racial\ngroups using the 2009 Medical Expenditure Panel Survey (MEPS) data. Simulations\nwere carried out to compare with existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 03:46:56 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 00:56:52 GMT"}, {"version": "v3", "created": "Sat, 29 Jun 2019 01:38:28 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Li", "Fan", ""], ["Li", "Fan", ""]]}, {"id": "1808.05362", "submitter": "Dandan Jiang", "authors": "Dandan Jiang and Zhidong Bai", "title": "Generalized Four Moment Theorem and an Application to CLT for Spiked\n  Eigenvalues of Large-dimensional Covariance Matrices", "comments": "48 pages, 4 figures,5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a more generalized spiked covariance matrix $\\Sigma$, which is a\ngeneral non-definite matrix with the spiked eigenvalues scattered into a few\nbulks and the largest ones allowed to tend to infinity. By relaxing the\nmatching of the 4th moment to a tail probability decay, a {\\it Generalized Four\nMoment Theorem} (G4MT) is proposed to show the universality of the asymptotic\nlaw for the local spectral statistics of generalized spiked covariance\nmatrices, which implies the limiting distribution of the spiked eigenvalues of\nthe generalized spiked covariance matrix is independent of the actual\ndistributions of the samples satisfying our relaxed assumptions. Moreover, by\napplying it to the Central Limit Theorem (CLT) for the spiked eigenvalues of\nthe generalized spiked covariance matrix, we also extend the result of Bai and\nYao (2012) to a general form of the population covariance matrix, where the 4th\nmoment is not necessarily required to exist and the spiked eigenvalues are\nallowed to be dependent on the non-spiked ones, thus meeting the actual cases\nbetter.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 07:15:28 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 10:55:34 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 03:22:18 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Jiang", "Dandan", ""], ["Bai", "Zhidong", ""]]}, {"id": "1808.05414", "submitter": "Wenlin Dai", "authors": "Wenlin Dai, Tomas Mrkvicka, Ying Sun, and Marc G. Genton", "title": "Functional Outlier Detection and Taxonomy by Sequential Transformations", "comments": "32 pages, 9 figures, and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis can be seriously impaired by abnormal observations,\nwhich can be classified as either magnitude or shape outliers based on their\nway of deviating from the bulk of data. Identifying magnitude outliers is\nrelatively easy, while detecting shape outliers is much more challenging. We\npropose turning the shape outliers into magnitude outliers through data\ntransformation and detecting them using the functional boxplot. Besides easing\nthe detection procedure, applying several transformations sequentially provides\na reasonable taxonomy for the flagged outliers. A joint functional ranking,\nwhich consists of several transformations, is also defined here. Simulation\nstudies are carried out to evaluate the performance of the proposed method\nusing different functional depth notions. Interesting results are obtained in\nseveral practical applications.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 11:04:35 GMT"}, {"version": "v2", "created": "Sat, 21 Mar 2020 16:14:13 GMT"}], "update_date": "2020-03-24", "authors_parsed": [["Dai", "Wenlin", ""], ["Mrkvicka", "Tomas", ""], ["Sun", "Ying", ""], ["Genton", "Marc G.", ""]]}, {"id": "1808.05441", "submitter": "Georg  Stadler", "authors": "Benjamin Crestel, Georg Stadler, Omar Ghattas", "title": "A comparative study of structural similarity and regularization for\n  joint inverse problems governed by PDEs", "comments": null, "journal-ref": null, "doi": "10.1088/1361-6420/aaf129", "report-no": null, "categories": "math.NA math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint inversion refers to the simultaneous inference of multiple parameter\nfields from observations of systems governed by single or multiple forward\nmodels. In many cases these parameter fields reflect different attributes of a\nsingle medium and are thus spatially correlated or structurally similar. By\nimposing prior information on their spatial correlations via a joint\nregularization term, we seek to improve the reconstruction of the parameter\nfields relative to inversion for each field independently. One of the main\nchallenges is to devise a joint regularization functional that conveys the\nspatial correlations or structural similarity between the fields while at the\nsame time permitting scalable and efficient solvers for the joint inverse\nproblem. We describe several joint regularizations that are motivated by these\ngoals: a cross-gradient and a normalized cross-gradient structural similarity\nterm, the vectorial total variation, and a joint regularization based on the\nnuclear norm of the gradients. Based on numerical results from three classes of\ninverse problems with piecewise-homogeneous parameter fields, we conclude that\nthe vectorial total variation functional is preferable to the other methods\nconsidered. Besides resulting in good reconstructions in all experiments, it\nallows for scalable, efficient solvers for joint inverse problems governed by\nPDE forward models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 12:20:49 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Crestel", "Benjamin", ""], ["Stadler", "Georg", ""], ["Ghattas", "Omar", ""]]}, {"id": "1808.05465", "submitter": "William Rosenthal", "authors": "Weixuan Li, W. Steven Rosenthal, Guang Lin", "title": "Trimmed Ensemble Kalman Filter for Nonlinear and Non-Gaussian Data\n  Assimilation Problems", "comments": "In revision, SIAM Journal of Uncertainty Quantification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the ensemble Kalman filter (EnKF) algorithm for sequential data\nassimilation in a general situation, that is, for nonlinear forecast and\nmeasurement models with non-additive and non-Gaussian noises. Such applications\ntraditionally force us to choose between inaccurate Gaussian assumptions that\npermit efficient algorithms (e.g., EnKF), or more accurate direct sampling\nmethods which scale poorly with dimension (e.g., particle filters, or PF). We\nintroduce a trimmed ensemble Kalman filter (TEnKF) which can interpolate\nbetween the limiting distributions of the EnKF and PF to facilitate adaptive\ncontrol over both accuracy and efficiency. This is achieved by introducing a\ntrimming function that removes non-Gaussian outliers that introduce errors in\nthe correlation between the model and observed forecast, which otherwise\nprevent the EnKF from proposing accurate forecast updates. We show for specific\ntrimming functions that the TEnKF exactly reproduces the limiting distributions\nof the EnKF and PF. We also develop an adaptive implementation which provides\ncontrol of the effective sample size and allows the filter to overcome periods\nof increased model nonlinearity. This algorithm allow us to demonstrate\nsubstantial improvements over the traditional EnKF in convergence and\nrobustness for the nonlinear Lorenz-63 and Lorenz-96 models.\n", "versions": [{"version": "v1", "created": "Wed, 15 Aug 2018 08:10:30 GMT"}], "update_date": "2018-08-17", "authors_parsed": [["Li", "Weixuan", ""], ["Rosenthal", "W. Steven", ""], ["Lin", "Guang", ""]]}, {"id": "1808.05528", "submitter": "Jesse Hemerik", "authors": "Jesse Hemerik, Aldo Solari, Jelle J. Goeman", "title": "Permutation-based simultaneous confidence bounds for the false discovery\n  proportion", "comments": null, "journal-ref": "Biometrika, 106(3):635-649, 2019", "doi": "10.1093/biomet/asz021", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When multiple hypotheses are tested, interest is often in ensuring that the\nproportion of false discoveries (FDP) is small with high confidence. In this\npaper, confidence upper bounds for the FDP are constructed, which are\nsimultaneous over all rejection cut-offs. In particular this allows the user to\nselect a set of hypotheses post hoc such that the FDP lies below some constant\nwith high confidence. Our method uses permutations to account for the\ndependence structure in the data. So far only Meinshausen provided an exact,\npermutation-based and computationally feasible method for simultaneous FDP\nbounds. We provide an exact method, which uniformly improves this procedure.\nFurther, we provide a generalization of this method. It lets the user select\nthe shape of the simultaneous confidence bounds. This gives the user more\nfreedom in determining the power properties of the method. Interestingly,\nseveral existing permutation methods, such as Significance Analysis of\nMicroarrays (SAM) and Westfall and Young's maxT method, are obtained as special\ncases.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 15:03:50 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Hemerik", "Jesse", ""], ["Solari", "Aldo", ""], ["Goeman", "Jelle J.", ""]]}, {"id": "1808.05541", "submitter": "Rune Christiansen", "authors": "Rune Christiansen (University of Copenhagen) and Jonas Peters\n  (University of Copenhagen)", "title": "Switching Regression Models and Causal Inference in the Presence of\n  Discrete Latent Variables", "comments": "46 pages, 14 figures; real-world application added in Section 5.2;\n  additional numerical experiments added in the Appendix E", "journal-ref": "Journal of Machine Learning Research 21(41): 1--46, 2020", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a response $Y$ and a vector $X = (X^1, \\dots, X^d)$ of $d$ predictors,\nwe investigate the problem of inferring direct causes of $Y$ among the vector\n$X$. Models for $Y$ that use all of its causal covariates as predictors enjoy\nthe property of being invariant across different environments or interventional\nsettings. Given data from such environments, this property has been exploited\nfor causal discovery. Here, we extend this inference principle to situations in\nwhich some (discrete-valued) direct causes of $ Y $ are unobserved. Such cases\nnaturally give rise to switching regression models. We provide sufficient\nconditions for the existence, consistency and asymptotic normality of the MLE\nin linear switching regression models with Gaussian noise, and construct a test\nfor the equality of such models. These results allow us to prove that the\nproposed causal discovery method obtains asymptotic false discovery control\nunder mild conditions. We provide an algorithm, make available code, and test\nour method on simulated data. It is robust against model violations and\noutperforms state-of-the-art approaches. We further apply our method to a real\ndata set, where we show that it does not only output causal predictors, but\nalso a process-based clustering of data points, which could be of additional\ninterest to practitioners.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 15:35:04 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2018 10:14:09 GMT"}, {"version": "v3", "created": "Wed, 11 Mar 2020 12:41:54 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Christiansen", "Rune", "", "University of Copenhagen"], ["Peters", "Jonas", "", "University of Copenhagen"]]}, {"id": "1808.05771", "submitter": "Sina Molavipour", "authors": "Sina Molavipour, Germ\\'an Bassi, Mikael Skoglund", "title": "Non-Asymptotic Behavior of the Maximum Likelihood Estimate of a Discrete\n  Distribution", "comments": "30 pages, 1 figure, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the maximum likelihood estimate of the probability\nmass function (pmf) of $n$ independent and identically distributed (i.i.d.)\nrandom variables, in the non-asymptotic regime. We are interested in\ncharacterizing the Neyman--Pearson criterion, i.e., the log-likelihood ratio\nfor testing a true hypothesis within a larger hypothesis. Wilks' theorem states\nthat this ratio behaves like a $\\chi^2$ random variable in the asymptotic case;\nhowever, less is known about the precise behavior of the ratio when the number\nof samples is finite. In this work, we find an explicit bound for the\ndifference between the cumulative distribution function (cdf) of the\nlog-likelihood ratio and the cdf of a $\\chi^2$ random variable. Furthermore, we\nshow that this difference vanishes with a rate of order $1/\\sqrt{n}$ in\naccordance with Wilks' theorem.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 06:40:11 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 16:49:37 GMT"}], "update_date": "2020-11-23", "authors_parsed": [["Molavipour", "Sina", ""], ["Bassi", "Germ\u00e1n", ""], ["Skoglund", "Mikael", ""]]}, {"id": "1808.05878", "submitter": "Dwueng-Chwuan Jhwueng", "authors": "Dwueng-Chwuan Jhwueng", "title": "Statistical modeling for adaptive trait evolution in randomly evolving\n  environment", "comments": "26 pages, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In past decades, Gaussian processes has been widely applied in studying trait\nevolution using phylogenetic comparative analysis. In particular, two members\nof Gaussian processes: Brownian motion and Ornstein-Uhlenbeck process, have\nbeen frequently used to describe continuous trait evolution. Under the\nassumption of adaptive evolution, several models have been created around\nOrnstein-Uhlenbeck process where the optimum $\\theta^y_t$ of a single trait\n$y_t$ is influenced with predictor $x_t$. Since in general the dynamics of rate\nof evolution $\\tau^y_t$ of trait could adopt a pertinent process, in this work\nwe extend models of adaptive evolution by considering the rate of evolution\n$\\tau_t^y$ following the Cox-Ingersoll-Ross (CIR) process. We provide a\nheuristic Monte Carlo simulation scheme to simulate trait along the phylogeny\nas a structure of dependence among species. We add a framework to incorporate\nmultiple regression with interaction between optimum of the trait and its\npotential predictors. Since the likelihood function for our models are\nintractable, we propose the use of Approximate Bayesian Computation (ABC) for\nparameter estimation and inference. Simulation as well as empirical study using\nthe proposed models are also performed and carried out to validate our models\nand for practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 14:25:39 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["Jhwueng", "Dwueng-Chwuan", ""]]}, {"id": "1808.05889", "submitter": "Andreas Lindholm", "authors": "Andreas Svensson, Dave Zachariah, Petre Stoica, and Thomas B. Sch\\\"on", "title": "Data Consistency Approach to Model Validation", "comments": null, "journal-ref": "IEEE Access, 7(1):59788-59796, 2019", "doi": null, "report-no": null, "categories": "stat.ME eess.SP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In scientific inference problems, the underlying statistical modeling\nassumptions have a crucial impact on the end results. There exist, however,\nonly a few automatic means for validating these fundamental modelling\nassumptions. The contribution in this paper is a general criterion to evaluate\nthe consistency of a set of statistical models with respect to observed data.\nThis is achieved by automatically gauging the models' ability to generate data\nthat is similar to the observed data. Importantly, the criterion follows from\nthe model class itself and is therefore directly applicable to a broad range of\ninference problems with varying data types, ranging from independent univariate\ndata to high-dimensional time-series. The proposed data consistency criterion\nis illustrated, evaluated and compared to several well-established methods\nusing three synthetic and two real data sets.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 14:51:09 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 07:07:35 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Svensson", "Andreas", ""], ["Zachariah", "Dave", ""], ["Stoica", "Petre", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1808.05895", "submitter": "Chris McKennan", "authors": "Chris McKennan and Dan Nicolae", "title": "Estimating and accounting for unobserved covariates in high dimensional\n  correlated data", "comments": "22 slides (54 including the supplement), 3 tables, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many high dimensional and high-throughput biological datasets have complex\nsample correlation structures, which include longitudinal and multiple tissue\ndata, as well as data with multiple treatment conditions or related\nindividuals. These data, as well as nearly all high-throughput `omic' data, are\ninfluenced by technical and biological factors unknown to the researcher,\nwhich, if unaccounted for, can severely obfuscate estimation and inference on\neffects due to the known covariate of interest. We therefore developed CBCV and\nCorrConf: provably accurate and computationally efficient methods to choose the\nnumber of and estimate latent confounding factors present in high dimensional\ndata with correlated or nonexchangeable residuals. We demonstrate each method's\nsuperior performance compared to other state of the art methods by analyzing\nsimulated multi-tissue gene expression data and identifying sex-associated DNA\nmethylation sites in a real, longitudinal twin study. As far as we are aware,\nthese are the first methods to estimate the number of and correct for latent\nconfounding factors in data with correlated or nonexchangeable residuals. An\nR-package is available for download at\nhttps://github.com/chrismckennan/CorrConf.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 15:12:39 GMT"}], "update_date": "2018-08-20", "authors_parsed": [["McKennan", "Chris", ""], ["Nicolae", "Dan", ""]]}, {"id": "1808.06016", "submitter": "Ruben Zamar", "authors": "Ginette Lafit, Francisco J. Nogales, Marcelo Ruiz and Ruben H. Zamar", "title": "A Stepwise Approach for High-Dimensional Gaussian Graphical Models", "comments": "26 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a stepwise approach to estimate high dimensional Gaussian\ngraphical models. We exploit the relation between the partial correlation\ncoefficients and the distribution of the prediction errors, and parametrize the\nmodel in terms of the Pearson correlation coefficients between the prediction\nerrors of the nodes' best linear predictors. We propose a novel stepwise\nalgorithm for detecting pairs of conditionally dependent variables. We show\nthat the proposed algorithm outperforms existing methods such as the graphical\nlasso and CLIME in simulation studies and real life applications. In our\ncomparison we report different performance measures that look at different\ndesirable features of the recovered graph and consider several model settings.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2018 22:36:10 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Lafit", "Ginette", ""], ["Nogales", "Francisco J.", ""], ["Ruiz", "Marcelo", ""], ["Zamar", "Ruben H.", ""]]}, {"id": "1808.06038", "submitter": "Xu Shi", "authors": "Eric J. Tchetgen Tchetgen, Xu Shi, Tamar Sofer, Benedict H.W. Wong", "title": "A general approach to detect gene (G)-environment (E) additive\n  interaction leveraging G-E independence in case-control studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is increasingly of interest in statistical genetics to test for the\npresence of a mechanistic interaction between genetic (G) and environmental (E)\nrisk factors by testing for the presence of an additive GxE interaction. In\ncase-control studies involving a rare disease, a statistical test of no\nadditive interaction typically entails a test of no relative excess risk due to\ninteraction (RERI). It is also well known that a test of multiplicative\ninteraction exploiting G-E independence can be dramatically more powerful than\nstandard logistic regression for case-control data. Likewise, it has recently\nbeen shown that a likelihood ratio test of a null RERI incorporating the G-E\nindependence assumption (RERI-LRT) outperforms the standard RERI approach. In\nthis paper, the authors describe a general, yet relatively straightforward\napproach to test for GxE additive interaction exploiting G-E independence. The\napproach which relies on regression models for G and E is particularly\nattractive because, unlike the RERI-LRT, it allows the regression model for the\nbinary outcome to remain unrestricted. Therefore, the new methodology is\ncompletely robust to possible mis-specification in the outcome regression. This\nis particularly important for settings not easily handled by RERI-LRT, such as\nwhen E is a count or a continuous exposure with multiple components, or when\nthere are several auxiliary covariates in the regression model. While the\nproposed approach avoids fitting an outcome regression, it nonetheless still\nallows for straightforward covariate adjustment. The methods are illustrated\nthrough an extensive simulation study and an ovarian cancer empirical\napplication.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 03:10:21 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Tchetgen", "Eric J. Tchetgen", ""], ["Shi", "Xu", ""], ["Sofer", "Tamar", ""], ["Wong", "Benedict H. W.", ""]]}, {"id": "1808.06040", "submitter": "Benjamin D. Wandelt", "authors": "Justin Alsing, Benjamin D. Wandelt, Stephen M. Feeney", "title": "Optimal proposals for Approximate Bayesian Computation", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the optimal proposal density for Approximate Bayesian Computation\n(ABC) using Sequential Monte Carlo (SMC) (or Population Monte Carlo, PMC). The\ncriterion for optimality is that the SMC/PMC-ABC sampler maximise the effective\nnumber of samples per parameter proposal. The optimal proposal density\nrepresents the optimal trade-off between favoring high acceptance rate and\nreducing the variance of the importance weights of accepted samples. We discuss\ntwo convenient approximations of this proposal and show that the optimal\nproposal density gives a significant boost in the expected sampling efficiency\ncompared to standard kernels that are in common use in the ABC literature,\nespecially as the number of parameters increases.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2018 04:52:37 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Alsing", "Justin", ""], ["Wandelt", "Benjamin D.", ""], ["Feeney", "Stephen M.", ""]]}, {"id": "1808.06152", "submitter": "Jayant Gupchup A", "authors": "Jayant Gupchup, Ebrahim Beyrami, Martin Ellis, Yasaman Hosseinkashi,\n  Sam Johnson, Ross Cutler", "title": "On Design of Problem Token Questions in Quality of Experience Surveys", "comments": null, "journal-ref": null, "doi": "10.1109/QoMEX.2018.8463424", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User surveys for Quality of Experience (QoE) are a critical source of\ninformation. In addition to the common \"star rating\" used to estimate Mean\nOpinion Score (MOS), more detailed survey questions (problem tokens) about\nspecific areas provide valuable insight into the factors impacting QoE. This\npaper explores two aspects of the problem token questionnaire design. First, we\nstudy the bias introduced by fixed question order, and second, we study the\nchallenge of selecting a subset of questions to keep the token set small. Based\non 900,000 calls gathered using a randomized controlled experiment from a live\nsystem, we find that the order bias can be significantly reduced by randomizing\nthe display order of tokens. The difference in response rate varies based on\ntoken position and display design. It is worth noting that the users respond to\nthe randomized-order variant at levels that are comparable to the fixed-order\nvariant. The effective selection of a subset of token questions is achieved by\nextracting tokens that provide the highest information gain over user ratings.\nThis selection is known to be in the class of NP-hard problems. We apply a\nwell-known greedy submodular maximization method on our dataset to capture 94%\nof the information using just 30% of the questions.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 02:16:40 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Gupchup", "Jayant", ""], ["Beyrami", "Ebrahim", ""], ["Ellis", "Martin", ""], ["Hosseinkashi", "Yasaman", ""], ["Johnson", "Sam", ""], ["Cutler", "Ross", ""]]}, {"id": "1808.06222", "submitter": "Albert Vexler", "authors": "Albert Vexler, Li Zou and Alan D. Hutson", "title": "The empirical likelihood prior applied to bias reduction of general\n  estimating equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The practice of employing empirical likelihood (EL) components in place of\nparametric likelihood functions in the construction of Bayesian-type procedures\nhas been well-addressed in the modern statistical literature. We rigorously\nderive the EL prior, a Jeffreys-type prior, which asymptotically maximizes the\nShannon mutual information between data and the parameters of interest. The\nfocus of our approach is on an integrated Kullback-Leibler distance between the\nEL-based posterior and prior density functions. The EL prior density is the\ndensity function for which the corresponding posterior form is asymptotically\nnegligibly different from the EL. We show that the proposed result can be used\nto develop a methodology for reducing the asymptotic bias of solutions of\ngeneral estimating equations and M-estimation schemes by removing the\nfirst-order term. This technique is developed in a similar manner to methods\nemployed to reduce the asymptotic bias of maximum likelihood estimates via\npenalizing the underlying parametric likelihoods by their Jeffreys invariant\npriors. A real data example related to a study of myocardial infarction\nillustrates the attractiveness of the proposed technique in practical aspects.\n  Keywords: Asymptotic bias, Biased estimating equations, Empirical likelihood,\nExpected Kullback-Leibler distance, Penalized likelihood, Reference prior.\n", "versions": [{"version": "v1", "created": "Sun, 19 Aug 2018 16:10:24 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Vexler", "Albert", ""], ["Zou", "Li", ""], ["Hutson", "Alan D.", ""]]}, {"id": "1808.06310", "submitter": "Daniel Nevo", "authors": "Daniel Nevo, Judith J. Lok, Donna Spiegelman", "title": "Analysis of \"Learn-As-You-Go\" (LAGO) Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In learn-as-you-go (LAGO) adaptive studies, the intervention is a complex\npackage consisting of multiple components, and is adapted in stages during the\nstudy based on past outcome data. This design formalizes standard practice, and\ndesires for practice, in public health intervention studies. An effective\nintervention package is sought, while minimizing intervention package cost.\nWhen analyzing data from a learn-as-you-go study, the interventions in later\nstages depend upon the outcomes in the previous stages, violating standard\nstatistical theory. We develop methods for estimating the intervention effects\nin a LAGO study. We prove consistency and asymptotic normality using a novel\ncoupling argument, ensuring the validity of the test for the hypothesis of no\noverall intervention effect. We develop a confidence set for the optimal\nintervention package and confidence bands for the success probabilities under\nalternative package compositions. We illustrate our methods in the BetterBirth\nStudy, which aimed to improve maternal and neonatal outcomes among 157,689\nbirths in Uttar Pradesh, India through a complex, multi-component intervention\npackage.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 05:29:53 GMT"}, {"version": "v2", "created": "Wed, 23 Jan 2019 08:43:31 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Nevo", "Daniel", ""], ["Lok", "Judith J.", ""], ["Spiegelman", "Donna", ""]]}, {"id": "1808.06399", "submitter": "Holger Sennhenn-Reulen", "authors": "Holger Sennhenn-Reulen", "title": "Bayesian Regression for a Dirichlet Distributed Response using Stan", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For an observed response that is composed by a set - or vector - of positive\nvalues that sum up to 1, the Dirichlet distribution (Bol'shev, 2018) is a\nhelpful mathematical construction for the quantification of the data-generating\nmechanics underlying this process. In applications, these response-sets are\nusually denoted as proportions, or compositions of proportions, and by means of\ncovariates, one wishes to manifest the underlying signal - by changes in the\nvalue of these covariates - leading to differently distributed response\ncompositions. This article gives a brief introduction into this class of\nregression models, and based on a recently developed formulation (Maier, 2014),\nillustrates the implementation in the Bayesian inference framework Stan.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 11:45:58 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["Sennhenn-Reulen", "Holger", ""]]}, {"id": "1808.06408", "submitter": "Shu Yang", "authors": "Shu Yang, Karen Pieper, and Frank Cools", "title": "Semiparametric estimation of structural failure time model in\n  continuous-time processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural failure time models are causal models for estimating the effect of\ntime-varying treatments on a survival outcome. G-estimation and artificial\ncensoring have been proposed to estimate the model parameters in the presence\nof time-dependent confounding and administrative censoring. However, most of\nexisting methods require manually preprocessing data into regularly spaced\ndata, which may invalidate the subsequent causal analysis. Moreover, the\ncomputation and inference are challenging due to the non-smoothness of\nartificial censoring. We propose a class of continuous-time structural failure\ntime models, which respects the continuous time nature of the underlying data\nprocesses. Under a martingale condition of no unmeasured confounding, we show\nthat the model parameters are identifiable from potentially infinite estimating\nequations. Using the semiparametric efficiency theory, we derive the first\nsemiparametric doubly robust estimators, in the sense that the estimators are\nconsistent if either the treatment process model or the failure time model is\ncorrectly specified, but not necessarily both. Moreover, we propose using\ninverse probability of censoring weighting to deal with dependent censoring. In\ncontrast to artificial censoring, our weighting strategy does not introduce\nnon-smoothness in estimation and ensures that the resampling methods can be\nused to make inference.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 12:01:43 GMT"}, {"version": "v2", "created": "Sun, 17 Feb 2019 14:55:19 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Yang", "Shu", ""], ["Pieper", "Karen", ""], ["Cools", "Frank", ""]]}, {"id": "1808.06418", "submitter": "Luke Keele", "authors": "Hyunseung Kang and Luke Keele", "title": "Spillover Effects in Cluster Randomized Trials with Noncompliance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster randomized trials (CRTs) are popular in public health and in the\nsocial sciences to evaluate a new treatment or policy where the new policy is\nrandomly allocated to clusters of units rather than individual units. CRTs\noften feature both noncompliance, when individuals within a cluster are not\nexposed to the intervention, and individuals within a cluster may influence\neach other through treatment spillovers where those who comply with the new\npolicy may affect the outcomes of those who do not. Here, we study the\nidentification of causal effects in CRTs when both noncompliance and treatment\nspillovers are present. We prove that the standard analysis of CRT data with\nnoncompliance using instrumental variables does not identify the usual complier\naverage causal effect when treatment spillovers are present. We extend this\nresult and show that no analysis of CRT data can unbiasedly estimate local\nnetwork causal effects. Finally, we develop bounds for these causal effects\nunder the assumption that the treatment is not harmful compared to the control.\nWe demonstrate these results with an empirical study of a deworming\nintervention in Kenya.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 12:29:43 GMT"}, {"version": "v2", "created": "Thu, 15 Aug 2019 00:18:09 GMT"}], "update_date": "2019-08-16", "authors_parsed": [["Kang", "Hyunseung", ""], ["Keele", "Luke", ""]]}, {"id": "1808.06482", "submitter": "Tomohiro Nishiyama", "authors": "Tomohiro Nishiyama", "title": "Divergence functions in dually flat spaces and their properties", "comments": "13 pages, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.DG math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of statistics, many kind of divergence functions have been\nstudied as an amount which measures the discrepancy between two probability\ndistributions. In the differential geometrical approach in statistics\n(information geometry), dually flat spaces play a key role. In a dually flat\nspace, there exist dual affine coordinate systems and strictly convex functions\ncalled potential and a canonical divergence is naturally introduced as a\nfunction of the affine coordinates and potentials. The canonical divergence\nsatisfies a relational expression called triangular relation. This can be\nregarded as a generalization of the law of cosines in Euclidean space.\n  In this paper, we newly introduce two kinds of divergences. The first\ndivergence is a function of affine coordinates and it is consistent with the\nJeffreys divergence for exponential or mixture families. For this divergence,\nwe show that more relational equations and theorems similar to Euclidean space\nhold in addition to the law of cosines. The second divergences are functions of\npotentials and they are consistent with the Bhattacharyya distance for\nexponential families and are consistent with the Jensen-Shannon divergence for\nmixture families respectively. We derive an inequality between the the first\nand the second divergences and show that the inequality is a generalization of\nLin's inequality.\n", "versions": [{"version": "v1", "created": "Thu, 16 Aug 2018 08:00:34 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 13:18:47 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Nishiyama", "Tomohiro", ""]]}, {"id": "1808.06518", "submitter": "Zhaoxing Gao", "authors": "Zhaoxing Gao and Ruey S Tsay", "title": "A Structural-Factor Approach to Modeling High-Dimensional Time Series\n  and Space-Time Data", "comments": "29 pages, 10 figures. arXiv admin note: text overlap with\n  arXiv:1310.1990 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a structural-factor approach to modeling\nhigh-dimensional time series and space-time data by decomposing individual\nseries into trend, seasonal, and irregular components. For ease in analyzing\nmany time series, we employ a time polynomial for the trend, a linear\ncombination of trigonometric series for the seasonal component, and a new\nfactor model for the irregular components. The new factor model can simplify\nthe modeling process and achieve parsimony in parameterization. We propose a\nBayesian Information Criterion (BIC) to consistently determine the order of the\npolynomial trend and the number of trigonometric functions. A test statistic is\nused to determine the number of common factors. The convergence rates for the\nestimators of the trend and seasonal components and the limiting distribution\nof the test statistic are established under the setting that the number of time\nseries tends to infinity with the sample size, but at a slower rate. We use\nsimulation to study the performance of the proposed analysis in finite samples\nand apply the proposed approach to two real examples. The first example\nconsiders modeling weekly PM$_{2.5}$ data of 15 monitoring stations in the\nsouthern region of Taiwan and the second example consists of monthly\nvalue-weighted returns of 12 industrial portfolios.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 15:39:00 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2019 16:25:55 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Gao", "Zhaoxing", ""], ["Tsay", "Ruey S", ""]]}, {"id": "1808.06638", "submitter": "Patrick Staples", "authors": "Patrick Staples, Min Ouyang, Robert F. Dougherty, Gregory A. Ryslik,\n  and Paul Dagum", "title": "Supervised Kernel PCA For Longitudinal Data", "comments": "17 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In statistical learning, high covariate dimensionality poses challenges for\nrobust prediction and inference. To address this challenge, supervised\ndimension reduction is often performed, where dependence on the outcome is\nmaximized for a selected covariate subspace with smaller dimensionality.\nPrevalent dimension reduction techniques assume data are $i.i.d.$, which is not\nappropriate for longitudinal data comprising multiple subjects with repeated\nmeasurements over time. In this paper, we derive a decomposition of the\nHilbert-Schmidt Independence Criterion as a supervised loss function for\nlongitudinal data, enabling dimension reduction between and within clusters\nseparately, and propose a dimensionality-reduction technique, $sklPCA$, that\nperforms this decomposed dimension reduction. We also show that this technique\nyields superior model accuracy compared to the model it extends.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 18:18:52 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 16:29:09 GMT"}, {"version": "v3", "created": "Thu, 23 Aug 2018 02:20:15 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Staples", "Patrick", ""], ["Ouyang", "Min", ""], ["Dougherty", "Robert F.", ""], ["Ryslik", "Gregory A.", ""], ["Dagum", "Paul", ""]]}, {"id": "1808.06689", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal and Daniel C. Bourgeois", "title": "Bayesian Function-on-Scalars Regression for High Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a fully Bayesian framework for function-on-scalars regression with\nmany predictors. The functional data response is modeled nonparametrically\nusing unknown basis functions, which produces a flexible and data-adaptive\nfunctional basis. We incorporate shrinkage priors that effectively remove\nunimportant scalar covariates from the model and reduce sensitivity to the\nnumber of (unknown) basis functions. For variable selection in functional\nregression, we propose a decision theoretic posterior summarization technique,\nwhich identifies a subset of covariates that retains nearly the predictive\naccuracy of the full model. Our approach is broadly applicable for Bayesian\nfunctional regression models, and unlike existing methods provides joint rather\nthan marginal selection of important predictor variables. Computationally\nscalable posterior inference is achieved using a Gibbs sampler with linear time\ncomplexity in the number of predictors. The resulting algorithm is empirically\nfaster than existing frequentist and Bayesian techniques, and provides joint\nestimation of model parameters, prediction and imputation of functional\ntrajectories, and uncertainty quantification via the posterior distribution. A\nsimulation study demonstrates improvements in estimation accuracy, uncertainty\nquantification, and variable selection relative to existing alternatives. The\nmethodology is applied to actigraphy data to investigate the association\nbetween intraday physical activity and responses to a sleep questionnaire.\n", "versions": [{"version": "v1", "created": "Mon, 20 Aug 2018 20:56:10 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 03:09:12 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Kowal", "Daniel R.", ""], ["Bourgeois", "Daniel C.", ""]]}, {"id": "1808.06924", "submitter": "Xing-gang Mao", "authors": "Xing-gang Mao, Xiao-yan Xue", "title": "General hypergeometric distribution: A basic statistical distribution\n  for the number of overlapped elements in multiple subsets drawn from a finite\n  population", "comments": "22 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General hypergeometric distribution (GHGD) describes the following\ndistribution: from a finite space containing N elements, select T subsets with\neach subset contains M[i] (T-1 >= i >= 0) elements, what is the probability\nthat exactly x elements are overlapped exactly t times or at least t times\n(XLO=t or XLO>=t, T >= t >= 0, here LO is level of overlap)? The classical\nhypergeometric distribution (HGD) describes the situation of two subsets, while\nthe general situation has not been resolved, despite the overlapped elements\nhas been visualized with the Venn diagram method for about 140 years. GHGD\ndescribed not only the distribution of XLO=t or XLO>=t that are overlapped in\nall of the subsets (XLO=T), but also the XLO=t or XLO>=t that are overlapped in\na portion of the subsets (LO = t or LO >= t, T >= t >= 0). Here, we developed\nalgorithms to calculate the GHGD and discovered graceful formulas of the\nessential statistics for the GHGD, including mathematical expectation,\nvariance, and high order moments. In addition, statistical theory to infer a\nstatistically reliable gene set from multiple datasets based on these formulas\nwas established by applying Chebyshev's inequalities.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 14:31:22 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2018 01:51:30 GMT"}], "update_date": "2018-12-13", "authors_parsed": [["Mao", "Xing-gang", ""], ["Xue", "Xiao-yan", ""]]}, {"id": "1808.06952", "submitter": "Vincent Audigier", "authors": "Avner Bar-Hen and Vincent Audigier", "title": "An ensemble learning method for variable selection: application to high\n  dimensional data and missing values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard approaches for variable selection in linear models are not tailored\nto deal properly with high-dimensional and incomplete data. Currently, methods\ndedicated to high-dimensional data handle missing values by ad-hoc strategies,\nlike complete case analysis or single imputation, while methods dedicated to\nmissing values, mainly based on multiple imputation, do not discuss the\nimputation method to use with high-dimensional data. Consequently, both\napproaches appear to be limited for many modern applications.\n  With inspiration from ensemble methods, a new variable selection method is\nproposed. It extends classical variable selection methods in the case of\nhigh-dimensional data with or without missing data. Theoretical properties are\nstudied and the practical interest is demonstrated through a simulation study,\nas well as through an application to models specification in sequential\nmultiple imputation.\n  In the low dimensional case, the procedure improves the control of the error\nrisks, especially type I error, even without missing values for stepwise, lasso\nor knockoff methods. With missing values, the method performs better than\nreference selection methods based on multiple imputation. Similar performances\nare obtained in the high-dimensional case with or without missing values.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 14:58:45 GMT"}, {"version": "v2", "created": "Mon, 9 Mar 2020 10:23:10 GMT"}, {"version": "v3", "created": "Tue, 8 Jun 2021 12:48:33 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Bar-Hen", "Avner", ""], ["Audigier", "Vincent", ""]]}, {"id": "1808.07086", "submitter": "Andi Wang", "authors": "Andi Q. Wang, Gareth O. Roberts, David Steinsaltz", "title": "An approximation scheme for quasi-stationary distributions of killed\n  diffusions", "comments": "v2: revised version, 29 pages, 1 figure", "journal-ref": "Stoc. Proc. Appl. (2020) 130(5): 3193-3219", "doi": "10.1016/j.spa.2019.09.010", "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the asymptotic behavior of the normalized weighted\nempirical occupation measures of a diffusion process on a compact manifold\nwhich is killed at a smooth rate and then regenerated at a random location,\ndistributed according to the weighted empirical occupation measure. We show\nthat the weighted occupation measures almost surely comprise an asymptotic\npseudo-trajectory for a certain deterministic measure-valued semiflow, after\nsuitably rescaling the time, and that with probability one they converge to the\nquasi-stationary distribution of the killed diffusion. These results provide\ntheoretical justification for a scalable quasi-stationary Monte Carlo method\nfor sampling from Bayesian posterior distributions.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 19:03:11 GMT"}, {"version": "v2", "created": "Mon, 1 Jul 2019 14:36:04 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Wang", "Andi Q.", ""], ["Roberts", "Gareth O.", ""], ["Steinsaltz", "David", ""]]}, {"id": "1808.07140", "submitter": "Daniel W. Heck", "authors": "Daniel W. Heck and Clintin P. Davis-Stober", "title": "Multinomial Models with Linear Inequality Constraints: Overview and\n  Improvements of Computational Methods for Bayesian Inference", "comments": null, "journal-ref": "Journal of Mathematical Psychology (2019) 91, 70-87", "doi": "10.1016/j.jmp.2019.03.004", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many psychological theories can be operationalized as linear inequality\nconstraints on the parameters of multinomial distributions (e.g., discrete\nchoice analysis). These constraints can be described in two equivalent ways:\nEither as the solution set to a system of linear inequalities or as the convex\nhull of a set of extremal points (vertices). For both representations, we\ndescribe a general Gibbs sampler for drawing posterior samples in order to\ncarry out Bayesian analyses. We also summarize alternative sampling methods for\nestimating Bayes factors for these model representations using the encompassing\nBayes factor method. We introduce the R package multinomineq, which provides an\neasily-accessible interface to a computationally efficient implementation of\nthese techniques.\n", "versions": [{"version": "v1", "created": "Tue, 21 Aug 2018 21:35:36 GMT"}, {"version": "v2", "created": "Mon, 11 Feb 2019 08:16:47 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Heck", "Daniel W.", ""], ["Davis-Stober", "Clintin P.", ""]]}, {"id": "1808.07287", "submitter": "Palash Ghosh", "authors": "Palash Ghosh and Bibhas Chakraborty", "title": "Comparison of Dynamic Treatment Regimes with An Ordinal Outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential multiple assignment randomized trials (SMART) are used to develop\noptimal treatment strategies for patients based on their medical histories in\ndifferent branches of medical and behavioral sciences where a sequence of\ntreatments are given to the patients; such sequential treatment strategies are\noften called dynamic treatment regimes. In the existing literature, the\nmajority of the analysis methodologies for SMART studies assume a continuous\nprimary outcome. However, ordinal outcomes are also quite common in medical\npractice; for example, the quality of life (poor, moderate, good) is an ordinal\nvariable. In this work, first, we develop the notion of dynamic generalized\nodds-ratio ($dGOR$) to compare two dynamic treatment regimes embedded in a\n2-stage SMART with an ordinal outcome. We propose a likelihood-based approach\nto estimate $dGOR$ from SMART data. Next, we discuss some results related to\n$dGOR$ and derive the asymptotic properties of it's estimate. We derive the\nrequired sample size formula. Then, we extend the proposed methodology to a\n$K$-stage SMART. Finally, we discuss some alternative ways to estimate $dGOR$\nusing concordant-discordant pairs and multi-sample $U$-statistic. A simulation\nstudy shows the performance of the estimated $dGOR$ in terms of the estimated\npower corresponding to the derived sample size. We analyze data from STAR*D, a\nmultistage randomized clinical trial for treating major depression, to\nillustrate the proposed methodology. A freely available online tool using R\nstatistical software is provided to make the proposed method accessible to\nother researchers and practitioners.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 09:11:22 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Ghosh", "Palash", ""], ["Chakraborty", "Bibhas", ""]]}, {"id": "1808.07309", "submitter": "BaoLuo Sun", "authors": "Katherine Evans, BaoLuo Sun, James Robins and Eric J. Tchetgen\n  Tchetgen", "title": "Doubly Robust Regression Analysis for Data Fusion", "comments": null, "journal-ref": null, "doi": "10.5705/ss.202018.0334", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problem of making inference about a parametric\nmodel for the regression of an outcome variable $Y$ on covariates $(V,L)$ when\ndata are fused from two separate sources, one which contains information only\non $(V, Y)$ while the other contains information only on covariates. This data\nfusion setting may be viewed as an extreme form of missing data in which the\nprobability of observing complete data $(V,L,Y)$ on any given subject is zero.\nWe have developed a large class of semiparametric estimators, which includes\ndoubly robust estimators, of the regression coefficients in fused data. The\nproposed method is DR in that it is consistent and asymptotically normal if, in\naddition to the model of interest, we correctly specify a model for either the\ndata source process under an ignorability assumption, or the distribution of\nunobserved covariates. We evaluate the performance of our various estimators\nvia an extensive simulation study, and apply the proposed methods to\ninvestigate the relationship between net asset value and total expenditure\namong U.S. households in 1998, while controlling for potential confounders\nincluding income and other demographic variables.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 10:50:25 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 13:44:47 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Evans", "Katherine", ""], ["Sun", "BaoLuo", ""], ["Robins", "James", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "1808.07387", "submitter": "Michal Koles\\'ar", "authors": "Timothy B. Armstrong and Michal Koles\\'ar", "title": "Sensitivity Analysis using Approximate Moment Condition Models", "comments": "69 pages, plus a 12-page supplemental appendix", "journal-ref": "Quantitative Economics, Volume 12, Issue 1, January 2021, pages\n  77-108", "doi": "10.3982/QE1609", "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider inference in models defined by approximate moment conditions. We\nshow that near-optimal confidence intervals (CIs) can be formed by taking a\ngeneralized method of moments (GMM) estimator, and adding and subtracting the\nstandard error times a critical value that takes into account the potential\nbias from misspecification of the moment conditions. In order to optimize\nperformance under potential misspecification, the weighting matrix for this GMM\nestimator takes into account this potential bias, and therefore differs from\nthe one that is optimal under correct specification. To formally show the\nnear-optimality of these CIs, we develop asymptotic efficiency bounds for\ninference in the locally misspecified GMM setting. These bounds may be of\nindependent interest, due to their implications for the possibility of using\nmoment selection procedures when conducting inference in moment condition\nmodels. We apply our methods in an empirical application to automobile demand,\nand show that adjusting the weighting matrix can shrink the CIs by a factor of\n3 or more.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 14:43:38 GMT"}, {"version": "v2", "created": "Fri, 16 Nov 2018 15:18:15 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 20:46:05 GMT"}, {"version": "v4", "created": "Tue, 21 Apr 2020 16:27:50 GMT"}, {"version": "v5", "created": "Wed, 29 Jul 2020 14:38:58 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Armstrong", "Timothy B.", ""], ["Koles\u00e1r", "Michal", ""]]}, {"id": "1808.07433", "submitter": "Fangzheng Xie", "authors": "Fangzheng Xie, Yanxun Xu, Carey E. Priebe, Joshua Cape", "title": "Bayesian Estimation of Sparse Spiked Covariance Matrices in High\n  Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian methodology for estimating spiked covariance matrices\nwith jointly sparse structure in high dimensions. The spiked covariance matrix\nis reparametrized in terms of the latent factor model, where the loading matrix\nis equipped with a novel matrix spike-and-slab LASSO prior, which is a\ncontinuous shrinkage prior for modeling jointly sparse matrices. We establish\nthe rate-optimal posterior contraction for the covariance matrix with respect\nto the operator norm as well as that for the principal subspace with respect to\nthe projection operator norm loss. We also study the posterior contraction rate\nof the principal subspace with respect to the two-to-infinity norm loss, a\nnovel loss function measuring the distance between subspaces that is able to\ncapture element-wise eigenvector perturbations. We show that the posterior\ncontraction rate with respect to the two-to-infinity norm loss is tighter than\nthat with respect to the routinely used projection operator norm loss under\ncertain low-rank and bounded coherence conditions. In addition, a point\nestimator for the principal subspace is proposed with the rate-optimal risk\nbound with respect to the projection operator norm loss. These results are\nbased on a collection of concentration and large deviation inequalities for the\nmatrix spike-and-slab LASSO prior. The numerical performance of the proposed\nmethodology is assessed through synthetic examples and the analysis of a\nreal-world face data example.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 16:42:04 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2019 03:04:23 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["Xie", "Fangzheng", ""], ["Xu", "Yanxun", ""], ["Priebe", "Carey E.", ""], ["Cape", "Joshua", ""]]}, {"id": "1808.07449", "submitter": "Simon Vandekar", "authors": "Simon N. Vandekar, Theodore D. Satterthwaite, Cedric H. Xia, Kosha\n  Ruparel, Ruben C. Gur, Raquel E. Gur, and Russell T. Shinohara", "title": "Robust Spatial Extent Inference with a Semiparametric Bootstrap Joint\n  Testing Procedure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial extent inference (SEI) is widely used across neuroimaging modalities\nto study brain-phenotype associations that inform our understanding of disease.\nRecent studies have shown that Gaussian random field (GRF) based tools can have\ninflated family-wise error rates (FWERs). This has led to fervent discussion as\nto which preprocessing steps are necessary to control the FWER using GRF-based\nSEI. The failure of GRF-based methods is due to unrealistic assumptions about\nthe covariance function of the imaging data. The permutation procedure is the\nmost robust SEI tool because it estimates the covariance function from the\nimaging data. However, the permutation procedure can fail because its\nassumption of exchangeability is violated in many imaging modalities. Here, we\npropose the (semi-) parametric bootstrap joint (PBJ; sPBJ) testing procedures\nthat are designed for SEI of multilevel imaging data. The sPBJ procedure uses a\nrobust estimate of the covariance function, which yields consistent estimates\nof standard errors, even if the covariance model is misspecified. We use our\nmethods to study the association between performance and executive functioning\nin a working fMRI study. The sPBJ procedure is robust to variance\nmisspecification and maintains nominal FWER in small samples, in contrast to\nthe GRF methods. The sPBJ also has equal or superior power to the PBJ and\npermutation procedures. We provide an R package\nhttps://github.com/simonvandekar/pbj to perform inference using the PBJ and\nsPBJ procedures\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 17:27:05 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Vandekar", "Simon N.", ""], ["Satterthwaite", "Theodore D.", ""], ["Xia", "Cedric H.", ""], ["Ruparel", "Kosha", ""], ["Gur", "Ruben C.", ""], ["Gur", "Raquel E.", ""], ["Shinohara", "Russell T.", ""]]}, {"id": "1808.07501", "submitter": "Spencer Greenberg", "authors": "Spencer Greenberg", "title": "Calibration Scoring Rules for Practical Prediction Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In situations where forecasters are scored on the quality of their\nprobabilistic predictions, it is standard to use `proper' scoring rules to\nperform such scoring. These rules are desirable because they give forecasters\nno incentive to lie about their probabilistic beliefs. However, in the real\nworld context of creating a training program designed to help people improve\ncalibration through prediction practice, there are a variety of desirable\ntraits for scoring rules that go beyond properness. These potentially may have\na substantial impact on the user experience, usability of the program, or\nefficiency of learning. The space of proper scoring rules is too broad, in the\nsense that most proper scoring rules lack these other desirable properties. On\nthe other hand, the space of proper scoring rules is potentially also too\nnarrow, in the sense that we may sometimes choose to give up properness when it\nconflicts with other properties that are even more desirable from the point of\nview of usability and effective training. We introduce a class of scoring rules\nthat we call `Practical' scoring rules, designed to be intuitive to users in\nthe context of `right' vs. `wrong' probabilistic predictions. We also introduce\ntwo specific scoring rules for prediction intervals, the `Distance' and `Order\nof magnitude' rules. These rules are designed to satisfy a variety of\nproperties that, based on user testing, we believe are desirable for applied\ncalibration training.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 18:06:38 GMT"}, {"version": "v2", "created": "Sun, 23 Aug 2020 17:22:26 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Greenberg", "Spencer", ""]]}, {"id": "1808.07563", "submitter": "Art Owen", "authors": "Art B. Owen and Hal Varian", "title": "Optimizing the tie-breaker regression discontinuity design", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by customer loyalty plans and scholarship programs, we study\ntie-breaker designs which are hybrids of randomized controlled trials (RCTs)\nand regression discontinuity designs (RDDs). We quantify the statistical\nefficiency of a tie-breaker design in which a proportion $\\Delta$ of observed\nsubjects are in the RCT. In a two line regression, statistical efficiency\nincreases monotonically with $\\Delta$, so efficiency is maximized by an RCT. We\npoint to additional advantages of tie-breakers versus RDD: for a nonparametric\nregression the boundary bias is much less severe and for quadratic regression,\nthe variance is greatly reduced. For a two line model we can quantify the short\nterm value of the treatment allocation and this comparison favors smaller\n$\\Delta$ with the RDD being best. We solve for the optimal tradeoff between\nthese exploration and exploitation goals. The usual tie-breaker design applies\nan RCT on the middle $\\Delta$ subjects as ranked by the assignment variable. We\nquantify the efficiency of other designs such as experimenting only in the\nsecond decile from the top. We also show that in some general parametric models\na Monte Carlo evaluation can be replaced by matrix algebra.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 20:57:38 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 21:42:35 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2020 22:39:41 GMT"}], "update_date": "2020-08-03", "authors_parsed": [["Owen", "Art B.", ""], ["Varian", "Hal", ""]]}, {"id": "1808.07704", "submitter": "Shrijita Bhattacharya", "authors": "Shrijita Bhattacharya, Michael Kallitsis and Stilian Stoev", "title": "Data-adaptive trimming of the Hill estimator and detection of outliers\n  in the extremes of heavy-tailed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a trimmed version of the Hill estimator for the index of a\nheavy-tailed distribution, which is robust to perturbations in the extreme\norder statistics. In the ideal Pareto setting, the estimator is essentially\nfinite-sample efficient among all unbiased estimators with a given strict upper\nbreak-down point. For general heavy-tailed models, we establish the asymptotic\nnormality of the estimator under second order regular variation conditions and\nalso show it is minimax rate-optimal in the Hall class of distributions. We\nalso develop an automatic, data-driven method for the choice of the trimming\nparameter which yields a new type of robust estimator that can adapt to the\nunknown level of contamination in the extremes. This adaptive robustness\nproperty makes our estimator particularly appealing and superior to other\nrobust estimators in the setting where the extremes of the data are\ncontaminated. As an important application of the data-driven selection of the\ntrimming parameters, we obtain a methodology for the principled identification\nof extreme outliers in heavy tailed data. Indeed, the method has been shown to\ncorrectly identify the number of outliers in the previously explored Condroz\ndata set.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 11:36:27 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Bhattacharya", "Shrijita", ""], ["Kallitsis", "Michael", ""], ["Stoev", "Stilian", ""]]}, {"id": "1808.07719", "submitter": "Katya Mauff", "authors": "Katya Mauff, Ewout Steyerberg, Isabella Kardys, Eric Boersma, Dimitris\n  Rizopoulos", "title": "Joint Models with Multiple Longitudinal Outcomes and a Time-to-Event\n  Outcome: a Corrected Two-Stage Approach", "comments": "33 pages, 7 figures and 7 tables including appendices. Accepted in\n  Statistics and Computing", "journal-ref": null, "doi": "10.1007/s11222-020-09927-9", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint models for longitudinal and survival data have gained a lot of\nattention in recent years, with the development of myriad extensions to the\nbasic model, including those which allow for multivariate longitudinal data,\ncompeting risks and recurrent events. Several software packages are now also\navailable for their implementation. Although mathematically straightforward,\nthe inclusion of multiple longitudinal outcomes in the joint model remains\ncomputationally difficult due to the large number of random effects required,\nwhich hampers the practical application of this extension. We present a novel\napproach that enables the fitting of such models with more realistic\ncomputational times. The idea behind the approach is to split the estimation of\nthe joint model in two steps; estimating a multivariate mixed model for the\nlongitudinal outcomes, and then using the output from this model to fit the\nsurvival submodel. So called two-stage approaches have previously been\nproposed, and shown to be biased. Our approach differs from the standard\nversion, in that we additionally propose the application of a correction\nfactor, adjusting the estimates obtained such that they more closely resemble\nthose we would expect to find with the multivariate joint model. This\ncorrection is based on importance sampling ideas. Simulation studies show that\nthis corrected-two-stage approach works satisfactorily, eliminating the bias\nwhile maintaining substantial improvement in computational time, even in more\ndifficult settings.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 12:41:24 GMT"}, {"version": "v2", "created": "Tue, 19 Mar 2019 13:19:45 GMT"}, {"version": "v3", "created": "Fri, 31 Jan 2020 10:30:59 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Mauff", "Katya", ""], ["Steyerberg", "Ewout", ""], ["Kardys", "Isabella", ""], ["Boersma", "Eric", ""], ["Rizopoulos", "Dimitris", ""]]}, {"id": "1808.07861", "submitter": "Dalia Ghanem", "authors": "Xiaomeng Cui, Dalia Ghanem and Todd Kuffner", "title": "On model selection criteria for climate change impact studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate change impact studies inform policymakers on the estimated damages of\nfuture climate change on economic, health and other outcomes. In most studies,\nan annual outcome variable is observed, e.g. agricultural yield, annual\nmortality or gross domestic product, along with a higher-frequency regressor,\ne.g. daily temperature. While applied researchers tend to consider multiple\nmodels to characterize the relationship between the outcome and the\nhigh-frequency regressor, to inform policy a choice between the damage\nfunctions implied by the different models has to be made. This paper formalizes\nthe model selection problem in this empirical setting and provides conditions\nfor the consistency of Monte Carlo Cross-validation and generalized information\ncriteria. A simulation study illustrates the theoretical results and points to\nthe relevance of the signal-to-noise ratio for the finite-sample behavior of\nthe model selection criteria. Two empirical applications with starkly different\nsignal-to-noise ratios illustrate the practical implications of the formal\nanalysis on model selection criteria provided in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 17:48:00 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 17:17:57 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Cui", "Xiaomeng", ""], ["Ghanem", "Dalia", ""], ["Kuffner", "Todd", ""]]}, {"id": "1808.07932", "submitter": "Zhaoxing Gao", "authors": "Zhaoxing Gao, Ruey S. Tsay", "title": "Modeling High-Dimensional Time Series: A Factor Model with Dynamically\n  Dependent Factors and Diverging Eigenvalues", "comments": "37 pages, 3 figures", "journal-ref": "Journal of the American Statistical Association, 2020", "doi": "10.1080/01621459.2020.1862668", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a new approach to modeling high-dimensional time series\nby treating a $p$-dimensional time series as a nonsingular linear\ntransformation of certain common factors and idiosyncratic components. Unlike\nthe approximate factor models, we assume that the factors capture all the\nnon-trivial dynamics of the data, but the cross-sectional dependence may be\nexplained by both the factors and the idiosyncratic components. Under the\nproposed model, (a) the factor process is dynamically dependent and the\nidiosyncratic component is a white noise process, and (b) the largest\neigenvalues of the covariance matrix of the idiosyncratic components may\ndiverge to infinity as the dimension $p$ increases. We propose a white noise\ntesting procedure for high-dimensional time series to determine the number of\nwhite noise components and, hence, the number of common factors, and introduce\na projected Principal Component Analysis (PCA) to eliminate the diverging\neffect of the idiosyncratic noises. Asymptotic properties of the proposed\nmethod are established for both fixed $p$ and diverging $p$ as the sample size\n$n$ increases to infinity. We use both simulated data and real examples to\nassess the performance of the proposed method. We also compare our method with\ntwo commonly used methods in the literature concerning the forecastability of\nthe extracted factors and find that the proposed approach not only provides\ninterpretable results, but also performs well in out-of-sample forecasting.\nSupplementary materials of the article are available online.\n", "versions": [{"version": "v1", "created": "Thu, 23 Aug 2018 20:24:36 GMT"}, {"version": "v2", "created": "Tue, 2 Jul 2019 21:20:55 GMT"}, {"version": "v3", "created": "Wed, 1 Jul 2020 14:50:57 GMT"}], "update_date": "2020-12-15", "authors_parsed": [["Gao", "Zhaoxing", ""], ["Tsay", "Ruey S.", ""]]}, {"id": "1808.08086", "submitter": "Alberto Sorrentino", "authors": "Gianvittorio Luria, Dunja Duran, Elisa Visani, Sara Sommariva, Fabio\n  Rotondi, Davide Rossi Sebastiano, Ferruccio Panzica, Michele Piana, Alberto\n  Sorrentino", "title": "Bayesian Multi--Dipole Modeling in the Frequency Domain", "comments": null, "journal-ref": "Journal of Neuroscience Methods Volume 312, 15 January 2019, Pages\n  27-36", "doi": "10.1016/j.jneumeth.2018.11.007", "report-no": null, "categories": "q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Magneto- and Electro-encephalography record the electromagnetic\nfield generated by neural currents with high temporal frequency and good\nspatial resolution, and are therefore well suited for source localization in\nthe time and in the frequency domain. In particular, localization of the\ngenerators of neural oscillations is very important in the study of cognitive\nprocesses in the healthy and in the pathological brain.\n  New method: We introduce the use of a Bayesian multi-dipole localization\nmethod in the frequency domain. Given the Fourier Transform of the data at one\nor multiple frequencies and/or trials, the algorithm approximates numerically\nthe posterior distribution with Monte Carlo techniques.\n  Results: We use synthetic data to show that the proposed method behaves well\nunder a wide range of experimental conditions, including low signal-to-noise\nratios and correlated sources. We use dipole clusters to mimic the effect of\nextended sources. In addition, we test the algorithm on real MEG data to\nconfirm its feasibility.\n  Comparison with existing method(s): Throughout the whole study, DICS (Dynamic\nImaging of Coherent Sources) is used systematically as a benchmark. The two\nmethods provide similar general pictures; the posterior distributions of the\nBayesian approach contain much richer information at the price of a higher\ncomputational cost.\n  Conclusions: The Bayesian method described in this paper represents a\nreliable approach for localization of multiple dipoles in the frequency domain.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 10:55:07 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 14:21:54 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Luria", "Gianvittorio", ""], ["Duran", "Dunja", ""], ["Visani", "Elisa", ""], ["Sommariva", "Sara", ""], ["Rotondi", "Fabio", ""], ["Sebastiano", "Davide Rossi", ""], ["Panzica", "Ferruccio", ""], ["Piana", "Michele", ""], ["Sorrentino", "Alberto", ""]]}, {"id": "1808.08104", "submitter": "Sylvain Le Corff", "authors": "Sylvain Le Corff (LMO), Matthieu Lerasle (LM-Orsay), Elodie Vernet\n  (CMAP)", "title": "A Bayesian nonparametric approach for generalized Bradley-Terry models\n  in random environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the estimation of the unknown distribution of hidden\nrandom variables from the observation of pairwise comparisons between these\nvariables. This problem is inspired by recent developments on Bradley-Terry\nmodels in random environment since this framework happens to be relevant to\npredict for instance the issue of a championship from the observation of a few\ncontests per team. This paper provides three contributions on a Bayesian\nnonparametric approach to solve this problem. First, we establish contraction\nrates of the posterior distribution. We also propose a Markov Chain Monte Carlo\nalgorithm to approximately sample from this posterior distribution inspired\nfrom a recent Bayesian nonparametric method for hidden Markov models. Finally,\nthe performance of this algorithm are appreciated by comparing predictions on\nthe issue of a championship based on the actual values of the teams and those\nobtained by sampling from the estimated posterior distribution.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 12:26:18 GMT"}], "update_date": "2018-08-27", "authors_parsed": [["Corff", "Sylvain Le", "", "LMO"], ["Lerasle", "Matthieu", "", "LM-Orsay"], ["Vernet", "Elodie", "", "CMAP"]]}, {"id": "1808.08153", "submitter": "Matthias L\\\"offler", "authors": "Matthias L\\\"offler and Antoine Picard", "title": "Spectral thresholding for the estimation of Markov chain transition\n  operators", "comments": "28 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric estimation of the transition operator $P$ of a\nMarkov chain and its transition density $p$ where the singular values of $P$\nare assumed to decay exponentially fast. This is for instance the case for\nperiodised, reversible multi-dimensional diffusion processes observed in low\nfrequency. We investigate the performance of a spectral hard thresholded\nGalerkin-type estimator for $P$ and ${p}$, discarding most of the estimated\nsingular triples. The construction is based on smooth basis functions such as\nwavelets or B-splines. We show its statistical optimality by establishing\nmatching minimax upper and lower bounds in $L^2$-loss. Particularly, the effect\nof the dimensionality $d$ of the state space on the nonparametric rate improves\nfrom $2d$ to $d$ compared to the case without singular value decay.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 14:22:18 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 09:04:41 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 12:58:28 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["L\u00f6ffler", "Matthias", ""], ["Picard", "Antoine", ""]]}, {"id": "1808.08326", "submitter": "Zhenke Wu", "authors": "Zhenke Wu, Livia Casciola-Rosen, Antony Rosen, Scott L. Zeger", "title": "A Bayesian Approach to Restricted Latent Class Models for\n  Scientifically-Structured Clustering of Multivariate Binary Outcomes", "comments": "37 pages and three figures in Main Paper, 24 pages in Supplementary\n  Materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a general framework for combining evidence of\nvarying quality to estimate underlying binary latent variables in the presence\nof restrictions imposed to respect the scientific context. The resulting\nalgorithms cluster the multivariate binary data in a manner partly guided by\nprior knowledge. The primary model assumptions are that 1) subjects belong to\nclasses defined by unobserved binary states, such as the true presence or\nabsence of pathogens in epidemiology, or of antibodies in medicine, or the\n\"ability\" to correctly answer test questions in psychology, 2) a binary design\nmatrix $\\Gamma$ specifies relevant features in each class, and 3) measurements\nare independent given the latent class but can have different error rates.\nConditions ensuring parameter identifiability from the likelihood function are\ndiscussed and inform the design of a novel posterior inference algorithm that\nsimultaneously estimates the number of clusters, design matrix $\\Gamma$, and\nmodel parameters. In finite samples and dimensions, we propose prior\nassumptions so that the posterior distribution of the number of clusters and\nthe patterns of latent states tend to concentrate on smaller values and sparser\npatterns, respectively. The model readily extends to studies where some\nsubjects' latent classes are known or important prior knowledge about\ndifferential measurement accuracy is available from external sources. The\nmethods are illustrated with an analysis of protein data to detect clusters\nrepresenting auto-antibody classes among scleroderma patients.\n", "versions": [{"version": "v1", "created": "Fri, 24 Aug 2018 22:23:45 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Wu", "Zhenke", ""], ["Casciola-Rosen", "Livia", ""], ["Rosen", "Antony", ""], ["Zeger", "Scott L.", ""]]}, {"id": "1808.08400", "submitter": "Dong Ding", "authors": "Dong Ding and Axel Gandy", "title": "Tree-based Particle Smoothing Algorithms in a Hidden Markov Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a new strategy built on the divide-and-conquer approach by\nLindsten et al. (2017) to investigate the smoothing problem in a hidden Markov\nmodel. We employ this approach to decompose a hidden Markov model into\nsub-models with intermediate target distributions based on an auxiliary tree\nstructure and produce independent samples from the sub-models at the leaf nodes\ntowards the original model of interest at the root. We review the target\ndistribution in the sub-models suggested by Lindsten et al. and propose two new\nclasses of target distributions, which are the estimates of the (joint)\nfiltering distributions and the (joint) smoothing distributions. The first\nproposed type is straightforwardly constructible by running a filtering\nalgorithm in advance. The algorithm using the second type of target\ndistributions has an advantage of roughly retaining the marginals of all random\nvariables invariant at all levels of the tree at the cost of approximating the\nmarginal smoothing distributions in advance. We further propose the\nconstructions of these target distributions using pre-generated Monte Carlo\nsamples. We show empirically the algorithms with the proposed intermediate\ntarget distributions give stable and comparable results as the conventional\nsmoothing methods in a linear Gaussian model and a non-linear model.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 10:09:33 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Ding", "Dong", ""], ["Gandy", "Axel", ""]]}, {"id": "1808.08440", "submitter": "Monica Musio", "authors": "Fabio Corradi and Monica Musio", "title": "Causes of Effects via a Bayesian Model Selection Procedure", "comments": "16 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In causal inference, and specifically in the \\textit{Causes of Effects}\nproblem, one is interested in how to use statistical evidence to understand\ncausation in an individual case, and so how to assess the so-called {\\em\nprobability of causation} (PC).\n  The answer relies on the potential responses, which can incorporate\ninformation about what would have happened to the outcome as we had observed a\ndifferent value of the exposure. However, even given the best possible\nstatistical evidence for the association between exposure and outcome, we can\ntypically only provide bounds for the PC. Dawid et al. (2016) highlighted some\nfundamental conditions, namely, exogeneity, comparability, and sufficiency,\nrequired to obtain such bounds, based on experimental data. The aim of the\npresent paper is to provide methods to find, in specific cases, the best\nsubsample of the reference dataset to satisfy such requirements. To this end,\nwe introduce a new variable, expressing the desire to be exposed or not, and we\nset the question up as a model selection problem. The best model will be\nselected using the marginal probability of the responses and a suitable prior\nproposal over the model space. An application in the educational field is\npresented.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 15:30:08 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 17:28:04 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Corradi", "Fabio", ""], ["Musio", "Monica", ""]]}, {"id": "1808.08478", "submitter": "Yunpeng Zhao", "authors": "Yunpeng Zhao", "title": "Network Inference from Temporal-Dependent Grouped Observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In social network analysis, the observed data is usually some social\nbehavior, such as the formation of groups, rather than an explicit network\nstructure. Zhao and Weko (2017) propose a model-based approach called the hub\nmodel to infer implicit networks from grouped observations. The hub model\nassumes independence between groups, which sometimes is not valid in practice.\nIn this article, we generalize the idea of the hub model into the case of\ngrouped observations with temporal dependence. As in the hub model, we assume\nthat the group at each time point is gathered by one leader. Unlike in the hub\nmodel, the group leaders are not sampled independently but follow a Markov\nchain, and other members in adjacent groups can also be correlated.\n  An expectation-maximization (EM) algorithm is developed for this model and a\npolynomial-time algorithm is proposed for the E-step. The performance of the\nnew model is evaluated under different simulation settings. We apply this model\nto a data set of the Kibale Chimpanzee Project.\n", "versions": [{"version": "v1", "created": "Sat, 25 Aug 2018 22:42:46 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Zhao", "Yunpeng", ""]]}, {"id": "1808.08507", "submitter": "Wenpin Tang", "authors": "Wenpin Tang", "title": "Mallows Ranking Models: Maximum Likelihood Estimate and Regeneration", "comments": "10 pages, 2 figures, 5 tables. This paper is published by\n  http://proceedings.mlr.press/v97/tang19a.html", "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning (ICML 2019), PMLR 97, 6125-6134", "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with various Mallows ranking models. We study the\nstatistical properties of the MLE of Mallows' $\\phi$ model. We also make\nconnections of various Mallows ranking models, encompassing recent progress in\nmathematics. Motivated by the infinite top-$t$ ranking model, we propose an\nalgorithm to select the model size $t$ automatically. The key idea relies on\nthe renewal property of such an infinite random permutation. Our algorithm\nshows good performance on several data sets.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 05:42:14 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 17:23:00 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Tang", "Wenpin", ""]]}, {"id": "1808.08551", "submitter": "Yong He", "authors": "Yong He, Liang Zhang, Jiadong JI, Xinsheng Zhang", "title": "Doubly Robust Sure Screening for Elliptical Copula Regression Model", "comments": "22 pages, 2 figures", "journal-ref": "Journal of multivariate analysis (2019) 568-582", "doi": "10.1016/j.jmva.2019.05.003", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression analysis has always been a hot research topic in statistics. We\npropose a very flexible semi-parametric regression model called Elliptical\nCopula Regression (ECR) model, which covers a large class of linear and\nnonlinear regression models such as additive regression model,single index\nmodel. Besides, ECR model can capture the heavy-tail characteristic and tail\ndependence between variables, thus it could be widely applied in many areas\nsuch as econometrics and finance. In this paper we mainly focus on the feature\nscreening problem for ECR model in ultra-high dimensional setting. We propose a\ndoubly robust sure screening procedure for ECR model, in which two types of\ncorrelation coefficient are involved: Kendall tau correlation and Canonical\ncorrelation. Theoretical analysis shows that the procedure enjoys sure\nscreening property, i.e., with probability tending to 1, the screening\nprocedure selects out all important variables and substantially reduces the\ndimensionality to a moderate size against the sample size. Thorough numerical\nstudies are conducted to illustrate its advantage over existing sure\nindependence screening methods and thus it can be used as a safe replacement of\nthe existing procedures in practice. At last, the proposed procedure is applied\non a gene-expression real data set to show its empirical usefulness.\n", "versions": [{"version": "v1", "created": "Sun, 26 Aug 2018 13:28:21 GMT"}], "update_date": "2020-05-12", "authors_parsed": [["He", "Yong", ""], ["Zhang", "Liang", ""], ["JI", "Jiadong", ""], ["Zhang", "Xinsheng", ""]]}, {"id": "1808.08683", "submitter": "Alex Chin", "authors": "Alex Chin", "title": "Regression adjustments for estimating the global treatment effect in\n  experiments with interference", "comments": "38 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard estimators of the global average treatment effect can be biased in\nthe presence of interference. This paper proposes regression adjustment\nestimators for removing bias due to interference in Bernoulli randomized\nexperiments. We use a fitted model to predict the counterfactual outcomes of\nglobal control and global treatment. Our work differs from standard regression\nadjustments in that the adjustment variables are constructed from functions of\nthe treatment assignment vector, and that we allow the researcher to use a\ncollection of any functions correlated with the response, turning the problem\nof detecting interference into a feature engineering problem. We characterize\nthe distribution of the proposed estimator in a linear model setting and\nconnect the results to the standard theory of regression adjustments under\nSUTVA. We then propose an estimator that allows for flexible machine learning\nestimators to be used for fitting a nonlinear interference functional form. We\npropose conducting statistical inference via bootstrap and resampling methods,\nwhich allow us to sidestep the complicated dependences implied by interference\nand instead rely on empirical covariance structures. Such variance estimation\nrelies on an exogeneity assumption akin to the standard unconfoundedness\nassumption invoked in observational studies. In simulation experiments, our\nmethods are better at debiasing estimates than existing inverse propensity\nweighted estimators based on neighborhood exposure modeling. We use our method\nto reanalyze an experiment concerning weather insurance adoption conducted on a\ncollection of villages in rural China.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 04:23:44 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 02:08:41 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Chin", "Alex", ""]]}, {"id": "1808.08764", "submitter": "Joris Chau", "authors": "Joris Chau and Rainer von Sachs", "title": "Intrinsic wavelet regression for surfaces of Hermitian positive definite\n  matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops intrinsic wavelet denoising methods for surfaces of\nHermitian positive definite matrices, with in mind the application to\nnonparametric estimation of the time-varying spectral matrix of a multivariate\nlocally stationary time series. First, we construct intrinsic\naverage-interpolating wavelet transforms acting directly on surfaces of\nHermitian positive definite matrices in a curved Riemannian manifold with\nrespect to an affine-invariant metric. Second, we derive the wavelet\ncoefficient decay and linear wavelet thresholding convergence rates of\nintrinsically smooth surfaces of Hermitian positive definite matrices, and\ninvestigate practical nonlinear thresholding of wavelet coefficients based on\ntheir trace in the context of intrinsic signal plus noise models in the\nRiemannian manifold. The finite-sample performance of nonlinear tree-structured\ntrace thresholding is assessed by means of simulated data, and the proposed\nintrinsic wavelet methods are used to estimate the time-varying spectral matrix\nof a nonstationary multivariate electroencephalography (EEG) time series\nrecorded during an epileptic brain seizure.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 10:04:24 GMT"}, {"version": "v2", "created": "Sun, 19 May 2019 14:34:39 GMT"}, {"version": "v3", "created": "Tue, 21 May 2019 07:03:34 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Chau", "Joris", ""], ["von Sachs", "Rainer", ""]]}, {"id": "1808.08778", "submitter": "Yi Ding", "authors": "Yi Ding, Panos Toulis", "title": "Dynamical systems theory for causal inference with application to\n  synthetic control methods", "comments": "17 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we adopt results in nonlinear time series analysis for causal\ninference in dynamical settings.~Our motivation is policy analysis with panel\ndata, particularly through the use of \"synthetic control\" methods. These\nmethods regress pre-intervention outcomes of the treated unit to outcomes from\na pool of control units, and then use the fitted regression model to estimate\ncausal effects post-intervention. In this setting, we propose to screen out\ncontrol units that have a weak dynamical relationship to the treated unit. In\nsimulations, we show that this method can mitigate bias from \"cherry-picking\"\nof control units, which is usually an important concern. We illustrate on\nreal-world applications, including the tobacco legislation example of\n\\citet{Abadie2010}, and Brexit.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 10:48:39 GMT"}, {"version": "v2", "created": "Thu, 23 May 2019 02:22:42 GMT"}, {"version": "v3", "created": "Sat, 29 Feb 2020 19:53:47 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Ding", "Yi", ""], ["Toulis", "Panos", ""]]}, {"id": "1808.08793", "submitter": "Yongsong Qin", "authors": "Yongsong Qin", "title": "Empirical likelihood for linear models with spatial errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For linear models with spatial errors, the empirical likelihood ratio\nstatistics are constructed for the parameters of the models. It is shown that\nthe limiting distributions of the empirical likelihood ratio statistics are\nchi-squared distributions, which are used to construct confidence regions for\nthe parameters of the models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 11:45:42 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Qin", "Yongsong", ""]]}, {"id": "1808.08986", "submitter": "Frank Konietschke", "authors": "Cong Cao, Markus Pauly, Frank Konietschke", "title": "The Behrens-Fisher Problem with Covariates and Baseline Adjustments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Welch-Satterthwaite t-test is one of the most prominent and often used\nstatistical inference method in applications. The method is, however, not\nflexible with respect to adjustments for baseline values or other covariates,\nwhich may impact the response variable. Existing analysis of covariance methods\nare typically based on the assumption of equal variances across the groups.\nThis assumption is hard to justify in real data applications and the methods\ntend to not control the type-1 error rate satisfactorily under variance\nheteroscedasticity. In the present paper, we tackle this problem and develop\nunbiased variance estimators of group specific variances, and especially of the\nvariance of the estimated adjusted treatment effect in a general analysis of\ncovariance model. These results are used to generalize the Welch-Satterthwaite\nt-test to covariates adjustments. Extensive simulation studies show that the\nmethod accurately controls the nominal type-1 error rate, even for very small\nsample sizes, moderately skewed distributions and under variance\nheteroscedasticity. A real data set motivates and illustrates the application\nof the proposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 18:24:26 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Cao", "Cong", ""], ["Pauly", "Markus", ""], ["Konietschke", "Frank", ""]]}, {"id": "1808.09011", "submitter": "Yaowu Liu", "authors": "Yaowu Liu and Jun Xie", "title": "Cauchy combination test: a powerful test with analytic p-value\n  calculation under arbitrary dependency structures", "comments": "To appear in Journal of the American Statistical Association, Theory\n  and Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining individual p-values to aggregate multiple small effects has a\nlong-standing interest in statistics, dating back to the classic Fisher's\ncombination test. In modern large-scale data analysis, correlation and sparsity\nare common features and efficient computation is a necessary requirement for\ndealing with massive data. To overcome these challenges, we propose a new test\nthat takes advantage of the Cauchy distribution. Our test statistic has a very\nsimple form and is defined as a weighted sum of Cauchy transformation of\nindividual p-values. We prove a non-asymptotic result that the tail of the null\ndistribution of our proposed test statistic can be well approximated by a\nCauchy distribution under arbitrary dependency structures. Based on this\ntheoretical result, the p-value calculation of our proposed test is not only\naccurate, but also as simple as the classic z-test or t-test, making our test\nwell suited for analyzing massive data. We further show that the power of the\nproposed test is asymptotically optimal in a strong sparsity setting. Extensive\nsimulations demonstrate that the proposed test has both strong power against\nsparse alternatives and a good accuracy with respect to p-value calculations,\nespecially for very small p-values. The proposed test has also been applied to\na genome-wide association study of Crohn's disease and compared with several\nexisting tests.\n", "versions": [{"version": "v1", "created": "Mon, 27 Aug 2018 19:38:16 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 23:02:58 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Liu", "Yaowu", ""], ["Xie", "Jun", ""]]}, {"id": "1808.09107", "submitter": "Yong He", "authors": "Long Yu, Yong He, Xinsheng Zhang", "title": "Robust Factor Number Specification for Large-dimensional Elliptical\n  Factor Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accurate specification of the number of factors is critical to the\nvalidity of factor models and the topic almost occupies the central position in\nfactor analysis. Plenty of estimators are available under the restrictive\ncondition that the fourth moments of the factors and idiosyncratic errors are\nbounded. In this paper we propose efficient and robust estimators for the\nfactor number via considering a more general static Elliptical Factor Model\n(EFM) framework. We innovatively propose to exploit the multivariate Kendall's\ntau matrix, which captures the correlation structure of elliptical random\nvectors. Theoretically we show that the proposed estimators are consistent\nwithout exerting any moment condition when both cross-sections N and time\ndimensions T go to infinity. Simulation study shows that the new estimators\nperform much better in heavy-tailed data setting while performing comparably\nwith the state-of-the-art methods in the light-tailed Gaussian setting. At\nlast, a real macroeconomic data example is given to illustrate its empirical\nadvantages and usefulness.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 04:09:50 GMT"}, {"version": "v2", "created": "Wed, 14 Aug 2019 01:49:26 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Yu", "Long", ""], ["He", "Yong", ""], ["Zhang", "Xinsheng", ""]]}, {"id": "1808.09152", "submitter": "Carol Alexander Prof.", "authors": "Carol Alexander and Emese Lazar", "title": "On the Continuous Limit of Weak GARCH", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that the symmetric weak GARCH limit is a geometric mean-reverting\nstochastic volatility process with diffusion determined by kurtosis of physical\nlog returns; this provides an improved fit to implied volatility surfaces. When\nlog returns are normal the limit coincides with Nelson's limit. The limit is\nunique, unlike strong GARCH limits, because assumptions about convergence of\nmodel parameters is unnecessary -- parameter convergence is uniquely determined\nby time-aggregation of the weak GARCH process.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 07:38:43 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Alexander", "Carol", ""], ["Lazar", "Emese", ""]]}, {"id": "1808.09200", "submitter": "Jia Liu", "authors": "Jia Liu and Jarno Vanhatalo", "title": "Bayesian model-based spatiotemporal survey design for log-Gaussian Cox\n  process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In geostatistics, the design for data collection is central for accurate\nprediction and parameter inference. One important class of geostatistical\nmodels is log-Gaussian Cox process (LGCP) which is used extensively, for\nexample, in ecology. However, there are no formal analyses on optimal designs\nfor LGCP models. In this work, we develop a novel model-based experimental\ndesign for LGCP modeling of spatiotemporal point process data. We propose a new\nspatially balanced rejection sampling design which directs sampling to\nspatiotemporal locations that are a priori expected to provide most\ninformation. We compare the rejection sampling design to traditional balanced\nand uniform random designs using the average predictive variance loss function\nand the Kullback-Leibler divergence between prior and posterior for the LGCP\nintensity function. Our results show that the rejection sampling method\noutperforms the corresponding balanced and uniform random sampling designs for\nLGCP whereas the latter work better for models with Gaussian models. We perform\na case study applying our new sampling design to plan a survey for species\ndistribution modeling on larval areas of two commercially important fish stocks\non Finnish coastal areas. The case study results show that rejection sampling\ndesigns give considerable benefit compared to traditional designs. Results show\nalso that best performing designs may vary considerably between target species.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 09:54:54 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Liu", "Jia", ""], ["Vanhatalo", "Jarno", ""]]}, {"id": "1808.09262", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli", "title": "The Sparse Latent Position Model for nonnegative weighted networks", "comments": "40 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new methodology to analyse bipartite and unipartite\nnetworks with nonnegative edge values. The proposed approach combines and\nadapts a number of ideas from the literature on latent variable network models.\nThe resulting framework is a new type of latent position model which exhibits\ngreat flexibility, and is able to capture important features that are generally\nexhibited by observed networks, such as sparsity and heavy tailed degree\ndistributions. A crucial advantage of the proposed method is that the number of\nlatent dimensions is automatically deduced from the data in one single\nalgorithmic framework. In addition, the model attaches a weight to each of the\nlatent dimensions, hence providing a measure of their relative importance. A\nfast variational Bayesian algorithm is proposed to estimate the parameters of\nthe model. Finally, applications of the proposed methodology are illustrated on\nboth artificial and real datasets, and comparisons with other existing\nprocedures are provided.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 12:58:35 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Rastelli", "Riccardo", ""]]}, {"id": "1808.09339", "submitter": "Myron Hlynka", "authors": "Nian Liu and Myron Hlynka", "title": "Scheduling a Rescue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scheduling service order, in a very specific queueing/inventory model with\nperishable inventory, is considered. Different strategies are discusses and\nresults are applied to the tragic cave situation in Thailand in June and July\nof 2018.\n", "versions": [{"version": "v1", "created": "Wed, 22 Aug 2018 13:46:40 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Liu", "Nian", ""], ["Hlynka", "Myron", ""]]}, {"id": "1808.09379", "submitter": "Benjamin Peherstorfer", "authors": "Benjamin Peherstorfer, Youssef Marzouk", "title": "A transport-based multifidelity preconditioner for Markov chain Monte\n  Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) sampling of posterior distributions arising\nin Bayesian inverse problems is challenging when evaluations of the forward\nmodel are computationally expensive. Replacing the forward model with a\nlow-cost, low-fidelity model often significantly reduces computational cost;\nhowever, employing a low-fidelity model alone means that the stationary\ndistribution of the MCMC chain is the posterior distribution corresponding to\nthe low-fidelity model, rather than the original posterior distribution\ncorresponding to the high-fidelity model. We propose a multifidelity approach\nthat combines, rather than replaces, the high-fidelity model with a\nlow-fidelity model. First, the low-fidelity model is used to construct a\ntransport map that deterministically couples a reference Gaussian distribution\nwith an approximation of the low-fidelity posterior. Then, the high-fidelity\nposterior distribution is explored using a non-Gaussian proposal distribution\nderived from the transport map. This multifidelity \"preconditioned\" MCMC\napproach seeks efficient sampling via a proposal that is explicitly tailored to\nthe posterior at hand and that is constructed efficiently with the low-fidelity\nmodel. By relying on the low-fidelity model only to construct the proposal\ndistribution, our approach guarantees that the stationary distribution of the\nMCMC chain is the high-fidelity posterior. In our numerical examples, our\nmultifidelity approach achieves significant speedups compared to\nsingle-fidelity MCMC sampling methods.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 16:04:08 GMT"}], "update_date": "2018-08-29", "authors_parsed": [["Peherstorfer", "Benjamin", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1808.09448", "submitter": "Vladimir Pastukhov", "authors": "Dragi Anevski and Vladimir Pastukhov", "title": "Estimating the distribution and thinning parameters of a homogeneous\n  multimode Poisson process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose estimators of the distribution of events of\ndifferent kinds in a multimode Poisson process. We give the explicit solution\nfor the maximum likelihood estimator, and derive its strong consistency and\nasymptotic normality. We also provide an order restricted estimator and derive\nits consistency and asymptotic distribution. We discuss the application of the\nestimator to the detection of neutrons in a novel detector being developed at\nthe European Spallation Source in Lund, Sweden. The inference problem gives\nrise to Sylvester-Ramanujan system of equations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 14:32:58 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Anevski", "Dragi", ""], ["Pastukhov", "Vladimir", ""]]}, {"id": "1808.09507", "submitter": "Pedro Henrique Filipini Dos Santos", "authors": "Pedro Henrique Filipini dos Santos, Hedibert Freitas Lopes", "title": "Tree-Based Bayesian Treatment Effect Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inclusion of the propensity score as a covariate in Bayesian regression\ntrees for causal inference can reduce the bias in treatment effect estimations,\nwhich occurs due to the regularization-induced confounding phenomenon. This\nstudy advocate for the use of the propensity score by evaluating it under a\nfull-Bayesian variable selection setting, and the use of Individual Conditional\nExpectation Plots, which is a graphical tool that can improve treatment effect\nanalysis on tree-based Bayesian models and others \"black box\" models. The first\none, even if poorly estimated, can lead to bias reduction on the estimated\ntreatment effects, while the latter can be used to found groups of individuals\nwhich have different responses to the applied treatment, and analyze the impact\nof each variable in the estimated treatment effect.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 19:31:28 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Santos", "Pedro Henrique Filipini dos", ""], ["Lopes", "Hedibert Freitas", ""]]}, {"id": "1808.09521", "submitter": "Steve Yadlowsky", "authors": "Steve Yadlowsky, Hongseok Namkoong, Sanjay Basu, John Duchi, Lu Tian", "title": "Bounds on the conditional and average treatment effect with unobserved\n  confounding factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For observational studies, we study the sensitivity of causal inference when\ntreatment assignments may depend on unobserved confounding factors. We develop\na loss minimization approach that quantifies bounds on the conditional average\ntreatment effect (CATE) when unobserved confounder have a bounded effect on the\nodds of treatment selection. Our approach is scalable and allows flexible use\nof model classes, including nonparametric and black-box machine learning\nmethods. Using these bounds, we propose a related sensitivity analysis for the\naverage treatment effect (ATE), and develop a semi-parametric framework that\nextends/bounds the augmented inverse propensity weighted (AIPW) estimator for\nthe ATE beyond the assumption that all confounders are observed. By\nconstructing a Neyman orthogonal score, our estimator is a regular root-n\nestimator so long as the nuisance parameters can be estimated at the\n$o_p(n^{-1/4})$ rate. We complement our methodological development with\noptimality results showing that our proposed bounds are tight in certain cases.\nWe demonstrate our method on simulated and real data examples, and show\naccurate coverage of our confidence intervals in practical finite sample\nregimes.\n", "versions": [{"version": "v1", "created": "Tue, 28 Aug 2018 20:11:16 GMT"}, {"version": "v2", "created": "Wed, 5 Jun 2019 16:36:28 GMT"}, {"version": "v3", "created": "Thu, 8 Aug 2019 20:46:46 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 05:20:33 GMT"}], "update_date": "2020-07-09", "authors_parsed": [["Yadlowsky", "Steve", ""], ["Namkoong", "Hongseok", ""], ["Basu", "Sanjay", ""], ["Duchi", "John", ""], ["Tian", "Lu", ""]]}, {"id": "1808.09758", "submitter": "Guillaume Chauvet", "authors": "Guillaume Chauvet (IRMAR), Audrey-Anne Vall\\'ee", "title": "Inference for two-stage sampling designs with application to a panel for\n  urban policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-stage sampling designs are commonly used for household and health\nsurveys. To produce reliable estimators with assorted confidence intervals,\nsome basic statistical properties like consistency and asymptotic normality of\nthe Horvitz-Thompson estimator are desirable, along with the consistency of\nassorted variance estimators. These properties have been mainly studied for\nsingle-stage sampling designs. In this work, we prove the consistency of the\nHorvitz-Thompson estimator and of associated variance estimators for a general\nclass of two-stage sampling designs, under mild assumptions. We also study\ntwo-stage sampling with a large entropy sampling design at the first stage, and\nprove that the Horvitz-Thompson estimator is asymptotically normally\ndistributed through a coupling argument. When the first-stage sampling fraction\nis negligible, simplified variance estimators which do not require estimating\nthe variance within the Primary Sampling Units are proposed, and shown to be\nconsistent. An application to a panel for urban policy, which is the initial\nmotivation for this work, is also presented.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 12:29:40 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 07:56:44 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Chauvet", "Guillaume", "", "IRMAR"], ["Vall\u00e9e", "Audrey-Anne", ""]]}, {"id": "1808.10019", "submitter": "Alejandra Estefan\\'ia Pati\\~no Hoyos", "authors": "Alejandra Estefan\\'ia Pati\\~no Hoyos and Victor Fossaluza", "title": "Adaptative significance levels in normal mean hypothesis testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Full Bayesian Significance Test (FBST) for precise hypotheses was\npresented by Pereira and Stern (1999) as a Bayesian alternative instead of the\ntraditional significance test based on p-value. The FBST uses the evidence in\nfavor of the null hypothesis ($H_0$) calculated as the complement of the\nposterior probability of the highest posterior density region, which is tangent\nto the set defined by $H_0$. An important practical issue for the\nimplementation of the FBST is the determination of how large the evidence must\nbe in order to decide for its rejection. In the Classical significance tests,\nthe most used measure for rejecting a hypothesis is p-value. It is known that\np-value decreases as sample size increases, so by setting a single significance\nlevel, it usually leads $H_0$ rejection. In the FBST procedure, the evidence in\nfavor of $H_0$ exhibits the same behavior as the p-value when the sample size\nincreases. This suggests that the cut-off point to define the rejection of\n$H_0$ in the FBST should be a sample size function. In this work, we focus on\nthe case of two-sided normal mean hypothesis testing and present a method to\nfind a cut-off value for the evidence in the FBST by minimizing the linear\ncombination of the type I error probability and the expected type II error\nprobability for a given sample size.\n", "versions": [{"version": "v1", "created": "Wed, 29 Aug 2018 19:36:30 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Hoyos", "Alejandra Estefan\u00eda Pati\u00f1o", ""], ["Fossaluza", "Victor", ""]]}, {"id": "1808.10188", "submitter": "Esa Ollila", "authors": "Esa Ollila and Elias Raninen", "title": "Optimal shrinkage covariance matrix estimation under random sampling\n  from elliptical distributions", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2908144", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating a high-dimensional (HD)\ncovariance matrix when the sample size is smaller, or not much larger, than the\ndimensionality of the data, which could potentially be very large. We develop a\nregularized sample covariance matrix (RSCM) estimator which can be applied in\ncommonly occurring sparse data problems. The proposed RSCM estimator is based\non estimators of the unknown optimal (oracle) shrinkage parameters that yield\nthe minimum mean squared error (MMSE) between the RSCM and the true covariance\nmatrix when the data is sampled from an unspecified elliptically symmetric\ndistribution. We propose two variants of the RSCM estimator which differ in the\napproach in which they estimate the underlying sphericity parameter involved in\nthe theoretical optimal shrinkage parameter. The performance of the proposed\nRSCM estimators are evaluated with numerical simulation studies. In particular\nwhen the sample sizes are low, the proposed RSCM estimators often show a\nsignificant improvement over the conventional RSCM estimator by Ledoit and Wolf\n(2004). We further evaluate the performance of the proposed estimators in\nclassification and portfolio optimization problems with real data wherein the\nproposed methods are able to outperform the benchmark methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 08:57:48 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Ollila", "Esa", ""], ["Raninen", "Elias", ""]]}, {"id": "1808.10415", "submitter": "Nicholas Tawn", "authors": "Nicholas G. Tawn and Gareth O. Roberts", "title": "Accelerating Parallel Tempering: Quantile Tempering Algorithm (QuanTA)", "comments": null, "journal-ref": "Adv. Appl. Probab. 51 (2019) 802-834", "doi": "10.1017/apr.2019.35", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using MCMC to sample from a target distribution, $\\pi(x)$ on a\n$d$-dimensional state space can be a difficult and computationally expensive\nproblem. Particularly when the target exhibits multimodality, then the\ntraditional methods can fail to explore the entire state space and this results\nin a bias sample output. Methods to overcome this issue include the parallel\ntempering algorithm which utilises an augmented state space approach to help\nthe Markov chain traverse regions of low probability density and reach other\nmodes. This method suffers from the curse of dimensionality which dramatically\nslows the transfer of mixing information from the auxiliary targets to the\ntarget of interest as $d \\rightarrow \\infty$. This paper introduces a novel\nprototype algorithm, QuanTA, that uses a Gaussian motivated transformation in\nan attempt to accelerate the mixing through the temperature schedule of a\nparallel tempering algorithm. This new algorithm is accompanied by a\ncomprehensive theoretical analysis quantifying the improved efficiency and\nscalability of the approach; concluding that under weak regularity conditions\nthe new approach gives accelerated mixing through the temperature schedule.\nEmpirical evidence of the effectiveness of this new algorithm is illustrated on\ncanonical examples.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 17:36:03 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Tawn", "Nicholas G.", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "1808.10477", "submitter": "Linh Nghiem", "authors": "Linh Nghiem, Cornelis Potgieter", "title": "Simulation-Selection-Extrapolation: Estimation in High-Dimensional\n  Errors-in-Variables Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers errors-in-variables models in a high-dimensional setting\nwhere the number of covariates can be much larger than the sample size, and\nthere are only a small number of non-zero covariates. The presence of\nmeasurement error in the covariates can result in severely biased parameter\nestimates, and also affects the ability of penalized methods such as the lasso\nto recover the true sparsity pattern. A new estimation procedure called\nSIMSELEX (SIMulation-SELection-EXtrapolation) is proposed. This procedure\naugments the traditional SIMEX approach with a variable selection step based on\nthe group lasso. The SIMSELEX estimator is shown to perform well in variable\nselection, and has significantly lower estimation error than naive estimators\nthat ignore measurement error. SIMSELEX can be applied in a variety of\nerrors-in-variables settings, including linear models, generalized linear\nmodels, and Cox survival models. It is furthermore shown how SIMSELEX can be\napplied to spline-based regression models. SIMSELEX estimators are compared to\nthe corrected lasso and the conic programming estimator for a linear model, and\nto the conditional scores lasso for a logistic regression model. Finally, the\nmethod is used to analyze a microarray dataset that contains gene expression\nmeasurements of favorable histology Wilms tumors.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 18:27:24 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Nghiem", "Linh", ""], ["Potgieter", "Cornelis", ""]]}, {"id": "1808.10483", "submitter": "William Valdar", "authors": "Jeffrey Roach and William Valdar", "title": "Permutation tests of non-exchangeable null models", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizations to the permutation test are introduced to allow for\nsituations in which the null model is not exchangeable. It is shown that the\ngeneralized permutation tests are exact, and a partial converse: that any test\nfunction that is exact on all probability densities coincides with a\ngeneralized permutation test on a particular region, is established. A most\npowerful generalized permutation test is derived in closed form. Approximations\nto the most powerful generalized permutation test are proposed to reduce the\ncomputational burden required to compute the complete test. In particular, an\nexplicit form for the approximate test is derived in terms of a multinomial\nBernstein polynomial approximation, and its convergence to the most powerful\ngeneralized permutation test is demonstrated. In the case where the\ndetermination of p-values is of greater interest than testing of hypotheses,\ntwo approaches to estimation of significance are analyzed. Bounds on the\ndeviation from significance of the exact most powerful test are given in terms\nof sample size. For both estimators, as sample size approaches infinity, the\nestimator converges to the significance of the most powerful generalized\npermutation test under mild conditions. Applications of generalized permutation\ntesting to linear mixed models are provided.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 18:57:04 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Roach", "Jeffrey", ""], ["Valdar", "William", ""]]}, {"id": "1808.10506", "submitter": "Zhiqin Xu", "authors": "Zhi-Qin John Xu, Jennifer Crodelle, Douglas Zhou, David Cai", "title": "Maximum Entropy Principle Analysis in Network Systems with Short-time\n  Recordings", "comments": "10 pages, 5 figures", "journal-ref": "Phys. Rev. E 99, 022409 (2019)", "doi": "10.1103/PhysRevE.99.022409", "report-no": null, "categories": "physics.bio-ph cs.IT math.IT physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many realistic systems, maximum entropy principle (MEP) analysis provides\nan effective characterization of the probability distribution of network\nstates. However, to implement the MEP analysis, a sufficiently long-time data\nrecording in general is often required, e.g., hours of spiking recordings of\nneurons in neuronal networks. The issue of whether the MEP analysis can be\nsuccessfully applied to network systems with data from short recordings has yet\nto be fully addressed. In this work, we investigate relationships underlying\nthe probability distributions, moments, and effective interactions in the MEP\nanalysis and then show that, with short recordings of network dynamics, the MEP\nanalysis can be applied to reconstructing probability distributions of network\nstates under the condition of asynchronous activity of nodes in the network.\nUsing spike trains obtained from both Hodgkin-Huxley neuronal networks and\nelectrophysiological experiments, we verify our results and demonstrate that\nMEP analysis provides a tool to investigate the neuronal population coding\nproperties, even for short recordings.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 20:28:43 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Xu", "Zhi-Qin John", ""], ["Crodelle", "Jennifer", ""], ["Zhou", "Douglas", ""], ["Cai", "David", ""]]}, {"id": "1808.10522", "submitter": "Teague Henry", "authors": "Teague R. Henry, Zachary F. Fisher, Kenneth A. Bollen", "title": "Bayesian Model Averaging for Model Implied Instrumental Variable Two\n  Stage Least Squares Estimators", "comments": "31 pages, 8 figures, supplementary materials available upon request", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-Implied Instrumental Variable Two-Stage Least Squares (MIIV-2SLS) is a\nlimited information, equation-by-equation, non-iterative estimator for latent\nvariable models. Associated with this estimator are equation specific tests of\nmodel misspecification. We propose an extension to the existing MIIV-2SLS\nestimator that utilizes Bayesian model averaging which we term Model-Implied\nInstrumental Variable Two-Stage Bayesian Model Averaging (MIIV-2SBMA).\nMIIV-2SBMA accounts for uncertainty in optimal instrument set selection, and\nprovides powerful instrument specific tests of model misspecification and\ninstrument strength. We evaluate the performance of MIIV-2SBMA against\nMIIV-2SLS in a simulation study and show that it has comparable performance in\nterms of parameter estimation. Additionally, our instrument specific\noveridentification tests developed within the MIIV-2SBMA framework show\nincreased power to detect model misspecification over the traditional equation\nlevel tests of model misspecification. Finally, we demonstrate the use of\nMIIV-2SBMA using an empirical example.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 21:11:04 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Henry", "Teague R.", ""], ["Fisher", "Zachary F.", ""], ["Bollen", "Kenneth A.", ""]]}, {"id": "1808.10532", "submitter": "Jannis K\\\"uck", "authors": "Sven Klaassen, Jannis K\\\"uck, Martin Spindler, Victor Chernozhukov", "title": "Uniform Inference in High-Dimensional Gaussian Graphical Models", "comments": "59 pages, 2 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models have become a very popular tool for representing\ndependencies within a large set of variables and are key for representing\ncausal structures. We provide results for uniform inference on high-dimensional\ngraphical models with the number of target parameters $d$ being possible much\nlarger than sample size. This is in particular important when certain features\nor structures of a causal model should be recovered. Our results highlight how\nin high-dimensional settings graphical models can be estimated and recovered\nwith modern machine learning methods in complex data sets. To construct\nsimultaneous confidence regions on many target parameters, sufficiently fast\nestimation rates of the nuisance functions are crucial. In this context, we\nestablish uniform estimation rates and sparsity guarantees of the square-root\nestimator in a random design under approximate sparsity conditions that might\nbe of independent interest for related problems in high-dimensions. We also\ndemonstrate in a comprehensive simulation study that our procedure has good\nsmall sample properties.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 21:53:06 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 14:40:37 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Klaassen", "Sven", ""], ["K\u00fcck", "Jannis", ""], ["Spindler", "Martin", ""], ["Chernozhukov", "Victor", ""]]}, {"id": "1808.10541", "submitter": "Aaron Molstad", "authors": "Aaron J. Molstad, Li Hsu, Wei Sun", "title": "Gaussian process regression for survival time prediction with\n  genome-wide gene expression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the survival time of a cancer patient based on his/her genome-wide\ngene expression remains a challenging problem. For certain types of cancer, the\neffects of gene expression on survival are both weak and abundant, so\nidentifying nonzero effects with reasonable accuracy is difficult. As an\nalternative to methods that use variable selection, we propose a Gaussian\nprocess accelerated failure time model to predict survival time using\ngenome-wide or pathway-wide gene expression data. Using a Monte Carlo EM\nalgorithm, we jointly impute censored log-survival time and estimate model\nparameters. We demonstrate the performance of our method and its advantage over\nexisting methods in both simulations and real data analysis. The real data that\nwe analyze were collected from 513 patients with kidney renal clear cell\ncarcinoma and include survival time, demographic/clinical variables, and\nexpression of more than 20,000 genes. Our method is widely applicable as it can\naccommodate right, left, and interval censored outcomes; and provides a natural\nway to combine multiple types of high-dimensional -omics data. An R package\nimplementing our method is available online.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 22:51:24 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Molstad", "Aaron J.", ""], ["Hsu", "Li", ""], ["Sun", "Wei", ""]]}, {"id": "1808.10558", "submitter": "Aaron Molstad", "authors": "Aaron J. Molstad, Guangwei Weng, Charles R. Doss, Adam J. Rothman", "title": "An explicit mean-covariance parameterization for multivariate response\n  linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new method to fit the multivariate response linear regression\nmodel that exploits a parametric link between the regression coefficient matrix\nand the error covariance matrix. Specifically, we assume that the correlations\nbetween entries in the multivariate error random vector are proportional to the\ncosines of the angles between their corresponding regression coefficient matrix\ncolumns, so as the angle between two regression coefficient matrix columns\ndecreases, the correlation between the corresponding errors increases. We\nhighlight two models under which this parameterization arises: the latent\nvariable reduced-rank regression model and the errors-in-variables regression\nmodel. We propose a novel non-convex weighted residual sum of squares criterion\nwhich exploits this parameterization and admits a new class of penalized\nestimators. The optimization is solved with an accelerated proximal gradient\ndescent algorithm. Our method is used to study the association between microRNA\nexpression and cancer drug activity measured on the NCI-60 cell lines. An R\npackage implementing our method, MCMVR, is available at\ngithub.com/ajmolstad/MCMVR.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 00:35:50 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2018 07:44:36 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 00:27:52 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Molstad", "Aaron J.", ""], ["Weng", "Guangwei", ""], ["Doss", "Charles R.", ""], ["Rothman", "Adam J.", ""]]}, {"id": "1808.10593", "submitter": "Yuling Yan", "authors": "Yuling Yan, Bret Hanlon, Sebastien Roch, Karl Rohe", "title": "Asymptotic Seed Bias in Respondent-driven Sampling", "comments": "37 pages, 7 figures; typos corrected, proof outlines added", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) collects a sample of individuals in a\nnetworked population by incentivizing the sampled individuals to refer their\ncontacts into the sample. This iterative process is initialized from some seed\nnode(s). Sometimes, this selection creates a large amount of seed bias. Other\ntimes, the seed bias is small. This paper gains a deeper understanding of this\nbias by characterizing its effect on the limiting distribution of various RDS\nestimators. Using classical tools and results from multi-type branching\nprocesses (Kesten and Stigum, 1966), we show that the seed bias is negligible\nfor the Generalized Least Squares (GLS) estimator and non-negligible for both\nthe inverse probability weighted and Volz-Heckathorn (VH) estimators. In\nparticular, we show that (i) above a critical threshold, VH converge to a\nnon-trivial mixture distribution, where the mixture component depends on the\nseed node, and the mixture distribution is possibly multi-modal. Moreover, (ii)\nGLS converges to a Gaussian distribution independent of the seed node, under a\ncertain condition on the Markov process. Numerical experiments with both\nsimulated data and empirical social networks suggest that these results appear\nto hold beyond the Markov conditions of the theorems.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 04:16:58 GMT"}, {"version": "v2", "created": "Wed, 21 Aug 2019 04:53:31 GMT"}], "update_date": "2019-08-22", "authors_parsed": [["Yan", "Yuling", ""], ["Hanlon", "Bret", ""], ["Roch", "Sebastien", ""], ["Rohe", "Karl", ""]]}, {"id": "1808.10770", "submitter": "Tomohiro Nishiyama", "authors": "Tomohiro Nishiyama", "title": "Improved Chebyshev inequality: new probability bounds with known\n  supremum of PDF", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive new probability bounds for Chebyshev's inequality if\nthe supremum of the probability density function is known. This result holds\nfor one-dimensional or multivariate continuous probability distributions with\nfinite mean and variance (covariance matrix). We also show that the similar\nresult holds for specific discrete probability distributions.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 14:26:28 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 12:47:54 GMT"}], "update_date": "2019-02-12", "authors_parsed": [["Nishiyama", "Tomohiro", ""]]}, {"id": "1808.10777", "submitter": "Francisco Novoa Mu\\~noz", "authors": "Francisco Novoa-Mu\\~noz", "title": "Tests de bondad de ajuste para la distribuci\\'on Poisson bivariante", "comments": "120 pages, in Spanish, 10 tables, book", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The objective of this text is to propose and study goodness-of-fit tests for\nDBP, which are consistent. Since the probability generating function (fgp)\ncharacterizes the distribution of a random vector and can be estimated\nconsistently by the empirical probability generating function (fgpe), the tests\nwe propose are functions of the fgpe. The first statistical test compares the\nfgpe of the data with an estimator of the fgp of the DPB. Then, we show that\nthe fgp of the DPB is the only fgp that satisfies a certain system of partial\ndifferential equations, which leads us to propose two statistical tests based\non the empirical analogy of this system, one of them Cramer-von Mises type and\nthe other is based on the coefficients of the polynomials of the empirical\nversion. The tests we propose can be seen as extensions to the bivariate case\nof some goodness of fit tests designed for the univariate case.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2018 15:36:18 GMT"}], "update_date": "2018-09-03", "authors_parsed": [["Novoa-Mu\u00f1oz", "Francisco", ""]]}, {"id": "1808.10868", "submitter": "Mengyang Gu", "authors": "Mengyang Gu and Weining Shen", "title": "Generalized probabilistic principal component analysis of correlated\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a well-established tool in machine\nlearning and data processing. The principal axes in PCA were shown to be\nequivalent to the maximum marginal likelihood estimator of the factor loading\nmatrix in a latent factor model for the observed data, assuming that the latent\nfactors are independently distributed as standard normal distributions.\nHowever, the independence assumption may be unrealistic for many scenarios such\nas modeling multiple time series, spatial processes, and functional data, where\nthe outcomes are correlated. In this paper, we introduce the generalized\nprobabilistic principal component analysis (GPPCA) to study the latent factor\nmodel for multiple correlated outcomes, where each factor is modeled by a\nGaussian process. Our method generalizes the previous probabilistic formulation\nof PCA (PPCA) by providing the closed-form maximum marginal likelihood\nestimator of the factor loadings and other parameters. Based on the explicit\nexpression of the precision matrix in the marginal likelihood that we derived,\nthe number of the computational operations is linear to the number of output\nvariables. Furthermore, we also provide the closed-form expression of the\nmarginal likelihood when other covariates are included in the mean structure.\nWe highlight the advantage of GPPCA in terms of the practical relevance,\nestimation accuracy and computational convenience. Numerical studies of\nsimulated and real data confirm the excellent finite-sample performance of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 17:53:10 GMT"}, {"version": "v2", "created": "Tue, 23 Apr 2019 16:29:26 GMT"}, {"version": "v3", "created": "Thu, 24 Oct 2019 00:31:18 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Gu", "Mengyang", ""], ["Shen", "Weining", ""]]}]