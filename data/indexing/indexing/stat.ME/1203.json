[{"id": "1203.0106", "submitter": "Francois Caron", "authors": "Fran\\c{c}ois Caron (INRIA Bordeaux - Sud-Ouest, IMB), Luke Bornn\n  (Statistics), Arnaud Doucet", "title": "Sparsity-Promoting Bayesian Dynamic Linear Models", "comments": null, "journal-ref": "N&deg; RR-7895 (2012)", "doi": null, "report-no": "RR-7895", "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparsity-promoting priors have become increasingly popular over recent years\ndue to an increased number of regression and classification applications\ninvolving a large number of predictors. In time series applications where\nobservations are collected over time, it is often unrealistic to assume that\nthe underlying sparsity pattern is fixed. We propose here an original class of\nflexible Bayesian linear models for dynamic sparsity modelling. The proposed\nclass of models expands upon the existing Bayesian literature on sparse\nregression using generalized multivariate hyperbolic distributions. The\nproperties of the models are explored through both analytic results and\nsimulation studies. We demonstrate the model on a financial application where\nit is shown that it accurately represents the patterns seen in the analysis of\nstock and derivative data, and is able to detect major events by filtering an\nartificial portfolio of assets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 07:39:21 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Caron", "Fran\u00e7ois", "", "INRIA Bordeaux - Sud-Ouest, IMB"], ["Bornn", "Luke", "", "Statistics"], ["Doucet", "Arnaud", ""]]}, {"id": "1203.0453", "submitter": "Song Liu Mr", "authors": "Song Liu, Makoto Yamada, Nigel Collier, Masashi Sugiyama", "title": "Change-Point Detection in Time-Series Data by Relative Density-Ratio\n  Estimation", "comments": null, "journal-ref": null, "doi": "10.1016/j.neunet.2013.01.012", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of change-point detection is to discover abrupt property\nchanges lying behind time-series data. In this paper, we present a novel\nstatistical change-point detection algorithm based on non-parametric divergence\nestimation between time-series samples from two retrospective segments. Our\nmethod uses the relative Pearson divergence as a divergence measure, and it is\naccurately and efficiently estimated by a method of direct density-ratio\nestimation. Through experiments on artificial and real-world datasets including\nhuman-activity sensing, speech, and Twitter messages, we demonstrate the\nusefulness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 13:12:03 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2013 06:44:58 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Liu", "Song", ""], ["Yamada", "Makoto", ""], ["Collier", "Nigel", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1203.0685", "submitter": "Gane  Samb Lo", "authors": "Gane Samb Lo", "title": "On a discrete Hill's statistical process based on sum-product statistics\n  and its finite-dimensional asymptotic theory", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The following class of sum-product statistics\n  T_n(p)=\\frac{1}{k}\\sum_{h=1}^p \\sum_{(s_1...s_h)\\in P(p,h)}\n\\sum_{i_1=l+1}^{i_0} ... \\sum_{i_h=l+1}^{i_{h-1}} i_h \\prod_{i=i_1}^{i_h}\n\\frac{(Y_{n-i+1,n}-Y_{n-i,n})^{s_i}}{s_i!}\n  (where $l,$ $k=i_{0}$ and n are positive integers, $0<l<k<n,$ $P(p,h)$ is the\nset of all ordered parititions of $\\ p>0$ into $\\ h$ positive integers and\n$Y_{1,n}\\leq ...\\leq Y_{n,n}$ are the order statistics based on a sequence of\nindependent random variables $Y_{1},$ $Y_{2},...$with underlying distribution\n$\\mathbb{P}(Y\\leq y)=G(Y)=F(e^{y})$), is introduced. For each p,\n$T_{n}(p)^{-1/p}$ is an estimator of the index of a distribution whose upper\ntail varies regularly at infinity. \\ This family generalizes the so called Hill\nstatistic and the Dekkers-Einmahl-De Haan one. We study the limiting laws of\nthe process ${T_{n}(p),1\\leq p<\\infty}$ and completely describe the covariance\nfunction of the Gaussian limiting process with the help of combinatorial\ntechniques. Many results available for Hill's statistic regarding asymptotic\nnormality and laws of the iterated logarithm are extended to each margin\n$T_{n}(p,k)$, for $p$ fixed, and for any distribution function lying in the\nextremal domain. In the process, we obtain special classes of numbers related\nto those of paths joining the opposite coins within a parallelogram.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 21:27:52 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Lo", "Gane Samb", ""]]}, {"id": "1203.0812", "submitter": "David Shilane", "authors": "David Shilane and Derek Bean", "title": "Two-Sample Inference in Highly Dispersed Negative Binomial Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-sample inference for the difference of population means typically relies\nupon a Central Limit Theorem approximation. When data are drawn from a Negative\nBinomial distribution, previous work of Shilane et al. (2010) showed that a\nNormal approximation is often unreliable in one-sample inference and proposed\nalternative techniques. We seek to extend these methods to the problem of\ntwo-sample inference on the difference of sample means in highly dispersed\nNegative Binomial models. We demonstrate that the Normal approximation is\nconsiderably more robust in the two-sample setting and may often be applied\neven when it is not appropriate for either sample individually. We also provide\nan intuitive extension of Bernstein's Inequality to the two-sample case. A\nsimple mixture of these two methods may improve coverage in borderline and\nsmall sample settings. We subsequently investigate the coverage quality of\nconfidence intervals and sample size considerations in a wide variety of\nsimulation studies. Overall, we demonstrate that the Normal approximation is\nconsiderably more robust in approximating the distribution of the mean\ndifference than the corresponding one-sample theory would suggest.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 04:22:06 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Shilane", "David", ""], ["Bean", "Derek", ""]]}, {"id": "1203.0813", "submitter": "David Shilane", "authors": "David Shilane and Derek Bean", "title": "Growth Estimators and Confidence Intervals for the Mean of Negative\n  Binomial Random Variables with Unknown Dispersion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Negative Binomial distribution becomes highly skewed under extreme\ndispersion. Even at moderately large sample sizes, the sample mean exhibits a\nheavy right tail. The standard Normal approximation often does not provide\nadequate inferences about the data's mean in this setting. In previous work, we\nhave examined alternative methods of generating confidence intervals for the\nexpected value. These methods were based upon Gamma and Chi Square\napproximations or tail probability bounds such as Bernstein's Inequality. We\nnow propose growth estimators of the Negative Binomial mean. Under high\ndispersion, zero values are likely to be overrepresented in the data. A growth\nestimator constructs a Normal-style confidence interval by effectively removing\na small, pre--determined number of zeros from the data. We propose growth\nestimators based upon multiplicative adjustments of the sample mean and direct\nremoval of zeros from the sample. These methods do not require estimating the\nnuisance dispersion parameter. We will demonstrate that the growth estimators'\nconfidence intervals provide improved coverage over a wide range of parameter\nvalues and asymptotically converge to the sample mean. Interestingly, the\nproposed methods succeed despite adding both bias and variance to the Normal\napproximation.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 04:33:46 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Shilane", "David", ""], ["Bean", "Derek", ""]]}, {"id": "1203.1078", "submitter": "Pritam Ranjan", "authors": "Hugh Chipman, Pritam Ranjan and Weiwei Wang", "title": "Sequential Design for Computer Experiments with a Flexible Bayesian\n  Additive Model", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computer experiments, a mathematical model implemented on a computer is\nused to represent complex physical phenomena. These models, known as computer\nsimulators, enable experimental study of a virtual representation of the\ncomplex phenomena. Simulators can be thought of as complex functions that take\nmany inputs and provide an output. Often these simulators are themselves\nexpensive to compute, and may be approximated by \"surrogate models\" such as\nstatistical regression models. In this paper we consider a new kind of\nsurrogate model, a Bayesian ensemble of trees (Chipman et al. 2010), with the\nspecific goal of learning enough about the simulator that a particular feature\nof the simulator can be estimated. We focus on identifying the simulator's\nglobal minimum. Utilizing the Bayesian version of the Expected Improvement\ncriterion (Jones et al. 1998), we show that this ensemble is particularly\neffective when the simulator is ill-behaved, exhibiting nonstationarity or\nabrupt changes in the response. A number of illustrations of the approach are\ngiven, including a tidal power application.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2012 00:45:50 GMT"}, {"version": "v2", "created": "Sun, 1 Jul 2012 10:46:44 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Chipman", "Hugh", ""], ["Ranjan", "Pritam", ""], ["Wang", "Weiwei", ""]]}, {"id": "1203.1365", "submitter": "Matthew Johnson", "authors": "Matthew J. Johnson and Alan S. Willsky", "title": "Bayesian Nonparametric Hidden Semi-Markov Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is much interest in the Hierarchical Dirichlet Process Hidden Markov\nModel (HDP-HMM) as a natural Bayesian nonparametric extension of the ubiquitous\nHidden Markov Model for learning from sequential and time-series data. However,\nin many settings the HDP-HMM's strict Markovian constraints are undesirable,\nparticularly if we wish to learn or encode non-geometric state durations. We\ncan extend the HDP-HMM to capture such structure by drawing upon\nexplicit-duration semi-Markovianity, which has been developed mainly in the\nparametric frequentist setting, to allow construction of highly interpretable\nmodels that admit natural prior information on state durations.\n  In this paper we introduce the explicit-duration Hierarchical Dirichlet\nProcess Hidden semi-Markov Model (HDP-HSMM) and develop sampling algorithms for\nefficient posterior inference. The methods we introduce also provide new\nmethods for sampling inference in the finite Bayesian HSMM. Our modular Gibbs\nsampling methods can be embedded in samplers for larger hierarchical Bayesian\nmodels, adding semi-Markov chain modeling as another tool in the Bayesian\ninference toolbox. We demonstrate the utility of the HDP-HSMM and our inference\nmethods on both synthetic and real experiments.\n", "versions": [{"version": "v1", "created": "Wed, 7 Mar 2012 01:53:14 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2012 22:21:25 GMT"}], "update_date": "2012-09-11", "authors_parsed": [["Johnson", "Matthew J.", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1203.1829", "submitter": "Nanny Wermuth", "authors": "Nanny Wermuth (Chalmers Technical University, Gothenburg and IARC\n  Lyon), Giovanni M. Marchetti (University of Florence, Department of\n  Statistics, Florence, Graham Byrnes, IARC Lyon)", "title": "Case-control studies for rare diseases: improved estimation of several\n  risks and of feature dependences", "comments": "27 pages, 5 figures, 19 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To capture the dependences of a disease on several risk factors, a challenge\nis to combine model-based estimation with evidence-based arguments. Standard\ncase-control methods allow estimation of the dependences of a rare disease on\nseveral regressors via logistic regressions. For case-control studies, the\nsampling design leads to samples from two different populations and for the set\nof regressors in every logistic regression, these samples are then mixed and\ntaken as given observations. But, it is the differences in independence\nstructures of regressors for cases and for controls that can improve logistic\nregression estimates and guide us to the important feature dependences that are\nspecific to the diseased. A case-control study on laryngeal cancer is used as\nillustration.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 15:35:26 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Wermuth", "Nanny", "", "Chalmers Technical University, Gothenburg and IARC\n  Lyon"], ["Marchetti", "Giovanni M.", "", "University of Florence, Department of\n  Statistics, Florence, Graham Byrnes, IARC Lyon"]]}, {"id": "1203.1975", "submitter": "Daniel Gervini", "authors": "Daniel Gervini", "title": "Warped Functional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A characteristic feature of functional data is the presence of phase\nvariability in addition to amplitude variability. Existing functional\nregression methods do not handle time variability in an explicit and efficient\nway. In this paper we introduce a functional regression method that\nincorporates time warping as an intrinsic part of the model. The method\nachieves good predictive power in a parsimonious way and allows unified\nstatistical inference about phase and amplitude components. The asymptotic\ndistribution of the estimators is derived and the finite-sample properties are\nstudied by simulation. An example of application involving ground-level ozone\ntrajectories is presented.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 02:27:54 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2013 19:33:28 GMT"}, {"version": "v3", "created": "Sun, 20 Apr 2014 18:48:57 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Gervini", "Daniel", ""]]}, {"id": "1203.2062", "submitter": "Bruno Sudret", "authors": "Bruno Sudret", "title": "Meta-models for structural reliability and uncertainty quantification", "comments": "Keynote lecture Fifth Asian-Pacific Symposium on Structural\n  Reliability and its Applications (5th APSSRA) May 2012, Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A meta-model (or a surrogate model) is the modern name for what was\ntraditionally called a response surface. It is intended to mimic the behaviour\nof a computational model M (e.g. a finite element model in mechanics) while\nbeing inexpensive to evaluate, in contrast to the original model which may take\nhours or even days of computer processing time. In this paper various types of\nmeta-models that have been used in the last decade in the context of structural\nreliability are reviewed. More specifically classical polynomial response\nsurfaces, polynomial chaos expansions and kriging are addressed. It is shown\nhow the need for error estimates and adaptivity in their construction has\nbrought this type of approaches to a high level of efficiency. A new technique\nthat solves the problem of the potential biasedness in the estimation of a\nprobability of failure through the use of meta-models is finally presented.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 12:49:35 GMT"}], "update_date": "2012-03-12", "authors_parsed": [["Sudret", "Bruno", ""]]}, {"id": "1203.2343", "submitter": "Ben Youngman", "authors": "B. D. Youngman", "title": "Spatial Interpolation of Extreme Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a method for spatial interpolation of extreme values,\nand in particular targets the case in which conventional data, resulting from a\nmeasurement for example, are available at only a few locations. To overcome\nthis the conventional data are supplemented with output from a computer\nsimulator. For environmental applications, such as extreme rainfall as we study\nhere, the simulator could be a regional climate model. Annual maxima are\nstudied and assumed to follow the generalised extreme value (GEV) distribution\nand dependence is accommodated between maxima by its parameters following\nGaussian processes. The GEV's parameters are now random and so a modification\nto the Monte Carlo EM algorithm is presented so that the model's parameters can\nbe estimated. Then a variety of checks for the model's goodness of fit are\ngiven. For the extreme rainfall application we find that the model estimates a\nvariety of extremal features of interest well, and then show how specific\nfeatures can be interpolated based on the model so that maps can be produced.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2012 15:34:50 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Youngman", "B. D.", ""]]}, {"id": "1203.2376", "submitter": "Adelchi Azzalini", "authors": "Adelchi Azzalini and Reinaldo B. Arellano-Valle", "title": "Maximum penalized likelihood estimation for skew-normal and skew-$t$\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The skew-normal and the skew-$t$ distributions are parametric families which\nare currently under intense investigation since they provide a more flexible\nformulation compared to the classical normal and $t$ distributions by\nintroducing a parameter which regulates their skewness. While these families\nenjoy attractive formal properties from the probability viewpoint, a practical\nproblem with their usage in applications is the possibility that the maximum\nlikelihood estimate of the parameter which regulates skewness diverges. This\nsituation has vanishing probability for increasing sample size, but for finite\nsamples it occurs with non-negligible probability, and its occurrence has\nunpleasant effects on the inferential process. Methods for overcoming this\nproblem have been put forward both in the classical and in the Bayesian\nformulation, but their applicability is restricted to simple situations. We\nformulate a proposal based on the idea of penalized likelihood, which has\nconnections with some of the existing methods, but it applies more generally,\nincluding in the multivariate case.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2012 21:22:11 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Azzalini", "Adelchi", ""], ["Arellano-Valle", "Reinaldo B.", ""]]}, {"id": "1203.2403", "submitter": "Sourabh Bhattacharya", "authors": "Sourabh Bhattacharya", "title": "A Fully Bayesian Approach to Assessment of Model Adequacy in Inverse\n  Problems", "comments": "A significantly updated version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of assessing goodness of fit of a single Bayesian\nmodel to the observed data in the inverse problem context. A novel procedure of\ngoodness of fit test is proposed, based on construction of reference\ndistributions using the `inverse' part of the given model. This is motivated by\nan example from palaeoclimatology in which it is of interest to reconstruct\npast climates using information obtained from fossils deposited in lake\nsediment.\n  Technically, given a model $f(Y\\mid X,\\theta)$, where $Y$ is the observed\ndata and $X$ is a set of (non-random) covariates, we obtain reference\ndistributions based on the posterior $\\pi(\\tilde X\\mid Y)$, where $\\tilde X$\nmust be interpreted as the {\\it unobserved} random vector corresponding to the\n{\\it observed} covariates $X$. Put simply, if the posterior distribution\n$\\pi(\\tilde X\\mid Y)$ gives high density to the observed covariates $X$, or\nequivalently, if the posterior distribution of $T(\\tilde X)$ gives high density\nto $T(X)$, where $T$ is any appropriate statistic, then we say that the model\nfits the data. Otherwise the model in question is not adequate. We provide\ndecision-theoretic justification of our proposed approach and discuss other\ntheoretical and computational advantages. We demonstrate our methodology with\nmany simulated examples and three complex, high-dimensional, realistic\npalaeoclimate problems, including the motivating palaeoclimate problem.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 05:31:10 GMT"}, {"version": "v2", "created": "Fri, 1 May 2020 06:45:09 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Bhattacharya", "Sourabh", ""]]}, {"id": "1203.2591", "submitter": "Zhijian Wang Dr.", "authors": "Zhijian Wang and Bin Xu", "title": "Evolutionary Rotation in Switching Incentive Zero-Sum Games", "comments": "8 pages, 3 figures; Keywords: experimental economics, evolutionary\n  rotation, replicator dynamics, zero-sum game, switching incentive", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST nlin.CD stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a laboratory experiment, round by round, individual interactions should\nlead to the social evolutionary rotation in population strategy state space.\nSuccessive switching the incentive parameter should lead to successive change\nof the rotation ---- both of its direction and its strength. In data from a\nswitching payoff matrix experiment of extended 2x2 games (Binmore, Swierzbinski\nand Proulx, 2001 [1]), we find the changing of the social evolutionary rotation\ncan be distinguished quantitatively. The evolutionary rotation can be captured\nby evolutionary dynamics. With eigenvalue from the Jacobian of a constrained\nreplicator dynamics model, an interpretation for observed rotation strength is\ngiven. In addition, equality-of-populations rank test shows that relative\nresponse coefficient of a group could persist cross the switching parameter\ngames. The data has successively been used to support Von Neumann's minimax\ntheory. Using the old data, with observed evolutionary rotation, this report\nprovides a new insight into evolutionary game theory and experimental social\ndynamics.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 19:15:24 GMT"}, {"version": "v2", "created": "Sat, 31 Mar 2012 17:16:10 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2012 09:26:35 GMT"}], "update_date": "2012-07-25", "authors_parsed": [["Wang", "Zhijian", ""], ["Xu", "Bin", ""]]}, {"id": "1203.2821", "submitter": "Edoardo Airoldi", "authors": "Hossein Azari Soufiani, Edoardo M Airoldi", "title": "Graphlet decomposition of a weighted network", "comments": "25 pages, 4 figures, 3 tables", "journal-ref": "Journal of Machine Learning Research, Workshop & Conference\n  Proceedings, vol. 22 (AISTATS), 2012", "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the graphlet decomposition of a weighted network, which encodes\na notion of social information based on social structure. We develop a scalable\ninference algorithm, which combines EM with Bron-Kerbosch in a novel fashion,\nfor estimating the parameters of the model underlying graphlets using one\nnetwork sample. We explore some theoretical properties of the graphlet\ndecomposition, including computational complexity, redundancy and expected\naccuracy. We demonstrate graphlets on synthetic and real data. We analyze\nmessaging patterns on Facebook and criminal associations in the 19th century.\n", "versions": [{"version": "v1", "created": "Tue, 13 Mar 2012 14:18:56 GMT"}], "update_date": "2012-03-14", "authors_parsed": [["Soufiani", "Hossein Azari", ""], ["Airoldi", "Edoardo M", ""]]}, {"id": "1203.3209", "submitter": "Hua Zhou", "authors": "Hua Zhou and Lexin Li and Hongtu Zhu", "title": "Tensor Regression with Applications in Neuroimaging Data Analysis", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": "10.1080/01621459.2013.776499", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Classical regression methods treat covariates as a vector and estimate a\ncorresponding vector of regression coefficients. Modern applications in medical\nimaging generate covariates of more complex form such as multidimensional\narrays (tensors). Traditional statistical and computational methods are proving\ninsufficient for analysis of these high-throughput data due to their ultrahigh\ndimensionality as well as complex structure. In this article, we propose a new\nfamily of tensor regression models that efficiently exploit the special\nstructure of tensor covariates. Under this framework, ultrahigh dimensionality\nis reduced to a manageable level, resulting in efficient estimation and\nprediction. A fast and highly scalable estimation algorithm is proposed for\nmaximum likelihood estimation and its associated asymptotic properties are\nstudied. Effectiveness of the new methods is demonstrated on both synthetic and\nreal MRI imaging data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 20:33:53 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Zhou", "Hua", ""], ["Li", "Lexin", ""], ["Zhu", "Hongtu", ""]]}, {"id": "1203.3278", "submitter": "Cheng Wang", "authors": "Cheng Wang, Jing Yang, Baiqi Miao and Longbing Cao", "title": "On Identity Tests for High Dimensional Data Using RMT", "comments": "16 pages, 2 figures, 3 tables, To be published in the Journal of\n  Multivariate Analysis", "journal-ref": null, "doi": "10.1016/j.jmva.2013.03.015", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we redefined two important statistics, the CLRT test (Bai\net.al., Ann. Stat. 37 (2009) 3822-3840) and the LW test (Ledoit and Wolf, Ann.\nStat. 30 (2002) 1081-1102) on identity tests for high dimensional data using\nrandom matrix theories. Compared with existing CLRT and LW tests, the new tests\ncan accommodate data which has unknown means and non-Gaussian distributions.\nSimulations demonstrate that the new tests have good properties in terms of\nsize and power. What is more, even for Gaussian data, our new tests perform\nfavorably in comparison to existing tests. Finally, we find the CLRT is more\nsensitive to eigenvalues less than 1 while the LW test has more advantages in\nrelation to detecting eigenvalues larger than 1.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 06:57:19 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2012 02:24:43 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2013 11:07:19 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Wang", "Cheng", ""], ["Yang", "Jing", ""], ["Miao", "Baiqi", ""], ["Cao", "Longbing", ""]]}, {"id": "1203.3328", "submitter": "Eike Christian Brechmann", "authors": "Eike Christian Brechmann and Claudia Czado", "title": "COPAR - Multivariate time series modeling using the COPula\n  AutoRegressive model", "comments": "arXiv admin note: extreme overlap with arXiv:1202.1998", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of multivariate time series is a common problem in areas like\nfinance and economics. The classical tool for this purpose are vector\nautoregressive models. These however are limited to the modeling of linear and\nsymmetric dependence. We propose a novel copula-based model which allows for\nnon-linear and asymmetric modeling of serial as well as between-series\ndependencies. The model exploits the flexibility of vine copulas which are\nbuilt up by bivariate copulas only. We describe statistical inference\ntechniques for the new model and demonstrate its usefulness in three relevant\napplications: We analyze time series of macroeconomic indicators, of\nelectricity load demands and of bond portfolio returns.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:38:16 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2012 12:05:06 GMT"}, {"version": "v3", "created": "Wed, 4 Apr 2012 15:35:26 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Brechmann", "Eike Christian", ""], ["Czado", "Claudia", ""]]}, {"id": "1203.3366", "submitter": "Chris Jewell PhD", "authors": "Chris Jewell and Gareth Roberts", "title": "Enhancing Bayesian risk prediction for epidemics using contact tracing", "comments": "40 pages, 9 figures. Submitted to Biostatistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contact tracing data collected from disease outbreaks has received relatively\nlittle attention in the epidemic modelling literature because it is thought to\nbe unreliable: infection sources might be wrongly attributed, or data might be\nmissing due to resource contraints in the questionnaire exercise. Nevertheless,\nthese data might provide a rich source of information on disease transmission\nrate. This paper presents novel methodology for combining contact tracing data\nwith rate-based contact network data to improve posterior precision, and\ntherefore predictive accuracy. We present an advancement in Bayesian inference\nfor epidemics that assimilates these data, and is robust to partial contact\ntracing. Using a simulation study based on the British poultry industry, we\nshow how the presence of contact tracing data improves posterior predictive\naccuracy, and can directly inform a more effective control strategy.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 14:22:10 GMT"}], "update_date": "2012-03-16", "authors_parsed": [["Jewell", "Chris", ""], ["Roberts", "Gareth", ""]]}, {"id": "1203.3380", "submitter": "Jonathan Lilly", "authors": "Jonathan M. Lilly and Sofia C. Olhede", "title": "Generalized Morse Wavelets as a Superfamily of Analytic Wavelets", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2012.2210890", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized Morse wavelets are shown to constitute a superfamily that\nessentially encompasses all other commonly used analytic wavelets, subsuming\neight apparently distinct types of analysis filters into a single common form.\nThis superfamily of analytic wavelets provides a framework for systematically\ninvestigating wavelet suitability for various applications. In addition to a\nparameter controlling the time-domain duration or Fourier-domain bandwidth, the\nwavelet {\\em shape} with fixed bandwidth may be modified by varying a second\nparameter, called $\\gamma$. For integer values of $\\gamma$, the most symmetric,\nmost nearly Gaussian, and generally most time-frequency concentrated member of\nthe superfamily is found to occur for $\\gamma=3$. These wavelets, known as\n\"Airy wavelets,\" capture the essential idea of popular Morlet wavelet, while\navoiding its deficiencies. They may be recommended as an ideal starting point\nfor general purpose use.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 15:03:49 GMT"}, {"version": "v2", "created": "Sat, 9 Jun 2012 19:46:38 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Lilly", "Jonathan M.", ""], ["Olhede", "Sofia C.", ""]]}, {"id": "1203.3479", "submitter": "Robin J. Evans", "authors": "Robin J. Evans, Thomas S. Richardson", "title": "Maximum likelihood fitting of acyclic directed mixed graphs to binary\n  data", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-177-184", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acyclic directed mixed graphs, also known as semi-Markov models represent the\nconditional independence structure induced on an observed margin by a DAG model\nwith latent variables. In this paper we present the first method for fitting\nthese models to binary data using maximum likelihood estimation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Evans", "Robin J.", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1203.3503", "submitter": "Judea Pearl", "authors": "Judea Pearl", "title": "On a Class of Bias-Amplifying Variables that Endanger Effect Estimates", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-417-424", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note deals with a class of variables that, if conditioned on, tends to\namplify confounding bias in the analysis of causal effects. This class,\nindependently discovered by Bhattacharya and Vogt (2007) and Wooldridge (2009),\nincludes instrumental variables and variables that have greater influence on\ntreatment selection than on the outcome. We offer a simple derivation and an\nintuitive explanation of this phenomenon and then extend the analysis to non\nlinear models. We show that: 1. the bias-amplifying potential of instrumental\nvariables extends over to non-linear models, though not as sweepingly as in\nlinear models; 2. in non-linear models, conditioning on instrumental variables\nmay introduce new bias where none existed before; 3. in both linear and\nnon-linear models, instrumental variables have no effect on selection-induced\nbias.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Pearl", "Judea", ""]]}, {"id": "1203.3504", "submitter": "Judea Pearl", "authors": "Judea Pearl", "title": "On Measurement Bias in Causal Inference", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-425-432", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of measurement errors in causal inference\nand highlights several algebraic and graphical methods for eliminating\nsystematic bias induced by such errors. In particulars, the paper discusses the\ncontrol of partially observable confounders in parametric and non parametric\nmodels and the computational problem of obtaining bias-free effect estimates in\nsuch models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Pearl", "Judea", ""]]}, {"id": "1203.3505", "submitter": "Judea Pearl", "authors": "Judea Pearl, Azaria Paz", "title": "Confounding Equivalence in Causal Inference", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-433-441", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper provides a simple test for deciding, from a given causal diagram,\nwhether two sets of variables have the same bias-reducing potential under\nadjustment. The test requires that one of the following two conditions holds:\neither (1) both sets are admissible (i.e., satisfy the back-door criterion) or\n(2) the Markov boundaries surrounding the manipulated variable(s) are identical\nin both sets. Applications to covariate selection and model testing are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Pearl", "Judea", ""], ["Paz", "Azaria", ""]]}, {"id": "1203.3515", "submitter": "Ilya Shpitser", "authors": "Ilya Shpitser, Tyler VanderWeele, James M. Robins", "title": "On the Validity of Covariate Adjustment for Estimating Causal Effects", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-527-536", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying effects of actions (treatments) on outcome variables from\nobservational data and causal assumptions is a fundamental problem in causal\ninference. This identification is made difficult by the presence of confounders\nwhich can be related to both treatment and outcome variables. Confounders are\noften handled, both in theory and in practice, by adjusting for covariates, in\nother words considering outcomes conditioned on treatment and covariate values,\nweighed by probability of observing those covariate values. In this paper, we\ngive a complete graphical criterion for covariate adjustment, which we term the\nadjustment criterion, and derive some interesting corollaries of the\ncompleteness of this criterion.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Shpitser", "Ilya", ""], ["VanderWeele", "Tyler", ""], ["Robins", "James M.", ""]]}, {"id": "1203.3663", "submitter": "Hung Hung", "authors": "Hung Hung", "title": "A Two-Stage Dimension Reduction Method for Induced Responses and Its\n  Applications", "comments": "18 pages, 2 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in the biological sciences nowadays often encounter the curse of\nhigh-dimensionality, which many previously developed statistical models fail to\novercome. To tackle this problem, sufficient dimension reduction aims to\nestimate the central subspace (CS), in which all the necessary information\nsupplied by the covariates regarding the response of interest is contained.\nSubsequent statistical analysis can then be made in a lower-dimensional space\nwhile preserving relevant information. Oftentimes studies are interested in a\ncertain transformation of the response (the induced response), instead of the\noriginal one, whose corresponding CS may vary. When estimating the CS of the\ninduced response, existing dimension reduction methods may, however, suffer the\nproblem of inefficiency. In this article, we propose a more efficient two-stage\nestimation procedure to estimate the CS of an induced response. This approach\nis further extended to the case of censored responses. An application for\ncombining multiple biomarkers is also illustrated. Simulation studies and two\ndata examples provide further evidence of the usefulness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 10:58:20 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Hung", "Hung", ""]]}, {"id": "1203.3896", "submitter": "Xi Luo", "authors": "Weidong Liu and Xi Luo", "title": "Fast and Adaptive Sparse Precision Matrix Estimation in High Dimensions", "comments": "Maintext: 24 pages. Supplement: 13 pages. R package scio implementing\n  the proposed method is available on CRAN at\n  https://cran.r-project.org/package=scio . Published in J of Multivariate\n  Analysis at\n  http://www.sciencedirect.com/science/article/pii/S0047259X14002607", "journal-ref": "Journal of Multivariate Analysis. 2015; 135:153 -62", "doi": "10.1016/J.Jmva.2014.11.005", "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for estimating sparse precision matrices in\nthe high dimensional setting. It has been popular to study fast computation and\nadaptive procedures for this problem. We propose a novel approach, called\nSparse Column-wise Inverse Operator, to address these two issues. We analyze an\nadaptive procedure based on cross validation, and establish its convergence\nrate under the Frobenius norm. The convergence rates under other matrix norms\nare also established. This method also enjoys the advantage of fast computation\nfor large-scale problems, via a coordinate descent algorithm. Numerical merits\nare illustrated using both simulated and real datasets. In particular, it\nperforms favorably on an HIV brain tissue dataset and an ADHD resting-state\nfMRI dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2012 21:58:02 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 07:09:46 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Liu", "Weidong", ""], ["Luo", "Xi", ""]]}, {"id": "1203.4326", "submitter": "Shuichi Kawano", "authors": "Shuichi Kawano", "title": "Selection of tuning parameters in bridge regression models via Bayesian\n  information criterion", "comments": "20 pages, 5 figures", "journal-ref": "Statistical Papers 55 (2014) 1207-1223", "doi": "10.1007/s00362-013-0561-7", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the bridge linear regression modeling, which can produce a sparse\nor non-sparse model. A crucial point in the model building process is the\nselection of adjusted parameters including a regularization parameter and a\ntuning parameter in bridge regression models. The choice of the adjusted\nparameters can be viewed as a model selection and evaluation problem. We\npropose a model selection criterion for evaluating bridge regression models in\nterms of Bayesian approach. This selection criterion enables us to select the\nadjusted parameters objectively. We investigate the effectiveness of our\nproposed modeling strategy through some numerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 06:41:16 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2012 00:11:15 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2012 02:29:41 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Kawano", "Shuichi", ""]]}, {"id": "1203.4427", "submitter": "Mohammad Arashi", "authors": "M. Arashi, A. K. Md E. Saleh, S. M. M. Tabatabaey", "title": "Regression Model With Elliptically Contoured Errors", "comments": "final version will be published in Statistics: A Journal of\n  Theoretical and Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the regression model where the errors follow the elliptically contoured\ndistribution (ECD), we consider the least squares (LS), restricted LS (RLS),\npreliminary test (PT), Stein-type shrinkage (S) and positive-rule shrinkage\n(PRS) estimators for the regression parameters. We compare the quadratic risks\nof the estimators to determine the relative dominance properties of the five\nestimators.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 13:26:40 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Arashi", "M.", ""], ["Saleh", "A. K. Md E.", ""], ["Tabatabaey", "S. M. M.", ""]]}, {"id": "1203.4480", "submitter": "Torsten Ensslin", "authors": "Torsten A. En{\\ss}lin and Cornelius Weig", "title": "Reply to \"Comment on `Inference with minimal Gibbs free energy in\n  information field theory'\" by Iatsenko, Stefanovska and McClintock", "comments": "1 page, no figures, Reply to Comment paper", "journal-ref": "Phys. Rev. E 85, 033102 (2012)", "doi": "10.1103/PhysRevE.85.033102", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We endorse the comment on our recent paper [En{\\ss}lin and Weig, Phys. Rev. E\n82, 051112 (2010)] by Iatsenko, Stefanovska and McClintock [Phys. Rev. E 85\n033101 (2012)] and we try to clarify the origin of the apparent controversy on\ntwo issues. The aim of the minimal Gibbs free energy approach to provide a\nsignal estimate is not affected by their Comment. However, if one wants to\nextend the method to also infer the a posteriori signal uncertainty any\ntempering of the posterior has to be undone at the end of the calculations, as\nthey correctly point out. Furthermore, a distinction is made here between\nmaximum entropy, the maximum entropy principle, and the so-called maximum\nentropy method in imaging, hopefully clarifying further the second issue of\ntheir Comment paper.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 15:56:02 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["En\u00dflin", "Torsten A.", ""], ["Weig", "Cornelius", ""]]}, {"id": "1203.4643", "submitter": "Chanseok Park", "authors": "Chanseok Park", "title": "Determination of the Joint Confidence Region of Optimal Operating\n  Conditions in Robust Design by Bootstrap Technique", "comments": "Two tables, Three figures", "journal-ref": "Int.J.of.Production.Research 51 (2013) 4695-4703", "doi": "10.1080/00207543.2013.792963", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust design has been widely recognized as a leading method in reducing\nvariability and improving quality. Most of the engineering statistics\nliterature mainly focuses on finding \"point estimates\" of the optimum operating\nconditions for robust design. Various procedures for calculating point\nestimates of the optimum operating conditions are considered. Although this\npoint estimation procedure is important for continuous quality improvement, the\nimmediate question is \"how accurate are these optimum operating conditions?\"\nThe answer for this is to consider interval estimation for a single variable or\njoint confidence regions for multiple variables.\n  In this paper, with the help of the bootstrap technique, we develop\nprocedures for obtaining joint \"confidence regions\" for the optimum operating\nconditions. Two different procedures using Bonferroni and multivariate normal\napproximation are introduced. The proposed methods are illustrated and\nsubstantiated using a numerical example.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 02:58:52 GMT"}], "update_date": "2013-08-14", "authors_parsed": [["Park", "Chanseok", ""]]}, {"id": "1203.4664", "submitter": "Edward I. George", "authors": "Edward I. George, William E. Strawderman", "title": "A Tribute to Charles Stein", "comments": "Published in at http://dx.doi.org/10.1214/11-STS385 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 1, 1-2", "doi": "10.1214/11-STS385", "report-no": "IMS-STS-STS385", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 1956, Charles Stein published an article that was to forever change the\nstatistical approach to high-dimensional estimation. His stunning discovery\nthat the usual estimator of the normal mean vector could be dominated in\ndimensions 3 and higher amazed many at the time, and became the catalyst for a\nvast and rich literature of substantial importance to statistical theory and\npractice. As a tribute to Charles Stein, this special issue on minimax\nshrinkage estimation is devoted to developments that ultimately arose from\nStein's investigations into improving on the UMVUE of a multivariate normal\nmean vector. Of course, much of the early literature on the subject was due to\nStein himself, including a key technical lemma commonly referred to as Stein's\nLemma, which leads to an unbiased estimator of the risk of an almost arbitrary\nestimator of the mean vector.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 07:27:22 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["George", "Edward I.", ""], ["Strawderman", "William E.", ""]]}, {"id": "1203.4690", "submitter": "James Berger", "authors": "James Berger, William H. Jefferys, Peter M\\\"uller", "title": "Bayesian Nonparametric Shrinkage Applied to Cepheid Star Oscillations", "comments": "Published in at http://dx.doi.org/10.1214/11-STS384 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 1, 3-10", "doi": "10.1214/11-STS384", "report-no": "IMS-STS-STS384", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian nonparametric regression with dependent wavelets has dual shrinkage\nproperties: there is shrinkage through a dependent prior put on functional\ndifferences, and shrinkage through the setting of most of the wavelet\ncoefficients to zero through Bayesian variable selection methods. The\nmethodology can deal with unequally spaced data and is efficient because of the\nexistence of fast moves in model space for the MCMC computation. The\nmethodology is illustrated on the problem of modeling the oscillations of\nCepheid variable stars; these are a class of pulsating variable stars with the\nuseful property that their periods of variability are strongly correlated with\ntheir absolute luminosity. Once this relationship has been calibrated,\nknowledge of the period gives knowledge of the luminosity. This makes these\nstars useful as \"standard candles\" for estimating distances in the universe.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 09:44:32 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Berger", "James", ""], ["Jefferys", "William H.", ""], ["M\u00fcller", "Peter", ""]]}, {"id": "1203.4724", "submitter": "Ann Cohen Brandwein", "authors": "Ann Cohen Brandwein, William E. Strawderman", "title": "Stein Estimation for Spherically Symmetric Distributions: Recent\n  Developments", "comments": "Published in at http://dx.doi.org/10.1214/10-STS323 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 1, 11-23", "doi": "10.1214/10-STS323", "report-no": "IMS-STS-STS323", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews advances in Stein-type shrinkage estimation for\nspherically symmetric distributions. Some emphasis is placed on developing\nintuition as to why shrinkage should work in location problems whether the\nunderlying population is normal or not. Considerable attention is devoted to\ngeneralizing the \"Stein lemma\" which underlies much of the theoretical\ndevelopment of improved minimax estimation for spherically symmetric\ndistributions. A main focus is on distributional robustness results in cases\nwhere a residual vector is available to estimate an unknown scale parameter,\nand, in particular, in finding estimators which are simultaneously generalized\nBayes and minimax over large classes of spherically symmetric distributions.\nSome attention is also given to the problem of estimating a location vector\nrestricted to lie in a polyhedral cone.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 12:52:01 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Brandwein", "Ann Cohen", ""], ["Strawderman", "William E.", ""]]}, {"id": "1203.4737", "submitter": "Lawrence D. Brown", "authors": "Lawrence D. Brown, Linda H. Zhao", "title": "A Geometrical Explanation of Stein Shrinkage", "comments": "Published in at http://dx.doi.org/10.1214/11-STS382 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 1, 24-30", "doi": "10.1214/11-STS382", "report-no": "IMS-STS-STS382", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shrinkage estimation has become a basic tool in the analysis of\nhigh-dimensional data. Historically and conceptually a key development toward\nthis was the discovery of the inadmissibility of the usual estimator of a\nmultivariate normal mean. This article develops a geometrical explanation for\nthis inadmissibility. By exploiting the spherical symmetry of the problem it is\npossible to effectively conceptualize the multidimensional setting in a\ntwo-dimensional framework that can be easily plotted and geometrically\nanalyzed. We begin with the heuristic explanation for inadmissibility that was\ngiven by Stein [In Proceedings of the Third Berkeley Symposium on Mathematical\nStatistics and Probability, 1954--1955, Vol. I (1956) 197--206, Univ.\nCalifornia Press]. Some geometric figures are included to make this reasoning\nmore tangible. It is also explained why Stein's argument falls short of\nyielding a proof of inadmissibility, even when the dimension, $p$, is much\nlarger than $p=3$. We then extend the geometric idea to yield increasingly\npersuasive arguments for inadmissibility when $p\\geq3$, albeit at the cost of\nincreased geometric and computational detail.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 14:00:34 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Brown", "Lawrence D.", ""], ["Zhao", "Linda H.", ""]]}, {"id": "1203.4911", "submitter": "T. Tony Cai", "authors": "T. Tony Cai", "title": "Minimax and Adaptive Inference in Nonparametric Function Estimation", "comments": "Published in at http://dx.doi.org/10.1214/11-STS355 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 1, 31-50", "doi": "10.1214/11-STS355", "report-no": "IMS-STS-STS355", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Stein's 1956 seminal paper, shrinkage has played a fundamental role in\nboth parametric and nonparametric inference. This article discusses minimaxity\nand adaptive minimaxity in nonparametric function estimation. Three\ninterrelated problems, function estimation under global integrated squared\nerror, estimation under pointwise squared error, and nonparametric confidence\nintervals, are considered. Shrinkage is pivotal in the development of both the\nminimax theory and the adaptation theory. While the three problems are closely\nconnected and the minimax theories bear some similarities, the adaptation\ntheories are strikingly different. For example, in a sharp contrast to adaptive\npoint estimation, in many common settings there do not exist nonparametric\nconfidence intervals that adapt to the unknown smoothness of the underlying\nfunction. A concise account of these theories is given. The connections as well\nas differences among these problems are discussed and illustrated through\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 08:39:59 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Cai", "T. Tony", ""]]}, {"id": "1203.4935", "submitter": "George Casella", "authors": "George Casella, J. T. Gene Hwang", "title": "Shrinkage Confidence Procedures", "comments": "Published in at http://dx.doi.org/10.1214/10-STS319 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 1, 51-60", "doi": "10.1214/10-STS319", "report-no": "IMS-STS-STS319", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The possibility of improving on the usual multivariate normal confidence was\nfirst discussed in Stein (1962). Using the ideas of shrinkage, through Bayesian\nand empirical Bayesian arguments, domination results, both analytic and\nnumerical, have been obtained. Here we trace some of the developments in\nconfidence set estimation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 09:53:26 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Casella", "George", ""], ["Hwang", "J. T. Gene", ""]]}, {"id": "1203.4989", "submitter": "Dominique Fourdrinier", "authors": "Dominique Fourdrinier, Martin T. Wells", "title": "On Improved Loss Estimation for Shrinkage Estimators", "comments": "Published in at http://dx.doi.org/10.1214/11-STS380 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 1, 61-81", "doi": "10.1214/11-STS380", "report-no": "IMS-STS-STS380", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X$ be a random vector with distribution $P_{\\theta}$ where $\\theta$ is\nan unknown parameter. When estimating $\\theta$ by some estimator $\\varphi(X)$\nunder a loss function $L(\\theta,\\varphi)$, classical decision theory advocates\nthat such a decision rule should be used if it has suitable properties with\nrespect to the frequentist risk $R(\\theta,\\varphi)$. However, after having\nobserved $X=x$, instances arise in practice in which $\\varphi$ is to be\naccompanied by an assessment of its loss, $L(\\theta,\\varphi(x))$, which is\nunobservable since $\\theta$ is unknown. A common approach to this assessment is\nto consider estimation of $L(\\theta,\\varphi(x))$ by an estimator $\\delta$,\ncalled a loss estimator. We present an expository development of loss\nestimation with substantial emphasis on the setting where the distributional\ncontext is normal and its extension to the case where the underlying\ndistribution is spherically symmetric. Our overview covers improved loss\nestimators for least squares but primarily focuses on shrinkage estimators.\nBayes estimation is also considered and comparisons are made with unbiased\nestimation.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 13:46:35 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Fourdrinier", "Dominique", ""], ["Wells", "Martin T.", ""]]}, {"id": "1203.5233", "submitter": "G. Datta", "authors": "G. Datta, M. Ghosh", "title": "Small Area Shrinkage Estimation", "comments": "Published in at http://dx.doi.org/10.1214/11-STS374 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 1, 95-114", "doi": "10.1214/11-STS374", "report-no": "IMS-STS-STS374", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for small area estimates is increasingly felt in both the public and\nprivate sectors in order to formulate their strategic plans. It is now widely\nrecognized that direct small area survey estimates are highly unreliable owing\nto large standard errors and coefficients of variation. The reason behind this\nis that a survey is usually designed to achieve a specified level of accuracy\nat a higher level of geography than that of small areas. Lack of additional\nresources makes it almost imperative to use the same data to produce small area\nestimates. For example, if a survey is designed to estimate per capita income\nfor a state, the same survey data need to be used to produce similar estimates\nfor counties, subcounties and census divisions within that state. Thus, by\nnecessity, small area estimation needs explicit, or at least implicit, use of\nmodels to link these areas. Improved small area estimates are found by\n\"borrowing strength\" from similar neighboring areas.\n", "versions": [{"version": "v1", "created": "Fri, 23 Mar 2012 13:25:44 GMT"}], "update_date": "2012-03-26", "authors_parsed": [["Datta", "G.", ""], ["Ghosh", "M.", ""]]}, {"id": "1203.5346", "submitter": "Tatiana Xifara", "authors": "Chris Sherlock, Tatiana Xifara, Sandra Telfer and Mike Begon", "title": "A coupled hidden Markov model for disease interactions", "comments": "25 pages, 2 figures, To appear in Journal of the Royal Statistical\n  Society: Series C", "journal-ref": "Journal of the Royal Statistical Society: Series C, 62 (2013)", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To investigate interactions between parasite species in a host, a population\nof field voles was studied longitudinally, with presence or absence of six\ndifferent parasites measured repeatedly. Although trapping sessions were\nregular, a different set of voles was caught at each session leading to\nincomplete profiles for all subjects. We use a discrete-time hidden Markov\nmodel for each disease with transition probabilities dependent on covariates\nvia a set of logistic regressions. For each disease the hidden states for each\nof the other diseases at a given time point form part of the covariate set for\nthe Markov transition probabilities from that time point. This allows us to\ngauge the influence of each parasite species on the transition probabilities\nfor each of the other parasite species. Inference is performed via a Gibbs\nsampler, which cycles through each of the diseases, first using an adaptive\nMetropolis-Hastings step to sample from the conditional posterior of the\ncovariate parameters for that particular disease given the hidden states for\nall other diseases and then sampling from the hidden states for that disease\ngiven the parameters. We find evidence for interactions between several pairs\nof parasites and of an acquired immune response for two of the parasites.\n", "versions": [{"version": "v1", "created": "Thu, 22 Mar 2012 16:00:10 GMT"}, {"version": "v2", "created": "Sun, 27 Jan 2013 13:37:25 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Sherlock", "Chris", ""], ["Xifara", "Tatiana", ""], ["Telfer", "Sandra", ""], ["Begon", "Mike", ""]]}, {"id": "1203.5405", "submitter": "Daniel Straub Daniel Straub", "authors": "Daniel Straub", "title": "Reliability updating with equality information", "comments": null, "journal-ref": "Probabilistic Engineering Mechanics, 2011, 26(2): 254-258", "doi": "10.1016/j.probengmech.2010.08.003", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many instances, information on engineering systems can be obtained through\nmeasurements, monitoring or direct observations of system performances and can\nbe used to update the system reliability estimate. In structural reliability\nanalysis, such information is expressed either by inequalities (e.g. for the\nobservation that no defect is present) or by equalities (e.g. for quantitative\nmeasurements of system characteristics). When information Z is of the equality\ntype, the a-priori probability of Z is zero and most structural reliability\nmethods (SRM) are not directly applicable to the computation of the updated\nreliability. Hitherto, the computation of the reliability of engineering\nsystems conditional on equality information was performed through first- and\nsecond order approximations. In this paper, it is shown how equality\ninformation can be transformed into inequality information, which enables\nreliability updating by solving a standard structural system reliability\nproblem. This approach enables the use of any SRM, including those based on\nsimulation, for reliability updating with equality information. It is\ndemonstrated on three numerical examples, including an application to fatigue\nreliability.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 11:06:22 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Straub", "Daniel", ""]]}, {"id": "1203.5422", "submitter": "Jing Lei", "authors": "Jing Lei and Larry Wasserman", "title": "Distribution Free Prediction Bands", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study distribution free, nonparametric prediction bands with a special\nfocus on their finite sample behavior. First we investigate and develop\ndifferent notions of finite sample coverage guarantees. Then we give a new\nprediction band estimator by combining the idea of \"conformal prediction\" (Vovk\net al. 2009) with nonparametric conditional density estimation. The proposed\nestimator, called COPS (Conformal Optimized Prediction Set), always has finite\nsample guarantee in a stronger sense than the original conformal prediction\nestimator. Under regularity conditions the estimator converges to an oracle\nband at a minimax optimal rate. A fast approximation algorithm and a data\ndriven method for selecting the bandwidth are developed. The method is\nillustrated first in simulated data. Then, an application shows that the\nproposed method gives desirable prediction intervals in an automatic way, as\ncompared to the classical linear regression modeling.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 15:04:02 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Lei", "Jing", ""], ["Wasserman", "Larry", ""]]}, {"id": "1203.5610", "submitter": "Carl N. Morris", "authors": "Carl N. Morris, Martin Lysy", "title": "Shrinkage Estimation in Multilevel Normal Models", "comments": "Published in at http://dx.doi.org/10.1214/11-STS363 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 1, 115-134", "doi": "10.1214/11-STS363", "report-no": "IMS-STS-STS363", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This review traces the evolution of theory that started when Charles Stein in\n1955 [In Proc. 3rd Berkeley Sympos. Math. Statist. Probab. I (1956) 197--206,\nUniv. California Press] showed that using each separate sample mean from\n$k\\ge3$ Normal populations to estimate its own population mean $\\mu_i$ can be\nimproved upon uniformly for every possible $\\mu=(\\mu_1,...,\\mu_k)'$. The\ndominating estimators, referred to here as being \"Model-I minimax,\" can be\nfound by shrinking the sample means toward any constant vector. Admissible\nminimax shrinkage estimators were derived by Stein and others as posterior\nmeans based on a random effects model, \"Model-II\" here, wherein the $\\mu_i$\nvalues have their own distributions. Section 2 centers on Figure 2, which\norganizes a wide class of priors on the unknown Level-II hyperparameters that\nhave been proved to yield admissible Model-I minimax shrinkage estimators in\nthe \"equal variance case.\" Putting a flat prior on the Level-II variance is\nunique in this class for its scale-invariance and for its conjugacy, and it\ninduces Stein's harmonic prior (SHP) on $\\mu_i$.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 09:38:12 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Morris", "Carl N.", ""], ["Lysy", "Martin", ""]]}, {"id": "1203.5617", "submitter": "Edward I. George", "authors": "Edward I. George, Feng Liang, Xinyi Xu", "title": "From Minimax Shrinkage Estimation to Minimax Shrinkage Prediction", "comments": "Published in at http://dx.doi.org/10.1214/11-STS383 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 1, 82-94", "doi": "10.1214/11-STS383", "report-no": "IMS-STS-STS383", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a remarkable series of papers beginning in 1956, Charles Stein set the\nstage for the future development of minimax shrinkage estimators of a\nmultivariate normal mean under quadratic loss. More recently, parallel\ndevelopments have seen the emergence of minimax shrinkage estimators of\nmultivariate normal predictive densities under Kullback--Leibler risk. We here\ndescribe these parallels emphasizing the focus on Bayes procedures and the\nderivation of the superharmonic conditions for minimaxity as well as further\ndevelopments of new minimax shrinkage predictive density estimators including\nmultiple shrinkage estimators, empirical Bayes estimators, normal linear model\nregression estimators and nonparametric regression estimators.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 09:58:59 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["George", "Edward I.", ""], ["Liang", "Feng", ""], ["Xu", "Xinyi", ""]]}, {"id": "1203.5626", "submitter": "Michael D. Perlman", "authors": "Michael D. Perlman, Sanjay Chaudhuri", "title": "Reversing the Stein Effect", "comments": "Published in at http://dx.doi.org/10.1214/09-STS278 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 1, 135-143", "doi": "10.1214/09-STS278", "report-no": "IMS-STS-STS278", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Reverse Stein Effect is identified and illustrated: A statistician who\nshrinks his/her data toward a point chosen without reliable knowledge about the\nunderlying value of the parameter to be estimated but based instead upon the\nobserved data will not be protected by the minimax property of shrinkage\nestimators such as that of James and Stein, but instead will likely incur a\ngreater error than if shrinkage were not used.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 11:01:15 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Perlman", "Michael D.", ""], ["Chaudhuri", "Sanjay", ""]]}, {"id": "1203.5668", "submitter": "Dalene Stangl", "authors": "Dalene Stangl, Lurdes Y. T. Inoue, Telba Z. Irony", "title": "Celebrating 70: An Interview with Don Berry", "comments": "Published in at http://dx.doi.org/10.1214/11-STS366 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 1, 144-159", "doi": "10.1214/11-STS366", "report-no": "IMS-STS-STS366", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Donald (Don) Arthur Berry, born May 26, 1940 in Southbridge, Massachusetts,\nearned his A.B. degree in mathematics from Dartmouth College and his M.A. and\nPh.D. in statistics from Yale University. He served first on the faculty at the\nUniversity of Minnesota and subsequently held endowed chair positions at Duke\nUniversity and The University of Texas M.D. Anderson Center. At the time of the\ninterview he served as Head of the Division of Quantitative Sciences, and\nChairman and Professor of the Department of Biostatistics at UT M.D. Anderson\nCenter.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 13:56:43 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Stangl", "Dalene", ""], ["Inoue", "Lurdes Y. T.", ""], ["Irony", "Telba Z.", ""]]}, {"id": "1203.5829", "submitter": "Kumar Sricharan", "authors": "Kumar Sricharan, Dennis Wei, Alfred O. Hero III", "title": "Ensemble estimators for multivariate entropy estimation", "comments": "version 3: correction of minor typos from version 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of estimation of density functionals like entropy and mutual\ninformation has received much attention in the statistics and information\ntheory communities. A large class of estimators of functionals of the\nprobability density suffer from the curse of dimensionality, wherein the mean\nsquared error (MSE) decays increasingly slowly as a function of the sample size\n$T$ as the dimension $d$ of the samples increases. In particular, the rate is\noften glacially slow of order $O(T^{-{\\gamma}/{d}})$, where $\\gamma>0$ is a\nrate parameter. Examples of such estimators include kernel density estimators,\n$k$-nearest neighbor ($k$-NN) density estimators, $k$-NN entropy estimators,\nintrinsic dimension estimators and other examples. In this paper, we propose a\nweighted affine combination of an ensemble of such estimators, where optimal\nweights can be chosen such that the weighted estimator converges at a much\nfaster dimension invariant rate of $O(T^{-1})$. Furthermore, we show that these\noptimal weights can be determined by solving a convex optimization problem\nwhich can be performed offline and does not require training data. We\nillustrate the superior performance of our weighted estimator for two important\napplications: (i) estimating the Panter-Dite distortion-rate factor and (ii)\nestimating the Shannon entropy for testing the probability distribution of a\nrandom sample.\n", "versions": [{"version": "v1", "created": "Mon, 26 Mar 2012 22:08:10 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2013 21:15:27 GMT"}, {"version": "v3", "created": "Sun, 3 Mar 2013 01:27:51 GMT"}], "update_date": "2013-03-05", "authors_parsed": [["Sricharan", "Kumar", ""], ["Wei", "Dennis", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1203.5950", "submitter": "Joseph Dureau", "authors": "Joseph Dureau, Konstantinos Kalogeropoulos and Marc Baguelin", "title": "Capturing the time-varying drivers of an epidemic using stochastic\n  dynamical systems", "comments": "21 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epidemics are often modelled using non-linear dynamical systems observed\nthrough partial and noisy data. In this paper, we consider stochastic\nextensions in order to capture unknown influences (changing behaviors, public\ninterventions, seasonal effects etc). These models assign diffusion processes\nto the time-varying parameters, and our inferential procedure is based on a\nsuitably adjusted adaptive particle MCMC algorithm. The performance of the\nproposed computational methods is validated on simulated data and the adopted\nmodel is applied to the 2009 H1N1 pandemic in England. In addition to\nestimating the effective contact rate trajectories, the methodology is applied\nin real time to provide evidence in related public health decisions. Diffusion\ndriven SEIR-type models with age structure are also introduced.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 12:25:00 GMT"}, {"version": "v2", "created": "Sat, 3 Nov 2012 14:18:44 GMT"}], "update_date": "2012-11-06", "authors_parsed": [["Dureau", "Joseph", ""], ["Kalogeropoulos", "Konstantinos", ""], ["Baguelin", "Marc", ""]]}, {"id": "1203.5986", "submitter": "Daniel Straub Daniel Straub", "authors": "Daniel Straub, Armen Der Kiureghian", "title": "Bayesian Network Enhanced with Structural Reliability Methods:\n  Methodology", "comments": null, "journal-ref": "Journal of Engineering Mechanics, Trans. ASCE, 2010, 136(10):\n  1248-1258", "doi": "10.1061/(ASCE)EM.1943-7889.0000173", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine Bayesian networks (BNs) and structural reliability methods (SRMs)\nto create a new computational framework, termed enhanced Bayesian network\n(eBN), for reliability and risk analysis of engineering structures and\ninfrastructure. BNs are efficient in representing and evaluating complex\nprobabilistic dependence structures, as present in infrastructure and\nstructural systems, and they facilitate Bayesian updating of the model when new\ninformation becomes available. On the other hand, SRMs enable accurate\nassessment of probabilities of rare events represented by computationally\ndemanding, physically-based models. By combining the two methods, the eBN\nframework provides a unified and powerful tool for efficiently computing\nprobabilities of rare events in complex structural and infrastructure systems\nin which information evolves in time. Strategies for modeling and efficiently\nanalyzing the eBN are described by way of several conceptual examples. The\ncompanion paper applies the eBN methodology to example structural and\ninfrastructure systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Mar 2012 14:50:56 GMT"}], "update_date": "2012-03-28", "authors_parsed": [["Straub", "Daniel", ""], ["Der Kiureghian", "Armen", ""]]}, {"id": "1203.6216", "submitter": "Konstantinos Kalogeropoulos", "authors": "Alexandros Beskos, Konstantinos Kalogeropoulos and Erik Pazos", "title": "Advanced MCMC Methods for Sampling on Diffusion Pathspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to calibrate increasingly complex statistical models requires a\npersistent effort for further advances on available, computationally intensive\nMonte Carlo methods. We study here an advanced version of familiar Markov Chain\nMonte Carlo (MCMC) algorithms that sample from target distributions defined as\nchange of measures from Gaussian laws on general Hilbert spaces. Such a model\nstructure arises in several contexts: we focus here at the important class of\nstatistical models driven by diffusion paths whence the Wiener process\nconstitutes the reference Gaussian law. Particular emphasis is given on\nadvanced Hybrid Monte-Carlo (HMC) which makes large, derivative-driven steps in\nthe state space (in contrast with local-move Random-walk-type algorithms) with\nanalytical and experimental results. We illustrate it's computational\nadvantages in various diffusion processes and observation regimes; examples\ninclude stochastic volatility and latent survival models. In contrast with\ntheir standard MCMC counterparts, the advanced versions have mesh-free mixing\ntimes, as these will not deteriorate upon refinement of the approximation of\nthe inherently infinite-dimensional diffusion paths by finite-dimensional ones\nused in practice when applying the algorithms on a computer.\n", "versions": [{"version": "v1", "created": "Wed, 28 Mar 2012 10:25:07 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2013 17:51:06 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Beskos", "Alexandros", ""], ["Kalogeropoulos", "Konstantinos", ""], ["Pazos", "Erik", ""]]}, {"id": "1203.6431", "submitter": "Andrew Critch", "authors": "Matteo Brunelli, Andrew Critch and Michele Fedrizzi", "title": "A note on the proportionality between some consistency indices in the\n  AHP", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyzing the consistency of preferences is an important step in decision\nmaking with pairwise comparison matrices, and several indices have been\nproposed in order to estimate it. In this paper we prove the proportionality\nbetween some consistency indices in the framework of the Analytic Hierarchy\nProcess. Knowing such equivalences eliminates redundancy in the consideration\nof evidence for consistent preferences.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 05:48:44 GMT"}], "update_date": "2012-03-30", "authors_parsed": [["Brunelli", "Matteo", ""], ["Critch", "Andrew", ""], ["Fedrizzi", "Michele", ""]]}, {"id": "1203.6665", "submitter": "Ryan Martin", "authors": "Ryan Martin", "title": "Plausibility functions and exact frequentist inference", "comments": "21 pages, 5 figures, 3 tables", "journal-ref": "Journal of the American Statistical Association, volume 110, pages\n  1552--1561, 2015", "doi": "10.1080/01621459.2014.983232", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the frequentist program, inferential methods with exact control on error\nrates are a primary focus. The standard approach, however, is to rely on\nasymptotic approximations, which may not be suitable. This paper presents a\ngeneral framework for the construction of exact frequentist procedures based on\nplausibility functions. It is shown that the plausibility function-based tests\nand confidence regions have the desired frequentist properties in finite\nsamples---no large-sample justification needed. An extension of the proposed\nmethod is also given for problems involving nuisance parameters. Examples\ndemonstrate that the plausibility function-based method is both exact and\nefficient in a wide variety of problems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 20:39:47 GMT"}, {"version": "v2", "created": "Mon, 20 Aug 2012 15:27:18 GMT"}, {"version": "v3", "created": "Tue, 30 Apr 2013 15:17:08 GMT"}, {"version": "v4", "created": "Wed, 12 Mar 2014 12:46:19 GMT"}], "update_date": "2016-01-26", "authors_parsed": [["Martin", "Ryan", ""]]}]