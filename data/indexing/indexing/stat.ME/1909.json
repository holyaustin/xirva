[{"id": "1909.00002", "submitter": "Steffen Betsch", "authors": "Steffen Betsch, Bruno Ebner, Bernhard Klar", "title": "Minimum $L^q$-distance estimators for non-normalized parametric models", "comments": "27 pages, 8 tables", "journal-ref": "The Canadian Journal of Statistics, Volume 49, Issue 2, pages\n  514-548, (2021)", "doi": "10.1002/cjs.11574", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and investigate a new estimation method for the parameters of\nmodels consisting of smooth density functions on the positive half axis. The\nprocedure is based on a recently introduced characterization result for the\nrespective probability distributions, and is to be classified as a minimum\ndistance estimator, incorporating as a distance function the $L^q$-norm.\nThroughout, we deal rigorously with issues of existence and measurability of\nthese implicitly defined estimators. Moreover, we provide consistency results\nin a common asymptotic setting, and compare our new method with classical\nestimators for the exponential-, the Rayleigh-, and the Burr Type XII\ndistribution in Monte Carlo simulation studies. We also assess the performance\nof different estimators for non-normalized models in the context of an\nexponential-polynomial family.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 15:19:28 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 10:22:14 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Betsch", "Steffen", ""], ["Ebner", "Bruno", ""], ["Klar", "Bernhard", ""]]}, {"id": "1909.00066", "submitter": "Amanda Coston", "authors": "Amanda Coston, Alan Mishler, Edward H. Kennedy, Alexandra Chouldechova", "title": "Counterfactual Risk Assessments, Evaluation, and Fairness", "comments": "To appear in ACM FAT* 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CY cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic risk assessments are increasingly used to help humans make\ndecisions in high-stakes settings, such as medicine, criminal justice and\neducation. In each of these cases, the purpose of the risk assessment tool is\nto inform actions, such as medical treatments or release conditions, often with\nthe aim of reducing the likelihood of an adverse event such as hospital\nreadmission or recidivism. Problematically, most tools are trained and\nevaluated on historical data in which the outcomes observed depend on the\nhistorical decision-making policy. These tools thus reflect risk under the\nhistorical policy, rather than under the different decision options that the\ntool is intended to inform. Even when tools are constructed to predict risk\nunder a specific decision, they are often improperly evaluated as predictors of\nthe target outcome.\n  Focusing on the evaluation task, in this paper we define counterfactual\nanalogues of common predictive performance and algorithmic fairness metrics\nthat we argue are better suited for the decision-making context. We introduce a\nnew method for estimating the proposed metrics using doubly robust estimation.\nWe provide theoretical results that show that only under strong conditions can\nfairness according to the standard metric and the counterfactual metric\nsimultaneously hold. Consequently, fairness-promoting methods that target\nparity in a standard fairness metric may --- and as we show empirically, do ---\ninduce greater imbalance in the counterfactual analogue. We provide empirical\ncomparisons on both synthetic data and a real world child welfare dataset to\ndemonstrate how the proposed method improves upon standard practice.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 20:47:20 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2019 15:15:16 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 14:08:46 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Coston", "Amanda", ""], ["Mishler", "Alan", ""], ["Kennedy", "Edward H.", ""], ["Chouldechova", "Alexandra", ""]]}, {"id": "1909.00072", "submitter": "Eshan Mitra", "authors": "Eshan D. Mitra, William S. Hlavacek", "title": "Bayesian Uncertainty Quantification for Systems Biology Models\n  Parameterized Using Qualitative Data", "comments": "19 pages, 4 figures, 11 supplementary figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Recent work has demonstrated the feasibility of using\nnon-numerical, qualitative data to parameterize mathematical models. However,\nuncertainty quantification (UQ) of such parameterized models has remained\nchallenging because of a lack of a statistical interpretation of the objective\nfunctions used in optimization. Results: We formulated likelihood functions\nsuitable for performing Bayesian UQ using qualitative data or a combination of\nqualitative and quantitative data. To demonstrate the resulting UQ\ncapabilities, we analyzed a published model for IgE receptor signaling using\nsynthetic qualitative and quantitative datasets. Remarkably, estimates of\nparameter values derived from the qualitative data were nearly as consistent\nwith the assumed ground-truth parameter values as estimates derived from the\nlower throughput quantitative data. These results provide further motivation\nfor leveraging qualitative data in biological modeling. Availability: The\nlikelihood functions presented here are implemented in a new release of\nPyBioNetFit, an open-source application for analyzing SBML- and BNGL-formatted\nmodels, available online at www.github.com/lanl/PyBNF.\n", "versions": [{"version": "v1", "created": "Fri, 30 Aug 2019 21:09:55 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Mitra", "Eshan D.", ""], ["Hlavacek", "William S.", ""]]}, {"id": "1909.00221", "submitter": "Feng Li", "authors": "Yanfei Kang, Evangelos Spiliotis, Fotios Petropoulos, Nikolaos\n  Athiniotis, Feng Li, Vassilios Assimakopoulos", "title": "D\\'ej\\`a vu: A data-centric forecasting approach through time series\n  cross-similarity", "comments": null, "journal-ref": "Journal of Business Research (2020)", "doi": "10.1016/j.jbusres.2020.10.051", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate forecasts are vital for supporting the decisions of modern\ncompanies. Forecasters typically select the most appropriate statistical model\nfor each time series. However, statistical models usually presume some data\ngeneration process while making strong assumptions about the errors. In this\npaper, we present a novel data-centric approach -- `forecasting with\nsimilarity', which tackles model uncertainty in a model-free manner. Existing\nsimilarity-based methods focus on identifying similar patterns within the\nseries, i.e., `self-similarity'. In contrast, we propose searching for similar\npatterns from a reference set, i.e., `cross-similarity'. Instead of\nextrapolating, the future paths of the similar series are aggregated to obtain\nthe forecasts of the target series. Building on the cross-learning concept, our\napproach allows the application of similarity-based forecasting on series with\nlimited lengths. We evaluate the approach using a rich collection of real data\nand show that it yields competitive accuracy in both points forecasts and\nprediction intervals.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 14:14:33 GMT"}, {"version": "v2", "created": "Mon, 11 Nov 2019 15:19:46 GMT"}, {"version": "v3", "created": "Fri, 4 Sep 2020 05:10:38 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Kang", "Yanfei", ""], ["Spiliotis", "Evangelos", ""], ["Petropoulos", "Fotios", ""], ["Athiniotis", "Nikolaos", ""], ["Li", "Feng", ""], ["Assimakopoulos", "Vassilios", ""]]}, {"id": "1909.00244", "submitter": "Georgia Papacharalampous", "authors": "Georgia Papacharalampous, Demetris Koutsoyiannis, Alberto Montanari", "title": "Quantification of predictive uncertainty in hydrological modelling by\n  harnessing the wisdom of the crowd: Methodology development and investigation\n  using toy models", "comments": null, "journal-ref": "Advances in Water Resources 136 (2020) 103471", "doi": "10.1016/j.advwatres.2019.103471", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an ensemble learning post-processing methodology for\nprobabilistic hydrological modelling. This methodology generates numerous point\npredictions by applying a single hydrological model, yet with different\nparameter values drawn from the respective simulated posterior distribution. We\ncall these predictions \"sister predictions\". Each sister prediction extending\nin the period of interest is converted into a probabilistic prediction using\ninformation about the hydrological model's errors. This information is obtained\nfrom a preceding period for which observations are available, and is exploited\nusing a flexible quantile regression model. All probabilistic predictions are\nfinally combined via simple quantile averaging to produce the output\nprobabilistic prediction. The idea is inspired by the ensemble learning methods\noriginating from the machine learning literature. The proposed methodology\noffers larger robustness in performance than basic post-processing\nmethodologies using a single hydrological point prediction. It is also\nempirically proven to \"harness the wisdom of the crowd\" in terms of average\ninterval score, i.e., the obtained quantile predictions score no worse --\nusually better -- than the average score of the combined individual\npredictions. This proof is provided within toy examples, which can be used for\ngaining insight on how the methodology works and under which conditions it can\noptimally convert point hydrological predictions to probabilistic ones. A\nlarge-scale hydrological application is made in a companion paper.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 17:15:18 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 23:50:05 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Papacharalampous", "Georgia", ""], ["Koutsoyiannis", "Demetris", ""], ["Montanari", "Alberto", ""]]}, {"id": "1909.00247", "submitter": "Georgia Papacharalampous", "authors": "Georgia Papacharalampous, Hristos Tyralis, Demetris Koutsoyiannis,\n  Alberto Montanari", "title": "Quantification of predictive uncertainty in hydrological modelling by\n  harnessing the wisdom of the crowd: A large-sample experiment at monthly\n  timescale", "comments": null, "journal-ref": "Advances in Water Resources 136 (2020) 103470", "doi": "10.1016/j.advwatres.2019.103470", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Predictive hydrological uncertainty can be quantified by using ensemble\nmethods. If properly formulated, these methods can offer improved predictive\nperformance by combining multiple predictions. In this work, we use\n50-year-long monthly time series observed in 270 catchments in the United\nStates to explore the performances provided by an ensemble learning\npost-processing methodology for issuing probabilistic hydrological predictions.\nThis methodology allows the utilization of flexible quantile regression models\nfor exploiting information about the hydrological model's error. Its key\ndifferences with respect to basic two-stage hydrological post-processing\nmethodologies using the same type of regression models are that (a) instead of\na single point hydrological prediction it generates a large number of \"sister\npredictions\" (yet using a single hydrological model), and that (b) it relies on\nthe concept of combining probabilistic predictions via simple quantile\naveraging. A major hydrological modelling challenge is obtaining probabilistic\npredictions that are simultaneously reliable and associated to prediction bands\nthat are as narrow as possible; therefore, we assess both these desired\nproperties of the predictions by computing their coverage probabilities,\naverage widths and average interval scores. The results confirm the usefulness\nof the proposed methodology and its larger robustness with respect to basic\ntwo-stage post-processing methodologies. Finally, this methodology is\nempirically proven to harness the \"wisdom of the crowd\" in terms of average\ninterval score, i.e., the average of the individual predictions combined by\nthis methodology scores no worse -- usually better -- than the average of the\nscores of the individual predictions.\n", "versions": [{"version": "v1", "created": "Sat, 31 Aug 2019 17:37:36 GMT"}, {"version": "v2", "created": "Fri, 3 Jan 2020 23:43:39 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Papacharalampous", "Georgia", ""], ["Tyralis", "Hristos", ""], ["Koutsoyiannis", "Demetris", ""], ["Montanari", "Alberto", ""]]}, {"id": "1909.00320", "submitter": "Hwiyoung Lee", "authors": "Hwiyoung Lee, Vic Patrangenaru", "title": "Anti-MANOVA on Compact Manifolds with Applications to 3D Projective\n  Shape Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of hypotheses testing for equality of extrinsic antimeans on compact\nmanifolds are unveiled in this paper. The two and multiple sample problem for\nantimeans on compact manifolds is addressed for large samples via asymptotic\ndistributions, as well as for small samples using nonparametric bootstrap. An\nexample of face differentiation using 3D VW antimean projective shape analysis\nfor data extracted from digital camera images is also given.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 04:14:50 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Lee", "Hwiyoung", ""], ["Patrangenaru", "Vic", ""]]}, {"id": "1909.00386", "submitter": "Du Nguyen", "authors": "Du Nguyen", "title": "Vector Autoregressive Moving Average Model with Scalar Moving Average", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show Vector Autoregressive Moving Average models with scalar Moving\nAverage components could be estimated by generalized least square (GLS) for\neach fixed moving average polynomial. The conditional variance of the GLS model\nis the concentrated covariant matrix of the moving average process. Under GLS\nthe likelihood function of these models has similar format to their VAR\ncounterparts. Maximum likelihood estimate can be done by optimizing with\ngradient over the moving average parameters. These models are inexpensive\ngeneralizations of Vector Autoregressive models. We discuss a relationship\nbetween this result and the Borodin-Okounkov formula in operator theory.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 12:02:10 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Nguyen", "Du", ""]]}, {"id": "1909.00456", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Dynamic principal component regression for forecasting functional time\n  series in a group structure", "comments": "19 pages, 7 figures, to appear in Scandinavian Actuarial Journal.\n  arXiv admin note: text overlap with arXiv:1705.08001, arXiv:1609.04222", "journal-ref": "Scandinavian Actuarial Journal, 2020, 2020(4), 307-322", "doi": "10.1080/03461238.2019.1663553", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When generating social policies and pricing annuity at national and\nsubnational levels, it is essential both to forecast mortality accurately and\nensure that forecasts at the subnational level add up to the forecasts at the\nnational level. This has motivated recent developments in forecasting\nfunctional time series in a group structure, where static principal component\nanalysis is used. In the presence of moderate to strong temporal dependence,\nstatic principal component analysis designed for independent and identically\ndistributed functional data may be inadequate. Thus, through using the dynamic\nfunctional principal component analysis, we consider a functional time series\nforecasting method with static and dynamic principal component regression to\nforecast each series in a group structure. Through using the regional\nage-specific mortality rates in Japan obtained from the Japanese Mortality\nDatabase (2019), we investigate the point and interval forecast accuracies of\nour proposed extension, and subsequently make recommendations.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 19:24:08 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1909.00472", "submitter": "Kathryn Turnbull", "authors": "Kathryn Turnbull, Sim\\'on Lunag\\'omez, Christopher Nemeth, Edoardo\n  Airoldi", "title": "Latent Space Modelling of Hypergraph Data", "comments": "44 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing prevalence of relational data describing interactions among a\ntarget population has motivated a wide literature on statistical network\nanalysis. In many applications, interactions may involve more than two members\nof the population and this data is more appropriately represented by a\nhypergraph. In this paper, we present a model for hypergraph data which extends\nthe latent space distance model of Hoff et al. (2002) and, by drawing a\nconnection to constructs from computational topology, we develop a model whose\nlikelihood is inexpensive to compute. We obtain posterior samples via an MCMC\nscheme and we rely on Bookstein coordinates to remove the identifiability\nissues associated with the latent representation. We demonstrate that the\nlatent space construction imposes desirable properties on the hypergraphs\ngenerated in our framework and provides a convenient visualisation of the data.\nFurthermore, through simulation, we investigate the flexibility of our model\nand consider estimating predictive distributions. Finally, we explore the\napplication of our model to two real world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 1 Sep 2019 20:53:07 GMT"}, {"version": "v2", "created": "Thu, 12 Dec 2019 18:20:56 GMT"}], "update_date": "2019-12-13", "authors_parsed": [["Turnbull", "Kathryn", ""], ["Lunag\u00f3mez", "Sim\u00f3n", ""], ["Nemeth", "Christopher", ""], ["Airoldi", "Edoardo", ""]]}, {"id": "1909.00515", "submitter": "Gauri Kamat", "authors": "Tanujit Chakraborty, Gauri Kamat, and Ashis Kumar Chakraborty", "title": "Bayesian Neural Tree Models for Nonparametric Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequentist and Bayesian methods differ in many aspects, but share some basic\noptimal properties. In real-life classification and regression problems,\nsituations exist in which a model based on one of the methods is preferable\nbased on some subjective criterion. Nonparametric classification and regression\ntechniques, such as decision trees and neural networks, have frequentist\n(classification and regression trees (CART) and artificial neural networks) as\nwell as Bayesian (Bayesian CART and Bayesian neural networks) approaches to\nlearning from data. In this work, we present two hybrid models combining the\nBayesian and frequentist versions of CART and neural networks, which we call\nthe Bayesian neural tree (BNT) models. Both models exploit the architecture of\ndecision trees and have lesser number of parameters to tune than advanced\nneural networks. Such models can simultaneously perform feature selection and\nprediction, are highly flexible, and generalize well in settings with a limited\nnumber of training observations. We study the consistency of the proposed\nmodels, and derive the optimal value of an important model parameter. We also\nprovide illustrative examples using a wide variety of real-life regression data\nsets.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 02:25:18 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2020 19:24:03 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Chakraborty", "Tanujit", ""], ["Kamat", "Gauri", ""], ["Chakraborty", "Ashis Kumar", ""]]}, {"id": "1909.00721", "submitter": "Nicolas Jouvin", "authors": "Nicolas Jouvin (1 and 2), Pierre Latouche (2), Charles Bouveyron (3),\n  Guillaume Bataillon (4), Alain Livartowski (4) ((1) Laboratoire SAMM EA 4543,\n  (2) Laboratoire MAP5 UMR 8145, (3) Laboratoire J.A. Dieudonn\\'e UMR 7351 (4)\n  Institut Curie)", "title": "Greedy clustering of count data through a mixture of multinomial PCA", "comments": "34 pages, 11 figures, published in : Computational Statistics", "journal-ref": null, "doi": "10.1007/s00180-020-01008-9", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Count data is becoming more and more ubiquitous in a wide range of\napplications, with datasets growing both in size and in dimension. In this\ncontext, an increasing amount of work is dedicated to the construction of\nstatistical models directly accounting for the discrete nature of the data.\nMoreover, it has been shown that integrating dimension reduction to clustering\ncan drastically improve performance and stability. In this paper, we rely on\nthe mixture of multinomial PCA, a mixture model for the clustering of count\ndata, also known as the probabilistic clustering-projection model in the\nliterature. Related to the latent Dirichlet allocation model, it offers the\nflexibility of topic modeling while being able to assign each observation to a\nunique cluster. We introduce a greedy clustering algorithm, where inference and\nclustering are jointly done by mixing a classification variational expectation\nmaximization algorithm, with a branch & bound like strategy on a variational\nlower bound. An integrated classification likelihood criterion is derived for\nmodel selection, and a thorough study with numerical experiments is proposed to\nassess both the performance and robustness of the method. Finally, we\nillustrate the qualitative interest of the latter in a real-world application,\nfor the clustering of anatomopathological medical reports, in partnership with\nexpert practitioners from the Institut Curie hospital.\n", "versions": [{"version": "v1", "created": "Mon, 2 Sep 2019 13:56:09 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2020 11:56:08 GMT"}, {"version": "v3", "created": "Fri, 10 Jul 2020 06:31:22 GMT"}], "update_date": "2020-07-13", "authors_parsed": [["Jouvin", "Nicolas", "", "1 and 2"], ["Latouche", "Pierre", ""], ["Bouveyron", "Charles", ""], ["Bataillon", "Guillaume", ""], ["Livartowski", "Alain", ""]]}, {"id": "1909.01035", "submitter": "Wendy Harrison", "authors": "Wendy J. Harrison (1 and 2), Paul D. Baxter (2) and Mark S. Gilthorpe\n  (1, 2 and 3) ((1) Leeds Institute for Data Analytics, University of Leeds,\n  Leeds, UK, (2) School of Medicine, University of Leeds, Leeds, UK, (3) The\n  Alan Turing Institute, London, UK)", "title": "Multilevel latent class (MLC) modelling of healthcare provider causal\n  effects on patient outcomes: Evaluation via simulation", "comments": "19 pages, 5 figures. Abstract to be published in the conference\n  proceedings for the Society for Social Medicine & Population Health and\n  International Epidemiology Association European Congress Joint Annual\n  Scientific Meeting, September 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Where performance comparison of healthcare providers is of interest,\ncharacteristics of both patients and the health condition of interest must be\nbalanced across providers for a fair comparison. This is unlikely to be\nfeasible within observational data, as patient population characteristics may\nvary geographically and patient care may vary by characteristics of the health\ncondition. We simulated data for patients and providers, based on a previously\nutilized real-world dataset, and separately considered both binary and\ncontinuous covariate-effects at the upper level. Multilevel latent class (MLC)\nmodelling is proposed to partition a prediction focus at the patient level\n(accommodating casemix) and a causal inference focus at the provider level. The\nMLC model recovered a range of simulated Trust-level effects. Median recovered\nvalues were almost identical to simulated values for the binary Trust-level\ncovariate, and we observed successful recovery of the continuous Trust-level\ncovariate with at least 3 latent Trust classes. Credible intervals widen as the\nerror variance increases. The MLC approach successfully partitioned modelling\nfor prediction and for causal inference, addressing the potential conflict\nbetween these two distinct analytical strategies. This improves upon strategies\nwhich only adjust for differential selection. Patient-level variation and\nmeasurement uncertainty are accommodated within the latent classes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 10:14:42 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Harrison", "Wendy J.", "", "1 and 2"], ["Baxter", "Paul D.", "", "1, 2 and 3"], ["Gilthorpe", "Mark S.", "", "1, 2 and 3"]]}, {"id": "1909.01062", "submitter": "Irene C\\'ordoba", "authors": "Irene C\\'ordoba, Gherardo Varando, Concha Bielza, Pedro Larra\\~naga", "title": "On generating random Gaussian graphical models", "comments": "Improved figures, algorithm descriptions and text exposition. arXiv\n  admin note: substantial text overlap with arXiv:1807.03090", "journal-ref": "International Journal of Approximate Reasoning, 125:240-250, 2020", "doi": "10.1016/j.ijar.2020.07.007", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure learning methods for covariance and concentration graphs are often\nvalidated on synthetic models, usually obtained by randomly generating: (i) an\nundirected graph, and (ii) a compatible symmetric positive definite (SPD)\nmatrix. In order to ensure positive definiteness in (ii), a dominant diagonal\nis usually imposed. In this work we investigate different methods to generate\nrandom symmetric positive definite matrices with undirected graphical\nconstraints. We show that if the graph is chordal it is possible to sample\nuniformly from the set of correlation matrices compatible with the graph, while\nfor general undirected graphs we rely on a partial orthogonalization method.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 11:03:22 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2020 08:27:02 GMT"}], "update_date": "2020-08-20", "authors_parsed": [["C\u00f3rdoba", "Irene", ""], ["Varando", "Gherardo", ""], ["Bielza", "Concha", ""], ["Larra\u00f1aga", "Pedro", ""]]}, {"id": "1909.01273", "submitter": "Trevor Harris", "authors": "Trevor Harris, Bo Li, Nathan Steiger, Jason Smerdon, Naveen Narisetty,\n  Derek Tucker", "title": "Evaluating proxy influence in assimilated paleoclimate reconstructions\n  -- Testing the exchangeability of two ensembles of spatial processes", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1799810", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Climate field reconstructions (CFR) attempt to estimate spatiotemporal fields\nof climate variables in the past using climate proxies such as tree rings, ice\ncores, and corals. Data Assimilation (DA) methods are a recent and promising\nnew means of deriving CFRs that optimally fuse climate proxies with climate\nmodel output. Despite the growing application of DA-based CFRs, little is\nunderstood about how much the assimilated proxies change the statistical\nproperties of the climate model data. To address this question, we propose a\nrobust and computationally efficient method, based on functional data depth, to\nevaluate differences in the distributions of two spatiotemporal processes. We\napply our test to study global and regional proxy influence in DA-based CFRs by\ncomparing the background and analysis states, which are treated as two samples\nof spatiotemporal fields. We find that the analysis states are significantly\naltered from the climate-model-based background states due to the assimilation\nof proxies. Moreover, the difference between the analysis and background states\nincreases with the number of proxies, even in regions far beyond proxy\ncollection sites. Our approach allows us to characterize the added value of\nproxies, indicating where and when the analysis states are distinct from the\nbackground states.\n", "versions": [{"version": "v1", "created": "Tue, 3 Sep 2019 16:12:35 GMT"}, {"version": "v2", "created": "Wed, 5 Aug 2020 00:46:07 GMT"}], "update_date": "2020-08-06", "authors_parsed": [["Harris", "Trevor", ""], ["Li", "Bo", ""], ["Steiger", "Nathan", ""], ["Smerdon", "Jason", ""], ["Narisetty", "Naveen", ""], ["Tucker", "Derek", ""]]}, {"id": "1909.01503", "submitter": "Claude Renaux", "authors": "Zijian Guo, Claude Renaux, Peter B\\\"uhlmann and T. Tony Cai", "title": "Group Inference in High Dimensions with Applications to Hierarchical\n  Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional group inference is an essential part of statistical methods\nfor analysing complex data sets, including hierarchical testing, tests of\ninteraction, detection of heterogeneous treatment effects and inference for\nlocal heritability. Group inference in regression models can be measured with\nrespect to a weighted quadratic functional of the regression sub-vector\ncorresponding to the group. Asymptotically unbiased estimators of these\nweighted quadratic functionals are constructed and a novel procedure using\nthese estimators for inference is proposed. We derive its asymptotic Gaussian\ndistribution which enables the construction of asymptotically valid confidence\nintervals and tests which perform well in terms of length or power. The\nproposed test is computationally efficient even for a large group,\nstatistically valid for any group size and achieving good power performance for\ntesting large groups with many small regression coefficients. We apply the\nmethodology to several interesting statistical problems and demonstrate its\nstrength and usefulness on simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 00:25:09 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2020 17:52:57 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2020 11:17:35 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Guo", "Zijian", ""], ["Renaux", "Claude", ""], ["B\u00fchlmann", "Peter", ""], ["Cai", "T. Tony", ""]]}, {"id": "1909.01637", "submitter": "Janet van Niekerk Dr", "authors": "Janet van Niekerk, Haakon Bakka, Haavard Rue", "title": "Competing risks joint models using R-INLA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The methodological advancements made in the field of joint models are\nnumerous. None the less, the case of competing risks joint models have largely\nbeen neglected, especially from a practitioner's point of view. In the relevant\nworks on competing risks joint models, the assumptions of Gaussian linear\nlongitudinal series and proportional cause-specific hazard functions, amongst\nothers, have remained unchallenged. In this paper, we provide a framework based\non R-INLA to apply competing risks joint models in a unifying way such that\nnon-Gaussian longitudinal data, spatial structures, time dependent splines and\nvarious latent association structures, to mention a few, are all embraced in\nour approach. Our motivation stems from the SANAD trial which exhibits\nnon-linear longitudinal trajectories and competing risks for failure of\ntreatment. We also present a discrete competing risks joint model for\nlongitudinal count data as well as a spatial competing risks joint model, as\nspecific examples.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 09:16:43 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["van Niekerk", "Janet", ""], ["Bakka", "Haakon", ""], ["Rue", "Haavard", ""]]}, {"id": "1909.01675", "submitter": "Tatiana Komarova", "authors": "Tatiana Komarova and Javier Hidalgo", "title": "Testing nonparametric shape restrictions", "comments": "62 pages, 6 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and examine a test for a general class of shape constraints, such\nas constraints on the signs of derivatives, U-(S-)shape, symmetry,\nquasi-convexity, log-convexity, $r$-convexity, among others, in a nonparametric\nframework using partial sums empirical processes. We show that, after a\nsuitable transformation, its asymptotic distribution is a functional of the\nstandard Brownian motion, so that critical values are available. However, due\nto the possible poor approximation of the asymptotic critical values to the\nfinite sample ones, we also describe a valid bootstrap algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 10:13:14 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 15:21:09 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Komarova", "Tatiana", ""], ["Hidalgo", "Javier", ""]]}, {"id": "1909.01691", "submitter": "Alexander Fisch", "authors": "Alexander T M Fisch, Idris A Eckley, Paul Fearnhead", "title": "Subset Multivariate Collective And Point Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a growing interest in identifying anomalous\nstructure within multivariate data streams. We consider the problem of\ndetecting collective anomalies, corresponding to intervals where one or more of\nthe data streams behaves anomalously. We first develop a test for a single\ncollective anomaly that has power to simultaneously detect anomalies that are\neither rare, that is affecting few data streams, or common. We then show how to\ndetect multiple anomalies in a way that is computationally efficient but avoids\nthe approximations inherent in binary segmentation-like approaches. This\napproach, which we call MVCAPA, is shown to consistently estimate the number\nand location of the collective anomalies, a property that has not previously\nbeen shown for competing methods. MVCAPA can be made robust to point anomalies\nand can allow for the anomalies to be imperfectly aligned. We show the\npractical usefulness of allowing for imperfect alignments through a resulting\nincrease in power to detect regions of copy number variation.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 10:58:46 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Fisch", "Alexander T M", ""], ["Eckley", "Idris A", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1909.01836", "submitter": "Pulong Ma", "authors": "Pulong Ma, Georgios Karagiannis, Bledar A. Konomi, Taylor G. Asher,\n  Gabriel R. Toro, Andrew T. Cox", "title": "Multifidelity Computer Model Emulation with High-Dimensional Output: An\n  Application to Storm Surge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hurricane-driven storm surge is one of the most deadly and costly natural\ndisasters, making precise quantification of the surge hazard of great\nimportance. Inference of such systems is done through physics-based computer\nmodels of the process. Such surge simulators can be implemented with a wide\nrange of fidelity levels, with computational burdens varying by several orders\nof magnitude due to the nature of the system. The danger posed by surge makes\ngreater fidelity highly desirable, however such models and their high-volume\noutput tend to come at great computational cost, which can make detailed study\nof coastal flood hazards prohibitive. These needs make the development of an\nemulator combining high-dimensional output from multiple complex computer\nmodels with different fidelity levels important. We propose a parallel partial\nautoregressive cokriging model to predict highly-accurate storm surges in a\ncomputationally efficient way over a large spatial domain. This emulator has\nthe capability of predicting storm surges as accurately as a high-fidelity\ncomputer model given any storm characteristics and allows accurate assessment\nof the hazards from storm surges over a large spatial domain.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 14:33:23 GMT"}, {"version": "v2", "created": "Mon, 30 Sep 2019 18:12:53 GMT"}, {"version": "v3", "created": "Mon, 30 Mar 2020 18:45:34 GMT"}, {"version": "v4", "created": "Wed, 1 Apr 2020 19:27:51 GMT"}, {"version": "v5", "created": "Wed, 29 Apr 2020 15:56:24 GMT"}, {"version": "v6", "created": "Tue, 5 May 2020 18:27:22 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Ma", "Pulong", ""], ["Karagiannis", "Georgios", ""], ["Konomi", "Bledar A.", ""], ["Asher", "Taylor G.", ""], ["Toro", "Gabriel R.", ""], ["Cox", "Andrew T.", ""]]}, {"id": "1909.01848", "submitter": "Daniel Malinsky", "authors": "Daniel Malinsky, Ilya Shpitser, Eric J Tchetgen Tchetgen", "title": "Semiparametric Inference for Non-monotone Missing-Not-at-Random Data:\n  the No Self-Censoring Model", "comments": "50 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the identification and estimation of statistical functionals of\nmultivariate data missing non-monotonically and not-at-random, taking a\nsemiparametric approach. Specifically, we assume that the missingness mechanism\nsatisfies what has been previously called \"no self-censoring\" or \"itemwise\nconditionally independent nonresponse,\" which roughly corresponds to the\nassumption that no partially-observed variable directly determines its own\nmissingness status. We show that this assumption, combined with an odds ratio\nparameterization of the joint density, enables identification of functionals of\ninterest, and we establish the semiparametric efficiency bound for the\nnonparametric model satisfying this assumption. We propose a practical\naugmented inverse probability weighted estimator, and in the setting with a\n(possibly high-dimensional) always-observed subset of covariates, our proposed\nestimator enjoys a certain double-robustness property. We explore the\nperformance of our estimator with simulation experiments and on a\npreviously-studied data set of HIV-positive mothers in Botswana.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 14:47:28 GMT"}, {"version": "v2", "created": "Tue, 5 May 2020 17:48:42 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2020 23:08:16 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Malinsky", "Daniel", ""], ["Shpitser", "Ilya", ""], ["Tchetgen", "Eric J Tchetgen", ""]]}, {"id": "1909.01930", "submitter": "Yujia Li", "authors": "Yujia Li, Xiangrui Zeng, Chien-Wei Lin and George Tseng", "title": "Simultaneous Estimation of Number of Clusters and Feature Sparsity in\n  Clustering High-Dimensional Data", "comments": "The earlier version won 2019 ENAR distinguished student paper award", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the number of clusters (K) is a critical and often difficult task\nin cluster analysis. Many methods have been proposed to estimate K, including\nsome top performers using resampling approach. When performing cluster analysis\nin high-dimensional data, simultaneous clustering and feature selection is\nneeded for improved interpretation and performance. To our knowledge, none has\ninvestigated simultaneous estimation of K and feature selection in an\nexploratory cluster analysis. In this paper, we propose a resampling method to\nmeet this gap and evaluate its performance under the sparse K-means clustering\nframework. The proposed target function balances between sensitivity and\nspecificity of clustering evaluation of pairwise subjects from clustering of\nfull and subsampled data. Through extensive simulations, the method performs\namong the best over classical methods in estimating K in low-dimensional data.\nFor high-dimensional simulation data, it also shows superior performance to\nsimultaneously estimate K and feature sparsity parameter. Finally, we evaluated\nthe methods in four microarray, two RNA-seq, one SNP and two non-omics\ndatasets. The proposed method achieves better clustering accuracy with fewer\nselected predictive genes in almost all real applications.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 16:36:34 GMT"}], "update_date": "2019-09-05", "authors_parsed": [["Li", "Yujia", ""], ["Zeng", "Xiangrui", ""], ["Lin", "Chien-Wei", ""], ["Tseng", "George", ""]]}, {"id": "1909.02058", "submitter": "Christine Peterson", "authors": "Elin Shaddox, Christine B. Peterson, Francesco C. Stingo, Nicola A.\n  Hanania, Charmion Cruickshank-Quinn, Katerina Kechris, Russell Bowler, and\n  Marina Vannucci", "title": "Bayesian Inference of Networks Across Multiple Sample Groups and Data\n  Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a graphical modeling framework for the inference of\nnetworks across multiple sample groups and data types. In medical studies, this\nsetting arises whenever a set of subjects, which may be heterogeneous due to\ndiffering disease stage or subtype, is profiled across multiple platforms, such\nas metabolomics, proteomics, or transcriptomics data. Our proposed Bayesian\nhierarchical model first links the network structures within each platform\nusing a Markov random field prior to relate edge selection across sample\ngroups, and then links the network similarity parameters across platforms. This\nenables joint estimation in a flexible manner, as we make no assumptions on the\ndirectionality of influence across the data types or the extent of network\nsimilarity across the sample groups and platforms. In addition, our model\nformulation allows the number of variables and number of subjects to differ\nacross the data types, and only requires that we have data for the same set of\ngroups. We illustrate the proposed approach through both simulation studies and\nan application to gene expression levels and metabolite abundances on subjects\nwith varying severity levels of Chronic Obstructive Pulmonary Disease (COPD).\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 19:06:46 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Shaddox", "Elin", ""], ["Peterson", "Christine B.", ""], ["Stingo", "Francesco C.", ""], ["Hanania", "Nicola A.", ""], ["Cruickshank-Quinn", "Charmion", ""], ["Kechris", "Katerina", ""], ["Bowler", "Russell", ""], ["Vannucci", "Marina", ""]]}, {"id": "1909.02139", "submitter": "Hyo Young Choi", "authors": "Hyo Young Choi and J. S. Marron", "title": "Theory of high-dimensional outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study concerns the issue of high dimensional outliers which are\nchallenging to distinguish from inliers due to the special structure of high\ndimensional space. We introduce a new notion of high dimensional outliers that\nembraces various types and provides deep insights into understanding the\nbehavior of these outliers based on several asymptotic regimes. Our study of\ngeometrical properties of high dimensional outliers reveals an interesting\ntransition phenomenon of outliers from near the surface of a high dimensional\nsphere to being distant from the sphere. Also, we study the PCA subspace\nconsistency when data contain a limited number of outliers.\n", "versions": [{"version": "v1", "created": "Wed, 4 Sep 2019 22:31:45 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Choi", "Hyo Young", ""], ["Marron", "J. S.", ""]]}, {"id": "1909.02182", "submitter": "Anne-Sophie Krah", "authors": "Anne-Sophie Krah, Zoran Nikoli\\'c, Ralf Korn", "title": "Machine Learning in Least-Squares Monte Carlo Proxy Modeling of Life\n  Insurance Companies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the Solvency II regime, life insurance companies are asked to derive\ntheir solvency capital requirements from the full loss distributions over the\ncoming year. Since the industry is currently far from being endowed with\nsufficient computational capacities to fully simulate these distributions, the\ninsurers have to rely on suitable approximation techniques such as the\nleast-squares Monte Carlo (LSMC) method. The key idea of LSMC is to run only a\nfew wisely selected simulations and to process their output further to obtain a\nrisk-dependent proxy function of the loss. In this paper, we present and\nanalyze various adaptive machine learning approaches that can take over the\nproxy modeling task. The studied approaches range from ordinary and generalized\nleast-squares regression variants over GLM and GAM methods to MARS and kernel\nregression routines. We justify the combinability of their regression\ningredients in a theoretical discourse. Further, we illustrate the approaches\nin slightly disguised real-world experiments and perform comprehensive\nout-of-sample tests.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 02:02:26 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Krah", "Anne-Sophie", ""], ["Nikoli\u0107", "Zoran", ""], ["Korn", "Ralf", ""]]}, {"id": "1909.02210", "submitter": "Guido Imbens", "authors": "Susan Athey, Guido Imbens, Jonas Metzger, Evan Munro", "title": "Using Wasserstein Generative Adversarial Networks for the Design of\n  Monte Carlo Simulations", "comments": "30 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When researchers develop new econometric methods it is common practice to\ncompare the performance of the new methods to those of existing methods in\nMonte Carlo studies. The credibility of such Monte Carlo studies is often\nlimited because of the freedom the researcher has in choosing the design. In\nrecent years a new class of generative models emerged in the machine learning\nliterature, termed Generative Adversarial Networks (GANs) that can be used to\nsystematically generate artificial data that closely mimics real economic\ndatasets, while limiting the degrees of freedom for the researcher and\noptionally satisfying privacy guarantees with respect to their training data.\nIn addition if an applied researcher is concerned with the performance of a\nparticular statistical method on a specific data set (beyond its theoretical\nproperties in large samples), she may wish to assess the performance, e.g., the\ncoverage rate of confidence intervals or the bias of the estimator, using\nsimulated data which resembles her setting. Tol illustrate these methods we\napply Wasserstein GANs (WGANs) to compare a number of different estimators for\naverage treatment effects under unconfoundedness in three distinct settings\n(corresponding to three real data sets) and present a methodology for assessing\nthe robustness of the results. In this example, we find that (i) there is not\none estimator that outperforms the others in all three settings, so researchers\nshould tailor their analytic approach to a given setting, and (ii) systematic\nsimulation studies can be helpful for selecting among competing methods in this\nsituation.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 05:01:38 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 07:07:15 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 03:47:55 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Athey", "Susan", ""], ["Imbens", "Guido", ""], ["Metzger", "Jonas", ""], ["Munro", "Evan", ""]]}, {"id": "1909.02243", "submitter": "Wenquan Cui", "authors": "Wenquan Cui, Jianjun Xu and Yuehua Wu", "title": "A new reproducing kernel based nonlinear dimension reduction method for\n  survival data", "comments": "51 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the theories of sliced inverse regression (SIR) and reproducing\nkernel Hilbert space (RKHS), a new approach RDSIR (RKHS-based Double SIR) to\nnonlinear dimension reduction for survival data is proposed and discussed. An\nisometrically isomorphism is constructed based on RKHS property, then the\nnonlinear function in the RKHS can be represented by the inner product of two\nelements which reside in the isomorphic feature space. Due to the censorship of\nsurvival data, double slicing is used to estimate weight function or\nconditional survival function to adjust for the censoring bias. The sufficient\ndimension reduction (SDR) subspace is estimated by a generalized\neigen-decomposition problem. Our method is computationally efficient with fast\ncalculation speed and small computational burden. The asymptotic property and\nthe convergence rate of the estimator are also discussed based on the\nperturbation theory. Finally, we illustrate the performance of RDSIR on\nsimulated and real data to confirm that RDSIR is comparable with linear SDR\nmethod. The most important is that RDSIR can also extract nonlinearity in\nsurvival data effectively.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 07:29:21 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Cui", "Wenquan", ""], ["Xu", "Jianjun", ""], ["Wu", "Yuehua", ""]]}, {"id": "1909.02282", "submitter": "Flavio Santi", "authors": "Giuseppe Arbia, Maria Michela Dickson, Giuseppe Espa, Diego Giuliani,\n  Flavio Santi", "title": "Reduced-bias estimation of spatial econometric models with incompletely\n  geocoded data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of state-of-the-art spatial econometric models requires that\nthe information about the spatial coordinates of statistical units is\ncompletely accurate, which is usually the case in the context of areal data.\nWith micro-geographic point-level data, however, such information is inevitably\naffected by locational errors, that can be generated intentionally by the data\nproducer for privacy protection or can be due to inaccuracy of the geocoding\nprocedures. This unfortunate circumstance can potentially limit the use of the\nspatial econometric modelling framework for the analysis of micro data. Indeed,\nsome recent contributions (see e.g. Arbia, Espa and Giuliani 2016) have shown\nthat the presence of locational errors may have a non-negligible impact on the\nresults. In particular, wrong spatial coordinates can lead to downward bias and\nincreased variance in the estimation of model parameters. This contribution\naims at developing a strategy to reduce the bias and produce more reliable\ninference for spatial econometrics models with location errors. The validity of\nthe proposed approach is assessed by means of a Monte Carlo simulation study\nunder different real-case scenarios. The study results show that the method is\npromising and can make the spatial econometric modelling of micro-geographic\ndata possible.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 09:33:34 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Arbia", "Giuseppe", ""], ["Dickson", "Maria Michela", ""], ["Espa", "Giuseppe", ""], ["Giuliani", "Diego", ""], ["Santi", "Flavio", ""]]}, {"id": "1909.02426", "submitter": "James Tucker", "authors": "Kyungmin Ahn, J. Derek Tucker, Wei Wu, and Anuj Srivastava", "title": "Regression Models Using Shapes of Functions as Predictors", "comments": "30 pages", "journal-ref": null, "doi": "10.1016/j.csda.2020.107017", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional variables are often used as predictors in regression problems. A\ncommonly-used parametric approach, called {\\it scalar-on-function regression},\nuses the $\\ltwo$ inner product to map functional predictors into scalar\nresponses. This method can perform poorly when predictor functions contain\nundesired phase variability, causing phases to have disproportionately large\ninfluence on the response variable. One past solution has been to perform\nphase-amplitude separation (as a pre-processing step) and then use only the\namplitudes in the regression model. Here we propose a more integrated approach,\ntermed elastic functional regression model (EFRM), where phase-separation is\nperformed inside the regression model, rather than as a pre-processing step.\nThis approach generalizes the notion of phase in functional data, and is based\non the norm-preserving time warping of predictors. Due to its invariance\nproperties, this representation provides robustness to predictor phase\nvariability and results in improved predictions of the response variable over\ntraditional models. We demonstrate this framework using a number of datasets\ninvolving gait signals, NMR data, and stock market prices.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 14:02:55 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 14:53:40 GMT"}, {"version": "v3", "created": "Mon, 25 May 2020 15:48:51 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Ahn", "Kyungmin", ""], ["Tucker", "J. Derek", ""], ["Wu", "Wei", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1909.02499", "submitter": "Giuseppe Sanfilippo", "authors": "Frank Lad and Giuseppe Sanfilippo", "title": "Predictive distributions that mimic frequencies over a restricted\n  subdomain (expanded preprint version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A predictive distribution over a sequence of $N+1$ events is said to be\n\"frequency mimicking\" whenever the probability for the final event conditioned\non the outcome of the first $N$ events equals the relative frequency of\nsuccesses among them. Infinitely extendible exchangeable distributions that\nuniversally inhere this property are known to have several annoying concomitant\nproperties. We motivate frequency mimicking assertions over a limited subdomain\nin practical problems of finite inference, and we identify their computable\ncoherent implications. We provide some computed examples using reference\ndistributions, and we introduce computational software to generate any\nspecification. The software derives from an inversion of the finite form of the\nexchangeability representation theorem. Three new theorems delineate the extent\nof the usefulness of such distributions, and we show why it may not be\nappropriate to extend the frequency mimicking assertions for a specified value\nof $N$ to any arbitrary larger size of $N$. The constructive results identify\nthe source and structure of \"adherent masses\" in the limit of a sequence of\nfinitely additive distributions. Appendices develop a novel geometrical\nrepresentation of conditional probabilities which illuminate the analysis.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 16:04:33 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Lad", "Frank", ""], ["Sanfilippo", "Giuseppe", ""]]}, {"id": "1909.02527", "submitter": "Danilo Bzdok", "authors": "Danilo Bzdok, Dorothea L. Floris, Andre F. Marquand", "title": "Analyzing Brain Circuits in Population Neuroscience: A Case to Be a\n  Bayesian", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional connectivity fingerprints are among today's best choices to obtain\na faithful sampling of an individual's brain and cognition in health and\ndisease. Here we make a case for key advantages of analyzing such connectome\nprofiles using Bayesian analysis strategies. They (i) afford full probability\nestimates of the studied neurocognitive phenomenon (ii) provide analytical\nmachinery to separate methodological uncertainty and biological variability in\na coherent manner (iii) usher towards avenues to go beyond classical\nnull-hypothesis significance testing and (iv) enable estimates of credibility\naround all model parameters at play and thus enable predictions with\nuncertainty intervals for single subject. We pick research questions about\nautism spectrum disorder as a recurring theme to illustrate our methodological\narguments.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 17:02:20 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Bzdok", "Danilo", ""], ["Floris", "Dorothea L.", ""], ["Marquand", "Andre F.", ""]]}, {"id": "1909.02528", "submitter": "Jonathan Bradley", "authors": "Zhixing Xu, Jonathan R. Bradley, Debajyoti Sinha", "title": "Latent Multivariate Log-Gamma Models for High-Dimensional Multi-Type\n  Responses with Application to Daily Fine Particulate Matter and Mortality\n  Counts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking and estimating Daily Fine Particulate Matter (PM2.5) is very\nimportant as it has been shown that PM2.5 is directly related to mortality\nrelated to lungs, cardiovascular system, and stroke. That is, high values of\nPM2.5 constitute a public health problem in the US, and it is important that we\nprecisely estimate PM2.5 to aid in public policy decisions. Thus, we propose a\nBayesian hierarchical model for high-dimensional \"multi-type\" responses. By\n\"multi-type\" responses we mean a collection of correlated responses that have\ndifferent distributional assumptions (e.g., continuous skewed observations, and\ncount-valued observations). The Centers for Disease Control and Prevention\n(CDC) database provides counts of mortalities related to PM2.5 and daily\naveraged PM2.5 which are both treated as responses in our analysis. Our model\ncapitalizes on the shared conjugate structure between the Weibull (to model\nPM2.5), Poisson (to model diseases mortalities), and multivariate log-gamma\ndistributions, and we use dimension reduction to aid with computation. Our\nmodel can also be used to improve the precision of estimates and estimate\nvalues at undisclosed/missing counties. We provide a simulation study to\nillustrate the performance of the model, and give an in-depth analysis of the\nCDC dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 17:04:47 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Xu", "Zhixing", ""], ["Bradley", "Jonathan R.", ""], ["Sinha", "Debajyoti", ""]]}, {"id": "1909.02623", "submitter": "Michael Guggisberg", "authors": "Michael Guggisberg", "title": "A Bayesian Approach to Multiple-Output Quantile Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian approach to multiple-output quantile\nregression. The unconditional model is proven to be consistent and\nasymptotically correct frequentist confidence intervals can be obtained. The\nprior for the unconditional model can be elicited as the ex-ante knowledge of\nthe distance of the tau-Tukey depth contour to the Tukey median, the first\nprior of its kind. A proposal for conditional regression is also presented. The\nmodel is applied to the Tennessee Project Steps to Achieving Resilience (STAR)\nexperiment and it finds a joint increase in tau-quantile subpopulations for\nmathematics and reading scores given a decrease in the number of students per\nteacher. This result is consistent with, and much stronger than, the result one\nwould find with multiple-output linear regression. Multiple-output linear\nregression finds the average mathematics and reading scores increase given a\ndecrease in the number of students per teacher. However, there could still be\nsubpopulations where the score declines. The multiple-output quantile\nregression approach confirms there are no quantile subpopulations (of the\ninspected subpopulations) where the score declines. This is truly a statement\nof `no child left behind' opposed to `no average child left behind.'\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 20:27:34 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Guggisberg", "Michael", ""]]}, {"id": "1909.02644", "submitter": "Chris McKennan", "authors": "Chris McKennan, Carole Ober, Dan Nicolae", "title": "Estimation and inference in metabolomics with non-random missing data\n  and latent factors", "comments": "22 pages of main text, 53 pages including supplement, 8 figures, 3\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High throughput metabolomics data are fraught with both non-ignorable missing\nobservations and unobserved factors that influence a metabolite's measured\nconcentration, and it is well known that ignoring either of these complications\ncan compromise estimators. However, current methods to analyze these data can\nonly account for the missing data or unobserved factors, but not both. We\ntherefore developed MetabMiss, a statistically rigorous method to account for\nboth non-random missing data and latent factors in high throughput metabolomics\ndata. Our methodology does not require the practitioner specify a probability\nmodel for the missing data, and makes investigating the relationship between\nthe metabolome and tens, or even hundreds, of phenotypes computationally\ntractable. We demonstrate the fidelity of MetabMiss's estimates using both\nsimulated and real metabolomics data. An R package that implements our method\nis available from https://github.com/chrismckennan/MetabMiss.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 21:47:55 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["McKennan", "Chris", ""], ["Ober", "Carole", ""], ["Nicolae", "Dan", ""]]}, {"id": "1909.02662", "submitter": "Todd Kuffner", "authors": "Todd A. Kuffner, Stephen M.-S. Lee and G. Alastair Young", "title": "Block bootstrap optimality for density estimation with dependent data", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate approximation of the sampling distribution of nonparametric kernel\ndensity estimators is crucial for many statistical inference problems. Since\nthese estimators have complex asymptotic distributions, bootstrap methods are\noften used for this purpose. With i.i.d. observations, a large literature\nexists concerning optimal bootstrap methods which achieve the fastest possible\nconvergence rate of the bootstrap estimator of the sampling distribution of the\nkernel density estimator. With dependent data, such an optimality theory is an\nimportant open problem. We establish a general theory of optimality of the\nblock bootstrap for kernel density estimation under weak dependence assumptions\nwhich are satisfied by many important time series models. We propose a unified\nframework for a theoretical study of a rich class of bootstrap methods which\ninclude as special cases subsampling, Kunsch's moving block bootstrap, Hall's\nunder-smoothing (UNS) as well as approaches incorporating no (NBC) or explicit\nbias correction (EBC). Moreover, we consider their accuracy under a broad\nspectrum of choices of the bandwidth $h$, which include as an important special\ncase the MSE-optimal choice, as well as other under-smoothed choices. Under\neach choice of $h$, we derive the optimal tuning parameters and compare optimal\nperformances between the main subclasses (EBC, NBC, UNS) of the bootstrap\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 22:49:31 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Kuffner", "Todd A.", ""], ["Lee", "Stephen M. -S.", ""], ["Young", "G. Alastair", ""]]}, {"id": "1909.02669", "submitter": "Erin Hartman", "authors": "Naoki Egami and Erin Hartman", "title": "Covariate Selection for Generalizing Experimental Results: Application\n  to Large-Scale Development Program in Uganda", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizing estimates of causal effects from an experiment to a target\npopulation is of interest to scientists. However, researchers are usually\nconstrained by available covariate information. Analysts can often collect much\nfewer variables from population samples than from experimental samples, which\nhas limited applicability of existing approaches that assume rich covariate\ndata from both experimental and population samples. In this article, we examine\nhow to select covariates necessary for generalizing experimental results under\nsuch data constraints. In our concrete context of a large-scale development\nprogram in Uganda, although more than 40 pre-treatment covariates are available\nin the experiment, only 8 of them were also measured in a target population. We\npropose a method to estimate a separating set -- a set of variables affecting\nboth the sampling mechanism and treatment effect heterogeneity -- and show that\nthe population average treatment effect (PATE) can be identified by adjusting\nfor estimated separating sets. Our algorithm only requires a rich set of\ncovariates in the experimental data, not in the target population, by\nincorporating researcher-specific constraints on what variables are measured in\nthe population data. Analyzing the development experiment in Uganda, we show\nthat the proposed algorithm can allow for the PATE estimation in situations\nwhere conventional methods fail due to data requirements.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 23:18:47 GMT"}, {"version": "v2", "created": "Tue, 25 Feb 2020 22:37:50 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2021 22:40:15 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Egami", "Naoki", ""], ["Hartman", "Erin", ""]]}, {"id": "1909.02736", "submitter": "Clara Grazian", "authors": "Clara Grazian and Yanan Fan", "title": "A review of Approximate Bayesian Computation methods via density\n  estimation: inference for simulator-models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a review of Approximate Bayesian Computation (ABC)\nmethods for carrying out Bayesian posterior inference, through the lens of\ndensity estimation. We describe several recent algorithms and make connection\nwith traditional approaches. We show advantages and limitations of models based\non parametric approaches and we then draw attention to developments in machine\nlearning, which we believe have the potential to make ABC scalable to higher\ndimensions and may be the future direction for research in this area.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 06:52:23 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Grazian", "Clara", ""], ["Fan", "Yanan", ""]]}, {"id": "1909.02878", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa, Kosuke Morikawa and Keisuke Takahata", "title": "Bayesian Semiparametric Modeling of Response Mechanism for Nonignorable\n  Missing Data", "comments": "25 pages; The title has been changed from \"Bayesian semiparametric\n  estimation under nonignorable nonresponse\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference with nonresponse is quite challenging, especially when\nthe response mechanism is nonignorable. In this case, the validity of\nstatistical inference depends on untestable correct specification of the\nresponse model. To avoid the misspecification, we propose semiparametric\nBayesian estimation in which an outcome model is parametric, but the response\nmodel is semiparametric in that we do not assume any parametric form for the\nnonresponse variable. We adopt penalized spline methods to estimate the unknown\nfunction. We also consider a fully nonparametric approach to modeling the\nresponse mechanism by using radial basis function methods. Using Polya-gamma\ndata augmentation, we developed an efficient posterior computation algorithm\nvia Gibbs sampling in which most full conditional distributions can be obtained\nin familiar forms. The performance of the proposed method is demonstrated in\nsimulation studies and an application to longitudinal data.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 12:54:09 GMT"}, {"version": "v2", "created": "Thu, 14 Jan 2021 13:44:45 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Morikawa", "Kosuke", ""], ["Takahata", "Keisuke", ""]]}, {"id": "1909.02929", "submitter": "Paolo Gorgi", "authors": "Paolo Gorgi", "title": "BNB autoregressions for modeling integer-valued time series with extreme\n  observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a general class of heavy-tailed autoregressions for\nmodeling integer-valued time series with outliers. The proposed specification\nis based on a heavy-tailed mixture of negative binomial distributions that\nfeatures an observation-driven dynamic equation for the conditional\nexpectation. The existence of a unique stationary and ergodic solution for the\nclass of autoregressive processes is shown under a general contraction\ncondition. The estimation of the model can be easily performed by Maximum\nLikelihood given the closed form of the likelihood function. The strong\nconsistency and the asymptotic normality of the estimator are formally derived.\nTwo examples of specifications illustrate the flexibility of the approach and\nthe relevance of the theoretical results. In particular, a linear dynamic\nequation and a score-driven equation for the conditional expectation are\nconsidered. The score-driven specification is shown to be particularly\nappealing as it delivers a robust filtering method that attenuates the impact\nof outliers. An empirical application to the time series of narcotics\ntrafficking reports in Sydney illustrates the effectiveness of the method in\nhandling extreme observations.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 14:27:51 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Gorgi", "Paolo", ""]]}, {"id": "1909.02989", "submitter": "Luca Rossini", "authors": "Luciana Dalla Valle, Fabrizio Leisen, Luca Rossini, Weixuan Zhu", "title": "A P\\'olya-Gamma Sampler for a Generalized Logistic Regression", "comments": "Revised Version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel Bayesian data augmentation approach for\nestimating the parameters of the generalised logistic regression model. We\npropose a P\\'olya-Gamma sampler algorithm that allows us to sample from the\nexact posterior distribution, rather than relying on approximations. A\nsimulation study illustrates the flexibility and accuracy of the proposed\napproach to capture heavy and light tails in binary response data of different\ndimensions. The methodology is applied to two different real datasets, where we\ndemonstrate that the P\\'olya-Gamma sampler provides more precise estimates than\nthe empirical likelihood method, outperforming approximate approaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:07:07 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 08:23:43 GMT"}, {"version": "v3", "created": "Mon, 21 Dec 2020 07:43:22 GMT"}], "update_date": "2020-12-22", "authors_parsed": [["Valle", "Luciana Dalla", ""], ["Leisen", "Fabrizio", ""], ["Rossini", "Luca", ""], ["Zhu", "Weixuan", ""]]}, {"id": "1909.03004", "submitter": "Jesse Dodge", "authors": "Jesse Dodge, Suchin Gururangan, Dallas Card, Roy Schwartz, Noah A.\n  Smith", "title": "Show Your Work: Improved Reporting of Experimental Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CL stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in natural language processing proceeds, in part, by demonstrating\nthat new models achieve superior performance (e.g., accuracy) on held-out test\ndata, compared to previous results. In this paper, we demonstrate that test-set\nperformance scores alone are insufficient for drawing accurate conclusions\nabout which model performs best. We argue for reporting additional details,\nespecially performance on validation data obtained during model development. We\npresent a novel technique for doing so: expected validation performance of the\nbest-found model as a function of computation budget (i.e., the number of\nhyperparameter search trials or the overall training time). Using our approach,\nwe find multiple recent model comparisons where authors would have reached a\ndifferent conclusion if they had used more (or less) computation. Our approach\nalso allows us to estimate the amount of computation required to obtain a given\naccuracy; applying it to several recently published results yields massive\nvariation across papers, from hours to weeks. We conclude with a set of best\npractices for reporting experimental results which allow for robust future\ncomparisons, and provide code to allow researchers to use our technique.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 16:40:42 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Dodge", "Jesse", ""], ["Gururangan", "Suchin", ""], ["Card", "Dallas", ""], ["Schwartz", "Roy", ""], ["Smith", "Noah A.", ""]]}, {"id": "1909.03017", "submitter": "Martin Law MSc", "authors": "Martin Law, Michael J. Grayling, Adrian P. Mander", "title": "Optimal curtailed designs for single arm phase II clinical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In single-arm phase II oncology trials, the most popular choice of design is\nSimon's two-stage design, which allows early stopping at one interim analysis.\nHowever, the expected trial sample size can be reduced further by allowing\ncurtailment. Curtailment is stopping when the final go or no-go decision is\ncertain, so-called non-stochastic curtailment, or very likely, known as\nstochastic curtailment.\n  In the context of single-arm phase II oncology trials, stochastic curtailment\nhas previously been restricted to stopping in the second stage and/or stopping\nfor a no-go decision only. We introduce two designs that incorporate stochastic\ncurtailment and allow stopping after every observation, for either a go or\nno-go decision. We obtain optimal stopping boundaries by searching over a range\nof potential conditional powers, beyond which the trial will stop for a go or\nno-go decision. This search is novel: firstly, the search is undertaken over a\nrange of values unique to each possible design realisation. Secondly, these\nvalues are evaluated taking into account the possibility of early stopping.\nFinally, each design realisation's operating characteristics are obtained\nexactly.\n  The proposed designs are compared to existing designs in a real data example.\nThey are also compared under three scenarios, both with respect to four single\noptimality criteria and using a loss function.\n  The proposed designs are superior in almost all cases. Optimising for the\nexpected sample size under either the null or alternative hypothesis, the\nsaving compared to the popular Simon's design ranges from 22% to 55%.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 17:13:24 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Law", "Martin", ""], ["Grayling", "Michael J.", ""], ["Mander", "Adrian P.", ""]]}, {"id": "1909.03302", "submitter": "Tong Li", "authors": "Tong Li and Ming Yuan", "title": "On the Optimality of Gaussian Kernel Based Nonparametric Tests against\n  Smooth Alternatives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric tests via kernel embedding of distributions have witnessed a\ngreat deal of practical successes in recent years. However, statistical\nproperties of these tests are largely unknown beyond consistency against a\nfixed alternative. To fill in this void, we study here the asymptotic\nproperties of goodness-of-fit, homogeneity and independence tests using\nGaussian kernels, arguably the most popular and successful among such tests.\nOur results provide theoretical justifications for this common practice by\nshowing that tests using Gaussian kernel with an appropriately chosen scaling\nparameter are minimax optimal against smooth alternatives in all three\nsettings. In addition, our analysis also pinpoints the importance of choosing a\ndiverging scaling parameter when using Gaussian kernels and suggests a\ndata-driven choice of the scaling parameter that yields tests optimal, up to an\niterated logarithmic factor, over a wide range of smooth alternatives.\nNumerical experiments are also presented to further demonstrate the practical\nmerits of the methodology.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 16:43:20 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Li", "Tong", ""], ["Yuan", "Ming", ""]]}, {"id": "1909.03395", "submitter": "Shadi Mohagheghi", "authors": "Shadi Mohagheghi, Pushkarini Agharkar, Noah E. Friedkin, Francesco\n  Bullo", "title": "Multi-group connectivity structures and their implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the implications of different forms of multi-group\nconnectivity. Four multi-group connectivity modalities are considered:\nco-memberships, edge bundles, bridges, and liaison hierarchies. We propose\ngenerative models to generate these four modalities. Our models are variants of\nplanted partition or stochastic block models conditioned under certain\ntopological constraints. We report findings of a comparative analysis in which\nwe evaluate these structures, controlling for their edge densities and sizes,\non mean rates of information propagation, convergence times to consensus, and\nsteady state deviations from the consensus value in the presence of noise as\nnetwork size increases.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 06:44:21 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Mohagheghi", "Shadi", ""], ["Agharkar", "Pushkarini", ""], ["Friedkin", "Noah E.", ""], ["Bullo", "Francesco", ""]]}, {"id": "1909.03457", "submitter": "C. H. Bryan Liu", "authors": "C. H. Bryan Liu (1) and Benjamin Paul Chamberlain (2) ((1) ASOS.com,\n  (2) Twitter Inc)", "title": "What is the value of experimentation & measurement?", "comments": "Accepted into IEEE International Conference on Data Mining (ICDM)\n  2019. Main paper: 6 pages, 3 figures; Supplementary document: 7 pages, 2\n  figures. Code available on:\n  https://github.com/liuchbryan/value_of_experimentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimentation and Measurement (E&M) capabilities allow organizations to\naccurately assess the impact of new propositions and to experiment with many\nvariants of existing products. However, until now, the question of measuring\nthe measurer, or valuing the contribution of an E&M capability to\norganizational success has not been addressed. We tackle this problem by\nanalyzing how, by decreasing estimation uncertainty, E&M platforms allow for\nbetter prioritization. We quantify this benefit in terms of expected relative\nimprovement in the performance of all new propositions and provide guidance for\nhow much an E&M capability is worth and when organizations should invest in\none.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 13:01:30 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Liu", "C. H. Bryan", ""], ["Chamberlain", "Benjamin Paul", ""]]}, {"id": "1909.03566", "submitter": "Zdravko Botev", "authors": "Zdravko I. Botev and Pierre L'Ecuyer", "title": "Sampling Conditionally on a Rare Event via Generalized Splitting", "comments": "29 pages; 4 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose and analyze a generalized splitting method to sample approximately\nfrom a distribution conditional on the occurrence of a rare event. This has\nimportant applications in a variety of contexts in operations research,\nengineering, and computational statistics. The method uses independent trials\nstarting from a single particle. We exploit this independence to obtain\nasymptotic and non-asymptotic bounds on the total variation error of the\nsampler. Our main finding is that the approximation error depends crucially on\nthe relative variability of the number of points produced by the splitting\nalgorithm in one run, and that this relative variability can be readily\nestimated via simulation. We illustrate the relevance of the proposed method on\nan application in which one needs to sample (approximately) from an intractable\nposterior density in Bayesian inference.\n", "versions": [{"version": "v1", "created": "Sun, 8 Sep 2019 23:43:29 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Botev", "Zdravko I.", ""], ["L'Ecuyer", "Pierre", ""]]}, {"id": "1909.03725", "submitter": "Alexander Henzi", "authors": "Alexander Henzi and Johanna F. Ziegel and Tilmann Gneiting", "title": "Isotonic Distributional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Isotonic distributional regression (IDR) is a powerful nonparametric\ntechnique for the estimation of conditional distributions under order\nrestrictions. In a nutshell, IDR learns conditional distributions that are\ncalibrated, and simultaneously optimal relative to comprehensive classes of\nrelevant loss functions, subject to isotonicity constraints in terms of a\npartial order on the covariate space. Nonparametric isotonic quantile\nregression and nonparametric isotonic binary regression emerge as special\ncases. For prediction, we propose an interpolation method that generalizes\nextant specifications under the pool adjacent violators algorithm. We recommend\nthe use of IDR as a generic benchmark technique in probabilistic forecast\nproblems, as it does not involve any parameter tuning nor implementation\nchoices, except for the selection of a partial order on the covariate space.\nThe method can be combined with subsample aggregation, with the benefits of\nsmoother regression functions and gains in computational efficiency. In a\nsimulation study, we compare methods for distributional regression in terms of\nthe continuous ranked probability score (CRPS) and $L_2$ estimation error,\nwhich are closely linked. In a case study on raw and postprocessed quantitative\nprecipitation forecasts from a leading numerical weather prediction system, IDR\nis competitive with state of the art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 09:43:07 GMT"}, {"version": "v2", "created": "Fri, 13 Nov 2020 10:34:50 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Henzi", "Alexander", ""], ["Ziegel", "Johanna F.", ""], ["Gneiting", "Tilmann", ""]]}, {"id": "1909.03796", "submitter": "Jesse Hemerik", "authors": "Jesse Hemerik, Jelle J Goeman and Livio Finos", "title": "Robust testing in generalized linear models by sign-flipping score\n  contributions", "comments": "To appear in Journal of the Royal Statistical Society: Series B\n  (Methodology). Early view version (2020)", "journal-ref": null, "doi": "10.1111/rssb.12369", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models are often misspecified due to overdispersion,\nheteroscedasticity and ignored nuisance variables. Existing quasi-likelihood\nmethods for testing in misspecified models often do not provide satisfactory\ntype-I error rate control. We provide a novel semi-parametric test, based on\nsign-flipping individual score contributions. The tested parameter is allowed\nto be multi-dimensional and even high-dimensional. Our test is often robust\nagainst the mentioned forms of misspecification and provides better type-I\nerror control than its competitors. When nuisance parameters are estimated, our\nbasic test becomes conservative. We show how to take nuisance estimation into\naccount to obtain an asymptotically exact test. Our proposed test is\nasymptotically equivalent to its parametric counterpart.\n", "versions": [{"version": "v1", "created": "Mon, 9 Sep 2019 12:25:14 GMT"}, {"version": "v2", "created": "Fri, 8 May 2020 15:11:31 GMT"}], "update_date": "2020-05-13", "authors_parsed": [["Hemerik", "Jesse", ""], ["Goeman", "Jelle J", ""], ["Finos", "Livio", ""]]}, {"id": "1909.03802", "submitter": "Silvia Montagna", "authors": "Silvia Montagna, Vanessa Orani, Raffaele Argiento", "title": "Bayesian isotonic logistic regression via constrained splines: an\n  application to estimating the serve advantage in professional tennis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In professional tennis, it is often acknowledged that the server has an\ninitial advantage. Indeed, the majority of points are won by the server, making\nthe serve one of the most important elements in this sport. In this paper, we\nfocus on the role of the serve advantage in winning a point as a function of\nthe rally length. We propose a Bayesian isotonic logistic regression model for\nthe probability of winning a point on serve. In particular, we decompose the\nlogit of the probability of winning via a linear combination of B-splines basis\nfunctions, with athlete-specific basis function coefficients. Further, we\nensure the serve advantage decreases with rally length by imposing constraints\non the spline coefficients. We also consider the rally ability of each player,\nand study how the different types of court may impact on the player's rally\nability. We apply our methodology to a Grand Slam singles matches dataset.\n", "versions": [{"version": "v1", "created": "Thu, 29 Aug 2019 09:07:45 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Montagna", "Silvia", ""], ["Orani", "Vanessa", ""], ["Argiento", "Raffaele", ""]]}, {"id": "1909.03816", "submitter": "Yawen Guan", "authors": "Yawen Guan, Brian J Reich, James A Mulholland, and Howard H Chang", "title": "Multivariate spectral downscaling for PM2.5 species", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fine particulate matter (PM2.5) is a mixture of air pollutants that has\nadverse effects on human health. Understanding the health effects of PM2.5\nmixture and its individual species has been a research priority over the past\ntwo decades. However, the limited availability of speciated PM2.5 measurements\ncontinues to be a major challenge in exposure assessment for conducting\nlarge-scale population-based epidemiology studies. The PM2.5 species have\ncomplex spatial-temporal and cross dependence structures that should be\naccounted for in estimating the spatiotemporal distribution of each component.\nTwo major sources of air quality data are commonly used for deriving exposure\nestimates: point-level monitoring data and gridded numerical computer model\nsimulation, such as the Community Multiscale Air Quality (CMAQ) model. We\npropose a statistical method to combine these two data sources for estimating\nspeciated PM2.5 concentration. Our method models the complex relationships\nbetween monitoring measurements and the numerical model output at different\nspatial resolutions, and we model the spatial dependence and cross dependence\namong PM2.5 species. We apply the method to combine CMAQ model output with\nmajor PM2.5 species measurements in the contiguous United States in 2011.\n", "versions": [{"version": "v1", "created": "Thu, 5 Sep 2019 20:36:44 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Guan", "Yawen", ""], ["Reich", "Brian J", ""], ["Mulholland", "James A", ""], ["Chang", "Howard H", ""]]}, {"id": "1909.04024", "submitter": "Priyam Das", "authors": "Priyam Das, Debsurya De, Raju Maiti, Mona Kamal, Katherine A.\n  Hutcheson, Clifton D. Fuller, Bibhas Chakraborty and Christine B. Peterson", "title": "Estimating the Optimal Linear Combination of Biomarkers using\n  Spherically Constrained Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of a binary classification problem, the optimal linear\ncombination of continuous predictors can be estimated by maximizing an\nempirical estimate of the area under the receiver operating characteristic\n(ROC) curve (AUC). For multi-category responses, the optimal predictor\ncombination can similarly be obtained by maximization of the empirical\nhypervolume under the manifold (HUM). This problem is particularly relevant to\nmedical research, where it may be of interest to diagnose a disease with\nvarious subtypes or predict a multi-category outcome. Since the empirical HUM\nis discontinuous, non-differentiable, and possibly multi-modal, solving this\nmaximization problem requires a global optimization technique. Estimation of\nthe optimal coefficient vector using existing global optimization techniques is\ncomputationally expensive, becoming prohibitive as the number of predictors and\nthe number of outcome categories increases. We propose an efficient\nderivative-free black-box optimization technique based on pattern search to\nsolve this problem. Through extensive simulation studies, we demonstrate that\nthe proposed method achieves better performance compared to existing methods\nincluding the step-down algorithm. Finally, we illustrate the proposed method\nto predict swallowing difficulty after radiation therapy for oropharyngeal\ncancer based on radiation dose to various structures in the head and neck.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 03:51:07 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2021 05:33:49 GMT"}], "update_date": "2021-01-08", "authors_parsed": [["Das", "Priyam", ""], ["De", "Debsurya", ""], ["Maiti", "Raju", ""], ["Kamal", "Mona", ""], ["Hutcheson", "Katherine A.", ""], ["Fuller", "Clifton D.", ""], ["Chakraborty", "Bibhas", ""], ["Peterson", "Christine B.", ""]]}, {"id": "1909.04222", "submitter": "Raj Agrawal", "authors": "Raj Agrawal and Uma Roy and Caroline Uhler", "title": "Covariance Matrix Estimation under Total Positivity for Portfolio\n  Selection", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selecting the optimal Markowitz porfolio depends on estimating the covariance\nmatrix of the returns of $N$ assets from $T$ periods of historical data.\nProblematically, $N$ is typically of the same order as $T$, which makes the\nsample covariance matrix estimator perform poorly, both empirically and\ntheoretically. While various other general purpose covariance matrix estimators\nhave been introduced in the financial economics and statistics literature for\ndealing with the high dimensionality of this problem, we here propose an\nestimator that exploits the fact that assets are typically positively\ndependent. This is achieved by imposing that the joint distribution of returns\nbe multivariate totally positive of order 2 ($\\text{MTP}_2$). This constraint\non the covariance matrix not only enforces positive dependence among the\nassets, but also regularizes the covariance matrix, leading to desirable\nstatistical properties such as sparsity. Based on stock-market data spanning\nover thirty years, we show that estimating the covariance matrix under\n$\\text{MTP}_2$ outperforms previous state-of-the-art methods including\nshrinkage estimators and factor models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 01:16:16 GMT"}, {"version": "v2", "created": "Mon, 28 Dec 2020 04:32:09 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Agrawal", "Raj", ""], ["Roy", "Uma", ""], ["Uhler", "Caroline", ""]]}, {"id": "1909.04232", "submitter": "James Weber PhD", "authors": "James S. Weber, Nicole A. Lazar", "title": "Method of Moments Histograms", "comments": "Updates: Weber, J. S. (2016) - What Can We Learn from Correct\n  Calculation of Histograms? - In JSM Proceedings, Stat. Graphics Sect..\n  Alexandria, VA: Amer. Stat. Assoc. 1893-1913. Weber, J. S. (2016) -\n  Calculating Method of Moments Uniform Bin Width Histograms. -\n  arXiv:1606.04891", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uniform bin width histograms are widely used so this data graphic should\nrepresent data as correctly as possible. Method of moments based on familiar\nmean, variance and Fisher-Pearson skewness cure this problem.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 01:44:21 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Weber", "James S.", ""], ["Lazar", "Nicole A.", ""]]}, {"id": "1909.04369", "submitter": "Krzysztof Rusek", "authors": "Lucjan Janowski, Bogdan \\'Cmiel, Krzysztof Rusek, Jakub Nawa{\\l}a, Zhi\n  Li", "title": "Generalized Score Distribution", "comments": "13 pages, 14 Figures Submitted to Journal of Survey Statistics and\n  Methodology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of discrete probability distributions contains distributions with\nlimited support, i.e. possible argument values are limited to a set of numbers\n(typically consecutive). Examples of such data are results from subjective\nexperiments utilizing the Absolute Category Rating (ACR) technique, where\npossible answers (argument values) are $\\{1, 2, \\cdots, 5\\}$ or typical Likert\nscale $\\{-3, -2, \\cdots, 3\\}$. An interesting subclass of those distributions\nare distributions limited to two parameters: describing the mean value and the\nspread of the answers, and having no more than one change in the probability\nmonotonicity. In this paper we propose a general distribution passing those\nlimitations called Generalized Score Distribution (GSD). The proposed GSD\ncovers all spreads of the answers, from very small, given by the Bernoulli\ndistribution, to the maximum given by a Beta Binomial distribution. We also\nshow that GSD correctly describes subjective experiments scores from video\nquality evaluations with probability of 99.7\\%. A Google Collaboratory website\nwith implementation of the GSD estimation, simulation, and visualization is\nprovided.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 09:37:14 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Janowski", "Lucjan", ""], ["\u0106miel", "Bogdan", ""], ["Rusek", "Krzysztof", ""], ["Nawa\u0142a", "Jakub", ""], ["Li", "Zhi", ""]]}, {"id": "1909.04706", "submitter": "Nicholas Illenberger", "authors": "Nicholas Illenberger, Dylan S. Small, Pamela A. Shaw", "title": "Regression to the Mean's Impact on the Synthetic Control Method: Bias\n  and Sensitivity Analysis", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To make informed policy recommendations from observational data, we must be\nable to discern true treatment effects from random noise and effects due to\nconfounding. Difference-in-Difference techniques which match treated units to\ncontrol units based on pre-treatment outcomes, such as the synthetic control\napproach have been presented as principled methods to account for confounding.\nHowever, we show that use of synthetic controls or other matching procedures\ncan introduce regression to the mean (RTM) bias into estimates of the average\ntreatment effect on the treated. Through simulations, we show RTM bias can lead\nto inflated type I error rates as well as decreased power in typical policy\nevaluation settings. Further, we provide a novel correction for RTM bias which\ncan reduce bias and attain appropriate type I error rates. This correction can\nbe used to perform a sensitivity analysis which determines how results may be\naffected by RTM. We use our proposed correction and sensitivity analysis to\nreanalyze data concerning the effects of California's Proposition 99, a\nlarge-scale tobacco control program, on statewide smoking rates.\n", "versions": [{"version": "v1", "created": "Tue, 10 Sep 2019 18:48:18 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Illenberger", "Nicholas", ""], ["Small", "Dylan S.", ""], ["Shaw", "Pamela A.", ""]]}, {"id": "1909.04811", "submitter": "Xianyang Zhang", "authors": "Xianyang Zhang and Jun Chen", "title": "Covariate Adaptive False Discovery Rate Control with Applications to\n  Omics-Wide Multiple Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional multiple testing procedures often assume hypotheses for\ndifferent features are exchangeable. However, in many scientific applications,\nadditional covariate information regarding the patterns of signals and nulls\nare available. In this paper, we introduce an FDR control procedure in\nlarge-scale inference problem that can incorporate covariate information. We\ndevelop a fast algorithm to implement the proposed procedure and prove its\nasymptotic validity even when the underlying model is misspecified and the\np-values are weakly dependent (e.g., strong mixing). Extensive simulations are\nconducted to study the finite sample performance of the proposed method and we\ndemonstrate that the new approach improves over the state-of-the-art approaches\nby being flexible, robust, powerful and computationally efficient. We finally\napply the method to several omics datasets arising from genomics studies with\nthe aim to identify omics features associated with some clinical and biological\nphenotypes. We show that the method is overall the most powerful among\ncompeting methods, especially when the signal is sparse. The proposed Covariate\nAdaptive Multiple Testing procedure is implemented in the R package CAMT.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 01:44:51 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 19:38:36 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Zhang", "Xianyang", ""], ["Chen", "Jun", ""]]}, {"id": "1909.04857", "submitter": "Jacob Priddle", "authors": "Jacob W. Priddle, Scott A. Sisson, David T. Frazier, Christopher\n  Drovandi", "title": "Efficient Bayesian synthetic likelihood with whitening transformations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Likelihood-free methods are an established approach for performing\napproximate Bayesian inference for models with intractable likelihood\nfunctions. However, they can be computationally demanding. Bayesian synthetic\nlikelihood (BSL) is a popular such method that approximates the likelihood\nfunction of the summary statistic with a known, tractable distribution --\ntypically Gaussian -- and then performs statistical inference using standard\nlikelihood-based techniques. However, as the number of summary statistics\ngrows, the number of model simulations required to accurately estimate the\ncovariance matrix for this likelihood rapidly increases. This poses significant\nchallenge for the application of BSL, especially in cases where model\nsimulation is expensive. In this article we propose whitening BSL (wBSL) -- an\nefficient BSL method that uses approximate whitening transformations to\ndecorrelate the summary statistics at each algorithm iteration. We show\nempirically that this can reduce the number of model simulations required to\nimplement BSL by more than an order of magnitude, without much loss of\naccuracy. We explore a range of whitening procedures and demonstrate the\nperformance of wBSL on a range of simulated and real modelling scenarios from\necology and biology.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 05:25:40 GMT"}, {"version": "v2", "created": "Sat, 1 Feb 2020 03:37:37 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Priddle", "Jacob W.", ""], ["Sisson", "Scott A.", ""], ["Frazier", "David T.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1909.04890", "submitter": "Guillaume Maillard", "authors": "Guillaume Maillard (LMO), Sylvain Arlot (LM-Orsay), Matthieu Lerasle\n  (LM-Orsay)", "title": "Aggregated Hold-Out", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregated hold-out (Agghoo) is a method which averages learning rules\nselected by hold-out (that is, cross-validation with a single split). We\nprovide the first theoretical guarantees on Agghoo, ensuring that it can be\nused safely: Agghoo performs at worst like the hold-out when the risk is\nconvex. The same holds true in classification with the 0-1 risk, with an\nadditional constant factor. For the hold-out, oracle inequalities are known for\nbounded losses, as in binary classification. We show that similar results can\nbe proved, under appropriate assumptions, for other risk-minimization problems.\nIn particular, we obtain an oracle inequality for regularized kernel regression\nwith a Lip-schitz loss, without requiring that the Y variable or the regressors\nbe bounded. Numerical experiments show that aggregation brings a significant\nimprovement over the hold-out and that Agghoo is competitive with\ncross-validation.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 07:46:09 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Maillard", "Guillaume", "", "LMO"], ["Arlot", "Sylvain", "", "LM-Orsay"], ["Lerasle", "Matthieu", "", "LM-Orsay"]]}, {"id": "1909.04990", "submitter": "Aditya Mishra", "authors": "Aditya Mishra, Christian L. M\u007fuller", "title": "Robust Regression with Compositional Covariates", "comments": "43 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many biological high-throughput data sets, such as targeted amplicon-based\nand metagenomic sequencing data, are compositional in nature. A common\nexploratory data analysis task is to infer statistical associations between the\nhigh-dimensional microbial compositions and habitat- or host-related\ncovariates. We propose a general robust statistical regression framework,\nRobRegCC (Robust Regression with Compositional Covariates), which extends the\nlinear log-contrast model by a mean shift formulation for capturing outliers.\nRobRegCC includes sparsity-promoting convex and non-convex penalties for\nparsimonious model estimation, a data-driven robust initialization procedure,\nand a novel robust cross-validation model selection scheme. We show RobRegCC's\nability to perform simultaneous sparse log-contrast regression and outlier\ndetection over a wide range of simulation settings and provide theoretical\nnon-asymptotic guarantees for the underlying estimators. To demonstrate the\nseamless applicability of the workflow on real data, we consider a gut\nmicrobiome data set from HIV patients and infer robust associations between a\nsparse set of microbial species and host immune response from soluble CD14\nmeasurements. All experiments are fully reproducible and available on GitHub at\nhttps://github.com/amishra-stats/robregcc.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 11:59:14 GMT"}, {"version": "v2", "created": "Sun, 26 Jul 2020 08:14:00 GMT"}], "update_date": "2020-07-28", "authors_parsed": [["Mishra", "Aditya", ""], ["M\u007fuller", "Christian L.", ""]]}, {"id": "1909.05018", "submitter": "Steven Thompson", "authors": "Steve Thompson", "title": "Design-adherent estimators for network surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network surveys of key populations at risk for HIV are an essential part of\nthe effort to understand how the epidemic spreads and how it can be prevented.\nEstimation of population values from the sample data has been probematical,\nhowever, because the link-tracing of the network surveys includes different\npeople in the sample with unequal probabilities, and these inclusion\nprobabilities have to be estimated accurately to avoid large biases in survey\nestimates. A new approach to estimation is introduced here, based on resampling\nthe sample network many times using a design that adheres to main features of\nthe design used in the field. These features include network link tracing,\nbranching, and without-replacement sampling. The frequency that a person is\nincluded in the resamples is used to estimate the inclusion probability for\neach person in the original sample, and these estimates of inclusion\nprobabilities are used in an unequal-probability estimator. In simulations\nusing a population of drug users, sex workers, and their partners for which the\nactual values of population characteristics are known, the design-adherent\nestimation approach increases the accuracy of estimates of population\nquantities, largely by eliminating most of the biases.\n", "versions": [{"version": "v1", "created": "Sat, 7 Sep 2019 21:06:19 GMT"}], "update_date": "2019-09-12", "authors_parsed": [["Thompson", "Steve", ""]]}, {"id": "1909.05041", "submitter": "Aaron Molstad", "authors": "Aaron J. Molstad", "title": "New insights for the multivariate square-root lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the multivariate square-root lasso, a method for fitting the\nmultivariate response (multi-task) linear regression model with dependent\nerrors. This estimator minimizes the nuclear norm of the residual matrix plus a\nconvex penalty. Unlike some existing methods for multivariate response linear\nregression, which require explicit estimates of the error covariance matrix or\nits inverse, the multivariate square-root lasso criterion implicitly accounts\nfor error dependence and is convex. To justify the use of this estimator, we\nestablish error bounds which illustrate that like the univariate square-root\nlasso, the multivariate square-root lasso is pivotal with respect to the\nunknown error covariance matrix. We propose a new algorithm to compute the\nestimator: a variation of the alternating direction method of multipliers\nalgorithm; and discuss an accelerated first order algorithm which can be\napplied in certain cases. In both simulation studies and a genomic data\napplication, we show that the multivariate square-root lasso can outperform\nmore computationally intensive methods which estimate both the regression\ncoefficient matrix and error precision matrix.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 13:31:23 GMT"}, {"version": "v2", "created": "Thu, 12 Sep 2019 01:38:06 GMT"}, {"version": "v3", "created": "Wed, 7 Apr 2021 21:54:45 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Molstad", "Aaron J.", ""]]}, {"id": "1909.05201", "submitter": "F Din-Houn Lau Dr", "authors": "F. Din-Houn Lau and Sebastian Krumscheid", "title": "Plateau Proposal Distributions for Adaptive Component-wise Multiple-Try\n  Metropolis", "comments": "24 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo (MCMC) methods are sampling methods that have become\na commonly used tool in statistics, for example to perform Monte Carlo\nintegration. As a consequence of the increase in computational power, many\nvariations of MCMC methods exist for generating samples from arbitrary,\npossibly complex, target distributions. The performance of an MCMC method is\npredominately governed by the choice of the so-called proposal distribution\nused. In this paper, we introduce a new type of proposal distribution for the\nuse in MCMC methods that operates component-wise and with multiple trials per\niteration. Specifically, the novel class of proposal distributions, called\nPlateau distributions, do not overlap, thus ensuring that the multiple trials\nare drawn from different regions of the state space. Furthermore, the Plateau\nproposal distributions allow for a bespoke adaptation procedure that lends\nitself to a Markov chain with efficient problem dependent state space\nexploration and improved burn-in properties. Simulation studies show that our\nnovel MCMC algorithm outperforms competitors when sampling from distributions\nwith a complex shape, highly correlated components or multiple modes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Sep 2019 10:29:10 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 21:01:39 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Lau", "F. Din-Houn", ""], ["Krumscheid", "Sebastian", ""]]}, {"id": "1909.05387", "submitter": "Steven Vidovic", "authors": "Steven U. Vidovic", "title": "Tree congruence: quantifying similarity between dendrogram topologies", "comments": "17 pages, 4 figures, 3 tables, 1 linked dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.GN q-bio.PE q-bio.QM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tree congruence metrics are typically global indices that describe the\nsimilarity or dissimilarity between dendrograms. This study principally focuses\non topological congruence metrics that quantify similarity between two\ndendrograms and can give a normalised score between 0 and 1. Specifically, this\narticle describes and tests two metrics the Clade Retention Index (CRI) and the\nMASTxCF which is derived from the combined information available from a maximum\nagreement subtree and a strict consensus. The two metrics were developed to\nstudy differences between evolutionary trees, but their applications are\nmultidisciplinary and can be used on hierarchical cluster diagrams derived from\nanalyses in science, technology, maths or social sciences disciplines. A\ncomprehensive, but non-exhaustive review of other tree congruence metrics is\nprovided and nine metrics are further analysed. 1,620 pairwise analyses of\nsimulated dendrograms (which could be derived from any type of analysis) were\nconducted and are compared in Pac-man piechart matrices. Kendalls tau-b is used\nto demonstrate the concordance of the different metrics and Spearmans rho\nranked correlations are used to support these findings. The results support the\nuse of the CRI and MASTxCF as part of a suite of metrics, but it is recommended\nthat permutation metrics such as SPR distances and weighted metrics are\ndisregarded for the specific purpose of measuring similarity.\n", "versions": [{"version": "v1", "created": "Wed, 11 Sep 2019 21:56:02 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Vidovic", "Steven U.", ""]]}, {"id": "1909.05428", "submitter": "Spencer Woody", "authors": "Spencer Woody, Novin Ghaffari, and Lauren Hund", "title": "Bayesian Model Calibration for Extrapolative Prediction via Gibbs\n  Posteriors", "comments": "38 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current standard Bayesian approach to model calibration, which assigns a\nGaussian process prior to the discrepancy term, often suffers from issues of\nunidentifiability and computational complexity and instability. When the goal\nis to quantify uncertainty in physical parameters for extrapolative prediction,\nthen there is no need to perform inference on the discrepancy term. With this\nin mind, we introduce Gibbs posteriors as an alternative Bayesian method for\nmodel calibration, which updates the prior with a loss function connecting the\ndata to the parameter. The target of inference is the physical parameter value\nwhich minimizes the expected loss. We propose to tune the loss scale of the\nGibbs posterior to maintain nominal frequentist coverage under assumptions of\nthe form of model discrepancy, and present a bootstrap implementation for\napproximating coverage rates. Our approach is highly modular, allowing an\nanalyst to easily encode a wide variety of such assumptions. Furthermore, we\nprovide a principled method of combining posteriors calculated from data\nsubsets. We apply our methods to data from an experiment measuring the material\nproperties of tantalum.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 01:30:43 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Woody", "Spencer", ""], ["Ghaffari", "Novin", ""], ["Hund", "Lauren", ""]]}, {"id": "1909.05433", "submitter": "Matteo Sesia", "authors": "Matteo Sesia, Emmanuel J. Cand\\`es", "title": "A comparison of some conformal quantile regression methods", "comments": "20 pages, 9 figures, 3 tables", "journal-ref": "Stat. 2020; 9:e261", "doi": "10.1002/sta4.261", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We compare two recently proposed methods that combine ideas from conformal\ninference and quantile regression to produce locally adaptive and marginally\nvalid prediction intervals under sample exchangeability (Romano et al., 2019;\nKivaranovic et al., 2019). First, we prove that these two approaches are\nasymptotically efficient in large samples, under some additional assumptions.\nThen we compare them empirically on simulated and real data. Our results\ndemonstrate that the method in Romano et al. (2019) typically yields tighter\nprediction intervals in finite samples. Finally, we discuss how to tune these\nprocedures by fixing the relative proportions of observations used for training\nand conformalization.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 01:48:11 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Sesia", "Matteo", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "1909.05440", "submitter": "Yawen Guan", "authors": "Yawen Guan and Murali Haran", "title": "Fast expectation-maximization algorithms for spatial generalized linear\n  mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial generalized linear mixed models (SGLMMs) are popular and flexible\nmodels for non-Gaussian spatial data. They are useful for spatial\ninterpolations as well as for fitting regression models that account for\nspatial dependence, and are commonly used in many disciplines such as\nepidemiology, atmospheric science, and sociology. Inference for SGLMMs is\ntypically carried out under the Bayesian framework at least in part because\ncomputational issues make maximum likelihood estimation challenging, especially\nwhen high-dimensional spatial data are involved. Here we provide a\ncomputationally efficient projection-based maximum likelihood approach and two\ncomputationally efficient algorithms for routinely fitting SGLMMs. The two\nalgorithms proposed are both variants of expectation maximization (EM)\nalgorithm, using either Markov chain Monte Carlo or a Laplace approximation for\nthe conditional expectation. Our methodology is general and applies to both\ndiscrete-domain (Gaussian Markov random field) as well as continuous-domain\n(Gaussian process) spatial models. Our methods are also able to adjust for\nspatial confounding issues that often lead to problems with interpreting\nregression coefficients. We show, via simulation and real data applications,\nthat our methods perform well both in terms of parameter estimation as well as\nprediction. Crucially, our methodology is computationally efficient and scales\nwell with the size of the data and is applicable to problems where maximum\nlikelihood estimation was previously infeasible.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 03:15:51 GMT"}, {"version": "v2", "created": "Mon, 26 Oct 2020 04:09:13 GMT"}, {"version": "v3", "created": "Wed, 11 Nov 2020 00:39:16 GMT"}], "update_date": "2020-11-12", "authors_parsed": [["Guan", "Yawen", ""], ["Haran", "Murali", ""]]}, {"id": "1909.05481", "submitter": "Aurelie Muller-Gueudin", "authors": "B\\'erang\\`ere Bastien, Taha Boukhobza (CRAN), H\\'el\\`ene Dumond\n  (CRAN), Anne G\\'egout-Petit (BIGS, IECL), Aur\\'elie Muller-Gueudin (BIGS,\n  IECL), Charl\\`ene Thi\\'ebaut (CRAN)", "title": "A statistical methodology to select covariates in high-dimensional data\n  under dependence. Application to the classification of genetic profiles in\n  oncology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new methodology for selecting and ranking covariates associated\nwith a variable of interest in a context of high-dimensional data under\ndependence but few observations. The methodology successively intertwines the\nclustering of covariates, decorrelation of covariates using Factor Latent\nAnalysis, selection using aggregation of adapted methods and finally ranking.\nSimulations study shows the interest of the decorrelation inside the different\nclusters of covariates. We first apply our method to transcriptomic data of 37\npatients with advanced non-small-cell lung cancer who have received\nchemotherapy, to select the transcriptomic covariates that explain the survival\noutcome of the treatment. Secondly, we apply our method to 79 breast tumor\nsamples to define patient profiles for a new metastatic biomarker and\nassociated gene network in order to personalize the treatments.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 06:45:21 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Bastien", "B\u00e9rang\u00e8re", "", "CRAN"], ["Boukhobza", "Taha", "", "CRAN"], ["Dumond", "H\u00e9l\u00e8ne", "", "CRAN"], ["G\u00e9gout-Petit", "Anne", "", "BIGS, IECL"], ["Muller-Gueudin", "Aur\u00e9lie", "", "BIGS,\n  IECL"], ["Thi\u00e9baut", "Charl\u00e8ne", "", "CRAN"]]}, {"id": "1909.05494", "submitter": "Faicel Chamroukhi", "authors": "Fa\u007f\\\"icel Chamroukhi, Florian Lecocq, and Hien D. Nguyen", "title": "Regularized Estimation and Feature Selection in Mixtures of\n  Gaussian-Gated Experts Models", "comments": "Research School on Statistics and Data Science - RSSDS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures-of-Experts models and their maximum likelihood estimation (MLE) via\nthe EM algorithm have been thoroughly studied in the statistics and machine\nlearning literature. They are subject of a growing investigation in the context\nof modeling with high-dimensional predictors with regularized MLE. We examine\nMoE with Gaussian gating network, for clustering and regression, and propose an\n$\\ell_1$-regularized MLE to encourage sparse models and deal with the\nhigh-dimensional setting. We develop an EM-Lasso algorithm to perform parameter\nestimation and utilize a BIC-like criterion to select the model parameters,\nincluding the sparsity tuning hyperparameters. Experiments conducted on\nsimulated data show the good performance of the proposed regularized MLE\ncompared to the standard MLE with the EM algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 07:56:27 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Chamroukhi", "Fa\u007f\u00efcel", ""], ["Lecocq", "Florian", ""], ["Nguyen", "Hien D.", ""]]}, {"id": "1909.05560", "submitter": "Mohammad Arshad Rahman", "authors": "Mohammad Arshad Rahman and Angela Vossmeyer", "title": "Estimation and Applications of Quantile Regression for Binary\n  Longitudinal Data", "comments": null, "journal-ref": "Advances in Econometrics, Volume 40B, 2019", "doi": "10.1108/S0731-90532019000040B009", "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a framework for quantile regression in binary\nlongitudinal data settings. A novel Markov chain Monte Carlo (MCMC) method is\ndesigned to fit the model and its computational efficiency is demonstrated in a\nsimulation study. The proposed approach is flexible in that it can account for\ncommon and individual-specific parameters, as well as multivariate\nheterogeneity associated with several covariates. The methodology is applied to\nstudy female labor force participation and home ownership in the United States.\nThe results offer new insights at the various quantiles, which are of interest\nto policymakers and researchers alike.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 10:44:15 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Rahman", "Mohammad Arshad", ""], ["Vossmeyer", "Angela", ""]]}, {"id": "1909.05575", "submitter": "Mar\\'ia \\'Alvarez Hern\\'andez", "authors": "A. Mart\\'in Andr\\'es and M. \\'Alvarez Hern\\'andez", "title": "Multi-rater delta: extending the delta nominal measure of agreement\n  between two raters to many raters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need to measure the degree of agreement among R raters who independently\nclassify n subjects within K nominal categories is frequent in many scientific\nareas. The most popular measures are Cohen's kappa (R = 2), Fleiss' kappa,\nConger's kappa and Hubert's kappa (R $\\geq$ 2) coefficients, which have several\ndefects. In 2004, the delta coefficient was defined for the case of R = 2,\nwhich did not have the defects of Cohen's kappa coefficient. This article\nextends the coefficient delta from R = 2 raters to R $\\geq$ 2. The coefficient\nmulti-rater delta has the same advantages as the coefficient delta with regard\nto the type kappa coefficients: i) it is intuitive and easy to interpret,\nbecause it refers to the proportion of replies that are concordant and non\nrandom; ii) the summands which give its value allow the degree of agreement in\neach category to be measured accurately, with no need to be collapsed; and iii)\nit is not affected by the marginal imbalance.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 11:29:11 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Andr\u00e9s", "A. Mart\u00edn", ""], ["Hern\u00e1ndez", "M. \u00c1lvarez", ""]]}, {"id": "1909.05748", "submitter": "Ren Hu", "authors": "Ren Hu, Qifeng Li, Feng Qiu", "title": "Ensemble Learning Based Convex Approximation of Three-Phase Power Flow", "comments": "8 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.LG cs.SY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though the convex optimization has been widely used in power systems, it\nstill cannot guarantee to yield a tight (accurate) solution to some problems.\nTo mitigate this issue, this paper proposes an ensemble learning based convex\napproximation for AC power flow equations that differs from the existing convex\nrelaxations. The proposed approach is based on quadratic power flow equations\nin rectangular coordinates and it can be used in both balanced and unbalanced\nthree-phase power networks. To develop this data-driven convex approximation of\npower flows, the polynomial regression (PR) is first deployed as a basic\nlearner to fit convex relationships between the independent and dependent\nvariables. Then, ensemble learning algorithms such as gradient boosting (GB)\nand bagging are introduced to combine learners to boost model performance.\nBased on the learned convex approximation of power flows, optimal power flow\n(OPF) is formulated as a convex quadratic programming problem. The simulation\nresults on IEEE standard cases show that, in the context of solving OPF, the\nproposed data-driven convex approximation outperforms the conventional SDP\nrelaxation in both accuracy and computational efficiency, especially in the\ncases that the conventional SDP relaxation fails.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 15:24:45 GMT"}, {"version": "v2", "created": "Fri, 15 May 2020 14:59:37 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Hu", "Ren", ""], ["Li", "Qifeng", ""], ["Qiu", "Feng", ""]]}, {"id": "1909.05782", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Iv\\'an Fern\\'andez-Val, and Blaise Melly", "title": "Fast Algorithms for the Quantile Regression Process", "comments": "29 pages, 3 figures, 4 tables; for associated Stata package, see\n  https://sites.google.com/site/blaisemelly/home/computer-programs/fast", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widespread use of quantile regression methods depends crucially on the\nexistence of fast algorithms. Despite numerous algorithmic improvements, the\ncomputation time is still non-negligible because researchers often estimate\nmany quantile regressions and use the bootstrap for inference. We suggest two\nnew fast algorithms for the estimation of a sequence of quantile regressions at\nmany quantile indexes. The first algorithm applies the preprocessing idea of\nPortnoy and Koenker (1997) but exploits a previously estimated quantile\nregression to guess the sign of the residuals. This step allows for a reduction\nof the effective sample size. The second algorithm starts from a previously\nestimated quantile regression at a similar quantile index and updates it using\na single Newton-Raphson iteration. The first algorithm is exact, while the\nsecond is only asymptotically equivalent to the traditional quantile regression\nestimator. We also apply the preprocessing idea to the bootstrap by using the\nsample estimates to guess the sign of the residuals in the bootstrap sample.\nSimulations show that our new algorithms provide very large improvements in\ncomputation time without significant (if any) cost in the quality of the\nestimates. For instance, we divide by 100 the time required to estimate 99\nquantile regressions with 20 regressors and 50,000 observations.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 16:34:37 GMT"}, {"version": "v2", "created": "Mon, 6 Apr 2020 19:25:47 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Melly", "Blaise", ""]]}, {"id": "1909.05813", "submitter": "Denis Agniel", "authors": "Denis Agniel, Bing Han, Matthew Cefalu", "title": "Synthetic estimation for the complier average causal effect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an improved estimator of the complier average causal effect\n(CACE). Researchers typically choose a presumably-unbiased estimator for the\nCACE in studies with noncompliance, when many other lower-variance estimators\nmay be available. We propose a synthetic estimator that combines information\nacross all available estimators, leveraging the efficiency in lower-variance\nestimators while maintaining low bias. Our approach minimizes an estimate of\nthe mean squared error of all convex combinations of the candidate estimators.\nWe derive the asymptotic distribution of the synthetic estimator and\ndemonstrate its good performance in simulation, displaying a robustness to\ninclusion of even high-bias estimators.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 17:16:01 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Agniel", "Denis", ""], ["Han", "Bing", ""], ["Cefalu", "Matthew", ""]]}, {"id": "1909.05892", "submitter": "Sen Na", "authors": "Sen Na, Mladen Kolar, Oluwasanmi Koyejo", "title": "Estimating Differential Latent Variable Graphical Models with\n  Applications to Brain Connectivity", "comments": "60 pages", "journal-ref": "Biometrika 2020", "doi": "10.1093/biomet/asaa066", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential graphical models are designed to represent the difference\nbetween the conditional dependence structures of two groups, thus are of\nparticular interest for scientific investigation. Motivated by modern\napplications, this manuscript considers an extended setting where each group is\ngenerated by a latent variable Gaussian graphical model. Due to the existence\nof latent factors, the differential network is decomposed into sparse and\nlow-rank components, both of which are symmetric indefinite matrices. We\nestimate these two components simultaneously using a two-stage procedure: (i)\nan initialization stage, which computes a simple, consistent estimator, and\n(ii) a convergence stage, implemented using a projected alternating gradient\ndescent algorithm applied to a nonconvex objective, initialized using the\noutput of the first stage. We prove that given the initialization, the\nestimator converges linearly with a nontrivial, minimax optimal statistical\nerror. Experiments on synthetic and real data illustrate that the proposed\nnonconvex procedure outperforms existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 18:12:46 GMT"}, {"version": "v2", "created": "Wed, 13 May 2020 23:58:09 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Na", "Sen", ""], ["Kolar", "Mladen", ""], ["Koyejo", "Oluwasanmi", ""]]}, {"id": "1909.05903", "submitter": "Xiaoou Li", "authors": "Yunxiao Chen, Xiaoou Li", "title": "Compound Sequential Change-point Detection in Parallel Data Streams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider sequential change-point detection in parallel data streams, where\neach stream has its own change point. Once a change is detected in a data\nstream, this stream is deactivated permanently. The goal is to maximize the\nnormal operation of the pre-change streams, while controlling the proportion of\npost-change streams among the active streams at all time points. Taking a\nBayesian formulation, we develop a compound decision framework for this\nproblem. A procedure is proposed that is uniformly optimal among all sequential\nprocedures which control the expected proportion of postchange streams at all\ntime points. We also investigate the asymptotic behavior of the proposed method\nwhen the number of data streams grows large. Numerical examples are provided to\nillustrate the use and performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 18:48:21 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 15:08:12 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Chen", "Yunxiao", ""], ["Li", "Xiaoou", ""]]}, {"id": "1909.05922", "submitter": "Chenguang Dai", "authors": "Chenguang Dai and Jun S. Liu", "title": "Monte Carlo Approximation of Bayes Factors via Mixing with Surrogate\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By mixing the target posterior distribution with a surrogate distribution, of\nwhich the normalizing constant is tractable, we propose a method for estimating\nthe marginal likelihood using the Wang-Landau algorithm. We show that a faster\nconvergence of the proposed method can be achieved via the momentum\nacceleration. Two implementation strategies are detailed: (i) facilitating\nglobal jumps between the posterior and surrogate distributions via the\nMultiple-try Metropolis; (ii) constructing the surrogate via the variational\napproximation. When a surrogate is difficult to come by, we describe a new\njumping mechanism for general reversible jump Markov chain Monte Carlo\nalgorithms, which combines the Multiple-try Metropolis and a directional\nsampling algorithm. We illustrate the proposed methods on several statistical\nmodels, including the Log-Gaussian Cox process, the Bayesian Lasso, the\nlogistic regression, and the g-prior Bayesian variable selection.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 19:52:56 GMT"}, {"version": "v2", "created": "Wed, 16 Dec 2020 02:52:16 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Dai", "Chenguang", ""], ["Liu", "Jun S.", ""]]}, {"id": "1909.06083", "submitter": "Israel Mart\\'inez Hern\\'andez", "authors": "Israel Mart\\'inez-Hern\\'andez and Marc G. Genton", "title": "Generalized Records for Functional Time Series with Application to Unit\n  Root Tests", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A generalization of the definition of records to functional data is proposed.\nThe definition is based on ranking curves using a notion of functional depth.\nThis approach allows us to study the curves of the number of records over time.\nWe focus on functional time series and apply ideas from univariate time series\nto demonstrate the asymptotic distribution describing the number of records. A\nunit root test is proposed as an application of functional record theory.\nThrough a Monte Carlo study, different scenarios of functional processes are\nsimulated to evaluate the performance of the unit root test. The generalized\nrecord definition is applied on two different datasets: Annual mortality rates\nin France and daily curves of wind speed at Yanbu, Saudi Arabia. The record\ncurves are identified and the underlying functional process is studied based on\nthe number of record curves observed.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 08:31:00 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Mart\u00ednez-Hern\u00e1ndez", "Israel", ""], ["Genton", "Marc G.", ""]]}, {"id": "1909.06094", "submitter": "Estelle Kuhn", "authors": "Maud Delattre (MIA-Paris), Estelle Kuhn (MaIAGE)", "title": "Estimating Fisher Information Matrix in Latent Variable Models based on\n  the Score Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher information matrix (FIM) is a key quantity in statistics as it is\nrequired for example for evaluating asymptotic precisions of parameter\nestimates, for computing test statistics or asymptotic distributions in\nstatistical testing, for evaluating post model selection inference results or\noptimality criteria in experimental designs. However its exact computation is\noften not trivial. In particular in many latent variable models, it is\nintricated due to the presence of unobserved variables. Therefore the observed\nFIM is usually considered in this context to estimate the FIM. Several methods\nhave been proposed to approximate the observed FIM when it can not be evaluated\nanalytically. Among the most frequently used approaches are Monte-Carlo methods\nor iterative algorithms derived from the missing information principle. All\nthese methods require to compute second derivatives of the complete data\nlog-likelihood which leads to some disadvantages from a computational point of\nview. In this paper, we present a new approach to estimate the FIM in latent\nvariable model. The advantage of our method is that only the first derivatives\nof the log-likelihood is needed, contrary to other approaches based on the\nobserved FIM. Indeed we consider the empirical estimate of the covariance\nmatrix of the score. We prove that this estimate of the Fisher information\nmatrix is unbiased, consistent and asymptotically Gaussian. Moreover we\nhighlight that none of both estimates is better than the other in terms of\nasymptotic covariance matrix. When the proposed estimate can not be directly\nanalytically evaluated, we present a stochastic approximation estimation\nalgorithm to compute it. This algorithm provides this estimate of the FIM as a\nby-product of the parameter estimates. We emphasize that the proposed algorithm\nonly requires to compute the first derivatives of the complete data\nlog-likelihood with respect to the parameters. We prove that the estimation\nalgorithm is consistent and asymptotically Gaussian when the number of\niterations goes to infinity. We evaluate the finite sample size properties of\nthe proposed estimate and of the observed FIM through simulation studies in\nlinear mixed effects models and mixture models. We also investigate the\nconvergence properties of the estimation algorithm in non linear mixed effects\nmodels. We compare the performances of the proposed algorithm to those of other\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 09:02:42 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Delattre", "Maud", "", "MIA-Paris"], ["Kuhn", "Estelle", "", "MaIAGE"]]}, {"id": "1909.06120", "submitter": "Miles Lopes", "authors": "Miles E. Lopes, N. Benjamin Erichson, Michael W. Mahoney", "title": "Bootstrapping the Operator Norm in High Dimensions: Error Estimation for\n  Covariance Matrices and Sketching", "comments": "52 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.NA math.NA stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the operator (spectral) norm is one of the most widely used metrics\nfor covariance estimation, comparatively little is known about the fluctuations\nof error in this norm. To be specific, let $\\hat\\Sigma$ denote the sample\ncovariance matrix of $n$ observations in $\\mathbb{R}^p$ that arise from a\npopulation matrix $\\Sigma$, and let\n$T_n=\\sqrt{n}\\|\\hat\\Sigma-\\Sigma\\|_{\\text{op}}$. In the setting where the\neigenvalues of $\\Sigma$ have a decay profile of the form\n$\\lambda_j(\\Sigma)\\asymp j^{-2\\beta}$, we analyze how well the bootstrap can\napproximate the distribution of $T_n$. Our main result shows that up to factors\nof $\\log(n)$, the bootstrap can approximate the distribution of $T_n$ at the\ndimension-free rate of $n^{-\\frac{\\beta-1/2}{6\\beta+4}}$, with respect to the\nKolmogorov metric. Perhaps surprisingly, a result of this type appears to be\nnew even in settings where $p< n$. More generally, we discuss the consequences\nof this result beyond covariance matrices and show how the bootstrap can be\nused to estimate the errors of sketching algorithms in randomized numerical\nlinear algebra (RandNLA). An illustration of these ideas is also provided with\na climate data example.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 10:02:08 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Lopes", "Miles E.", ""], ["Erichson", "N. Benjamin", ""], ["Mahoney", "Michael W.", ""]]}, {"id": "1909.06263", "submitter": "Wenjia Wang", "authors": "Wenjia Wang and Yi-Hui Zhou", "title": "A Double Penalty Model for Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern statistical learning techniques have often emphasized prediction\nperformance over interpretability, giving rise to \"black box\" models that may\nbe difficult to understand, and to generalize to other settings. We\nconceptually divide a prediction model into interpretable and non-interpretable\nportions, as a means to produce models that are highly interpretable with\nlittle loss in performance. Implementation of the model is achieved by\nconsidering separability of the interpretable and non-interpretable portions,\nalong with a doubly penalized procedure for model fitting. We specify\nconditions under which convergence of model estimation can be achieved via\ncyclic coordinate ascent, and the consistency of model estimation holds. We\napply the methods to datasets for microbiome host trait prediction and a\ndiabetes trait, and discuss practical tradeoff diagnostics to select models\nwith high interpretability.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 14:40:42 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Wang", "Wenjia", ""], ["Zhou", "Yi-Hui", ""]]}, {"id": "1909.06307", "submitter": "Weichi Wu", "authors": "Weichi Wu, Zhou Zhou", "title": "Multiscale Jump Testing and Estimation Under Complex Temporal Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting jumps in an otherwise smoothly evolving\ntrend whilst the covariance and higher-order structures of the system can\nexperience both smooth and abrupt changes over time. The number of jump points\nis allowed to diverge to infinity with the jump sizes possibly shrinking to\nzero. The method is based on a multiscale application of an optimal jump-pass\nfilter to the time series, where the scales are dense between admissible lower\nand upper bounds. For a wide class of non-stationary time series models and\nassociated trend functions, the proposed method is shown to be able to detect\nall jump points within a nearly optimal range with a prescribed probability\nasymptotically. For a time series of length $n$, the computational complexity\nof the proposed method is $O(n)$ for each scale and $O(n\\log^{1+\\epsilon} n)$\noverall, where $\\epsilon$ is an arbitrarily small positive constant.\nSimulations and data analysis show that the proposed jump testing and\nestimation method performs robustly and accurately under complex temporal\ndynamics.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 15:58:20 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2020 15:36:20 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Wu", "Weichi", ""], ["Zhou", "Zhou", ""]]}, {"id": "1909.06359", "submitter": "Yi Yu", "authors": "Daren Wang and Yi Yu and Alessandro Rinaldo and Rebecca Willett", "title": "Localizing Changes in High-Dimensional Vector Autoregressive Processes", "comments": "53 pages; 4 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autoregressive models capture stochastic processes in which past realizations\ndetermine the generative distribution of new data; they arise naturally in a\nvariety of industrial, biomedical, and financial settings. A key challenge when\nworking with such data is to determine when the underlying generative model has\nchanged, as this can offer insights into distinct operating regimes of the\nunderlying system. This paper describes a novel dynamic programming approach to\nlocalizing changes in high-dimensional autoregressive processes and associated\nerror rates that improve upon the prior state of the art. When the model\nparameters are piecewise constant over time and the corresponding process is\npiecewise stable, the proposed dynamic programming algorithm consistently\nlocalizes change points even as the dimensionality, the sparsity of the\ncoefficient matrices, the temporal spacing between two consecutive change\npoints, and the magnitude of the difference of two consecutive coefficient\nmatrices are allowed to vary with the sample size. Furthermore, the accuracy of\ninitial, coarse change point localization estimates can be boosted via a\ncomputationally-efficient refinement algorithm that provably improves the\nlocalization error rate. Finally, a comprehensive simulation experiments and a\nreal data analysis are provided to show the numerical superiority of our\nproposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 12 Sep 2019 15:07:32 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2020 09:45:38 GMT"}], "update_date": "2020-07-30", "authors_parsed": [["Wang", "Daren", ""], ["Yu", "Yi", ""], ["Rinaldo", "Alessandro", ""], ["Willett", "Rebecca", ""]]}, {"id": "1909.06432", "submitter": "Reagan Mozer", "authors": "Reagan Mozer, Mark E. Glickman", "title": "Bayesian analysis of longitudinal studies with treatment by indication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often of interest in observational studies to measure the causal effect\nof a treatment on time-to-event outcomes. In a medical setting, observational\nstudies commonly involve patients who initiate medication therapy and others\nwho do not, and the goal is to infer the effect of medication therapy on time\nuntil recovery, a pre-defined level of improvement, or some other time-to-event\noutcome. A difficulty with such studies is that the notion of a medication\ninitiation time does not exist in the control group. We propose an approach to\ninfer causal effects of an intervention in longitudinal observational studies\nwhen the time of treatment assignment is only observed for treated units and\nwhere treatment is given by indication. We present a framework for\nconceptualizing an underlying randomized experiment in this setting based on\nseparating the process that governs the time of study arm assignment from the\nmechanism that determines the assignment. Our approach involves inferring the\nmissing times of assignment followed by estimating treatment effects. This\napproach allows us to incorporate uncertainty about the missing times of study\narm assignment, which induces uncertainty in both the selection of the control\ngroup and the measurement of time-to-event outcomes for these controls. We\ndemonstrate our approach to study the effects on mortality of inappropriately\nprescribing phosphodiesterase type 5 inhibitors (PDE5Is), a medication\ncontraindicated for groups 2 and 3 pulmonary hypertension, using administrative\ndata from the Veterans Affairs (VA) health care system.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 20:18:20 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 16:52:24 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Mozer", "Reagan", ""], ["Glickman", "Mark E.", ""]]}, {"id": "1909.06439", "submitter": "Toby Kenney", "authors": "Lihui Liu, Hong Gu, Johan Van Limbergen and Toby Kenney", "title": "SuRF: a New Method for Sparse Variable Selection, with Application in\n  Microbiome Data Analysis", "comments": "23 pages", "journal-ref": "Statistics in Medicine 40 (2021), 897-919", "doi": "10.1002/sim.8809", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new variable selection method for regression and\nclassification purposes. Our method, called Subsampling Ranking Forward\nselection (SuRF), is based on LASSO penalised regression, subsampling and\nforward-selection methods. SuRF offers major advantages over existing variable\nselection methods in terms of both sparsity of selected models and model\ninference. We provide an R package that can implement our method for\ngeneralized linear models. We apply our method to classification problems from\nmicrobiome data, using a novel agglomeration approach to deal with the special\ntree-like correlation structure of the variables. Existing methods arbitrarily\nchoose a taxonomic level a priori before performing the analysis, whereas by\ncombining SuRF with these aggregated variables, we are able to identify the key\nbiomarkers at the appropriate taxonomic level, as suggested by the data. We\npresent simulations in multiple sparse settings to demonstrate that our\napproach performs better than several other popularly used existing approaches\nin recovering the true variables. We apply SuRF to two microbiome data sets:\none about prediction of pouchitis and another for identifying samples from two\nhealthy individuals. We find that SuRF can provide a better or comparable\nprediction with other methods while controlling the false positive rate of\nvariable selection.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 20:41:27 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Liu", "Lihui", ""], ["Gu", "Hong", ""], ["Van Limbergen", "Johan", ""], ["Kenney", "Toby", ""]]}, {"id": "1909.06476", "submitter": "Dhaker Hamza", "authors": "Youssou Ciss, El hadji Deme and Hamza Dhaker", "title": "Some improvement on non-parametric estimation of income distribution and\n  poverty index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we propose an estimator of Foster, Greer and Thorbecke class\nof measures $\\displaystyle P(z,\\alpha) =\n\\int_0^{z}\\Big(\\frac{z-x}{z}\\Big)^{\\alpha}f(x)\\, dx$, where $z>0$ is the\npoverty line, $f$ is the probabily density function of the income distribution\nand $\\alpha$ is the so-called poverty aversion. The estimator is constructed\nwith a bias reduced kernel estimator. Uniform almost sure consistency and\nuniform mean square consistenty are established. A simulation study indicates\nthat our new estimator performs well.\n", "versions": [{"version": "v1", "created": "Fri, 13 Sep 2019 22:27:01 GMT"}, {"version": "v2", "created": "Tue, 17 Sep 2019 13:59:53 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Ciss", "Youssou", ""], ["Deme", "El hadji", ""], ["Dhaker", "Hamza", ""]]}, {"id": "1909.06503", "submitter": "Zheng Tracy Ke", "authors": "Zheng Tracy Ke, Feng Shi, Dong Xia", "title": "Community Detection for Hypergraph Networks via Regularized Tensor Power\n  Iteration", "comments": "53 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To date, social network analysis has been largely focused on pairwise\ninteractions. The study of higher-order interactions, via a hypergraph network,\nbrings in new insights. We study community detection in a hypergraph network. A\npopular approach is to project the hypergraph to a graph and then apply\ncommunity detection methods for graph networks, but we show that this approach\nmay cause unwanted information loss. We propose a new method for community\ndetection that operates directly on the hypergraph. At the heart of our method\nis a regularized higher-order orthogonal iteration (reg-HOOI) algorithm that\ncomputes an approximate low-rank decomposition of the network adjacency tensor.\nCompared with existing tensor decomposition methods such as HOSVD and vanilla\nHOOI, reg-HOOI yields better performance, especially when the hypergraph is\nsparse. Given the output of tensor decomposition, we then generalize the\ncommunity detection method SCORE (Jin, 2015) from graph networks to hypergraph\nnetworks. We call our new method Tensor-SCORE.\n  In theory, we introduce a degree-corrected block model for hypergraphs\n(hDCBM), and show that Tensor-SCORE yields consistent community detection for a\nwide range of network sparsity and degree heterogeneity. As a byproduct, we\nderive the rates of convergence on estimating the principal subspace by\nreg-HOOI, with different initializations, including the two new initialization\nmethods we propose, a diagonal-removed HOSVD and a randomized graph projection.\n  We apply our method to several real hypergraph networks which yields\nencouraging results. It suggests that exploring higher-order interactions\nprovides additional information not seen in graph representations.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 01:50:19 GMT"}, {"version": "v2", "created": "Thu, 2 Jan 2020 17:53:29 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Ke", "Zheng Tracy", ""], ["Shi", "Feng", ""], ["Xia", "Dong", ""]]}, {"id": "1909.06519", "submitter": "Juan Sosa", "authors": "Juan Sosa, Abel Rodriguez", "title": "A Bayesian Approach for De-duplication in the Presence of Relational\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the impact of combining profile and network data in a\nde-duplication setting. We also assess the influence of a range of prior\ndistributions on the linkage structure, including our proposal. Our proposed\nprior makes it straightforward to specify prior believes and naturally enforces\nthe microclustering property. Furthermore, we explore stochastic gradient\nHamiltonian Monte Carlo methods as a faster alternative to obtain samples for\nthe network parameters. Our methodology is evaluated using the RLdata500 data,\nwhich is a popular dataset in the record linkage literature.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 03:38:41 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 17:17:56 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Sosa", "Juan", ""], ["Rodriguez", "Abel", ""]]}, {"id": "1909.06534", "submitter": "Danhyang Lee", "authors": "Danhyang Lee and Jae Kwang Kim", "title": "Semiparametric Imputation Using Conditional Gaussian Mixture Models\n  under Item Nonresponse", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputation is a popular technique for handling item nonresponse in survey\nsampling. Parametric imputation is based on a parametric model for imputation\nand is less robust against the failure of the imputation model. Nonparametric\nimputation is fully robust but is not applicable when the dimension of\ncovariates is large due to the curse of dimensionality. Semiparametric\nimputation is another robust imputation based on a flexible model where the\nnumber of model parameters can increase with the sample size. In this paper, we\npropose another semiparametric imputation based on a more flexible model\nassumption than the Gaussian mixture model. In the proposed mixture model, we\nassume a conditional Gaussian model for the study variable given the auxiliary\nvariables, but the marginal distribution of the auxiliary variables is not\nnecessarily Gaussian. We show that the proposed mixture model achieves a lower\napproximation error bound to any unknown target density than the Gaussian\nmixture model in terms of the Kullback-Leibler divergence. The proposed method\nis applicable to high dimensional covariate problem by including a penalty\nfunction in the conditional log-likelihood function. The proposed method is\napplied to 2017 Korean Household Income and Expenditure Survey conducted by\nStatistics Korea. Supplementary material is available online.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 05:07:41 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 06:38:13 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Lee", "Danhyang", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1909.06583", "submitter": "Stephan Huckemann", "authors": "Fabian J.E. Telschow, Michael R. Pierrynowski and Stephan F. Huckemann", "title": "Confidence Tubes for Curves on SO(3) and Identification of\n  Subject-Specific Gait Change after Kneeling", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to identify changes of gait patterns, e.g. due to prolonged\noccupational kneeling, which is believed to be major risk factor, among others,\nfor the development of knee osteoarthritis, we develop confidence tubes for\ncurves following a Gaussian perturbation model on SO(3). These are based on an\napplication of the Gaussian kinematic formula to a process of Hotelling\nstatistics and we approximate them by a computible version, for which we show\nconvergence. Simulations endorse our method, which in application to gait\ncurves from eight volunteers undergoing kneeling tasks, identifies phases of\nthe gait cycle that have changed due to kneeling tasks. We find that after\nkneeling, deviation from normal gait is stronger, in particular for older aged\nmale volunteers. Notably our method adjusts for different walking speeds and\nmarker replacement at different visits.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 11:47:33 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Telschow", "Fabian J. E.", ""], ["Pierrynowski", "Michael R.", ""], ["Huckemann", "Stephan F.", ""]]}, {"id": "1909.06624", "submitter": "Guodong Li", "authors": "Di Wang, Yao Zheng, Heng Lian and Guodong Li", "title": "High-dimensional vector autoregressive time series modeling via tensor\n  decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical vector autoregressive model is a fundamental tool for\nmultivariate time series analysis. However, it involves too many parameters\nwhen the number of time series and lag order are even moderately large. This\npaper proposes to rearrange the transition matrices of the model into a tensor\nform such that the parameter space can be restricted along three directions\nsimultaneously via tensor decomposition. In contrast, the reduced-rank\nregression method can restrict the parameter space in only one direction.\nBesides achieving substantial dimension reduction, the proposed model is\ninterpretable from the factor modeling perspective. Moreover, to handle\nhigh-dimensional time series, this paper considers imposing sparsity on factor\nmatrices to improve the model interpretability and estimation efficiency, which\nleads to a sparsity-inducing estimator. For the low-dimensional case, we derive\nasymptotic properties of the proposed least squares estimator and introduce an\nalternating least squares algorithm. For the high-dimensional case, we\nestablish non-asymptotic properties of the sparsity-inducing estimator and\npropose an ADMM algorithm for regularized estimation. Simulation experiments\nand a real data example demonstrate the advantages of the proposed approach\nover various existing methods.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 16:24:28 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2020 07:14:13 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Wang", "Di", ""], ["Zheng", "Yao", ""], ["Lian", "Heng", ""], ["Li", "Guodong", ""]]}, {"id": "1909.06631", "submitter": "Wei Jiang", "authors": "Wei Jiang, Malgorzata Bogdan, Julie Josse, Blazej Miasojedow, Veronika\n  Rockova, TraumaBase Group", "title": "Adaptive Bayesian SLOPE -- High-dimensional Model Selection with Missing\n  Values", "comments": "R package https://github.com/wjiang94/ABSLOPE", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variable selection in high-dimensional settings\nwith missing observations among the covariates. To address this relatively\nunderstudied problem, we propose a new synergistic procedure -- adaptive\nBayesian SLOPE -- which effectively combines the SLOPE method (sorted $l_1$\nregularization) together with the Spike-and-Slab LASSO method. We position our\napproach within a Bayesian framework which allows for simultaneous variable\nselection and parameter estimation, despite the missing values. As with the\nSpike-and-Slab LASSO, the coefficients are regarded as arising from a\nhierarchical model consisting of two groups: (1) the spike for the inactive and\n(2) the slab for the active. However, instead of assigning independent spike\npriors for each covariate, here we deploy a joint \"SLOPE\" spike prior which\ntakes into account the ordering of coefficient magnitudes in order to control\nfor false discoveries. Through extensive simulations, we demonstrate\nsatisfactory performance in terms of power, FDR and estimation bias under a\nwide range of scenarios. Finally, we analyze a real dataset consisting of\npatients from Paris hospitals who underwent a severe trauma, where we show\nexcellent performance in predicting platelet levels. Our methodology has been\nimplemented in C++ and wrapped into an R package ABSLOPE for public use.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 17:09:21 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2019 09:12:37 GMT"}], "update_date": "2019-11-07", "authors_parsed": [["Jiang", "Wei", ""], ["Bogdan", "Malgorzata", ""], ["Josse", "Julie", ""], ["Miasojedow", "Blazej", ""], ["Rockova", "Veronika", ""], ["Group", "TraumaBase", ""]]}, {"id": "1909.06649", "submitter": "Debraj Das", "authors": "Debraj Das, Arindam Chatterjee and S. N. Lahiri", "title": "Higher Order Refinements by Bootstrap in Lasso and other Penalized\n  Regression Methods", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selection of important covariates and to drop the unimportant ones from a\nhigh-dimensional regression model is a long standing problem and hence have\nreceived lots of attention in the last two decades. After selecting the correct\nmodel, it is also important to properly estimate the existing parameters\ncorresponding to important covariates. In this spirit, Fan and Li (2001)\nproposed Oracle property as a desired feature of a variable selection method.\nOracle property has two parts; one is the variable selection consistency (VSC)\nand the other one is the asymptotic normality. Keeping VSC fixed and making the\nother part stronger, Fan and Lv (2008) introduced the strong oracle property.\nIn this paper, we consider different penalized regression techniques which are\nVSC and classify those based on oracle and strong oracle property. We show that\nboth the residual and the perturbation bootstrap methods are second order\ncorrect for any penalized estimator irrespective of its class. Most interesting\nof all is the Lasso, introduced by Tibshirani (1996). Although Lasso is VSC, it\nis not asymptotically normal and hence fails to satisfy the oracle property.\n", "versions": [{"version": "v1", "created": "Sat, 14 Sep 2019 18:13:58 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Das", "Debraj", ""], ["Chatterjee", "Arindam", ""], ["Lahiri", "S. N.", ""]]}, {"id": "1909.06753", "submitter": "Willem Van Den Boom", "authors": "Willem van den Boom, Galen Reeves, David B. Dunson", "title": "Approximating posteriors with high-dimensional nuisance parameters via\n  integrated rotated Gaussian approximation", "comments": "32 pages, 8 figures", "journal-ref": "Biometrika 108 (2021) 269-282", "doi": "10.1093/biomet/asaa068", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Posterior computation for high-dimensional data with many parameters can be\nchallenging. This article focuses on a new method for approximating posterior\ndistributions of a low- to moderate-dimensional parameter in the presence of a\nhigh-dimensional or otherwise computationally challenging nuisance parameter.\nThe focus is on regression models and the key idea is to separate the\nlikelihood into two components through a rotation. One component involves only\nthe nuisance parameters, which can then be integrated out using a novel type of\nGaussian approximation. We provide theory on approximation accuracy that holds\nfor a broad class of forms of the nuisance component and priors. Applying our\nmethod to simulated and real data sets shows that it can outperform\nstate-of-the-art posterior approximation approaches.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 07:34:04 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Boom", "Willem van den", ""], ["Reeves", "Galen", ""], ["Dunson", "David B.", ""]]}, {"id": "1909.06853", "submitter": "Charles Manski", "authors": "Charles F. Manski", "title": "Statistical inference for statistical decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wald development of statistical decision theory addresses decision making\nwith sample data. Wald's concept of a statistical decision function (SDF)\nembraces all mappings of the form [data -> decision]. An SDF need not perform\nstatistical inference; that is, it need not use data to draw conclusions about\nthe true state of nature. Inference-based SDFs have the sequential form [data\n-> inference -> decision]. This paper motivates inference-based SDFs as\npractical procedures for decision making that may accomplish some of what Wald\nenvisioned. The paper first addresses binary choice problems, where all SDFs\nmay be viewed as hypothesis tests. It next considers as-if optimization, which\nuses a point estimate of the true state as if the estimate were accurate. It\nthen extends this idea to as-if maximin and minimax-regret decisions, which use\npoint estimates of some features of the true state as if they were accurate.\nThe paper primarily uses finite-sample maximum regret to evaluate the\nperformance of inference-based SDFs. To illustrate abstract ideas, it presents\nspecific findings concerning treatment choice and point prediction with sample\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 18:24:47 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Manski", "Charles F.", ""]]}, {"id": "1909.06950", "submitter": "Sheng Wang", "authors": "Sheng Wang, Hyunseung Kang", "title": "Weak-Instrument Robust Tests in Two-Sample Summary-Data Mendelian\n  Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mendelian randomization (MR) has been a popular method in genetic\nepidemiology to estimate the effect of an exposure on an outcome using genetic\nvariants as instrumental variables (IV), with two-sample summary-data MR being\nthe most popular. Unfortunately, instruments in MR studies are often weakly\nassociated with the exposure, which can bias effect estimates and inflate Type\nI errors. In this work, we propose test statistics that are robust under weak\ninstrument asymptotics by extending the Anderson-Rubin, Kleibergen, and the\nconditional likelihood ratio test in econometrics to two-sample summary-data\nMR. We also use the proposed Anderson-Rubin test to develop a point estimator\nand to detect invalid instruments. We conclude with a simulation and an\nempirical study and show that the proposed tests control size and have better\npower than existing methods with weak instruments.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 02:42:42 GMT"}, {"version": "v2", "created": "Mon, 22 Feb 2021 04:44:04 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2021 07:59:33 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Sheng", ""], ["Kang", "Hyunseung", ""]]}, {"id": "1909.07056", "submitter": "Estelle Kuhn", "authors": "Oodally Ajmal (MaIAGE), Luc Duchateau, Estelle Kuhn (MaIAGE)", "title": "Convergent stochastic algorithm for parameter estimation in frailty\n  models using integrated partial likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frailty models are often the model of choice for heterogeneous survival data.\nA frailty model contains both random effects and fixed effects, with the random\neffects accommodating for the correlation in the data. Different estimation\nprocedures have been proposed for the fixed effects and the variances of and\ncovariances between the random effects. Especially with an unspecified baseline\nhazard, i.e., the Cox model, the few available methods deal only with a\nspecific correlation structure. In this paper, an estimation procedure, based\non the integrated partial likelihood, is introduced, which can generally deal\nwith any kind of correlation structure. The new approach, namely the\nmaximisation of the integrated partial likelihood, combined with a stochastic\nestimation procedure allows also for a wide choice of distributions for the\nrandom effects. First, we demonstrate the almost sure convergence of the\nstochastic algorithm towards a critical point of the integrated partial\nlikelihood. Second, numerical convergence properties are evaluated by\nsimulation. Third, the advantage of using an unspecified baseline hazard is\ndemonstrated through application on cancer clinical trial data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 08:38:29 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Ajmal", "Oodally", "", "MaIAGE"], ["Duchateau", "Luc", "", "MaIAGE"], ["Kuhn", "Estelle", "", "MaIAGE"]]}, {"id": "1909.07123", "submitter": "David Firth", "authors": "David Firth, Ioannis Kosmidis and Heather Turner", "title": "Davidson-Luce model for multi-item choice with ties", "comments": "11 pages, including Appendix with example R code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a natural extension of the pair-comparison-with-ties\nmodel of Davidson (1970, J. Amer. Statist. Assoc.), to allow for ties when more\nthan two items are compared. Properties of the new model are discussed. It is\nfound that this \"Davidson-Luce\" model retains the many appealing features of\nDavidson's solution, while extending the scope of application substantially\nbeyond the domain of pair-comparison data. The model introduced here already\nunderpins the handling of tied rankings in the \"PlackettLuce\" R package.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 11:11:22 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Firth", "David", ""], ["Kosmidis", "Ioannis", ""], ["Turner", "Heather", ""]]}, {"id": "1909.07187", "submitter": "Fabian Mies", "authors": "Fabian Mies, Stefan Bedbur", "title": "Exact Semiparametric Inference and Model Selection for Load-Sharing\n  Systems", "comments": "To appear in: IEEE Transactions on Reliability", "journal-ref": null, "doi": "10.1109/TR.2019.2935869", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a specific proportional hazard rates model, sequential order statistics\ncan be used to describe the lifetimes of load-sharing systems. Inference for\nthese systems needs to account for small sample sizes, which are prevalent in\nreliability applications. By exploiting the probabilistic structure of\nsequential order statistics, we derive exact finite sample inference procedures\nto test for the load-sharing parameters and for the nonparametrically specified\nbaseline distribution, treating the respective other part as a nuisance\nquantity. This improves upon previous approaches for the model, which either\nassume a fully parametric specification or rely on asymptotic results.\nSimulations show that the tests derived are able to detect deviations from the\nnull hypothesis at small sample sizes. Critical values for a prominent case are\ntabulated.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 13:30:44 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Mies", "Fabian", ""], ["Bedbur", "Stefan", ""]]}, {"id": "1909.07233", "submitter": "Lee Kennedy-Shaffer", "authors": "Lee Kennedy-Shaffer, Victor De Gruttola, Marc Lipsitch", "title": "Novel Methods for the Analysis of Stepped Wedge Cluster Randomized\n  Trials", "comments": "49 total pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stepped wedge cluster randomized trials (SW-CRTs) have become increasingly\npopular and are used for a variety of interventions and outcomes, often chosen\nfor their feasibility advantages. SW-CRTs must account for time trends in the\noutcome because of the staggered rollout of the intervention inherent in the\ndesign. Robust inference procedures and non-parametric analysis methods have\nrecently been proposed to handle such trends without requiring strong\nparametric modeling assumptions, but these are less powerful than model-based\napproaches. We propose several novel analysis methods that reduce reliance on\nmodeling assumptions while preserving some of the increased power provided by\nthe use of mixed effects models. In one method, we use the synthetic control\napproach to find the best matching clusters for a given intervention cluster.\nThis approach can improve the power of the analysis but is fully\nnon-parametric. Another method makes use of within-cluster crossover\ninformation to construct an overall estimator. We also consider methods that\ncombine these approaches to further improve power. We test these methods on\nsimulated SW-CRTs and identify settings for which these methods gain robustness\nto model misspecification while retaining some of the power advantages of mixed\neffects models. Finally, we propose avenues for future research on the use of\nthese methods; motivation for such research arises from their flexibility,\nwhich allows the identification of specific causal contrasts of interest, their\nrobustness, and the potential for incorporating covariates to further increase\npower. Investigators conducting SW-CRTs might well consider such methods when\ncommon modeling assumptions may not hold.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 14:23:41 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Kennedy-Shaffer", "Lee", ""], ["De Gruttola", "Victor", ""], ["Lipsitch", "Marc", ""]]}, {"id": "1909.07324", "submitter": "Kai Qi", "authors": "Kai Qi, Yang Chen, Wei Wu", "title": "Dirichlet Depths for Point Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical depths have been well studied for multivariate and functional\ndata over the past few decades, but remain under-explored for point processes.\nA first attempt on the notion of point process depth was conducted recently\nwhere the depth was defined as a weighted product of two terms: (1) the\nprobability of the number of events in each process and (2) the depth of the\nevent times conditioned on the number of events by using a Mahalanobis depth.\nWe point out that multivariate depths such as the Mahalanobis depth cannot be\ndirectly used because they often neglect the important ordered property in the\npoint process events. To deal with this problem, we propose a model-based\napproach for point processes systematically. In particular, we develop a\nDirichlet-distribution-based framework on the conditional depth term, where the\nnew methods are referred to as Dirichlet depths. We examine the mathematical\nproperties of the new depths and conduct the asymptotic analysis. In addition,\nwe illustrate the new methods using various simulated and real experiment data.\nIt is found that the proposed framework provides a proper center-outward rank\nand the new methods have superior decoding performance to previous methods in\ntwo neural spike train datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 16:36:47 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Qi", "Kai", ""], ["Chen", "Yang", ""], ["Wu", "Wei", ""]]}, {"id": "1909.07339", "submitter": "Boyan Duan", "authors": "Boyan Duan, Aaditya Ramdas, Sivaraman Balakrishnan, Larry Wasserman", "title": "Interactive Martingale Tests for the Global Null", "comments": "48 pages, 16 figures", "journal-ref": "Electronic Journal of Statistics 14 (2020) 4489-4551", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global null testing is a classical problem going back about a century to\nFisher's and Stouffer's combination tests. In this work, we present simple\nmartingale analogs of these classical tests, which are applicable in two\ndistinct settings: (a) the online setting in which there is a possibly infinite\nsequence of $p$-values, and (b) the batch setting, where one uses prior\nknowledge to preorder the hypotheses. Through theory and simulations, we\ndemonstrate that our martingale variants have higher power than their classical\ncounterparts even when the preordering is only weakly informative. Finally,\nusing a recent idea of \"masking\" $p$-values, we develop a novel interactive\ntest for the global null that can take advantage of covariates and repeated\nuser guidance to create a data-adaptive ordering that achieves higher detection\npower against structured alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 17:16:40 GMT"}, {"version": "v2", "created": "Wed, 25 Sep 2019 13:12:58 GMT"}, {"version": "v3", "created": "Sat, 7 Dec 2019 15:51:47 GMT"}, {"version": "v4", "created": "Tue, 6 Oct 2020 09:10:56 GMT"}, {"version": "v5", "created": "Tue, 2 Mar 2021 19:20:08 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Duan", "Boyan", ""], ["Ramdas", "Aaditya", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "1909.07501", "submitter": "Tianying Wang", "authors": "Tianying Wang and Alex Asher", "title": "Improved Semiparametric Analysis of Polygenic Gene-Environment\n  Interactions in Case-Control Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard logistic regression analysis of case-control data has low power to\ndetect gene-environment interactions, but until recently it was the only method\nthat could be used on complex polygenic data for which parametric\ndistributional models are not feasible. Under the assumption of\ngene-environment independence in the underlying population, Stalder et al.\n(2017, Biometrika, 104, 801-812) developed a retrospective method that treats\nboth genetic and environmental variables nonparametrically. However, the\nmathematical symmetry of genetic and environmental variables is overlooked. We\npropose an improvement to the method of Stalder et al. (2017) that increases\nthe efficiency of the estimates with no additional assumptions and modest\ncomputational cost. This improvement is achieved by treating the genetic and\nenvironmental variables symmetrically to generate two sets of parameter\nestimates that are combined to generate a more efficient estimate. We employ a\nsemiparametric framework to develop the asymptotic theory of the estimator,\nshow its asymptotic efficiency gain, and evaluate its performance via\nsimulation studies. The method is illustrated using data from a case-control\nstudy of breast cancer.\n", "versions": [{"version": "v1", "created": "Mon, 16 Sep 2019 22:07:49 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 07:54:31 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Wang", "Tianying", ""], ["Asher", "Alex", ""]]}, {"id": "1909.07550", "submitter": "Vincent Chin", "authors": "Vincent Chin, Jarod Y. L. Lee, Louise M. Ryan, Robert Kohn, Scott A.\n  Sisson", "title": "Multiclass classification of growth curves using random change points\n  and heterogeneous random effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faltering growth among children is a nutritional problem prevalent in low to\nmedium income countries; it is generally defined as a slower rate of growth\ncompared to a reference healthy population of the same age and gender. As\nfaltering is closely associated with reduced physical, intellectual and\neconomic productivity potential, it is important to identify faltered children\nand be able to characterise different growth patterns so that targeted\ntreatments can be designed and administered. We introduce a multiclass\nclassification model for growth trajectory that flexibly extends a current\nclassification approach called the broken stick model, which is a piecewise\nlinear model with breaks at fixed knot locations. Heterogeneity in growth\npatterns among children is captured using mixture distributed random effects,\nwhereby the mixture components determine the classification of children into\nsubgroups. The mixture distribution is modelled using a Dirichlet process\nprior, which avoids the need to choose the \"true\" number of mixture components,\nand allows this to be driven by the complexity of the data. Because children\nhave individual differences in the onset of growth stages, we introduce\nchild-specific random change points. Simulation results show that the random\nchange point model outperforms the broken stick model because it has fewer\nrestrictions on knot locations. We illustrate our model on a longitudinal birth\ncohort from the Healthy Birth, Growth and Development knowledge integration\nproject funded by the Bill and Melinda Gates Foundation. Analysis reveals 9\nsubgroups of children within the population which exhibit varying faltering\ntrends between birth and age one.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 02:05:07 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Chin", "Vincent", ""], ["Lee", "Jarod Y. L.", ""], ["Ryan", "Louise M.", ""], ["Kohn", "Robert", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1909.07686", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Javier \\'Alvarez-Li\\'ebana, Gonzalo\n  \\'Alvarez-P\\'erez, Wenceslao Gonz\\'alez-Manteiga", "title": "A goodness-of-fit test for the functional linear model with functional\n  response", "comments": "24 pages, 2 figures, 10 tables. Suplementary material: 2 pages, 1\n  figure", "journal-ref": "Scandinavian Journal of Statistics, 2021", "doi": "10.1111/sjos.12486", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Functional Linear Model with Functional Response (FLMFR) is one of the\nmost fundamental models to assess the relation between two functional random\nvariables. In this paper, we propose a novel goodness-of-fit test for the FLMFR\nagainst a general, unspecified, alternative. The test statistic is formulated\nin terms of a Cram\\'er-von Mises norm over a doubly-projected empirical process\nwhich, using geometrical arguments, yields an easy-to-compute weighted\nquadratic norm. A resampling procedure calibrates the test through a wild\nbootstrap on the residuals and the use of convenient computational procedures.\nAs a sideways contribution, and since the statistic requires a reliable\nestimator of the FLMFR, we discuss and compare several regularized estimators,\nproviding a new one specifically convenient for our test. The finite sample\nbehavior of the test is illustrated via a simulation study. Also, the new\nproposal is compared with previous significance tests. Two novel real datasets\nillustrate the application of the new test.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 09:55:25 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 10:24:54 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["\u00c1lvarez-Li\u00e9bana", "Javier", ""], ["\u00c1lvarez-P\u00e9rez", "Gonzalo", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""]]}, {"id": "1909.07719", "submitter": "Alain Desgagn\\'e", "authors": "Alain Desgagn\\'e", "title": "Efficient and Robust Estimation of Linear Regression with Normal Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear regression with normally distributed errors - including particular\ncases such as ANOVA, Student's t-test or location-scale inference - is a widely\nused statistical procedure. In this case the ordinary least squares estimator\npossesses remarkable properties but is very sensitive to outliers. Several\nrobust alternatives have been proposed, but there is still significant room for\nimprovement. This paper thus proposes an original method of estimation that\noffers the best efficiency simultaneously in the absence and the presence of\noutliers, both for the estimation of the regression coefficients and the scale\nparameter. The approach first consists in broadening the normal assumption of\nthe errors to a mixture of the normal and the filtered-log-Pareto (FLP), an\noriginal distribution designed to represent the outliers. The\nexpectation-maximization (EM) algorithm is then adapted and we obtain the N-FLP\nestimators of the regression coefficients, the scale parameter and the\nproportion of outliers, along with probabilities of each observation being an\noutlier. The performance of the N-FLP estimators is compared with the best\nalternatives in an extensive Monte Carlo simulation. The paper demonstrates\nthat this method of estimation can also be used for a complete robust\ninference, including confidence intervals, hypothesis testing and model\nselection.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 11:18:07 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Desgagn\u00e9", "Alain", ""]]}, {"id": "1909.07889", "submitter": "Yinchu Zhu", "authors": "Victor Chernozhukov, Kaspar W\\\"uthrich, Yinchu Zhu", "title": "Distributional conformal prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a robust method for constructing conditionally valid prediction\nintervals based on models for conditional distributions such as quantile and\ndistribution regression. Our approach can be applied to many important\nprediction problems including cross-sectional prediction, $k$-step-ahead\nforecasts, synthetic controls and counterfactual prediction, and individual\ntreatment effects prediction. Our method exploits the probability integral\ntransform and relies on permuting estimated ranks. Unlike regression residuals,\nranks are independent of the predictors, allowing us to construct conditionally\nvalid prediction intervals under arbitrary heteroskedasticity. We establish the\nconditional validity under consistent estimation and also provide theoretical\nperformance guarantees under model misspecification, overfitting, and with time\nseries data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 15:30:21 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 14:45:54 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Chernozhukov", "Victor", ""], ["W\u00fcthrich", "Kaspar", ""], ["Zhu", "Yinchu", ""]]}, {"id": "1909.08022", "submitter": "Carel F.W. Peeters", "authors": "Carel F.W. Peeters", "title": "Rotational Uniqueness Conditions Under Oblique Factor Correlation Metric", "comments": "Postprint, 5 pages", "journal-ref": "Psychometrika, 77 (2012): 288-292", "doi": "10.1007/s11336-012-9259-3", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an addendum to his seminal 1969 article J\\\"{o}reskog stated two sets of\nconditions for rotational identification of the oblique factor solution under\nutilization of fixed zero elements in the factor loadings matrix. These\ncondition sets, formulated under factor correlation and factor covariance\nmetrics, respectively, were claimed to be equivalent and to lead to global\nrotational uniqueness of the factor solution. It is shown here that the\nconditions for the oblique factor correlation structure need to be amended for\nglobal rotational uniqueness, and hence, that the condition sets are not\nequivalent in terms of unicity of the solution.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 18:46:39 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Peeters", "Carel F. W.", ""]]}, {"id": "1909.08024", "submitter": "Robert Krafty", "authors": "Jun Zhang, Greg J Siegle, Wendy D'Andrea, Robert T Krafty", "title": "Interpretable Principal Components Analysis for Multilevel Multivariate\n  Functional Data, with Application to EEG Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies collect functional data from multiple subjects that have both\nmultilevel and multivariate structures. An example of such data comes from\npopular neuroscience experiments where participants' brain activity is recorded\nusing modalities such as EEG and summarized as power within multiple\ntime-varying frequency bands within multiple electrodes, or brain regions.\nSummarizing the joint variation across multiple frequency bands for both\nwhole-brain variability between subjects, as well as location-variation within\nsubjects, can help to explain neural reactions to stimuli. This article\nintroduces a novel approach to conducting interpretable principal components\nanalysis on multilevel multivariate functional data that decomposes total\nvariation into subject-level and replicate-within-subject-level (i.e.\nelectrode-level) variation, and provides interpretable components that can be\nboth sparse among variates (e.g. frequency bands) and have localized support\nover time within each frequency band. The sparsity and localization of\ncomponents is achieved by solving an innovative rank-one based convex\noptimization problem with block Frobenius and matrix $L_1$-norm based\npenalties. The method is used to analyze data from a study to better understand\nreactions to emotional information in individuals with histories of trauma and\nthe symptom of dissociation, revealing new neurophysiological insights into how\nsubject- and electrode-level brain activity are associated with these\nphenomena.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 18:51:24 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Zhang", "Jun", ""], ["Siegle", "Greg J", ""], ["D'Andrea", "Wendy", ""], ["Krafty", "Robert T", ""]]}, {"id": "1909.08101", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul, Venkata K Jandhyala, and Stergios B Fotopoulos", "title": "Inference on the change point with the jump size near the boundary of\n  the region of detectability in high dimensional time series models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a projected least squares estimator for the change point parameter\nin a high dimensional time series model with a potential change point.\nImportantly we work under the setup where the jump size may be near the\nboundary of the region of detectability. The proposed methodology yields an\noptimal rate of convergence despite high dimensionality of the assumed model\nand a potentially diminishing jump size. The limiting distribution of this\nestimate is derived, thereby allowing construction of a confidence interval for\nthe location of the change point. A secondary near optimal estimate is proposed\nwhich is required for the implementation of the optimal projected least squares\nestimate. The prestep estimation procedure is designed to also agnostically\ndetect the case where no change point exists, thereby removing the need to\npretest for the existence of a change point for the implementation of the\ninference methodology. Our results are presented under a general positive\ndefinite spatial dependence setup, assuming no special structure on this\ndependence. The proposed methodology is designed to be highly scalable, and\napplicable to very large data. Theoretical results regarding detection and\nestimation consistency and the limiting distribution are numerically supported\nvia monte carlo simulations.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 21:08:15 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Kaul", "Abhishek", ""], ["Jandhyala", "Venkata K", ""], ["Fotopoulos", "Stergios B", ""]]}, {"id": "1909.08336", "submitter": "Jonas Crevecoeur", "authors": "Roel Verbelen, Katrien Antonio, Gerda Claeskens, Jonas Crevecoeur", "title": "Modeling the occurrence of events subject to a reporting delay via an EM\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A delay between the occurrence and the reporting of events often has\npractical implications such as for the amount of capital to hold for insurance\ncompanies, or for taking preventive actions in case of infectious diseases. The\naccurate estimation of the number of incurred but not (yet) reported events\nforms an essential part of properly dealing with this phenomenon. We review the\ncurrent practice for analysing such data and we present a flexible regression\nframework to jointly estimate the occurrence and reporting of events. By\nlinking this setting to an incomplete data problem, estimation is performed via\nan expectation-maximization algorithm. The resulting method is elegant, easy to\nunderstand and implement, and provides refined insights in the nowcasts. The\nproposed methodology is applied to a European general liability portfolio in\ninsurance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 10:24:10 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2021 12:45:29 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 09:51:40 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Verbelen", "Roel", ""], ["Antonio", "Katrien", ""], ["Claeskens", "Gerda", ""], ["Crevecoeur", "Jonas", ""]]}, {"id": "1909.08364", "submitter": "Adam Lane", "authors": "Adam Lane", "title": "Conditional Information and Inference in Response-Adaptive Allocation\n  Designs", "comments": "20 pages, 1 figure, 3 tables, 1 supplemental document 4 pages, 3\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Response-adaptive allocation designs refer to a class of designs where the\nprobability an observation is assigned to a treatment is changed throughout an\nexperiment based on the accrued responses. Such procedures result in random\ntreatment sample sizes. Most of the current literature considers unconditional\ninference procedures in the analysis of response-adaptive allocation designs.\nThe focus of this work is inference conditional on the observed treatment\nsample sizes. The inverse of information is a description of the large sample\nvariance of the parameter estimates. A simple form for the conditional\ninformation relative to unconditional information is derived. It is found that\nconditional information can be greater than unconditional information. It is\nalso shown that the variance of the conditional maximum likelihood estimate can\nbe less than the variance of the unconditional maximum likelihood estimate.\nFinally, a conditional bootstrap procedure is developed that, in the majority\nof cases examined, resulted in narrower confidence intervals than relevant\nunconditional procedures.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 11:21:49 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Lane", "Adam", ""]]}, {"id": "1909.08436", "submitter": "Mikael Escobar-Bach", "authors": "Mikael Escobar-Bach, Ingrid Van Keilegom", "title": "Nonparametric estimation of conditional cure models for heavy-tailed\n  distributions and under insufficient follow-up", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When analyzing time-to-event data, it often happens that some subjects do not\nexperience the event of interest. Survival models that take this feature into\naccount (called `cure models') have been developed in the presence of\ncovariates. However, the current literature on nonparametric cure models with\ncovariates cannot be applied when the follow-up is insufficient, i.e., when the\nright endpoint of the support of the censoring time is strictly smaller than\nthat of the survival time of the susceptible subjects. In this paper we attempt\nto fill this gap in the literature by proposing new estimators of the\nconditional cure rate and the conditional survival function using extrapolation\ntechniques coming from extreme value theory. We establish the asymptotic\nnormality of the proposed estimators, and show how the estimators work for\nsmall samples by means of a simulation study. We also illustrate their\npractical applicability through the analysis of data on the survival of colon\ncancer patients.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 13:26:58 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Escobar-Bach", "Mikael", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "1909.08447", "submitter": "Indranil Ghosh", "authors": "Indranil Ghosh, N.Balakrishnan", "title": "On compatibility/incompatibility of two discrete probability\n  distributions in the presence of incomplete specification", "comments": "19 pages article. arXiv admin note: substantial text overlap with\n  arXiv:1711.00608", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional specification of distributions is a developing area with many\napplications. In the finite discrete case, a variety of compatible conditions\ncan be derived. In this paper, we propose an alternative approach to study the\ncompatibility of two conditional probability distributions under the finite\ndiscrete set up. A technique based on rank-based criterion is shown to be\nparticularly convenient for identifying compatible distributions corresponding\nto complete conditional specification, including the case with zeros. The\nproposed methods are finally illustrated with several examples.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 13:41:20 GMT"}], "update_date": "2019-09-19", "authors_parsed": [["Ghosh", "Indranil", ""], ["Balakrishnan", "N.", ""]]}, {"id": "1909.08498", "submitter": "Mohammad Arashi", "authors": "M. Taavoni and M. Arashi", "title": "High-dimensional generalized semiparametric model for longitudinal data", "comments": "25 pages, 3 Figures, 4 Tables-The supplementary file includes the\n  proofs, but not added here", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimation in the generalized\nsemiparametric model for longitudinal data when the number of parameters\ndiverges with the sample size. A penalization type of generalized estimating\nequation method is proposed, while we use the regression spline to approximate\nthe nonparametric component. The proposed procedure involves the specification\nof the posterior distribution of the random effects, which cannot be evaluated\nin a closed-form. However, it is possible to approximate this posterior\ndistribution by producing random draws from the distribution using a Metropolis\nalgorithm. Under some regularity conditions, the resulting estimators enjoy the\noracle properties, under the high-dimensional regime. Simulation studies are\ncarried out to assess the performance of our proposed method, and two real data\nsets are analyzed to illustrate the procedure.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 15:23:26 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2020 16:32:22 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Taavoni", "M.", ""], ["Arashi", "M.", ""]]}, {"id": "1909.08579", "submitter": "Zad Rafi", "authors": "Zad Rafi and Sander Greenland", "title": "Semantic and Cognitive Tools to Aid Statistical Science: Replace\n  Confidence and Significance by Compatibility and Surprise", "comments": "22 pages; 5 figures; 2 tables; 94 references; Published at BMC\n  Medical Research Methodology", "journal-ref": "BMC Med Res Methodol 20, 244 (2020)", "doi": "10.1186/s12874-020-01105-9", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often misinterpret and misrepresent statistical outputs. This\nabuse has led to a large literature on modification or replacement of testing\nthresholds and $P$-values with confidence intervals, Bayes factors, and other\ndevices. Because the core problems appear cognitive rather than statistical, we\nreview simple aids to statistical interpretations. These aids emphasize logical\nand information concepts over probability, and thus may be more robust to\ncommon misinterpretations than are traditional descriptions. We use the Shannon\ntransform of the $P$-value $p$, also known as the binary surprisal or $S$-value\n$s=-\\log_{2}(p)$, to measure the information supplied by the testing procedure,\nand to help calibrate intuitions against simple physical experiments like coin\ntossing. We also use tables or graphs of test statistics for alternative\nhypotheses, and interval estimates for different percentile levels, to thwart\nfallacies arising from arbitrary dichotomies. Finally, we reinterpret\n$P$-values and interval estimates in unconditional terms, which describe\ncompatibility of data with the entire set of analysis assumptions. We\nillustrate these methods with a reanalysis of data from an existing\nrecord-based cohort study. In line with other recent recommendations, we advise\nthat teaching materials and research reports discuss $P$-values as measures of\ncompatibility rather than significance, compute $P$-values for alternative\nhypotheses whenever they are computed for null hypotheses, and interpret\ninterval estimates as showing values of high compatibility with data, rather\nthan regions of confidence. Our recommendations emphasize cognitive devices for\ndisplaying the compatibility of the observed data with various hypotheses of\ninterest, rather than focusing on single hypothesis tests or interval\nestimates. We believe these simple reforms are well worth the minor effort they\nrequire.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 17:09:43 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 02:49:12 GMT"}, {"version": "v3", "created": "Sun, 22 Sep 2019 02:18:45 GMT"}, {"version": "v4", "created": "Fri, 19 Jun 2020 00:27:15 GMT"}, {"version": "v5", "created": "Wed, 8 Jul 2020 01:55:45 GMT"}, {"version": "v6", "created": "Tue, 29 Sep 2020 18:28:15 GMT"}, {"version": "v7", "created": "Thu, 1 Oct 2020 01:43:23 GMT"}], "update_date": "2020-10-02", "authors_parsed": [["Rafi", "Zad", ""], ["Greenland", "Sander", ""]]}, {"id": "1909.08583", "submitter": "Zad Rafi", "authors": "Sander Greenland and Zad Rafi", "title": "To Aid Scientific Inference, Emphasize Unconditional Descriptions of\n  Statistics", "comments": "11 pages; 1 figure; 47 references with added DOI hyperlinks", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have elsewhere reviewed proposals to reform terminology and improve\ninterpretations of conventional statistics by emphasizing logical and\ninformation concepts over probability concepts. We here give detailed reasons\nand methods for reinterpreting statistics (including but not limited to)\nP-values and interval estimates in unconditional terms, which describe\ncompatibility of observations with an entire set of analysis assumptions,\nrather than just a narrow target hypothesis. Such reinterpretations help avoid\noverconfident inferences whenever there is uncertainty about the assumptions\nused to derive and compute the statistical results. Examples of such\nassumptions include not only standard statistical modeling assumptions, but\nalso assumptions about absence of systematic errors, protocol violations, and\ndata corruption. Unconditional descriptions introduce uncertainty about such\nassumptions directly into statistical presentations of results, rather than\nleaving that only to the informal discussion that ensues. We thus view\nunconditional description as a vital component of good statistical training and\npresentation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 17:12:02 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 02:50:44 GMT"}, {"version": "v3", "created": "Sun, 22 Sep 2019 02:20:13 GMT"}, {"version": "v4", "created": "Wed, 8 Jul 2020 01:49:30 GMT"}, {"version": "v5", "created": "Tue, 17 Nov 2020 01:27:28 GMT"}, {"version": "v6", "created": "Sat, 27 Feb 2021 18:40:52 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Greenland", "Sander", ""], ["Rafi", "Zad", ""]]}, {"id": "1909.08733", "submitter": "Nabarun Deb", "authors": "Nabarun Deb and Bodhisattva Sen", "title": "Multivariate Rank-based Distribution-free Nonparametric Testing using\n  Measure Transportation", "comments": "77 pages, 5 figures, and 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general framework for distribution-free\nnonparametric testing in multi-dimensions, based on a notion of multivariate\nranks defined using the theory of measure transportation. Unlike other existing\nproposals in the literature, these multivariate ranks share a number of useful\nproperties with the usual one-dimensional ranks; most importantly, these ranks\nare distribution-free. This crucial observation allows us to design\nnonparametric tests that are exactly distribution-free under the null\nhypothesis. We demonstrate the applicability of this approach by constructing\nexact distribution-free tests for two classical nonparametric problems: (i)\ntesting for mutual independence between random vectors, and (ii) testing for\nthe equality of multivariate distributions. In particular, we propose\n(multivariate) rank versions of distance covariance (Sz\\'ekely et al., 2007)\nand energy statistic (Sz\\'ekely and Rizzo, 2013) for testing scenarios (i) and\n(ii) respectively. In both these problems, we derive the asymptotic null\ndistribution of the proposed test statistics. We further show that our tests\nare consistent against all fixed alternatives. Moreover, the proposed tests are\ntuning-free, computationally feasible and are well-defined under minimal\nassumptions on the underlying distributions (e.g., they do not need any moment\nassumptions). We also demonstrate the efficacy of these procedures via\nextensive simulations. In the process of analyzing the theoretical properties\nof our procedures, we end up proving some new results in the theory of measure\ntransportation and in the limit theory of permutation statistics using Stein's\nmethod for exchangeable pairs, which may be of independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 23:19:35 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 20:00:41 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Deb", "Nabarun", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1909.08763", "submitter": "Donatello Telesca", "authors": "John Shamshoian, Damla Senturk, Shafali Jeste, Donatello Telesca", "title": "Bayesian Analysis of Multidimensional Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-dimensional functional data arises in numerous modern scientific\nexperimental and observational studies. In this paper we focus on longitudinal\nfunctional data, a structured form of multidimensional functional data.\nOperating within a longitudinal functional framework we aim to capture low\ndimensional interpretable features. We propose a computationally efficient\nnonparametric Bayesian method to simultaneously smooth observed data, estimate\nconditional functional means and functional covariance surfaces. Statistical\ninference is based on Monte Carlo samples from the posterior measure through\nadaptive blocked Gibbs sampling. Several operative characteristics associated\nwith the proposed modeling framework are assessed comparatively in a simulated\nenvironment. We illustrate the application of our work in two case studies. The\nfirst case study involves age-specific fertility collected over time for\nvarious countries. The second case study is an implicit learning experiment in\nchildren with Autism Spectrum Disorder (ASD).\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 01:23:03 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Shamshoian", "John", ""], ["Senturk", "Damla", ""], ["Jeste", "Shafali", ""], ["Telesca", "Donatello", ""]]}, {"id": "1909.08988", "submitter": "Gr\\'egoire Clart\\'e", "authors": "Gr\\'egoire Clart\\'e, Antoine Diez, Jean Feydy", "title": "Collective Proposal Distributions for Nonlinear MCMC samplers:\n  Mean-Field Theory and Fast Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decades, various \"non-linear\" MCMC methods have arised. While\nappealing for their convergence speed and efficiency, their practical\nimplementation and theoretical study remain challenging. In this paper, we\nintroduce a large class of non-linear samplers that can be studied and\nsimulated as the mean-field limit of a system of interacting particles. The\npractical implementation we propose leverages the computational power of modern\nhardware (GPU).\n", "versions": [{"version": "v1", "created": "Wed, 18 Sep 2019 14:53:13 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 10:33:41 GMT"}, {"version": "v3", "created": "Fri, 2 Apr 2021 11:36:38 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Clart\u00e9", "Gr\u00e9goire", ""], ["Diez", "Antoine", ""], ["Feydy", "Jean", ""]]}, {"id": "1909.09111", "submitter": "Leo Polansky", "authors": "Leo Polansky, Ken B. Newman, Lara Mitchell", "title": "Improving inference for nonlinear state-space models of animal\n  population dynamics given biased sequential life stage data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models (SSMs) are a popular tool for modeling animal abundances.\nInference difficulties for simple linear SSMs are well known, particularly in\nrelation to simultaneous estimation of process and observation variances.\nSeveral remedies to overcome estimation problems have been studied for\nrelatively simple SSMs, but whether these challenges and proposed remedies\napply for nonlinear stage-structured SSMs, an important class of ecological\nmodels, is less well understood. Here we identify improvements for inference\nabout nonlinear stage-structured SSMs fit with biased sequential life stage\ndata. Theoretical analyses indicate parameter identifiability requires\ncovariates in the state processes. Simulation studies show that plugging in\nexternally estimated observation variances, as opposed to jointly estimating\nthem with other parameters, reduces bias and standard error of estimates. In\ncontrast to previous results for simple linear SSMs, strong confounding between\njointly estimated process and observation variance parameters was not found in\nthe models explored here. However, when observation variance was also estimated\nin the motivating case study, the resulting process variance estimates were\nimplausibly low (near-zero). As SSMs are used in increasingly complex ways,\nunderstanding when inference can be expected to be successful, and what aids\nit, becomes more important. Our study illustrates (i) the need for relevant\nprocess covariates and (ii) the benefits of using externally estimated\nobservation variances for inference for nonlinear stage-structured SSMs.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 17:38:24 GMT"}], "update_date": "2019-09-20", "authors_parsed": [["Polansky", "Leo", ""], ["Newman", "Ken B.", ""], ["Mitchell", "Lara", ""]]}, {"id": "1909.09261", "submitter": "Tianjian Zhou", "authors": "Tong Li, Tianjian Zhou, Kam-Wah Tsui, Lin Wei, Yuan Ji", "title": "Posterior Contraction Rate of Sparse Latent Feature Models with\n  Application to Proteomics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Indian buffet process (IBP) and phylogenetic Indian buffet process (pIBP)\ncan be used as prior models to infer latent features in a data set. The\ntheoretical properties of these models are under-explored, however, especially\nin high dimensional settings. In this paper, we show that under mild sparsity\ncondition, the posterior distribution of the latent feature matrix, generated\nvia IBP or pIBP priors, converges to the true latent feature matrix\nasymptotically. We derive the posterior convergence rate, referred to as the\ncontraction rate. We show that the convergence holds even when the\ndimensionality of the latent feature matrix increases with the sample size,\ntherefore making the posterior inference valid in high dimensional setting. We\ndemonstrate the theoretical results using computer simulation, in which the\nparallel-tempering Markov chain Monte Carlo method is applied to overcome\ncomputational hurdles. The practical utility of the derived properties is\ndemonstrated by inferring the latent features in a reverse phase protein arrays\n(RPPA) dataset under the IBP prior model. Software and dataset reported in the\nmanuscript are provided at http://www.compgenome.org/IBP.\n", "versions": [{"version": "v1", "created": "Thu, 19 Sep 2019 23:44:27 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Li", "Tong", ""], ["Zhou", "Tianjian", ""], ["Tsui", "Kam-Wah", ""], ["Wei", "Lin", ""], ["Ji", "Yuan", ""]]}, {"id": "1909.09302", "submitter": "Guan'ao Yan", "authors": "Jun Zhao, Guan'ao Yan and Yi Zhang", "title": "Robust Estimation and Shrinkage in Ultrahigh Dimensional Expectile\n  Regression with Heavy Tails and Variance Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional data subject to heavy-tailed phenomena and heterogeneity are\ncommonly encountered in various scientific fields and bring new challenges to\nthe classical statistical methods. In this paper, we combine the asymmetric\nsquare loss and huber-type robust technique to develop the robust expectile\nregression for ultrahigh dimensional heavy-tailed heterogeneous data. Different\nfrom the classical huber method, we introduce two different tuning parameters\non both sides to account for possibly asymmetry and allow them to diverge to\nreduce bias induced by the robust approximation. In the regularized framework,\nwe adopt the generally folded concave penalty function like the SCAD or MCP\npenalty for the seek of bias reduction. We investigate the finite sample\nproperty of the corresponding estimator and figure out how our method plays its\nrole to trades off the estimation accuracy against the heavy-tailed\ndistribution. Also, noting that the robust asymmetric loss function is\neverywhere differentiable, based on our theoretical study, we propose an\nefficient first-order optimization algorithm after locally linear approximation\nof the non-convex problem. Simulation studies under various distributions\ndemonstrates the satisfactory performances of our method in coefficient\nestimation, model selection and heterogeneity detection.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 03:16:33 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 13:19:05 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["Zhao", "Jun", ""], ["Yan", "Guan'ao", ""], ["Zhang", "Yi", ""]]}, {"id": "1909.09370", "submitter": "Sothea Has", "authors": "Aur\\'elie Fisher (LPSM UMR 8001), Mathilde Mougeot (CMLA, ENSIIE, LPSM\n  UMR 8001), Sothea Has (LPSM UMR 8001)", "title": "A clusterwise supervised learning procedure based on aggregation of\n  distances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, many machine learning procedures are available on the shelve and\nmay be used easily to calibrate predictive models on supervised data. However,\nwhen the input data consists of more than one unknown cluster, and when\ndifferent underlying predictive models exist, fitting a model is a more\nchallenging task. We propose, in this paper, a procedure in three steps to\nautomatically solve this problem. The KFC procedure aggregates different models\nadaptively on data. The first step of the procedure aims at catching the\nclustering structure of the input data, which may be characterized by several\nstatistical distributions. It provides several partitions, given the\nassumptions on the distributions. For each partition, the second step fits a\nspecific predictive model based on the data in each cluster. The overall model\nis computed by a consensual aggregation of the models corresponding to the\ndifferent partitions. A comparison of the performances on different simulated\nand real data assesses the excellent performance of our method in a large\nvariety of prediction problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 08:37:04 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2019 13:20:31 GMT"}, {"version": "v3", "created": "Fri, 15 Nov 2019 15:51:30 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Fisher", "Aur\u00e9lie", "", "LPSM UMR 8001"], ["Mougeot", "Mathilde", "", "CMLA, ENSIIE, LPSM\n  UMR 8001"], ["Has", "Sothea", "", "LPSM UMR 8001"]]}, {"id": "1909.09421", "submitter": "Matthew Ludkin", "authors": "Matthew Ludkin", "title": "Inference for a generalised stochastic block model with unknown number\n  of blocks and non-conjugate edge models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic block model (SBM) is a popular model for capturing community\nstructure and interaction within a network. Network data with non-Boolean edge\nweights is becoming commonplace; however, existing analysis methods convert\nsuch data to a binary representation to apply the SBM, leading to a loss of\ninformation. A generalisation of the SBM is considered, which allows edge\nweights to be modelled in their recorded state. An effective reversible jump\nMarkov chain Monte Carlo sampler is proposed for estimating the parameters and\nthe number of blocks for this generalised SBM. The methodology permits\nnon-conjugate distributions for edge weights, which enable more flexible\nmodelling than current methods as illustrated on synthetic data, a network of\nbrain activity and an email communication network.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 10:46:35 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2020 15:43:12 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Ludkin", "Matthew", ""]]}, {"id": "1909.09438", "submitter": "Jan-Frederik Mai", "authors": "Jan-Frederik Mai and Matthias Scherer", "title": "On the structure of exchangeable extreme-value copulas", "comments": null, "journal-ref": "Journal of Multivariate Analysis 180, 104670 [11 pages] (2020)", "doi": "10.1016/j.jmva.2020.104670", "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that the set of $d$-variate symmetric stable tail dependence\nfunctions, uniquely associated with exchangeable $d$-dimensional extreme-value\ncopulas, is a simplex and determine its extremal boundary. The subset of\nelements which arises as $d$-margins of the set of $(d+k)$-variate symmetric\nstable tail dependence functions is shown to be proper for arbitrary $k \\geq\n1$. Finally, we derive an intuitive and useful necessary condition for a\nbivariate extreme-value copula to arise as bi-margin of an exchangeable\nextreme-value copula of arbitrarily large dimension, and thus to be\nconditionally iid.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 11:53:25 GMT"}], "update_date": "2020-12-11", "authors_parsed": [["Mai", "Jan-Frederik", ""], ["Scherer", "Matthias", ""]]}, {"id": "1909.09591", "submitter": "Alexandre Thiery", "authors": "Aaron Myers, Alexandre H. Thiery, Kainan Wang, Tan Bui-Thanh", "title": "Sequential Ensemble Transform for Bayesian Inverse Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Sequential Ensemble Transform (SET) method, an approach for\ngenerating approximate samples from a Bayesian posterior distribution. The\nmethod explores the posterior distribution by solving a sequence of discrete\noptimal transport problems to produce a series of transport plans which map\nprior samples to posterior samples. We prove that the sequence of Dirac mixture\ndistributions produced by the SET method converges weakly to the true posterior\nas the sample size approaches infinity. Furthermore, our numerical results\nindicate that, when compared to standard Sequential Monte Carlo (SMC) methods,\nthe SET approach is more robust to the choice of Markov mutation kernels and\nrequires less computational efforts to reach a similar accuracy when used to\nexplore complex posterior distributions. Finally, we describe adaptive schemes\nthat allow to completely automate the use of the SET method.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 16:13:47 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 07:29:45 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Myers", "Aaron", ""], ["Thiery", "Alexandre H.", ""], ["Wang", "Kainan", ""], ["Bui-Thanh", "Tan", ""]]}, {"id": "1909.09603", "submitter": "Daniel Rojas-D\\'iaz", "authors": "Daniel Rojas-Diaz and Alexandra Catano-Lopez and Carlos M.\n  Velez-Sanchez", "title": "A novel algorithm for confidence sub-contour box estimation: an\n  alternative to traditional confidence intervals", "comments": "17 pages, 8 figures, 6 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The factor estimation process is a really challenging task for non-linear\nmodels. Even whether researchers manage to successfully estimate model factors,\nthey still must estimate their confidence intervals, which could require a high\ncomputational cost to turn them into informative measures. Some methods in the\nliterature attempt to estimate regions within the estimation search space where\nfactors may jointly exist and fit the real data (confidence contours), however,\nits estimation process raises several issues as the number of factors\nincreases. Hence, in this paper, we focus on the estimation of a subregion\nwithin the confidence contour that we called as Confidence Subcontour Box\n(CSB). We proposed two main algorithms for CSB estimation, as well as its\ninterpretation and validation. Given the way we estimated CSB, we expected and\nvalidated some useful properties of this new kind of confidence interval: a\nuser-defined uncertainty level, asymmetrical intervals, sensitivity assessment\nrelated to the interval length for each factor, and the identification of\ntrue-influential factors.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 16:34:56 GMT"}, {"version": "v2", "created": "Tue, 1 Oct 2019 14:55:05 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2019 00:17:36 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Rojas-Diaz", "Daniel", ""], ["Catano-Lopez", "Alexandra", ""], ["Velez-Sanchez", "Carlos M.", ""]]}, {"id": "1909.09611", "submitter": "Rachel Nethery", "authors": "Rachel C. Nethery, Fabrizia Mealli, Jason D. Sacks, Francesca Dominici", "title": "Causal inference and machine learning approaches for evaluation of the\n  health impacts of large-scale air quality regulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a causal inference approach to estimate the number of adverse\nhealth events prevented by large-scale air quality regulations via changes in\nexposure to multiple pollutants. This approach is motivated by regulations that\nimpact pollution levels in all areas within their purview. We introduce a\ncausal estimand called the Total Events Avoided (TEA) by the regulation,\ndefined as the difference in the expected number of health events under the\nno-regulation pollution exposures and the observed number of health events\nunder the with-regulation pollution exposures. We propose a matching method and\na machine learning method that leverage high-resolution, population-level\npollution and health data to estimate the TEA. Our approach improves upon\ntraditional methods for regulation health impact analyses by clarifying the\ncausal identifying assumptions, utilizing population-level data, minimizing\nparametric assumptions, and considering the impacts of multiple pollutants\nsimultaneously. To reduce model-dependence, the TEA estimate captures health\nimpacts only for units in the data whose anticipated no-regulation features are\nwithin the support of the observed with-regulation data, thereby providing a\nconservative but data-driven assessment to complement traditional parametric\napproaches. We apply these methods to investigate the health impacts of the\n1990 Clean Air Act Amendments in the US Medicare population.\n", "versions": [{"version": "v1", "created": "Sun, 15 Sep 2019 20:40:52 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Nethery", "Rachel C.", ""], ["Mealli", "Fabrizia", ""], ["Sacks", "Jason D.", ""], ["Dominici", "Francesca", ""]]}, {"id": "1909.09681", "submitter": "H{\\aa}kon Otneim", "authors": "H{\\aa}kon Otneim and Dag Tj{\\o}stheim", "title": "The Locally Gaussian Partial Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the dependence structure for jointly Gaussian variables\ncan be fully captured using correlations, and that the conditional dependence\nstructure in the same way can be described using partial correlations. The\npartial orrelation does not, however, characterize conditional dependence in\nmany non-Gaussian populations. This paper introduces the local Gaussian partial\ncorrelation (LGPC), a new measure of conditional dependence. It is a local\nversion of the partial correlation coefficient that characterizes conditional\ndependence in a large class of populations. It has some useful and novel\nproperties besides: The LGPC reduces to the ordinary partial correlation for\njointly normal variables, and it distinguishes between positive and negative\nconditional dependence. Furthermore, the LGPC can be used to study departures\nfrom conditional independence in specific parts of the distribution. We provide\nseveral examples of this, both simulated and real, and derive estimation theory\nunder a local likelihood framework. Finally, we indicate how the LGPC can be\nused to construct a powerful test for conditional independence, which, again,\ncan be used to detect Granger causality in time series.\n", "versions": [{"version": "v1", "created": "Fri, 20 Sep 2019 19:06:07 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Otneim", "H\u00e5kon", ""], ["Tj\u00f8stheim", "Dag", ""]]}, {"id": "1909.09859", "submitter": "Thomas Boquet", "authors": "Thomas Boquet, Laure Delisle, Denis Kochetkov, Nathan Schucher,\n  Parmida Atighehchian, Boris Oreshkin, Julien Cornebise", "title": "DECoVaC: Design of Experiments with Controlled Variability Components", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reproducible research in Machine Learning has seen a salutary abundance of\nprogress lately: workflows, transparency, and statistical analysis of\nvalidation and test performance. We build on these efforts and take them\nfurther. We offer a principled experimental design methodology, based on linear\nmixed models, to study and separate the effects of multiple factors of\nvariation in machine learning experiments. This approach allows to account for\nthe effects of architecture, optimizer, hyper-parameters, intentional\nrandomization, as well as unintended lack of determinism across reruns. We\nillustrate that methodology by analyzing Matching Networks, Prototypical\nNetworks and TADAM on the miniImagenet dataset.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 17:41:12 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Boquet", "Thomas", ""], ["Delisle", "Laure", ""], ["Kochetkov", "Denis", ""], ["Schucher", "Nathan", ""], ["Atighehchian", "Parmida", ""], ["Oreshkin", "Boris", ""], ["Cornebise", "Julien", ""]]}, {"id": "1909.10060", "submitter": "John W. Jackson", "authors": "John W. Jackson", "title": "Meaningful causal decompositions in health equity research: definition,\n  identification, and estimation through a weighting framework", "comments": "39 pages, 1 Table and 1 Figure in the main text, 1 Figure in the\n  Supplement. Notational system changed for clarity", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal decomposition analyses can help build the evidence base for\ninterventions that address health disparities (inequities). They ask how\ndisparities in outcomes may change under hypothetical intervention. Through\nstudy design and assumptions, they can rule out alternate explanations such as\nconfounding, selection-bias, and measurement error, thereby identifying\npotential targets for intervention. Unfortunately, the literature on causal\ndecomposition analysis and related methods have largely ignored equity concerns\nthat actual interventionists would respect, limiting their relevance and\npractical value. This paper addresses these concerns by explicitly considering\nwhat covariates the outcome disparity and hypothetical intervention adjust for\n(so-called allowable covariates) and the equity value judgements these choices\nconvey, drawing from the bioethics, biostatistics, epidemiology, and health\nservices research literatures. From this discussion, we generalize\ndecomposition estimands and formulae to incorporate allowable covariate sets,\nto reflect equity choices, while still allowing for adjustment of non-allowable\ncovariates needed to satisfy causal assumptions. For these general formulae, we\nprovide weighting-based estimators based on adaptations of\nratio-of-mediator-probability and inverse-odds-ratio weighting. We discuss when\nthese estimators reduce to already used estimators under certain equity value\njudgements, and a novel adaptation under other judgements.\n", "versions": [{"version": "v1", "created": "Sun, 22 Sep 2019 18:19:10 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2020 13:08:01 GMT"}, {"version": "v3", "created": "Tue, 15 Sep 2020 22:43:42 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Jackson", "John W.", ""]]}, {"id": "1909.10108", "submitter": "Yufan Li Mr.", "authors": "Yufan Li", "title": "Improve Orthogonal GARCH with Hidden Markov Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal Generalized Autoregressive Conditional Heteroskedasticity model\n(OGARCH) is widely used in finance industry to produce volatility and\ncorrelation forecasts. We show that the classic OGARCH model, nevertheless,\ntends to be too slow in reflecting sudden changes in market condition due to\nexcessive persistence of the integral univariate GARCH processes. To obtain\nmore flexibility to accommodate abrupt market changes, e.g. financial crisis,\nwe extend classic OGARCH model by incorporating a two-state Markov\nregime-switching GARCH process. This novel construction allows us to capture\nrecurrent systemic regime shifts. Empirical results show that this\ngeneralization resolves the problem of excessive persistency effectively and\ngreatly enhances OGARCH's ability to adapt to sudden market breaks while\npreserving OGARCH's most attractive features such as dimension reduction and\nmulti-step ahead forecasting. By constructing a global minimum variance\nportfolio (GMVP), we are able to demonstrate significant outperformance of the\nextended model over the classic OGARCH model and the commonly used\nExponentially Weighted Moving Average (EWMA) model. In addition, we show that\nthe extended model is superior to OGARCH and EWMA in terms of predictive\naccuracy.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 00:33:56 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Li", "Yufan", ""]]}, {"id": "1909.10143", "submitter": "Haolei Weng", "authors": "Rahul Mazumder and Haolei Weng", "title": "Computing the degrees of freedom of rank-regularized estimators and\n  cousins", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a low rank matrix from its linear measurements is a problem of\ncentral importance in contemporary statistical analysis. The choice of tuning\nparameters for estimators remains an important challenge from a theoretical and\npractical perspective. To this end, Stein's Unbiased Risk Estimate (SURE)\nframework provides a well-grounded statistical framework for degrees of freedom\nestimation. In this paper, we use the SURE framework to obtain degrees of\nfreedom estimates for a general class of spectral regularized matrix\nestimators, generalizing beyond the class of estimators that have been studied\nthus far. To this end, we use a result due to Shapiro (2002) pertaining to the\ndifferentiability of symmetric matrix valued functions, developed in the\ncontext of semidefinite optimization algorithms. We rigorously verify the\napplicability of Stein's lemma towards the derivation of degrees of freedom\nestimates; and also present new techniques based on Gaussian convolution to\nestimate the degrees of freedom of a class of spectral estimators to which\nStein's lemma is not directly applicable.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 03:44:09 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Mazumder", "Rahul", ""], ["Weng", "Haolei", ""]]}, {"id": "1909.10285", "submitter": "Abhik Ghosh PhD", "authors": "Amarnath Nandy, Ayanendranath Basu, Abhik Ghosh", "title": "Robust Inference for Skewed data in Health Sciences", "comments": "Pre-print Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health data are often not symmetric to be adequately modeled through the\nusual normal distributions; most of them exhibit skewed patterns. They can\nindeed be modeled better through the larger family of skew-normal distributions\ncovering both skewed and symmetric cases. However, the existing likelihood\nbased inference, that is routinely performed in these cases, is extremely\nnon-robust against data contamination/outliers. Since outliers are not uncommon\nin complex real-life experimental datasets, a robust methodology automatically\ntaking care of the noises in the data would be of great practical value to\nproduce stable and more precise research insights leading to better policy\nformulation. In this paper, we develop a class of robust estimators and testing\nprocedures for the family of skew-normal distributions using the minimum\ndensity power divergence approach with application to health data. In\nparticular, a robust procedure for testing of symmetry is discussed in the\npresence of outliers. Two efficient computational algorithms are discussed.\nBesides deriving the asymptotic and robustness theory for the proposed methods,\ntheir advantages and utilities are illustrated through simulations and a couple\nof real-life applications for health data of athletes from Australian Institute\nof Sports and AIDS clinical trial data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 11:15:05 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Nandy", "Amarnath", ""], ["Basu", "Ayanendranath", ""], ["Ghosh", "Abhik", ""]]}, {"id": "1909.10635", "submitter": "Shih-Ting Huang", "authors": "Shih-Ting Huang, Yannick D\\\"uren, Kristoffer H. Hellton and Johannes\n  Lederer", "title": "Tuning parameter calibration for prediction in personalized medicine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized medicine has become an important part of medicine, for instance\npredicting individual drug responses based on genomic information. However,\nmany current statistical methods are not tailored to this task, because they\noverlook the individual heterogeneity of patients. In this paper, we look at\npersonalized medicine from a linear regression standpoint. We introduce an\nalternative version of the ridge estimator and target individuals by\nestablishing a tuning parameter calibration scheme that minimizes prediction\nerrors of individual patients. In stark contrast, classical schemes such as\ncross-validation minimize prediction errors only on average. We show that our\npipeline is optimal in terms of oracle inequalities, fast, and highly effective\nboth in simulations and on real data.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 22:00:14 GMT"}, {"version": "v2", "created": "Fri, 27 Sep 2019 09:39:10 GMT"}, {"version": "v3", "created": "Wed, 2 Oct 2019 10:57:57 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Huang", "Shih-Ting", ""], ["D\u00fcren", "Yannick", ""], ["Hellton", "Kristoffer H.", ""], ["Lederer", "Johannes", ""]]}, {"id": "1909.10653", "submitter": "Wei Wu", "authors": "Glenna Schluck, Wei Wu, Anuj Srivastava", "title": "Intensity Estimation for Poisson Process with Compositional Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intensity estimation for Poisson processes is a classical problem and has\nbeen extensively studied over the past few decades. Practical observations,\nhowever, often contain compositional noise, i.e. a nonlinear shift along the\ntime axis, which makes standard methods not directly applicable. The key\nchallenge is that these observations are not \"aligned\", and registration\nprocedures are required for successful estimation. In this paper, we propose an\nalignment-based framework for positive intensity estimation. We first show that\nthe intensity function is area-preserved with respect to compositional noise.\nSuch a property implies that the time warping is only encoded in the normalized\nintensity, or density, function. Then, we decompose the estimation of the\nintensity by the product of the estimated total intensity and estimated\ndensity. The estimation of the density relies on a metric which measures the\nphase difference between two density functions. An asymptotic study shows that\nthe proposed estimation algorithm provides a consistent estimator for the\nnormalized intensity. We then extend the framework to estimating non-negative\nintensity functions. The success of the proposed estimation algorithms is\nillustrated using two simulations. Finally, we apply the new framework in a\nreal data set of neural spike trains, and find that the newly estimated\nintensities provide better classification accuracy than previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Sep 2019 23:49:08 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Schluck", "Glenna", ""], ["Wu", "Wei", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1909.10655", "submitter": "Li-Pang Chen", "authors": "Li-Pang Chen", "title": "Variable selection and estimation for the additive hazards model subject\n  to left-truncation, right-censoring and measurement error in covariates", "comments": "26 pages, 2 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional sparse modeling with censored survival data is of great\npractical importance, and several methods have been proposed for variable\nselection based on different models. However, the impact of biased sample\ncaused by left-truncation and covariates measurement error to variable\nselection is not fully explored. In this paper, we mainly focus on the additive\nhazards model and analyze the high-dimensional survival data subject to\nleft-truncation and measurement error in covariates. We develop the three-stage\nprocedure to correct the error effect, select variables, and estimate the\nparameters of interest simultaneously. Numerical studies are reported to assess\nthe performance of the proposed methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 00:00:24 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Chen", "Li-Pang", ""]]}, {"id": "1909.10678", "submitter": "Evan Martin", "authors": "Evan A Martin and Audrey Qiuyan Fu", "title": "A Bayesian Approach to Directed Acyclic Graphs with a Candidate Graph", "comments": "Included analyses for data from GEUVADIS and GTEX", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graphs represent the dependence structure among variables.\nWhen learning these graphs from data, different amounts of information may be\navailable for different edges. Although many methods have been developed to\nlearn the topology of these graphs, most of them do not provide a measure of\nuncertainty in the inference. We propose a Bayesian method, baycn (BAYesian\nCausal Network), to estimate the posterior probability of three states for each\nedge: present with one direction ($X \\rightarrow Y$), present with the opposite\ndirection ($X \\leftarrow Y$), and absent. Unlike existing Bayesian methods, our\nmethod requires that the prior probabilities of these states be specified, and\ntherefore provides a benchmark for interpreting the posterior probabilities. We\ndevelop a fast Metropolis-Hastings Markov chain Monte Carlo algorithm for the\ninference. Our algorithm takes as input the edges of a candidate graph, which\nmay be the output of another graph inference method and may contain false\nedges. In simulation studies our method achieves high accuracy with small\nvariation across different scenarios and is comparable or better than existing\nBayesian methods. We apply baycn to genomic data to distinguish the direct and\nindirect targets of genetic variants.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 01:51:55 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2019 00:25:47 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Martin", "Evan A", ""], ["Fu", "Audrey Qiuyan", ""]]}, {"id": "1909.10700", "submitter": "Aleksandr Aravkin", "authors": "Peng Zheng, Ryan Barber, Reed J.D. Sorensen, Christopher J.L. Murray,\n  and Aleksandr Y. Aravkin", "title": "Trimmed Constrained Mixed Effects Models: Formulations and Algorithms", "comments": "33 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed effects (ME) models inform a vast array of problems in the physical and\nsocial sciences, and are pervasive in meta-analysis. We consider ME models\nwhere the random effects component is linear. We then develop an efficient\napproach for a broad problem class that allows nonlinear measurements, priors,\nand constraints, and finds robust estimates in all of these cases using\ntrimming in the associated marginal likelihood.\n  The software accompanying this paper is disseminated as an open-source Python\npackage called LimeTr. LimeTr is able to recover results more accurately in the\npresence of outliers compared to available packages for both standard\nlongitudinal analysis and meta-analysis, and is also more computationally\nefficient than competing robust alternatives. Supplementary materials that\nreproduce the simulations, as well as run LimeTr and third party code are\navailable online. We also present analyses of global health data, where we use\nadvanced functionality of LimeTr, including constraints to impose monotonicity\nand concavity for dose-response relationships. Nonlinear observation models\nallow new analyses in place of classic approximations, such as log-linear\nmodels. Robust extensions in all analyses ensure that spurious data points do\nnot drive our understanding of either mean relationships or between-study\nheterogeneity.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 03:59:59 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2020 15:11:34 GMT"}], "update_date": "2020-10-28", "authors_parsed": [["Zheng", "Peng", ""], ["Barber", "Ryan", ""], ["Sorensen", "Reed J. D.", ""], ["Murray", "Christopher J. L.", ""], ["Aravkin", "Aleksandr Y.", ""]]}, {"id": "1909.10710", "submitter": "Shurong Zheng", "authors": "Jianqing Fan and Jianhua Guo and Shurong Zheng", "title": "Estimating Number of Factors by Adjusted Eigenvalues Thresholding", "comments": "35 pages; 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the number of common factors is an important and practical topic\nin high dimensional factor models. The existing literatures are mainly based on\nthe eigenvalues of the covariance matrix. Due to the incomparability of the\neigenvalues of the covariance matrix caused by heterogeneous scales of observed\nvariables, it is very difficult to give an accurate relationship between these\neigenvalues and the number of common factors. To overcome this limitation, we\nappeal to the correlation matrix and show surprisingly that the number of\neigenvalues greater than $1$ of population correlation matrix is the same as\nthe number of common factors under some mild conditions. To utilize such a\nrelationship, we study the random matrix theory based on the sample correlation\nmatrix in order to correct the biases in estimating the top eigenvalues and to\ntake into account of estimation errors in eigenvalue estimation. This leads us\nto propose adjusted correlation thresholding (ACT) for determining the number\nof common factors in high dimensional factor models, taking into account the\nsampling variabilities and biases of top sample eigenvalues. We also establish\nthe optimality of the proposed methods in terms of minimal signal strength and\noptimal threshold. Simulation studies lend further support to our proposed\nmethod and show that our estimator outperforms other competing methods in most\nof our testing cases.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 05:22:00 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Fan", "Jianqing", ""], ["Guo", "Jianhua", ""], ["Zheng", "Shurong", ""]]}, {"id": "1909.10734", "submitter": "Prashant Jha", "authors": "Subhra Sankar Dhar, Prashant Jha and Prabrisha Rakhshit", "title": "The Trimmed Mean in Non-parametric Regression Function Estimation", "comments": "37 pages, 9 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies a trimmed version of the Nadaraya-Watson estimator to\nestimate the unknown non-parametric regression function. The characterization\nof the estimator through minimization problem is established, and its pointwise\nasymptotic distribution is also derived. The robustness property of the\nproposed estimator is also studied through breakdown point. Besides, the\nasymptotic efficiency study along with an extensive simulation study shows that\nthis estimator performs well for various cases. The practicability of the\nestimator is shown for three benchmark real data as well.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 06:55:10 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 10:55:56 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Dhar", "Subhra Sankar", ""], ["Jha", "Prashant", ""], ["Rakhshit", "Prabrisha", ""]]}, {"id": "1909.10739", "submitter": "Prashant Jha", "authors": "Subhra Sankar Dhar, Prashant Jha and Aranyak Acharyya", "title": "On Variable Screening in Multiple Nonparametric Regression Model", "comments": "There were several modifications needed in Sections 3, 4, and\n  Appendix, due to errors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we study the problem of variable screening in multiple\nnonparametric regression model. The proposed methodology is based on the fact\nthat the partial derivative of the regression function with respect to the\nirrelevant variable should be negligible. The Statistical property of the\nproposed methodology is investigated under both cases : (i) when the variance\nof the error term is known, and (ii) when the variance of the error term is\nunknown. Moreover, we establish the practicality of our proposed methodology\nfor various simulated and real data related to interdisciplinary sciences such\nas Economics, Finance and other sciences.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 07:23:09 GMT"}, {"version": "v2", "created": "Mon, 17 Aug 2020 14:06:02 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2021 15:12:11 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Dhar", "Subhra Sankar", ""], ["Jha", "Prashant", ""], ["Acharyya", "Aranyak", ""]]}, {"id": "1909.10765", "submitter": "Alberto Pessia", "authors": "Alberto Pessia, Jing Tang", "title": "Numerical evaluation of the transition probability of the simple\n  birth-and-death process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simple (linear) birth-and-death process is a widely used stochastic model\nfor describing the dynamics of a population. When the process is observed\ndiscretely over time, despite the large amount of literature on the subject,\nlittle is known about formal estimator properties. Here we will show that its\napplication to observed data is further complicated by the fact that numerical\nevaluation of the well-known transition probability is an ill-conditioned\nproblem. To overcome this difficulty we will rewrite the transition probability\nin terms of a Gaussian hypergeometric function and subsequently obtain a\nthree-term recurrence relation for its accurate evaluation. We will also study\nthe properties of the hypergeometric function as a solution to the three-term\nrecurrence relation. We will then provide formulas for the gradient and Hessian\nof the log-likelihood function and conclude the article by applying our methods\nfor numerically computing maximum likelihood estimates in both simulated and\nreal dataset.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 08:59:49 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Pessia", "Alberto", ""], ["Tang", "Jing", ""]]}, {"id": "1909.10832", "submitter": "Laura Anderlucci", "authors": "Laura Anderlucci, Francesca Fortunato and Angela Montanari", "title": "High-dimensional clustering via Random Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we address the unsupervised classification issue by exploiting\nthe general idea of Random Projection Ensemble. Specifically, we propose to\ngenerate a set of low dimensional independent random projections and to perform\nmodel-based clustering on each of them. The top $B^*$ projections, i.e. the\nprojections which show the best grouping structure are then retained. The final\npartition is obtained by aggregating the clusters found in the projections via\nconsensus. The performances of the method are assessed on both real and\nsimulated datasets. The obtained results suggest that the proposal represents a\npromising tool for high-dimensional clustering.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 12:14:41 GMT"}, {"version": "v2", "created": "Mon, 23 Nov 2020 17:41:19 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Anderlucci", "Laura", ""], ["Fortunato", "Francesca", ""], ["Montanari", "Angela", ""]]}, {"id": "1909.10923", "submitter": "Nathanael Randriamihamison", "authors": "Nathana\\\"el Randriamihamison (MIAT INRA, IMT), Nathalie Vialaneix\n  (MIAT INRA), Pierre Neuvial (IMT)", "title": "Applicability and Interpretability of Hierarchical Agglomerative\n  Clustering With or Without Contiguity Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Agglomerative Classification (HAC) with Ward's linkage has been\nwidely used since its introduction in Ward (1963). The present article reviews\nthe different extensions of the method to various input data and the\nconstrained framework, while providing applicability conditions. In addition,\nvarious versions of the graphical representation of the results as a dendrogram\nare also presented and their properties are clarified. While some of these\nresults can sometimes be found in an heteroclite literature, we clarify and\ncomplete them all using a uniform background. In particular, this study reveals\nan important distinction between a consistency property of the dendrogram and\nthe absence of crossover within it. Finally, a simulation study shows that the\nconstrained version of HAC can sometimes provide more relevant results than its\nunconstrained version despite the fact that the latter optimizes the objective\ncriterion on a reduced set of solutions at each step. Overall, the article\nprovides comprehensive recommandations for the use of HAC and constrained HAC\ndepending on the input data as well as for the representation of the results.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 13:48:05 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Randriamihamison", "Nathana\u00ebl", "", "MIAT INRA, IMT"], ["Vialaneix", "Nathalie", "", "MIAT INRA"], ["Neuvial", "Pierre", "", "IMT"]]}, {"id": "1909.11009", "submitter": "Han Lin Shang", "authors": "Fearghal Kearney, Han Lin Shang, Lisa Sheenan", "title": "Implied volatility surface predictability: the case of commodity markets", "comments": "35 pages, 6 figures, 9 tables, to appear in Journal of Banking and\n  Finance", "journal-ref": "Journal of Banking & Finance, 2019, 108, 105657", "doi": "10.1016/j.jbankfin.2019.105657", "report-no": null, "categories": "q-fin.ST stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent literature seek to forecast implied volatility derived from equity,\nindex, foreign exchange, and interest rate options using latent factor and\nparametric frameworks. Motivated by increased public attention borne out of the\nfinancialization of futures markets in the early 2000s, we investigate if these\nextant models can uncover predictable patterns in the implied volatility\nsurfaces of the most actively traded commodity options between 2006 and 2016.\nAdopting a rolling out-of-sample forecasting framework that addresses the\ncommon multiple comparisons problem, we establish that, for energy and precious\nmetals options, explicitly modeling the term structure of implied volatility\nusing the Nelson-Siegel factors produces the most accurate forecasts.\n", "versions": [{"version": "v1", "created": "Sat, 21 Sep 2019 05:37:14 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Kearney", "Fearghal", ""], ["Shang", "Han Lin", ""], ["Sheenan", "Lisa", ""]]}, {"id": "1909.11161", "submitter": "Joshua Keller", "authors": "Joshua P. Keller and Adam A. Szpiro", "title": "Selecting a Scale for Spatial Confounding Adjustment", "comments": "22 pages, 6 figures", "journal-ref": "Journal of the Royal Statistical Society: Series A (2020) 183,\n  Part 3, 1121-1143", "doi": "10.1111/rssa.12556", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unmeasured, spatially-structured factors can confound associations between\nspatial environmental exposures and health outcomes. Adding flexible splines to\na regression model is a simple approach for spatial confounding adjustment, but\nthe spline degrees of freedom do not provide an easily interpretable spatial\nscale. We describe a method for quantifying the extent of spatial confounding\nadjustment in terms of the Euclidean distance at which variation is removed. We\ndevelop this approach for confounding adjustment with splines and using Fourier\nand wavelet filtering. We demonstrate differences in the spatial scales these\nbases can represent and provide a comparison of methods for selecting the\namount of confounding adjustment. We find the best performance for selecting\nthe amount of adjustment using an information criterion evaluated on an outcome\nmodel without exposure. We apply this method to spatial adjustment in an\nanalysis of particulate matter and blood pressure in a cohort of United States\nwomen.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 20:29:04 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Keller", "Joshua P.", ""], ["Szpiro", "Adam A.", ""]]}, {"id": "1909.11171", "submitter": "Rob Tibshirani", "authors": "Chenyang Zhong and Robert Tibshirani", "title": "Survival analysis as a classification problem", "comments": "15 pages, 8 figures, 3 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore a method for treating survival analysis as a\nclassification problem. The method uses a \"stacking\" idea that collects the\nfeatures and outcomes of the survival data in a large data frame, and then\ntreats it as a classification problem. In this framework, various statistical\nlearning algorithms (including logistic regression, random forests, gradient\nboosting machines and neural networks) can be applied to estimate the\nparameters and make predictions. For stacking with logistic regression, we show\nthat this approach is approximately equivalent to the Cox proportional hazards\nmodel with both theoretical analysis and simulation studies. For stacking with\nother machine learning algorithms, we show through simulation studies that our\nmethod can outperform Cox proportional hazards model in terms of estimated\nsurvival curves. This idea is not new, but we believe that it should be better\nknown by statistiicians and other data scientists.\n", "versions": [{"version": "v1", "created": "Tue, 24 Sep 2019 20:57:34 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 15:29:17 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Zhong", "Chenyang", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1909.11566", "submitter": "Carel F.W. Peeters", "authors": "Carel F.W. Peeters, Gerty J.L.M. Lensvelt-Mulders, Karin Lasthuizen", "title": "A Note on a Simple and Practical Randomized Response Framework for\n  Eliciting Sensitive Dichotomous & Quantitative Information", "comments": "Postprint, 11 pages, 1 figure", "journal-ref": "Sociological Methods & Research, 39 (2010): 283-296", "doi": "10.1177/0049124110378099", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many issues of interest to social scientists and policymakers are of a\nsensitive nature in the sense that they are intrusive, stigmatizing or\nincriminating to the respondent. This results in refusals to cooperate or\nevasive cooperation in studies using self-reports. In a seminal article Warner\nproposed to curb this problem by generating an artificial variability in\nresponses to inoculate the individual meaning of answers to sensitive\nquestions. This procedure was further developed and extended, and came to be\nknown as the randomized response (RR) technique. Here, we propose a unified\ntreatment for eliciting sensitive binary as well as quantitative information\nwith RR based on a model where the inoculating elements are provided for by the\nrandomization device. The procedure is simple and we will argue that its\nimplementation in a computer-assisted setting may have superior practical\ncapabilities.\n", "versions": [{"version": "v1", "created": "Tue, 17 Sep 2019 18:35:32 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Peeters", "Carel F. W.", ""], ["Lensvelt-Mulders", "Gerty J. L. M.", ""], ["Lasthuizen", "Karin", ""]]}, {"id": "1909.11640", "submitter": "Lucy Gao", "authors": "Lucy L. Gao, Daniela Witten, Jacob Bien", "title": "Testing for Association in Multi-View Network Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider data consisting of multiple networks, each\ncomprised of a different edge set on a common set of nodes. Many models have\nbeen proposed for the analysis of such multi-view network data under the\nassumption that the data views are closely related. In this paper, we provide\ntools for evaluating this assumption. In particular, we ask: given two networks\nthat each follow a stochastic block model, is there an association between the\nlatent community memberships of the nodes in the two networks? To answer this\nquestion, we extend the stochastic block model for a single network view to the\ntwo-view setting, and develop a new hypothesis test for the null hypothesis\nthat the latent community memberships in the two data views are independent. We\napply our test to protein-protein interaction data from the HINT database (Das\nand Hint, 2012). We find evidence of a weak association between the latent\ncommunity memberships of proteins defined with respect to binary interaction\ndata and the latent community memberships of proteins defined with respect to\nco-complex association data. We also extend this proposal to the setting of a\nnetwork with node covariates.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 17:41:51 GMT"}, {"version": "v2", "created": "Tue, 11 Feb 2020 19:32:51 GMT"}, {"version": "v3", "created": "Mon, 22 Mar 2021 20:13:00 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Gao", "Lucy L.", ""], ["Witten", "Daniela", ""], ["Bien", "Jacob", ""]]}, {"id": "1909.11696", "submitter": "Stefan Wager", "authors": "Stefan Wager", "title": "Cross-Validation, Risk Estimation, and Model Selection", "comments": "This note was prepared as a comment on a paper by Rosset and\n  Tibshirani, forthcoming in the Journal of the American Statistical\n  Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation is a popular non-parametric method for evaluating the\naccuracy of a predictive rule. The usefulness of cross-validation depends on\nthe task we want to employ it for. In this note, I discuss a simple\nnon-parametric setting, and find that cross-validation is asymptotically\nuninformative about the expected test error of any given predictive rule, but\nallows for asymptotically consistent model selection. The reason for this\nphenomenon is that the leading-order error term of cross-validation doesn't\ndepend on the model being evaluated, and so cancels out when we compare two\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 18:32:13 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Wager", "Stefan", ""]]}, {"id": "1909.11784", "submitter": "Achim Zeileis", "authors": "Nikolaus Umlauf, Nadja Klein, Thorsten Simon, Achim Zeileis", "title": "bamlss: A Lego Toolbox for Flexible Bayesian Regression (and Beyond)", "comments": "48 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the last decades, the challenges in applied regression and in predictive\nmodeling have been changing considerably: (1) More flexible model\nspecifications are needed as big(ger) data become available, facilitated by\nmore powerful computing infrastructure. (2) Full probabilistic modeling rather\nthan predicting just means or expectations is crucial in many applications. (3)\nInterest in Bayesian inference has been increasing both as an appealing\nframework for regularizing or penalizing model estimation as well as a natural\nalternative to classical frequentist inference. However, while there has been a\nlot of research in all three areas, also leading to associated software\npackages, a modular software implementation that allows to easily combine all\nthree aspects has not yet been available. For filling this gap, the R package\nbamlss is introduced for Bayesian additive models for location, scale, and\nshape (and beyond). At the core of the package are algorithms for\nhighly-efficient Bayesian estimation and inference that can be applied to\ngeneralized additive models (GAMs) or generalized additive models for location,\nscale, and shape (GAMLSS), also known as distributional regression. However,\nits building blocks are designed as \"Lego bricks\" encompassing various\ndistributions (exponential family, Cox, joint models, ...), regression terms\n(linear, splines, random effects, tensor products, spatial fields, ...), and\nestimators (MCMC, backfitting, gradient boosting, lasso, ...). It is\ndemonstrated how these can be easily recombined to make classical models more\nflexible or create new custom models for specific modeling challenges.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 21:31:02 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Umlauf", "Nikolaus", ""], ["Klein", "Nadja", ""], ["Simon", "Thorsten", ""], ["Zeileis", "Achim", ""]]}, {"id": "1909.11796", "submitter": "Terrance Savitsky", "authors": "Terrance D. Savitsky, Matthew R. Williams, Jingchen Hu", "title": "Bayesian Pseudo Posterior Mechanism under Asymptotic Differential\n  Privacy", "comments": "35 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian pseudo posterior mechanism to generate record-level\nsynthetic databases equipped with an $(\\epsilon,\\delta)-$ probabilistic\ndifferential privacy (pDP) guarantee, where $\\delta$ denotes the probability\nthat any observed database exceeds $\\epsilon$. The pseudo posterior mechanism\nemploys a data record-indexed, risk-based weight vector with weight values $\\in\n[0, 1]$ that surgically downweight the likelihood contributions for high-risk\nrecords for model estimation and the generation of record-level synthetic data\nfor public release. The pseudo posterior synthesizer constructs a weight for\neach data record using the Lipschitz bound for that record under a log-pseudo\nlikelihood utility function that generalizes the exponential mechanism (EM)\nused to construct a formally private data generating mechanism. By selecting\nweights to remove likelihood contributions with non-finite log-likelihood\nvalues, we guarantee a finite local privacy guarantee for our pseudo posterior\nmechanism at every sample size. Our results may be applied to \\emph{any}\nsynthesizing model envisioned by the data disseminator in a computationally\ntractable way that only involves estimation of a pseudo posterior distribution\nfor parameters, $\\theta$, unlike recent approaches that use naturally-bounded\nutility functions implemented through the EM. We specify mild conditions that\nguarantee the asymptotic contraction of $\\delta$ to $0$ over the space of\ndatabases. We illustrate our pseudo posterior mechanism on the sensitive family\nincome variable from the Consumer Expenditure Surveys database published by the\nU.S. Bureau of Labor Statistics. We show that utility is better preserved in\nthe synthetic data for our pseudo posterior mechanism as compared to the EM,\nboth estimated using the same non-private synthesizer, due to our use of\ntargeted downweighting.\n", "versions": [{"version": "v1", "created": "Wed, 25 Sep 2019 22:24:25 GMT"}, {"version": "v2", "created": "Fri, 18 Oct 2019 22:09:44 GMT"}, {"version": "v3", "created": "Wed, 11 Dec 2019 21:23:52 GMT"}, {"version": "v4", "created": "Tue, 28 Jan 2020 17:44:29 GMT"}, {"version": "v5", "created": "Mon, 6 Apr 2020 20:45:45 GMT"}, {"version": "v6", "created": "Thu, 3 Sep 2020 15:04:06 GMT"}, {"version": "v7", "created": "Thu, 13 May 2021 20:59:58 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Savitsky", "Terrance D.", ""], ["Williams", "Matthew R.", ""], ["Hu", "Jingchen", ""]]}, {"id": "1909.11948", "submitter": "Lu Li", "authors": "Lu Li, Kai Tan, Xuerong Meggie Wen and Zhou Yu", "title": "Dynamic Partial Sufficient Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sufficient dimension reduction aims for reduction of dimensionality of a\nregression without loss of information by replacing the original predictor with\nits lower-dimensional subspace. Partial (sufficient) dimension reduction arises\nwhen the predictors naturally fall into two sets, X and W, and we seek\ndimension reduction on X alone while considering all predictors in the\nregression analysis. Though partial dimension reduction is a very general\nproblem, only very few research results are available when W is continuous. To\nthe best of our knowledge, these methods generally perform poorly when X and W\nare related, furthermore, none can deal with the situation where the reduced\nlower-dimensional subspace of X varies dynamically with W. In this paper, We\ndevelop a novel dynamic partial dimension reduction method, which could handle\nthe dynamic dimension reduction issue and also allows the dependency of X on W.\nThe asymptotic consistency of our method is investigated. Extensive numerical\nstudies and real data analysis show that our {\\it Dynamic Partial Dimension\nReduction} method has superior performance comparing to the existing methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 07:27:20 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Li", "Lu", ""], ["Tan", "Kai", ""], ["Wen", "Xuerong Meggie", ""], ["Yu", "Zhou", ""]]}, {"id": "1909.12049", "submitter": "Simon Bang Kristensen", "authors": "Simon Bang Kristensen and Bo Martin Bibby", "title": "A bivariate logistic regression model based on latent variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bivariate observations of binary and ordinal data arise frequently and\nrequire a bivariate modelling approach in cases where one is interested in\naspects of the marginal distributions as separate outcomes along with the\nassociation between the two. We consider methods for constructing such\nbivariate models with logistic marginals and propose a model based on the\nAli-Mikhail-Haq bivariate logistic distribution. We motivate the model as an\nextension of that based on the Gumbel type 2 distribution as considered by\nother authors and as a bivariate extension of the logistic distribution which\npreserves certain natural characteristics. Basic properties of the obtained\nmodel are studied and the proposed methods are illustrated through analysis of\ntwo data sets, one describing the trekking habits of Norwegian hikers, the\nother stemming from a cognitive experiment of visual recognition and awareness.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 12:33:34 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 10:55:26 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Kristensen", "Simon Bang", ""], ["Bibby", "Bo Martin", ""]]}, {"id": "1909.12078", "submitter": "Kolyan Ray", "authors": "Kolyan Ray and Botond Szabo", "title": "Debiased Bayesian inference for average treatment effects", "comments": "NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian approaches have become increasingly popular in causal inference\nproblems due to their conceptual simplicity, excellent performance and in-built\nuncertainty quantification ('posterior credible sets'). We investigate Bayesian\ninference for average treatment effects from observational data, which is a\nchallenging problem due to the missing counterfactuals and selection bias.\nWorking in the standard potential outcomes framework, we propose a data-driven\nmodification to an arbitrary (nonparametric) prior based on the propensity\nscore that corrects for the first-order posterior bias, thereby improving\nperformance. We illustrate our method for Gaussian process (GP) priors using\n(semi-)synthetic data. Our experiments demonstrate significant improvement in\nboth estimation accuracy and uncertainty quantification compared to the\nunmodified GP, rendering our approach highly competitive with the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 13:19:51 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["Ray", "Kolyan", ""], ["Szabo", "Botond", ""]]}, {"id": "1909.12112", "submitter": "Fabrizio Leisen", "authors": "Alan Riva Palacio and Fabrizio Leisen", "title": "Compound vectors of subordinators and their associated positive L\\'evy\n  copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  L\\'evy copulas are an important tool which can be used to build dependent\nL\\'evy processes. In a classical setting, they have been used to model\nfinancial applications. In a Bayesian framework they have been employed to\nintroduce dependent nonparametric priors which allow to model heterogeneous\ndata. This paper focuses on introducing a new class of L\\'evy copulas based on\na class of subordinators recently appeared in the literature, called\n\\textit{Compound Random Measures}. The well-known Clayton L\\'evy copula is a\nspecial case of this new class. Furthermore, we provide some novel results\nabout the underlying vector of subordinators such as a series representation\nand relevant moments. The article concludes with an application to a Danish\nfire dataset.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 14:00:53 GMT"}, {"version": "v2", "created": "Sat, 22 Feb 2020 09:57:54 GMT"}, {"version": "v3", "created": "Mon, 31 Aug 2020 06:55:30 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Palacio", "Alan Riva", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1909.12331", "submitter": "Weixing Song", "authors": "Jianhong Shi, Yujing Zhang, Ping Yu, Weixing Song", "title": "SIMEX Estimation in Parametric Modal Regression with Measurement Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a class of parametric modal regression models with measurement error, a\nsimulation extrapolation estimation procedure is proposed in this paper for\nestimating the modal regression coefficients. Large sample properties of the\nproposed estimation procedure, including the consistency and asymptotic\nnormality, are thoroughly investigated. Simulation studies are conducted to\nevaluate its robustness to potential outliers and the effectiveness in reducing\nthe bias caused by the measurement error.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 18:49:47 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 17:23:11 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["Shi", "Jianhong", ""], ["Zhang", "Yujing", ""], ["Yu", "Ping", ""], ["Song", "Weixing", ""]]}, {"id": "1909.12356", "submitter": "Fatima Batool", "authors": "Fatima Batool", "title": "An agglomerative hierarchical clustering method by optimizing the\n  average silhouette width", "comments": "43 pages, several figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  An agglomerative hierarchical clustering (AHC) framework and algorithm named\nHOSil based on a new linkage metric optimized by the average silhouette width\n(ASW) index is proposed. A conscientious investigation of various clustering\nmethods and estimation indices is conducted across a diverse verities of data\nstructures for three aims: a) clustering quality, b) clustering recovery, and\nc) estimation of number of clusters. HOSil has shown better clustering quality\nfor a range of artificial and real world data structures as compared to\nk-means, PAM, single, complete, average, Ward, McQuitty, spectral, model-based,\nand several estimation methods. It can identify clusters of various shapes\nincluding spherical, elongated, relatively small sized clusters, clusters\ncoming from different distributions including uniform, t, gamma and others.\nHOSil has shown good recovery for correct determination of the number of\nclusters. For some data structures only HOSil was able to identify the correct\nnumber of clusters.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 19:46:28 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Batool", "Fatima", ""]]}, {"id": "1909.12378", "submitter": "Thiago do R\\^ego Sousa", "authors": "Thiago do R\\^ego Sousa and Robert Stelzer", "title": "Moment based estimation for the multivariate COGARCH(1,1) process", "comments": "36 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For the multivariate COGARCH process, we obtain explicit expressions for the\nsecond-order structure of the \"squared returns\" process observed on an\nequidistant grid. Based on this, we present a generalized method of moments\nestimator for its parameters. Under appropriate moment and strong mixing\nconditions, we show that the resulting estimator is consistent and\nasymptotically normal. Sufficient conditions for strong mixing, stationarity\nand identifiability of the model parameters are discussed in detail. We\ninvestigate the finite sample behavior of the estimator in a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 20:39:24 GMT"}, {"version": "v2", "created": "Tue, 2 Feb 2021 11:38:41 GMT"}], "update_date": "2021-02-03", "authors_parsed": [["Sousa", "Thiago do R\u00eago", ""], ["Stelzer", "Robert", ""]]}, {"id": "1909.12384", "submitter": "Xiangrui Zeng", "authors": "Xiangrui Zeng, Hongyu Zheng", "title": "CS Sparse K-means: An Algorithm for Cluster-Specific Feature Selection\n  in High-Dimensional Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature selection is an important and challenging task in high dimensional\nclustering. For example, in genomics, there may only be a small number of genes\nthat are differentially expressed, which are informative to the overall\nclustering structure. Existing feature selection methods, such as Sparse\nK-means, rarely tackle the problem of accounting features that can only\nseparate a subset of clusters. In genomics, it is highly likely that a gene can\nonly define one subtype against all the other subtypes or distinguish a pair of\nsubtypes but not others. In this paper, we propose a K-means based clustering\nalgorithm that discovers informative features as well as which cluster pairs\nare separable by each selected features. The method is essentially an EM\nalgorithm, in which we introduce lasso-type constraints on each cluster pair in\nthe M step, and make the E step possible by maximizing the raw cross-cluster\ndistance instead of minimizing the intra-cluster distance. The results were\ndemonstrated on simulated data and a leukemia gene expression dataset.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 20:57:17 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 19:23:13 GMT"}], "update_date": "2019-10-07", "authors_parsed": [["Zeng", "Xiangrui", ""], ["Zheng", "Hongyu", ""]]}, {"id": "1909.12412", "submitter": "Zishen Xu", "authors": "Weilong Zhao, Zishen Xu, Yun Yang, Wei Wu", "title": "Model-based Statistical Depth with Applications to Functional Data", "comments": "37 pages (not including reference), 8 figures, 1 supplementary file\n  with 22 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical depth, a commonly used analytic tool in non-parametric\nstatistics, has been extensively studied for multivariate and functional\nobservations over the past few decades. Although various forms of depth were\nintroduced, they are mainly procedure-based whose definitions are independent\nof the generative model for observations. To address this problem, we introduce\na generative model-based approach to define statistical depth for both\nmultivariate and functional data. The proposed model-based depth framework\npermits simple computation via Monte Carlo sampling and improves the depth\nestimation accuracy. When applied to functional data, the proposed depth can\ncapture important features such as continuity, smoothness, or phase\nvariability, depending on the defining criteria. Specifically, we view\nfunctional data as realizations from a second-order stochastic process, and\ndefine their depths through the eigensystem of the covariance operator. These\nnew definitions are given through a proper metric related to the reproducing\nkernel Hilbert space of the covariance operator. We propose efficient\nalgorithms to compute the proposed depths and establish estimation consistency.\nThrough simulations and real data, we demonstrate that the proposed functional\ndepths reveal important statistical information such as those captured by the\nmedian and quantiles, and detect outliers.\n", "versions": [{"version": "v1", "created": "Thu, 26 Sep 2019 21:58:57 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Zhao", "Weilong", ""], ["Xu", "Zishen", ""], ["Yang", "Yun", ""], ["Wu", "Wei", ""]]}, {"id": "1909.12502", "submitter": "James Cavenaugh", "authors": "James Stephens Cavenaugh", "title": "Bootstrap Cross-validation Improves Model Selection in Pharmacometrics", "comments": "submitted to Statistics in Biopharmaceutical Research 16 pages + 2\n  tables + 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation assesses the predictive ability of a model, allowing one to\nrank models accordingly. Although the nonparametric bootstrap is almost always\nused to assess the variability of a parameter, it can be used as the basis for\ncross-validation if one keeps track of which items were not selected in a given\nbootstrap iteration. The items which were selected constitute the training data\nand the omitted items constitute the testing data. This bootstrap\ncross-validation (BS-CV) allows model selection to be made on the basis of\npredictive ability by comparing the median values of ensembles of summary\nstatistics of testing data. BS-CV is herein demonstrated using several summary\nstatistics, including a new one termed the simple metric for prediction quality\n(SMPQ), and using the warfarin data included in the Monolix distribution with\n13 pharmacokinetics (PK) models and 12 pharmacodynamics (PD) models. Of note\nthe two best PK models by AIC had the worst predictive ability, underscoring\nthe danger of using single realizations of a random variable (such as AIC) as\nthe basis for model selection. Using these data BS-CV was able to discriminate\nbetween similar indirect response models (inhibition of input versus\nstimulation of output). This could be useful in situations in which the\nmechanism of action is unknown (unlike warfarin).\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 05:40:40 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Cavenaugh", "James Stephens", ""]]}, {"id": "1909.12551", "submitter": "Pascal Schlosser", "authors": "Pascal Schlosser and Jochen Knaus and Maximilian Schmutz and Konstanze\n  D\\\"ohner and Christoph Plass and Lars Bullinger and Rainer Claus and Harald\n  Binder and Michael L\\\"ubbert and Martin Schumacher", "title": "Netboost: Boosting-supported network analysis improves high-dimensional\n  omics prediction in acute myeloid leukemia and Huntington's disease", "comments": null, "journal-ref": "IEEE/ACM Transactions on Computational Biology and Bioinformatics\n  (May 2020)", "doi": "10.1109/TCBB.2020.2983010", "report-no": null, "categories": "q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: State-of-the art selection methods fail to identify weak but\ncumulative effects of features found in many high-dimensional omics datasets.\nNevertheless, these features play an important role in certain diseases.\n  Results: We present Netboost, a three-step dimension reduction technique.\nFirst, a boosting-based filter is combined with the topological overlap measure\nto identify the essential edges of the network. Second, sparse hierarchical\nclustering is applied on the selected edges to identify modules and finally\nmodule information is aggregated by the first principal components. The primary\nanalysis is than carried out on these summary measures instead of the original\ndata. We demonstrate the application of the newly developed Netboost in\ncombination with CoxBoost for survival prediction of DNA methylation and gene\nexpression data from 180 acute myeloid leukemia (AML) patients and show, based\non cross-validated prediction error curve estimates, its prediction superiority\nover variable selection on the full dataset as well as over an alternative\nclustering approach. The identified signature related to chromatin modifying\nenzymes was replicated in an independent dataset of AML patients in the phase\nII AMLSG 12-09 study. In a second application we combine Netboost with Random\nForest classification and improve the disease classification error in\nRNA-sequencing data of Huntington's disease mice.\n  Conclusion: Netboost improves definition of predictive variables for survival\nanalysis and classification. It is a freely available Bioconductor R package\nfor dimension reduction and hypothesis generation in high-dimensional omics\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 08:21:35 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Schlosser", "Pascal", ""], ["Knaus", "Jochen", ""], ["Schmutz", "Maximilian", ""], ["D\u00f6hner", "Konstanze", ""], ["Plass", "Christoph", ""], ["Bullinger", "Lars", ""], ["Claus", "Rainer", ""], ["Binder", "Harald", ""], ["L\u00fcbbert", "Michael", ""], ["Schumacher", "Martin", ""]]}, {"id": "1909.12570", "submitter": "Antony Overstall", "authors": "Antony M. Overstall, James M. McGree", "title": "Bayesian decision-theoretic design of experiments under an alternative\n  model", "comments": "Supplementary material appears as an appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally Bayesian decision-theoretic design of experiments proceeds by\nchoosing a design to minimise expectation of a given loss function over the\nspace of all designs. The loss function encapsulates the aim of the experiment,\nand the expectation is taken with respect to the joint distribution of all\nunknown quantities implied by the statistical model that will be fitted to\nobserved responses. In this paper, an extended framework is proposed whereby\nthe expectation of the loss is taken with respect to a joint distribution\nimplied by an alternative statistical model. Motivation for this includes\npromoting robustness, ensuring computational feasibility and for allowing\nrealistic prior specification when deriving a design. To aid in exploring the\nnew framework, an asymptotic approximation to the expected loss under an\nalternative model is derived, and the properties of different loss functions\nare established. The framework is then demonstrated on a linear regression\nversus full-treatment model scenario, on estimating parameters of a non-linear\nmodel under model discrepancy and a cubic spline model under an unknown number\nof basis functions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 09:08:35 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2020 13:44:39 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 07:06:08 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Overstall", "Antony M.", ""], ["McGree", "James M.", ""]]}, {"id": "1909.12624", "submitter": "Bruno Ebner", "authors": "Philip D\\\"orr, Bruno Ebner, Norbert Henze", "title": "Testing multivariate normality by zeros of the harmonic oscillator in\n  characteristic function spaces", "comments": "29 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel class of affine invariant and consistent tests for normality\nin any dimension. The tests are based on a characterization of the standard\n$d$-variate normal distribution as the unique solution of an initial value\nproblem of a partial differential equation motivated by the harmonic\noscillator, which is a special case of a Schr\\\"odinger operator. We derive the\nasymptotic distribution of the test statistics under the hypothesis of\nnormality as well as under fixed and contiguous alternatives. The tests are\nconsistent against general alternatives, exhibit strong power performance for\nfinite samples, and they are applied to a classical data set due to R.A.\nFisher. The results can also be used for a neighborhood-of-model validation\nprocedure.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 11:20:52 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["D\u00f6rr", "Philip", ""], ["Ebner", "Bruno", ""], ["Henze", "Norbert", ""]]}, {"id": "1909.12688", "submitter": "Radu V. Craiu", "authors": "Evgeny Levi and Radu V Craiu", "title": "Assessing Data Support for the Simplifying Assumption in Bivariate\n  Conditional Copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers the problem of establishing data support for the\nsimplifying assumption (SA) in a bivariate conditional copula model. It is\nknown that SA greatly simplifies the inference for a conditional copula model,\nbut standard tools and methods for testing SA tend to not provide reliable\nresults. After splitting the observed data into training and test sets, the\nmethod proposed will use a flexible training data Bayesian fit to define tests\nbased on randomization and standard asymptotic theory. Theoretical\njustification for the method is provided and its performance is studied using\nsimulated data. The paper also discusses implementations in alternative models\nof interest, e.g. Gaussian, Logistic and Quantile regressions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 13:46:04 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Levi", "Evgeny", ""], ["Craiu", "Radu V", ""]]}, {"id": "1909.12710", "submitter": "Jan van Waaij PhD", "authors": "Jan van Waaij", "title": "Adaptive posterior contraction rates for empirical Bayesian drift\n  estimation of a diffusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to their conjugate posteriors, Gaussian process priors are attractive for\nestimating the drift of stochastic differential equations with continuous time\nobservations. However, their performance strongly depends on the choice of the\nhyper-parameters. We employ the marginal maximum likelihood estimator to\nestimate the scaling and/or smoothness parameter(s) of the prior and show that\nthe corresponding posterior has optimal rates of convergence. General theorems\ndo not apply directly to this model as the usual test functions are with\nrespect to a random Hellinger-type metric. We allow for continuous and\ndiscrete, one- and two-dimensional sets of hyper-parameters, where optimising\nover the two-dimensional set of smoothness and scaling hyper-parameters is\nshown to be beneficial in terms of the adaptive range.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 14:31:38 GMT"}, {"version": "v2", "created": "Mon, 3 Feb 2020 15:07:08 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["van Waaij", "Jan", ""]]}, {"id": "1909.12907", "submitter": "Xiaoyang Guo", "authors": "Xiaoyang Guo, Anuj Srivastava, Sudeep Sarkar", "title": "A Quotient Space Formulation for Generative Statistical Analysis of\n  Graphical Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex analyses involving multiple, dependent random quantities often lead\nto graphical models - a set of nodes denoting variables of interest, and\ncorresponding edges denoting statistical interactions between nodes. To develop\nstatistical analyses for graphical data, especially towards generative\nmodeling, one needs mathematical representations and metrics for matching and\ncomparing graphs, and subsequent tools, such as geodesics, means, and\ncovariances. This paper utilizes a quotient structure to develop efficient\nalgorithms for computing these quantities, leading to useful statistical tools,\nincluding principal component analysis, statistical testing, and modeling. We\ndemonstrate the efficacy of this framework using datasets taken from several\nproblem areas, including letters, biochemical structures, and social networks.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 13:29:04 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 01:33:17 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Guo", "Xiaoyang", ""], ["Srivastava", "Anuj", ""], ["Sarkar", "Sudeep", ""]]}, {"id": "1909.12974", "submitter": "Vadim Marmer", "authors": "Jun Ma, Vadim Marmer, Artyom Shneyerov, Pai Xu", "title": "Monotonicity-Constrained Nonparametric Estimation and Inference for\n  First-Price Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose a new nonparametric estimator for first-price auctions with\nindependent private values that imposes the monotonicity constraint on the\nestimated inverse bidding strategy. We show that our estimator has a smaller\nasymptotic variance than that of Guerre, Perrigne and Vuong's (2000) estimator.\nIn addition to establishing pointwise asymptotic normality of our estimator, we\nprovide a bootstrap-based approach to constructing uniform confidence bands for\nthe density function of latent valuations.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 22:29:17 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Ma", "Jun", ""], ["Marmer", "Vadim", ""], ["Shneyerov", "Artyom", ""], ["Xu", "Pai", ""]]}, {"id": "1909.12989", "submitter": "Yuke Zhu", "authors": "Linxi Fan, Yuke Zhu, Jiren Zhu, Zihua Liu, Orien Zeng, Anchit Gupta,\n  Joan Creus-Costa, Silvio Savarese, Li Fei-Fei", "title": "SURREAL-System: Fully-Integrated Stack for Distributed Deep\n  Reinforcement Learning", "comments": "Technical report of the SURREAL system. See more details at\n  https://surreal.stanford.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.RO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an overview of SURREAL-System, a reproducible, flexible, and\nscalable framework for distributed reinforcement learning (RL). The framework\nconsists of a stack of four layers: Provisioner, Orchestrator, Protocol, and\nAlgorithms. The Provisioner abstracts away the machine hardware and node pools\nacross different cloud providers. The Orchestrator provides a unified interface\nfor scheduling and deploying distributed algorithms by high-level description,\nwhich is capable of deploying to a wide range of hardware from a personal\nlaptop to full-fledged cloud clusters. The Protocol provides network\ncommunication primitives optimized for RL. Finally, the SURREAL algorithms,\nsuch as Proximal Policy Optimization (PPO) and Evolution Strategies (ES), can\neasily scale to 1000s of CPU cores and 100s of GPUs. The learning performances\nof our distributed algorithms establish new state-of-the-art on OpenAI Gym and\nRobotics Suites tasks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Sep 2019 23:54:42 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2019 06:19:15 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Fan", "Linxi", ""], ["Zhu", "Yuke", ""], ["Zhu", "Jiren", ""], ["Liu", "Zihua", ""], ["Zeng", "Orien", ""], ["Gupta", "Anchit", ""], ["Creus-Costa", "Joan", ""], ["Savarese", "Silvio", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1909.13017", "submitter": "Kabir Olorede Opeyemi", "authors": "Kabir Opeyemi Olorede and Waheed Babatunde Yahya", "title": "A New Covariance Estimator for Sufficient Dimension Reduction in\n  High-Dimensional and Undersized Sample Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": "JCGS-19-251", "categories": "stat.ME cs.CC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of standard sufficient dimension reduction methods for\nreducing the dimension space of predictors without losing regression\ninformation requires inverting the covariance matrix of the predictors. This\nhas posed a number of challenges especially when analyzing high-dimensional\ndata sets in which the number of predictors $\\mathit{p}$ is much larger than\nnumber of samples $n,~(n\\ll p)$. A new covariance estimator, called the\n\\textit{Maximum Entropy Covariance} (MEC) that addresses loss of covariance\ninformation when similar covariance matrices are linearly combined using\n\\textit{Maximum Entropy} (ME) principle is proposed in this work. By\nbenefitting naturally from slicing or discretizing range of the response\nvariable, y into \\textit{H} non-overlapping categories, $\\mathit{h_{1},\\ldots\n,h_{H}}$, MEC first combines covariance matrices arising from samples in each y\nslice $\\mathit{h\\in H}$ and then select the one that maximizes entropy under\nthe principle of maximum uncertainty. The MEC estimator is then formed from\nconvex mixture of such entropy-maximizing sample covariance $S_{\\mbox{mec}}$\nestimate and pooled sample covariance $\\mathbf{S}_{\\mathit{p}}$ estimate across\nthe $\\mathit{H}$ slices without requiring time-consuming covariance\noptimization procedures. MEC deals directly with singularity and instability of\nsample group covariance estimate in both regression and classification\nproblems. The efficiency of the MEC estimator is studied with the existing\nsufficient dimension reduction methods such as \\textit{Sliced Inverse\nRegression} (SIR) and \\textit{Sliced Average Variance Estimator} (SAVE) as\ndemonstrated on both classification and regression problems using real life\nLeukemia cancer data and customers' electricity load profiles from smart meter\ndata sets respectively.\n", "versions": [{"version": "v1", "created": "Sat, 28 Sep 2019 03:34:42 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Olorede", "Kabir Opeyemi", ""], ["Yahya", "Waheed Babatunde", ""]]}, {"id": "1909.13189", "submitter": "Bryon Aragam", "authors": "Xun Zheng, Chen Dan, Bryon Aragam, Pradeep Ravikumar, and Eric P. Xing", "title": "Learning Sparse Nonparametric DAGs", "comments": "To appear in AISTATS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a framework for learning sparse nonparametric directed acyclic\ngraphs (DAGs) from data. Our approach is based on a recent algebraic\ncharacterization of DAGs that led to a fully continuous program for score-based\nlearning of DAG models parametrized by a linear structural equation model\n(SEM). We extend this algebraic characterization to nonparametric SEM by\nleveraging nonparametric sparsity based on partial derivatives, resulting in a\ncontinuous optimization problem that can be applied to a variety of\nnonparametric and semiparametric models including GLMs, additive noise models,\nand index models as special cases. Unlike existing approaches that require\nspecific modeling choices, loss functions, or algorithms, we present a\ncompletely general framework that can be applied to general nonlinear models\n(e.g. without additive noise), general differentiable loss functions, and\ngeneric black-box optimization routines. The code is available at\nhttps://github.com/xunzheng/notears.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 02:20:56 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2020 23:21:15 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Zheng", "Xun", ""], ["Dan", "Chen", ""], ["Aragam", "Bryon", ""], ["Ravikumar", "Pradeep", ""], ["Xing", "Eric P.", ""]]}, {"id": "1909.13361", "submitter": "Christoph Kern", "authors": "Christoph Kern, Bernd Weiss, Jan-Philipp Kolb", "title": "A Longitudinal Framework for Predicting Nonresponse in Panel Surveys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonresponse in panel studies can lead to a substantial loss in data quality\ndue to its potential to introduce bias and distort survey estimates. Recent\nwork investigates the usage of machine learning to predict nonresponse in\nadvance, such that predicted nonresponse propensities can be used to inform the\ndata collection process. However, predicting nonresponse in panel studies\nrequires accounting for the longitudinal data structure in terms of model\nbuilding, tuning, and evaluation. This study proposes a longitudinal framework\nfor predicting nonresponse with machine learning and multiple panel waves and\nillustrates its application. With respect to model building, this approach\nutilizes information from multiple waves by introducing features that aggregate\nprevious (non)response patterns. Concerning model tuning and evaluation,\ntemporal cross-validation is employed by iterating through pairs of panel waves\nsuch that the training and test sets move in time. Implementing this approach\nwith data from a German probability-based mixed-mode panel shows that\naggregating information over multiple panel waves can be used to build\nprediction models with competitive and robust performance over all test waves.\n", "versions": [{"version": "v1", "created": "Sun, 29 Sep 2019 20:28:45 GMT"}, {"version": "v2", "created": "Sun, 3 Nov 2019 02:41:27 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Kern", "Christoph", ""], ["Weiss", "Bernd", ""], ["Kolb", "Jan-Philipp", ""]]}, {"id": "1909.13464", "submitter": "Sen Zhao", "authors": "Sen Zhao, Stephen Ottinger, Suzanne Peck, Christine Mac Donald, Ali\n  Shojaie", "title": "Network Differential Connectivity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying differences in networks has become a canonical problem in many\nbiological applications. Here, we focus on testing whether two Gaussian\ngraphical models are the same. Existing methods try to accomplish this goal by\neither directly comparing their estimated structures, or testing the null\nhypothesis that the partial correlation matrices are equal. However, estimation\napproaches do not provide measures of uncertainty, e.g., $p$-values, which are\ncrucial in drawing scientific conclusions. On the other hand, existing testing\napproaches could lead to misleading results in some cases. To address these\nshortcomings, we propose a qualitative hypothesis testing framework, which\ntests whether the connectivity patterns in the two networks are the same. Our\nframework is especially appropriate if the goal is to identify nodes or edges\nthat are differentially connected. No existing approach could test such\nhypotheses and provide corresponding measures of uncertainty, e.g., $p$-values.\nWe investigate theoretical and numerical properties of our proposal and\nillustrate its utility in biological applications. Theoretically, we show that\nunder appropriate conditions, our proposal correctly controls the type-I error\nrate in testing the qualitative hypothesis. Empirically, we demonstrate the\nperformance of our proposal using simulation datasets and applications in\ncancer genetics and brain imaging studies.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 05:26:20 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Zhao", "Sen", ""], ["Ottinger", "Stephen", ""], ["Peck", "Suzanne", ""], ["Mac Donald", "Christine", ""], ["Shojaie", "Ali", ""]]}, {"id": "1909.13469", "submitter": "Shubhadeep Chakraborty", "authors": "Shubhadeep Chakraborty and Xianyang Zhang", "title": "A New Framework for Distance and Kernel-based Metrics in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents new metrics to quantify and test for (i) the equality of\ndistributions and (ii) the independence between two high-dimensional random\nvectors. We show that the energy distance based on the usual Euclidean distance\ncannot completely characterize the homogeneity of two high-dimensional\ndistributions in the sense that it only detects the equality of means and the\ntraces of covariance matrices in the high-dimensional setup. We propose a new\nclass of metrics which inherits the desirable properties of the energy distance\nand maximum mean discrepancy/(generalized) distance covariance and the\nHilbert-Schmidt Independence Criterion in the low-dimensional setting and is\ncapable of detecting the homogeneity of/completely characterizing independence\nbetween the low-dimensional marginal distributions in the high dimensional\nsetup. We further propose t-tests based on the new metrics to perform\nhigh-dimensional two-sample testing/independence testing and study their\nasymptotic behavior under both high dimension low sample size (HDLSS) and high\ndimension medium sample size (HDMSS) setups. The computational complexity of\nthe t-tests only grows linearly with the dimension and thus is scalable to very\nhigh dimensional data. We demonstrate the superior power behavior of the\nproposed tests for homogeneity of distributions and independence via both\nsimulated and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 06:19:42 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Chakraborty", "Shubhadeep", ""], ["Zhang", "Xianyang", ""]]}, {"id": "1909.13499", "submitter": "Sylvain Arlot", "authors": "Sylvain Arlot (LMO, CELESTE)", "title": "Rejoinder on: Minimal penalties and the slope heuristics: a survey", "comments": null, "journal-ref": "Journal de la Societe Fran{\\c c}aise de Statistique, Societe\n  Fran{\\c c}aise de Statistique et Societe Mathematique de France, Vol 106,\n  No.3, 158-168. 2019", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This text is the rejoinder following the discussion of a survey paper about\nminimal penalties and the slope heuristics (Arlot, 2019. Minimal penalties and\nthe slope heuristics: a survey. Journal de la SFDS). While commenting on the\nremarks made by the discussants, it provides two new results about the slope\nheuristics for model selection among a collection of projection estimators in\nleast-squares fixed-design regression. First, we prove that the slope\nheuristics works even when all models are significantly biased. Second, when\nthe noise is Gaussian with a general dependence structure, we compute\nexpectations of key quantities, showing that the slope heuristics certainly is\nvalid in this setting also.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 08:04:46 GMT"}], "update_date": "2019-10-25", "authors_parsed": [["Arlot", "Sylvain", "", "LMO, CELESTE"]]}, {"id": "1909.13773", "submitter": "Gianmarco Alto\\`e", "authors": "Gianmarco Alto\\`e, Giulia Bertoldo, Claudio Zandonella Callegher,\n  Enrico Toffalini, Antonio Calcagn\\`i, Livio Finos and Massimiliano Pastore", "title": "Enhancing statistical inference in psychological research via\n  prospective and retrospective design analysis", "comments": "35 pages, 4 figures, manuscript currently under review", "journal-ref": "Front. Psychol. (2020). 10:2893", "doi": "10.3389/fpsyg.2019.02893", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the past two decades, psychological science has experienced an\nunprecedented replicability crisis which uncovered several issues. Among\nothers, statistical inference is too often viewed as an isolated procedure\nlimited to the analysis of data that have already been collected. We build on\nand further develop an idea proposed by Gelman and Carlin (2014) termed\n\"prospective and retrospective design analysis\". Rather than focusing only on\nthe statistical significance of a result and on the classical control of type I\nand type II errors, a comprehensive design analysis involves reasoning about\nwhat can be considered a plausible effect size. Furthermore, it introduces two\nrelevant inferential risks: the exaggeration ratio or Type M error (i.e., the\npredictable average overestimation of an effect that emerges as statistically\nsignificant), and the sign error or Type S error (i.e., the risk that a\nstatistically significant effect is estimated in the wrong direction). Another\nimportant aspect of design analysis is that it can be usefully carried out both\nin the planning phase of a study and for the evaluation of studies that have\nalready been conducted, thus increasing researchers' awareness during all\nphases of a research project. We use a familiar example in psychology where the\nresearcher is interested in analyzing the differences between two independent\ngroups. We examine the case in which the plausible effect size is formalized as\na single value, and propose a method in which uncertainty concerning the\nmagnitude of the effect is formalized via probability distributions. Through\nseveral examples, we show that even though a design analysis requires big\neffort, it has the potential to contribute to planning more robust and\nreplicable studies. Finally, future developments in the Bayesian framework are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Mon, 30 Sep 2019 15:14:00 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Alto\u00e8", "Gianmarco", ""], ["Bertoldo", "Giulia", ""], ["Callegher", "Claudio Zandonella", ""], ["Toffalini", "Enrico", ""], ["Calcagn\u00ec", "Antonio", ""], ["Finos", "Livio", ""], ["Pastore", "Massimiliano", ""]]}]