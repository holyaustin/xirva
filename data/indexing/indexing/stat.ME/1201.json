[{"id": "1201.0153", "submitter": "David R. Bickel", "authors": "Zhenyu Yang, Zuojing Li, David R. Bickel", "title": "Empirical Bayes estimation of posterior probabilities of enrichment", "comments": "exhaustive revision of Zhenyu Yang and David R. Bickel, \"Minimum\n  Description Length Measures of Evidence for Enrichment\" (December 2010).\n  COBRA Preprint Series. Article 76. http://biostats.bepress.com/cobra/ps/art76", "journal-ref": "A comparative study of five estimators of the local false\n  discovery rate,\" BMC Bioinformatics 14, art. 87 (2013)", "doi": "10.1186/1471-2105-14-87", "report-no": null, "categories": "q-bio.GN stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To interpret differentially expressed genes or other discovered features,\nresearchers conduct hypothesis tests to determine which biological categories\nsuch as those of the Gene Ontology (GO) are enriched in the sense of having\ndifferential representation among the discovered features. We study application\nof better estimators of the local false discovery rate (LFDR), a probability\nthat the biological category has equivalent representation among the\npreselected features.\n  We identified three promising estimators of the LFDR for detecting\ndifferential representation: a semiparametric estimator (SPE), a normalized\nmaximum likelihood estimator (NMLE), and a maximum likelihood estimator (MLE).\nWe found that the MLE performs at least as well as the SPE for on the order of\n100 of GO categories even when the ideal number of components in its underlying\nmixture model is unknown. However, the MLE is unreliable when the number of GO\ncategories is small compared to the number of PMM components. Thus, if the\nnumber of categories is on the order of 10, the SPE is a more reliable LFDR\nestimator. The NMLE depends not only on the data but also on a specified value\nof the prior probability of differential representation. It is therefore an\nappropriate LFDR estimator only when the number of GO categories is too small\nfor application of the other methods.\n  For enrichment detection, we recommend estimating the LFDR by the MLE given\nat least a medium number (~100) of GO categories, by the SPE given a small\nnumber of GO categories (~10), and by the NMLE given a very small number (~1)\nof GO categories.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2011 16:59:25 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Yang", "Zhenyu", ""], ["Li", "Zuojing", ""], ["Bickel", "David R.", ""]]}, {"id": "1201.0220", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Victor Chernozhukov and Christian Hansen", "title": "Inference for High-Dimensional Sparse Econometric Models", "comments": null, "journal-ref": "Advances in Economics and Econometrics, 10th World Congress of\n  Econometric Society, 2011", "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is about estimation and inference methods for high dimensional\nsparse (HDS) regression models in econometrics. High dimensional sparse models\narise in situations where many regressors (or series terms) are available and\nthe regression function is well-approximated by a parsimonious, yet unknown set\nof regressors. The latter condition makes it possible to estimate the entire\nregression function effectively by searching for approximately the right set of\nregressors. We discuss methods for identifying this set of regressors and\nestimating their coefficients based on $\\ell_1$-penalization and describe key\ntheoretical results. In order to capture realistic practical situations, we\nexpressly allow for imperfect selection of regressors and study the impact of\nthis imperfect selection on estimation and inference results. We focus the main\npart of the article on the use of HDS models and methods in the instrumental\nvariables model and the partially linear model. We present a set of novel\ninference results for these models and illustrate their use with applications\nto returns to schooling and growth regression.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 04:31:00 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Hansen", "Christian", ""]]}, {"id": "1201.0224", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Victor Chernozhukov and Christian Hansen", "title": "Inference on Treatment Effects After Selection Amongst High-Dimensional\n  Controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose robust methods for inference on the effect of a treatment variable\non a scalar outcome in the presence of very many controls. Our setting is a\npartially linear model with possibly non-Gaussian and heteroscedastic\ndisturbances. Our analysis allows the number of controls to be much larger than\nthe sample size. To make informative inference feasible, we require the model\nto be approximately sparse; that is, we require that the effect of confounding\nfactors can be controlled for up to a small approximation error by conditioning\non a relatively small number of controls whose identities are unknown. The\nlatter condition makes it possible to estimate the treatment effect by\nselecting approximately the right set of controls. We develop a novel\nestimation and uniformly valid inference method for the treatment effect in\nthis setting, called the \"post-double-selection\" method. Our results apply to\nLasso-type methods used for covariate selection as well as to any other model\nselection method that is able to find a sparse model with good approximation\nproperties.\n  The main attractive feature of our method is that it allows for imperfect\nselection of the controls and provides confidence intervals that are valid\nuniformly across a large class of models. In contrast, standard post-model\nselection estimators fail to provide uniform inference even in simple cases\nwith a small, fixed number of controls. Thus our method resolves the problem of\nuniform inference after model selection for a large, interesting class of\nmodels. We illustrate the use of the developed methods with numerical\nsimulations and an application to the effect of abortion on crime rates.\n", "versions": [{"version": "v1", "created": "Sat, 31 Dec 2011 04:37:19 GMT"}, {"version": "v2", "created": "Tue, 1 May 2012 01:30:10 GMT"}, {"version": "v3", "created": "Wed, 9 May 2012 15:28:20 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Hansen", "Christian", ""]]}, {"id": "1201.0381", "submitter": "Kun Chen", "authors": "Kun Chen, Hongbo Dong, Kung-Sik Chan", "title": "Reduced rank regression via adaptive nuclear norm penalization", "comments": "Presentation streamlined; numerical studies revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive nuclear-norm penalization is proposed for low-rank matrix\napproximation, by which we develop a new reduced-rank estimation method for the\ngeneral high-dimensional multivariate regression problems. The adaptive nuclear\nnorm of a matrix is defined as the weighted sum of the singular values of the\nmatrix. For example, the pre-specified weights may be some negative power of\nthe singular values of the data matrix (or its projection in regression\nsetting). The adaptive nuclear norm is generally non-convex under the natural\nrestriction that the weight decreases with the singular value. However, we show\nthat the proposed non-convex penalized regression method has a global optimal\nsolution obtained from an adaptively soft-thresholded singular value\ndecomposition. This new reduced-rank estimator is computationally efficient,\nhas continuous solution path and possesses better bias-variance property than\nits classical counterpart. The rank consistency and prediction/estimation\nperformance bounds of the proposed estimator are established under\nhigh-dimensional asymptotic regime. Simulation studies and an application in\ngenetics demonstrate that the proposed estimator has superior performance to\nseveral existing methods. The adaptive nuclear-norm penalization can also serve\nas a building block to study a broad class of singular value penalties.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2012 20:22:34 GMT"}, {"version": "v2", "created": "Sun, 23 Sep 2012 21:09:40 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Chen", "Kun", ""], ["Dong", "Hongbo", ""], ["Chan", "Kung-Sik", ""]]}, {"id": "1201.0400", "submitter": "Alexandre Patriota", "authors": "Alexandre G. Patriota", "title": "A classical measure of evidence for general null hypotheses", "comments": "26 pages, one figure and one table. Corrected version", "journal-ref": "Fuzzy sets and Systems, 233, 74-88, 2013", "doi": "10.1016/j.fss.2013.03.007", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In science, the most widespread statistical quantities are perhaps\n$p$-values. A typical advice is to reject the null hypothesis $H_0$ if the\ncorresponding p-value is sufficiently small (usually smaller than 0.05). Many\ncriticisms regarding p-values have arisen in the scientific literature. The\nmain issue is that in general optimal p-values (based on likelihood ratio\nstatistics) are not measures of evidence over the parameter space $\\Theta$.\nHere, we propose an \\emph{objective} measure of evidence for very general null\nhypotheses that satisfies logical requirements (i.e., operations on the subsets\nof $\\Theta$) that are not met by p-values (e.g., it is a possibility measure).\nWe study the proposed measure in the light of the abstract belief calculus\nformalism and we conclude that it can be used to establish objective states of\nbelief on the subsets of $\\Theta$. Based on its properties, we strongly\nrecommend this measure as an additional summary of significance tests. At the\nend of the paper we give a short listing of possible open problems.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jan 2012 23:31:13 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2012 22:01:58 GMT"}, {"version": "v3", "created": "Tue, 12 Mar 2013 14:13:10 GMT"}, {"version": "v4", "created": "Tue, 9 Apr 2013 22:32:05 GMT"}, {"version": "v5", "created": "Fri, 15 Nov 2013 12:50:32 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Patriota", "Alexandre G.", ""]]}, {"id": "1201.0480", "submitter": "Arnaud  Doucet", "authors": "Bernard Bercu, Pierre Del Moral and Arnaud Doucet", "title": "Fluctuations of Interacting Markov Chain Monte Carlo Methods", "comments": "Second revision of the INRIA-RR-6438 technical report (available\n  February 2008) at http://hal.inria.fr/docs/00/23/92/48/PDF/RR-6438.pdf To\n  appear in Stochastic Processes and Their Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multivariate central limit theorem for a general class of\ninteracting Markov chain Monte Carlo algorithms used to solve nonlinear\nmeasure-valued equations. These algorithms generate stochastic processes which\nbelong to the class of nonlinear Markov chains interacting with their empirical\noccupation measures. We develop an original theoretical analysis based on\nresolvent operators and semigroup techniques to analyze the fluctuations of\ntheir occupation measures around their limiting values.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2012 15:15:58 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Bercu", "Bernard", ""], ["Del Moral", "Pierre", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1201.0588", "submitter": "Tong Zhang", "authors": "Tong Zhang", "title": "Discussion of \"Is Bayes Posterior just Quick and Dirty Confidence?\" by\n  D. A. S. Fraser", "comments": "Published in at http://dx.doi.org/10.1214/11-STS352D the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 3, 326-328", "doi": "10.1214/11-STS352D", "report-no": "IMS-STS-STS352D", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Is Bayes Posterior just Quick and Dirty Confidence?\" by D. A.\nS. Fraser [arXiv:1112.5582]\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2012 08:30:24 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Zhang", "Tong", ""]]}, {"id": "1201.0611", "submitter": "D. A. S. Fraser", "authors": "D. A. S. Fraser", "title": "Rejoinder to \"Is Bayes Posterior just Quick and Dirty Confidence?\"", "comments": "Published in at http://dx.doi.org/10.1214/11-STS352REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 3, 329-331", "doi": "10.1214/11-STS352REJ", "report-no": "IMS-STS-STS352REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder to \"Is Bayes Posterior just Quick and Dirty Confidence?\" by D. A.\nS. Fraser [arXiv:1112.5582]\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2012 11:33:44 GMT"}], "update_date": "2012-01-04", "authors_parsed": [["Fraser", "D. A. S.", ""]]}, {"id": "1201.0646", "submitter": "Luca Martino", "authors": "Luca Martino and Jesse Read", "title": "On the flexibility of the design of Multiple Try Metropolis schemes", "comments": null, "journal-ref": "Computational Statistics, Volume 28, Issue 6, Pages: 2797-2823,\n  2013", "doi": "10.1007/s00180-013-0429-2", "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multiple Try Metropolis (MTM) method is a generalization of the classical\nMetropolis-Hastings algorithm in which the next state of the chain is chosen\namong a set of samples, according to normalized weights. In the literature,\nseveral extensions have been proposed. In this work, we show and remark upon\nthe flexibility of the design of MTM-type methods, fulfilling the detailed\nbalance condition. We discuss several possibilities and show different\nnumerical results.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2012 14:38:14 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2012 10:57:38 GMT"}, {"version": "v3", "created": "Thu, 28 Feb 2013 14:28:35 GMT"}], "update_date": "2014-05-20", "authors_parsed": [["Martino", "Luca", ""], ["Read", "Jesse", ""]]}, {"id": "1201.0651", "submitter": "Rainer Dahlhaus", "authors": "Rainer Dahlhaus, Istv\\'an Z. Kiss, and Jan C. Neddermeyer", "title": "On the relationship between the theory of cointegration and the theory\n  of phase synchronization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of cointegration has been a leading theory in econometrics with\npowerful applications to macroeconomics during the last decades. On the other\nhand the theory of phase synchronization for weakly coupled complex oscillators\nhas been one of the leading theories in physics for many years with many\napplications to different areas of science. For example, in neuroscience phase\nsynchronization is regarded as essential for functional coupling of different\nbrain regions. In an abstract sense both theories describe the dynamic\nfluctuation around some equilibrium. In this paper, we point out that there\nexists a very close connection between both theories. Apart from phase jumps, a\nstochastic version of the Kuramoto equations can be approximated by a\ncointegrated system of difference equations. As one consequence, the rich\ntheory on statistical inference for cointegrated systems can immediately be\napplied for statistical inference on phase synchronization based on empirical\ndata. This includes tests for phase synchronization, tests for unidirectional\ncoupling and the identification of the equilibrium from data including phase\nshifts. We study two examples on a unidirectionally coupled R\\\"ossler-Lorenz\nsystem and on electrochemical oscillators. The methods from cointegration may\nalso be used to investigate phase synchronization in complex networks.\nConversely, there are many interesting results on phase synchronization which\nmay inspire new research on cointegration.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2012 14:55:58 GMT"}, {"version": "v2", "created": "Wed, 4 Jan 2012 19:35:45 GMT"}, {"version": "v3", "created": "Fri, 11 May 2018 15:32:01 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Dahlhaus", "Rainer", ""], ["Kiss", "Istv\u00e1n Z.", ""], ["Neddermeyer", "Jan C.", ""]]}, {"id": "1201.0794", "submitter": "John Lafferty", "authors": "John Lafferty, Han Liu, Larry Wasserman", "title": "Sparse Nonparametric Graphical Models", "comments": "Published in at http://dx.doi.org/10.1214/12-STS391 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 4, 519-537", "doi": "10.1214/12-STS391", "report-no": "IMS-STS-STS391", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some nonparametric methods for graphical modeling. In the discrete\ncase, where the data are binary or drawn from a finite alphabet, Markov random\nfields are already essentially nonparametric, since the cliques can take only a\nfinite number of values. Continuous data are different. The Gaussian graphical\nmodel is the standard parametric model for continuous data, but it makes\ndistributional assumptions that are often unrealistic. We discuss two\napproaches to building more flexible graphical models. One allows arbitrary\ngraphs and a nonparametric extension of the Gaussian; the other uses kernel\ndensity estimation and restricts the graphs to trees and forests. Examples of\nboth methods are presented. We also discuss possible future research directions\nfor nonparametric graphical modeling.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 00:43:53 GMT"}, {"version": "v2", "created": "Mon, 7 Jan 2013 13:43:13 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Lafferty", "John", ""], ["Liu", "Han", ""], ["Wasserman", "Larry", ""]]}, {"id": "1201.0846", "submitter": "Camelia Goga", "authors": "Mohamed Chaouch and Camelia Goga", "title": "Using complex surveys to estimate the $L_1$-median of a functional\n  variable: application to electricity load curves", "comments": "to appear in International Statistical Review", "journal-ref": "International Statistical Review, 2012, Vol. 80, pages 40-59", "doi": "10.1111/j.1751-5823.2011.00172.x", "report-no": null, "categories": "stat.OT stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Mean profiles are widely used as indicators of the electricity consumption\nhabits of customers. Currently, in \\'Electricit\\'e De France (EDF), class load\nprofiles are estimated using point-wise mean function. Unfortunately, it is\nwell known that the mean is highly sensitive to the presence of outliers, such\nas one or more consumers with unusually high-levels of consumption. In this\npaper, we propose an alternative to the mean profile: the $L_1$-median profile\nwhich is more robust. When dealing with large datasets of functional data (load\ncurves for example), survey sampling approaches are useful for estimating the\nmedian profile avoiding storing the whole data. We propose here estimators of\nthe median trajectory using several sampling strategies and estimators. A\ncomparison between them is illustrated by means of a test population. We\ndevelop a stratification based on the linearized variable which substantially\nimproves the accuracy of the estimator compared to simple random sampling\nwithout replacement. We suggest also an improved estimator that takes into\naccount auxiliary information. Some potential areas for future research are\nalso highlighted.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 08:53:42 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Chaouch", "Mohamed", ""], ["Goga", "Camelia", ""]]}, {"id": "1201.0862", "submitter": "Zhilin Zhang", "authors": "Zhilin Zhang, Bhaskar D. Rao", "title": "Extension of SBL Algorithms for the Recovery of Block Sparse Signals\n  with Intra-Block Correlation", "comments": "Matlab codes can be downloaded at:\n  https://sites.google.com/site/researchbyzhang/bsbl, or\n  http://dsp.ucsd.edu/~zhilin/BSBL.html", "journal-ref": null, "doi": "10.1109/TSP.2013.2241055", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the recovery of block sparse signals and extend the framework in\ntwo important directions; one by exploiting signals' intra-block correlation\nand the other by generalizing signals' block structure. We propose two families\nof algorithms based on the framework of block sparse Bayesian learning (BSBL).\nOne family, directly derived from the BSBL framework, requires knowledge of the\nblock structure. Another family, derived from an expanded BSBL framework, is\nbased on a weaker assumption on the block structure, and can be used when the\nblock structure is completely unknown. Using these algorithms we show that\nexploiting intra-block correlation is very helpful in improving recovery\nperformance. These algorithms also shed light on how to modify existing\nalgorithms or design new ones to exploit such correlation and improve\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 10:01:32 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2012 11:41:31 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2012 10:51:34 GMT"}, {"version": "v4", "created": "Sun, 16 Dec 2012 08:27:30 GMT"}, {"version": "v5", "created": "Sun, 2 Nov 2014 05:55:59 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Zhang", "Zhilin", ""], ["Rao", "Bhaskar D.", ""]]}, {"id": "1201.0942", "submitter": "Anna Kucerova", "authors": "Eliska Janouchova and Anna Kucerova", "title": "Competitive Comparison of Optimal Designs of Experiments for\n  Sampling-based Sensitivity Analysis", "comments": "18 pages, 15 figures, 4 tables, CSC2011 special issue, corrected and\n  extended after the first review", "journal-ref": "Computers & Structures, 124, 47-60, 2013", "doi": "10.1016/j.compstruc.2013.04.009", "report-no": null, "categories": "cs.CE cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the numerical models of real-world structures are more precise,\nmore complex and, of course, more time-consuming. Despite the growth of a\ncomputational effort, the exploration of model behaviour remains a complex\ntask. The sensitivity analysis is a basic tool for investigating the\nsensitivity of the model to its inputs. One widely used strategy to assess the\nsensitivity is based on a finite set of simulations for a given sets of input\nparameters, i.e. points in the design space. An estimate of the sensitivity can\nbe then obtained by computing correlations between the input parameters and the\nchosen response of the model. The accuracy of the sensitivity prediction\ndepends on the choice of design points called the design of experiments. The\naim of the presented paper is to review and compare available criteria\ndetermining the quality of the design of experiments suitable for\nsampling-based sensitivity analysis.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2012 17:35:27 GMT"}, {"version": "v2", "created": "Thu, 6 Dec 2012 08:11:50 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Janouchova", "Eliska", ""], ["Kucerova", "Anna", ""]]}, {"id": "1201.1141", "submitter": "Joshua Landon", "authors": "Joshua Landon, Frank X. Lee, Nozer D. Singpurwalla", "title": "A Problem in Particle Physics and Its Bayesian Analysis", "comments": "Published in at http://dx.doi.org/10.1214/11-STS364 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 3, 352-368", "doi": "10.1214/11-STS364", "report-no": "IMS-STS-STS364", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a class of statistical problems that arises in several contexts, the\nLattice QCD problem of particle physics being one that has attracted the most\nattention. In essence, the problem boils down to the estimation of an infinite\nnumber of parameters from a finite number of equations, each equation being an\ninfinite sum of exponential functions. By introducing a latent parameter into\nthe QCD system, we are able to identify a pattern which tantamounts to reducing\nthe system to a telescopic series. A statistical model is then endowed on the\nseries, and inference about the unknown parameters done via a Bayesian\napproach. A computationally intensive Markov Chain Monte Carlo (MCMC) algorithm\nis invoked to implement the approach. The algorithm shares some parallels with\nthat used in the particle Kalman filter. The approach is validated against\nsimulated as well as data generated by a physics code pertaining to the quark\nmasses of protons. The value of our approach is that we are now able to answer\nquestions that could not be readily answered using some standard approaches in\nparticle physics. The structure of the Lattice QCD equations is not unique to\nphysics. Such architectures also appear in mathematical biology, nuclear\nmagnetic imaging, network analysis, ultracentrifuge, and a host of other\nrelaxation and time decay phenomena. Thus, the methodology of this paper should\nhave an appeal that transcends the Lattice QCD scenario which motivated us.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2012 12:10:05 GMT"}], "update_date": "2012-01-06", "authors_parsed": [["Landon", "Joshua", ""], ["Lee", "Frank X.", ""], ["Singpurwalla", "Nozer D.", ""]]}, {"id": "1201.1314", "submitter": "Christian P. Robert", "authors": "Christophe Andrieu, Simon Barthelme, Nicolas Chopin, Julien Cornebise,\n  Arnaud Doucet, Mark Girolami, Ioannis Kosmidis, Ajay Jasra, Anthony Lee,\n  Jean-Michel Marin, Pierre Pudlo, Christian P. Robert, Mohammed Sedki. and\n  Sumeetpal S. Singh", "title": "Some discussions of D. Fearnhead and D. Prangle's Read Paper\n  \"Constructing summary statistics for approximate Bayesian computation:\n  semi-automatic approximate Bayesian computation\"", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report is a collection of comments on the Read Paper of Fearnhead and\nPrangle (2011), to appear in the Journal of the Royal Statistical Society\nSeries B, along with a reply from the authors.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2012 22:02:07 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Andrieu", "Christophe", ""], ["Barthelme", "Simon", ""], ["Chopin", "Nicolas", ""], ["Cornebise", "Julien", ""], ["Doucet", "Arnaud", ""], ["Girolami", "Mark", ""], ["Kosmidis", "Ioannis", ""], ["Jasra", "Ajay", ""], ["Lee", "Anthony", ""], ["Marin", "Jean-Michel", ""], ["Pudlo", "Pierre", ""], ["Robert", "Christian P.", ""], ["Sedki.", "Mohammed", ""], ["Singh", "Sumeetpal S.", ""]]}, {"id": "1201.1356", "submitter": "Bruce E. Hansen", "authors": "Bruce E. Hansen", "title": "Discussion of \"Feature Matching in Time Series Modeling\" by Y. Xia and\n  H. Tong", "comments": "Published in at http://dx.doi.org/10.1214/11-STS345A the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 47-48", "doi": "10.1214/11-STS345A", "report-no": "IMS-STS-STS345A", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Feature Matching in Time Series Modeling\" by Y. Xia and H.\nTong [arXiv:1104.3073]\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 06:59:24 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Hansen", "Bruce E.", ""]]}, {"id": "1201.1367", "submitter": "Kung-Sik Chan", "authors": "Kung-Sik Chan, Ruey S. Tsay", "title": "Discussion of \"Feature Matching in Time Series Modeling\" by Y. Xia and\n  H. Tong", "comments": "Published in at http://dx.doi.org/10.1214/11-STS345B the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 53-56", "doi": "10.1214/11-STS345B", "report-no": "IMS-STS-STS345B", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Feature Matching in Time Series Modeling\" by Y. Xia and H.\nTong [arXiv:1104.3073]\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 08:24:26 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Chan", "Kung-Sik", ""], ["Tsay", "Ruey S.", ""]]}, {"id": "1201.1373", "submitter": "Edward L. Ionides", "authors": "Edward L. Ionides", "title": "Discussion of \"Feature Matching in Time Series Modeling\" by Y. Xia and\n  H. Tong", "comments": "Published in at http://dx.doi.org/10.1214/11-STS345C the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 49-52", "doi": "10.1214/11-STS345C", "report-no": "IMS-STS-STS345C", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Feature Matching in Time Series Modeling\" by Y. Xia and H.\nTong [arXiv:1104.3073]\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 09:01:17 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Ionides", "Edward L.", ""]]}, {"id": "1201.1375", "submitter": "Camelia Goga", "authors": "Camelia Goga and Anne Ruiz-Gazen", "title": "Efficient Estimation of Nonlinear Finite Population Parameters Using\n  Nonparametrics", "comments": null, "journal-ref": "Journal of the Royal Statistical Society, Series B, 2014, 76,\n  113-140", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Currently, the high-precision estimation of nonlinear parameters such as Gini\nindices, low-income proportions or other measures of inequality is particularly\ncrucial. In the present paper, we propose a general class of estimators for\nsuch parameters that take into account univariate auxiliary information assumed\nto be known for every unit in the population. Through a nonparametric\nmodel-assisted approach, we construct a unique system of survey weights that\ncan be used to estimate any nonlinear parameter associated with any study\nvariable of the survey, using a plug-in principle. Based on a rigorous\nfunctional approach and a linearization principle, the asymptotic variance of\nthe proposed estimators is derived, and variance estimators are shown to be\nconsistent under mild assumptions. The theory is fully detailed for penalized\nB-spline estimators together with suggestions for practical implementation and\nguidelines for choosing the smoothing parameters. The validity of the method is\ndemonstrated on data extracted from the French Labor Force Survey. Point and\nconfidence intervals estimation for the Gini index and the low-income\nproportion are derived. Theoretical and empirical results highlight our\ninterest in using a nonparametric approach versus a parametric one when\nestimating nonlinear parameters in the presence of auxiliary information.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 09:09:32 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2012 15:33:19 GMT"}], "update_date": "2014-07-01", "authors_parsed": [["Goga", "Camelia", ""], ["Ruiz-Gazen", "Anne", ""]]}, {"id": "1201.1376", "submitter": "Qiwei Yao", "authors": "Qiwei Yao", "title": "Discussion of \"Feature Matching in Time Series Modeling\" by Y. Xia and\n  H. Tong", "comments": "Published in at http://dx.doi.org/10.1214/11-STS345D the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 57-58", "doi": "10.1214/11-STS345D", "report-no": "IMS-STS-STS345D", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Feature Matching in Time Series Modeling\" by Y. Xia and H.\nTong [arXiv:1104.3073]\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 09:09:51 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Yao", "Qiwei", ""]]}, {"id": "1201.1379", "submitter": "Yingcun Xia", "authors": "Yingcun Xia, Howell Tong", "title": "Rejoinder to \"Feature Matching in Time Series Modeling\"", "comments": "Published in at http://dx.doi.org/10.1214/11-STS345REJ the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 59-61", "doi": "10.1214/11-STS345REJ", "report-no": "IMS-STS-STS345REJ", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder to \"Feature Matching in Time Series Modeling\" by Y. Xia and H. Tong\n[arXiv:1104.3073]\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 09:33:44 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Xia", "Yingcun", ""], ["Tong", "Howell", ""]]}, {"id": "1201.1384", "submitter": "Takashi Isozaki Dr.", "authors": "Takashi Isozaki", "title": "A Thermodynamical Approach for Probability Estimation", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The issue of discrete probability estimation for samples of small size is\naddressed in this study. The maximum likelihood method often suffers\nover-fitting when insufficient data is available. Although the Bayesian\napproach can avoid over-fitting by using prior distributions, it still has\nproblems with objective analysis. In response to these drawbacks, a new\ntheoretical framework based on thermodynamics, where energy and temperature are\nintroduced, was developed. Entropy and likelihood are placed at the center of\nthis method. The key principle of inference for probability mass functions is\nthe minimum free energy, which is shown to unify the two principles of maximum\nlikelihood and maximum entropy. Our method can robustly estimate probability\nfunctions from small size data.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 10:15:37 GMT"}, {"version": "v2", "created": "Wed, 12 Dec 2012 06:27:55 GMT"}], "update_date": "2012-12-13", "authors_parsed": [["Isozaki", "Takashi", ""]]}, {"id": "1201.1421", "submitter": "Mark Tygert", "authors": "Mark Tygert", "title": "Testing the significance of assuming homogeneity in\n  contingency-tables/cross-tabulations", "comments": "14 pages, 18 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model for homogeneity of proportions in a two-way\ncontingency-table/cross-tabulation is the same as the model of independence,\nexcept that the probabilistic process generating the data is viewed as fixing\nthe column totals (but not the row totals). When gauging the consistency of\nobserved data with the assumption of independence, recent work has illustrated\nthat the Euclidean/Frobenius/Hilbert-Schmidt distance is often far more\nstatistically powerful than the classical statistics such as chi-square, the\nlog-likelihood-ratio (G), the Freeman-Tukey/Hellinger distance, and other\nmembers of the Cressie-Read power-divergence family. The present paper\nindicates that the Euclidean/Frobenius/Hilbert-Schmidt distance can be more\npowerful for gauging the consistency of observed data with the assumption of\nhomogeneity, too.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 14:06:43 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Tygert", "Mark", ""]]}, {"id": "1201.1431", "submitter": "Mark Tygert", "authors": "William Perkins, Mark Tygert, and Rachel Ward", "title": "An introduction to how chi-square and classical exact tests often wildly\n  misreport significance and how the remedy lies in computers", "comments": "41 pages, 25 figures, 7 tables. arXiv admin note: near complete text\n  overlap with arXiv:1108.4126", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goodness-of-fit tests based on the Euclidean distance often outperform\nchi-square and other classical tests (including the standard exact tests) by at\nleast an order of magnitude when the model being tested for goodness-of-fit is\na discrete probability distribution that is not close to uniform. The present\narticle discusses numerous examples of this. Goodness-of-fit tests based on the\nEuclidean metric are now practical and convenient: although the actual values\ntaken by the Euclidean distance and similar goodness-of-fit statistics are\nseldom humanly interpretable, black-box computer programs can rapidly calculate\ntheir precise significance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 15:20:40 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2012 21:20:35 GMT"}], "update_date": "2012-01-27", "authors_parsed": [["Perkins", "William", ""], ["Tygert", "Mark", ""], ["Ward", "Rachel", ""]]}, {"id": "1201.1490", "submitter": "Eric Lesage", "authors": "Fran\\c{c}ois Coquet (CREST, IRMAR), \\'Eric Lesage (CREST, IRMAR)", "title": "Conditional inference with a complex sampling: exact computations and\n  Monte Carlo estimations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In survey statistics, the usual technique for estimating a population total\nconsists in summing appropriately weighted variable values for the units in the\nsample. Different weighting systems exit: sampling weights, GREG weights or\ncalibration weights for example. In this article, we propose to use the inverse\nof conditional inclusion probabilities as weighting system. We study examples\nwhere an auxiliary information enables to perform an a posteriori\nstratification of the population. We show that, in these cases, exact\ncomputations of the conditional weights are possible. When the auxiliary\ninformation consists in the knowledge of a quantitative variable for all the\nunits of the population, then we show that the conditional weights can be\nestimated via Monte-Carlo simulations. This method is applied to outlier and\nstrata-Jumper adjustments.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2012 20:39:48 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Coquet", "Fran\u00e7ois", "", "CREST, IRMAR"], ["Lesage", "\u00c9ric", "", "CREST, IRMAR"]]}, {"id": "1201.1578", "submitter": "Brahimi Brahim", "authors": "Brahim Brahimi, Djamel Meraghni, Abdelhakim Necir, Djabrane Yahia", "title": "A Bias-reduced Estimator for the Mean of a Heavy-tailed Distribution\n  with an Infinite Second Moment", "comments": "Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use bias-reduced estimators of high quantiles, of heavy-tailed\ndistributions, to introduce a new estimator of the mean in the case of infinite\nsecond moment. The asymptotic normality of the proposed estimator is\nestablished and checked, in a simulation study, by four of the most popular\ngoodness-of-fit tests for different sample sizes. Moreover, we compare, in\nterms of bias and mean squared error, our estimator with Peng's estimator\n(Peng, 2001) and we evaluate the accuracy of some resulting confidence\nintervals.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2012 17:40:49 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2012 19:10:14 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Brahimi", "Brahim", ""], ["Meraghni", "Djamel", ""], ["Necir", "Abdelhakim", ""], ["Yahia", "Djabrane", ""]]}, {"id": "1201.1587", "submitter": "Houtao Deng", "authors": "Houtao Deng and George Runger", "title": "Feature Selection via Regularized Trees", "comments": "8 pages; The 2012 International Joint Conference on Neural Networks\n  (IJCNN), IEEE, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a tree regularization framework, which enables many tree models to\nperform feature selection efficiently. The key idea of the regularization\nframework is to penalize selecting a new feature for splitting when its gain\n(e.g. information gain) is similar to the features used in previous splits. The\nregularization framework is applied on random forest and boosted trees here,\nand can be easily applied to other tree models. Experimental studies show that\nthe regularized trees can select high-quality feature subsets with regard to\nboth strong and weak classifiers. Because tree models can naturally deal with\ncategorical and numerical variables, missing values, different scales between\nvariables, interactions and nonlinearities etc., the tree regularization\nframework provides an effective and efficient feature selection solution for\nmany practical problems.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jan 2012 21:15:32 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2012 01:12:49 GMT"}, {"version": "v3", "created": "Wed, 21 Mar 2012 06:31:53 GMT"}], "update_date": "2012-03-22", "authors_parsed": [["Deng", "Houtao", ""], ["Runger", "George", ""]]}, {"id": "1201.1658", "submitter": "Debdeep Pati", "authors": "Kelvin Gu, Debdeep Pati and David B. Dunson", "title": "Bayesian hierarchical modeling of simply connected 2D shapes", "comments": "24 pages, working manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for distributions of shapes contained within images can be widely used\nin biomedical applications ranging from tumor tracking for targeted radiation\ntherapy to classifying cells in a blood sample. Our focus is on hierarchical\nprobability models for the shape and size of simply connected 2D closed curves,\navoiding the need to specify landmarks through modeling the entire curve while\nborrowing information across curves for related objects. Prevalent approaches\nfollow a fundamentally different strategy in providing an initial point\nestimate of the curve and/or locations of landmarks, which are then fed into\nsubsequent statistical analyses. Such two-stage methods ignore uncertainty in\nthe first stage, and do not allow borrowing of information across objects in\nestimating object shapes and sizes. Our fully Bayesian hierarchical model is\nbased on multiscale deformations within a linear combination of cyclic basis\ncharacterization, which facilitates automatic alignment of the different curves\naccounting for uncertainty. The characterization is shown to be highly flexible\nin representing 2D closed curves, leading to a nonparametric Bayesian prior\nwith large support. Efficient Markov chain Monte Carlo methods are developed\nfor simultaneous analysis of many objects. The methods are evaluated through\nsimulation examples and applied to yeast cell imaging data.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2012 20:58:38 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Gu", "Kelvin", ""], ["Pati", "Debdeep", ""], ["Dunson", "David B.", ""]]}, {"id": "1201.1766", "submitter": "Michael Evans", "authors": "Michael Evans, Gun Ho Jang", "title": "Weak Informativity and the Information in One Prior Relative to Another", "comments": "Published in at http://dx.doi.org/10.1214/11-STS357 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 3, 423-439", "doi": "10.1214/11-STS357", "report-no": "IMS-STS-STS357", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A question of some interest is how to characterize the amount of information\nthat a prior puts into a statistical analysis. Rather than a general\ncharacterization, we provide an approach to characterizing the amount of\ninformation a prior puts into an analysis, when compared to another base prior.\nThe base prior is considered to be the prior that best reflects the current\navailable information. Our purpose then is to characterize priors that can be\nused as conservative inputs to an analysis relative to the base prior. The\ncharacterization that we provide is in terms of a priori measures of prior-data\nconflict.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2012 14:12:45 GMT"}], "update_date": "2012-01-10", "authors_parsed": [["Evans", "Michael", ""], ["Jang", "Gun Ho", ""]]}, {"id": "1201.1927", "submitter": "Xin Lu", "authors": "Xin Lu, Jens Malmros, Fredrik Liljeros, Tom Britton", "title": "Respondent-driven Sampling on Directed Networks", "comments": "22 pages, 1 table, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is a commonly used substitute for random\nsampling when studying hidden populations, such as injecting drug users or men\nwho have sex with men, for which no sampling frame is known. The method is an\nextension of the snowball sample method and can, given that some assumptions\nare met, generate unbiased population estimates. One key assumption, not likely\nto be met, is that the acquaintance network in which the recruitment process\ntakes place is undirected, meaning that all recruiters should have the\npotential to be recruited by the person they recruit. Here we investigate the\npotential bias of directedness by simulating RDS on real and artificial network\nstructures. We show that directedness is likely to generate bias that cannot be\ncompensated for unless the sampled individuals know how many that potentially\nmay have recruited them (i.e. their indegree), which is unlikely in most\nsituations. Based on one known parameter, we propose an estimator for RDS on\ndirected networks when only outdegrees are observed.\n  By comparison of current RDS estimators' performances on networks with\nvarying structures, we find that our new estimator, together with a recent\nestimator, which requires the population size as a known quantity, have\nrelatively low level of estimate error and bias. Based on our new estimator,\nsensitivity analysis can be made by varying values of the known parameter to\ntake uncertainty of network directedness and error in reporting degrees into\naccount. Finally, we have developed a bootstrap procedure for the new estimator\nto construct confidence intervals.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2012 21:25:00 GMT"}, {"version": "v2", "created": "Sun, 29 Apr 2012 12:14:17 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Lu", "Xin", ""], ["Malmros", "Jens", ""], ["Liljeros", "Fredrik", ""], ["Britton", "Tom", ""]]}, {"id": "1201.1979", "submitter": "Ting-Li Chen", "authors": "Shang-Ying Shiu and Ting-Li Chen", "title": "On the strengths of the self-updating process clustering algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a simple, intuitive and yet powerful algorithm for clustering\nanalysis. This algorithm is an iterative process on the sample space, which\narises as an extension of the iteratively generated correlation matrices. It\nallows for both time-varying and time-invariant operators, therefore can be\nconsidered more general than the blurring mean-shift algorithm in which\noperators are time-invariant. The algorithm stands from the viewpoint of data\npoints and simulates the process how data points move and perform\nself-clustering, therefore is named Self-Updating Process (SUP). It is\nparticularly competitive for (i) data with noise, (ii) data with large number\nof clusters and (iii) unbalanced data. When noise is present in the data, the\nalgorithm is able to isolate noisy points while performing clustering\nsimultaneously. Simulation studies and real data applications are presented to\ndemonstrate the performance of SUP.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2012 07:47:02 GMT"}, {"version": "v2", "created": "Tue, 8 May 2012 17:18:28 GMT"}, {"version": "v3", "created": "Sat, 17 Aug 2013 01:07:18 GMT"}, {"version": "v4", "created": "Thu, 20 Aug 2015 01:15:48 GMT"}], "update_date": "2015-08-21", "authors_parsed": [["Shiu", "Shang-Ying", ""], ["Chen", "Ting-Li", ""]]}, {"id": "1201.1980", "submitter": "Charles E. McCulloch", "authors": "Charles E. McCulloch, John M. Neuhaus", "title": "Misspecifying the Shape of a Random Effects Distribution: Why Getting It\n  Wrong May Not Matter", "comments": "Published in at http://dx.doi.org/10.1214/11-STS361 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 3, 388-402", "doi": "10.1214/11-STS361", "report-no": "IMS-STS-STS361", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical models that include random effects are commonly used to analyze\nlongitudinal and correlated data, often with strong and parametric assumptions\nabout the random effects distribution. There is marked disagreement in the\nliterature as to whether such parametric assumptions are important or\ninnocuous. In the context of generalized linear mixed models used to analyze\nclustered or longitudinal data, we examine the impact of random effects\ndistribution misspecification on a variety of inferences, including prediction,\ninference about covariate effects, prediction of random effects and estimation\nof random effects variances. We describe examples, theoretical calculations and\nsimulations to elucidate situations in which the specification is and is not\nimportant. A key conclusion is the large degree of robustness of maximum\nlikelihood for a wide variety of commonly encountered situations.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2012 07:50:22 GMT"}], "update_date": "2012-01-11", "authors_parsed": [["McCulloch", "Charles E.", ""], ["Neuhaus", "John M.", ""]]}, {"id": "1201.2047", "submitter": "Victor M. Panaretos", "authors": "Victor M. Panaretos", "title": "A Conversation with David R. Brillinger", "comments": "Published in at http://dx.doi.org/10.1214/10-STS324 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 3, 440-469", "doi": "10.1214/10-STS324", "report-no": "IMS-STS-STS324", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  David Ross Brillinger was born on the 27th of October 1937, in Toronto,\nCanada. In 1955, he entered the University of Toronto, graduating with a B.A.\nwith Honours in Pure Mathematics in 1959, while also serving as a Lieutenant in\nthe Royal Canadian Naval Reserve. He was one of the five winners of the Putnam\nmathematical competition in 1958. He then went on to obtain his M.A. and Ph.D.\nin Mathematics at Princeton University, in 1960 and 1961, the latter under the\nguidance of John W. Tukey. During the period 1962--1964 he held halftime\nappointments as a Lecturer in Mathematics at Princeton, and a Member of\nTechnical Staff at Bell Telephone Laboratories, Murray Hill, New Jersey. In\n1964, he was appointed Lecturer and, two years later, Reader in Statistics at\nthe London School of Economics. After spending a sabbatical year at Berkeley in\n1967--1968, he returned to become Professor of Statistics in 1970, and has been\nthere ever since. During his 40 years (and counting) as a faculty member at\nBerkeley, he has supervised 40 doctoral theses. He has a record of academic and\nprofessional service and has received a number of honors and awards.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2012 13:23:33 GMT"}], "update_date": "2012-01-11", "authors_parsed": [["Panaretos", "Victor M.", ""]]}, {"id": "1201.2371", "submitter": "Gane  Samb Lo", "authors": "Mohamed Cheikh Haidara, Gane Samb Lo", "title": "Statistical estimation of gap of decomposability of the general poverty\n  index", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the decomposability property is very a practical one in Welfare analysis,\nmost researchers and users favor decomposable poverty indices such as the\nFoster-Greer-Thorbeck poverty index. This may lead to neglect the so important\nweighted indices like the Kakwani and Shorrocks ones which have interesting\nother properties in Welfare analysis. To face up to this problem, we give in\nthis paper, statistical estimations of the gap of decomposability of a large\nclass of such indices using the General Poverty Indice (GPI) and of a new\nasymptotic representation Theorem for it, in terms of functional empirical\nprocesses theory. The results then enable independent handling of targeted\ngroups and next global reporting with significant confidence intervals.\nData-driven examples are given with real data.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2012 18:39:01 GMT"}], "update_date": "2012-01-12", "authors_parsed": [["Haidara", "Mohamed Cheikh", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1201.2487", "submitter": "Stijn Vansteelandt", "authors": "Stijn Vansteelandt, Jack Bowden, Manoochehr Babanezhad, Els\n  Goetghebeur", "title": "On Instrumental Variables Estimation of Causal Odds Ratios", "comments": "Published in at http://dx.doi.org/10.1214/11-STS360 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 3, 403-422", "doi": "10.1214/11-STS360", "report-no": "IMS-STS-STS360", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference for causal effects can benefit from the availability of an\ninstrumental variable (IV) which, by definition, is associated with the given\nexposure, but not with the outcome of interest other than through a causal\nexposure effect. Estimation methods for instrumental variables are now well\nestablished for continuous outcomes, but much less so for dichotomous outcomes.\nIn this article we review IV estimation of so-called conditional causal odds\nratios which express the effect of an arbitrary exposure on a dichotomous\noutcome conditional on the exposure level, instrumental variable and measured\ncovariates. In addition, we propose IV estimators of so-called marginal causal\nodds ratios which express the effect of an arbitrary exposure on a dichotomous\noutcome at the population level, and are therefore of greater public health\nrelevance. We explore interconnections between the different estimators and\nsupport the results with extensive simulation studies and three applications.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2012 07:03:22 GMT"}], "update_date": "2012-01-13", "authors_parsed": [["Vansteelandt", "Stijn", ""], ["Bowden", "Jack", ""], ["Babanezhad", "Manoochehr", ""], ["Goetghebeur", "Els", ""]]}, {"id": "1201.2899", "submitter": "Tony Sit", "authors": "Steven Kou and Tony Sit and Zhiliang Ying", "title": "Parameter Estimation using Empirical Likelihood combined with Market\n  Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decade Levy processes with jumps have received increasing\npopularity for modelling market behaviour for both derviative pricing and risk\nmanagement purposes. Chan et al. (2009) introduced the use of empirical\nlikelihood methods to estimate the parameters of various diffusion processes\nvia their characteristic functions which are readily avaiable in most cases.\nReturn series from the market are used for estimation. In addition to the\nreturn series, there are many derivatives actively traded in the market whose\nprices also contain information about parameters of the underlying process.\nThis observation motivates us, in this paper, to combine the return series and\nthe associated derivative prices observed at the market so as to provide a more\nreflective estimation with respect to the market movement and achieve a gain of\neffciency. The usual asymptotic properties, including consistency and\nasymptotic normality, are established under suitable regularity conditions.\nSimulation and case studies are performed to demonstrate the feasibility and\neffectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2012 17:20:30 GMT"}], "update_date": "2012-01-16", "authors_parsed": [["Kou", "Steven", ""], ["Sit", "Tony", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1201.3245", "submitter": "Anthony Davison C.", "authors": "Rapha\\\"el Huser and A. C. Davison", "title": "Space-time modelling of extreme events", "comments": null, "journal-ref": null, "doi": "10.1111/rssb.12035", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable processes are the natural analogues of the generalized\nextreme-value distribution for the modelling of extreme events in space and\ntime. Under suitable conditions, these processes are asymptotically justified\nmodels for maxima of independent replications of random fields, and they are\nalso suitable for the modelling of joint individual extreme measurements over\nhigh thresholds. This paper extends a model of Schlather (2001) to the\nspace-time framework, and shows how a pairwise censored likelihood can be used\nfor consistent estimation under mild mixing conditions. Estimator efficiency is\nalso assessed and the choice of pairs to be included in the pairwise likelihood\nis discussed based on computations for simple time series models. The ideas are\nillustrated by an application to hourly precipitation data over Switzerland.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2012 13:08:15 GMT"}], "update_date": "2013-09-27", "authors_parsed": [["Huser", "Rapha\u00ebl", ""], ["Davison", "A. C.", ""]]}, {"id": "1201.3522", "submitter": "Ruth Heller", "authors": "Ruth Heller, Yair Heller, and Malka Gorfine", "title": "A consistent multivariate test of association based on ranks of\n  distances", "comments": null, "journal-ref": "Biometrika (2013), 100, 2, pp. 503-510", "doi": "10.1093/biomet/ass070", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with the detection of associations between random vectors of\nany dimension. Few tests of independence exist that are consistent against all\ndependent alternatives. We propose a powerful test that is applicable in all\ndimensions and is consistent against all alternatives. The test has a simple\nform and is easy to implement. We demonstrate its good power properties in\nsimulations and on examples.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2012 14:41:24 GMT"}, {"version": "v2", "created": "Wed, 30 May 2012 19:20:36 GMT"}, {"version": "v3", "created": "Thu, 31 May 2012 14:46:15 GMT"}], "update_date": "2013-08-08", "authors_parsed": [["Heller", "Ruth", ""], ["Heller", "Yair", ""], ["Gorfine", "Malka", ""]]}, {"id": "1201.3528", "submitter": "Hua Zhou", "authors": "Hua Zhou and Artin Armagan and David B. Dunson", "title": "Path Following and Empirical Bayes Model Selection for Sparse Regression", "comments": "35 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In recent years, a rich variety of regularization procedures have been\nproposed for high dimensional regression problems. However, tuning parameter\nchoice and computational efficiency in ultra-high dimensional problems remain\nvexing issues. The routine use of $\\ell_1$ regularization is largely\nattributable to the computational efficiency of the LARS algorithm, but similar\nefficiency for better behaved penalties has remained elusive. In this article,\nwe propose a highly efficient path following procedure for combination of any\nconvex loss function and a broad class of penalties. From a Bayesian\nperspective, this algorithm rapidly yields maximum a posteriori estimates at\ndifferent hyper-parameter values. To bypass the inefficiency and potential\ninstability of cross validation, we propose an empirical Bayes procedure for\nrapidly choosing the optimal model and corresponding hyper-parameter value.\nThis approach applies to any penalty that corresponds to a proper prior\ndistribution on the regression coefficients. While we mainly focus on sparse\nestimation of generalized linear models, the method extends to more general\nregularizations such as polynomial trend filtering after reparameterization.\nThe proposed algorithm scales efficiently to large $p$ and/or $n$. Solution\npaths of 10,000 dimensional examples are computed within one minute on a laptop\nfor various generalized linear models (GLM). Operating characteristics are\nassessed through simulation studies and the methods are applied to several real\ndata sets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2012 15:18:25 GMT"}], "update_date": "2012-01-18", "authors_parsed": [["Zhou", "Hua", ""], ["Armagan", "Artin", ""], ["Dunson", "David B.", ""]]}, {"id": "1201.3571", "submitter": "Hua Zhou", "authors": "Hua Zhou and Yichao Wu", "title": "A Generic Path Algorithm for Regularized Statistical Estimation", "comments": "28 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Regularization is widely used in statistics and machine learning to prevent\noverfitting and gear solution towards prior information. In general, a\nregularized estimation problem minimizes the sum of a loss function and a\npenalty term. The penalty term is usually weighted by a tuning parameter and\nencourages certain constraints on the parameters to be estimated. Particular\nchoices of constraints lead to the popular lasso, fused-lasso, and other\ngeneralized $l_1$ penalized regression methods. Although there has been a lot\nof research in this area, developing efficient optimization methods for many\nnonseparable penalties remains a challenge. In this article we propose an exact\npath solver based on ordinary differential equations (EPSODE) that works for\nany convex loss function and can deal with generalized $l_1$ penalties as well\nas more complicated regularization such as inequality constraints encountered\nin shape-restricted regressions and nonparametric density estimation. In the\npath following process, the solution path hits, exits, and slides along the\nvarious constraints and vividly illustrates the tradeoffs between goodness of\nfit and model parsimony. In practice, the EPSODE can be coupled with AIC, BIC,\n$C_p$ or cross-validation to select an optimal tuning parameter. Our\napplications to generalized $l_1$ regularized generalized linear models,\nshape-restricted regressions, Gaussian graphical models, and nonparametric\ndensity estimation showcase the potential of the EPSODE algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2012 17:42:46 GMT"}], "update_date": "2012-01-18", "authors_parsed": [["Zhou", "Hua", ""], ["Wu", "Yichao", ""]]}, {"id": "1201.3702", "submitter": "Paul Kabaila", "authors": "Waruni Abeysekera, Paul Kabaila and Oguzhan Yilmaz", "title": "The coverage probability of confidence intervals in one-way analysis of\n  covariance after two F tests", "comments": "A few typographical errors have been corrected", "journal-ref": "The coverage probability of confidence intervals in one-way\n  analysis of covariance after two F tests. Australian & New Zealand Journal of\n  Statistics, 55, 221-234 (2013)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a one-way analysis of covariance model. Suppose that the parameter\nof interest theta is a specified linear contrast of the expected responses, for\na given value of the covariate. Also suppose that the inference of interest is\na 1-alpha confidence interval for theta. The following two-stage procedure has\nbeen proposed to determine the form of the model. In Stage 1, we carry out an F\ntest of the null hypothesis that the slopes are all zero against the\nalternative hypothesis that they are not all zero. If this null hypothesis is\naccepted then we assume that the slopes are all zero; otherwise we proceed to\nStage 2. In Stage 2, we carry out an F test of the null hypothesis that the\nslopes are all equal against the alternative hypothesis that they are not all\nequal. If this null hypothesis is accepted then we assume that the slopes are\nall equal; otherwise this assumption is not made. We present a general\nmethodology for the examination of the effect of this two-stage model selection\nprocedure on the coverage probability of a subsequently-constructed confidence\ninterval for theta, with nominal coverage 1-alpha. This methodology is applied\nto a numerical example for which it is shown that this confidence interval is\ncompletely inadequate.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2012 06:31:29 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2012 00:29:55 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Abeysekera", "Waruni", ""], ["Kabaila", "Paul", ""], ["Yilmaz", "Oguzhan", ""]]}, {"id": "1201.3767", "submitter": "Nikolas Kantas", "authors": "Ajay Jasra and Nikolas Kantas", "title": "Bayesian Parameter Inference for Partially Observed Stopped Processes", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider Bayesian parameter inference associated to\npartially-observed stochastic processes that start from a set B0 and are\nstopped or killed at the first hitting time of a known set A. Such processes\noccur naturally within the context of a wide variety of applications. The\nassociated posterior distributions are highly complex and posterior parameter\ninference requires the use of advanced Markov chain Monte Carlo (MCMC)\ntechniques. Our approach uses a recently introduced simulation methodology,\nparticle Markov chain Monte Carlo (PMCMC) (Andrieu et. al. 2010 [1]), where\nsequential Monte Carlo (SMC) approximations (see Doucet et. al. 2001 [18] and\nLiu 2001 [27]) are embedded within MCMC. However, when the parameter of\ninterest is fixed, standard SMC algorithms are not always appropriate for many\nstopped processes. In Chen et. al. [11] and Del Moral 2004 [15] the authors\nintroduce SMC approximations of multi-level Feynman-Kac formulae, which can\nlead to more efficient algorithms. This is achieved by devising a sequence of\nnested sets from B0 to A and then perform the resampling step only when the\nsamples of the process reach intermediate level sets in the sequence.\nNaturally, the choice of the intermediate level sets is critical to the\nperformance of such a scheme. In this paper, we demonstrate that multi-level\nSMC algorithms can be used as a proposal in PMCMC. In addition, we propose a\nflexible strategy that adapts the level sets for different parameter proposals.\nOur methodology is illustrated on the coalescent model with migration.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2012 12:22:52 GMT"}], "update_date": "2012-01-19", "authors_parsed": [["Jasra", "Ajay", ""], ["Kantas", "Nikolas", ""]]}, {"id": "1201.3937", "submitter": "Jarad Niemi", "authors": "Michael D. Porter and Jarad B. Niemi and Brian J. Reich", "title": "Mixture Likelihood Ratio Scan Statistic for Disease Outbreak Detection", "comments": "The research was performed in connection with a technical contest put\n  on by the International Society for Disease Surveillance. The article was\n  prepared for a special edition of the journal Advances in Disease\n  Surveillance that never materialized", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Early detection of disease outbreaks is of paramount importance to\nimplementing intervention strategies to mitigate the severity and duration of\nthe outbreak. We build methodology that utilizes the characteristic profile of\ndisease outbreaks to reduce the time to detection and false positive rate. We\nmodel daily counts through a Poisson distribution with additive background plus\noutbreak components. The outbreak component has a parametric form with unknown\nunderlying parameters. A mixture likelihood ratio scan statistic is developed\nto maximize parameters over a window in time. This provides an alert statistic\nwith early time to detection and low false positive rate. The methodology is\ndemonstrated on three simulated data sets meant to represent E. coli,\nCryptosporidium, and Influenza outbreaks.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2012 21:59:02 GMT"}], "update_date": "2012-01-20", "authors_parsed": [["Porter", "Michael D.", ""], ["Niemi", "Jarad B.", ""], ["Reich", "Brian J.", ""]]}, {"id": "1201.4403", "submitter": "Bin Zhu", "authors": "Bin Zhu and David B. Dunson", "title": "Locally Adaptive Bayes Nonparametric Regression via Nested Gaussian\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nested Gaussian process (nGP) as a locally adaptive prior for\nBayesian nonparametric regression. Specified through a set of stochastic\ndifferential equations (SDEs), the nGP imposes a Gaussian process prior for the\nfunction's $m$th-order derivative. The nesting comes in through including a\nlocal instantaneous mean function, which is drawn from another Gaussian process\ninducing adaptivity to locally-varying smoothness. We discuss the support of\nthe nGP prior in terms of the closure of a reproducing kernel Hilbert space,\nand consider theoretical properties of the posterior. The posterior mean under\nthe nGP prior is shown to be equivalent to the minimizer of a nested penalized\nsum-of-squares involving penalties for both the global and local roughness of\nthe function. Using highly-efficient Markov chain Monte Carlo for posterior\ninference, the proposed method performs well in simulation studies compared to\nseveral alternatives, and is scalable to massive data, illustrated through a\nproteomics application.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2012 21:44:14 GMT"}], "update_date": "2012-01-24", "authors_parsed": [["Zhu", "Bin", ""], ["Dunson", "David B.", ""]]}, {"id": "1201.4529", "submitter": "James S. Martin", "authors": "James S. Martin, Ajay Jasra and Emma McCoy", "title": "Inference for a Class of Partially Observed Point Process Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a simulation-based framework for sequential inference\nfrom partially and discretely observed point process (PP's) models with static\nparameters. Taking on a Bayesian perspective for the static parameters, we\nbuild upon sequential Monte Carlo (SMC) methods, investigating the problems of\nperforming sequential filtering and smoothing in complex examples, where\ncurrent methods often fail. We consider various approaches for approximating\nposterior distributions using SMC. Our approaches, with some theoretical\ndiscussion are illustrated on a doubly stochastic point process applied in the\ncontext of finance.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2012 06:05:29 GMT"}], "update_date": "2012-01-24", "authors_parsed": [["Martin", "James S.", ""], ["Jasra", "Ajay", ""], ["McCoy", "Emma", ""]]}, {"id": "1201.4632", "submitter": "Ngoc Mai Tran", "authors": "Ngoc Mai Tran", "title": "HodgeRank is the limit of Perron Rank", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the map which takes an elementwise positive matrix to the k-th root\nof the principal eigenvector of its k-th Hadamard power. We show that as $k$\ntends to 0 one recovers the row geometric mean vector and discuss the geometric\nsignificance of this convergence. In the context of pairwise comparison\nranking, our result states that HodgeRank is the limit of Perron Rank, thereby\nproviding a novel mathematical link between two important pairwise ranking\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2012 06:08:28 GMT"}], "update_date": "2012-01-24", "authors_parsed": [["Tran", "Ngoc Mai", ""]]}, {"id": "1201.4647", "submitter": "Klaas Slooten", "authors": "Klaas Slooten and Ronald Meester", "title": "Forensic identification: the Island Problem and its generalisations", "comments": null, "journal-ref": "Statistica Neerlandica 65 (2), 202-237, 2011", "doi": "10.1111/j.1467-9574.2011.00484.x", "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In forensics it is a classical problem to determine, when a suspect $S$\nshares a property $\\Gamma$ with a criminal $C$, the probability that $S=C$. In\nthis paper we give a detailed account of this problem in various degrees of\ngenerality. We start with the classical case where the probability of having\n$\\Gamma$, as well as the a priori probability of being the criminal, is the\nsame for all individuals. We then generalize the solution to deal with\nheterogeneous populations, biased search procedures for the suspect,\n$\\Gamma$-correlations, uncertainty about the subpopulation of the criminal and\nthe suspect, and uncertainty about the $\\Gamma$-frequencies. We also consider\nthe effect of the way the search for $S$ is conducted, in particular when this\nis done by a database search. A returning theme is that we show that\nconditioning is of importance when one wants to quantify the \"weight\" of the\nevidence by a likelihood ratio. Apart from these mathematical issues, we also\ndiscuss the practical problems in applying these issues to the legal process.\nThe posterior probabilities of $C=S$ are typically the same for all reasonable\nchoices of the hypotheses, but this is not the whole story. The legal process\nmight force one to dismiss certain hypotheses, for instance when the relevant\nlikelihood ratio depends on prior probabilities. We discuss this and related\nissues as well. As such, the paper is relevant both from a theoretical and from\nan applied point of view.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2012 08:33:30 GMT"}], "update_date": "2012-01-24", "authors_parsed": [["Slooten", "Klaas", ""], ["Meester", "Ronald", ""]]}, {"id": "1201.4667", "submitter": "Silvia Bacci Dr", "authors": "Silvia Bacci, Francesco Bartolucci, Michela Gnaldi", "title": "A class of Multidimensional Latent Class IRT models for ordinal\n  polytomous item responses", "comments": "25 pages; 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of Item Response Theory models for items with ordinal\npolytomous responses, which extends an existing class of multidimensional\nmodels for dichotomously-scored items measuring more than one latent trait. In\nthe proposed approach, the random vector used to represent the latent traits is\nassumed to have a discrete distribution with support points corresponding to\ndifferent latent classes in the population. We also allow for different\nparameterizations for the conditional distribution of the response variables\ngiven the latent traits - such as those adopted in the Graded Response model,\nin the Partial Credit model, and in the Rating Scale model - depending on both\nthe type of link function and the constraints imposed on the item parameters.\nFor the proposed models we outline how to perform maximum likelihood estimation\nvia the Expectation-Maximization algorithm. Moreover, we suggest a strategy for\nmodel selection which is based on a series of steps consisting of selecting\nspecific features, such as the number of latent dimensions, the number of\nlatent classes, and the specific parametrization. In order to illustrate the\nproposed approach, we analyze data deriving from a study on anxiety and\ndepression as perceived by oncological patients.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2012 10:18:40 GMT"}], "update_date": "2012-01-24", "authors_parsed": [["Bacci", "Silvia", ""], ["Bartolucci", "Francesco", ""], ["Gnaldi", "Michela", ""]]}, {"id": "1201.5076", "submitter": "Vural Aksakalli", "authors": "Vural Aksakalli, Elvan Ceyhan", "title": "Optimal obstacle placement with disambiguations", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS556 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 4, 1730-1774", "doi": "10.1214/12-AOAS556", "report-no": "IMS-AOAS-AOAS556", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the optimal obstacle placement with disambiguations problem\nwherein the goal is to place true obstacles in an environment cluttered with\nfalse obstacles so as to maximize the total traversal length of a navigating\nagent (NAVA). Prior to the traversal, the NAVA is given location information\nand probabilistic estimates of each disk-shaped hindrance (hereinafter referred\nto as disk) being a true obstacle. The NAVA can disambiguate a disk's status\nonly when situated on its boundary. There exists an obstacle placing agent\n(OPA) that locates obstacles prior to the NAVA's traversal. The goal of the OPA\nis to place true obstacles in between the clutter in such a way that the NAVA's\ntraversal length is maximized in a game-theoretic sense. We assume the OPA\nknows the clutter spatial distribution type, but not the exact locations of\nclutter disks. We analyze the traversal length using repeated measures analysis\nof variance for various obstacle number, obstacle placing scheme and clutter\nspatial distribution type combinations in order to identify the optimal\ncombination. Our results indicate that as the clutter becomes more regular\n(clustered), the NAVA's traversal length gets longer (shorter). On the other\nhand, the traversal length tends to follow a concave-down trend as the number\nof obstacles increases. We also provide a case study on a real-world maritime\nminefield data set.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2012 18:20:47 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2012 00:21:30 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2013 08:04:44 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Aksakalli", "Vural", ""], ["Ceyhan", "Elvan", ""]]}, {"id": "1201.5133", "submitter": "Johan Segers", "authors": "Ingrid Hobaek Haff, Johan Segers", "title": "Nonparametric estimation of pair-copula constructions with the empirical\n  pair-copula", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A pair-copula construction is a decomposition of a multivariate copula into a\nstructured system, called regular vine, of bivariate copulae or pair-copulae.\nThe standard practice is to model these pair-copulae parametrically, which\ncomes at the cost of a large model risk, with errors propagating throughout the\nvine structure. The empirical pair-copula proposed in the paper provides a\nnonparametric alternative still achieving the parametric convergence rate. It\ncan be used as a basis for inference on dependence measures, for selecting and\npruning the vine structure, and for hypothesis tests concerning the form of the\npair-copulae.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2012 21:31:51 GMT"}], "update_date": "2012-01-26", "authors_parsed": [["Haff", "Ingrid Hobaek", ""], ["Segers", "Johan", ""]]}, {"id": "1201.5568", "submitter": "Robert B. Gramacy", "authors": "Christoforos Anagnostopoulos and Robert B. Gramacy", "title": "Dynamic trees for streaming and massive data contexts", "comments": "18 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data collection at a massive scale is becoming ubiquitous in a wide variety\nof settings, from vast offline databases to streaming real-time information.\nLearning algorithms deployed in such contexts must rely on single-pass\ninference, where the data history is never revisited. In streaming contexts,\nlearning must also be temporally adaptive to remain up-to-date against\nunforeseen changes in the data generating mechanism. Although rapidly growing,\nthe online Bayesian inference literature remains challenged by massive data and\ntransient, evolving data streams. Non-parametric modelling techniques can prove\nparticularly ill-suited, as the complexity of the model is allowed to increase\nwith the sample size. In this work, we take steps to overcome these challenges\nby porting standard streaming techniques, like data discarding and\ndownweighting, into a fully Bayesian framework via the use of informative\npriors and active learning heuristics. We showcase our methods by augmenting a\nmodern non-parametric modelling framework, dynamic trees, and illustrate its\nperformance on a number of practical examples. The end product is a powerful\nstreaming regression and classification tool, whose performance compares\nfavourably to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2012 16:20:05 GMT"}], "update_date": "2012-01-27", "authors_parsed": [["Anagnostopoulos", "Christoforos", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "1201.5687", "submitter": "Giovanni Montana", "authors": "Alberto Cozzini, Ajay Jasra, Giovanni Montana", "title": "Robust model-based clustering with gene ranking", "comments": "18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis of biological samples using gene expression measurements is\na common task which aids the discovery of heterogeneous biological\nsub-populations having distinct mRNA profiles. Several model-based clustering\nalgorithms have been proposed in which the distribution of gene expression\nvalues within each sub-group is assumed to be Gaussian. In the presence of\nnoise and extreme observations, a mixture of Gaussian densities may over-fit\nand overestimate the true number of clusters. Moreover, commonly used\nmodel-based clustering algorithms do not generally provide a mechanism to\nquantify the relative contribution of each gene to the final partitioning of\nthe data. We propose a penalised mixture of Student's t distributions for\nmodel-based clustering and gene ranking. Together with a bootstrap procedure,\nthe proposed approach provides a means for ranking genes according to their\ncontributions to the clustering process. Experimental results show that the\nalgorithm performs well comparably to traditional Gaussian mixtures in the\npresence of outliers and longer tailed distributions. The algorithm also\nidentifies the true informative genes with high sensitivity, and achieves\nimproved model selection. An illustrative application to breast cancer data is\nalso presented which confirms established tumor subclasses.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2012 01:38:11 GMT"}], "update_date": "2012-01-30", "authors_parsed": [["Cozzini", "Alberto", ""], ["Jasra", "Ajay", ""], ["Montana", "Giovanni", ""]]}, {"id": "1201.5745", "submitter": "Giovanni Montana", "authors": "Matt Silver, Giovanni Montana", "title": "Fast Identification of Biological Pathways Associated with a\n  Quantitative Trait Using Group Lasso with Overlaps", "comments": "29 pages", "journal-ref": "Statistical Applications in Genetics and Molecular Biology, Volume\n  11, Issue 1, Article 7, 2012", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Where causal SNPs (single nucleotide polymorphisms) tend to accumulate within\nbiological pathways, the incorporation of prior pathways information into a\nstatistical model is expected to increase the power to detect true associations\nin a genetic association study. Most existing pathways-based methods rely on\nmarginal SNP statistics and do not fully exploit the dependence patterns among\nSNPs within pathways. We use a sparse regression model, with SNPs grouped into\npathways, to identify causal pathways associated with a quantitative trait.\nNotable features of our \"pathways group lasso with adaptive weights\" (P-GLAW)\nalgorithm include the incorporation of all pathways in a single regression\nmodel, an adaptive pathway weighting procedure that accounts for factors\nbiasing pathway selection, and the use of a bootstrap sampling procedure for\nthe ranking of important pathways. P-GLAW takes account of the presence of\noverlapping pathways and uses a novel combination of techniques to optimise\nmodel estimation, making it fast to run, even on whole genome datasets. In a\ncomparison study with an alternative pathways method based on univariate SNP\nstatistics, our method demonstrates high sensitivity and specificity for the\ndetection of important pathways, showing the greatest relative gains in\nperformance where marginal SNP effect sizes are small.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2012 11:01:31 GMT"}], "update_date": "2012-01-30", "authors_parsed": [["Silver", "Matt", ""], ["Montana", "Giovanni", ""]]}, {"id": "1201.5786", "submitter": "Torsten Hothorn", "authors": "Torsten Hothorn, Thomas Kneib and Peter B\\\"uhlmann", "title": "Conditional Transformation Models", "comments": null, "journal-ref": "Journal of the Royal Statistical Society, Series B (Methodology),\n  2014", "doi": "10.1111/rssb.12017", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ultimate goal of regression analysis is to obtain information about the\nconditional distribution of a response given a set of explanatory variables.\nThis goal is, however, seldom achieved because most established regression\nmodels only estimate the conditional mean as a function of the explanatory\nvariables and assume that higher moments are not affected by the regressors.\nThe underlying reason for such a restriction is the assumption of additivity of\nsignal and noise. We propose to relax this common assumption in the framework\nof transformation models. The novel class of semiparametric regression models\nproposed herein allows transformation functions to depend on explanatory\nvariables. These transformation functions are estimated by regularised\noptimisation of scoring rules for probabilistic forecasts, e.g. the continuous\nranked probability score. The corresponding estimated conditional distribution\nfunctions are consistent. Conditional transformation models are potentially\nuseful for describing possible heteroscedasticity, comparing spatially varying\ndistributions, identifying extreme events, deriving prediction intervals and\nselecting variables beyond mean regression effects. An empirical investigation\nbased on a heteroscedastic varying coefficient simulation model demonstrates\nthat semiparametric estimation of conditional distribution functions can be\nmore beneficial than kernel-based non-parametric approaches or parametric\ngeneralised additive models for location, scale and shape.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2012 14:38:48 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2012 11:40:34 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Hothorn", "Torsten", ""], ["Kneib", "Thomas", ""], ["B\u00fchlmann", "Peter", ""]]}, {"id": "1201.5871", "submitter": "Patrick Perry", "authors": "Patrick O. Perry, Patrick J. Wolfe", "title": "Null models for network data", "comments": "12 pages, 2 figures; submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.SI stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of datasets taking the form of simple, undirected graphs\ncontinues to gain in importance across a variety of disciplines. Two choices of\nnull model, the logistic-linear model and the implicit log-linear model, have\ncome into common use for analyzing such network data, in part because each\naccounts for the heterogeneity of network node degrees typically observed in\npractice. Here we show how these both may be viewed as instances of a broader\nclass of null models, with the property that all members of this class give\nrise to essentially the same likelihood-based estimates of link probabilities\nin sparse graph regimes. This facilitates likelihood-based computation and\ninference, and enables practitioners to choose the most appropriate null model\nfrom this family based on application context. Comparative model fits for a\nvariety of network datasets demonstrate the practical implications of our\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2012 19:30:46 GMT"}], "update_date": "2012-02-13", "authors_parsed": [["Perry", "Patrick O.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1201.5893", "submitter": "Ralph Brinks", "authors": "Ralph Brinks", "title": "A new stochastic differential equation modelling incidence and\n  prevalence with an application to systemic lupus erythematosus in England and\n  Wales, 1995", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reformulates a common illness-death model in terms of a new\nsystem of stochastical differential equations (SDEs). The SDEs are used to\nestimate epidemiological characteristics and burden of systemic lupus\nerythematosus in England and Wales in 1995.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2012 21:01:14 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Brinks", "Ralph", ""]]}, {"id": "1201.5962", "submitter": "Gane  Samb Lo", "authors": "Gane Samb Lo", "title": "How many statistics are needed to characterize the univariate extremes", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_{1},X_{2},...$ be a sequence of independent random variables ($rv$)\nwith common distribution function ($df$) $F$ such that $F(1)=0$. We consider\nthe simple statistical problem : find a statistics family of size $m\\geq 1$\nwhose convergence, in probability or almost surely, to a point of some domain\n$\\mathcal{S} \\in \\mathbb{R}^{m}$ is equivalent that $F$ lies in the extremal\ndomain of attraction $\\Gamma$. Such a family, whenever it exists, is called an\nEmpirical Characterizing Statistics Family for the EXTtremes (ECSFEXT). The\ndeparture point of this theory goes back to Mason, who proved that the Hill\nestimator converges a.s. to a positive real number for some particular\nsequences if and only $F$ lies in the attaction domain of a Fr\\'echet's law.\nConsidered for the whole attraction domain, the question becomes more complex.\nWe provide here an ECSFEXT of nine (9) elements and also characterize the\nsubdomains of $\\Gamma$. The question of lowering m=9 to a minimum number is\nlaunched.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2012 13:55:20 GMT"}], "update_date": "2012-01-31", "authors_parsed": [["Lo", "Gane Samb", ""]]}, {"id": "1201.5984", "submitter": "John Fricks", "authors": "Gustavo Didier and Scott McKinley and David B. Hill and John Fricks", "title": "Statistical Challenges in Microrheology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microrheology is the study of the properties of a complex fluid through the\ndiffusion dynamics of small particles, typically latex beads, moving through\nthat material. Currently, it is the dominant technique in the study of the\nphysical properties of biological fluids, of the material properties of\nmembranes or the cytoplasm of cells, or of the entire cell. The theoretical\nunderpinning of microrheology was given in Mason and Weitz (Physical Review\nLetters; 1995), who introduced a framework for the use of path data of\ndiffusing particles to infer viscoelastic properties of its fluid environment.\nThe multi-particle tracking techniques that were subsequently developed have\npresented numerous challenges for experimentalists and theoreticians. This\npaper describes some specific challenges that await the attention of\nstatisticians and applied probabilists. We describe relevant aspects of the\nphysical theory, current inferential efforts and simulation aspects of a\ncentral model for the dynamics of nano-scale particles in viscoelastic fluids,\nthe generalized Langevin equation.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2012 20:03:55 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2012 23:20:45 GMT"}], "update_date": "2012-02-13", "authors_parsed": [["Didier", "Gustavo", ""], ["McKinley", "Scott", ""], ["Hill", "David B.", ""], ["Fricks", "John", ""]]}, {"id": "1201.6476", "submitter": "Shogo Kato Ph.D.", "authors": "Shogo Kato and Shinto Eguchi", "title": "Robust estimation of location and concentration parameters for the von\n  Mises-Fisher distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust estimation of location and concentration parameters for the von\nMises-Fisher distribution is discussed. A key reparametrisation is achieved by\nexpressing the two parameters as one vector on the Euclidean space. With this\nrepresentation, we first show that maximum likelihood estimator for the von\nMises-Fisher distribution is not robust in some situations. Then we propose two\nfamilies of robust estimators which can be derived as minimisers of two density\npower divergences. The presented families enable us to estimate both location\nand concentration parameters simultaneously. Some properties of the estimators\nare explored. Simple iterative algorithms are suggested to find the estimates\nnumerically. A comparison with the existing robust estimators is given as well\nas discussion on difference and similarity between the two proposed estimators.\nA simulation study is made to evaluate finite sample performance of the\nestimators. We consider a sea star dataset and discuss the selection of the\ntuning parameters and outlier detection.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2012 09:01:08 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Kato", "Shogo", ""], ["Eguchi", "Shinto", ""]]}]