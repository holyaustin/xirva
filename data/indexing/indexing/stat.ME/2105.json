[{"id": "2105.00031", "submitter": "Diego Nascimento", "authors": "Diego C Nascimento, Pedro Luiz Ramos, David Elal-Olivero, Milton\n  Cortes-Araya, Francisco Louzada", "title": "Generalizing the normality: a novel towards different estimation methods\n  for skewed information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Normality is the most often mathematical supposition used in data modeling.\nNonetheless, even based on the law of large numbers (LLN), normality is a\nstrong presumption given that the presence of asymmetry and multi-modality in\nreal-world problems is expected. Thus, a flexible modification in the Normal\ndistribution proposed by Elal-Olivero [12] adds a skewness parameter, called\nAlpha-skew Normal (ASN) distribution, enabling bimodality and fat-tail, if\nneeded, although sometimes not trivial to estimate this third parameter\n(regardless of the location and scale). This work analyzed seven different\nstatistical inferential methods towards the ASNdistribution on synthetic data\nand historical data of water flux from 21 rivers (channels) in the Atacama\nregion. Moreover, the contribution of this paper is related to the probability\nestimation surrounding the rivers' flux level in Copiapo city neighborhood, the\nmost important economic city of the third Chilean region, and known to be\nlocated in one of the driest areas on Earth, besides the North and the South\nPole\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 18:19:58 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nascimento", "Diego C", ""], ["Ramos", "Pedro Luiz", ""], ["Elal-Olivero", "David", ""], ["Cortes-Araya", "Milton", ""], ["Louzada", "Francisco", ""]]}, {"id": "2105.00045", "submitter": "Xiaoli Gao", "authors": "Xiaoli Gao", "title": "Estimation and Selection Properties of the LAD Fused Lasso Signal\n  Approximator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The fused lasso is an important method for signal processing when the hidden\nsignals are sparse and blocky. It is often used in combination with the squared\nloss function. However, the squared loss is not suitable for heavy tail error\ndistributions nor is robust against outliers which arise often in practice. The\nleast absolute deviations (LAD) loss provides a robust alternative to the\nsquared loss. In this paper, we study the asymptotic properties of the fused\nlasso estimator with the LAD loss for signal approximation. We refer to this\nestimator as the LAD fused lasso signal approximator, or LAD-FLSA. We\ninvestigate the estimation consistency properties of the LAD-FLSA and provide\nsufficient conditions under which the LAD-FLSA is sign consistent. We also\nconstruct an unbiased estimator for the degrees of freedom of the LAD-FLSA for\nany given tuning parameters. Both simulation studies and real data analysis are\nconducted to illustrate the performance of the LAD-FLSA and the effect of the\nunbiased estimator of the degrees of freedom.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 19:03:39 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Gao", "Xiaoli", ""]]}, {"id": "2105.00211", "submitter": "Emmanuel Ramasso", "authors": "Pablo Juesas, Emmanuel Ramasso, S\\'ebastien Drujont, Vincent Placet", "title": "Autoregressive Hidden Markov Models with partial knowledge on latent\n  space applied to aero-engines prognostics", "comments": null, "journal-ref": "European Conference of the PHM Society 2016, selected for extended\n  version in IJPHM", "doi": "10.36001/phme.2016.v3i1.1642", "report-no": "hal-02131233", "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  [This paper was initially published in PHME conference in 2016, selected for\nfurther publication in International Journal of Prognostics and Health\nManagement.]\n  This paper describes an Autoregressive Partially-hidden Markov model (ARPHMM)\nfor fault detection and prognostics of equipments based on sensors' data. It is\na particular dynamic Bayesian network that allows to represent the dynamics of\na system by means of a Hidden Markov Model (HMM) and an autoregressive (AR)\nprocess. The Markov chain assumes that the system is switching back and forth\nbetween internal states while the AR process ensures a temporal coherence on\nsensor measurements. A sound learning procedure of standard ARHMM based on\nmaximum likelihood allows to iteratively estimate all parameters\nsimultaneously. This paper suggests a modification of the learning procedure\nconsidering that one may have prior knowledge about the structure which becomes\npartially hidden. The integration of the prior is based on the Theory of\nWeighted Distributions which is compatible with the Expectation-Maximization\nalgorithm in the sense that the convergence properties are still satisfied. We\nshow how to apply this model to estimate the remaining useful life based on\nhealth indicators. The autoregressive parameters can indeed be used for\nprediction while the latent structure can be used to get information about the\ndegradation level. The interest of the proposed method for prognostics and\nhealth assessment is demonstrated on CMAPSS datasets.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 10:23:22 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Juesas", "Pablo", ""], ["Ramasso", "Emmanuel", ""], ["Drujont", "S\u00e9bastien", ""], ["Placet", "Vincent", ""]]}, {"id": "2105.00224", "submitter": "Debasis Kundu Professor", "authors": "Debashis Samanta and Debasis Kundu", "title": "Bayesian Inference of a Dependent Competing Risk Data", "comments": "26 pages 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysis of competing risks data plays an important role in the lifetime data\nanalysis. Recently Feizjavadian and Hashemi (Computational Statistics and Data\nAnalysis, vol. 82, 19-34, 2015) provided a classical inference of a competing\nrisks data set using four-parameter Marshall-Olkin bivariate Weibull\ndistribution when the failure of an unit at a particular time point can happen\ndue to more than one cause. The aim of this paper is to provide the Bayesian\nanalysis of the same model based on a very flexible Gamma-Dirichlet prior on\nthe scale parameters. It is observed that the Bayesian inference has certain\nadvantages over the classical inference in this case. We provide the Bayes\nestimates of the unknown parameters and the associated highest posterior\ndensity credible intervals based on Gibbs sampling technique. We further\nconsider the Bayesian inference of the model parameters assuming partially\nordered Gamma-Dirichlet prior on the scale parameters when one cause is more\nsevere than the other cause. We have extended the results for different\ncensoring schemes also.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 11:39:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Samanta", "Debashis", ""], ["Kundu", "Debasis", ""]]}, {"id": "2105.00231", "submitter": "Anton Glushchenko", "authors": "Anton Glushchenko, Vladislav Petrov and Konstantin Lastochkin", "title": "Normalization of regressor excitation as a part of dynamic regressor\n  extension and mixing procedure", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of excitation normalization of the regressor, which is used in the\nestimation loop to solve the plant identification problem, is proposed. It is\nbased on the dynamic regressor extension and mixing procedure. Its application\nallows to obtain the same upper bound of the parameter identification error for\nthe scalar regressors with different excitation level, using a constant value\nof the adaptation rate for all of them. This fact is a significant advantage\nfrom the practical point of view. Comparison of the developed method with the\nknown one of the regressor amplitude normalization is conducted. It is shown\nthat the classical approach does not have the above-stated property. To\nvalidate the theoretical conclusions made, the results of the comparative\nmathematical modeling of three loops are presented: 1) the classical gradient\none, 2) the one with the normalization of the regressor amplitude, 3) the\nproposed one with the normalization of the regressor excitation.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 12:07:45 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 17:14:39 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Glushchenko", "Anton", ""], ["Petrov", "Vladislav", ""], ["Lastochkin", "Konstantin", ""]]}, {"id": "2105.00288", "submitter": "Marie Perrot-Dockes", "authors": "Marie Perrot-Dock\\`es, Gilles Blanchard, Pierre Neuvial, Etienne\n  Roquain", "title": "Post hoc false discovery proportion inference under a Hidden Markov\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the multiple testing problem under the assumption that the\ntrue/false hypotheses are driven by a Hidden Markov Model (HMM), which is\nrecognized as a fundamental setting to model multiple testing under dependence\nsince the seminal work of \\citet{sun2009large}. While previous work has\nconcentrated on deriving specific procedures with a controlled False Discovery\nRate (FDR) under this model, following a recent trend in selective inference,\nwe consider the problem of establishing confidence bounds on the false\ndiscovery proportion (FDP), for a user-selected set of hypotheses that can\ndepend on the observed data in an arbitrary way. We develop a methodology to\nconstruct such confidence bounds first when the HMM model is known, then when\nits parameters are unknown and estimated, including the data distribution under\nthe null and the alternative, using a nonparametric approach. In the latter\ncase, we propose a bootstrap-based methodology to take into account the effect\nof parameter estimation error. We show that taking advantage of the assumed HMM\nstructure allows for a substantial improvement of confidence bound sharpness\nover existing agnostic (structure-free) methods, as witnessed both via\nnumerical experiments and real data examples.\n", "versions": [{"version": "v1", "created": "Sat, 1 May 2021 15:40:26 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Perrot-Dock\u00e8s", "Marie", ""], ["Blanchard", "Gilles", ""], ["Neuvial", "Pierre", ""], ["Roquain", "Etienne", ""]]}, {"id": "2105.00364", "submitter": "Min Wang", "authors": "Shen Zhang, Keying Ye, Min Wang", "title": "A simple consistent Bayes factor for testing the Kendall rank\n  correlation coefficient", "comments": "19 pages, 6 figures and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a simple and easy-to-implement Bayesian hypothesis\ntest for the presence of an association, described by Kendall's tau\ncoefficient, between two variables measured on at least an ordinal scale. Owing\nto the absence of the likelihood functions for the data, we employ the\nasymptotic sampling distributions of the test statistic as the working\nlikelihoods and then specify a truncated normal prior distribution on the\nnoncentrality parameter of the alternative hypothesis, which results in the\nBayes factor available in closed form in terms of the cumulative distribution\nfunction of the standard normal distribution. Investigating the asymptotic\nbehavior of the Bayes factor we find the conditions of the priors so that it is\nconsistent to whichever the hypothesis is true. Simulation studies and a\nreal-data application are used to illustrate the effectiveness of the proposed\nBayes factor. It deserves mentioning that the proposed method can be easily\ncovered in undergraduate and graduate courses in nonparametric statistics with\nan emphasis on students' Bayesian thinking for data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 00:33:27 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zhang", "Shen", ""], ["Ye", "Keying", ""], ["Wang", "Min", ""]]}, {"id": "2105.00416", "submitter": "Yoshiyuki Ninomiya", "authors": "Yoshiyuki Ninomiya, Yuta Umezu, Ichiro Takeuchi", "title": "Selective Inference in Propensity Score Analysis", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Selective inference (post-selection inference) is a methodology that has\nattracted much attention in recent years in the fields of statistics and\nmachine learning. Naive inference based on data that are also used for model\nselection tends to show an overestimation, and so the selective inference\nconditions the event that the model was selected. In this paper, we develop\nselective inference in propensity score analysis with a semiparametric\napproach, which has become a standard tool in causal inference. Specifically,\nfor the most basic causal inference model in which the causal effect can be\nwritten as a linear sum of confounding variables, we conduct Lasso-type\nvariable selection by adding an $\\ell_1$ penalty term to the loss function that\ngives a semiparametric estimator. Confidence intervals are then given for the\ncoefficients of the selected confounding variables, conditional on the event of\nvariable selection, with asymptotic guarantees. An important property of this\nmethod is that it does not require modeling of nonparametric regression\nfunctions for the outcome variables, as is usually the case with semiparametric\npropensity score analysis.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 08:14:29 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ninomiya", "Yoshiyuki", ""], ["Umezu", "Yuta", ""], ["Takeuchi", "Ichiro", ""]]}, {"id": "2105.00455", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Thomas A. Lasko", "title": "Synthesized Difference in Differences", "comments": "Accepted to ACM BCB 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating the conditional average treatment effect for everyone\nby eliminating confounding and selection bias. Unfortunately, randomized\nclinical trials (RCTs) eliminate confounding but impose strict exclusion\ncriteria that prevent sampling of the entire clinical population. Observational\ndatasets are more inclusive but suffer from confounding. We therefore analyze\nRCT and observational data simultaneously in order to extract the strengths of\neach. Our solution builds upon Difference in Differences (DD), an algorithm\nthat eliminates confounding from observational data by comparing outcomes\nbefore and after treatment administration. DD requires a parallel slopes\nassumption that may not apply in practice when confounding shifts across time.\nWe instead propose Synthesized Difference in Differences (SDD) that infers the\ncorrect (possibly non-parallel) slopes by linearly adjusting a conditional\nversion of DD using additional RCT data. The algorithm achieves state of the\nart performance across multiple synthetic and real datasets even when the RCT\nexcludes the majority of patients.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 12:19:16 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 02:19:44 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Strobl", "Eric V.", ""], ["Lasko", "Thomas A.", ""]]}, {"id": "2105.00482", "submitter": "Aba Diop", "authors": "Aba Diop, El Hadji Deme, Aliou Diop", "title": "Zero-inflated generalized extreme value regression model for binary data\n  and application in health study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression model is widely used in many studies to investigate the\nrelationship between a binary response variable $Y$ and a set of potential\npredictors $\\mathbf X$. The binary response may represent, for example, the\noccurrence of some outcome of interest ($Y=1$ if the outcome occurred and $Y=0$\notherwise). When the dependent variable $Y$ represents a rare event, the\nlogistic regression model shows relevant drawbacks. In order to overcome these\ndrawbacks we propose the Generalized Extreme Value (GEV) regression model. In\nparticular, we suggest the quantile function of the GEV distribution as link\nfunction, so our attention is focused on the tail of the response curve for\nvalues close to one. A sample of observations is said to contain a cure\nfraction when a proportion of the study subjects (the so-called cured\nindividuals, as opposed to the susceptibles) cannot experience the outcome of\ninterest. One problem arising then is that it is usually unknown who are the\ncured and the susceptible subjects, unless the outcome of interest has been\nobserved. In these settings, a logistic regression analysis of the relationship\nbetween $\\mathbf X$ and $Y$ among the susceptibles is no more straightforward.\nWe develop a maximum likelihood estimation procedure for this problem, based on\nthe joint modeling of the binary response of interest and the cure status. We\ninvestigate the identifiability of the resulting model. Then, we conduct a\nsimulation study to investigate its finite-sample behavior, and application to\nreal data.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 14:24:11 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Diop", "Aba", ""], ["Deme", "El Hadji", ""], ["Diop", "Aliou", ""]]}, {"id": "2105.00489", "submitter": "Aba Diop", "authors": "Aba Diop, El Hadji Deme", "title": "Parametric bootstrapping in a generalized extreme value regression model\n  for binary response", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized extreme value (GEV) regression is often more adapted when we\ninvestigate a relationship between a binary response variable $Y$ which\nrepresents a rare event and potentiel predictors $\\mathbf{X}$. In particular,\nwe use the quantile function of the GEV distribution as link function.\nBootstrapping assigns measures of accuracy (bias, variance, confidence\nintervals, prediction error, test of hypothesis) to sample estimates. This\ntechnique allows estimation of the sampling distribution of almost any\nstatistic using random sampling methods. Bootstrapping estimates the properties\nof an estimator by measuring those properties when sampling from an\napproximating distribution. In this paper, we fitted the generalized extreme\nvalue regression model, then we performed parametric bootstrap method for\ntesting hupthesis, estimating confidence interval of parameters for generalized\nextreme value regression model and a real data application.\n", "versions": [{"version": "v1", "created": "Sun, 2 May 2021 14:43:31 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Diop", "Aba", ""], ["Deme", "El Hadji", ""]]}, {"id": "2105.00581", "submitter": "Guanhua Chen", "authors": "Rui Chen, Jared D. Huling, Guanhua Chen, Menggang Yu", "title": "Robust Sample Weighting to Facilitate Individualized Treatment Rule\n  Learning for a Target Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning individualized treatment rules (ITRs) is an important topic in\nprecision medicine. Current literature mainly focuses on deriving ITRs from a\nsingle source population. We consider the observational data setting when the\nsource population differs from a target population of interest. We assume\nsubject covariates are available from both populations, but treatment and\noutcome data are only available from the source population. Although adjusting\nfor differences between source and target populations can potentially lead to\nan improved ITR for the target population, it can substantially increase the\nvariability in ITR estimation. To address this dilemma, we develop a weighting\nframework that aims to tailor an ITR for a given target population and protect\nagainst high variability due to superfluous covariate shift adjustments. Our\nmethod seeks covariate balance over a nonparametric function class\ncharacterized by a reproducing kernel Hilbert space and can improve many ITR\nlearning methods that rely on weights. We show that the proposed method\nencompasses importance weights and the so-called overlap weights as two extreme\ncases, allowing for a better bias-variance trade-off in between. Numerical\nexamples demonstrate that the use of our weighting method can greatly improve\nITR estimation for the target population compared with other weighting methods.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 00:05:18 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chen", "Rui", ""], ["Huling", "Jared D.", ""], ["Chen", "Guanhua", ""], ["Yu", "Menggang", ""]]}, {"id": "2105.00672", "submitter": "Stavros Nikolakopoulos", "authors": "Stavros Nikolakopoulos", "title": "Misuse of the sign test in narrative synthesis of evidence", "comments": "6 pages, 1 figure", "journal-ref": "Res Synth Methods 2020 Sep;11(5):714-719", "doi": "10.1002/jrsm.1427", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In narrative synthesis of evidence, it can be the case that the only\nquantitative measures available concerning the efficacy of an intervention is\nthe direction of the effect, i.e. whether it is positive or negative. In such\nsituations, the sign test has been proposed in the literature and in recent\nCochrane guidelines as a way to test whether the proportion of positive effects\nis favourable. I argue that the sign test is inappropriate in this context as\nthe data are not generated according to the Binomial distribution it employs. I\ndemonstrate possible consequences for both hypothesis testing and estimation\nvia hypothetical examples.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 08:02:57 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 06:49:59 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Nikolakopoulos", "Stavros", ""]]}, {"id": "2105.00700", "submitter": "David Mori\\~na Prof.", "authors": "David Mori\\~na, Pedro Puig and Albert Navarro", "title": "Analysis of zero inflated dichotomous variables from a Bayesian\n  perspective: Application to occupational health", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work proposes a new methodology to fit zero inflated Bernoulli data from\na Bayesian approach, able to distinguish between two potential sources of zeros\n(structurals and non-structurals). Its usage is illustrated by means of a real\nexample from the field of occupational health as the phenomenon of sickness\npresenteeism, in which it is reasonable to think that some individuals will\nnever be at risk of suffering it because they have not been sick in the period\nof study (structural zeros). Without separating structural and non-structural\nzeros one would one would be studying jointly the general health status and the\npresenteeism itself, and therefore obtaining potentially biased estimates as\nthe phenomenon is being implicitly underestimated by diluting it into the\ngeneral health status. The proposed methodology performance has been evaluated\nthrough a comprehensive simulation study, and it has been compiled as an R\npackage freely available to the community.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 09:13:08 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Mori\u00f1a", "David", ""], ["Puig", "Pedro", ""], ["Navarro", "Albert", ""]]}, {"id": "2105.00819", "submitter": "Schyan Zafar", "authors": "Schyan Zafar and Geoff Nicholls", "title": "Measuring diachronic sense change: new models and Monte Carlo methods\n  for Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a bag-of-words model, the senses of a word with multiple meanings, e.g.\n\"bank\" (used either in a river-bank or an institution sense), are represented\nas probability distributions over context words, and sense prevalence is\nrepresented as a probability distribution over senses. Both of these may change\nwith time. Modelling and measuring this kind of sense change is challenging due\nto the typically high-dimensional parameter space and sparse datasets. A\nrecently published corpus of ancient Greek texts contains expert-annotated\nsense labels for selected target words. Automatic sense-annotation for the word\n\"kosmos\" (meaning decoration, order or world) has been used as a test case in\nrecent work with related generative models and Monte Carlo methods. We adapt an\nexisting generative sense change model to develop a simpler model for the main\neffects of sense and time, and give MCMC methods for Bayesian inference on all\nthese models that are more efficient than existing methods. We carry out\nautomatic sense-annotation of snippets containing \"kosmos\" using our model, and\nmeasure the time-evolution of its three senses and their prevalence. As far as\nwe are aware, ours is the first analysis of this data, within the class of\ngenerative models we consider, that quantifies uncertainty and returns credible\nsets for evolving sense prevalence in good agreement with those given by expert\nannotation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:40:21 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zafar", "Schyan", ""], ["Nicholls", "Geoff", ""]]}, {"id": "2105.00860", "submitter": "Giovanni Ballarin", "authors": "Giovanni Ballarin", "title": "Ridge Regularized Estimation of VAR Models for Inference and Sieve\n  Approximation", "comments": "46 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developments in statistical learning have fueled the analysis of\nhigh-dimensional time series. However, even in low-dimensional contexts the\nissues arising from ill-conditioned regression problems are well-known. Because\nlinear time series modeling is naturally prone to such issues, I propose to\napply ridge regression to the estimation of dense VAR models. Theoretical\nnon-asymptotic results concerning the addition of a ridge-type penalty to the\nleast squares estimator are discussed, while standard asymptotic and inference\ntechniques are proven to be valid under mild conditions on the regularizer. The\nproposed estimator is then applied to the problem of sieve approximation of\nVAR($\\infty$) processes under moderately harsh sample sizes. Simulation\nevidence is used to discuss the small sample properties of the ridge estimator\n(RLS) when compared to least squares and local projection approaches: I use a\nMonte Carlo exercise to argue that RLS with a lag-adapted cross-validated\nregularizer achieve meaningfully better performance in recovering impulse\nresponse functions and asymptotic confidence intervals than other common\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 13:44:55 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ballarin", "Giovanni", ""]]}, {"id": "2105.00879", "submitter": "Xavier D'Haultfoeuille", "authors": "Laurent Davezies, Xavier D'Haultfoeuille and Louise Laage", "title": "Identification and Estimation of Average Marginal Effects in Fixed\n  Effects Logit Models", "comments": "68 pages (online appendix starting at p.45), 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers average marginal effects (AME) in a panel data fixed\neffects logit model. Relating the identified set of the AME to an extremal\nmoment problem, we first show how to obtain sharp bounds on the AME\nstraightforwardly, without any optimization. Then, we consider two strategies\nto build confidence intervals on the AME. In the first, we estimate the sharp\nbounds with a semiparametric two-step estimator. The second, very simple\nstrategy estimates instead a quantity known to be at a bounded distance from\nthe AME. It does not require any nonparametric estimation but may result in\nlarger confidence intervals. Monte Carlo simulations suggest that both\napproaches work well in practice, the second being often very competitive.\nFinally, we show that our results also apply to average treatment effects, the\naverage structural functions and ordered, fixed effects logit models.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 14:01:06 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 14:24:19 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Davezies", "Laurent", ""], ["D'Haultfoeuille", "Xavier", ""], ["Laage", "Louise", ""]]}, {"id": "2105.00918", "submitter": "Xingguo Wu", "authors": "Xingguo Wu", "title": "Explanation of multicollinearity using the decomposition theorem of\n  ordinary linear regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In a multiple linear regression model, the algebraic formula of the\ndecomposition theorem explains the relationship between the univariate\nregression coefficient and partial regression coefficient using geometry. It\nwas found that univariate regression coefficients are decomposed into their\nrespective partial regression coefficients according to the parallelogram rule.\nMulticollinearity is analyzed with the help of the decomposition theorem. It\nwas also shown that it is a sample phenomenon that the partial regression\ncoefficients of important explanatory variables are not significant, but the\nsign expectation deviation cause may be the population structure between the\nexplained variables and explanatory variables or may be the result of sample\nselection. At present, some methods of diagnostic multicollinearity only\nconsider the correlation of explanatory variables, so these methods are\nbasically unreliable, and handling multicollinearity is blind before the causes\nare not distinguished. The increase in the sample size can help identify the\ncauses of multicollinearity, and the difference method can play an auxiliary\nrole.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 09:09:15 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Wu", "Xingguo", ""]]}, {"id": "2105.00946", "submitter": "Jad Beyhum", "authors": "Jad Beyhum, Jean-Pierre Florens, Ingrid Van Keilegom", "title": "A nonparametric instrumental approach to endogeneity in competing risks\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses endogenous treatment models with duration outcomes,\ncompeting risks and random right censoring. The endogeneity issue is solved\nusing a discrete instrumental variable. We show that the competing risks model\ngenerates a non-parametric quantile instrumental regression problem. The\ncause-specific cumulative incidence, the cause-specific hazard and the\nsubdistribution hazard can be recovered from the regression function. A\ndistinguishing feature of the model is that censoring and competing risks\nprevent identification at some quantiles. We characterize the set of quantiles\nfor which exact identification is possible and give partial identification\nresults for other quantiles. We outline an estimation procedure and discuss its\nproperties. The finite sample performance of the estimator is evaluated through\nsimulations. We apply the proposed method to the Health Insurance Plan of\nGreater New York experiment.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:22:27 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Beyhum", "Jad", ""], ["Florens", "Jean-Pierre", ""], ["Van Keilegom", "Ingrid", ""]]}, {"id": "2105.00953", "submitter": "Shishi Liu", "authors": "Shishi Liu, Hao Zhang, Jingxiao Zhang", "title": "Model Averaging Estimation for Partially Linear Functional Score Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with model averaging estimation for partially linear\nfunctional score models. These models predict a scalar response using both\nparametric effect of scalar predictors and non-parametric effect of a\nfunctional predictor. Within this context, we develop a Mallows-type criterion\nfor choosing weights. The resulting model averaging estimator is proved to be\nasymptotically optimal under certain regularity conditions in terms of\nachieving the smallest possible squared error loss. Simulation studies\ndemonstrate its superiority or comparability to information criterion\nscore-based model selection and averaging estimators. The proposed procedure is\nalso applied to two real data sets for illustration. That the components of\nnonparametric part are unobservable leads to a more complicated situation than\nordinary partially linear models (PLM) and a different theoretical derivation\nfrom those of PLM.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 15:36:52 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Liu", "Shishi", ""], ["Zhang", "Hao", ""], ["Zhang", "Jingxiao", ""]]}, {"id": "2105.00966", "submitter": "Shishi Liu", "authors": "Shishi Liu, Jingxiao Zhang", "title": "Model Averaging by Cross-validation for Partially Linear Functional\n  Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider averaging a number of candidate models to produce a prediction of\nlower risk in the context of partially linear functional additive models. These\nmodels incorporate the parametric effect of scalar variables and the additive\neffect of a functional variable to describe the relationship between a response\nvariable and regressors. We develop a model averaging scheme that assigns the\nweights by minimizing a cross-validation criterion. Under the framework of\nmodel misspecification, the resulting estimator is proved to be asymptotically\noptimal in terms of the lowest possible square error loss for prediction. Also,\nsimulation studies and real data analysis demonstrate the good performance of\nour proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:03:11 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 16:38:09 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Liu", "Shishi", ""], ["Zhang", "Jingxiao", ""]]}, {"id": "2105.00987", "submitter": "Patrick Rubin-Delanchy Dr", "authors": "Alexander Modell and Patrick Rubin-Delanchy", "title": "Spectral clustering under degree heterogeneity: a case for the random\n  walk Laplacian", "comments": "22 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that graph spectral embedding using the random walk\nLaplacian produces vector representations which are completely corrected for\nnode degree. Under a generalised random dot product graph, the embedding\nprovides uniformly consistent estimates of degree-corrected latent positions,\nwith asymptotically Gaussian error. In the special case of a degree-corrected\nstochastic block model, the embedding concentrates about K distinct points,\nrepresenting communities. These can be recovered perfectly, asymptotically,\nthrough a subsequent clustering step, without spherical projection, as commonly\nrequired by algorithms based on the adjacency or normalised, symmetric\nLaplacian matrices. While the estimand does not depend on degree, the\nasymptotic variance of its estimate does -- higher degree nodes are embedded\nmore accurately than lower degree nodes. Our central limit theorem therefore\nsuggests fitting a weighted Gaussian mixture model as the subsequent clustering\nstep, for which we provide an expectation-maximisation algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 16:36:27 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 07:20:12 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Modell", "Alexander", ""], ["Rubin-Delanchy", "Patrick", ""]]}, {"id": "2105.01039", "submitter": "Christian Staerk", "authors": "Christian Staerk, Maria Kateri, Ioannis Ntzoufras", "title": "A Metropolized adaptive subspace algorithm for high-dimensional Bayesian\n  variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple and efficient adaptive Markov Chain Monte Carlo (MCMC) method,\ncalled the Metropolized Adaptive Subspace (MAdaSub) algorithm, is proposed for\nsampling from high-dimensional posterior model distributions in Bayesian\nvariable selection. The MAdaSub algorithm is based on an independent\nMetropolis-Hastings sampler, where the individual proposal probabilities of the\nexplanatory variables are updated after each iteration using a form of Bayesian\nadaptive learning, in a way that they finally converge to the respective\ncovariates' posterior inclusion probabilities. We prove the ergodicity of the\nalgorithm and present a parallel version of MAdaSub with an adaptation scheme\nfor the proposal probabilities based on the combination of information from\nmultiple chains. The effectiveness of the algorithm is demonstrated via various\nsimulated and real data examples, including a high-dimensional problem with\nmore than 20,000 covariates.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 17:37:46 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Staerk", "Christian", ""], ["Kateri", "Maria", ""], ["Ntzoufras", "Ioannis", ""]]}, {"id": "2105.01098", "submitter": "Reza Hosseini", "authors": "Reza Hosseini, Kaixu Yang, Albert Chen, Sayan Patra", "title": "A flexible forecasting model for production systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper discusses desirable properties of forecasting models in production\nsystems. It then develops a family of models which are designed to satisfy\nthese properties: highly customizable to capture complex patterns; accommodates\na large variety of objectives; has interpretable components; produces robust\nresults; has automatic changepoint detection for trend and seasonality; and\nruns fast -- making it a good choice for reliable and scalable production\nsystems. The model allows for seasonality at various time scales,\nevents/holidays, and change points in trend and seasonality. The volatility is\nfitted separately to maintain flexibility and speed and is allowed to be a\nfunction of specified features.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 18:07:23 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Hosseini", "Reza", ""], ["Yang", "Kaixu", ""], ["Chen", "Albert", ""], ["Patra", "Sayan", ""]]}, {"id": "2105.01124", "submitter": "Ting Ye", "authors": "Ting Ye and Dylan S. Small", "title": "Combining Broad and Narrow Case Definitions in Matched Case-Control\n  Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a matched case-control study, cases are compared to noncases, who are\nsimilar in observed covariates, in terms of their retrospective exposure to a\ntreatment to assess the impact of the treatment on the outcome. In the absence\nof a gold standard case definition, there is often a broad case definition and\na narrow case definition. The broad case definition offers a larger sample size\nof cases but the narrow case definition may offer a larger effect size.\nRestricting to the narrow case definition may introduce selection bias because\nthe treatment may affect the type of a case a subject is. In this article, we\npropose a new sensitivity analysis framework for combining broad and narrow\ncase definitions in matched case-control studies, that considers the unmeasured\nconfounding bias and selection bias simultaneously. We develop a valid\nrandomization-based testing procedure using only the narrow case matched sets\nwhen the effect of the unmeasured confounder on receiving treatment and the\neffect of the treatment on case definition among the always-cases are\ncontrolled by sensitivity parameters. We then use the Bonferroni method to\ncombine the testing procedures using the broad and narrow case definitions. We\nalso study comprehensively the proposed testing procedures' sensitivity to\nunmeasured biases using the design sensitivity and extensive power analyses.\nOur method is applied to study whether having firearms at home increases\nsuicide risk.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 18:53:22 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ye", "Ting", ""], ["Small", "Dylan S.", ""]]}, {"id": "2105.01152", "submitter": "Andre Ribeiro", "authors": "Andre F. Ribeiro, Frank Neffke and Ricardo Hausmann", "title": "What can the millions of random treatments in nonexperimental data\n  reveal about causes?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a new method to estimate causal effects from nonexperimental data.\nEach pair of sample units is first associated with a stochastic 'treatment' -\ndifferences in factors between units - and an effect - a resultant outcome\ndifference. It is then proposed that all such pairs can be combined to provide\nmore accurate estimates of causal effects in observational data, provided a\nstatistical model connecting combinatorial properties of treatments to the\naccuracy and unbiasedness of their effects. The article introduces one such\nmodel and a Bayesian approach to combine the $O(n^2)$ pairwise observations\ntypically available in nonexperimnetal data. This also leads to an\ninterpretation of nonexperimental datasets as incomplete, or noisy, versions of\nideal factorial experimental designs.\n  This approach to causal effect estimation has several advantages: (1) it\nexpands the number of observations, converting thousands of individuals into\nmillions of observational treatments; (2) starting with treatments closest to\nthe experimental ideal, it identifies noncausal variables that can be ignored\nin the future, making estimation easier in each subsequent iteration while\ndeparting minimally from experiment-like conditions; (3) it recovers individual\ncausal effects in heterogeneous populations. We evaluate the method in\nsimulations and the National Supported Work (NSW) program, an intensively\nstudied program whose effects are known from randomized field experiments. We\ndemonstrate that the proposed approach recovers causal effects in common NSW\nsamples, as well as in arbitrary subpopulations and an order-of-magnitude\nlarger supersample with the entire national program data, outperforming\nStatistical, Econometrics and Machine Learning estimators in all cases...\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 20:13:34 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Ribeiro", "Andre F.", ""], ["Neffke", "Frank", ""], ["Hausmann", "Ricardo", ""]]}, {"id": "2105.01157", "submitter": "Neha Agarwala", "authors": "Neha Agarwala, Junyong Park and Anindya Roy", "title": "Efficient Integration of Aggregate Data and Individual Patient Data in\n  One-Way Mixed Models", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Often both Aggregate Data (AD) studies and Individual Patient Data (IPD)\nstudies are available for specific treatments. Combining these two sources of\ndata could improve the overall meta-analytic estimates of treatment effects.\nMoreover, often for some studies with AD, the associated IPD maybe available,\nalbeit at some extra effort or cost to the analyst. We propose a method for\ncombining treatment effects across trials when the response is from the\nexponential family of distribution and hence a generalized linear model\nstructure can be used. We consider the case when treatment effects are fixed\nand common across studies. Using the proposed combination method, we evaluate\nthe wisdom of choosing AD when IPD is available by studying the relative\nefficiency of analyzing all IPD studies versus combining various percentages of\nAD and IPD studies. For many different models design constraints under which\nthe AD estimators are the IPD estimators, and hence fully efficient, are known.\nFor such models we advocate a selection procedure that chooses AD studies over\nIPD studies in a manner that force least departure from design constraints and\nhence ensures a fully efficient combined AD and IPD estimator.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 20:24:18 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Agarwala", "Neha", ""], ["Park", "Junyong", ""], ["Roy", "Anindya", ""]]}, {"id": "2105.01184", "submitter": "Peng Ding", "authors": "Anqi Zhao and Peng Ding", "title": "Reconciling design-based and model-based causal inferences for\n  split-plot experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The split-plot design assigns different interventions at the whole-plot and\nsub-plot levels, respectively, and induces a group structure on the final\ntreatment assignments. A common strategy is to use the OLS fit of the outcome\non the treatment indicators coupled with the robust standard errors clustered\nat the whole-plot level. It does not give consistent estimator for the causal\neffects of interest when the whole-plot sizes vary. Another common strategy is\nto fit the linear mixed-effects model of the outcome with Normal random effects\nand errors. It is a purely model-based approach and can be sensitive to\nviolations of parametric assumptions. In contrast, the design-based inference\nassumes no outcome models and relies solely on the controllable randomization\nmechanism determined by the physical experiment. We first extend the existing\ndesign-based inference based on the {\\htf} estimator to the Hajek estimator,\nand establish the finite-population central limit theorem for both under\nsplit-plot randomization. We then reconcile the results with those under the\nmodel-based approach, and propose two regression strategies, namely (i) the WLS\nfit of the unit-level data based on the inverse probability weighting and (ii)\nthe OLS fit of the aggregate data based on whole-plot total outcomes, to\nreproduce the Hajek and {\\htf} estimators from least squares, respectively.\nThis, together with the asymptotic conservativeness of the corresponding\ncluster-robust covariances for estimating the true design-based covariances as\nwe establish in the process, justifies the validity of regression-based\nestimators for design-based inference. In light of the flexibility of\nregression formulation with covariate adjustment, we further extend the theory\nto the case with covariates and demonstrate the efficiency gain by\nregression-based covariate adjustment via both asymptotic theory and\nsimulation.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 21:38:33 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhao", "Anqi", ""], ["Ding", "Peng", ""]]}, {"id": "2105.01187", "submitter": "Zhengling Qi", "authors": "Zhengling Qi, Rui Miao, Xiaoke Zhang", "title": "Proximal Learning for Individualized Treatment Regimes Under Unmeasured\n  Confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven individualized decision making has recently received increasing\nresearch interests. Most existing methods rely on the assumption of no\nunmeasured confounding, which unfortunately cannot be ensured in practice\nespecially in observational studies. Motivated by the recent proposed proximal\ncausal inference, we develop several proximal learning approaches to estimating\noptimal individualized treatment regimes (ITRs) in the presence of unmeasured\nconfounding. In particular, we establish several identification results for\ndifferent classes of ITRs, exhibiting the trade-off between the risk of making\nuntestable assumptions and the value function improvement in decision making.\nBased on these results, we propose several classification-based approaches to\nfinding a variety of restricted in-class optimal ITRs and develop their\ntheoretical properties. The appealing numerical performance of our proposed\nmethods is demonstrated via an extensive simulation study and one real data\napplication.\n", "versions": [{"version": "v1", "created": "Mon, 3 May 2021 21:49:49 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 02:35:00 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Qi", "Zhengling", ""], ["Miao", "Rui", ""], ["Zhang", "Xiaoke", ""]]}, {"id": "2105.01264", "submitter": "Jue Hou", "authors": "Jue Hou, Zijian Guo and Tianxi Cai", "title": "Surrogate Assisted Semi-supervised Inference for High Dimensional Risk\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Risk modeling with EHR data is challenging due to a lack of direct\nobservations on the disease outcome, and the high dimensionality of the\ncandidate predictors. In this paper, we develop a surrogate assisted\nsemi-supervised-learning (SAS) approach to risk modeling with high dimensional\npredictors, leveraging a large unlabeled data on candidate predictors and\nsurrogates of outcome, as well as a small labeled data with annotated outcomes.\nThe SAS procedure borrows information from surrogates along with candidate\npredictors to impute the unobserved outcomes via a sparse working imputation\nmodel with moment conditions to achieve robustness against mis-specification in\nthe imputation model and a one-step bias correction to enable interval\nestimation for the predicted risk. We demonstrate that the SAS procedure\nprovides valid inference for the predicted risk derived from a high dimensional\nworking model, even when the underlying risk prediction model is dense and the\nrisk model is mis-specified. We present an extensive simulation study to\ndemonstrate the superiority of our SSL approach compared to existing supervised\nmethods. We apply the method to derive genetic risk prediction of type-2\ndiabetes mellitus using a EHR biobank cohort.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 03:08:51 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Hou", "Jue", ""], ["Guo", "Zijian", ""], ["Cai", "Tianxi", ""]]}, {"id": "2105.01278", "submitter": "Guodong Li", "authors": "Xiaoyu Zhang, Di Wang, Heng Lian and Guodong Li", "title": "Nonparametric Quantile Regression for Homogeneity Pursuit in Panel Data\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many panel data have the latent subgroup effect on individuals, and it is\nimportant to correctly identify these groups since the efficiency of resulting\nestimators can be improved significantly by pooling the information of\nindividuals within each group. However, the currently assumed parametric and\nsemiparametric relationship between the response and predictors may be\nmisspecified, which leads to a wrong grouping result, and the nonparametric\napproach hence can be considered to avoid such mistakes. Moreover, the response\nmay depend on predictors in different ways at various quantile levels, and the\ncorresponding grouping structure may also vary. To tackle these problems, this\npaper proposes a nonparametric quantile regression method for homogeneity\npursuit, and a pairwise fused penalty is used to automatically select the\nnumber of groups. The asymptotic properties are established, and an ADMM\nalgorithm is also developed. The finite sample performance is evaluated by\nsimulation experiments, and the usefulness of the proposed methodology is\nfurther illustrated by an empirical example.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 03:56:05 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhang", "Xiaoyu", ""], ["Wang", "Di", ""], ["Lian", "Heng", ""], ["Li", "Guodong", ""]]}, {"id": "2105.01322", "submitter": "Erniel Barrios", "authors": "Vladimir A. Malabanan, Joseph Ryan G. Lansangan and Erniel B. Barrios", "title": "Semiparametric Spatiotemporal Model with Mixed Frequencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In modelling time series data coming from different sources, frequencies can\neasily vary since some variable can be measured at higher frequencies, others,\nat lower frequencies. Given data measured over spatial units and at varying\nfrequencies, we postulated a semiparametric spatiotemporal model. This\noptimizes utilization of information from variables measured at higher\nfrequency by estimating its nonparametric effect on the response through the\nbackfitting algorithm in and additive modelling framework. Simulation studies\nsupport the optimality of the model over simple generalized additive model with\naggregation of high frequency predictors to match the dependent variable\nmeasured at lower frequency. With quarterly corn production and the dependent\nvariable, the model is fitted with predictors coming from remotely-sensed data\n(vegetation and precipitation indices), predictive ability is better compared\nto two benchmark generalized additive models.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 07:10:56 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Malabanan", "Vladimir A.", ""], ["Lansangan", "Joseph Ryan G.", ""], ["Barrios", "Erniel B.", ""]]}, {"id": "2105.01460", "submitter": "Harrison Zhu", "authors": "Harrison Zhu, Adam Howes, Owen van Eer, Maxime Rischard, Dino\n  Sejdinovic, Seth Flaxman", "title": "Multi-resolution Spatial Regression for Aggregated Data with an\n  Application to Crop Yield Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a new methodology for spatial regression of aggregated outputs on\nmulti-resolution covariates. Such problems often occur with spatial data, for\nexample in crop yield prediction, where the output is spatially-aggregated over\nan area and the covariates may be observed at multiple resolutions. Building\nupon previous work on aggregated output regression, we propose a regression\nframework to synthesise the effects of the covariates at different resolutions\non the output and provide uncertainty estimation. We show that, for a crop\nyield prediction problem, our approach is more scalable, via variational\ninference, than existing multi-resolution regression models. We also show that\nour framework yields good predictive performance, compared to existing\nmulti-resolution crop yield models, whilst being able to provide estimation of\nthe underlying spatial effects.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 12:41:33 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Zhu", "Harrison", ""], ["Howes", "Adam", ""], ["van Eer", "Owen", ""], ["Rischard", "Maxime", ""], ["Sejdinovic", "Dino", ""], ["Flaxman", "Seth", ""]]}, {"id": "2105.01496", "submitter": "Lucas Kock", "authors": "Lucas Kock, Nadja Klein, David J. Nott", "title": "Variational Inference and Sparsity in High-Dimensional Deep Gaussian\n  Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian mixture models are a popular tool for model-based clustering, and\nmixtures of factor analyzers are Gaussian mixture models having parsimonious\nfactor covariance structure for mixture components. There are several recent\nextensions of mixture of factor analyzers to deep mixtures, where the Gaussian\nmodel for the latent factors is replaced by a mixture of factor analyzers. This\nconstruction can be iterated to obtain a model with many layers. These deep\nmodels are challenging to fit, and we consider Bayesian inference using\nsparsity priors to further regularize the estimation. A scalable natural\ngradient variational inference algorithm is developed for fitting the model,\nand we suggest computationally efficient approaches to the architecture choice\nusing overfitted mixtures where unnecessary components drop out in the\nestimation. In a number of simulated and two real examples, we demonstrate the\nversatility of our approach for high-dimensional problems, and demonstrate that\nthe use of sparsity inducing priors can be helpful for obtaining improved\nclustering results.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 13:48:47 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Kock", "Lucas", ""], ["Klein", "Nadja", ""], ["Nott", "David J.", ""]]}, {"id": "2105.01501", "submitter": "Daniel Wilson", "authors": "Daniel J. Wilson", "title": "The L\\'evy combination test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A novel class of methods for combining $p$-values to perform aggregate\nhypothesis tests has emerged that exploit the properties of heavy-tailed Stable\ndistributions. These methods offer important practical advantages including\nrobustness to dependence and better-than-Bonferroni scaleability, and they\nreveal theoretical connections between Bayesian and classical hypothesis tests.\nThe harmonic mean $p$-value (HMP) procedure is based on the convergence of\nsummed inverse $p$-values to the Landau distribution, while the Cauchy\ncombination test (CCT) is based on the self-similarity of summed\nCauchy-transformed $p$-values. The CCT has the advantage that it is analytic\nand exact. The HMP has the advantage that it emulates a model-averaged Bayes\nfactor, is insensitive to $p$-values near 1, and offers multilevel testing via\na closed testing procedure. Here I investigate whether other Stable combination\ntests can combine these benefits, and identify a new method, the L\\'evy\ncombination test (LCT). The LCT exploits the self-similarity of sums of L\\'evy\nrandom variables transformed from $p$-values. Under arbitrary dependence, the\nLCT possesses better robustness than the CCT and HMP, with two-fold worst-case\ninflation at small significance thresholds. It controls the strong-sense\nfamilywise error rate through a multilevel test uniformly more powerful than\nBonferroni. Simulations show that the LCT behaves like Simes' test in some\nrespects, with power intermediate between the HMP and Bonferroni. The LCT\nrepresents an interesting and attractive addition to combined testing methods\nbased on heavy-tailed distributions.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 13:58:44 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Wilson", "Daniel J.", ""]]}, {"id": "2105.01552", "submitter": "Tao Li", "authors": "Tao Li and Cheng Meng", "title": "Modern Subsampling Methods for Large-Scale Least Squares Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subsampling methods aim to select a subsample as a surrogate for the observed\nsample. As a powerful technique for large-scale data analysis, various\nsubsampling methods are developed for more effective coefficient estimation and\nmodel prediction. This review presents some cutting-edge subsampling methods\nbased on the large-scale least squares estimation. Two major families of\nsubsampling methods are introduced, respectively, the randomized subsampling\napproach and the optimal subsampling approach. The former aims to develop a\nmore effective data-dependent sampling probability, while the latter aims to\nselect a deterministic subsample in accordance with certain optimality\ncriteria. Real data examples are provided to compare these methods empirically,\nrespecting both the estimation accuracy and the computing time.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:05:39 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Li", "Tao", ""], ["Meng", "Cheng", ""]]}, {"id": "2105.01557", "submitter": "Jordi Tur", "authors": "Jordi Tur (Centre de Recerca Matem\\`atica), David Mori\\~na (Department\n  of Econometrics, Statistics and Applied Economics, Riskcenter-IREA,\n  Universitat de Barcelona), Pedro Puig (Barcelona Graduate School of\n  Mathematics (BGSMath), Departament de Matem\\`atiques, Universitat Aut\\`onoma\n  de Barcelona), Alejandra Caba\\~na (Barcelona Graduate School of Mathematics\n  (BGSMath), Departament de Matem\\`atiques, Universitat Aut\\`onoma de\n  Barcelona), Argimiro Arratia (Department of Computer Science, Universitat\n  Polit\\`ecnica de Catalunya), Amanda Fern\\'andez-Fontelo (Chair of Statistics,\n  School of Business and Economics, Humboldt-Universit\\\"at zu Berlin)", "title": "Good distribution modelling with the R package good", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Although models for count data with over-dispersion have been widely\nconsidered in the literature, models for under-dispersion -- the opposite\nphenomenon -- have received less attention as it is only relatively common in\nparticular research fields such as biodosimetry and ecology. The Good\ndistribution is a flexible alternative for modelling count data showing either\nover-dispersion or under-dispersion, although no R packages are still available\nto the best of our knowledge. We aim to present in the following the R package\ngood that computes the standard probabilistic functions (i.e., probability\ndensity function, cumulative distribution function, and quantile function) and\ngenerates random samples from a population following a Good distribution. The\npackage also considers a function for Good regression, including covariates in\na similar way to that of the standard glm function. We finally show the use of\nsuch a package with some real-world data examples addressing both\nover-dispersion and especially under-dispersion.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:14:03 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Tur", "Jordi", "", "Centre de Recerca Matem\u00e0tica"], ["Mori\u00f1a", "David", "", "Department\n  of Econometrics, Statistics and Applied Economics, Riskcenter-IREA,\n  Universitat de Barcelona"], ["Puig", "Pedro", "", "Barcelona Graduate School of\n  Mathematics"], ["Caba\u00f1a", "Alejandra", "", "Barcelona Graduate School of Mathematics"], ["Arratia", "Argimiro", "", "Department of Computer Science, Universitat\n  Polit\u00e8cnica de Catalunya"], ["Fern\u00e1ndez-Fontelo", "Amanda", "", "Chair of Statistics,\n  School of Business and Economics, Humboldt-Universit\u00e4t zu Berlin"]]}, {"id": "2105.01562", "submitter": "Juergen Lerner", "authors": "J\\\"urgen Lerner and Marian-Gabriel H\\^ancean", "title": "Micro-level network dynamics of scientific collaboration and impact:\n  relational hyperevent models for the analysis of coauthor networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss a recently proposed family of statistical network models -\nrelational hyperevent models (RHEM) - for analyzing team selection and team\nperformance in scientific coauthor networks. The underlying rationale for using\nRHEM in studies of coauthor networks is that scientific collaboration is\nintrinsically polyadic, that is, it typically involves teams of any size.\nConsequently, RHEM specify publication rates associated with hyperedges\nrepresenting groups of scientists of any size. Going beyond previous work on\nRHEM for meeting data, we adapt this model family to settings in which\nrelational hyperevents have a dedicated outcome, such as a scientific paper\nwith a measurable impact (e.g., the received number of citations). Relational\noutcome can on the one hand be used to specify additional explanatory variables\nin RHEM since the probability of coauthoring may be influenced, for instance,\nby prior (shared) success of scientists. On the other hand relational outcome\ncan also serve as a response variable in models seeking to explain the\nperformance of scientific teams. To tackle the latter we propose relational\nhyperevent outcome models (RHOM) that are closely related with RHEM to the\npoint that both model families can specify the likelihood of scientific\ncollaboration - and the expected performance, respectively - with the same set\nof explanatory variables allowing to assess, for instance, whether variables\nleading to increased collaboration also tend to increase scientific impact. For\nillustration, we apply RHEM to empirical coauthor networks comprising more than\n350,000 published papers by scientists working in three scientific disciplines.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:22:06 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Lerner", "J\u00fcrgen", ""], ["H\u00e2ncean", "Marian-Gabriel", ""]]}, {"id": "2105.01566", "submitter": "Zachary Pisano", "authors": "Zachary M. Pisano and Daniel Q. Naiman and Carey E. Priebe", "title": "Occam Factor for Gaussian Models With Unknown Variance Structure", "comments": "46 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We discuss model selection to determine whether the variance-covariance\nmatrix of a multivariate Gaussian model with known mean should be considered to\nbe a constant diagonal, a non-constant diagonal, or an arbitrary positive\ndefinite matrix. Of particular interest is the relationship between Bayesian\nevidence and the flexibility penalty due to Priebe and Rougier. For the case of\nan exponential family in canonical form equipped with a conjugate prior for the\ncanonical parameter, flexibility may be exactly decomposed into the usual BIC\nlikelihood penalty and a $O_p(1)$ term, the latter of which we explicitly\ncompute. We also investigate the asymptotics of Bayes factors for linearly\nnested canonical exponential families equipped with conjugate priors; in\nparticular, we find the exact rates at which Bayes factors correctly diverge in\nfavor of the correct model: linearly and logarithmically in the number of\nobservations when the full and nested models are true, respectively. Such\ntheoretical considerations for the general case permit us to fully express the\nasymptotic behavior of flexibility and Bayes factors for the\nvariance-covariance structure selection problem when we assume that the prior\nfor the model precision is a member of the gamma/Wishart family of\ndistributions or is uninformative. Simulations demonstrate evidence's immediate\nand superior performance in model selection compared to approximate criteria\nsuch as the BIC. We extend the framework to the multivariate Gaussian linear\nmodel with three data-driven examples.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 15:29:36 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Pisano", "Zachary M.", ""], ["Naiman", "Daniel Q.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "2105.01654", "submitter": "Assaf Rabinowicz", "authors": "Assaf Rabinowicz, Saharon Rosset", "title": "Resampling Methods for Detecting Anisotropic Correlation Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes parametric and non-parametric hypothesis testing\nalgorithms for detecting anisotropy -- rotational variance of the covariance\nfunction in random fields. Both algorithms are based on resampling mechanisms,\nwhich enable avoiding relying on asymptotic assumptions, as is common in\nprevious algorithms. The algorithms' performance is illustrated numerically in\nsimulation experiments and on real datasets representing a variety of potential\nchallenges.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 17:58:51 GMT"}, {"version": "v2", "created": "Sun, 6 Jun 2021 14:32:56 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Rabinowicz", "Assaf", ""], ["Rosset", "Saharon", ""]]}, {"id": "2105.01733", "submitter": "Bart J. A. Mertens", "authors": "Bart J. A. Mertens", "title": "Calibration of prediction rules for life-time outcomes using prognostic\n  Cox regression survival models and multiple imputations to account for\n  missing predictor data with cross-validatory assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we expand the methodology presented in Mertens et. al (2020,\nBiometrical Journal) to the study of life-time (survival) outcome which is\nsubject to censoring and when imputation is used to account for missing values.\nWe consider the problem where missing values can occur in both the calibration\ndata as well as newly - to-be-predicted - observations (validation). We focus\non the Cox model. Methods are described to combine imputation with predictive\ncalibration in survival modeling subject to censoring. Application to\ncross-validation is discussed. We demonstrate how conclusions broadly confirm\nthe first paper which restricted to the study of binary outcomes only.\nSpecifically prediction-averaging appears to have superior statistical\nproperties, especially smaller predictive variation, as opposed to a direct\napplication of Rubin's rules. Distinct methods for dealing with the baseline\nhazards are discussed when using Rubin's rules-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 20:10:12 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Mertens", "Bart J. A.", ""]]}, {"id": "2105.01783", "submitter": "Chanwoo Lee", "authors": "Chanwoo Lee, Lexin Li, Hao Helen Zhang, and Miaoyan Wang", "title": "Nonparametric Trace Regression in High Dimensions via Sign Series\n  Representation", "comments": "66 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning of matrix-valued data has recently surged in a range of scientific\nand business applications. Trace regression is a widely used method to model\neffects of matrix predictors and has shown great success in matrix learning.\nHowever, nearly all existing trace regression solutions rely on two\nassumptions: (i) a known functional form of the conditional mean, and (ii) a\nglobal low-rank structure in the entire range of the regression function, both\nof which may be violated in practice. In this article, we relax these\nassumptions by developing a general framework for nonparametric trace\nregression models via structured sign series representations of high\ndimensional functions. The new model embraces both linear and nonlinear trace\neffects, and enjoys rank invariance to order-preserving transformations of the\nresponse. In the context of matrix completion, our framework leads to a\nsubstantially richer model based on what we coin as the \"sign rank\" of a\nmatrix. We show that the sign series can be statistically characterized by\nweighted classification tasks. Based on this connection, we propose a learning\nreduction approach to learn the regression model via a series of classifiers,\nand develop a parallelable computation algorithm to implement sign series\naggregations. We establish the excess risk bounds, estimation error rates, and\nsample complexities. Our proposal provides a broad nonparametric paradigm to\nmany important matrix learning problems, including matrix regression, matrix\ncompletion, multi-task learning, and compressed sensing. We demonstrate the\nadvantages of our method through simulations and two applications, one on brain\nconnectivity study and the other on high-rank image completion.\n", "versions": [{"version": "v1", "created": "Tue, 4 May 2021 22:20:00 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Lee", "Chanwoo", ""], ["Li", "Lexin", ""], ["Zhang", "Hao Helen", ""], ["Wang", "Miaoyan", ""]]}, {"id": "2105.01874", "submitter": "Yunhua Xiang", "authors": "Yunhua Xiang, Tianyu Zhang, Xu Wang, Ali Shojaie, Noah Simon", "title": "On the Optimality of Nuclear-norm-based Matrix Completion for Problems\n  with Smooth Non-linear Structure", "comments": "47 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Originally developed for imputing missing entries in low rank, or\napproximately low rank matrices, matrix completion has proven widely effective\nin many problems where there is no reason to assume low-dimensional linear\nstructure in the underlying matrix, as would be imposed by rank constraints. In\nthis manuscript, we build some theoretical intuition for this behavior. We\nconsider matrices which are not necessarily low-rank, but lie in a\nlow-dimensional non-linear manifold. We show that nuclear-norm penalization is\nstill effective for recovering these matrices when observations are missing\ncompletely at random. In particular, we give upper bounds on the rate of\nconvergence as a function of the number of rows, columns, and observed entries\nin the matrix, as well as the smoothness and dimension of the non-linear\nembedding. We additionally give a minimax lower bound: This lower bound agrees\nwith our upper bound (up to a logarithmic factor), which shows that\nnuclear-norm penalization is (up to log terms) minimax rate optimal for these\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 05:34:32 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Xiang", "Yunhua", ""], ["Zhang", "Tianyu", ""], ["Wang", "Xu", ""], ["Shojaie", "Ali", ""], ["Simon", "Noah", ""]]}, {"id": "2105.02030", "submitter": "Christian Bartels", "authors": "Christian Bartels and Thomas Dumortier", "title": "Inverse probability of censoring weighting for visual predictive checks\n  of time-to-event models with time-varying covariates", "comments": null, "journal-ref": "Pharmaceutical Statistics (2021) 1-10", "doi": "10.1002/pst.2124", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  When constructing models to summarize clinical data to be used for\nsimulations, it is good practice to evaluate the models for their capacity to\nreproduce the data. This can be done by means of Visual Predictive Checks\n(VPC), which consist of (1) several reproductions of the original study by\nsimulation from the model under evaluation, (2) calculating estimates of\ninterest for each simulated study and (3) comparing the distribution of those\nestimates with the estimate from the original study. This procedure is a\ngeneric method that is straightforward to apply, in general. Here we consider\nthe application of the method to time to event data and consider the special\ncase when a time-varying covariate is not known or cannot be approximated after\nevent time. In this case, simulations cannot be conducted beyond the end of the\nfollow-up time (event or censoring time) in the original study. Thus, the\nsimulations must be censored at the end of the follow-up time. Since this\ncensoring is not random, the standard KM estimates from the simulated studies\nand the resulting VPC will be biased. We propose to use inverse probability of\ncensoring weighting (IPoC) method to correct the KM estimator for the simulated\nstudies and obtain unbiased VPCs. For analyzing the Cantos study, the IPoC\nweighting as described here proved valuable and enabled the generation of VPCs\nto qualify PKPD models for simulations. Here, we use a generated data set,\nwhich allows illustration of the different situations and evaluation against\nthe known truth.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 12:59:30 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Bartels", "Christian", ""], ["Dumortier", "Thomas", ""]]}, {"id": "2105.02071", "submitter": "Xavier de Luna", "authors": "Niloofar Moosavi, Jenny H\\\"aggstr\\\"om and Xavier de Luna", "title": "The costs and benefits of uniformly valid causal inference with\n  high-dimensional nuisance parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Important advances have recently been achieved in developing procedures\nyielding uniformly valid inference for a low dimensional causal parameter when\nhigh-dimensional nuisance models must be estimated. In this paper, we review\nthe literature on uniformly valid causal inference and discuss the costs and\nbenefits of using uniformly valid inference procedures. Naive estimation\nstrategies based on regularisation, machine learning, or a preliminary model\nselection stage for the nuisance models have finite sample distributions which\nare badly approximated by their asymptotic distributions. To solve this serious\nproblem, estimators which converge uniformly in distribution over a class of\ndata generating mechanisms have been proposed in the literature. In order to\nobtain uniformly valid results in high-dimensional situations, sparsity\nconditions for the nuisance models need typically to be made, although a double\nrobustness property holds, whereby if one of the nuisance model is more sparse,\nthe other nuisance model is allowed to be less sparse. While uniformly valid\ninference is a highly desirable property, uniformly valid procedures pay a high\nprice in terms of inflated variability. Our discussion of this dilemma is\nillustrated by the study of a double-selection outcome regression estimator,\nwhich we show is uniformly asymptotically unbiased, but is less variable than\nuniformly valid estimators in the numerical experiments conducted.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:13:41 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Moosavi", "Niloofar", ""], ["H\u00e4ggstr\u00f6m", "Jenny", ""], ["de Luna", "Xavier", ""]]}, {"id": "2105.02077", "submitter": "Bas Penning De Vries", "authors": "Bas B.L. Penning de Vries, Rolf H.H. Groenwold", "title": "Identification of causal effects in case-control studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Case-control designs are an important tool in contrasting the effects of\nwell-defined treatments. In this paper, we reconsider classical concepts,\nassumptions and principles and explore when the results of case-control studies\ncan be endowed a causal interpretation. Our focus is on identification of\ntarget causal quantities, or estimands. We cover various estimands relating to\nintention-to-treat or per-protocol effects for popular sampling schemes\n(case-base, survivor, and risk-set sampling), each with and without matching.\nOur approach may inform future research on different estimands, other\nvariations of the case-control design or settings with additional complexities.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:20:36 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["de Vries", "Bas B. L. Penning", ""], ["Groenwold", "Rolf H. H.", ""]]}, {"id": "2105.02088", "submitter": "Helene Charlotte Rytgaard", "authors": "Helene C. Rytgaard, Thomas A. Gerds and Mark J. van der Laan", "title": "Continuous-time targeted minimum loss-based estimation of\n  intervention-specific mean outcomes", "comments": "27 pages (excluding supplementary material), 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the generalization of the targeted minimum loss-based\nestimation (TMLE) framework to estimation of effects of time-varying\ninterventions in settings where both interventions, covariates, and outcome can\nhappen at subject-specific time-points on an arbitrarily fine time-scale. TMLE\nis a general template for constructing asymptotically linear substitution\nestimators for smooth low-dimensional parameters in infinite-dimensional\nmodels. Existing longitudinal TMLE methods are developed for data where\nobservations are made on a discrete time-grid.\n  We consider a continuous-time counting process model where intensity measures\ntrack the monitoring of subjects, and focus on a low-dimensional target\nparameter defined as the intervention-specific mean outcome at the end of\nfollow-up. To construct our TMLE algorithm for the given statistical estimation\nproblem we derive an expression for the efficient influence curve and represent\nthe target parameter as a functional of intensities and conditional\nexpectations. The high-dimensional nuisance parameters of our model are\nestimated and updated in an iterative manner according to separate targeting\nsteps for the involved intensities and conditional expectations.\n  The resulting estimator solves the efficient influence curve equation. We\nstate a general efficiency theorem and describe a highly adaptive lasso\nestimator for nuisance parameters that allows us to establish asymptotic\nlinearity and efficiency of our estimator under minimal conditions on the\nunderlying statistical model.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 14:39:14 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Rytgaard", "Helene C.", ""], ["Gerds", "Thomas A.", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "2105.02140", "submitter": "Luiza Piancastelli", "authors": "Luiza Piancastelli, Nial Friel, Julie Vercelloni, Kerrie Mengersen,\n  Antonietta Mira", "title": "A Bayesian latent allocation model for clustering compositional data\n  with application to the Great Barrier Reef", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative abundance is a common metric to estimate the composition of species\nin ecological surveys reflecting patterns of commonness and rarity of\nbiological assemblages. Measurements of coral reef compositions formed by four\ncommunities along Australia's Great Barrier Reef (GBR) gathered between 2012\nand 2017 are the focus of this paper. We undertake the task of finding clusters\nof transect locations with similar community composition and investigate\nchanges in clustering dynamics over time. During these years, an unprecedented\nsequence of extreme weather events (cyclones and coral bleaching) impacted the\n58 surveyed locations. The dependence between constituent parts of a\ncomposition presents a challenge for existing multivariate clustering\napproaches. In this paper, we introduce a finite mixture of Dirichlet\ndistributions with group-specific parameters, where cluster memberships are\ndictated by unobserved latent variables. The inference is carried in a Bayesian\nframework, where MCMC strategies are outlined to sample from the posterior\nmodel. Simulation studies are presented to illustrate the performance of the\nmodel in a controlled setting. The application of the model to the 2012 coral\nreef data reveals that clusters were spatially distributed in similar ways\nacross reefs which indicates a potential influence of wave exposure at the\norigin of coral reef community composition. The number of clusters estimated by\nthe model decreased from four in 2012 to two from 2014 until 2017. Posterior\nprobabilities of transect allocations to the same cluster substantially\nincrease through time showing a potential homogenization of community\ncomposition across the whole GBR. The Bayesian model highlights the diversity\nof coral reef community composition within a coral reef and rapid changes\nacross large spatial scales that may contribute to undermining the future of\nthe GBR's biodiversity.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 15:47:33 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Piancastelli", "Luiza", ""], ["Friel", "Nial", ""], ["Vercelloni", "Julie", ""], ["Mengersen", "Kerrie", ""], ["Mira", "Antonietta", ""]]}, {"id": "2105.02172", "submitter": "Robert R. Tucci", "authors": "Robert R. Tucci", "title": "Goodness of Causal Fit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a Goodness of Causal Fit (GCF) measure which depends on Pearl \"do\"\ninterventions. This is different from a measure of Goodness of Fit (GF), which\ndoes not use interventions. Given a DAG set ${\\cal G}$, to find a good $G\\in\n{\\cal G}$, we propose plotting $GCF(G)$ versus $GF(G)$ for all $G\\in {\\cal G}$,\nand finding a graph $G\\in {\\cal G}$ with a large amount of both types of\ngoodness.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 16:37:07 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Tucci", "Robert R.", ""]]}, {"id": "2105.02203", "submitter": "Guilherme Oliveira Lopes de", "authors": "Guilherme Lopes de Oliveira, Rosangela Helena Loschi, Renato Martins\n  Assun\\c{c}\\~ao", "title": "Bayesian Dynamic Estimation of Mortality Schedules in Small Areas", "comments": "25 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The determination of the shapes of mortality curves, the estimation and\nprojection of mortality patterns over time, and the investigation of\ndifferences in mortality patterns across different small underdeveloped\npopulations have received special attention in recent years. The challenges\ninvolved in this type of problems are the common sparsity and the unstable\nbehavior of observed death counts in small areas (populations). These features\nimpose many dificulties in the estimation of reasonable mortality schedules. In\nthis chapter, we present a discussion about this problem and we introduce the\nuse of relational Bayesian dynamic models for estimating and smoothing\nmortality schedules by age and sex. Preliminary results are presented,\nincluding a comparison with a methodology recently proposed in the literature.\nThe analyzes are based on simulated data as well as mortality data observed in\nsome Brazilian municipalities.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 17:28:08 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["de Oliveira", "Guilherme Lopes", ""], ["Loschi", "Rosangela Helena", ""], ["Assun\u00e7\u00e3o", "Renato Martins", ""]]}, {"id": "2105.02362", "submitter": "R. Michael Alvarez", "authors": "R. Michael Alvarez and Ines Levin", "title": "Uncertain Neighbors: Bayesian Propensity Score Matching For Causal\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We compare the performance of standard nearest-neighbor propensity score\nmatching with that of an analogous Bayesian propensity score matching\nprocedure. We show that the Bayesian approach makes better use of available\ninformation, as it makes less arbitrary decisions about which observations to\ndrop and which ones to keep in the matched sample. We conduct a simulation\nstudy to evaluate the performance of standard and Bayesian nearest-neighbor\nmatching when matching is done with and without replacement. We then use both\nmethods to replicate a recent study about the impact of land reform on\nguerrilla activity in Colombia.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 22:52:10 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Alvarez", "R. Michael", ""], ["Levin", "Ines", ""]]}, {"id": "2105.02393", "submitter": "Jose R. Zubizarreta", "authors": "Ambarish Chattopadhyay, Carl N. Morris, Jose R. Zubizarreta", "title": "Randomized and Balanced Allocation of Units into Treatment Groups Using\n  the Finite Selection Model for R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The original Finite Selection Model (FSM) was developed in the 1970s to\nenhance the design of the RAND Health Insurance Experiment (HIE; Newhouse et\nal. 1993). At the time of its development by Carl Morris (Morris 1979), there\nwere fundamental computational limitations to make the method widely available\nfor practitioners. Today, as randomized experiments increasingly become more\ncommon, there is a need for implementing experimental designs that are\nrandomized, balanced, robust, and easily applicable to several treatment\ngroups. To help address this problem, we revisit the original FSM under the\npotential outcome framework for causal inference and provide its first readily\navailable software implementation. In this paper, we provide an introduction to\nthe FSM and a step-by-step guide for its use in R.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 02:04:16 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Chattopadhyay", "Ambarish", ""], ["Morris", "Carl N.", ""], ["Zubizarreta", "Jose R.", ""]]}, {"id": "2105.02487", "submitter": "Boxin Zhao", "authors": "Boxin Zhao, Shengjun Zhai, Y. Samuel Wang, Mladen Kolar", "title": "High-dimensional Functional Graphical Model Structure Learning via\n  Neighborhood Selection Approach", "comments": "52 pages, 1 figure and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Undirected graphical models have been widely used to model the conditional\nindependence structure of high-dimensional random vector data for years. In\nmany modern applications such as EEG and fMRI data, the observations are\nmultivariate random functions rather than scalars. To model the conditional\nindependence of this type of data, functional graphical models are proposed and\nhave attracted an increasing attention in recent years. In this paper, we\npropose a neighborhood selection approach to estimate Gaussian functional\ngraphical models. We first estimate the neighborhood of all nodes via\nfunction-on-function regression, and then we can recover the whole graph\nstructure based on the neighborhood information. By estimating conditional\nstructure directly, we can circumvent the need of a well-defined precision\noperator which generally does not exist. Besides, we can better explore the\neffect of the choice of function basis for dimension reduction. We give a\ncriterion for choosing the best function basis and motivate two practically\nuseful choices, which we justified by both theory and experiments and show that\nthey are better than expanding each function onto its own FPCA basis as in\nprevious literature. In addition, the neighborhood selection approach is\ncomputationally more efficient than fglasso as it is more easy to do parallel\ncomputing. The statistical consistency of our proposed methods in\nhigh-dimensional setting are supported by both theory and experiment.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:38:50 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhao", "Boxin", ""], ["Zhai", "Shengjun", ""], ["Wang", "Y. Samuel", ""], ["Kolar", "Mladen", ""]]}, {"id": "2105.02488", "submitter": "{\\O}ystein S{\\o}rensen", "authors": "{\\O}ystein S{\\o}rensen, Anders M. Fjell, Kristine B. Walhovd", "title": "Longitudinal modeling of age-dependent latent traits with generalized\n  additive latent and mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present generalized additive latent and mixed models (GALAMMs) for\nanalysis of clustered data with latent and observed variables depending\nsmoothly on observed variables. A profile likelihood algorithm is proposed, and\nwe derive asymptotic standard errors of both smooth and parametric terms. The\nwork was motivated by applications in cognitive neuroscience, and we show how\nGALAMMs can successfully model the complex lifespan trajectory of latent\nepisodic memory, along with a discrepant trajectory of working memory, as well\nas the effect of latent socioeconomic status on hippocampal development.\nSimulation experiments suggest that model estimates are accurate even with\nmoderate sample sizes.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 07:42:21 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["S\u00f8rensen", "\u00d8ystein", ""], ["Fjell", "Anders M.", ""], ["Walhovd", "Kristine B.", ""]]}, {"id": "2105.02499", "submitter": "Xavier de Luna", "authors": "Mohammad Ghasempour and Xavier de Luna", "title": "SDRcausal: an R package for causal inference based on sufficient\n  dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SDRcausal is a package that implements sufficient dimension reduction methods\nfor causal inference as proposed in Ghosh, Ma, and de Luna (2021). The package\nimplements (augmented) inverse probability weighting and outcome regression\n(imputation) estimators of an average treatment effect (ATE) parameter.\nNuisance models, both treatment assignment probability given the covariates\n(propensity score) and outcome regression models, are fitted by using\nsemiparametric locally efficient dimension reduction estimators, thereby\nallowing for large sets of confounding covariates. Techniques including linear\nextrapolation, numerical differentiation, and truncation have been used to\nobtain a practicable implementation of the methods. Finding the suitable\ndimension reduction map (central mean subspace) requires solving an\noptimization problem, and several optimization algorithms are given as choices\nto the user. The package also provides estimators of the asymptotic variances\nof the causal effect estimators implemented. Plotting options are provided. The\ncore of the methods are implemented in C language, and parallelization is\nallowed for. The user-friendly and freeware R language is used as interface.\nThe package can be downloaded from Github repository:\nhttps://github.com/stat4reg.\n", "versions": [{"version": "v1", "created": "Thu, 6 May 2021 08:05:15 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Ghasempour", "Mohammad", ""], ["de Luna", "Xavier", ""]]}, {"id": "2105.02675", "submitter": "Ali Shojaie", "authors": "Ali Shojaie and Emily B. Fox", "title": "Granger Causality: A Review and Recent Advances", "comments": "40 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Introduced more than a half century ago, Granger causality has become a\npopular tool for analyzing time series data in many application domains, from\neconomics and finance to genomics and neuroscience. Despite this popularity,\nthe validity of this notion for inferring causal relationships among time\nseries has remained the topic of continuous debate. Moreover, while the\noriginal definition was general, limitations in computational tools have\nprimarily limited the applications of Granger causality to simple bivariate\nvector auto-regressive processes or pairwise relationships among a set of\nvariables. Starting with a review of early developments and debates, this paper\ndiscusses recent advances that address various shortcomings of the earlier\napproaches, from models for high-dimensional time series to more recent\ndevelopments that account for nonlinear and non-Gaussian observations and allow\nfor sub-sampled and mixed frequency time series.\n", "versions": [{"version": "v1", "created": "Wed, 5 May 2021 17:37:18 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 02:38:08 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Shojaie", "Ali", ""], ["Fox", "Emily B.", ""]]}, {"id": "2105.03022", "submitter": "Shirin Golchi", "authors": "Shirin Golchi", "title": "Estimating the Design Operating Characteristics in Clinical Trials with\n  the Ordinal Scale Disease Progression Endpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian adaptive designs have gained popularity in all phases of clinical\ntrials in the recent years. The COVID-19 pandemic, however, has brought these\ndesigns to the centre stage. The need for establishing evidence for the\neffectiveness of vaccines, therapeutic treatments and policies that could\nresolve or control the crisis has resulted in development of efficient designs\nfor clinical trials that can be concluded with smaller sample sizes in a\nshorter time period. Design of Bayesian adaptive trials, however, requires\nextensive simulation studies that is considered a disadvantage in\ntime-sensitive settings such as the pandemic. In this paper, we propose a set\nof methods for efficient estimation and uncertainty quantification for the\ndesign operating characteristics of Bayesian adaptive trials. The proposed\napproach is tailored to address design of clinical trials with the ordinal\ndisease progression scale endpoint but can be used generally in the clinical\ntrials context where design operating characteristics cannot be obtained\nanalytically.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 01:17:19 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Golchi", "Shirin", ""]]}, {"id": "2105.03067", "submitter": "Suyash Gupta", "authors": "Suyash Gupta and Dominik Rothenh\\\"ausler", "title": "The $s$-value: evaluating stability with respect to distributional\n  shifts", "comments": "43 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common statistical measures of uncertainty such as $p$-values and confidence\nintervals quantify the uncertainty due to sampling, that is, the uncertainty\ndue to not observing the full population. However, sampling is not the only\nsource of uncertainty. In practice, distributions change between locations and\nacross time. This makes it difficult to gather knowledge that transfers across\ndata sets. We propose a measure of uncertainty or instability that quantifies\nthe distributional instability of a statistical parameter with respect to\nKullback-Leibler divergence, that is, the sensitivity of the parameter under\ngeneral distributional perturbations within a Kullback-Leibler divergence ball.\nIn addition, we propose measures to elucidate the instability of parameters\nwith respect to directional or variable-specific shifts. Measuring instability\nwith respect to directional shifts can be used to detect the type of shifts a\nparameter is sensitive to. We discuss how such knowledge can inform data\ncollection for improved estimation of statistical parameters under shifted\ndistributions. We evaluate the performance of the proposed measure on real data\nand show that it can elucidate the distributional (in-)stability of a parameter\nwith respect to certain shifts and can be used to improve the accuracy of\nestimation under shifted distributions.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 05:18:12 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 00:39:28 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Gupta", "Suyash", ""], ["Rothenh\u00e4usler", "Dominik", ""]]}, {"id": "2105.03101", "submitter": "Alexander Henzi", "authors": "Alexander Henzi", "title": "Consistent estimation of distribution functions under increasing concave\n  and convex stochastic ordering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A random variable $Y_1$ is said to be smaller than $Y_2$ in the increasing\nconcave stochastic order if $\\mathbb{E}[\\phi(Y_1)] \\leq \\mathbb{E}[\\phi(Y_2)]$\nfor all increasing concave functions $\\phi$ for which the expected values\nexist, and smaller than $Y_2$ in the increasing convex order if\n$\\mathbb{E}[\\psi(Y_1)] \\leq \\mathbb{E}[\\psi(Y_2)]$ for all increasing convex\n$\\psi$. This article develops nonparametric estimators for the conditional\ncumulative distribution functions $F_x(y) = \\mathbb{P}(Y \\leq y \\mid X = x)$ of\na response variable $Y$ given a covariate $X$, solely under the assumption that\nthe conditional distributions are increasing in $x$ in the increasing concave\nor increasing convex order. Uniform consistency and rates of convergence are\nestablished both for the $K$-sample case $X \\in \\{1, \\dots, K\\}$ and for\ncontinuously distributed $X$.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 08:00:59 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Henzi", "Alexander", ""]]}, {"id": "2105.03228", "submitter": "Jocelyn Chi", "authors": "Jocelyn T. Chi, Ilse C. F. Ipsen, Tzu-Hung Hsiao, Ching-Heng Lin,\n  Li-San Wang, Wan-Ping Lee, Tzu-Pin Lu, Jung-Ying Tzeng", "title": "SEAGLE: A Scalable Exact Algorithm for Large-Scale Set-Based GxE Tests\n  in Biobank Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The explosion of biobank data offers immediate opportunities for\ngene-environment (GxE) interaction studies of complex diseases because of the\nlarge sample sizes and the rich collection in genetic and non-genetic\ninformation. However, the extremely large sample size also introduces new\ncomputational challenges in GxE assessment, especially for set-based GxE\nvariance component (VC) tests, which are a widely used strategy to boost\noverall GxE signals and to evaluate the joint GxE effect of multiple variants\nfrom a biologically meaningful unit (e.g., gene). In this work, we focus on\ncontinuous traits and present SEAGLE, a Scalable Exact AlGorithm for\nLarge-scale set-based GxE tests, to permit GxE VC tests for biobank-scale data.\nSEAGLE employs modern matrix computations to achieve the same \"exact\" results\nas the original GxE VC tests without imposing additional assumptions or relying\non approximations. SEAGLE can easily accommodate sample sizes in the order of\n$10^5$, is implementable on standard laptops, and does not require specialized\ncomputing equipment. We demonstrate SEAGLE's performance through extensive\nsimulations. We illustrate its utility by conducting genome-wide gene-based GxE\nanalysis on the Taiwan Biobank data to explore the interaction of gene and\nphysical activity status on body mass index.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 13:01:12 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 02:39:30 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chi", "Jocelyn T.", ""], ["Ipsen", "Ilse C. F.", ""], ["Hsiao", "Tzu-Hung", ""], ["Lin", "Ching-Heng", ""], ["Wang", "Li-San", ""], ["Lee", "Wan-Ping", ""], ["Lu", "Tzu-Pin", ""], ["Tzeng", "Jung-Ying", ""]]}, {"id": "2105.03241", "submitter": "Cristiano Villa", "authors": "Stephen G. Walker and Cristiano Villa", "title": "An Objective Prior from a Scoring Rule", "comments": null, "journal-ref": null, "doi": "10.3390/e23070833", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper we introduce a novel objective prior distribution levering on\nthe connections between information, divergence and scoring rules. In\nparticular, we do so from the starting point of convex functions representing\ninformation in density functions. This provides a natural route to proper local\nscoring rules using Bregman divergence. Specifically, we determine the prior\nwhich solves setting the score function to be a constant. While in itself this\nprovides motivation for an objective prior, the prior also minimizes a\ncorresponding information criterion.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 13:14:33 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 08:34:52 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Walker", "Stephen G.", ""], ["Villa", "Cristiano", ""]]}, {"id": "2105.03269", "submitter": "Stephen Johnson", "authors": "Stephen Richard Johnson, Sarah Elizabeth Heaps, Kevin James Wilson and\n  Darren James Wilkinson", "title": "Bayesian spatio-temporal model for high-resolution short-term\n  forecasting of precipitation fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With extreme weather events becoming more common, the risk posed by surface\nwater flooding is ever increasing. In this work we propose a model, and\nassociated Bayesian inference scheme, for generating probabilistic\n(high-resolution short-term) forecasts of localised precipitation. The\nparametrisation of our underlying hierarchical dynamic spatio-temporal model is\nmotivated by a forward-time, centred-space finite difference solution to a\ncollection of stochastic partial differential equations, where the main driving\nforces are advection and diffusion. Observations from both weather radar and\nground based rain gauges provide information from which we can learn about the\nlikely values of the (latent) precipitation field in addition to other unknown\nmodel parameters. Working in the Bayesian paradigm provides a coherent\nframework for capturing uncertainty both in the underlying model parameters and\nalso in our forecasts. Further, appealing to simulation based (MCMC) sampling\nyields a straightforward solution to handling zeros, treated as censored\nobservations, via data augmentation. Both the underlying state and the\nobservations are of moderately large dimension ($\\mathcal{O}(10^4)$ and\n$\\mathcal{O}(10^3)$ respectively) and this renders standard inference\napproaches computationally infeasible. Our solution is to embed the ensemble\nKalman smoother within a Gibbs sampling scheme to facilitate approximate\nBayesian inference in reasonable time. Both the methodology and the\neffectiveness of our posterior sampling scheme are demonstrated via simulation\nstudies and also by a case study of real data from the Urban Observatory\nproject based in Newcastle upon Tyne, UK.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 13:48:33 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Johnson", "Stephen Richard", ""], ["Heaps", "Sarah Elizabeth", ""], ["Wilson", "Kevin James", ""], ["Wilkinson", "Darren James", ""]]}, {"id": "2105.03309", "submitter": "Antonio Calcagn\\`i", "authors": "Antonio Calcagn\\`i", "title": "Estimating latent linear correlations from fuzzy frequency tables", "comments": "27 pages, 5 figures, 9 tables, 2 supplementary figures, 2\n  supplementary tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research concerns the estimation of latent linear or polychoric\ncorrelations from fuzzy frequency tables. Fuzzy counts are of particular\ninterest to many disciplines including social and behavioral sciences, and are\nespecially relevant when observed data are classified using fuzzy categories -\nas for socio-economic studies, clinical evaluations, content analysis,\ninter-rater reliability analysis - or when imprecise observations are\nclassified into either precise or imprecise categories - as for the analysis of\nratings data or fuzzy coded variables. In these cases, the space of count\nmatrices is no longer defined over naturals and, consequently, the polychoric\nestimator cannot be used to accurately estimate latent linear correlations. The\naim of this contribution is twofold. First, we illustrate a computational\nprocedure based on generalized natural numbers for computing fuzzy frequencies.\nSecond, we reformulate the problem of estimating latent linear correlations\nfrom fuzzy counts in the context of Expectation-Maximization based maximum\nlikelihood estimation. A simulation study and two applications are used to\ninvestigate the characteristics of the proposed method. Overall, the results\nshow that the fuzzy EM-based polychoric estimator is more efficient to deal\nwith imprecise count data as opposed to standard polychoric estimators that may\nbe used in this context.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:00:57 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Calcagn\u00ec", "Antonio", ""]]}, {"id": "2105.03325", "submitter": "Ruohong Li", "authors": "Ruohong Li, Honglang Wang, Wanzhu Tu", "title": "Robust Estimation of Heterogeneous Treatment Effects using Electronic\n  Health Record Data", "comments": "Statistics in Medicine (2021)", "journal-ref": null, "doi": "10.1002/sim.8926", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of heterogeneous treatment effects is an essential component of\nprecision medicine. Model and algorithm-based methods have been developed\nwithin the causal inference framework to achieve valid estimation and\ninference. Existing methods such as the A-learner, R-learner, modified\ncovariates method (with and without efficiency augmentation), inverse\npropensity score weighting, and augmented inverse propensity score weighting\nhave been proposed mostly under the square error loss function. The performance\nof these methods in the presence of data irregularity and high dimensionality,\nsuch as that encountered in electronic health record (EHR) data analysis, has\nbeen less studied. In this research, we describe a general formulation that\nunifies many of the existing learners through a common score function. The new\nformulation allows the incorporation of least absolute deviation (LAD)\nregression and dimension reduction techniques to counter the challenges in EHR\ndata analysis. We show that under a set of mild regularity conditions, the\nresultant estimator has an asymptotic normal distribution. Within this\nframework, we proposed two specific estimators for EHR analysis based on\nweighted LAD with penalties for sparsity and smoothness simultaneously. Our\nsimulation studies show that the proposed methods are more robust to outliers\nunder various circumstances. We use these methods to assess the blood\npressure-lowering effects of two commonly used antihypertensive therapies.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 15:21:45 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Li", "Ruohong", ""], ["Wang", "Honglang", ""], ["Tu", "Wanzhu", ""]]}, {"id": "2105.03396", "submitter": "Dongbang Yuan", "authors": "Dongbang Yuan and Irina Gaynanova", "title": "Double-matched matrix decomposition for multi-view data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of extracting joint and individual signals from\nmulti-view data, that is data collected from different sources on matched\nsamples. While existing methods for multi-view data decomposition explore\nsingle matching of data by samples, we focus on double-matched multi-view data\n(matched by both samples and source features). Our motivating example is the\nmiRNA data collected from both primary tumor and normal tissues of the same\nsubjects; the measurements from two tissues are thus matched both by subjects\nand by miRNAs. Our proposed double-matched matrix decomposition allows to\nsimultaneously extract joint and individual signals across subjects, as well as\njoint and individual signals across miRNAs. Our estimation approach takes\nadvantage of double-matching by formulating a new type of optimization problem\nwith explicit row space and column space constraints, for which we develop an\nefficient iterative algorithm. Numerical studies indicate that taking advantage\nof double-matching leads to superior signal estimation performance compared to\nexisting multi-view data decomposition based on single-matching. We apply our\nmethod to miRNA data as well as data from the English Premier League soccer\nmatches, and find joint and individual multi-view signals that align with\ndomain specific knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 17:09:57 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Yuan", "Dongbang", ""], ["Gaynanova", "Irina", ""]]}, {"id": "2105.03454", "submitter": "Boyu Ren", "authors": "Boyu Ren, Xiao Wu, Danielle Braun, Natesh Pillai, Francesca Dominici", "title": "Bayesian Modeling for Exposure Response Curve via Gaussian Processes:\n  Causal Effects of Exposure to Air Pollution on Health Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by environmental health research on air pollution, we address the\nchallenge of estimation and uncertainty quantification of causal\nexposure-response function (CERF). The CERF describes the relationship between\na continuously varying exposure (or treatment) and its causal effect on a\noutcome. We propose a new Bayesian approach that relies on a Gaussian process\n(GP) model to estimate the CERF. We parametrize the covariance (kernel)\nfunction of the GP to mimic matching via a Generalized Propensity Score (GPS).\nThe tuning parameters of the matching function are chosen to optimize covariate\nbalance. Our approach achieves automatic uncertainty evaluation of the CERF\nwith high computational efficiency, enables change point detection through\ninference on derivatives of the CERF, and yields the desired separation of\ndesign and analysis phases for causal estimation. We provide theoretical\nresults showing the correspondence between our Bayesian GP framework and\ntraditional approaches in causal inference for estimating causal effects of a\ncontinuous exposure. We apply the methods to 520,711 ZIP-code-level\nobservations to estimate the causal effect of long-term exposures to PM2.5 on\nall-cause mortality among Medicare enrollees in the United States.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 18:06:46 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 15:18:20 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Ren", "Boyu", ""], ["Wu", "Xiao", ""], ["Braun", "Danielle", ""], ["Pillai", "Natesh", ""], ["Dominici", "Francesca", ""]]}, {"id": "2105.03478", "submitter": "Soichiro Yamauchi", "authors": "Matthew Blackwell, Soichiro Yamauchi", "title": "Adjusting for Unmeasured Confounding in Marginal Structural Models with\n  Propensity-Score Fixed Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Marginal structural models are a popular tool for investigating the effects\nof time-varying treatments, but they require an assumption of no unobserved\nconfounders between the treatment and outcome. With observational data, this\nassumption may be difficult to maintain, and in studies with panel data, many\nresearchers use fixed effects models to purge the data of time-constant\nunmeasured confounding. Unfortunately, traditional linear fixed effects models\nare not suitable for estimating the effects of time-varying treatments, since\nthey can only estimate lagged effects under implausible assumptions. To resolve\nthis tension, we a propose a novel inverse probability of treatment weighting\nestimator with propensity-score fixed effects to adjust for time-constant\nunmeasured confounding in marginal structural models of fixed-length treatment\nhistories. We show that these estimators are consistent and asymptotically\nnormal when the number of units and time periods grow at a similar rate. Unlike\ntraditional fixed effect models, this approach works even when the outcome is\nonly measured at a single point in time as is common in marginal structural\nmodels. We apply these methods to estimating the effect of negative advertising\non the electoral success of candidates for statewide offices in the United\nStates.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 19:43:06 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 17:29:15 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Blackwell", "Matthew", ""], ["Yamauchi", "Soichiro", ""]]}, {"id": "2105.03481", "submitter": "Christophe Ley", "authors": "Andreas Anastasiou, Alessandro Barp, Fran\\c{c}ois-Xavier Briol, Bruno\n  Ebner, Robert E. Gaunt, Fatemeh Ghaderinezhad, Jackson Gorham, Arthur\n  Gretton, Christophe Ley, Qiang Liu, Lester Mackey, Chris. J. Oates, Gesine\n  Reinert, Yvik Swan", "title": "Stein's Method Meets Statistics: A Review of Some Recent Developments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stein's method is a collection of tools for analysing distributional\ncomparisons through the study of a class of linear operators called Stein\noperators. Originally studied in probability, Stein's method has also enabled\nsome important developments in statistics. This early success has led to a high\nresearch activity in this area in recent years. The goal of this survey is to\nbring together some of these developments in theoretical statistics as well as\nin computational statistics and, in doing so, to stimulate further research\ninto the successful field of Stein's method and statistics. The topics we\ndiscuss include: explicit error bounds for asymptotic approximations of\nestimators and test statistics, a measure of prior sensitivity in Bayesian\nstatistics, tools to benchmark and compare sampling methods such as approximate\nMarkov chain Monte Carlo, deterministic alternatives to sampling methods,\ncontrol variate techniques, and goodness-of-fit testing.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 19:55:14 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Anastasiou", "Andreas", ""], ["Barp", "Alessandro", ""], ["Briol", "Fran\u00e7ois-Xavier", ""], ["Ebner", "Bruno", ""], ["Gaunt", "Robert E.", ""], ["Ghaderinezhad", "Fatemeh", ""], ["Gorham", "Jackson", ""], ["Gretton", "Arthur", ""], ["Ley", "Christophe", ""], ["Liu", "Qiang", ""], ["Mackey", "Lester", ""], ["Oates", "Chris. J.", ""], ["Reinert", "Gesine", ""], ["Swan", "Yvik", ""]]}, {"id": "2105.03493", "submitter": "Forrest Crawford", "authors": "Xiaoxuan Cai, Eben Kenah, and Forrest W. Crawford", "title": "Causal identification of infectious disease intervention effects in a\n  clustered population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causal identification of treatment effects for infectious disease outcomes in\ninterconnected populations is challenging because infection outcomes may be\ntransmissible to others, and treatment given to one individual may affect\nothers' outcomes. Contagion, or transmissibility of outcomes, complicates\nstandard conceptions of treatment interference in which an intervention\ndelivered to one individual can affect outcomes of others. Several statistical\nframeworks have been proposed to measure causal treatment effects in this\nsetting, including structural transmission models, mediation-based partnership\nmodels, and randomized trial designs. However, existing estimands for\ninfectious disease intervention effects are of limited conceptual usefulness:\nSome are parameters in a structural model whose causal interpretation is\nunclear, others are causal effects defined only in a restricted two-person\nsetting, and still others are nonparametric estimands that arise naturally in\nthe context of a randomized trial but may not measure any biologically\nmeaningful effect. In this paper, we describe a unifying formalism for defining\nnonparametric structural causal estimands and an identification strategy for\nlearning about infectious disease intervention effects in clusters of\ninteracting individuals when infection times are observed. The estimands\ngeneralize existing quantities and provide a framework for causal\nidentification in randomized and observational studies, including situations\nwhere only binary infection outcomes are observed. A semiparametric class of\npairwise Cox-type transmission hazard models is used to facilitate statistical\ninference in finite samples. A comprehensive simulation study compares existing\nand proposed estimands under a variety of randomized and observational vaccine\ntrial designs.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 20:25:53 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cai", "Xiaoxuan", ""], ["Kenah", "Eben", ""], ["Crawford", "Forrest W.", ""]]}, {"id": "2105.03508", "submitter": "Heejong Bong", "authors": "Heejong Bong (1), Val\\'erie Ventura (1), Eric A. Yttri (3, 4 and 5),\n  Matthew A. Smith (4 and 5) and Robert E. Kass (1, 2 and 5) ((1) Department of\n  Statistics and Data Sciences, Carnegie Mellon University, (2) Machine\n  Learning Department, Carnegie Mellon University, (3) Department of Biological\n  Sciences, Carnegie Mellon University, (4) Department of Biomedical\n  Engineering, Carnegie Mellon University, (5) Neuroscience Institute, Carnegie\n  Mellon University)", "title": "Latent Cross-population Dynamic Time-series Analysis of High-dimensional\n  Neural Recordings", "comments": "19 pages, 9 figures, submitted to Annals of Applied Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important problem in analysis of neural data is to characterize\ninteractions across brain regions from high-dimensional multiple-electrode\nrecordings during a behavioral experiment. Lead-lag effects indicate possible\ndirectional flows of neural information, but they are often transient,\nappearing during short intervals of time. Such non-stationary interactions can\nbe difficult to identify, but they can be found by taking advantage of the\nreplication structure inherent to many neurophysiological experiments. To\ndescribe non-stationary interactions between replicated pairs of\nhigh-dimensional time series, we developed a method of estimating latent,\nnon-stationary cross-correlation. Our approach begins with an extension of\nprobabilistic CCA to the time series setting, which provides a model-based\ninterpretation of multiset CCA. Because the covariance matrix describing\nnon-stationary dependence is high-dimensional, we assume sparsity of\ncross-correlations within a range of possible interesting lead-lag effects. We\nshow that the method can perform well in realistic settings and we apply it to\n192 simultaneous local field potential (LFP) recordings from prefrontal cortex\n(PFC) and visual cortex (area V4) during a visual memory task. We find lead-lag\nrelationships that are highly plausible, being consistent with related results\nin the literature.\n", "versions": [{"version": "v1", "created": "Fri, 7 May 2021 21:13:42 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Bong", "Heejong", "", "3, 4 and 5"], ["Ventura", "Val\u00e9rie", "", "3, 4 and 5"], ["Yttri", "Eric A.", "", "3, 4 and 5"], ["Smith", "Matthew A.", "", "4 and 5"], ["Kass", "Robert E.", "", "1, 2 and 5"]]}, {"id": "2105.03557", "submitter": "Wenpo Yao", "authors": "Wenpo Yao and Wenli Yao", "title": "Comparative analysis of the original and amplitude permutations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We compare the two basic ordinal patterns, i.e., the original and amplitude\npermutations, used to characterize vector structures. The original permutation\nconsists of the indexes of reorganized values in the original vector. By\ncontrast, the amplitude permutation comprises the positions of values in the\nreordered vector, and it directly reflects the temporal structure. To\naccurately convey the structural characteristics of vectors, we modify indexes\nof equal values in permutations to be the same as, for example, the smallest or\nlargest indexes in each group of equalities. Overall, we clarify the\nrelationship between the original and amplitude permutations. And the results\nhave implications for time- and amplitude-symmetric vectors and will lead to\nfurther theoretical and experimental studies.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 02:04:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yao", "Wenpo", ""], ["Yao", "Wenli", ""]]}, {"id": "2105.03699", "submitter": "Bruno Santos", "authors": "Agatha Rodrigues, Patrick Borges and Bruno Santos", "title": "A defective cure rate quantile regression model for male breast cancer\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article, we particularly address the problem of assessing the impact\nof clinical stage and age on the specific survival times of men with breast\ncancer when cure is a possibility, where there is also the interest of\nexplaining this impact on different quantiles of the survival times. To this\nend, we developed a quantile regression model for survival data in the presence\nof long-term survivors based on the generalized distribution of Gompertz in a\ndefective version, which is conveniently reparametrized in terms of the q-th\nquantile and then linked to covariates via a logarithm link function. This\nproposal allows us to obtain how each variable affects the survival times in\ndifferent quantiles. In addition, we are able to study the effects of\ncovariates on the cure rate as well. We consider Markov Chain Monte Carlo\n(MCMC) methods to develop a Bayesian analysis in the proposed model and we\nevaluate its performance through a Monte Carlo simulation study. Finally, we\nillustrate the advantages of our model in a data set about male breast cancer\nfrom Brazil.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 13:31:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Rodrigues", "Agatha", ""], ["Borges", "Patrick", ""], ["Santos", "Bruno", ""]]}, {"id": "2105.03734", "submitter": "Moreno Bevilacqua", "authors": "Diego Morales-Navarrete, Moreno Bevilacqua, Christian\n  Caama\\~no-Carrillo, Luis M. Castro", "title": "Modelling Point Referenced Spatial Count Data: A Poisson Process\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Random fields are useful mathematical tools for representing natural\nphenomena with complex dependence structures in space and/or time. In\nparticular, the Gaussian random field is commonly used due to its attractive\nproperties and mathematical tractability. However, this assumption seems to be\nrestrictive when dealing with counting data. To deal with this situation, we\npropose a random field with a Poisson marginal distribution by considering a\nsequence of independent copies of a random field with an exponential marginal\ndistribution as 'inter-arrival times' in the counting renewal processes\nframework. Our proposal can be viewed as a spatial generalization of the\nPoisson process.\n  Unlike the classical hierarchical Poisson Log-Gaussian model, our proposal\ngenerates a (non)-stationary random field that is mean square continuous and\nwith Poisson marginal distributions. For the proposed Poisson spatial random\nfield, analytic expressions for the covariance function and the bivariate\ndistribution are provided. In an extensive simulation study, we investigate the\nweighted pairwise likelihood as a method for estimating the Poisson random\nfield parameters.\n  Finally, the effectiveness of our methodology is illustrated by an analysis\nof reindeer pellet-group survey data, where a zero-inflated version of the\nproposed model is compared with zero-inflated Poisson Log-Gaussian and Poisson\nGaussian copula models. Supplementary materials for this article, include\ntechnical proofs and R code for reproducing the work, are available as an\nonline supplement.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 16:34:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Morales-Navarrete", "Diego", ""], ["Bevilacqua", "Moreno", ""], ["Caama\u00f1o-Carrillo", "Christian", ""], ["Castro", "Luis M.", ""]]}, {"id": "2105.03806", "submitter": "Helton Saulo", "authors": "Dan\\'ubia R. Cunha, Jose A. Divino and Helton Saulo", "title": "The zero-adjusted log-symmetric quantile regression model applied to\n  extramarital affairs data", "comments": "14 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a zero-adjusted log-symmetric quantile regression\nmodel. Initially, we introduce zero-adjusted log-symmetric distributions, which\nallow for the accommodation of zeros. The estimation of the parameters is\napproached by the maximum likelihood method and a Monte Carlo simulation is\nperformed to evaluate the estimates. Finally, we illustrate the proposed\nmethodology with the use of a real extramarital affairs data set.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 01:01:55 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Cunha", "Dan\u00fabia R.", ""], ["Divino", "Jose A.", ""], ["Saulo", "Helton", ""]]}, {"id": "2105.03810", "submitter": "Eric Auerbach", "authors": "Eric Auerbach and Max Tabord-Meehan", "title": "The Local Approach to Causal Inference under Network Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a new unified framework for causal inference when outcomes depend\non how agents are linked in a social or economic network. Such network\ninterference describes a large literature on treatment spillovers, social\ninteractions, social learning, information diffusion, social capital formation,\nand more. Our approach works by first characterizing how an agent is linked in\nthe network using the configuration of other agents and connections nearby as\nmeasured by path distance. The impact of a policy or treatment assignment is\nthen learned by pooling outcome data across similarly configured agents. In the\npaper, we propose a new nonparametric modeling approach and consider two\napplications to causal inference. The first application is to testing policy\nirrelevance/no treatment effects. The second application is to estimating\npolicy effects/treatment response. We conclude by evaluating the finite-sample\nproperties of our estimation and inference procedures via simulation.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 01:27:05 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 23:37:06 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Auerbach", "Eric", ""], ["Tabord-Meehan", "Max", ""]]}, {"id": "2105.03865", "submitter": "Erniel Barrios", "authors": "Jetrei Benedick R. Benito, Joseph Ryan G. Lansangan and Erniel B.\n  Barrios", "title": "Semiparametric Volatility Model with Varying Frequencies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In extracting time series data from various sources, it is inevitable to\ncompile variables measured at varying frequencies as this is often dependent on\nthe source. Modeling from these data can be facilitated by aggregating high\nfrequency data to match the relatively lower frequencies of the rest of the\nvariables. This however, can easily loss vital information that characterizes\nthe system ought to be modelled. Two semiparametric volatility models are\npostulated to account for covariates of varying frequencies without aggregation\nof the data to lower frequencies. First is an extension of the autoregressive\nintegrated moving average with explanatory variable (ARMAX) model, it\nintegrates high frequency data into the mean equation (VF-ARMA). Second is an\nextension of the Glosten, Jagannathan and Rankle (GJR) model that incorporates\nthe high frequency data into the variance equation (VF-GARCH). In both models,\nhigh frequency data was introduced as a nonparametric function in the model.\nBoth models are estimated using a hybrid estimation procedure that benefits\nfrom the additive nature of the models. Simulation studies illustrate the\nadvantages of postulated models in terms of predictive ability compared to\ngeneralized autoregressive conditionally heteroscedastic (GARCH) and GJR models\nthat simply aggregates high frequency covariates to the same frequency as the\noutput variable. Furthermore, VF-ARMA is superior to VF-GARCH since it exhibits\nsome degree of robustness in a wide range of scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 08:00:08 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Benito", "Jetrei Benedick R.", ""], ["Lansangan", "Joseph Ryan G.", ""], ["Barrios", "Erniel B.", ""]]}, {"id": "2105.03893", "submitter": "Xiaowei Zhang", "authors": "L. Jeff Hong and Xiaowei Zhang", "title": "Surrogate-Based Simulation Optimization", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Simulation models are widely used in practice to facilitate decision-making\nin a complex, dynamic and stochastic environment. But they are computationally\nexpensive to execute and optimize, due to lack of analytical tractability.\nSimulation optimization is concerned with developing efficient sampling schemes\n-- subject to a computational budget -- to solve such optimization problems. To\nmitigate the computational burden, surrogates are often constructed using\nsimulation outputs to approximate the response surface of the simulation model.\nIn this tutorial, we provide an up-to-date overview of surrogate-based methods\nfor simulation optimization with continuous decision variables. Typical\nsurrogates, including linear basis function models and Gaussian processes, are\nintroduced. Surrogates can be used either as a local approximation or a global\napproximation. Depending on the choice, one may develop algorithms that\nconverge to either a local optimum or a global optimum. Representative examples\nare presented for each category. Recent advances in large-scale computation for\nGaussian processes are also discussed.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 10:06:11 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 06:38:02 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hong", "L. Jeff", ""], ["Zhang", "Xiaowei", ""]]}, {"id": "2105.03948", "submitter": "David Hand Prof", "authors": "David J. Hand", "title": "Trustworthiness of statistical inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We examine the role of trustworthiness and trust in statistical inference,\narguing that it is the extent of trustworthiness in inferential statistical\ntools which enables trust in the conclusions. Certain tools, such as the\np-value and significance test, have recently come under renewed criticism, with\nsome arguing that they damage trust in statistics. We argue the contrary,\nbeginning from the position that the central role of these methods is to form\nthe basis for trusted conclusions in the face of uncertainty in the data, and\nnoting that it is the misuse and misunderstanding of these tools which damages\ntrustworthiness and hence trust. We go on to argue that recent calls to ban\nthese tools would tackle the symptom, not the cause, and themselves risk\ndamaging the capability of science to advance, and feeding into public\nsuspicion of the discipline of statistics. The consequence could be aggravated\nmistrust of our discipline and of science more generally. In short, the very\nproposals could work in quite the contrary direction from that intended. We\nmake some alternative proposals for tackling the misuse and misunderstanding of\nthese methods, and for how trust in our discipline might be promoted.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 14:16:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Hand", "David J.", ""]]}, {"id": "2105.03993", "submitter": "Xiaoquan Wen", "authors": "Yi Zhao and Xiaoquan Wen", "title": "Statistical Assessment of Replicability via Bayesian Model Criticism", "comments": "45 pages, 9 figures, and 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Assessment of replicability is critical to ensure the quality and rigor of\nscientific research. In this paper, we discuss inference and modeling\nprinciples for replicability assessment. Targeting distinct application\nscenarios, we propose two types of Bayesian model criticism approaches to\nidentify potentially irreproducible results in scientific experiments. They are\nmotivated by established Bayesian prior and posterior predictive model-checking\nprocedures and generalize many existing replicability assessment methods.\nFinally, we discuss the statistical properties of the proposed replicability\nassessment approaches and illustrate their usages by simulations and examples\nof real data analysis, including the data from the Reproducibility Project:\nPsychology and a systematic review of impacts of pre-existing cardiovascular\ndisease on COVID-19 outcomes.\n", "versions": [{"version": "v1", "created": "Sun, 9 May 2021 18:56:10 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Zhao", "Yi", ""], ["Wen", "Xiaoquan", ""]]}, {"id": "2105.04099", "submitter": "Yunan Wu", "authors": "Yunan Wu, Lan Wang and Haoda Fu", "title": "Model-Assisted Uniformly Honest Inference for Optimal Treatment Regimes\n  in High Dimension", "comments": "196 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops new tools to quantify uncertainty in optimal decision\nmaking and to gain insight into which variables one should collect information\nabout given the potential cost of measuring a large number of variables. We\ninvestigate simultaneous inference to determine if a group of variables is\nrelevant for estimating an optimal decision rule in a high-dimensional\nsemiparametric framework. The unknown link function permits flexible modeling\nof the interactions between the treatment and the covariates, but leads to\nnonconvex estimation in high dimension and imposes significant challenges for\ninference. We first establish that a local restricted strong convexity\ncondition holds with high probability and that any feasible local sparse\nsolution of the estimation problem can achieve the near-oracle estimation error\nbound. We further rigorously verify that a wild bootstrap procedure based on a\ndebiased version of the local solution can provide asymptotically honest\nuniform inference for the effect of a group of variables on optimal decision\nmaking. The advantage of honest inference is that it does not require the\ninitial estimator to achieve perfect model selection and does not require the\nzero and nonzero effects to be well-separated. We also propose an efficient\nalgorithm for estimation. Our simulations suggest satisfactory performance. An\nexample from a diabetes study illustrates the real application.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 04:00:34 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wu", "Yunan", ""], ["Wang", "Lan", ""], ["Fu", "Haoda", ""]]}, {"id": "2105.04134", "submitter": "Daniel Barreiro Ures", "authors": "D. Barreiro-Ures, R. Cao and M. Francisco-Fern\\'andez", "title": "Bagging cross-validated bandwidth selection in nonparametric regression\n  estimation with applications to large-sized samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cross-validation is a well-known and widely used bandwidth selection method\nin nonparametric regression estimation. However, this technique has two\nremarkable drawbacks: (i) the large variability of the selected bandwidths, and\n(ii) the inability to provide results in a reasonable time for very large\nsample sizes. To overcome these problems, bagging cross-validation bandwidths\nare analyzed in this paper. This approach consists in computing the\ncross-validation bandwidths for a finite number of subsamples and then\nrescaling the averaged smoothing parameters to the original sample size. Under\na random-design regression model, asymptotic expressions up to a second-order\nfor the bias and variance of the leave-one-out cross-validation bandwidth for\nthe Nadaraya--Watson estimator are obtained. Subsequently, the asymptotic bias\nand variance and the limit distribution are derived for the bagged\ncross-validation selector. Suitable choices of the number of subsamples and the\nsubsample size lead to an $n^{-1/2}$ rate for the convergence in distribution\nof the bagging cross-validation selector, outperforming the rate $n^{-3/10}$ of\nleave-one-out cross-validation. Several simulations and an illustration on a\nreal dataset related to the COVID-19 pandemic show the behavior of our proposal\nand its better performance, in terms of statistical efficiency and computing\ntime, when compared to leave-one-out cross-validation.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 06:31:37 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Barreiro-Ures", "D.", ""], ["Cao", "R.", ""], ["Francisco-Fern\u00e1ndez", "M.", ""]]}, {"id": "2105.04379", "submitter": "Steven Kleinegesse", "authors": "Steven Kleinegesse and Michael U. Gutmann", "title": "Gradient-based Bayesian Experimental Design for Implicit Models using\n  Mutual Information Lower Bounds", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a framework for Bayesian experimental design (BED) with implicit\nmodels, where the data-generating distribution is intractable but sampling from\nit is still possible. In order to find optimal experimental designs for such\nmodels, our approach maximises mutual information lower bounds that are\nparametrised by neural networks. By training a neural network on sampled data,\nwe simultaneously update network parameters and designs using stochastic\ngradient-ascent. The framework enables experimental design with a variety of\nprominent lower bounds and can be applied to a wide range of scientific tasks,\nsuch as parameter estimation, model discrimination and improving future\npredictions. Using a set of intractable toy models, we provide a comprehensive\nempirical comparison of prominent lower bounds applied to the aforementioned\ntasks. We further validate our framework on a challenging system of stochastic\ndifferential equations from epidemiology.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 13:59:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Kleinegesse", "Steven", ""], ["Gutmann", "Michael U.", ""]]}, {"id": "2105.04399", "submitter": "Antonio Elias Fernandez", "authors": "Antonio El\\'ias (1) and Ra\\'ul Jim\\'enez (2) and Hanlin Shang (3) ((1)\n  OASYS group, Department of Applied Mathematics, Universidad de M\\'alaga,\n  M\\'alaga, Spain, (2) Department of Statistics, Universidad Carlos III de\n  Madrid, Madrid, Spain, (3) Department of Actuarial Studies and Business\n  Analytics, Macquarie University, Sydney, Australia)", "title": "On projection methods for functional time series forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Two nonparametric methods are presented for forecasting functional time\nseries (FTS). The FTS we observe is a curve at a discrete-time point. We\naddress both one-step-ahead forecasting and dynamic updating. Dynamic updating\nis a forward prediction of the unobserved segment of the most recent curve.\nAmong the two proposed methods, the first one is a straightforward adaptation\nto FTS of the $k$-nearest neighbors methods for univariate time series\nforecasting. The second one is based on a selection of curves, termed \\emph{the\ncurve envelope}, that aims to be representative in shape and magnitude of the\nmost recent functional observation, either a whole curve or the observed part\nof a partially observed curve. In a similar fashion to $k$-nearest neighbors\nand other projection methods successfully used for time series forecasting, we\n``project'' the $k$-nearest neighbors and the curves in the envelope for\nforecasting. In doing so, we keep track of the next period evolution of the\ncurves. The methods are applied to simulated data, daily electricity demand,\nand NOx emissions and provide competitive results with and often superior to\nseveral benchmark predictions. The approach offers a model-free alternative to\nstatistical methods based on FTS modeling to study the cyclic or seasonal\nbehavior of many FTS.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 14:24:38 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["El\u00edas", "Antonio", ""], ["Jim\u00e9nez", "Ra\u00fal", ""], ["Shang", "Hanlin", ""]]}, {"id": "2105.04451", "submitter": "David B. Dahl", "authors": "David B. Dahl, Devin J. Johnson, Peter Mueller", "title": "Search Algorithms and Loss Functions for Bayesian Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a randomized greedy search algorithm to find a point estimate for\na random partition based on a loss function and posterior Monte Carlo samples.\nGiven the large size and awkward discrete nature of the search space, the\nminimization of the posterior expected loss is challenging. Our approach is a\nstochastic search based on a series of greedy optimizations performed in a\nrandom order and is embarrassingly parallel. We consider several loss\nfunctions, including Binder loss and variation of information. We note that\ncriticisms of Binder loss are the result of using equal penalties of\nmisclassification and we show an efficient means to compute Binder loss with\npotentially unequal penalties. Furthermore, we extend the original variation of\ninformation to allow for unequal penalties and show no increased computational\ncosts. We provide a reference implementation of our algorithm. Using a variety\nof examples, we show that our method produces clustering estimates that better\nminimize the expected loss and are obtained faster than existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 15:28:10 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Dahl", "David B.", ""], ["Johnson", "Devin J.", ""], ["Mueller", "Peter", ""]]}, {"id": "2105.04480", "submitter": "Diederick Vermetten", "authors": "Diederick Vermetten, Anna V. Kononova, Fabio Caraffini, Hao Wang,\n  Thomas B\\\"ack", "title": "Is there Anisotropy in Structural Bias?", "comments": null, "journal-ref": null, "doi": "10.1145/3449726.3463218", "report-no": null, "categories": "stat.ME cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural Bias (SB) is an important type of algorithmic deficiency within\niterative optimisation heuristics. However, methods for detecting structural\nbias have not yet fully matured, and recent studies have uncovered many\ninteresting questions. One of these is the question of how structural bias can\nbe related to anisotropy. Intuitively, an algorithm that is not isotropic would\nbe considered structurally biased. However, there have been cases where\nalgorithms appear to only show SB in some dimensions. As such, we investigate\nwhether these algorithms actually exhibit anisotropy, and how this impacts the\ndetection of SB. We find that anisotropy is very rare, and even in cases where\nit is present, there are clear tests for SB which do not rely on any\nassumptions of isotropy, so we can safely expand the suite of SB tests to\nencompass these kinds of deficiencies not found by the original tests.\n  We propose several additional testing procedures for SB detection and aim to\nmotivate further research into the creation of a robust portfolio of tests.\nThis is crucial since no single test will be able to work effectively with all\ntypes of SB we identify.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 16:20:21 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Vermetten", "Diederick", ""], ["Kononova", "Anna V.", ""], ["Caraffini", "Fabio", ""], ["Wang", "Hao", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "2105.04518", "submitter": "Wenrui Li", "authors": "Wenrui Li, Daniel L. Sussman, Eric D. Kolaczyk", "title": "Causal Inference under Network Interference with Noise", "comments": "57 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Increasingly, there is a marked interest in estimating causal effects under\nnetwork interference due to the fact that interference manifests naturally in\nnetworked experiments. However, network information generally is available only\nup to some level of error. We study the propagation of such errors to\nestimators of average causal effects under network interference. Specifically,\nassuming a four-level exposure model and Bernoulli random assignment of\ntreatment, we characterize the impact of network noise on the bias and variance\nof standard estimators in homogeneous and inhomogeneous networks. In addition,\nwe propose method-of-moments estimators for bias reduction. We illustrate the\npractical performance of our estimators through simulation studies in British\nsecondary school contact networks.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 17:12:09 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Li", "Wenrui", ""], ["Sussman", "Daniel L.", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "2105.04628", "submitter": "Giorgio Pioda", "authors": "Giorgio Pioda", "title": "New ideas for method comparison: a Monte Carlo power analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper some new proposals for method comparison are presented. On the\none hand, two new robust regressions, the M-Deming and the MM-Deming, have been\ndeveloped by modifying Linnet's method of the weighted Deming regression. The\nM-Deming regression shows superior qualities to the Passing-Bablok regression;\nit does not suffer from bias when the data to be validated have a reduced\nprecision, and therefore turns out to be much more reliable. On the other hand,\na graphical method (box and ellipses) for validations has been developed which\nis also equipped with a unified statistical test. In this test the intercept\nand slope pairs obtained from a bootstrap process are combined into a\nmultinomial distribution by robust determination of the covariance matrix. The\nMahalanobis distance from the point representing the null hypothesis is\nevaluated using the $\\chi^{2}$ distribution. It is emphasized that the\ninterpretation of the graph is more important than the probability obtained\nfrom the test. The unified test has been evaluated through Monte Carlo\nsimulations, comparing the theoretical $\\alpha$ levels with the empirical rate\nof rejections (type-I errors). In addition, a power comparison of the various\n(new and old) methods was conducted using the same techniques. This unified\nmethod, regardless of the regression chosen, shows much higher power and allows\na significant reduction in the sample size required for validations.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 19:29:41 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Pioda", "Giorgio", ""]]}, {"id": "2105.04648", "submitter": "Hyungrok Do", "authors": "Hyungrok Do, Shinjini Nandi, Preston Putzel, Padhraic Smyth, Judy\n  Zhong", "title": "Joint Fairness Model with Applications to Risk Predictions for\n  Under-represented Populations", "comments": "62 pages, 16 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Under-representation of certain populations, based on gender, race/ethnicity,\nand age, in data collection for predictive modeling may yield less-accurate\npredictions for the under-represented groups. Recently, this issue of fairness\nin predictions has attracted significant attention, as data-driven models are\nincreasingly utilized to perform crucial decision-making tasks. Methods to\nachieve fairness in the machine learning literature typically build a single\nprediction model subject to some fairness criteria in a manner that encourages\nfair prediction performances for all groups. These approaches have two major\nlimitations: i) fairness is often achieved by compromising accuracy for some\ngroups; ii) the underlying relationship between dependent and independent\nvariables may not be the same across groups. We propose a Joint Fairness Model\n(JFM) approach for binary outcomes that estimates group-specific classifiers\nusing a joint modeling objective function that incorporates fairness criteria\nfor prediction. We introduce an Accelerated Smoothing Proximal Gradient\nAlgorithm to solve the convex objective function, and demonstrate the\nproperties of the proposed JFM estimates. Next, we presented the key asymptotic\nproperties for the JFM parameter estimates. We examined the efficacy of the JFM\napproach in achieving prediction performances and parities, in comparison with\nthe Single Fairness Model, group-separate model, and group-ignorant model\nthrough extensive simulations. Finally, we demonstrated the utility of the JFM\nmethod in the motivating example to obtain fair risk predictions for\nunder-represented older patients diagnosed with coronavirus disease 2019\n(COVID-19).\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 20:05:39 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 14:47:52 GMT"}, {"version": "v3", "created": "Fri, 21 May 2021 15:16:51 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Do", "Hyungrok", ""], ["Nandi", "Shinjini", ""], ["Putzel", "Preston", ""], ["Smyth", "Padhraic", ""], ["Zhong", "Judy", ""]]}, {"id": "2105.04656", "submitter": "Chirag Gupta", "authors": "Chirag Gupta, Aaditya K. Ramdas", "title": "Distribution-free calibration guarantees for histogram binning without\n  sample splitting", "comments": "Appears at ICML 2021\n  (http://proceedings.mlr.press/v139/gupta21b.html)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove calibration guarantees for the popular histogram binning (also\ncalled uniform-mass binning) method of Zadrozny and Elkan [2001]. Histogram\nbinning has displayed strong practical performance, but theoretical guarantees\nhave only been shown for sample split versions that avoid 'double dipping' the\ndata. We demonstrate that the statistical cost of sample splitting is\npractically significant on a credit default dataset. We then prove calibration\nguarantees for the original method that double dips the data, using a certain\nMarkov property of order statistics. Based on our results, we make practical\nrecommendations for choosing the number of bins in histogram binning. In our\nillustrative simulations, we propose a new tool for assessing calibration --\nvalidity plots -- which provide more information than an ECE estimate. Code for\nthis work will be made publicly available at\nhttps://github.com/aigen/df-posthoc-calibration.\n", "versions": [{"version": "v1", "created": "Mon, 10 May 2021 20:26:26 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 20:09:35 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Gupta", "Chirag", ""], ["Ramdas", "Aaditya K.", ""]]}, {"id": "2105.04795", "submitter": "Rajarshi Guhaniyogi", "authors": "Rajarshi Guhaniyogi and Aaron Scheffler", "title": "Sketching in Bayesian High Dimensional Regression With Big Data Using\n  Gaussian Scale Mixture Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Bayesian computation of high dimensional linear regression models with a\npopular Gaussian scale mixture prior distribution using Markov Chain Monte\nCarlo (MCMC) or its variants can be extremely slow or completely prohibitive\ndue to the heavy computational cost that grows in the cubic order of p, with p\nas the number of features. Although a few recently developed algorithms make\nthe computation efficient in presence of a small to moderately large sample\nsize (with the complexity growing in the cubic order of n), the computation\nbecomes intractable when sample size n is also large. In this article we adopt\nthe data sketching approach to compress the n original samples by a random\nlinear transformation to m<<n samples in p dimensions, and compute Bayesian\nregression with Gaussian scale mixture prior distributions with the randomly\ncompressed response vector and feature matrix. Our proposed approach yields\ncomputational complexity growing in the cubic order of m. Another important\nmotivation for this compression procedure is that it anonymizes the data by\nrevealing little information about the original data in the course of analysis.\nOur detailed empirical investigation with the Horseshoe prior from the class of\nGaussian scale mixture priors shows closely similar inference and a massive\nreduction in per iteration computation time of the proposed approach compared\nto the regression with the full sample. One notable contribution of this\narticle is to derive posterior contraction rate for high dimensional predictor\ncoefficient with a general class of shrinkage priors on them under data\ncompression/sketching. In particular, we characterize the dimension of the\ncompressed response vector m as a function of the sample size, number of\npredictors and sparsity in the regression to guarantee accurate estimation of\npredictor coefficients asymptotically, even after data compression.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 06:05:45 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Guhaniyogi", "Rajarshi", ""], ["Scheffler", "Aaron", ""]]}, {"id": "2105.04912", "submitter": "Jeremie Houssineau", "authors": "Jeremy Heng, Jeremie Houssineau and Ajay Jasra", "title": "On Unbiased Score Estimation for Partially Observed Diffusions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of statistical inference for a class of\npartially-observed diffusion processes, with discretely-observed data and\nfinite-dimensional parameters. We construct unbiased estimators of the score\nfunction, i.e. the gradient of the log-likelihood function with respect to\nparameters, with no time-discretization bias. These estimators can be\nstraightforwardly employed within stochastic gradient methods to perform\nmaximum likelihood estimation or Bayesian inference. As our proposed\nmethodology only requires access to a time-discretization scheme such as the\nEuler-Maruyama method, it is applicable to a wide class of diffusion processes\nand observation models. Our approach is based on a representation of the score\nas a smoothing expectation using Girsanov theorem, and a novel adaptation of\nthe randomization schemes developed in Mcleish [2011], Rhee and Glynn [2015],\nJacob et al. [2020a]. This allows one to remove the time-discretization bias\nand burn-in bias when computing smoothing expectations using the conditional\nparticle filter of Andrieu et al. [2010]. Central to our approach is the\ndevelopment of new couplings of multiple conditional particle filters. We prove\nunder assumptions that our estimators are unbiased and have finite variance.\nThe methodology is illustrated on several challenging applications from\npopulation ecology and neuroscience.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 09:58:12 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Heng", "Jeremy", ""], ["Houssineau", "Jeremie", ""], ["Jasra", "Ajay", ""]]}, {"id": "2105.04981", "submitter": "Cecilia Balocchi", "authors": "Cecilia Balocchi, Ray Bai, Jessica Liu, Silvia P. Canel\\'on, Edward I.\n  George, Yong Chen, Mary R. Boland", "title": "A Bayesian Hierarchical Modeling Framework for Geospatial Analysis of\n  Adverse Pregnancy Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying the determinants of adverse pregnancy outcomes like stillbirth and\npreterm birth is of considerable interest in epidemiology. Understanding the\nrole of both individual and community risk factors for these outcomes is\ncrucial for planning appropriate clinical and public health interventions. With\nthis goal, we develop geospatial mixed effects logistic regression models for\nadverse pregnancy outcomes. Our models account for both spatial autocorrelation\nand heterogeneity between neighborhoods. To mitigate the low incidence of\nstillbirth and preterm births in our data, we explore using class rebalancing\ntechniques to improve predictive power. To assess the informative value of the\ncovariates in our models, we use posterior distributions of their coefficients\nto gauge how well they can be distinguished from zero. As a case study, we\nmodel stillbirth and preterm birth in the city of Philadelphia, incorporating\nboth patient-level data from electronic health records (EHR) data and publicly\navailable neighborhood data at the census tract level. We find that\npatient-level features like self-identified race and ethnicity were highly\ninformative for both outcomes. Neighborhood-level factors were also\ninformative, with poverty important for stillbirth and crime important for\npreterm birth. Finally, we identify the neighborhoods in Philadelphia at\nhighest risk of stillbirth and preterm birth.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 12:35:50 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Balocchi", "Cecilia", ""], ["Bai", "Ray", ""], ["Liu", "Jessica", ""], ["Canel\u00f3n", "Silvia P.", ""], ["George", "Edward I.", ""], ["Chen", "Yong", ""], ["Boland", "Mary R.", ""]]}, {"id": "2105.05082", "submitter": "Hee Cheol Chung Dr.", "authors": "Hee Cheol Chung, Irina Gaynanova, Yang Ni", "title": "Phylogenetically informed Bayesian truncated copula graphical models for\n  microbial association networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microorganisms play a critical role in host health. The advancement of\nhigh-throughput sequencing technology provides opportunities for a deeper\nunderstanding of microbial interactions. However, due to the limitations of 16S\nribosomal RNA sequencing, microbiome data are zero-inflated, and a quantitative\ncomparison of microbial abundances cannot be made across subjects. By\nleveraging a recent microbiome profiling technique that quantifies 16S\nribosomal RNA microbial counts, we propose a novel Bayesian graphical model\nthat incorporates microorganisms' evolutionary history through a phylogenetic\ntree prior and explicitly accounts for zero-inflation using the truncated\nGaussian copula. Our simulation study reveals that the evolutionary information\nsubstantially improves the network estimation accuracy. We apply the proposed\nmodel to the quantitative gut microbiome data of 106 healthy subjects, and\nidentify three distinct microbial communities that are not determined by\nexisting microbial network estimation models. We further find that these\ncommunities are discriminated based on microorganisms' ability to utilize\noxygen as an energy source.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 14:38:06 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chung", "Hee Cheol", ""], ["Gaynanova", "Irina", ""], ["Ni", "Yang", ""]]}, {"id": "2105.05157", "submitter": "Brady Nifong", "authors": "Brady Nifong, Matthew A. Psioda and Joseph G. Ibrahim", "title": "The scale transformed power prior for use with historical data from a\n  different outcome model", "comments": "21 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the scale transformed power prior for settings where historical\nand current data involve different data types, such as binary and continuous\ndata, respectively. This situation arises often in clinical trials, for\nexample, when historical data involve binary responses and the current data\ninvolve time-to-event or some other type of continuous or discrete outcome. The\npower prior proposed by Ibrahim and Chen (2000) does not address the issue of\ndifferent data types. Herein, we develop a new type of power prior, which we\ncall the scale transformed power prior (straPP). The straPP is constructed by\ntransforming the power prior for the historical data by rescaling the parameter\nusing a function of the Fisher information matrices for the historical and\ncurrent data models, thereby shifting the scale of the parameter vector from\nthat of the historical to that of the current data. Examples are presented to\nmotivate the need for a scale transformation and simulation studies are\npresented to illustrate the performance advantages of the straPP over the power\nprior and other informative and non-informative priors. A real dataset from a\nclinical trial undertaken to study a novel transitional care model for stroke\nsurvivors is used to illustrate the methodology.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 16:11:42 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Nifong", "Brady", ""], ["Psioda", "Matthew A.", ""], ["Ibrahim", "Joseph G.", ""]]}, {"id": "2105.05213", "submitter": "Oluwasegun Ojo", "authors": "Oluwasegun Ojo, Rosa E. Lillo, Antonio Fern\\'andez Anta", "title": "Outlier Detection for Functional Data with R Package fdaoutlier", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Outlier detection is one of the standard exploratory analysis tasks in\nfunctional data analysis. We present the R package fdaoutlier which contains\nimplementations of some of the latest techniques for detecting functional\noutliers. The package makes it easy to detect different types of outliers\n(magnitude, shape, and amplitude) in functional data, and some of the\nimplemented methods can be applied to both univariate and multivariate\nfunctional data. We illustrate the main functionality of the R package with\ncommon functional datasets in the literature.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 17:28:53 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Ojo", "Oluwasegun", ""], ["Lillo", "Rosa E.", ""], ["Anta", "Antonio Fern\u00e1ndez", ""]]}, {"id": "2105.05249", "submitter": "Chris Tofallis Dr.", "authors": "Chris Tofallis", "title": "A better measure of relative prediction accuracy for model selection and\n  model estimation", "comments": null, "journal-ref": "Journal of the Operational Research Society, 66(8), 1352-1362\n  (2015)", "doi": "10.1057/jors.2014.103", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Surveys show that the mean absolute percentage error (MAPE) is the most\nwidely used measure of forecast accuracy in businesses and organizations. It is\nhowever, biased: When used to select among competing prediction methods it\nsystematically selects those whose predictions are too low. This is not widely\ndiscussed and so is not generally known among practitioners. We explain why\nthis happens. We investigate an alternative relative accuracy measure which\navoids this bias: the log of the accuracy ratio: log (prediction / actual).\nRelative accuracy is particularly relevant if the scatter in the data grows as\nthe value of the variable grows (heteroscedasticity). We demonstrate using\nsimulations that for heteroscedastic data (modelled by a multiplicative error\nfactor) the proposed metric is far superior to MAPE for model selection.\nAnother use for accuracy measures is in fitting parameters to prediction\nmodels. Minimum MAPE models do not predict a simple statistic and so\ntheoretical analysis is limited. We prove that when the proposed metric is used\ninstead, the resulting least squares regression model predicts the geometric\nmean. This important property allows its theoretical properties to be\nunderstood.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 20:02:42 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Tofallis", "Chris", ""]]}, {"id": "2105.05280", "submitter": "Xinrui Liu", "authors": "Xinrui Liu, Yifeng Wu, Douglas L. Irving, Meng Li", "title": "Gaussian graphical models with graph constraints for magnetic moment\n  interaction in high entropy alloys", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article is motivated by studying the interaction of magnetic moments in\nhigh entropy alloys (HEAs), which plays an important role in guiding HEA\ndesigns in materials science. While first principles simulations can capture\nmagnetic moments of individual atoms, explicit models are required to analyze\ntheir interactions. This is essentially an inverse covariance matrix estimation\nproblem. Unlike most of the literature on graphical models, the inverse\ncovariance matrix possesses inherent structural constraints encompassing node\ntypes, topological distance of nodes, and partially specified conditional\ndependence patterns. The present article is, to our knowledge, the first to\nconsider such intricate structures in graphical models. In particular, we\nintroduce graph constraints to formulate these structures that arise from\ndomain knowledge and are critical for interpretability, which leads to a\nBayesian conditional autoregressive model with graph constraints (CARGO) for\nstructured inverse covariance matrices. The CARGO method enjoys efficient\nimplementation with a modified Gauss-Seidel scheme through proximity operators\nfor closed-form posterior exploration. We establish algorithmic convergence for\nthe proposed algorithm under a verifiable stopping criterion. Simulations show\ncompetitive performance of CARGO relative to several other methods and confirm\nour convergence analysis. In a novel real data application to HEAs, the\nproposed methods lead to data-driven quantification and interpretation of\nmagnetic moment interactions with high tractability and transferability.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 18:26:31 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Liu", "Xinrui", ""], ["Wu", "Yifeng", ""], ["Irving", "Douglas L.", ""], ["Li", "Meng", ""]]}, {"id": "2105.05314", "submitter": "Raphael Huser", "authors": "Zhongwei Zhang, Rapha\\\"el Huser, Thomas Opitz and Jennifer L.\n  Wadsworth", "title": "Modeling spatial extremes using normal mean-variance mixtures", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical models for multivariate or spatial extremes are mainly based upon\nthe asymptotically justified max-stable or generalized Pareto processes. These\nmodels are suitable when asymptotic dependence is present, i.e., the joint tail\ndecays at the same rate as the marginal tail. However, recent environmental\ndata applications suggest that asymptotic independence is equally important\nand, unfortunately, existing spatial models in this setting that are both\nflexible and can be fitted efficiently are scarce. Here, we propose a new\nspatial copula model based on the generalized hyperbolic distribution, which is\na specific normal mean-variance mixture and is very popular in financial\nmodeling. The tail properties of this distribution have been studied in the\nliterature, but with contradictory results. It turns out that the proofs from\nthe literature contain mistakes. We here give a corrected theoretical\ndescription of its tail dependence structure and then exploit the model to\nanalyze a simulated dataset from the inverted Brown-Resnick process, hindcast\nsignificant wave height data in the North Sea, and wind gust data in the state\nof Oklahoma, USA. We demonstrate that our proposed model is flexible enough to\ncapture the dependence structure not only in the tail but also in the bulk.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 19:17:51 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zhang", "Zhongwei", ""], ["Huser", "Rapha\u00ebl", ""], ["Opitz", "Thomas", ""], ["Wadsworth", "Jennifer L.", ""]]}, {"id": "2105.05341", "submitter": "Xiaodong Wang", "authors": "Xiaodong Wang and Fushing Hsieh", "title": "An Encoding Approach for Stable Change Point Detection", "comments": "30 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Without imposing prior distributional knowledge underlying multivariate time\nseries of interest, we propose a nonparametric change-point detection approach\nto estimate the number of change points and their locations along the temporal\naxis. We develop a structural subsampling procedure such that the observations\nare encoded into multiple sequences of Bernoulli variables. A maximum\nlikelihood approach in conjunction with a newly developed searching algorithm\nis implemented to detect change points on each Bernoulli process separately.\nThen, aggregation statistics are proposed to collectively synthesize\nchange-point results from all individual univariate time series into consistent\nand stable location estimations. We also study a weighting strategy to measure\nthe degree of relevance for different subsampled groups. Simulation studies are\nconducted and shown that the proposed change-point methodology for multivariate\ntime series has favorable performance comparing with currently popular\nnonparametric methods under various settings with different degrees of\ncomplexity. Real data analyses are finally performed on categorical, ordinal,\nand continuous time series taken from fields of genetics, climate, and finance.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 21:00:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Wang", "Xiaodong", ""], ["Hsieh", "Fushing", ""]]}, {"id": "2105.05363", "submitter": "Peiyi Zhang", "authors": "Peiyi Zhang, Qifan Song, Faming Liang", "title": "A Langevinized Ensemble Kalman Filter for Large-Scale Static and Dynamic\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ensemble Kalman Filter (EnKF) has achieved great successes in data\nassimilation in atmospheric and oceanic sciences, but its failure in\nconvergence to the right filtering distribution precludes its use for\nuncertainty quantification. We reformulate the EnKF under the framework of\nLangevin dynamics, which leads to a new particle filtering algorithm, the\nso-called Langevinized EnKF. The Langevinized EnKF inherits the\nforecast-analysis procedure from the EnKF and the use of mini-batch data from\nthe stochastic gradient Langevin-type algorithms, which make it scalable with\nrespect to both the dimension and sample size. We prove that the Langevinized\nEnKF converges to the right filtering distribution in Wasserstein distance\nunder the big data scenario that the dynamic system consists of a large number\nof stages and has a large number of samples observed at each stage. We\nreformulate the Bayesian inverse problem as a dynamic state estimation problem\nbased on the techniques of subsampling and Langevin diffusion process. We\nillustrate the performance of the Langevinized EnKF using a variety of\nexamples, including the Lorenz-96 model, high-dimensional variable selection,\nBayesian deep learning, and Long Short Term Memory (LSTM) network learning with\ndynamic data.\n", "versions": [{"version": "v1", "created": "Tue, 11 May 2021 23:20:18 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zhang", "Peiyi", ""], ["Song", "Qifan", ""], ["Liang", "Faming", ""]]}, {"id": "2105.05373", "submitter": "Yue You", "authors": "Yue You, Mark van der Laan, Philip Collender, Qu Cheng, Alan Hubbard,\n  Nicholas P Jewell, Zhiyue Tom Hu, Robin Mejia and Justin Remais", "title": "Estimation of population size based on capture recapture designs and\n  evaluation of the estimation reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a modern method to estimate population size based on\ncapture-recapture designs of K samples. The observed data is formulated as a\nsample of n i.i.d. K-dimensional vectors of binary indicators, where the k-th\ncomponent of each vector indicates the subject being caught by the k-th sample,\nsuch that only subjects with nonzero capture vectors are observed. The target\nquantity is the unconditional probability of the vector being nonzero across\nboth observed and unobserved subjects. We cover models assuming a single\nconstraint (identification assumption) on the K-dimensional distribution such\nthat the target quantity is identified and the statistical model is\nunrestricted. We present solutions for linear and non-linear constraints\ncommonly assumed to identify capture-recapture models, including no K-way\ninteraction in linear and log-linear models, independence or conditional\nindependence. We demonstrate that the choice of constraint has a dramatic\nimpact on the value of the estimand, showing that it is crucial that the\nconstraint is known to hold by design. For the commonly assumed constraint of\nno K-way interaction in a log-linear model, the statistical target parameter is\nonly defined when each of the $2^K - 1$ observable capture patterns is present,\nand therefore suffers from the curse of dimensionality. We propose a targeted\nMLE based on undersmoothed lasso model to smooth across the cells while\ntargeting the fit towards the single valued target parameter of interest. For\neach identification assumption, we provide simulated inference and confidence\nintervals to assess the performance on the estimator under correct and\nincorrect identifying assumptions. We apply the proposed method, alongside\nexisting estimators, to estimate prevalence of a parasitic infection using\nmulti-source surveillance data from a region in southwestern China, under the\nfour identification assumptions.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 00:12:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["You", "Yue", ""], ["van der Laan", "Mark", ""], ["Collender", "Philip", ""], ["Cheng", "Qu", ""], ["Hubbard", "Alan", ""], ["Jewell", "Nicholas P", ""], ["Hu", "Zhiyue Tom", ""], ["Mejia", "Robin", ""], ["Remais", "Justin", ""]]}, {"id": "2105.05439", "submitter": "Changbo Zhu", "authors": "Changbo Zhu and Hans-Georg M\\\"uller", "title": "Autoregressive Optimal Transport Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Series of distributions indexed by equally spaced time points are ubiquitous\nin applications and their analysis constitutes one of the challenges of the\nemerging field of distributional data analysis. To quantify such distributional\ntime series, we propose a class of intrinsic autoregressive models that operate\nin the space of optimal transport maps. The autoregressive transport models\nthat we introduce here are based on regressing optimal transport maps on each\nother, where predictors can be transport maps from an overall barycenter to a\ncurrent distribution or transport maps between past consecutive distributions\nof the distributional time series. Autoregressive transport models and\nassociated distributional regression models specify the link between predictor\nand response transport maps by moving along geodesics in Wasserstein space.\nThese models emerge as natural extensions of the classical autoregressive\nmodels in Euclidean space. Unique stationary solutions of autoregressive\ntransport models are shown to exist under a geometric moment contraction\ncondition of Wu and Shao (2004), using properties of iterated random functions.\nWe also discuss an extension to a varying coefficient coefficient for first\norder autoregressive transport models. In addition to simulations, the proposed\nmodels are illustrated with distributional time series of house prices across\nU.S. counties and of stock returns across the S&P 500 stock index.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 05:40:27 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 18:56:22 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Zhu", "Changbo", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "2105.05483", "submitter": "Chi-Hong Tseng", "authors": "Chi-Hong Tseng, Danielle Sim", "title": "Sample size planning for pilot studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pilot studies are often the first step of experimental research. It is\nusually on a smaller scale and the results can inform intervention development,\nstudy feasibility and how the study implementation will play out, if such a\nlarger main study is undertaken. This paper illustrates the relationship\nbetween pilot study sample size and the performance study design of main\nstudies. We present two simple sample size calculation methods to ensure\nadequate study planning for main studies. We use numerical examples and\nsimulations to demonstrate the use and performance of proposed methods.\nPractical heuristic guidelines are provided based on the results.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 07:45:42 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Tseng", "Chi-Hong", ""], ["Sim", "Danielle", ""]]}, {"id": "2105.05523", "submitter": "Martin Bladt", "authors": "Martin Bladt, Hansjoerg Albrecher, Jan Beirlant", "title": "Trimmed extreme value estimators for censored heavy-tailed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimation of the extreme value index and extreme quantiles for\nheavy-tailed data that are right-censored. We study a general procedure of\nremoving low importance observations in tail estimators. This trimming\nprocedure is applied to the state-of-the-art estimators for randomly\nright-censored tail estimators. Through an averaging procedure over the amount\nof trimming we derive new kernel type estimators. Extensive simulation suggests\nthat one of the new considered kernels leads to a highly competitive estimator\nagainst virtually any other available alternative in this framework. Moreover,\nwe propose an adaptive selection method for the amount of top data used in\nestimation based on the trimming procedure minimizing the asymptotic mean\nsquared error. We also provide an illustration of this approach to simulated as\nwell as to real-world MTPL insurance data.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 08:56:23 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Bladt", "Martin", ""], ["Albrecher", "Hansjoerg", ""], ["Beirlant", "Jan", ""]]}, {"id": "2105.05532", "submitter": "Tingguo Zheng", "authors": "Tingguo Zheng, Han Xiao, Rong Chen", "title": "Generalized Autoregressive Moving Average Models with GARCH Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the important and widely used classes of models for non-Gaussian time\nseries is the generalized autoregressive model average models (GARMA), which\nspecifies an ARMA structure for the conditional mean process of the underlying\ntime series. However, in many applications one often encounters conditional\nheteroskedasticity. In this paper we propose a new class of models, referred to\nas GARMA-GARCH models, that jointly specify both the conditional mean and\nconditional variance processes of a general non-Gaussian time series. Under the\ngeneral modeling framework, we propose three specific models, as examples, for\nproportional time series, nonnegative time series, and skewed and heavy-tailed\nfinancial time series. Maximum likelihood estimator (MLE) and quasi Gaussian\nMLE (GMLE) are used to estimate the parameters. Simulation studies and three\napplications are used to demonstrate the properties of the models and the\nestimation procedures.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 09:19:13 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Zheng", "Tingguo", ""], ["Xiao", "Han", ""], ["Chen", "Rong", ""]]}, {"id": "2105.05548", "submitter": "Diakarya Barro Pr", "authors": "B\\'ewentaor\\'e Sawadogo and Diakarya Barro", "title": "Modeling space-time trends and dependence in extreme precipitations of\n  Burkina Faso by the approach of the Peaks-Over-Threshold", "comments": "18 pages 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modeling extremes of climate variables in the framework of climate change is\na particularly difficult task, since it implies taking into account\nspatio-temporal nonstationarities. In this paper, we propose a new method for\nestimating extreme precipitation at the points where we have not observations\nusing information from marginal distributions and dependence structure. To\nreach this goal we combine two statistical approaches of extreme values theory\nallowing on the one hand to control temporal and spatial non-stationarities via\na tail trend function with a spatio-temporal structure in the marginal\ndistributions and by modeling on the other hand the dependence structure by a\nlatent spatial process using generalized `-Pareto processes. This new\nmethodology for trend analysis of extreme events is applied to rainfall data\nfrom Burkina Faso. We show that extreme precipitation is spatially and\ntemporally correlated for distances of approximately 200 km. Locally, extreme\nrainfall has more of an upward than downward trend.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 10:00:29 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Sawadogo", "B\u00e9wentaor\u00e9", ""], ["Barro", "Diakarya", ""]]}, {"id": "2105.05719", "submitter": "Quan Zhou", "authors": "Quan Zhou, Jun Yang, Dootika Vats, Gareth O. Roberts and Jeffrey S.\n  Rosenthal", "title": "Dimension-free Mixing for High-dimensional Bayesian Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Yang et al. (2016) proved that the symmetric random walk Metropolis--Hastings\nalgorithm for Bayesian variable selection is rapidly mixing under mild\nhigh-dimensional assumptions. We propose a novel MCMC sampler using an informed\nproposal scheme, which we prove achieves a much faster mixing time that is\nindependent of the number of covariates, under the same assumptions. To the\nbest of our knowledge, this is the first high-dimensional result which\nrigorously shows that the mixing rate of informed MCMC methods can be fast\nenough to offset the computational cost of local posterior evaluation.\nMotivated by the theoretical analysis of our sampler, we further propose a new\napproach called \"two-stage drift condition\" to studying convergence rates of\nMarkov chains on general state spaces, which can be useful for obtaining tight\ncomplexity bounds in high-dimensional settings. The practical advantages of our\nalgorithm are illustrated by both simulation studies and real data analysis.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 15:12:44 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 23:17:48 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Zhou", "Quan", ""], ["Yang", "Jun", ""], ["Vats", "Dootika", ""], ["Roberts", "Gareth O.", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "2105.05829", "submitter": "Soichiro Yamauchi", "authors": "Shiro Kuriwaki, Soichiro Yamauchi", "title": "Synthetic Area Weighting for Measuring Public Opinion in Small Areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The comparison of subnational areas is ubiquitous but survey samples of these\nareas are often biased or prohibitively small. Researchers turn to methods such\nas multilevel regression and poststratification (MRP) to improve the efficiency\nof estimates by partially pooling data across areas via random effects.\nHowever, the random effect approach can pool observations only through\narea-level aggregates. We instead propose a weighting estimator, the synthetic\narea estimator, which weights on variables measured only in the survey to\npartially pool observations individually. The proposed method consists of\ntwo-step weighting: first to adjust differences across areas and then to adjust\nfor differences between the sample and population. Unlike MRP, our estimator\ncan directly use the national weights that are often estimated from pollsters\nusing proprietary information. Our approach also clarifies the assumptions\nneeded for valid partial pooling, without imposing an outcome model. We apply\nthe proposed method to estimate the support for immigration policies at the\ncongressional district level in Florida. Our empirical results show that small\narea estimation models with insufficient covariates can mask opinion\nheterogeneities across districts.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:41:25 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Kuriwaki", "Shiro", ""], ["Yamauchi", "Soichiro", ""]]}, {"id": "2105.05842", "submitter": "Raaz Dwivedi", "authors": "Raaz Dwivedi, Lester Mackey", "title": "Kernel Thinning", "comments": "Accepted for presentation as an extended abstract at the Conference\n  on Learning Theory (COLT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.CO stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce kernel thinning, a new procedure for compressing a distribution\n$\\mathbb{P}$ more effectively than i.i.d. sampling or standard thinning. Given\na suitable reproducing kernel $\\mathbf{k}$ and $\\mathcal{O}(n^2)$ time, kernel\nthinning compresses an $n$-point approximation to $\\mathbb{P}$ into a\n$\\sqrt{n}$-point approximation with comparable worst-case integration error in\nthe associated reproducing kernel Hilbert space. With high probability, the\nmaximum discrepancy in integration error is\n$\\mathcal{O}_d(n^{-\\frac{1}{2}}\\sqrt{\\log n})$ for compactly supported\n$\\mathbb{P}$ and $\\mathcal{O}_d(n^{-\\frac{1}{2}} \\sqrt{(\\log n)^{d+1}\\log\\log\nn})$ for sub-exponential $\\mathbb{P}$ on $\\mathbb{R}^d$. In contrast, an\nequal-sized i.i.d. sample from $\\mathbb{P}$ suffers $\\Omega(n^{-\\frac14})$\nintegration error. Our sub-exponential guarantees resemble the classical\nquasi-Monte Carlo error rates for uniform $\\mathbb{P}$ on $[0,1]^d$ but apply\nto general distributions on $\\mathbb{R}^d$ and a wide range of common kernels.\nWe use our results to derive explicit non-asymptotic maximum mean discrepancy\nbounds for Gaussian, Mat\\'ern, and B-spline kernels and present two vignettes\nillustrating the practical benefits of kernel thinning over i.i.d. sampling and\nstandard Markov chain Monte Carlo thinning.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 17:56:42 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 17:59:23 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 20:57:55 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Dwivedi", "Raaz", ""], ["Mackey", "Lester", ""]]}, {"id": "2105.05901", "submitter": "Anna Heath", "authors": "Anna Heath", "title": "Calculating Expected Value of Sample Information Adjusting for Imperfect\n  Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The Expected Value of Sample Information (EVSI) calculates the\nvalue of collecting additional information through a study with a given design.\nStandard EVSI analyses assume that the treatment recommendations based on the\nnew information will be implemented immediately and completely once the study\nhas finished. However, treatment implementation is often slow and incomplete,\ngiving a biased estimation of the study value. Previous methods have adjusted\nfor this bias, but they typically make the unrealistic assumption that the\nstudy outcomes do not impact the implementation. One method does assume that\nthe implementation is related to the strength of evidence in favour of the\ntreatment but this method uses analytical results, which require alternative\nrestrictive assumptions. Methods: We develop two implementation-adjusted EVSI\ncalculation methods that relax these assumptions. The first method uses\ncomputationally demanding nested simulations, using the definition of the\nimplementation-adjusted EVSI. The second method aims to facilitate the\ncomputation by adapting a recently developed efficient EVSI computation method\nto adjust for imperfect implementation. The implementation-adjusted EVSI is\nthen calculated with the two methods across three examples. Results: The\nmaximum difference between the two methods is at most 6% in all examples. The\nefficient computation method is between 6 and 60 times faster than the nested\nsimulation method in this case study and could be used in practice.\nConclusions: The methods developed in this paper calculate\nimplementation-adjusted EVSI using realistic assumptions. The efficient\nestimation method is accurate and can estimate the implementation-adjusted EVSI\nin practice. By adapting standard EVSI estimation methods, we allow accurate\nadjustments for imperfect implementation with the same computational cost as a\nstandard analysis.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 18:43:45 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Heath", "Anna", ""]]}, {"id": "2105.05952", "submitter": "Jakub Stan\\v{e}k", "authors": "Vesna Gotovac {\\DJ}oga\\v{s}, Kate\\v{r}ina Helisov\\'a, Bogdan\n  Radovi\\'c, Jakub Stan\\v{e}k, Mark\\'eta Zikmundov\\'a, Kate\\v{r}ina Brejchov\\'a", "title": "Two-step method for assessing dissimilarity of random sets", "comments": "17 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The paper concerns a new statistical method for assessing dissimilarity of\ntwo random sets based on one realisation of each of them. The method focuses on\nshapes of the components of the random sets, namely on the curvature of their\nboundaries together with the ratios of their perimeters and areas. Theoretical\nbackground is introduced and then, the method is described, justified by a\nsimulation study and applied to real data of two different types of tissue -\nmammary cancer and mastopathy.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 20:28:20 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["\u0110oga\u0161", "Vesna Gotovac", ""], ["Helisov\u00e1", "Kate\u0159ina", ""], ["Radovi\u0107", "Bogdan", ""], ["Stan\u011bk", "Jakub", ""], ["Zikmundov\u00e1", "Mark\u00e9ta", ""], ["Brejchov\u00e1", "Kate\u0159ina", ""]]}, {"id": "2105.05953", "submitter": "Babak Barazandeh", "authors": "Babak Barazandeh, Ali Ghafelebashi, Meisam Razaviyayn, Ram Sriharsha", "title": "Efficient Algorithms for Estimating the Parameters of Mixed Linear\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed linear regression (MLR) model is among the most exemplary statistical\ntools for modeling non-linear distributions using a mixture of linear models.\nWhen the additive noise in MLR model is Gaussian, Expectation-Maximization (EM)\nalgorithm is a widely-used algorithm for maximum likelihood estimation of MLR\nparameters. However, when noise is non-Gaussian, the steps of EM algorithm may\nnot have closed-form update rules, which makes EM algorithm impractical. In\nthis work, we study the maximum likelihood estimation of the parameters of MLR\nmodel when the additive noise has non-Gaussian distribution. In particular, we\nconsider the case that noise has Laplacian distribution and we first show that\nunlike the the Gaussian case, the resulting sub-problems of EM algorithm in\nthis case does not have closed-form update rule, thus preventing us from using\nEM in this case. To overcome this issue, we propose a new algorithm based on\ncombining the alternating direction method of multipliers (ADMM) with EM\nalgorithm idea. Our numerical experiments show that our method outperforms the\nEM algorithm in statistical accuracy and computational time in non-Gaussian\nnoise case.\n", "versions": [{"version": "v1", "created": "Wed, 12 May 2021 20:29:03 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Barazandeh", "Babak", ""], ["Ghafelebashi", "Ali", ""], ["Razaviyayn", "Meisam", ""], ["Sriharsha", "Ram", ""]]}, {"id": "2105.06142", "submitter": "Jessica Silva Lomba", "authors": "Jessica Silva Lomba and Maria Isabel Fraga Alves", "title": "Threshold selection for wave heights: asymptotic methods based on\n  L-moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two automatic threshold selection (TS) methods for Extreme Value analysis\nunder a peaks-over-threshold (POT) approach are presented and evaluated, both\nbuilt on: fitting the Generalized Pareto distribution (GPd) to excesses'\nsamples over candidate levels ; the GPd-specific relation between L-skewness\nand L-kurtosis; the asymptotic behaviour of the matching L-statistics.\nPerformance is illustrated on significant wave heights data sets and compared\nto the L-moment-based heuristic in [10], which is found to be favorable.\n  PUBLISHED VERSION AVAILABLE AT:\nhttps://www.spestatistica.pt/storage/app/uploads/public/609/28f/6d0/60928f6d08a0c016386627.pdf\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 08:42:34 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Lomba", "Jessica Silva", ""], ["Alves", "Maria Isabel Fraga", ""]]}, {"id": "2105.06418", "submitter": "Marco Antonio Pinto Orellana", "authors": "Marco Antonio Pinto-Orellana and Peyman Mirtaheri and Hugo L. Hammer\n  and Hernando Ombao", "title": "SCAU: Modeling spectral causality for multivariate time series with\n  applications to electroencephalograms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Electroencephalograms (EEG) are noninvasive measurement signals of electrical\nneuronal activity in the brain. One of the current major statistical challenges\nis formally measuring functional dependency between those complex signals. This\npaper, proposes the spectral causality model (SCAU), a robust linear model,\nunder a causality paradigm, to reflect inter- and intra-frequency modulation\neffects that cannot be identifiable using other methods. SCAU inference is\nconducted with three main steps: (a) signal decomposition into frequency bins,\n(b) intermediate spectral band mapping, and (c) dependency modeling through\nfrequency-specific autoregressive models (VAR). We apply SCAU to study complex\ndependencies during visual and lexical fluency tasks (word generation and\nvisual fixation) in 26 participants' EEGs. We compared the connectivity\nnetworks estimated using SCAU with respect to a VAR model. SCAU networks show a\nclear contrast for both stimuli while the magnitude links also denoted a low\nvariance in comparison with the VAR networks. Furthermore, SCAU dependency\nconnections not only were consistent with findings in the neuroscience\nliterature, but it also provided further evidence on the directionality of the\nspatio-spectral dependencies such as the delta-originated and theta-induced\nlinks in the fronto-temporal brain network.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:50:38 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Pinto-Orellana", "Marco Antonio", ""], ["Mirtaheri", "Peyman", ""], ["Hammer", "Hugo L.", ""], ["Ombao", "Hernando", ""]]}, {"id": "2105.06435", "submitter": "B\\'en\\'edicte Colnet", "authors": "B\\'en\\'edicte Colnet, Julie Josse, Erwan Scornet, Ga\\\"el Varoquaux", "title": "Generalizing a causal effect: sensitivity analysis and missing\n  covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While a randomized controlled trial (RCT) readily measures the average\ntreatment effect (ATE), this measure may need to be generalized to the target\npopulation to account for a sampling bias in the RCT's population. Identifying\nthis target population treatment effect needs covariates in both sets to\ncapture all treatment effect modifiers that are shifted between the two sets.\nStandard estimators then use either weighting (IPSW), outcome modeling\n(G-formula), or combine the two in doubly robust approaches (AIPSW). However\nsuch covariates are often not available in both sets. Therefore, after\ncompleting existing proofs on the complete case consistency of those three\nestimators, we compute the expected bias induced by a missing covariate,\nassuming a Gaussian distribution and a semi-parametric linear model. This\nenables sensitivity analysis for each missing covariate pattern, giving the\nsign of the expected bias. We also show that there is no gain in imputing a\npartially-unobserved covariate. Finally we study the replacement of a missing\ncovariate by a proxy. We illustrate all these results on simulations, as well\nas semi-synthetic benchmarks using data from the Tennessee Student/Teacher\nAchievement Ratio (STAR), and with a real-world example from critical care\nmedicine.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 17:14:58 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 13:28:16 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Colnet", "B\u00e9n\u00e9dicte", ""], ["Josse", "Julie", ""], ["Scornet", "Erwan", ""], ["Varoquaux", "Ga\u00ebl", ""]]}, {"id": "2105.06523", "submitter": "Tianyu Zhan", "authors": "Tianyu Zhan, Haoda Fu, Jian Kang", "title": "Deep Neural Networks Guided Ensemble Learning for Point Estimation in\n  Finite Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As one of the most important estimators in classical statistics, the\nuniformly minimum variance unbiased estimator (UMVUE) has been adopted for\npoint estimation in many statistical studies, especially for small sample\nproblems. Moving beyond typical settings in the exponential distribution\nfamily, it is usually challenging to prove the existence and further construct\nsuch UMVUE in finite samples. For example in the ongoing Adaptive COVID-19\nTreatment Trial (ACTT), it is hard to characterize the complete sufficient\nstatistics of the underlying treatment effect due to pre-planned modifications\nto design aspects based on accumulated unblinded data. As an alternative\nsolution, we propose a Deep Neural Networks (DNN) guided ensemble learning\nframework to construct an improved estimator from existing ones. We show that\nour estimator is consistent and asymptotically reaches the minimal variance\nwithin the class of linearly combined estimators. Simulation studies are\nfurther performed to demonstrate that our proposed estimator has considerable\nfinite-sample efficiency gain. In the ACTT on COVID-19 as an important\napplication, our method essentially contributes to a more ethical and efficient\nadaptive clinical trial with fewer patients enrolled.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 19:26:14 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhan", "Tianyu", ""], ["Fu", "Haoda", ""], ["Kang", "Jian", ""]]}, {"id": "2105.06559", "submitter": "Theodore Huang", "authors": "Theodore Huang, Gregory Idos, Christine Hong, Stephen Gruber, Giovanni\n  Parmigiani, Danielle Braun", "title": "Extending Models Via Gradient Boosting: An Application to Mendelian\n  Models", "comments": "46 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving existing widely-adopted prediction models is often a more efficient\nand robust way towards progress than training new models from scratch. Existing\nmodels may (a) incorporate complex mechanistic knowledge, (b) leverage\nproprietary information and, (c) have surmounted barriers to adoption. Compared\nto model training, model improvement and modification receive little attention.\nIn this paper we propose a general approach to model improvement: we combine\ngradient boosting with any previously developed model to improve model\nperformance while retaining important existing characteristics. To exemplify,\nwe consider the context of Mendelian models, which estimate the probability of\ncarrying genetic mutations that confer susceptibility to disease by using\nfamily pedigrees and health histories of family members. Via simulations we\nshow that integration of gradient boosting with an existing Mendelian model can\nproduce an improved model that outperforms both that model and the model built\nusing gradient boosting alone. We illustrate the approach on genetic testing\ndata from the USC-Stanford Cancer Genetics Hereditary Cancer Panel (HCP) study.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 21:21:05 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Huang", "Theodore", ""], ["Idos", "Gregory", ""], ["Hong", "Christine", ""], ["Gruber", "Stephen", ""], ["Parmigiani", "Giovanni", ""], ["Braun", "Danielle", ""]]}, {"id": "2105.06600", "submitter": "Ke Wang", "authors": "Ke Wang, Alexander Franks, Sang-Yun Oh", "title": "Learning Gaussian Graphical Models with Latent Confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian Graphical models (GGM) are widely used to estimate the network\nstructures in many applications ranging from biology to finance. In practice,\ndata is often corrupted by latent confounders which biases inference of the\nunderlying true graphical structure. In this paper, we compare and contrast two\nstrategies for inference in graphical models with latent confounders: Gaussian\ngraphical models with latent variables (LVGGM) and PCA-based removal of\nconfounding (PCA+GGM). While these two approaches have similar goals, they are\nmotivated by different assumptions about confounding. In this paper, we explore\nthe connection between these two approaches and propose a new method, which\ncombines the strengths of these two approaches. We prove the consistency and\nconvergence rate for the PCA-based method and use these results to provide\nguidance about when to use each method. We demonstrate the effectiveness of our\nmethodology using both simulations and in two real-world applications.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 00:53:03 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Wang", "Ke", ""], ["Franks", "Alexander", ""], ["Oh", "Sang-Yun", ""]]}, {"id": "2105.06646", "submitter": "Aaron Hudson", "authors": "Aaron Hudson, Marco Carone, Ali Shojaie", "title": "Inference on function-valued parameters using a restricted score test", "comments": "39 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often of interest to make inference on an unknown function that is a\nlocal parameter of the data-generating mechanism, such as a density or\nregression function. Such estimands can typically only be estimated at a\nslower-than-parametric rate in nonparametric and semiparametric models, and\nperforming calibrated inference can be challenging. In many cases, these\nestimands can be expressed as the minimizer of a population risk functional.\nHere, we propose a general framework that leverages such representation and\nprovides a nonparametric extension of the score test for inference on an\ninfinite-dimensional risk minimizer. We demonstrate that our framework is\napplicable in a wide variety of problems. As both analytic and computational\nexamples, we describe how to use our general approach for inference on a mean\nregression function under (i) nonparametric and (ii) partially additive models,\nand evaluate the operating characteristics of the resulting procedures via\nsimulations.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 05:12:23 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Hudson", "Aaron", ""], ["Carone", "Marco", ""], ["Shojaie", "Ali", ""]]}, {"id": "2105.06834", "submitter": "Dean Foster", "authors": "Dean P. Foster and Robert A. Stine", "title": "Threshold Martingales and the Evolution of Forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a martingale that characterizes two properties of\nevolving forecast distributions. Ideal forecasts of a future event behave as\nmartingales, sequen- tially updating the forecast to leverage the available\ninformation as the future event approaches. The threshold martingale introduced\nhere measures the proportion of the forecast distribution lying below a\nthreshold. In addition to being calibrated, a threshold martingale has\nquadratic variation that accumulates to a total determined by a quantile of the\ninitial forecast distribution. Deviations from calibration or to- tal\nvolatility signal problems in the underlying model. Calibration adjustments are\nwell-known, and we augment these by introducing a martingale filter that\nimproves volatility while guaranteeing smaller mean squared error. Thus,\npost-processing can rectify problems with calibration and volatility without\nrevisiting the original forecast- ing model. We apply threshold martingales\nfirst to forecasts from simulated models and then to models that predict the\nwinner in professional basketball games.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 13:49:55 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Foster", "Dean P.", ""], ["Stine", "Robert A.", ""]]}, {"id": "2105.06852", "submitter": "Xinwei Deng", "authors": "Peng Tang, Huijing Jiang, Heeyoung Kim, Xinwei Deng", "title": "Robust Estimation of Sparse Precision Matrix using Adaptive Weighted\n  Graphical Lasso Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimation of a precision matrix (i.e., inverse covariance matrix) is widely\nused to exploit conditional independence among continuous variables. The\ninfluence of abnormal observations is exacerbated in a high dimensional setting\nas the dimensionality increases. In this work, we propose robust estimation of\nthe inverse covariance matrix based on an $l_1$ regularized objective function\nwith a weighted sample covariance matrix. The robustness of the proposed\nobjective function can be justified by a nonparametric technique of the\nintegrated squared error criterion. To address the non-convexity of the\nobjective function, we develop an efficient algorithm in a similar spirit of\nmajorization-minimization. Asymptotic consistency of the proposed estimator is\nalso established. The performance of the proposed method is compared with\nseveral existing approaches via numerical simulations. We further demonstrate\nthe merits of the proposed method with application in genetic network\ninference.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:28:39 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Tang", "Peng", ""], ["Jiang", "Huijing", ""], ["Kim", "Heeyoung", ""], ["Deng", "Xinwei", ""]]}, {"id": "2105.06855", "submitter": "Hongtao Zhang", "authors": "Hongtao Zhang and Alan Y Chiang and Jixian Wang", "title": "Improving the Performance of Bayesian Logistic Regression Model with\n  Overdose Control in Oncology Dose-Finding Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An accurately identified maximum tolerated dose (MTD) serves as the\ncornerstone of successful subsequent phases in oncology drug development.\nBayesian logistic regression model (BLRM) is a popular and versatile\nmodel-based dose-finding design. However, BLRM with original overdose control\nstrategy has been reported to be safe but \"excessively conservative\". In this\nmanuscript, we investigate the reason for conservativeness and point out that a\nmajor reason could be the lack of appropriate underdose control. We propose\ndesigns that balance overdose and underdose control to improve the performance\nover original BLRM. Simulation results reveal that the new designs have better\naccuracy and treat more patients at MTD.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:31:36 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhang", "Hongtao", ""], ["Chiang", "Alan Y", ""], ["Wang", "Jixian", ""]]}, {"id": "2105.06902", "submitter": "Ethan Lawler", "authors": "Ethan Lawler, Chris Field, Joanna Mills Flemming", "title": "A Play on Birds! The staRVe Package for Analyzing Spatio-Temporal\n  Point-Referenced Data in R", "comments": "27 pages, 7 figures, submitted to Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the \\proglang{R} package \\pkg{staRVe} for analyzing\nspatio-temporal point-referenced data in a hierarchical generalized linear\nmixed model framework. Our package is designed to be easy-to-use and\ncomputationally efficient for model fitting, covariate effect estimation,\nprediction, and model validation. Existing workflows can easily take advantage\nof our package, since data input and output uses the \\pkg{sf} and \\pkg{raster}\ndata formats. We develop an understanding of the model through simulation, and\nwork through a typical analysis using a case study of Carolina wren abundance.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 15:40:54 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Lawler", "Ethan", ""], ["Field", "Chris", ""], ["Flemming", "Joanna Mills", ""]]}, {"id": "2105.06995", "submitter": "Nathan Hara", "authors": "Nathan C. Hara, Nicolas Unger, Jean-Baptiste Delisle, Rodrigo D\\'iaz,\n  Damien S\\'egransan", "title": "Improving exoplanet detection capabilities with the false inclusion\n  probability. Comparison with other detection criteria in the context of\n  radial velocities", "comments": "Accepted for publication in Astronomy & Astrophysics", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.EP astro-ph.IM stat.AP stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Context. In exoplanet searches with radial velocity data, the most common\nstatistical significance metrics are the Bayes factor and the false alarm\nprobability (FAP). Both have proved useful, but do not directly address whether\nan exoplanet detection should be claimed. Furthermore, it is unclear which\ndetection threshold should be taken and how robust the detections are to model\nmisspecification. Aims. The present work aims at defining a detection criterion\nwhich conveys as precisely as possible the information needed to claim an\nexoplanet detection. We compare this new criterion to existing ones in terms of\nsensitivity and robustness. Methods. We define a significance metric called the\nfalse inclusion probability (FIP) based on the posterior probability of\npresence of a planet. Posterior distributions are computed with the nested\nsampling package Polychord. We show that for FIP and Bayes factor calculations,\ndefining priors on linear parameters as Gaussian mixture models allows to\nsignificantly speed up computations. The performances of the FAP, Bayes factor\nand FIP are studied with simulations as well as analytical arguments. We\ncompare the methods assuming the model is correct, then evaluate their\nsensitivity to the prior and likelihood choices. Results. Among other\nproperties, the FIP offers ways to test the reliability of the significance\nlevels, it is particularly efficient to account for aliasing and allows to\nexclude the presence of planets with a certain confidence. We find that, in our\nsimulations, the FIP outperforms existing detection metrics. We show that\nplanet detections are sensitive to priors on period and semi-amplitude and that\nletting free the noise parameters offers better performances than fixing a\nnoise model based on a fit to ancillary indicators.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 17:59:04 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Hara", "Nathan C.", ""], ["Unger", "Nicolas", ""], ["Delisle", "Jean-Baptiste", ""], ["D\u00edaz", "Rodrigo", ""], ["S\u00e9gransan", "Damien", ""]]}, {"id": "2105.07027", "submitter": "Andrea Scarinci", "authors": "Andrea Scarinci, Michael Fehler and Youssef Marzouk", "title": "Bayesian inference under model misspecification using\n  transport-Lagrangian distances: an application to seismic inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model misspecification constitutes a major obstacle to reliable inference in\nmany inverse problems. Inverse problems in seismology, for example, are\nparticularly affected by misspecification of wave propagation velocities. In\nthis paper, we focus on a specific seismic inverse problem - full-waveform\nmoment tensor inversion - and develop a Bayesian framework that seeks\nrobustness to velocity misspecification. A novel element of our framework is\nthe use of transport-Lagrangian (TL) distances between observed and model\npredicted waveforms to specify a loss function, and the use of this loss to\ndefine a generalized belief update via a Gibbs posterior. The TL distance\nnaturally disregards certain features of the data that are more sensitive to\nmodel misspecification, and therefore produces less biased or dispersed\nposterior distributions in this setting. To make the latter notion precise, we\nuse several diagnostics to assess the quality of inference and uncertainty\nquantification, i.e., continuous rank probability scores and rank histograms.\nWe interpret these diagnostics in the Bayesian setting and compare the results\nto those obtained using more typical Gaussian noise models and squared-error\nloss, under various scenarios of misspecification. Finally, we discuss\npotential generalizability of the proposed framework to a broader class of\ninverse problems affected by model misspecification.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 18:41:45 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Scarinci", "Andrea", ""], ["Fehler", "Michael", ""], ["Marzouk", "Youssef", ""]]}, {"id": "2105.07060", "submitter": "Aiyou Chen", "authors": "Aiyou Chen, Marco Longfils, Nicolas Remy", "title": "Trimmed Match Design for Randomized Paired Geo Experiments", "comments": "19 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  How to measure the incremental Return On Ad Spend (iROAS) is a fundamental\nproblem for the online advertising industry. A standard modern tool is to run\nrandomized geo experiments, where experimental units are non-overlapping\nad-targetable geographical areas (Vaver & Koehler 2011). However, how to design\na reliable and cost-effective geo experiment can be complicated, for example:\n1) the number of geos is often small, 2) the response metric (e.g. revenue)\nacross geos can be very heavy-tailed due to geo heterogeneity, and furthermore\n3) the response metric can vary dramatically over time. To address these\nissues, we propose a robust nonparametric method for the design, called Trimmed\nMatch Design (TMD), which extends the idea of Trimmed Match (Chen & Au 2019)\nand furthermore integrates the techniques of optimal subset pairing and sample\nsplitting in a novel and systematic manner. Some simulation and real case\nstudies are presented. We also point out a few open problems for future\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 20:28:01 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 21:00:05 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Chen", "Aiyou", ""], ["Longfils", "Marco", ""], ["Remy", "Nicolas", ""]]}, {"id": "2105.07119", "submitter": "Alexander Fisher", "authors": "Alexander A. Fisher, Xiang Ji, Akihiko Nishimura, Philippe Lemey and\n  Marc A. Suchard", "title": "Shrinkage-based random local clocks with scalable inference", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local clock models propose that the rate of molecular evolution is constant\nwithin phylogenetic sub-trees. Current local clock inference procedures scale\npoorly to large taxa problems, impose model misspecification, or require a\npriori knowledge of the existence and location of clocks. To overcome these\nchallenges, we present an autocorrelated, Bayesian model of heritable clock\nrate evolution that leverages heavy-tailed priors with mean zero to shrink\nincrements of change between branch-specific clocks. We further develop an\nefficient Hamiltonian Monte Carlo sampler that exploits closed form gradient\ncomputations to scale our model to large trees. Inference under our\nshrinkage-clock exhibits an over 3-fold speed increase compared to the popular\nrandom local clock when estimating branch-specific clock rates on a simulated\ndataset. We further show our shrinkage-clock recovers known local clocks within\na rodent and mammalian phylogeny. Finally, in a problem that once appeared\ncomputationally impractical, we investigate the heritable clock structure of\nvarious surface glycoproteins of influenza A virus in the absence of prior\nknowledge about clock placement.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 02:43:53 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Fisher", "Alexander A.", ""], ["Ji", "Xiang", ""], ["Nishimura", "Akihiko", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "2105.07216", "submitter": "Matthew Moores", "authors": "Noel Cressie and Matthew T. Moores", "title": "Spatial Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial statistics is an area of study devoted to the statistical analysis of\ndata that have a spatial label associated with them. Geographers often refer to\nthe \"location information\" associated with the \"attribute information,\" whose\nstudy defines a research area called \"spatial analysis.\" Many of the ways to\nmanipulate spatial data are driven by algorithms with no uncertainty\nquantification associated with them. When a spatial analysis is statistical,\nthat is, it incorporates uncertainty quantification, it falls in the research\narea called spatial statistics. The primary feature of spatial statistical\nmodels is that nearby attribute values are more statistically dependent than\ndistant attribute values; this is a paraphrasing of what is sometimes called\nthe First Law of Geography (Tobler, 1970).\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 12:41:41 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Cressie", "Noel", ""], ["Moores", "Matthew T.", ""]]}, {"id": "2105.07285", "submitter": "Jake Shannin", "authors": "Jake Shannin and Babette A. Brumback", "title": "Disagreement Concerning Effect-Measure Modification", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stratifying factors, like age and gender, can modify the effect of treatments\nand exposures on risk of a studied outcome. Several effect measures, including\nthe relative risk, hazard ratio, odds ratio, and risk difference, can be used\nto measure this modification. It is known that choice of effect measure may\ndetermine the presence and direction of effect-measure modification. We show\nthat considering the opposite outcome -- for example, recovery instead of death\n-- may similarly influence effect-measure modification. In fact, if the\nrelative risk for the studied outcome and the relative risk for the opposite\noutcome agree about the direction of effect-measure modification, then so will\nthe two cumulative hazard ratios, the risk difference, and the odds ratio. When\nrisks are randomly sampled from the uniform (0,1) distribution, the probability\nof this happening is 5/6. Disagreement is probable enough that researchers\nconsidering one relative risk should also consider the other and further\ndiscussion if they disagree. (If possible, researchers should also report\nestimated risks.) We provide examples through case studies on HCV, COVID-19,\nand bankruptcy following melanoma treatment.\n", "versions": [{"version": "v1", "created": "Sat, 15 May 2021 19:50:51 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Shannin", "Jake", ""], ["Brumback", "Babette A.", ""]]}, {"id": "2105.07424", "submitter": "Chen Huang", "authors": "Victor Chernozhukov, Chen Huang, Weining Wang", "title": "Learning Financial Network with Focally Sparse Structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the estimation of network connectedness with focally\nsparse structure. We try to uncover the network effect with a flexible sparse\ndeviation from a predetermined adjacency matrix. To be more specific, the\nsparse deviation structure can be regarded as latent or misspecified linkages.\nTo obtain high-quality estimator for parameters of interest, we propose to use\na double regularized high-dimensional generalized method of moments (GMM)\nframework. Moreover, this framework also facilitates us to conduct the\ninference. Theoretical results on consistency and asymptotic normality are\nprovided with accounting for general spatial and temporal dependency of the\nunderlying data generating processes. Simulations demonstrate good performance\nof our proposed procedure. Finally, we apply the methodology to study the\nspatial network effect of stock returns.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 12:52:18 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Huang", "Chen", ""], ["Wang", "Weining", ""]]}, {"id": "2105.07514", "submitter": "Jaromil Frossard", "authors": "Jaromil Frossard and Olivier Renaud", "title": "The Cluster Depth Tests: Toward Point-Wise Strong Control of the\n  Family-Wise Error Rate in Massively Univariate Tests with Application to\n  M/EEG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The cluster mass test has been widely used for massively univariate tests in\nM/EEG, fMRI and, recently, pupillometry analysis. It is a powerful method for\ndetecting effects while controlling weakly the family-wise error rate (FWER),\nalthough its correct interpretation can only be performed at the cluster level\nwithout any point-wise conclusion. It implies that the discoveries of a cluster\nmass test cannot be precisely localized in time or in space. We propose a new\nmultiple comparison procedure, the cluster depth tests, that both controls the\nFWER while allowing an interpretation at the time point level. The simulation\nstudy shows that the cluster depth tests achieve large power and guarantee the\nFWER even in the presence of physiologically plausible effects. By having an\ninterpretation at the time point/voxel level, the cluster depth tests make it\npossible to take full advantage of the high temporal resolution of EEG\nrecording to precisely time the appearance of an effect.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 21:05:54 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Frossard", "Jaromil", ""], ["Renaud", "Olivier", ""]]}, {"id": "2105.07538", "submitter": "Hyeyoung Maeng", "authors": "Hyeyoung Maeng, Idris Eckley and Paul Fearnhead", "title": "Collective anomaly detection in High-dimensional VAR Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is increasing interest in detecting collective anomalies: potentially\nshort periods of time where the features of data change before reverting back\nto normal behaviour. We propose a new method for detecting a collective anomaly\nin VAR models. Our focus is on situations where the change in the VAR\ncoefficient matrix at an anomaly is sparse, i.e. a small number of entries of\nthe VAR coefficient matrix change. To tackle this problem, we propose a test\nstatistic for a local segment that is built on the lasso estimator of the\nchange in model parameters. This enables us to detect a sparse change more\nefficiently and our lasso-based approach becomes especially advantageous when\nthe anomalous interval is short. We show that the new procedure controls Type 1\nerror and has asymptotic power tending to one. The practicality of our approach\nis demonstrated through simulations and two data examples, involving New York\ntaxi trip data and EEG data.\n", "versions": [{"version": "v1", "created": "Sun, 16 May 2021 22:53:52 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Maeng", "Hyeyoung", ""], ["Eckley", "Idris", ""], ["Fearnhead", "Paul", ""]]}, {"id": "2105.07555", "submitter": "Zhanrui Cai", "authors": "Zhanrui Cai, Runze Li, Yaowu Zhang", "title": "A Distribution Free Conditional Independence Test with Applications to\n  Causal Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with test of the conditional independence. We first\nestablish an equivalence between the conditional independence and the mutual\nindependence. Based on the equivalence, we propose an index to measure the\nconditional dependence by quantifying the mutual dependence among the\ntransformed variables. The proposed index has several appealing properties. (a)\nIt is distribution free since the limiting null distribution of the proposed\nindex does not depend on the population distributions of the data. Hence the\ncritical values can be tabulated by simulations. (b) The proposed index ranges\nfrom zero to one, and equals zero if and only if the conditional independence\nholds. Thus, it has nontrivial power under the alternative hypothesis. (c) It\nis robust to outliers and heavy-tailed data since it is invariant to\nconditional strictly monotone transformations. (d) It has low computational\ncost since it incorporates a simple closed-form expression and can be\nimplemented in quadratic time. (e) It is insensitive to tuning parameters\ninvolved in the calculation of the proposed index. (f) The new index is\napplicable for multivariate random vectors as well as for discrete data. All\nthese properties enable us to use the new index as statistical inference tools\nfor various data. The effectiveness of the method is illustrated through\nextensive simulations and a real application on causal discovery.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 00:48:03 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Cai", "Zhanrui", ""], ["Li", "Runze", ""], ["Zhang", "Yaowu", ""]]}, {"id": "2105.07563", "submitter": "Shonosuke Sugasawa", "authors": "Tatsuya Kubokawa, Shonosuke Sugasawa, Hiromasa Tamae and Sanjay\n  Chaudhuri", "title": "General Unbiased Estimating Equations for Variance Components in Linear\n  Mixed Models", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a general framework for estimating variance components\nin the linear mixed models via general unbiased estimating equations, which\ninclude some well-used estimators such as the restricted maximum likelihood\nestimator. We derive the asymptotic covariance matrices and second-order biases\nunder general estimating equations without assuming the normality of the\nunderlying distributions and identify a class of second-order unbiased\nestimators of variance components. It is also shown that the asymptotic\ncovariance matrices and second-order biases do not depend on whether the\nregression coefficients are estimated by the generalized or ordinary least\nsquares methods. We carry out numerical studies to check the performance of the\nproposed method based on typical linear mixed models.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 01:21:28 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Kubokawa", "Tatsuya", ""], ["Sugasawa", "Shonosuke", ""], ["Tamae", "Hiromasa", ""], ["Chaudhuri", "Sanjay", ""]]}, {"id": "2105.07570", "submitter": "Jin Jin", "authors": "Jin Jin, Yue Wang", "title": "A powerful test for differentially expressed gene pathways via\n  graph-informed structural equation modeling", "comments": "26 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A major task in genetic studies is to identify genes related to human\ndiseases and traits to understand functional characteristics of genetic\nmutations and enhance patient diagnosis. Besides marginal analyses of\nindividual genes, identification of gene pathways, i.e., a set of genes with\nknown interactions that collectively contribute to specific biological\nfunctions, can provide more biologically meaningful results. Such gene pathway\nanalysis can be formulated into a high-dimensional two-sample testing problem.\nDue to the typically limited sample size of gene expression datasets, most\nexisting two-sample tests may have compromised powers because they ignore or\nonly inefficiently incorporate the auxiliary pathway information on gene\ninteractions. We propose T2-DAG, a Hotelling's $T^2$-type test for detecting\ndifferentially expressed gene pathways, which efficiently leverages the\nauxiliary pathway information on gene interactions through a linear structural\nequation model. We establish the asymptotic distribution of the test statistic\nunder pertinent assumptions. Simulation studies under various scenarios show\nthat T2-DAG outperforms several representative existing methods with\nwell-controlled type-I error rates and substantially improved powers, even with\nincomplete or inaccurate pathway information or unadjusted confounding effects.\nWe also illustrate the performance of T2-DAG in an application to detect\ndifferentially expressed KEGG pathways between different stages of lung cancer.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 01:40:20 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Jin", "Jin", ""], ["Wang", "Yue", ""]]}, {"id": "2105.07587", "submitter": "Ran Dai", "authors": "Ran Dai, Hyebin Song, Rina Foygel Barber, Garvesh Raskutti", "title": "Convergence guarantee for the sparse monotone single index model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We consider a high-dimensional monotone single index model (hdSIM), which is\na semiparametric extension of a high-dimensional generalize linear model\n(hdGLM), where the link function is unknown, but constrained with monotone and\nnon-decreasing shape. We develop a scalable projection-based iterative\napproach, the \"Sparse Orthogonal Descent Single-Index Model\" (SOD-SIM), which\nalternates between sparse-thresholded orthogonalized \"gradient-like\" steps and\nisotonic regression steps to recover the coefficient vector. Our main\ncontribution is that we provide finite sample estimation bounds for both the\ncoefficient vector and the link function in high-dimensional settings under\nvery mild assumptions on the design matrix $\\mathbf{X}$, the error term\n$\\epsilon$, and their dependence. The convergence rate for the link function\nmatched the low-dimensional isotonic regression minimax rate up to some\npoly-log terms ($n^{-1/3}$). The convergence rate for the coefficients is also\n$n^{-1/3}$ up to some poly-log terms. This method can be applied to many real\ndata problems, including GLMs with misspecified link, classification with\nmislabeled data, and classification with positive-unlabeled (PU) data. We study\nthe performance of this method via both numerical studies and also an\napplication on a rocker protein sequence data.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 03:09:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Dai", "Ran", ""], ["Song", "Hyebin", ""], ["Barber", "Rina Foygel", ""], ["Raskutti", "Garvesh", ""]]}, {"id": "2105.07685", "submitter": "Rik Van Eekelen", "authors": "Rik van Eekelen, Patrick M.M. Bossuyt, Madelon van Wely, Nan van\n  Geloven", "title": "Reducing survivorship bias due to heterogeneity when comparing treated\n  and controls with a different start of follow up", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In comparative effectiveness research, treated and control patients might\nhave a different start of follow up, e.g. when treatment is started later in\nthe disease trajectory. When the follow up period of control patients starts\nearlier in the disease trajectory than follow up of the treated, estimation of\nthe treatment effect suffers from different types of survival/survivorship\nbias. We study unobserved heterogeneity and illustrate how failing to account\nfor the time difference in recruitment between treated and controls leads to\nbias in the estimated treatment effect. We explore five methods to adjust for\nthis survivorship bias by including the time between diagnosis and treatment\ninitiation (wait time) in the analysis in different ways. We first conducted a\nsimulation study on whether these methods reduce survivorship bias, then\napplied our methods to fertility data on insemination. The five methods were:\nfirst, to regress on wait time as an additional covariate in the analysis\nmodel. Second, to match on wait time. Third, to select treated who started\ntreatment immediately. Fourth, to select controls who survived up to the median\ntime of treatment initiation. Fifth, to consider the wait time as the time of\nleft truncation. All methods reduced survivorship bias in the simulation. In\nthe application to fertility data, failing to adjust for survivorship bias led\nto a hazard ratio (HR) of 1.63 (95%CI: 1.13-1.65) whereas a HR of 2.15\n(1.71-2.69) was expected when using a time-varying covariate for treatment,\nwhich in this prospective cohort coincided with the left truncation approach.\nIn agreement with our simulations, the method in which adjustment corrected the\nHR upwards the most was left truncation. We conclude that the wait time between\ndiagnosis and treatment initiation should be taken into account in the analysis\nto respect the chronology of the disease and treatment trajectory.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 09:12:58 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["van Eekelen", "Rik", ""], ["Bossuyt", "Patrick M. M.", ""], ["van Wely", "Madelon", ""], ["van Geloven", "Nan", ""]]}, {"id": "2105.07767", "submitter": "Ting-Kam Leonard Wong", "authors": "Zhixu Tao and Ting-Kam Leonard Wong", "title": "Projections with logarithmic divergences", "comments": "9 pages, 2 figures. To appear in GSI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In information geometry, generalized exponential families and statistical\nmanifolds with curvature are under active investigation in recent years. In\nthis paper we consider the statistical manifold induced by a logarithmic\n$L^{(\\alpha)}$-divergence which generalizes the Bregman divergence. It is known\nthat such a manifold is dually projectively flat with constant negative\nsectional curvature, and is closely related to the\n$\\mathcal{F}^{(\\alpha)}$-family, a generalized exponential family introduced by\nthe second author. Our main result constructs a dual foliation of the\nstatistical manifold, i.e., an orthogonal decomposition consisting of primal\nand dual autoparallel submanifolds. This decomposition, which can be naturally\ninterpreted in terms of primal and dual projections with respect to the\nlogarithmic divergence, extends the dual foliation of a dually flat manifold\nstudied by Amari. As an application, we formulate a new $L^{(\\alpha)}$-PCA\nproblem which generalizes the exponential family PCA.\n", "versions": [{"version": "v1", "created": "Sat, 8 May 2021 21:04:06 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Tao", "Zhixu", ""], ["Wong", "Ting-Kam Leonard", ""]]}, {"id": "2105.07935", "submitter": "Alessandro Casa", "authors": "Alessandro Casa, Andrea Cappozzo, Michael Fop", "title": "Group-wise shrinkage for multiclass Gaussian Graphical Models", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Gaussian Graphical Models are widely employed for modelling dependence among\nvariables. Likewise, finite Gaussian mixtures are often the standard way to go\nfor model-based clustering of continuous features. With the increasing\navailability of high-dimensional datasets, a methodological link between these\ntwo approaches has been established in order to provide a framework for\nperforming penalized model-based clustering in the presence of large precision\nmatrices. Notwithstanding, current methodologies do not account for the fact\nthat groups may possess different degrees of association among the variables,\nthus implicitly assuming similar levels of sparsity across the classes. We\novercome this limitation by deriving group-wise penalty factors, automatically\nenforcing under or over-connectivity in the estimated graphs. The approach is\nentirely data-driven and does not require any additional hyper-parameter\nspecification. Simulated data experiments showcase the validity of our\nproposal.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 15:21:25 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Casa", "Alessandro", ""], ["Cappozzo", "Andrea", ""], ["Fop", "Michael", ""]]}, {"id": "2105.08004", "submitter": "Jonathan Koh", "authors": "Jonathan Koh, Fran\\c{c}ois Pimont, Jean-Luc Dupuy, Thomas Opitz", "title": "Spatiotemporal wildfire modeling through point processes with moderate\n  and extreme marks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Accurate spatiotemporal modeling of conditions leading to moderate and large\nwildfires provides better understanding of mechanisms driving fire-prone\necosystems and improves risk management. We here develop a joint model for the\noccurrence intensity and the wildfire size distribution by combining\nextreme-value theory and point processes within a novel Bayesian hierarchical\nmodel, and use it to study daily summer wildfire data for the French\nMediterranean basin during 1995--2018. The occurrence component models wildfire\nignitions as a spatiotemporal log-Gaussian Cox process. Burnt areas are\nnumerical marks attached to points and are considered as extreme if they exceed\na high threshold. The size component is a two-component mixture varying in\nspace and time that jointly models moderate and extreme fires. We capture\nnon-linear influence of covariates (Fire Weather Index, forest cover) through\ncomponent-specific smooth functions, which may vary with season. We propose\nestimating shared random effects between model components to reveal and\ninterpret common drivers of different aspects of wildfire activity. This leads\nto increased parsimony and reduced estimation uncertainty with better\npredictions. Specific stratified subsampling of zero counts is implemented to\ncope with large observation vectors. We compare and validate models through\npredictive scores and visual diagnostics. Our methodology provides a holistic\napproach to explaining and predicting the drivers of wildfire activity and\nassociated uncertainties.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 16:39:36 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 14:53:31 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Koh", "Jonathan", ""], ["Pimont", "Fran\u00e7ois", ""], ["Dupuy", "Jean-Luc", ""], ["Opitz", "Thomas", ""]]}, {"id": "2105.08186", "submitter": "Jorge Castillo-Mateo", "authors": "Jorge Castillo-Mateo (University of Zaragoza)", "title": "Distribution-Free Changepoint Detection Tests Based on the Breaking of\n  Records", "comments": "30 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of record-breaking events is of interest in fields such as\nclimatology, hydrology, economy or sports. In connection with the record\noccurrence, we propose three distribution-free statistics for the changepoint\ndetection problem. They are CUSUM-type statistics based on the upper and/or\nlower record indicators which occur in a series. Using a version of the\nfunctional central limit theorem, we show that the CUSUM-type statistics are\nasymptotically Kolmogorov distributed. The main results under the null\nhypothesis are based on series of independent and identically distributed\nrandom variables, but a statistic to deal with series with seasonal component\nand serial correlation is also proposed. A Monte Carlo study of size, power and\nchangepoint estimate has been performed. Finally, the methods are illustrated\nby analyzing the time series of temperatures at Madrid, Spain. The $\\textsf{R}$\npackage $\\texttt{RecordTest}$ publicly available on CRAN implements the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 22:47:19 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Castillo-Mateo", "Jorge", "", "University of Zaragoza"]]}, {"id": "2105.08230", "submitter": "Runxiong Wu", "authors": "Runxiong Wu, Chang Deng and Xin Chen", "title": "High-Dimensional Sparse Single-Index Regression Via Hilbert-Schmidt\n  Independence Criterion", "comments": "24 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hilbert-Schmidt Independence Criterion (HSIC) has recently been used in the\nfield of single-index models to estimate the directions. Compared with some\nother well-established methods, it requires relatively weaker conditions.\nHowever, its performance has not yet been studied in the high-dimensional\nscenario, where the number of covariates is much larger than the sample size.\nIn this article, we propose a new efficient sparse estimate in HSIC based\nsingle-index model. This new method estimates the subspace spanned by the\nlinear combinations of the covariates directly and performs variable selection\nsimultaneously. Due to the non-convexity of the objective function, we use a\nmajorize-minimize approach together with the linearized alternating direction\nmethod of multipliers algorithm to solve the optimization problem. The\nalgorithm does not involve the inverse of the covariance matrix and therefore\ncan handle the large p small n scenario naturally. Through extensive simulation\nstudies and a real data analysis, we show our proposal is efficient and\neffective in the high-dimensional setting. The Matlab codes for this method are\navailable online.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 02:14:09 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Wu", "Runxiong", ""], ["Deng", "Chang", ""], ["Chen", "Xin", ""]]}, {"id": "2105.08379", "submitter": "Rapha\\\"el Jauslin", "authors": "Rapha\\\"el Jauslin and Yves Till\\'e", "title": "An Efficient Approach for Statistical Matching of Survey Data Trough\n  Calibration, Optimal Transport and Balanced Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Statistical matching aims to integrate two statistical sources. These sources\ncan be two samples or a sample and the entire population. If two samples have\nbeen selected from the same population and information has been collected on\ndifferent variables of interest, then it is interesting to match the two\nsurveys to analyse, for example, contingency tables or correlations. In this\npaper, we propose an efficient method for matching two samples that may each\ncontain a weighting scheme. The method matches the records of the two sources.\nSeveral variants are proposed in order to create a directly usable file\nintegrating data from both information sources.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 09:09:44 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Jauslin", "Rapha\u00ebl", ""], ["Till\u00e9", "Yves", ""]]}, {"id": "2105.08451", "submitter": "Sourabh Bhattacharya", "authors": "Sourabh Bhattacharya", "title": "Bayesian Levy-Dynamic Spatio-Temporal Process: Towards Big Data Analysis", "comments": "Feedback welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of big data, all scientific disciplines are evolving fast to cope\nup with the enormity of the available information. So is statistics, the queen\nof science. Big data are particularly relevant to spatio-temporal statistics,\nthanks to much-improved technology in satellite based remote sensing and\nGeographical Information Systems. However, none of the existing approaches seem\nto meet the simultaneous demand of reality emulation and cheap computation. In\nthis article, with the Levy random fields as the starting point, e construct a\nnew Bayesian nonparametric, nonstationary and nonseparable dynamic spatio-\ntemporal model with the additional realistic property that the lagged\nspatio-temporal correlations converge to zero as the lag tends to infinity.\nAlthough our Bayesian model seems to be intricately structured and is\nvariable-dimensional with respect to each time index, we are able to devise a\nfast and efficient parallel Markov Chain Monte Carlo (MCMC) algorithm for\nBayesian inference. Our simulation experiment brings out quite encouraging\nperformance from our Bayesian Levy-dynamic approach. We finally apply our\nBayesian Levy-dynamic model and methods to a sea surface temperature dataset\nconsisting of 139,300 data points in space and time. Although not big data in\nthe true sense, this is a large and highly structured data by any standard.\nEven for this large and complex data, our parallel MCMC algorithm, implemented\non 80 processors, generated 110,000 MCMC realizations from the Levy-dynamic\nposterior within a single day, and the resultant Bayesian posterior predictive\nanalysis turned out to be encouraging. Thus, it is not unreasonable to expect\nthat with significantly more computing resources, it is feasible to analyse\nterabytes of spatio-temporal data with our new model and methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 11:45:07 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Bhattacharya", "Sourabh", ""]]}, {"id": "2105.08530", "submitter": "Jan Decuyper", "authors": "Jan Decuyper, David Westwick, Kiana Karami, Johan Schoukens", "title": "Decoupling P-NARX models using filtered CPD", "comments": "Accepted for presentation at the 19th IFAC Symposium on System\n  Identification (SYSID 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Nonlinear Auto-Regressive eXogenous input (NARX) models are a popular class\nof nonlinear dynamical models. Often a polynomial basis expansion is used to\ndescribe the internal multivariate nonlinear mapping (P-NARX). Resorting to\nfixed basis functions is convenient since it results in a closed form solution\nof the estimation problem. The drawback, however, is that the predefined basis\ndoes not necessarily lead to a sparse representation of the relationship,\ntypically resulting in very large numbers of parameters. So-called decoupling\ntechniques were specifically designed to reduce large multivariate functions.\nIt was found that, often, a more efficient parameterisation can be retrieved by\nrotating towards a new basis. Characteristic to the decoupled structure is\nthat, expressed in the new basis, the relationship is structured such that only\nsingle-input single-output nonlinear functions are required. Classical\ndecoupling techniques are unfit to deal with the case of single-output NARX\nmodels. In this work, this limitation is overcome by adopting the filtered CPD\ndecoupling method of Decuyper et al. (2021b). The approach is illustrated on\ndata from the Sliverbox benchmark: measurement data from an electronic circuit\nimplementation of a forced Duffing oscillator.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 13:54:44 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Decuyper", "Jan", ""], ["Westwick", "David", ""], ["Karami", "Kiana", ""], ["Schoukens", "Johan", ""]]}, {"id": "2105.08604", "submitter": "Darshan Bryner", "authors": "Darshan Bryner and Anuj Srivastava", "title": "Shape Analysis of Functional Data with Elastic Partial Matching", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Elastic Riemannian metrics have been used successfully in the past for\nstatistical treatments of functional and curve shape data. However, this usage\nhas suffered from an important restriction: the function boundaries are assumed\nfixed and matched. Functional data exhibiting unmatched boundaries typically\narise from dynamical systems with variable evolution rates such as COVID-19\ninfection rate curves associated with different geographical regions. In this\ncase, it is more natural to model such data with sliding boundaries and use\npartial matching, i.e., only a part of a function is matched to another\nfunction. Here, we develop a comprehensive Riemannian framework that allows for\npartial matching, comparing, and clustering of functions under both phase\nvariability and uncertain boundaries. We extend past work by: (1) Forming a\njoint action of the time-warping and time-scaling groups; (2) Introducing a\nmetric that is invariant to this joint action, allowing for a gradient-based\napproach to elastic partial matching; and (3) Presenting a modification that,\nwhile losing the metric property, allows one to control relative influence of\nthe two groups. This framework is illustrated for registering and clustering\nshapes of COVID-19 rate curves, identifying essential patterns, minimizing\nmismatch errors, and reducing variability within clusters compared to previous\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:36:51 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Bryner", "Darshan", ""], ["Srivastava", "Anuj", ""]]}, {"id": "2105.08677", "submitter": "Pengfei Li", "authors": "Pengfei Li, Tao Yu, Baojiang Chen, and Jing Qin", "title": "Maximum profile binomial likelihood estimation for the semiparametric\n  Box--Cox power transformation model", "comments": "70 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Box--Cox transformation model has been widely applied for many years. The\nparametric version of this model assumes that the random error follows a\nparametric distribution, say the normal distribution, and estimates the model\nparameters using the maximum likelihood method. The semiparametric version\nassumes that the distribution of the random error is completely unknown;\nexisting methods either need strong assumptions, or are less effective when the\ndistribution of the random error significantly deviates from the normal\ndistribution. We adopt the semiparametric assumption and propose a maximum\nprofile binomial likelihood method. We theoretically establish the joint\ndistribution of the estimators of the model parameters. Through extensive\nnumerical studies, we demonstrate that our method has an advantage over\nexisting methods, especially when the distribution of the random error deviates\nfrom the normal distribution. Furthermore, we compare the performance of our\nmethod and existing methods on an HIV data set.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:07:02 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 03:57:51 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Li", "Pengfei", ""], ["Yu", "Tao", ""], ["Chen", "Baojiang", ""], ["Qin", "Jing", ""]]}, {"id": "2105.08679", "submitter": "Prajamitra Bhuyan Dr.", "authors": "Kiranmoy Chatterjee and Prajamitra Bhuyan", "title": "Estimation of Population Size with Heterogeneous Catchability and\n  Behavioural Dependence: Applications to Air and Water Borne Disease\n  Surveillance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population size estimation based on the capture-recapture experiment is an\ninteresting problem in various fields including epidemiology, criminology,\ndemography, etc. In many real-life scenarios, there exists inherent\nheterogeneity among the individuals and dependency between capture and\nrecapture attempts. A novel trivariate Bernoulli model is considered to\nincorporate these features, and the Bayesian estimation of the model parameters\nis suggested using data augmentation. Simulation results show robustness under\nmodel misspecification and the superiority of the performance of the proposed\nmethod over existing competitors. The method is applied to analyse real case\nstudies on epidemiological surveillance. The results provide interesting\ninsight on the heterogeneity and dependence involved in the capture-recapture\nmechanism. The methodology proposed can assist in effective decision-making and\npolicy formulation.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:09:51 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 16:27:08 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 20:06:03 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chatterjee", "Kiranmoy", ""], ["Bhuyan", "Prajamitra", ""]]}, {"id": "2105.08686", "submitter": "Mahsa Nadifar", "authors": "Mahsa Nadifar (1 and 2), Hossein Baghishani (1), Thomas Kneib (2) and\n  Afshin Fallah (3) ((1) Shahrood University of Technology, (2) Georg August\n  University, (3) Internatinal Imam Khomeini University)", "title": "Flexible Bayesian Modeling of Counts: Constructing Penalized Complexity\n  Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the data, particularly in medicine and disease mapping are count.\nIndeed, the under or overdispersion problem in count data distrusts the\nperformance of the classical Poisson model. For taking into account this\nproblem, in this paper, we introduce a new Bayesian structured additive\nregression model, called gamma count, with enough flexibility in modeling\ndispersion. Setting convenient prior distributions on the model parameters is a\nmomentous issue in Bayesian statistics that characterize the nature of our\nuncertainty parameters. Relying on a recently proposed class of penalized\ncomplexity priors, motivated from a general set of construction principles, we\nderive the prior structure. The model can be formulated as a latent Gaussian\nmodel, and consequently, we can carry out the fast computation by using the\nintegrated nested Laplace approximation method. We investigate the proposed\nmethodology simulation study. Different expropriate prior distribution are\nexamined to provide reasonable sensitivity analysis. To explain the\napplicability of the proposed model, we analyzed two real-world data sets\nrelated to the larynx mortality cancer in Germany and the handball champions\nleague.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 17:16:23 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Nadifar", "Mahsa", "", "1 and 2"], ["Baghishani", "Hossein", ""], ["Kneib", "Thomas", ""], ["Fallah", "Afshin", ""]]}, {"id": "2105.08747", "submitter": "Matteo Sesia", "authors": "Matteo Sesia, Yaniv Romano", "title": "Conformal histogram regression", "comments": "11 pages, 4 figures. Supplement: 13 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a conformal method to compute prediction intervals for\nnon-parametric regression that can automatically adapt to skewed data.\nLeveraging black-box machine learning algorithms to estimate the conditional\ndistribution of the outcome using histograms, it translates their output into\nthe shortest prediction intervals with approximate conditional coverage. The\nresulting prediction intervals provably have marginal coverage in finite\nsamples, while asymptotically achieving conditional coverage and optimal length\nif the black-box model is consistent. Numerical experiments with simulated and\nreal data demonstrate improved performance compared to state-of-the-art\nalternatives, including conformalized quantile regression and other\ndistributional conformal prediction approaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:05:02 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Sesia", "Matteo", ""], ["Romano", "Yaniv", ""]]}, {"id": "2105.08776", "submitter": "Kyu Ha Lee", "authors": "Sebastien Haneuse, Deborah Schrag, Francesca Dominici, Sharon-Lise\n  Normand, and Kyu Ha Lee", "title": "Measuring performance for end-of-life care", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although not without controversy, readmission is entrenched as a hospital\nquality metric, with statistical analyses generally based on fitting a\nlogistic-Normal generalized linear mixed model. Such analyses, however, ignore\ndeath as a competing risk, although doing so for clinical conditions with high\nmortality can have profound effects; a hospitals seemingly good performance for\nreadmission may be an artifact of it having poor performance for mortality. In\nthis paper we propose novel multivariate hospital-level performance measures\nfor readmission and mortality, that derive from framing the analysis as one of\ncluster-correlated semi-competing risks data. We also consider a number of\nprofiling-related goals, including the identification of extreme performers and\na bivariate classification of whether the hospital has\nhigher-/lower-than-expected readmission and mortality rates, via a Bayesian\ndecision-theoretic approach that characterizes hospitals on the basis of\nminimizing the posterior expected loss for an appropriate loss function. In\nsome settings, particularly if the number of hospitals is large, the\ncomputational burden may be prohibitive. To resolve this, we propose a series\nof analysis strategies that will be useful in practice. Throughout the methods\nare illustrated with data from CMS on N=17,685 patients diagnosed with\npancreatic cancer between 2000-2012 at one of J=264 hospitals in California.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 18:49:06 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Haneuse", "Sebastien", ""], ["Schrag", "Deborah", ""], ["Dominici", "Francesca", ""], ["Normand", "Sharon-Lise", ""], ["Lee", "Kyu Ha", ""]]}, {"id": "2105.08836", "submitter": "David Robertson", "authors": "David S. Robertson, Babak Choodari-Oskooei, Munya Dimairo, Laura\n  Flight, Philip Pallmann, Thomas Jaki", "title": "Point estimation for adaptive trial designs", "comments": "Fix references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent FDA guidance on adaptive clinical trial designs defines bias as \"a\nsystematic tendency for the estimate of treatment effect to deviate from its\ntrue value\", and states that it is desirable to obtain and report estimates of\ntreatment effects that reduce or remove this bias. In many adaptive designs,\nthe conventional end-of-trial point estimates of the treatment effects are\nprone to bias, because they do not take into account the potential and realised\ntrial adaptations. While much of the methodological developments on adaptive\ndesigns have tended to focus on control of type I error rates and power\nconsiderations, in contrast the question of biased estimation has received less\nattention. This article addresses this issue by providing a comprehensive\noverview of proposed approaches to remove or reduce the potential bias in point\nestimation of treatment effects in an adaptive design, as well as illustrating\nhow to implement them. We first discuss how bias can affect standard estimators\nand critically assess the negative impact this can have. We then describe and\ncompare proposed unbiased and bias-adjusted estimators of treatment effects for\ndifferent types of adaptive designs. Furthermore, we illustrate the computation\nof different estimators in practice using a real trial example. Finally, we\npropose a set of guidelines for researchers around the choice of estimators and\nthe reporting of estimates following an adaptive design.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 21:26:11 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 13:58:54 GMT"}, {"version": "v3", "created": "Sat, 22 May 2021 12:32:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Robertson", "David S.", ""], ["Choodari-Oskooei", "Babak", ""], ["Dimairo", "Munya", ""], ["Flight", "Laura", ""], ["Pallmann", "Philip", ""], ["Jaki", "Thomas", ""]]}, {"id": "2105.08868", "submitter": "Dan Scharfstein", "authors": "Daniel O. Scharfstein, Jaron J. R. Lee, Aidan McDermott, Aimee\n  Campbell, Edward Nunes, Abigail G. Matthews, and Ilya Shpitser", "title": "Markov-Restricted Analysis of Randomized Trials with Non-Monotone\n  Missing Binary Outcomes: Sensitivity Analysis and Identification Results", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scharfstein et al. (2021) developed a sensitivity analysis model for\nanalyzing randomized trials with repeatedly measured binary outcomes that are\nsubject to nonmonotone missingness. Their approach becomes computationally\nintractable when the number of repeated measured is large (e.g., greater than\n15). In this paper, we repair this problem by introducing an $m$th-order\nMarkovian restriction. We establish an identification by representing the model\nas a directed acyclic graph (DAG). We illustrate our methodology in the context\nof a randomized trial designed to evaluate a web-delivered psychosocial\nintervention to reduce substance use, assessed by testing urine samples twice\nweekly for 12 weeks, among patients entering outpatient addiction treatment. We\nevaluate the finite sample properties of our method in a realistic simulation\nstudy. Our methods have been integrated into the R package entitled slabm.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 01:00:54 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Scharfstein", "Daniel O.", ""], ["Lee", "Jaron J. R.", ""], ["McDermott", "Aidan", ""], ["Campbell", "Aimee", ""], ["Nunes", "Edward", ""], ["Matthews", "Abigail G.", ""], ["Shpitser", "Ilya", ""]]}, {"id": "2105.08893", "submitter": "Zishen Xu", "authors": "Zishen Xu, Chenran Wang, Wei Wu", "title": "A unified framework on defining depth for point process using function\n  smoothing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The notion of statistical depth has been extensively studied in multivariate\nand functional data over the past few decades. In contrast, the depth on\ntemporal point process is still under-explored. The problem is challenging\nbecause a point process has two types of randomness: 1) the number of events in\na process, and 2) the distribution of these events. Recent studies proposed\ndepths in a weighted product of two terms, describing the above two types of\nrandomness, respectively. In this paper, we propose to unify these two\nrandomnesses under one framework by a smoothing procedure. Basically, we\ntransform the point process observations into functions using conventional\nkernel smoothing methods, and then adopt the well-known functional $h$-depth\nand its modified, center-based, version to describe the center-outward rank in\nthe original data. To do so, we define a proper metric on the point processes\nwith smoothed functions. We then propose an efficient algorithm to estimated\nthe defined \"center\". We further explore the mathematical properties of the\nnewly defined depths and study asymptotics. Simulation results show that the\nproposed depths can properly rank the point process observations. Finally, we\ndemonstrate the new method in a classification task using a real neuronal spike\ntrain dataset.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 02:42:31 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 19:43:41 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Xu", "Zishen", ""], ["Wang", "Chenran", ""], ["Wu", "Wei", ""]]}, {"id": "2105.08966", "submitter": "Fabio Sigrist", "authors": "Fabio Sigrist", "title": "Latent Gaussian Model Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Gaussian models and boosting are widely used techniques in statistics\nand machine learning. Tree-boosting shows excellent predictive accuracy on many\ndata sets, but potential drawbacks are that it assumes conditional independence\nof samples, produces discontinuous predictions for, e.g., spatial data, and it\ncan have difficulty with high-cardinality categorical variables. Latent\nGaussian models, such as Gaussian process and grouped random effects models,\nare flexible prior models that allow for making probabilistic predictions.\nHowever, existing latent Gaussian models usually assume either a zero or a\nlinear prior mean function which can be an unrealistic assumption. This article\nintroduces a novel approach that combines boosting and latent Gaussian models\nin order to remedy the above-mentioned drawbacks and to leverage the advantages\nof both techniques. We obtain increased predictive accuracy compared to\nexisting approaches in both simulated and real-world data experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 07:36:30 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 13:42:12 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Sigrist", "Fabio", ""]]}, {"id": "2105.08976", "submitter": "Shubhadeep Chakraborty", "authors": "Shubhadeep Chakraborty and Xianyang Zhang", "title": "High-dimensional Change-point Detection Using Generalized Homogeneity\n  Metrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Change-point detection has been a classical problem in statistics and\neconometrics. This work focuses on the problem of detecting abrupt\ndistributional changes in the data-generating distribution of a sequence of\nhigh-dimensional observations, beyond the first two moments. This has remained\na substantially less explored problem in the existing literature, especially in\nthe high-dimensional context, compared to detecting changes in the mean or the\ncovariance structure. We develop a nonparametric methodology to (i) detect an\nunknown number of change-points in an independent sequence of high-dimensional\nobservations and (ii) test for the significance of the estimated change-point\nlocations. Our approach essentially rests upon nonparametric tests for the\nhomogeneity of two high-dimensional distributions. We construct a single\nchange-point location estimator via defining a cumulative sum process in an\nembedded Hilbert space. As the key theoretical innovation, we rigorously derive\nits limiting distribution under the high dimension medium sample size (HDMSS)\nframework. Subsequently we combine our statistic with the idea of wild binary\nsegmentation to recursively estimate and test for multiple change-point\nlocations. The superior performance of our methodology compared to other\nexisting procedures is illustrated via extensive simulation studies as well as\nover stock prices data observed during the period of the Great Recession in the\nUnited States.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 08:13:51 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Chakraborty", "Shubhadeep", ""], ["Zhang", "Xianyang", ""]]}, {"id": "2105.09003", "submitter": "Tim Kutzker", "authors": "Tim Kutzker, Nadja Klein, Dominik Wied", "title": "Flexible Specification Testing in Semi-Parametric Quantile Regression\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose several novel consistent specification tests for quantile\nregression models which generalize former tests by important characteristics.\nFirst, we allow the covariate effects to be quantile-dependent and nonlinear.\nSecond, we allow for parameterizing the conditional quantile functions by\nappropriate basis functions, rather than parametrically.We are hence able to\ntest for functional forms beyond linearity, while retaining the linear effects\nas special cases. In both cases, the induced class of conditional distribution\nfunctions is tested with a Cram\\'{e}r-von Mises type test statistic for which\nwe derive the theoretical limit distribution and propose a practical bootstrap\nmethod. To increase the power of the first test, we further suggest a modified\ntest statistic using the B-spline approach from the second test. A detailed\nMonte Carlo experiment shows that the test results in a reasonable sized\ntesting procedure with large power. Our first application to conditional income\ndisparities between East and West Germany over the period 2001-2010 indicates\nthat there are not only still significant differences between East and West but\nalso across the quantiles of the conditional income distributions, when\nconditioning on age and year. The second application to data from the\nAustralian national electricity market reveals the importance of using\ninteraction effects for modelling the highly skewed and heavy-tailed\ndistributions of energy prices conditional one day, time of day and demand.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:13:51 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Kutzker", "Tim", ""], ["Klein", "Nadja", ""], ["Wied", "Dominik", ""]]}, {"id": "2105.09019", "submitter": "Jaco Visagie", "authors": "E Bothma and JS Allison and IJH Visagie", "title": "New classes of tests for the Weibull distribution using Stein's method\n  in the presence of random right censoring", "comments": "14 pages. arXiv admin note: text overlap with arXiv:2011.04519", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop two new classes of tests for the Weibull distribution based on\nStein's method. The proposed tests are applied in the full sample case as well\nas in the framework of random right censoring. We investigate the finite sample\nperformance of the new tests using a comprehensive Monte Carlo study. In both\nthe absence and presence of censoring, it is found that the newly proposed\nclasses of tests outperform competing tests against the majority of the\ndistributions considered. In the cases where censoring is present we consider\nvarious censoring distributions. Some remarks on the asymptotic properties of\nthe proposed tests are included. The paper presents another result of\nindependent interest; the test initially proposed in Krit (2014) for use with\nfull samples is amended to allow for testing for the Weibull distribution in\nthe presence of censoring. The techniques developed in the paper are\nillustrated using two practical examples. In the first, we consider the\nsurvival times of patients with a certain type of leukemia. The second example\nis concerned with the initial remission times of leukemia patients, where the\nobserved remission times are subject to random right censoring. We further\ninclude some concluding remarks along with avenues for future research.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:34:47 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Bothma", "E", ""], ["Allison", "JS", ""], ["Visagie", "IJH", ""]]}, {"id": "2105.09031", "submitter": "Thorsten Dickhaus", "authors": "Jost Viebrock and Thorsten Dickhaus", "title": "Standard Curves for Empirical Likelihood Ratio Tests of Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present simulated standard curves for the calibration of empirical\nlikelihood ratio (ELR) tests of means. With the help of these curves, the\nnominal significance level of the ELR test can be adjusted in order to achieve\n(quasi-) exact type I error rate control for a given, finite sample size. By\ntheoretical considerations and by computer simulations, we demonstrate that the\nadjusted significance level depends most crucially on the skewness and on the\nkurtosis of the parent distribution. For practical purposes, we tabulate\nadjusted critical values under several prototypical statistical models.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 09:55:44 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Viebrock", "Jost", ""], ["Dickhaus", "Thorsten", ""]]}, {"id": "2105.09177", "submitter": "Junhui Zhang", "authors": "Henry Lam, Junhui Zhang", "title": "Distributionally Constrained Black-Box Stochastic Gradient Estimation\n  and Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider stochastic gradient estimation using only black-box function\nevaluations, where the function argument lies within a probability simplex.\nThis problem is motivated from gradient-descent optimization procedures in\nmultiple applications in distributionally robust analysis and inverse model\ncalibration involving decision variables that are probability distributions. We\nare especially interested in obtaining gradient estimators where one or few\nsample observations or simulation runs apply simultaneously to all directions.\nConventional zeroth-order gradient schemes such as simultaneous perturbation\nface challenges as the required moment conditions that allow the \"canceling\" of\nhigher-order biases cannot be satisfied without violating the simplex\nconstraints. We investigate a new set of required conditions on the random\nperturbation generator, which leads us to a class of implementable gradient\nestimators using Dirichlet mixtures. We study the statistical properties of\nthese estimators and their utility in constrained stochastic approximation,\nincluding both Frank-Wolfe and mirror descent update schemes. We demonstrate\nthe effectiveness of our procedures and compare with benchmarks via several\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 14:50:25 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Lam", "Henry", ""], ["Zhang", "Junhui", ""]]}, {"id": "2105.09223", "submitter": "Jakob Richter", "authors": "Jakob Richter, Tim Friede, J\\\"org Rahnenf\\\"uhrer", "title": "Improving Adaptive Seamless Designs through Bayesian optimization", "comments": "Submitted to: Biometrical Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose to use Bayesian optimization (BO) to improve the efficiency of the\ndesign selection process in clinical trials. BO is a method to optimize\nexpensive black-box functions, by using a regression as a surrogate to guide\nthe search. In clinical trials, planning test procedures and sample sizes is a\ncrucial task. A common goal is to maximize the test power, given a set of\ntreatments, corresponding effect sizes, and a total number of samples. From a\nwide range of possible designs we aim to select the best one in a short time to\nallow quick decisions. The standard approach to simulate the power for each\nsingle design can become too time-consuming. When the number of possible\ndesigns becomes very large, either large computational resources are required\nor an exhaustive exploration of all possible designs takes too long. Here, we\npropose to use BO to quickly find a clinical trial design with high power from\na large number of candidate designs. We demonstrate the effectiveness of our\napproach by optimizing the power of adaptive seamless designs for different\nsets of treatment effect sizes. Comparing BO with an exhaustive evaluation of\nall candidate designs shows that BO finds competitive designs in a fraction of\nthe time.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 16:07:02 GMT"}], "update_date": "2021-05-20", "authors_parsed": [["Richter", "Jakob", ""], ["Friede", "Tim", ""], ["Rahnenf\u00fchrer", "J\u00f6rg", ""]]}, {"id": "2105.09429", "submitter": "Simon Godsill Prof.", "authors": "Simon Godsill and Yaman K{\\i}ndap", "title": "Point process simulation of Generalised inverse Gaussian processes and\n  estimation of the Jaeger Integral", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP math.PR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper novel simulation methods are provided for the Generalised\ninverse Gaussian (GIG) L\\'{e}vy process. Such processes are intractable for\nsimulation except in certain special edge cases, since the L\\'{e}vy density\nassociated with the GIG process is expressed as an integral involving certain\nBessel Functions, known as the Jaeger Integral in diffusive transport\napplications. We here show for the first time how to solve the problem\nindirectly, using generalised shot-noise methods to simulate the underlying\npoint processes and constructing an auxiliary variables approach that avoids\nany direct calculation of the integrals involved. The augmented bivariate\nprocess is still intractable and so we propose a novel thinning method based on\nupper bounds on the intractable integrand. Moreover our approach leads to lower\nand upper bounds on the Jaeger integral itself, which may be compared with\nother approximation methods. We note that the GIG process is the required\nBrownian motion subordinator for the generalised hyperbolic (GH) L\\'{e}vy\nprocess and so our simulation approach will straightforwardly extend also to\nthe simulation of these intractable proceses. Our new methods will find\napplication in forward simulation of processes of GIG and GH type, in financial\nand engineering data, for example, as well as inference for states and\nparameters of stochastic processes driven by GIG and GH L\\'{e}vy processes.\n", "versions": [{"version": "v1", "created": "Wed, 19 May 2021 22:48:55 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Godsill", "Simon", ""], ["K\u0131ndap", "Yaman", ""]]}, {"id": "2105.09473", "submitter": "Diakarya Barro Pr", "authors": "Dodo Natatou Moutari, Hassane Abba Mallam, Diakarya Barro, Bisso Saley", "title": "Dependence Modeling and Risk Assessment of a Financial Portfolio with\n  ARMA-APARCH-EVT models based on HACs", "comments": "16 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This study aims to widen the sphere of pratical applicability of the HAC\nmodel combined with the ARMA-APARCH volatility forecast model and the extreme\nvalues theory. A sequential process of modeling of the VaR of a portfolio based\non the ARMA-APARCH-EVT-HAC model was discussed. The empirical analysis\nconducted with data from international stock market indices clearly illustrates\nthe performance and accuracy of modeling based on HACs.\n", "versions": [{"version": "v1", "created": "Mon, 17 May 2021 11:30:19 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Moutari", "Dodo Natatou", ""], ["Mallam", "Hassane Abba", ""], ["Barro", "Diakarya", ""], ["Saley", "Bisso", ""]]}, {"id": "2105.09695", "submitter": "Zheng Zhao", "authors": "Zheng Zhao, Rui Gao, Simo S\\\"arkk\\\"a", "title": "Hierarchical Non-Stationary Temporal Gaussian Processes With\n  $L^1$-Regularization", "comments": "20 pages. Submitted to Statistics and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with regularized extensions of hierarchical\nnon-stationary temporal Gaussian processes (NSGPs) in which the parameters\n(e.g., length-scale) are modeled as GPs. In particular, we consider two\ncommonly used NSGP constructions which are based on explicitly constructed\nnon-stationary covariance functions and stochastic differential equations,\nrespectively. We extend these NSGPs by including $L^1$-regularization on the\nprocesses in order to induce sparseness. To solve the resulting regularized\nNSGP (R-NSGP) regression problem we develop a method based on the alternating\ndirection method of multipliers (ADMM) and we also analyze its convergence\nproperties theoretically. We also evaluate the performance of the proposed\nmethods in simulated and real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 12:15:33 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zhao", "Zheng", ""], ["Gao", "Rui", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2105.09712", "submitter": "Ingeborg Gullikstad Hem", "authors": "Ingeborg Gullikstad Hem, Geir-Arne Fuglstad and Andrea Riebler", "title": "makemyprior: Intuitive Construction of Joint Priors for Variance\n  Parameters in R", "comments": "40 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Priors allow us to robustify inference and to incorporate expert knowledge in\nBayesian hierarchical models. This is particularly important when there are\nrandom effects that are hard to identify based on observed data. The challenge\nlies in understanding and controlling the joint influence of the priors for the\nvariance parameters, and makemyprior is an R package that guides the\nformulation of joint prior distributions for variance parameters. A joint prior\ndistribution is constructed based on a hierarchical decomposition of the total\nvariance in the model along a tree, and takes the entire model structure into\naccount. Users input their prior beliefs or express ignorance at each level of\nthe tree. Prior beliefs can be general ideas about reasonable ranges of\nvariance values and need not be detailed expert knowledge. The constructed\npriors lead to robust inference and guarantee proper posteriors. A graphical\nuser interface facilitates construction and assessment of different choices of\npriors through visualization of the tree and joint prior. The package aims to\nexpand the toolbox of applied researchers and make priors an active component\nin their Bayesian workflow.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 12:52:17 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Hem", "Ingeborg Gullikstad", ""], ["Fuglstad", "Geir-Arne", ""], ["Riebler", "Andrea", ""]]}, {"id": "2105.09748", "submitter": "Ehsan Zamanzade", "authors": "Elham Zamanzade, Majid Asadi, Afshin Parvardeh, and Ehsan Zamanzade", "title": "A ranked-based estimator of the mean past lifetime with its application", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The mean past lifetime (MPL) is an important tool in reliability and survival\nanalysis for measuring the average time elapsed since the occurrence of an\nevent, under the condition that the event has occurred before a specific time\n$t>0$. This article develops a nonparametric estimator for MPL based on\nobservations collected according to ranked set sampling (RSS) design. It is\nshown that the estimator that we have developed is a strongly uniform\nconsistent. It is also proved that the introduced estimator tends to a Gaussian\nprocess under some mild conditions. A Monte Carlo simulation study is employed\nto evaluate the performance of the proposed estimator with its competitor in\nsimple random sampling (SRS). Our findings show the introduced estimator is\nmore efficient than its counterpart estimator in SRS as long as the quality of\nranking is better than random. Finally, an illustrative example is provided to\ndescribe the potential application of the developed estimator in assessing the\naverage time between the infection and diagnosis in HIV patients.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 13:47:51 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zamanzade", "Elham", ""], ["Asadi", "Majid", ""], ["Parvardeh", "Afshin", ""], ["Zamanzade", "Ehsan", ""]]}, {"id": "2105.09788", "submitter": "Ruiqi Liu", "authors": "Ruiqi Liu, Ganggang Xu, Zuofeng Shang", "title": "Distributed Adaptive Nearest Neighbor Classifier: Algorithm and Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data is of an extraordinarily large size or physically stored in\ndifferent locations, the distributed nearest neighbor (NN) classifier is an\nattractive tool for classification. We propose a novel distributed adaptive NN\nclassifier for which the number of nearest neighbors is a tuning parameter\nstochastically chosen by a data-driven criterion. An early stopping rule is\nproposed when searching for the optimal tuning parameter, which not only speeds\nup the computation but also improves the finite sample performance of the\nproposed Algorithm. Convergence rate of excess risk of the distributed adaptive\nNN classifier is investigated under various sub-sample size compositions. In\nparticular, we show that when the sub-sample sizes are sufficiently large, the\nproposed classifier achieves the nearly optimal convergence rate. Effectiveness\nof the proposed approach is demonstrated through simulation studies as well as\nan empirical application to a real-world dataset.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 14:38:41 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Liu", "Ruiqi", ""], ["Xu", "Ganggang", ""], ["Shang", "Zuofeng", ""]]}, {"id": "2105.09798", "submitter": "Martin Wortman", "authors": "Martin A. Wortman, Ernest J.L. Kee, Pranav Kannan", "title": "Is Core Damage Frequency an Informative Risk Metric?", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Core Damage Frequency (CDF) is a risk metric often employed by nuclear\nregulatory bodies worldwide. Numerical values for this metric are required by\nU.S. regulators, prior to reactor licensing, and reported values can trigger\nregulatory inspections. CDF is reported as a constant, sometimes accompanied by\na confidence interval. It is well understood that CDF characterizes the arrival\nrate of a stochastic point process modeling core damage events. However,\nconsequences of the assumptions imposed on this stochastic process as a\ncomputational necessity are often overlooked. Herein, we revisit CDF in the\ncontext of modern point process theory. We argue that the assumptions required\nto yield a constant CDF are typically unrealistic. We further argue that\ntreating CDF as an informative approximation is suspect, because of the\ninherent difficulties in quantifying its quality as an approximation.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 12:58:08 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 15:49:50 GMT"}, {"version": "v3", "created": "Sat, 10 Jul 2021 01:42:05 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wortman", "Martin A.", ""], ["Kee", "Ernest J. L.", ""], ["Kannan", "Pranav", ""]]}, {"id": "2105.09869", "submitter": "Marcos Netto", "authors": "Amir Hossein Abolmasoumi, Marcos Netto, Lamine Mili", "title": "Robust Dynamic Mode Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The paper develops a robust estimation method that makes the dynamic mode\ndecomposition method resistant to outliers while being fast to compute and\nstatistically efficient (i.e. accurate) at the Gaussian and non-Gaussian thick\ntailed distributions. The proposed robust dynamic mode decomposition (RDMD) is\nanchored on the theory of robust statistics. Specifically, it relies on the\nSchweppe-type Huber generalized maximum-likelihood estimator that minimizes a\nconvex weighted Huber loss function, where the weights are calculated via\nprojection statistics, thereby making the proposed RDMD robust to outliers,\nwhether vertical outliers or bad leverage points. The performance of the\nproposed RDMD is demonstrated numerically using canonical models of dynamical\nsystems. Simulation results reveal that it outperforms several other methods\nproposed in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:13:38 GMT"}, {"version": "v2", "created": "Sun, 23 May 2021 17:45:33 GMT"}, {"version": "v3", "created": "Tue, 25 May 2021 17:56:40 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Abolmasoumi", "Amir Hossein", ""], ["Netto", "Marcos", ""], ["Mili", "Lamine", ""]]}, {"id": "2105.09893", "submitter": "Mahsa Nadifar", "authors": "Mahsa Nadifar (1), Hossein Baghishani (1), Afshin Fallah (2) ((1)\n  Shahrood University of Technology, (2) IKIU)", "title": "A flexible Bayesian non-confounding spatial model for analysis of\n  dispersed count data in clinical studies", "comments": "arXiv admin note: text overlap with arXiv:1908.02344", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In employing spatial regression models for counts, we usually meet two\nissues. First, ignoring the inherent collinearity between covariates and the\nspatial effect would lead to causal inferences. Second, real count data usually\nreveal over or under-dispersion where the classical Poisson model is not\nappropriate to use. We propose a flexible Bayesian hierarchical modeling\napproach by joining non-confounding spatial methodology and a newly\nreconsidered dispersed count modeling from the renewal theory to control the\nissues. Specifically, we extend the methodology for analyzing spatial count\ndata based on the gamma distribution assumption for waiting times. The model\ncan be formulated as a latent Gaussian model, and consequently, we can carry\nout the fast computation using the integrated nested Laplace approximation\nmethod. We also examine different popular approaches for handling spatial\nconfounding and compare their performances in the presence of dispersion. We\nuse the proposed methodology to analyze a clinical dataset related to stomach\ncancer incidence in Slovenia and perform a simulation study to understand the\nproposed approach's merits better.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 16:55:28 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Nadifar", "Mahsa", ""], ["Baghishani", "Hossein", ""], ["Fallah", "Afshin", ""]]}, {"id": "2105.10017", "submitter": "Abhishek Kaul", "authors": "Abhishek Kaul", "title": "Segmentation of high dimensional means over multi-dimensional change\n  points and connections to regression trees", "comments": "All implementations carried out in R (code available upon request)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This article is motivated by the objective of providing a new analytically\ntractable and fully frequentist framework to characterize and implement\nregression trees while also allowing a multivariate (potentially high\ndimensional) response. The connection to regression trees is made by a high\ndimensional model with dynamic mean vectors over multi-dimensional change axes.\nOur theoretical analysis is carried out under a single two dimensional change\npoint setting. An optimal rate of convergence of the proposed estimator is\nobtained, which in turn allows existence of limiting distributions.\nDistributional behavior of change point estimates are split into two distinct\nregimes, the limiting distributions under each regime is then characterized, in\nturn allowing construction of asymptotically valid confidence intervals for\n$2d$-location of change. All results are obtained under a high dimensional\nscaling $s\\log^2 p=o(T_wT_h),$ where $p$ is the response dimension, $s$ is a\nsparsity parameter, and $T_w,T_h$ are sampling periods along change axes. We\ncharacterize full regression trees by defining a multiple multi-dimensional\nchange point model. Natural extensions of the single $2d$-change point\nestimation methodology are provided. Two applications, first on segmentation of\n{\\it Infra-red astronomy satellite (IRAS)} data and second to segmentation of\ndigital images are provided. Methodology and theoretical results are supported\nwith monte-carlo simulations.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 20:29:48 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Kaul", "Abhishek", ""]]}, {"id": "2105.10060", "submitter": "Jose R. Zubizarreta", "authors": "Eric R. Cohn, Jose R. Zubizarreta", "title": "Profile Matching for the Generalization and Personalization of Causal\n  Inferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce profile matching, a multivariate matching method for randomized\nexperiments and observational studies that finds the largest possible\nself-weighted samples across multiple treatment groups that are balanced\nrelative to a covariate profile. This covariate profile can represent a\nspecific population or a target individual, facilitating the tasks of\ngeneralization and personalization of causal inferences. For generalization,\nbecause the profile often amounts to summary statistics for a target\npopulation, profile matching does not require accessing individual-level data,\nwhich may be unavailable for confidentiality reasons. For personalization, the\nprofile can characterize a single patient. Profile matching achieves covariate\nbalance by construction, but unlike existing approaches to matching, it does\nnot require specifying a matching ratio, as this is implicitly optimized for\nthe data. The method can also be used for the selection of units for study\nfollow-up, and it readily applies to multi-valued treatments with many\ntreatment categories. We evaluate the performance of profile matching in a\nsimulation study of generalization of a randomized trial to a target\npopulation. We further illustrate this method in an exploratory observational\nstudy of the relationship between opioid use treatment and mental health\noutcomes. We analyze these relationships for three covariate profiles\nrepresenting: (i) sexual minorities, (ii) the Appalachian United States, and\n(iii) a hypothetical vulnerable patient. We provide R code with step-by-step\nexplanations to implement the methods in the paper in the Supplementary\nMaterials.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 22:50:21 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Cohn", "Eric R.", ""], ["Zubizarreta", "Jose R.", ""]]}, {"id": "2105.10324", "submitter": "Guidong Zhang", "authors": "Guidong Zhang and Yuhong Sheng", "title": "Estimating Unknown Time-Varying Parameters in Uncertain Differential\n  Equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertain differential equations have a wide range of applications. How to\nobtain estimated values of unknown parameters in uncertain differential\nequations through observations has always been a subject of concern and\nresearch, many methods have been developed to estimate unknown parameters.\nHowever, these parameters are constants. In this paper, the method of least\nsquares estimation is recast for estimating the unknown time-varying parameters\nin uncertain differential equations. A set of unknown time-varying parameter\nestimates will be obtained, and then the unknown time-varying parameters will\nbe obtained by regression fitting using the estimated values. Using this\nmethod, the uncertain differential equation of blood alcohol concentration in\nhuman body after drinking and the uncertain differential equation of COVID-19\nare derived.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 12:55:25 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Zhang", "Guidong", ""], ["Sheng", "Yuhong", ""]]}, {"id": "2105.10350", "submitter": "Wenyu Chen", "authors": "Wenyu Chen, Mathias Drton and Ali Shojaie", "title": "Definite Non-Ancestral Relations and Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In causal graphical models based on directed acyclic graphs (DAGs), directed\npaths represent causal pathways between the corresponding variables. The\nvariable at the beginning of such a path is referred to as an ancestor of the\nvariable at the end of the path. Ancestral relations between variables play an\nimportant role in causal modeling. In existing literature on structure\nlearning, these relations are usually deduced from learned structures and used\nfor orienting edges or formulating constraints of the space of possible DAGs.\nHowever, they are usually not posed as immediate target of inference. In this\nwork we investigate the graphical characterization of ancestral relations via\nCPDAGs and d-separation relations. We propose a framework that can learn\ndefinite non-ancestral relations without first learning the skeleton. This\nframe-work yields structural information that can be used in both score- and\nconstraint-based algorithms to learn causal DAGs more efficiently.\n", "versions": [{"version": "v1", "created": "Thu, 20 May 2021 06:53:52 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Chen", "Wenyu", ""], ["Drton", "Mathias", ""], ["Shojaie", "Ali", ""]]}, {"id": "2105.10360", "submitter": "Doudou Zhou", "authors": "Doudou Zhou, and Tianxi Cai, and Junwei Lu", "title": "BELT: Block-wise Missing Embedding Learning Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix completion has attracted attention in many fields, including\nstatistics, applied mathematics, and electrical engineering. Most of the works\nfocus on the independent sampling models under which the observed entries are\nsampled independently. Motivated by applications in the integration of multiple\nElectronic Health Record (EHR) datasets, we propose the method {\\bf B}lock-wise\nmissing {\\bf E}mbedding {\\bf L}earning {\\bf T}ransformer (BELT) to treat\nrow-wise/column-wise missingness. Specifically, BELT can recover block-wise\nmissing matrices efficiently when every pair of matrices has an overlap. Our\nidea is to exploit the orthogonal Procrustes problem to align the eigenspace of\nthe two sub-matrices using their overlap, then complete the missing blocks by\nthe inner product of the two low-rank components. Besides, we prove the\nstatistical rate for the eigenspace of the underlying matrix, which is\ncomparable to the rate under the independently missing assumption. Simulation\nstudies show that the method performs well under a variety of configurations.\nIn the real data analysis, the method is applied to two tasks: (i) the\nintegrating of several point-wise mutual information matrices built by English\nEHR and Chinese medical text data, and (ii) the machine translation between\nEnglish and Chinese medical concepts. Our method shows an advantage over\nexisting methods.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 13:55:30 GMT"}, {"version": "v2", "created": "Thu, 17 Jun 2021 12:20:03 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhou", "Doudou", ""], ["Cai", "Tianxi", ""], ["Lu", "Junwei", ""]]}, {"id": "2105.10406", "submitter": "Marco Antonio Pinto Orellana", "authors": "Marco A. Pinto-Orellana and Habib Sherkat and Peyman Mirtaheri and\n  Hugo L. Hammer", "title": "Dyadic aggregated autoregressive (DASAR) model for time-frequency\n  representation of biomedical signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP physics.med-ph stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper introduces a new time-frequency representation method for\nbiomedical signals: the dyadic aggregated autoregressive (DASAR) model.\nSignals, such as electroencephalograms (EEGs) and functional near-infrared\nspectroscopy (fNIRS), exhibit physiological information through time-evolving\nspectrum components at specific frequency intervals: 0-50 Hz (EEG) or 0-150 mHz\n(fNIRS). Spectrotemporal features in signals are conventionally estimated using\nshort-time Fourier transform (STFT) and wavelet transform (WT). However, both\nmethods may not offer the most robust or compact representation despite their\nwidespread use in biomedical contexts. The presented method, DASAR, improves\nprecise frequency identification and tracking of interpretable frequency\ncomponents with a parsimonious set of parameters. DASAR achieves these\ncharacteristics by assuming that the biomedical time-varying spectrum comprises\nseveral independent stochastic oscillators with (piecewise) time-varying\nfrequencies. Local stationarity can be assumed within dyadic subdivisions of\nthe recordings, while the stochastic oscillators can be modeled with an\naggregation of second-order autoregressive models (ASAR). DASAR can provide a\nmore accurate representation of the (highly contrasted) EEG and fNIRS frequency\nranges by increasing the estimation accuracy in user-defined spectrum region of\ninterest (SROI). A mental arithmetic experiment on a hybrid EEG-fNIRS was\nconducted to assess the efficiency of the method. Our proposed technique, STFT,\nand WT were applied on both biomedical signals to discover potential\noscillators that improve the discrimination between the task condition and its\nbaseline. The results show that DASAR provided the highest spectrum\ndifferentiation and it was the only method that could identify Mayer waves as\nnarrow-band artifacts at 97.4-97.5 mHz.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:42:40 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Pinto-Orellana", "Marco A.", ""], ["Sherkat", "Habib", ""], ["Mirtaheri", "Peyman", ""], ["Hammer", "Hugo L.", ""]]}, {"id": "2105.10417", "submitter": "Mengchu Li", "authors": "Mengchu Li and Yi Yu", "title": "Adversarially Robust Change Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Change point detection is becoming increasingly popular in many application\nareas. On one hand, most of the theoretically-justified methods are\ninvestigated in an ideal setting without model violations, or merely robust\nagainst identical heavy-tailed noise distribution across time and/or against\nisolate outliers; on the other hand, we are aware that there have been\nexponentially growing attacks from adversaries, who may pose systematic\ncontamination on data to purposely create spurious change points or disguise\ntrue change points. In light of the timely need for a change point detection\nmethod that is robust against adversaries, we start with, arguably, the\nsimplest univariate mean change point detection problem. The adversarial\nattacks are formulated through the Huber $\\varepsilon$-contamination framework,\nwhich in particular allows the contamination distributions to be different at\neach time point. In this paper, we demonstrate a phase transition phenomenon in\nchange point detection. This detection boundary is a function of the\ncontamination proportion $\\varepsilon$ and is the first time shown in the\nliterature. In addition, we derive the minimax-rate optimal localisation error\nrate, quantifying the cost of accuracy in terms of the contamination\nproportion. We propose a computationally feasible method, matching the minimax\nlower bound under certain conditions, saving for logarithmic factors. Extensive\nnumerical experiments are conducted with comparisons to robust change point\ndetection methods in the existing literature.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 15:37:04 GMT"}], "update_date": "2021-05-24", "authors_parsed": [["Li", "Mengchu", ""], ["Yu", "Yi", ""]]}, {"id": "2105.10470", "submitter": "Philipp Frank", "authors": "Philipp Frank, Reimar Leike, and Torsten A. En{\\ss}lin", "title": "Geometric variational inference", "comments": "42 pages, 18 figures, accepted by Entropy", "journal-ref": null, "doi": "10.3390/e23070853", "report-no": null, "categories": "stat.ME astro-ph.IM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Efficiently accessing the information contained in non-linear and high\ndimensional probability distributions remains a core challenge in modern\nstatistics. Traditionally, estimators that go beyond point estimates are either\ncategorized as Variational Inference (VI) or Markov-Chain Monte-Carlo (MCMC)\ntechniques. While MCMC methods that utilize the geometric properties of\ncontinuous probability distributions to increase their efficiency have been\nproposed, VI methods rarely use the geometry. This work aims to fill this gap\nand proposes geometric Variational Inference (geoVI), a method based on\nRiemannian geometry and the Fisher information metric. It is used to construct\na coordinate transformation that relates the Riemannian manifold associated\nwith the metric to Euclidean space. The distribution, expressed in the\ncoordinate system induced by the transformation, takes a particularly simple\nform that allows for an accurate variational approximation by a normal\ndistribution. Furthermore, the algorithmic structure allows for an efficient\nimplementation of geoVI which is demonstrated on multiple examples, ranging\nfrom low-dimensional illustrative ones to non-linear, hierarchical Bayesian\ninverse problems in thousands of dimensions.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 17:18:50 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 07:41:31 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Frank", "Philipp", ""], ["Leike", "Reimar", ""], ["En\u00dflin", "Torsten A.", ""]]}, {"id": "2105.10565", "submitter": "Adam Peterson", "authors": "Adam Peterson, Jana Hirsch, Brisa Sanchez", "title": "Spatial Temporal Aggregated Predictors to Examine Built Environment\n  Health Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We propose the spatial-temporal aggregated predictor (STAP) modeling\nframework to address measurement and estimation issues that arise when\nassessing the relationship between built environment features (BEF) and health\noutcomes. Many BEFs can be mapped as point locations and thus traditional\nexposure metrics are based on the number of features within a pre-specified\nspatial unit. The size of the spatial unit--or spatial scale--that is most\nappropriate for a particular health outcome is unknown and its choice\ninextricably impacts the estimated health effect. A related issue is the lack\nof knowledge of the temporal scale--or the length of exposure time that is\nnecessary for the BEF to render its full effect on the health outcome. The\nproposed STAP model enables investigators to estimate both the spatial and\ntemporal scales for a given BEF in a data-driven fashion, thereby providing a\nflexible solution for measuring the relationship between outcomes and spatial\nproximity to point-referenced exposures. Simulation studies verify the validity\nof our method for estimating the scales as well as the association between\navailability of BEFs' and health outcomes. We apply this method to estimate the\nspatial-temporal association between supermarkets and BMI using data from the\nMulti-Ethnic Atherosclerosis Study, demonstrating the method's applicability in\ncohort studies.\n", "versions": [{"version": "v1", "created": "Fri, 21 May 2021 20:45:31 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Peterson", "Adam", ""], ["Hirsch", "Jana", ""], ["Sanchez", "Brisa", ""]]}, {"id": "2105.10624", "submitter": "Richard Berk", "authors": "Richard A. Berk", "title": "Post-Model-Selection Statistical Inference with Interrupted Time Series\n  Designs: An Evaluation of an Assault Weapons Ban in California", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been many claims in the media and a bit of respectable research\nabout the causes of variation in firearm sales. The challenges for causal\ninference can be quite daunting. This paper reports an analysis of daily\nhandgun sales in California from 1996 through 2018 using an interrupted time\nseries design and analysis. The design was introduced to social scientists in\n1963 by Campbell and Stanley, analysis methods were proposed by Box and Tiao in\n1975, and more recent treatments are easily found (Box et al., 2016). But this\napproach to causal inference can be badly overmatched by the data on handgun\nsales, especially when the causal effects are estimated. More important for\nthis paper are fundamental oversights in the standard statistical methods\nemployed. Test multiplicity problems are introduced by adaptive model selection\nbuilt into recommended practice. The challenges are computational and\nconceptual. Some progress is made on both problems that arguably improves on\npast research, but the take-home message may be to reduce aspirations about\nwhat can be learned.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 02:45:28 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Berk", "Richard A.", ""]]}, {"id": "2105.10637", "submitter": "Candace Berrett", "authors": "Candace Berrett, Brianne Gurney, David Arthur, Todd Moon, Gus P.\n  Williams", "title": "A Bayesian change point model for spatio-temporal data", "comments": "24 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Urbanization of an area is known to increase the temperature of the\nsurrounding area. This phenomenon -- a so-called urban heat island (UHI) --\noccurs at a local level over a period of time and has lasting impacts for\nhistorical data analysis. We propose a methodology to examine if long-term\nchanges in temperature increases and decreases across time exist (and to what\nextent) at the local level for a given set of temperature readings at various\nlocations. Specifically, we propose a Bayesian change point model for\nspatio-temporally dependent data where we select the number of change points at\neach location using a \"forwards\" selection process using deviance information\ncriteria (DIC). We then fit the selected model and examine the linear slopes\nacross time to quantify changes in long-term temperature behavior. We show the\nutility of this model and method using a synthetic data set and temperature\nmeasurements from eight stations in Utah consisting of daily temperature data\nfor 60 years.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 04:38:08 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 14:47:00 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Berrett", "Candace", ""], ["Gurney", "Brianne", ""], ["Arthur", "David", ""], ["Moon", "Todd", ""], ["Williams", "Gus P.", ""]]}, {"id": "2105.10675", "submitter": "Yi Yu", "authors": "Thomas Berrett and Yi Yu", "title": "Locally private online change point detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study online change point detection problems under the constraint of local\ndifferential privacy (LDP) where, in particular, the statistician does not have\naccess to the raw data. As a concrete problem, we study a multivariate\nnonparametric regression problem. At each time point $t$, the raw data are\nassumed to be of the form $(X_t, Y_t)$, where $X_t$ is a $d$-dimensional\nfeature vector and $Y_t$ is a response variable. Our primary aim is to detect\nchanges in the regression function $m_t(x)=\\mathbb{E}(Y_t |X_t=x)$ as soon as\nthe change occurs. We provide algorithms which respect the LDP constraint,\nwhich control the false alarm probability, and which detect changes with a\nminimal (minimax rate-optimal) delay. To quantify the cost of privacy, we also\npresent the optimal rate in the benchmark, non-private setting. These\nnon-private results are also new to the literature and thus are interesting\n\\emph{per se}. In addition, we study the univariate mean online change point\ndetection problem, under privacy constraints. This serves as the blueprint of\nstudying more complicated private change point detection problems.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 10:03:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Berrett", "Thomas", ""], ["Yu", "Yi", ""]]}, {"id": "2105.10737", "submitter": "Laura Boeschoten", "authors": "Laura Boeschoten, Sander Scholtus, Arnout van Delden", "title": "A note on efficient audit sample selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Auditing is a widely used method for quality improvement, and many guidelines\nare available advising on how to draw samples for auditing. However,\nresearchers or auditors sometimes find themselves in situations that are not\nstraightforward and the standard sampling techniques are not sufficient, for\nexample when a selective sample has initially been audited and the auditor\ndesires to re-use as many cases as possible from this initial audit in a new\naudit sample that is representative with respect to some background\ncharacteristics. In this paper, we introduce a method that selects an audit\nsample that re-uses initially audited cases by considering the selection of a\nrepresentative audit sample as a constrained minimization problem. In addition,\nwe evaluate the performance of this method by means of a simulation study and\nwe apply the method to draw an audit sample of establishments to evaluate the\nquality of an establishment registry used to produce statistics on energy\nconsumption per type of economic activity.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 14:23:36 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Boeschoten", "Laura", ""], ["Scholtus", "Sander", ""], ["van Delden", "Arnout", ""]]}, {"id": "2105.10809", "submitter": "Brian Hentschel", "authors": "Brian Hentschel and Peter J. Haas and Yuanyuan Tian", "title": "Exact PPS Sampling with Bounded Sample Size", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Probability proportional to size (PPS) sampling schemes with a target sample\nsize aim to produce a sample comprising a specified number $n$ of items while\nensuring that each item in the population appears in the sample with a\nprobability proportional to its specified \"weight\" (also called its \"size\").\nThese two objectives, however, cannot always be achieved simultaneously.\nExisting PPS schemes prioritize control of the sample size, violating the PPS\nproperty if necessary. We provide a new PPS scheme that allows a different\ntrade-off: our method enforces the PPS property at all times while ensuring\nthat the sample size never exceeds the target value $n$. The sample size is\nexactly equal to $n$ if possible, and otherwise has maximal expected value and\nminimal variance. Thus we bound the sample size, thereby avoiding storage\noverflows and helping to control the time required for analytics over the\nsample, while allowing the user complete control over the sample contents. The\nmethod is both simple to implement and efficient, being a one-pass streaming\nalgorithm with an amortized processing time of $O(1)$ per item.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 21:07:18 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Hentschel", "Brian", ""], ["Haas", "Peter J.", ""], ["Tian", "Yuanyuan", ""]]}, {"id": "2105.10821", "submitter": "Nikolaj Thams", "authors": "Nikolaj Thams, Sorawit Saengkyongam, Niklas Pfister and Jonas Peters", "title": "Statistical Testing under Distributional Shifts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce statistical testing under distributional shifts.\nWe are interested in the hypothesis $P^* \\in H_0$ for a target distribution\n$P^*$, but observe data from a different distribution $Q^*$. We assume that\n$P^*$ is related to $Q^*$ through a known shift $\\tau$ and formally introduce\nhypothesis testing in this setting. We propose a general testing procedure that\nfirst resamples from the observed data to construct an auxiliary data set and\nthen applies an existing test in the target domain. We prove that if the size\nof the resample is at most $o(\\sqrt{n})$ and the resampling weights are\nwell-behaved, this procedure inherits the pointwise asymptotic level and power\nfrom the target test. If the map $\\tau$ is estimated from data, we can maintain\nthe above guarantees under mild conditions if the estimation works sufficiently\nwell. We further extend our results to uniform asymptotic level and a different\nresampling scheme. Testing under distributional shifts allows us to tackle a\ndiverse set of problems. We argue that it may prove useful in reinforcement\nlearning and covariate shift, we show how it reduces conditional to\nunconditional independence testing and we provide example applications in\ncausal inference.\n", "versions": [{"version": "v1", "created": "Sat, 22 May 2021 22:36:05 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 06:26:56 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Thams", "Nikolaj", ""], ["Saengkyongam", "Sorawit", ""], ["Pfister", "Niklas", ""], ["Peters", "Jonas", ""]]}, {"id": "2105.10838", "submitter": "Xinjie Du", "authors": "Xinjie Du, Minh Tang", "title": "Hypothesis Testing for Equality of Latent Positions in Random Graphs", "comments": "67 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the hypothesis testing problem that two vertices $i$ and $j$ of a\ngeneralized random dot product graph have the same latent positions, possibly\nup to scaling. Special cases of this hypotheses test include testing whether\ntwo vertices in a stochastic block model or degree-corrected stochastic block\nmodel graph have the same block membership vectors. We propose several test\nstatistics based on the empirical Mahalanobis distances between the $i$th and\n$j$th rows of either the adjacency or the normalized Laplacian spectral\nembedding of the graph. We show that, under mild conditions, these test\nstatistics have limiting chi-square distributions under both the null and local\nalternative hypothesis, and we derived explicit expressions for the\nnon-centrality parameters under the local alternative. Using these limit\nresults, we address the model selection problem of choosing between the\nstandard stochastic block model and its degree-corrected variant. The\neffectiveness of our proposed tests are illustrated via both simulation studies\nand real data applications.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 01:27:23 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Du", "Xinjie", ""], ["Tang", "Minh", ""]]}, {"id": "2105.10839", "submitter": "Shinjini Nandi", "authors": "Shinjini Nandi and Sanat K. Sarkar", "title": "Controlling the False Discovery Rate in Complex Multi-Way Classified\n  Hypotheses", "comments": "36 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a generalized weighted version of the well-known\nBenjamini-Hochberg (BH) procedure. The rigorous weighting scheme used by our\nmethod enables it to encode structural information from simultaneous multi-way\nclassification as well as hierarchical partitioning of hypotheses into groups,\nwith provisions to accommodate overlapping groups. The method is proven to\ncontrol the False Discovery Rate (FDR) when the p-values involved are\nPositively Regression Dependent on the Subset (PRDS) of null p-values. A\ndata-adaptive version of the method is proposed. Simulations show that our\nproposed methods control FDR at desired level and are more powerful than\nexisting comparable multiple testing procedures, when the p-values are\nindependent or satisfy certain dependence conditions. We apply this\ndata-adaptive method to analyze a neuro-imaging dataset and understand the\nimpact of alcoholism on human brain. Neuro-imaging data typically have complex\nclassification structure, which have not been fully utilized in subsequent\ninference by previously proposed multiple testing procedures. With a flexible\nweighting scheme, our method is poised to extract more information from the\ndata and use it to perform a more informed and efficient test of the\nhypotheses.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 01:31:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Nandi", "Shinjini", ""], ["Sarkar", "Sanat K.", ""]]}, {"id": "2105.10888", "submitter": "Fr\\'ed\\'eric Pro\\\"ia", "authors": "Eunice Okome Obiang, Pascal J\\'ez\\'equel, Fr\\'ed\\'eric Pro\\\"ia", "title": "A Bayesian approach for partial Gaussian graphical models with sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore various Bayesian approaches to estimate partial Gaussian graphical\nmodels. Our hierarchical structures enable to deal with single-output as well\nas multiple-output linear regressions, in small or high dimension, enforcing\neither no sparsity, sparsity, group sparsity or even sparse-group sparsity for\na bi-level selection in the direct links between predictors and responses,\nthanks to spike-and-slab priors corresponding to each setting. Adaptative and\nglobal shrinkages are also incorporated in the Bayesian modeling of the direct\nlinks. Gibbs samplers are developed and a simulation study shows the efficiency\nof our models which regularly give better results than the usual Lasso-type\nprocedures, especially in terms of support recovery. To conclude, a real\ndataset is investigated.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 08:47:44 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Obiang", "Eunice Okome", ""], ["J\u00e9z\u00e9quel", "Pascal", ""], ["Pro\u00efa", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "2105.10890", "submitter": "Nadja Klein Prof. Dr.", "authors": "Nadja Klein and Jorge Mateu", "title": "Bayesian Effect Selection for Additive Quantile Regression with an\n  Analysis to Air Pollution Thresholds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical techniques used in air pollution modelling usually lack the\npossibility to understand which predictors affect air pollution in which\nfunctional form; and are not able to regress on exceedances over certain\nthresholds imposed by authorities directly. The latter naturally induce\nconditional quantiles and reflect the seriousness of particular events. In the\npresent paper we focus on this important aspect by developing quantile\nregression models further. We propose a general Bayesian effect selection\napproach for additive quantile regression within a highly interpretable\nframework. We place separate normal beta prime spike and slab priors on the\nscalar importance parameters of effect parts and implement a fast Gibbs\nsampling scheme. Specifically, it enables to study quantile-specific covariate\neffects, allows these covariates to be of general functional form using\nadditive predictors, and facilitates the analysts' decision whether an effect\nshould be included linearly, non-linearly or not at all in the quantiles of\ninterest. In a detailed analysis on air pollution data in Madrid (Spain) we\nfind the added value of modelling extreme nitrogen dioxide (NO2) concentrations\nand how thresholds are driven differently by several climatological variables\nand traffic as a spatial proxy. Our results underpin the need of enhanced\nstatistical models to support short-term decisions and enable local authorities\nto mitigate or even prevent exceedances of NO2 concentration limits.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 09:02:46 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Klein", "Nadja", ""], ["Mateu", "Jorge", ""]]}, {"id": "2105.10965", "submitter": "Demian Pouzo", "authors": "Marina Dias and Demian Pouzo", "title": "Inference for multi-valued heterogeneous treatment effects when the\n  number of treated units is small", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM econ.GN math.ST q-fin.EC stat.AP stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a method for conducting asymptotically valid inference for\ntreatment effects in a multi-valued treatment framework where the number of\nunits in the treatment arms can be small and do not grow with the sample size.\nWe accomplish this by casting the model as a semi-/non-parametric conditional\nquantile model and using known finite sample results about the law of the\nindicator function that defines the conditional quantile. Our framework allows\nfor structural functions that are non-additively separable, with flexible\nfunctional forms and heteroskedasticy in the residuals, and it also encompasses\ncommonly used designs like difference in difference. We study the finite sample\nbehavior of our test in a Monte Carlo study and we also apply our results to\nassessing the effect of weather events on GDP growth.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 16:06:22 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Dias", "Marina", ""], ["Pouzo", "Demian", ""]]}, {"id": "2105.11007", "submitter": "Yue Bai", "authors": "Peiliang Bai, Yue Bai, Abolfazl Safikhani, George Michailidis", "title": "Multiple Change Point Detection in Structured VAR Models: the VARDetect\n  R Package", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vector Auto-Regressive (VAR) models capture lead-lag temporal dynamics of\nmultivariate time series data. They have been widely used in macroeconomics,\nfinancial econometrics, neuroscience and functional genomics. In many\napplications, the data exhibit structural changes in their autoregressive\ndynamics, which correspond to changes in the transition matrices of the VAR\nmodel that specify such dynamics. We present the R package VARDetect that\nimplements two classes of algorithms to detect multiple change points in\npiecewise stationary VAR models. The first exhibits sublinear computational\ncomplexity in the number of time points and is best suited for structured\nsparse models, while the second exhibits linear time complexity and is designed\nfor models whose transition matrices are assumed to have a low rank plus sparse\ndecomposition. The package also has functions to generate data from the various\nvariants of VAR models discussed, which is useful in simulation studies, as\nwell as to visualize the results through network layouts.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 19:45:11 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 15:00:45 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Bai", "Peiliang", ""], ["Bai", "Yue", ""], ["Safikhani", "Abolfazl", ""], ["Michailidis", "George", ""]]}, {"id": "2105.11022", "submitter": "Alberto Cabezas", "authors": "Alberto Cabezas, Marco Battiston, Christopher Nemeth", "title": "Robust Bayesian Nonparametric Variable Selection for Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spike-and-slab and horseshoe regression are arguably the most popular\nBayesian variable selection approaches for linear regression models. However,\ntheir performance can deteriorate if outliers and heteroskedasticity are\npresent in the data, which are common features in many real-world statistics\nand machine learning applications. In this work, we propose a Bayesian\nnonparametric approach to linear regression that performs variable selection\nwhile accounting for outliers and heteroskedasticity. Our proposed model is an\ninstance of a Dirichlet process scale mixture model with the advantage that we\ncan derive the full conditional distributions of all parameters in closed form,\nhence producing an efficient Gibbs sampler for posterior inference. Moreover,\nwe present how to extend the model to account for heavy-tailed response\nvariables. The performance of the model is tested against competing algorithms\non synthetic and real-world datasets.\n", "versions": [{"version": "v1", "created": "Sun, 23 May 2021 21:18:35 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Cabezas", "Alberto", ""], ["Battiston", "Marco", ""], ["Nemeth", "Christopher", ""]]}, {"id": "2105.11177", "submitter": "Marta Bonsaglio", "authors": "Marta Bonsaglio, Sandra Fortini, Steffen Ventz and Lorenzo Trippa", "title": "Approximating the Operating Characteristics of Bayesian Uncertainty\n  Directed Trial Designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian response adaptive clinical trials are currently evaluating\nexperimental therapies for several diseases. Adaptive decisions, such as\npre-planned variations of the randomization probabilities, attempt to\naccelerate the development of new treatments. The design of response adaptive\ntrials, in most cases, requires time consuming simulation studies to describe\noperating characteristics, such as type I/II error rates, across plausible\nscenarios. We investigate large sample approximations of pivotal operating\ncharacteristics in Bayesian Uncertainty directed trial Designs (BUDs). A BUD\ntrial utilizes an explicit metric u to quantify the information accrued during\nthe study on parameters of interest, for example the treatment effects. The\nrandomization probabilities vary during time to minimize the uncertainty\nsummary u at completion of the study. We provide an asymptotic analysis (i) of\nthe allocation of patients to treatment arms and (ii) of the randomization\nprobabilities. For BUDs with outcome distributions belonging to the natural\nexponential family with quadratic variance function, we illustrate the\nasymptotic normality of the number of patients assigned to each arm and of the\nrandomization probabilities. We use these results to approximate relevant\noperating characteristics such as the power of the BUD. We evaluate the\naccuracy of the approximations through simulations under several scenarios for\nbinary, time-to-event and continuous outcome models.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 09:53:10 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Bonsaglio", "Marta", ""], ["Fortini", "Sandra", ""], ["Ventz", "Steffen", ""], ["Trippa", "Lorenzo", ""]]}, {"id": "2105.11205", "submitter": "Toby Kenney", "authors": "Yun Cai, Hong Gu and Toby Kenney", "title": "Deconvolution density estimation with penalised MLE", "comments": "25 pages, 4 figures, Appendix - 30 pages 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deconvolution is the important problem of estimating the distribution of a\nquantity of interest from a sample with additive measurement error. Nearly all\nmethods in the literature are based on Fourier transformation because it is\nmathematically a very neat solution. However, in practice these methods are\nunstable, and produce bad estimates when signal-noise ratio or sample size are\nlow. In this paper, we develop a new deconvolution method based on maximum\nlikelihood with a smoothness penalty. We show that our new method has much\nbetter performance than existing methods, particularly for small sample size or\nsignal-noise ratio.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 11:21:55 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Cai", "Yun", ""], ["Gu", "Hong", ""], ["Kenney", "Toby", ""]]}, {"id": "2105.11353", "submitter": "Saumya Sakitha Ariyarathne", "authors": "Sakitha Ariyarathne, Harsha Gangammanavar, Raanju R. Sundararajan", "title": "Change Point Detection in Nonstationary Sub-Hourly Wind Time Series", "comments": "18 pages, 3 figures, 3 tables, and 5 sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SY eess.SY stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present a change point detection method for detecting\nchange points in multivariate nonstationary wind speed time series. The change\npoint method identifies changes in the covariance structure and decomposes the\nnonstationary multivariate time series into stationary segments. We also\npresent parametric and nonparametric simulation techniques to simulate new wind\ntime series within each stationary segment. The proposed simulation methods\nretain statistical properties of the original time series and therefore, can be\nemployed for simulation-based analysis of power systems planning and operations\nproblems. We demonstrate the capabilities of the change point detection method\nthrough computational experiments conducted on wind speed time series at\nfive-minute resolution. We also conduct experiments on the economic dispatch\nproblem to illustrate the impact of nonstationarity in wind generation on\nconventional generation and location marginal prices.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:37:54 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Ariyarathne", "Sakitha", ""], ["Gangammanavar", "Harsha", ""], ["Sundararajan", "Raanju R.", ""]]}, {"id": "2105.11357", "submitter": "Austin Cole", "authors": "D. Austin Cole, Robert B. Gramacy, James E. Warner, Geoffrey F.\n  Bomarito, Patrick E. Leser, William P. Leser", "title": "Entropy-based adaptive design for contour finding and estimating\n  reliability", "comments": "41 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reliability analysis, methods used to estimate failure probability are\noften limited by the costs associated with model evaluations. Many of these\nmethods, such as multifidelity importance sampling (MFIS), rely upon a\ncomputationally efficient, surrogate model like a Gaussian process (GP) to\nquickly generate predictions. The quality of the GP fit, particularly in the\nvicinity of the failure region(s), is instrumental in supplying accurately\npredicted failures for such strategies. We introduce an entropy-based GP\nadaptive design that, when paired with MFIS, provides more accurate failure\nprobability estimates and with higher confidence. We show that our greedy data\nacquisition strategy better identifies multiple failure regions compared to\nexisting contour-finding schemes. We then extend the method to batch selection,\nwithout sacrificing accuracy. Illustrative examples are provided on benchmark\ndata as well as an application to an impact damage simulator for National\nAeronautics and Space Administration (NASA) spacesuits.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:41:15 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Cole", "D. Austin", ""], ["Gramacy", "Robert B.", ""], ["Warner", "James E.", ""], ["Bomarito", "Geoffrey F.", ""], ["Leser", "Patrick E.", ""], ["Leser", "William P.", ""]]}, {"id": "2105.11362", "submitter": "Peng Wu", "authors": "Peng Wu, Zhiqiang Tan, Wenjie Hu and Xiao-Hua Zhou", "title": "Model-Assisted Inference for Covariate-Specific Treatment Effects with\n  High-dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate-specific treatment effects (CSTEs) represent heterogeneous\ntreatment effects across subpopulations defined by certain selected covariates.\nIn this article, we consider marginal structural models where CSTEs are\nlinearly represented using a set of basis functions of the selected covariates.\nWe develop a new approach in high-dimensional settings to obtain not only\ndoubly robust point estimators of CSTEs, but also model-assisted confidence\nintervals, which are valid when a propensity score model is correctly specified\nbut an outcome regression model may be misspecified. With a linear outcome\nmodel and subpopulations defined by discrete covariates, both point estimators\nand confidence intervals are doubly robust for CSTEs. In contrast, confidence\nintervals from existing high-dimensional methods are valid only when both the\npropensity score and outcome models are correctly specified. We establish\nasymptotic properties of the proposed point estimators and the associated\nconfidence intervals. We present simulation studies and empirical applications\nwhich demonstrate the advantages of the proposed method compared with competing\nones.\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 15:45:31 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Wu", "Peng", ""], ["Tan", "Zhiqiang", ""], ["Hu", "Wenjie", ""], ["Zhou", "Xiao-Hua", ""]]}, {"id": "2105.11387", "submitter": "Wenyu Chen", "authors": "Wenyu Chen, Rahul Mazumder, Richard J. Samworth", "title": "A new computational framework for log-concave density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Statistics, log-concave density estimation is a central problem within the\nfield of nonparametric inference under shape constraints. Despite great\nprogress in recent years on the statistical theory of the canonical estimator,\nnamely the log-concave maximum likelihood estimator, adoption of this method\nhas been hampered by the complexities of the non-smooth convex optimization\nproblem that underpins its computation. We provide enhanced understanding of\nthe structural properties of this optimization problem, which motivates the\nproposal of new algorithms, based on both randomized and Nesterov smoothing,\ncombined with an appropriate integral discretization of increasing accuracy. We\nprove that these methods enjoy, both with high probability and in expectation,\na convergence rate of order $1/T$ up to logarithmic factors on the objective\nfunction scale, where $T$ denotes the number of iterations. The benefits of our\nnew computational framework are demonstrated on both synthetic and real data,\nand our implementation is available in a github repository \\texttt{LogConcComp}\n(Log-Concave Computation).\n", "versions": [{"version": "v1", "created": "Mon, 24 May 2021 16:25:18 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Chen", "Wenyu", ""], ["Mazumder", "Rahul", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2105.11659", "submitter": "Xiaowu Dai", "authors": "Xiaowu Dai, Xiang Lyu, Lexin Li", "title": "Kernel Knockoffs Selection for Nonparametric Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thanks to its fine balance between model flexibility and interpretability,\nthe nonparametric additive model has been widely used, and variable selection\nfor this type of model has received constant attention. However, none of the\nexisting solutions can control the false discovery rate (FDR) under the finite\nsample setting. The knockoffs framework is a recent proposal that can\neffectively control the FDR with a finite sample size, but few knockoffs\nsolutions are applicable to nonparametric models. In this article, we propose a\nnovel kernel knockoffs selection procedure for the nonparametric additive\nmodel. We integrate three key components: the knockoffs, the subsampling for\nstability, and the random feature mapping for nonparametric function\napproximation. We show that the proposed method is guaranteed to control the\nFDR under any finite sample size, and achieves a power that approaches one as\nthe sample size tends to infinity. We demonstrate the efficacy of our method\nthrough intensive numerical analyses and comparisons with the alternative\nsolutions. Our proposal thus makes useful contributions to the methodology of\nnonparametric variable selection, FDR-based inference, as well as knockoffs.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 04:28:08 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Dai", "Xiaowu", ""], ["Lyu", "Xiang", ""], ["Li", "Lexin", ""]]}, {"id": "2105.11807", "submitter": "Jake Carson", "authors": "Jake Carson, Trevelyan J. McKinley, Peter Neal and Simon E. F. Spencer", "title": "Efficient Bayesian model selection for coupled hidden Markov models with\n  application to infectious diseases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing model selection for coupled hidden Markov models (CHMMs) is highly\nchallenging, owing to the large dimension of the hidden state process. Whilst\nin principle the hidden state process can be marginalized out via forward\nfiltering, in practice the computational cost of doing so increases\nexponentially with the number of coupled Markov chains, making this approach\ninfeasible in most applications. Monte Carlo methods can be utilized, but\ndespite many remarkable developments in model selection methodology, generic\napproaches continue to be ill-suited for such high-dimensional problems. Here\nwe develop specialized solutions for CHMMs with weak inter-chain dependencies.\nSpecifically we construct effective proposal distributions for the hidden state\nprocess that remain computationally viable as the number of chains increases,\nand that require little user input or tuning. This methodology is particularly\napplicable to individual-level infectious disease models characterized as\nCHMMs, in which each chain represents an individual, and the coupling\nrepresents contact between individuals. Since the only significant contacts are\nbetween susceptible and infectious individuals, and since multiple infection\npathways are often possible, the resulting CHMMs naturally have low inter-chain\ndependencies. We demonstrate the utility of our methodology with an application\nto a study of highly pathogenic avian influenza in chickens.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 10:20:06 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Carson", "Jake", ""], ["McKinley", "Trevelyan J.", ""], ["Neal", "Peter", ""], ["Spencer", "Simon E. F.", ""]]}, {"id": "2105.11873", "submitter": "Daisuke Kurisu", "authors": "Daisuke Kurisu", "title": "On the estimation of locally stationary functional time series", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops an asymptotic theory for estimating the time-varying\ncharacteristics of locally stationary functional time series. We introduce a\nkernel-based method to estimate the time-varying covariance operator and the\ntime-varying mean function of a locally stationary functional time series.\nSubsequently, we derive the convergence rate of the kernel estimator of the\ncovariance operator and associated eigenvalue and eigenfunctions. We also\nestablish a central limit theorem for the kernel-based locally weighted sample\nmean. As applications of our results, we discuss the prediction of locally\nstationary functional time series and methods for testing the equality of\ntime-varying mean functions in two functional samples.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:18:41 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 04:16:38 GMT"}, {"version": "v3", "created": "Sun, 20 Jun 2021 04:03:50 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Kurisu", "Daisuke", ""]]}, {"id": "2105.11886", "submitter": "Chen Xu", "authors": "Chen Xu, Yao Xie", "title": "Conformal Anomaly Detection on Spatio-Temporal Observations with Missing\n  Data", "comments": "Submitted to ICML 2021 Workshop--Distribution-free Uncertainty\n  Quantification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a distribution-free, unsupervised anomaly detection method called\nECAD, which wraps around any regression algorithm and sequentially detects\nanomalies. Rooted in conformal prediction, ECAD does not require data\nexchangeability but approximately controls the Type-I error when data are\nnormal. Computationally, it involves no data-splitting and efficiently trains\nensemble predictors to increase statistical power. We demonstrate the superior\nperformance of ECAD on detecting anomalous spatio-temporal traffic flow.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 12:44:14 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 02:49:29 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Xu", "Chen", ""], ["Xie", "Yao", ""]]}, {"id": "2105.11917", "submitter": "Jordan Richards", "authors": "Jordan Richards and Jonathan A. Tawn", "title": "On the Tail Behaviour of Aggregated Random Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many areas of interest, modern risk assessment requires estimation of the\nextremal behaviour of sums of random variables. We derive the first order\nupper-tail behaviour of the weighted sum of bivariate random variables under\nweak assumptions on their marginal distributions and their copula. The extremal\nbehaviour of the marginal variables is characterised by the generalised Pareto\ndistribution and their extremal dependence through subclasses of the limiting\nrepresentations of Ledford and Tawn (1997) and Heffernan and Tawn (2004). We\nfind that the upper tail behaviour of the aggregate is driven by different\nfactors dependent on the signs of the marginal shape parameters; if they are\nboth negative, the extremal behaviour of the aggregate is determined by both\nmarginal shape parameters and the coefficient of asymptotic independence\n(Ledford and Tawn, 1996); if they are both positive or have different signs,\nthe upper-tail behaviour of the aggregate is given solely by the largest\nmarginal shape. We also derive the aggregate upper-tail behaviour for some well\nknown copulae which reveals further insight into the tail structure when the\ncopula falls outside the conditions for the subclasses of the limiting\ndependence representations.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:22:03 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 16:38:25 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 15:35:13 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Richards", "Jordan", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "2105.11939", "submitter": "Bal\\'azs R. Sziklai", "authors": "Bal\\'azs R. Sziklai and M\\'at\\'e Baranyi and K\\'aroly H\\'eberger", "title": "Testing Cross-Validation Variants in Ranking Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This research investigates how to determine whether two rankings can come\nfrom the same distribution. We evaluate three hybrid tests: Wilcoxon's,\nDietterich's, and Alpaydin's statistical tests combined with cross-validation,\neach operating with folds ranging from 5 to 10, thus altogether 18 variants. We\nhave used the framework of a popular comparative statistical test, the Sum of\nRanking Differences, but our results are representative of all ranking\nenvironments. To compare these methods, we have followed an innovative approach\nborrowed from Economics. We designed eight scenarios for testing type I and II\nerrors. These represent typical situations (i.e., different data structures)\nthat cross-validation (CV) tests face routinely. The optimal CV method depends\non the preferences regarding the minimization of type I/II errors, size of the\ninput, and expected patterns in the data. The Wilcoxon method with eight folds\nproved to be the best under all three investigated input sizes, although there\nwere scenarios and decision aspects where other methods, namely Wilcoxon~10 and\nAlpaydin~10, performed better.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 13:39:53 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Sziklai", "Bal\u00e1zs R.", ""], ["Baranyi", "M\u00e1t\u00e9", ""], ["H\u00e9berger", "K\u00e1roly", ""]]}, {"id": "2105.12035", "submitter": "Neda Mohammadi Jouzdani", "authors": "Neda Mohammadi Jouzdani and Victor M. Panaretos", "title": "Functional Data Analysis with Rough Sampled Paths?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data are typically modeled as sampled paths of smooth stochastic\nprocesses in order to mitigate the fact that they are often observed discretely\nand noisily, occasionally irregularly and sparsely. The required smoothness\nallows for the use of smoothing techniques but excludes many stochastic\nprocesses, most notably diffusion processes. Such processes would otherwise be\nwell within the realm of functional data analysis, at least under complete\nobservation. In this short note we demonstrate that a simple modification of\nexisting methods allows for the functional data analysis of processes with\nnowhere differentiable sample paths, even when these are discretely and noisily\nobserved, including under irregular and sparse designs. By way of simulation it\nis shown that this is not a theoretical curiosity, but can work well in\npractice, hinting at potential closer links with the field of diffusion\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:15:32 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Jouzdani", "Neda Mohammadi", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "2105.12061", "submitter": "Benjamin Eltzner", "authors": "Pernille Hansen, Benjamin Eltzner, Stephan F. Huckemann, Stefan Sommer", "title": "Diffusion Means in Geometric Spaces", "comments": "28 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a location statistic for distributions on non-linear geometric\nspaces, the diffusion mean, serving both as an extension of and an alternative\nto the Fr\\'echet mean. The diffusion mean arises as the generalization of\nGaussian maximum likelihood analysis to non-linear spaces by maximizing the\nlikelihood of a Brownian motion. The diffusion mean depends on a time parameter\n$t$, which admits the interpretation of the allowed variance of the mean. The\ndiffusion $t$-mean of a distribution $X$ is the most likely origin of a\nBrownian motion at time $t$, given the end-point distribution $X$. We give a\ndetailed description of the asymptotic behavior of the diffusion estimator and\nprovide sufficient conditions for the diffusion estimator to be strongly\nconsistent. Furthermore, we present a smeary central limit theorem for\ndiffusion means and investigate properties of the diffusion mean for\ndistributions on the sphere $\\mathcal{S}^n$. Experimentally, we consider\nsimulated data and data from magnetic pole reversals, all indicating similar or\nimproved convergence rate compared to the Fr\\'echet mean. Here, we additionally\nestimate $t$ and consider its effects on smeariness and uniqueness of the\ndiffusion mean for distributions on the sphere.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 16:46:08 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Hansen", "Pernille", ""], ["Eltzner", "Benjamin", ""], ["Huckemann", "Stephan F.", ""], ["Sommer", "Stefan", ""]]}, {"id": "2105.12081", "submitter": "Ryan Thompson", "authors": "Ryan Thompson and Farshid Vahid", "title": "Group selection and shrinkage with application to sparse semiparametric\n  modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse regression and classification estimators capable of group selection\nhave application to an assortment of statistical problems, from multitask\nlearning to sparse additive modeling to hierarchical selection. This work\nintroduces a class of group-sparse estimators that combine group subset\nselection with group lasso or ridge shrinkage. We develop an optimization\nframework for fitting the nonconvex regularization surface and present\nfinite-sample error bounds for estimation of the regression function. Our\nmethods and analyses accommodate the general setting where groups overlap. As\nan application of group selection, we study sparse semiparametric modeling, a\nprocedure that allows the effect of each predictor to be zero, linear, or\nnonlinear. For this task, the new estimators improve across several metrics on\nsynthetic data compared to alternatives. Finally, we demonstrate their efficacy\nin modeling supermarket foot traffic and economic recessions using many\npredictors. All of our proposals are made available in the scalable\nimplementation grpsel.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 17:00:25 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Thompson", "Ryan", ""], ["Vahid", "Farshid", ""]]}, {"id": "2105.12120", "submitter": "Upasana Dutta", "authors": "Upasana Dutta, Aaron Clauset", "title": "Convergence criteria for sampling random graphs with specified degree\n  sequences", "comments": "13 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.data-an stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The configuration model is a standard tool for generating random graphs with\na specified degree sequence, and is often used as a null model to evaluate how\nmuch of an observed network's structure is explained by its degrees alone.\nExcept for networks with both self-loops and multi-edges, we lack a direct\nsampling algorithm for the configuration model, e.g., for simple graphs. A\nMarkov chain Monte Carlo (MCMC) algorithm, based on a degree-preserving\ndouble-edge swap, provides an asymptotic solution to sample from the\nconfiguration model without bias. However, accurately detecting convergence of\nthis Markov chain on its stationary distribution remains an unsolved problem.\nHere, we provide a concrete solution to detect convergence and sample from the\nconfiguration model without bias. We first develop an algorithm for estimating\na sufficient gap between sampled MCMC states for them to be effectively\nindependent. Applying this algorithm to a corpus of 509 empirical networks, we\nderive a set of computationally efficient heuristics, based on scaling laws,\nfor choosing this sampling gap automatically. We then construct a convergence\ndetection method that applies a Kolmogorov-Smirnov test to sequences of network\nassortativity values derived from the Markov chain's sampled states. Comparing\nthis test to three generic Markov chain convergence diagnostics, we find that\nour method is both more accurate and more efficient at detecting convergence.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 05:04:49 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Dutta", "Upasana", ""], ["Clauset", "Aaron", ""]]}, {"id": "2105.12230", "submitter": "Benedikt Kriegesmann", "authors": "Benedikt Kriegesmann and Julian K. L\\\"udeker", "title": "Reciprocal first-order second-moment method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper shows a simple parameter substitution, which makes use of the\nreciprocal relation of typical objective functions with typical random\nparameters. Thereby, the accuracy of first-order probabilistic analysis\nimproves significantly at almost no additional computational cost. The\nparameter substitution requires a transformation of the stochastic distribution\nof the substituted parameter, which is explained for different cases.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 21:41:37 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Kriegesmann", "Benedikt", ""], ["L\u00fcdeker", "Julian K.", ""]]}, {"id": "2105.12259", "submitter": "Daniel Rodriguez Duque", "authors": "Daniel Rodriguez Duque, David A. Stephens, Erica E.M. Moodie", "title": "Estimation of Optimal Dynamic Treatment Regimes via Gaussian Process\n  Emulation: A Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference of treatment effects is a challenging undertaking in it of\nitself; inference for sequential treatments leads to even more hurdles. In\nprecision medicine, one additional ambitious goal may be to infer about effects\nof dynamic treatment regimes (DTRs) and to identify optimal DTRs. Conventional\nmethods for inferring about DTRs involve powerful semi-parametric estimators.\nHowever, these are not without their strong assumptions. Dynamic Marginal\nStructural Models (MSMs) are one semi-parametric approach used to infer about\noptimal DTRs in a family of regimes. To achieve this, investigators are forced\nto model the expected outcome under adherence to a DTR in the family;\nrelatively straightforward models may lead to bias in the optimum. One way to\nobviate this difficulty is to perform a grid search for the optimal DTR.\nUnfortunately, this approach becomes prohibitive as the complexity of regimes\nconsidered increases. In recently developed Bayesian methods for dynamic MSMs,\ncomputational challenges may be compounded by the fact that at each grid point,\na posterior mean must be calculated. We propose a manner by which to alleviate\nmodelling difficulties for DTRs by using Gaussian process optimization. More\nprecisely, we show how to pair this optimization approach with robust\nestimators for the causal effect of adherence to a DTR to identify optimal\nDTRs. We examine how to find the optimum in complex, multi-modal settings which\nare not generally addressed in the DTR literature. We further evaluate the\nsensitivity of the approach to a variety of modeling assumptions in the\nGaussian process.\n", "versions": [{"version": "v1", "created": "Tue, 25 May 2021 23:34:06 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Duque", "Daniel Rodriguez", ""], ["Stephens", "David A.", ""], ["Moodie", "Erica E. M.", ""]]}, {"id": "2105.12286", "submitter": "Amadou Barry", "authors": "Amadou Barry, Nikhil Bhagwat, Bratislav Misic, Jean-Baptiste Poline\n  and Celia M. T. Greenwood", "title": "An algorithm-based multiple detection influence measure for high\n  dimensional regression using expectile", "comments": "38 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of influential observations is an important part of data\nanalysis that can prevent erroneous conclusions drawn from biased estimators.\nHowever, in high dimensional data, this identification is challenging.\nClassical and recently-developed methods often perform poorly when there are\nmultiple influential observations in the same dataset. In particular, current\nmethods can fail when there is masking several influential observations with\nsimilar characteristics, or swamping when the influential observations are near\nthe boundary of the space spanned by well-behaved observations. Therefore, we\npropose an algorithm-based, multi-step, multiple detection procedure to\nidentify influential observations that addresses current limitations. Our\nthree-step algorithm to identify and capture undesirable variability in the\ndata, $\\asymMIP,$ is based on two complementary statistics, inspired by\nasymmetric correlations, and built on expectiles. Simulations demonstrate\nhigher detection power than competing methods. Use of the resulting asymptotic\ndistribution leads to detection of influential observations without the need\nfor computationally demanding procedures such as the bootstrap. The application\nof our method to the Autism Brain Imaging Data Exchange neuroimaging dataset\nresulted in a more balanced and accurate prediction of brain maturity based on\ncortical thickness. See our GitHub for a free R package that implements our\nalgorithm: \\texttt{asymMIP} (\\url{github.com/AmBarry/hidetify}).\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 01:16:24 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Barry", "Amadou", ""], ["Bhagwat", "Nikhil", ""], ["Misic", "Bratislav", ""], ["Poline", "Jean-Baptiste", ""], ["Greenwood", "Celia M. T.", ""]]}, {"id": "2105.12325", "submitter": "Kattumannil Sudheesh Dr", "authors": "Sreedevi E. P., Sudheesh K. K. and Isha Dewan", "title": "A non-parametric test for testing independence between time to failure\n  and cause of failure of discrete competing risks data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Competing risks data with discrete lifetime comes up in practice. However,\nonly limited literature exists for such data. In this paper, we propose a\nnon-parametric test based on U-statistics for testing independence of time to\nfailure and cause of failure of competing risks data when the lifetime is a\ndiscrete random variable. Asymptotic distribution of the proposed test\nstatistic is derived. An extensive Monte Carlo simulation study is conducted to\nassess the finite sample performance of the proposed test. The flexibility of\nthe testing procedure is illustrated using real data sets on oral cancer\npatients and drug exposed pregnancies.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 04:53:34 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["P.", "Sreedevi E.", ""], ["K.", "Sudheesh K.", ""], ["Dewan", "Isha", ""]]}, {"id": "2105.12488", "submitter": "Neil Chada", "authors": "Neil K. Chada, Lassi Roininen, Jarkko Suuronen", "title": "Cauchy Markov Random Field Priors for Bayesian Inversion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of Cauchy Markov random field priors in statistical inverse problems\ncan potentially lead to posterior distributions which are non-Gaussian,\nhigh-dimensional, multimodal and heavy-tailed. In order to use such priors\nsuccessfully, sophisticated optimization and Markov chain Monte Carlo (MCMC)\nmethods are usually required. In this paper, our focus is largely on reviewing\nrecently developed Cauchy difference priors, while introducing interesting new\nvariants, whilst providing a comparison. We firstly propose a one-dimensional\nsecond order Cauchy difference prior, and construct new first and second order\ntwo-dimensional isotropic Cauchy difference priors. Another new Cauchy prior is\nbased on the stochastic partial differential equation approach, derived from\nMat\\'{e}rn type Gaussian presentation. The comparison also includes Cauchy\nsheets. Our numerical computations are based on both maximum a posteriori and\nconditional mean estimation.We exploit state-of-the-art MCMC methodologies such\nas Metropolis-within-Gibbs, Repelling-Attracting Metropolis, and No-U-Turn\nsampler variant of Hamiltonian Monte Carlo. We demonstrate the models and\nmethods constructed for one-dimensional and two-dimensional deconvolution\nproblems. Thorough MCMC statistics are provided for all test cases, including\npotential scale reduction factors.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 11:46:48 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chada", "Neil K.", ""], ["Roininen", "Lassi", ""], ["Suuronen", "Jarkko", ""]]}, {"id": "2105.12704", "submitter": "Juan Nelson Mart\\'inez Dahbura", "authors": "Juan Nelson Mart\\'inez Dahbura, Shota Komatsu, Takanori Nishida and\n  Angelo Mele", "title": "A Structural Model of Business Card Exchange Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.SI stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social and professional networks affect labor market dynamics, knowledge\ndiffusion and new business creation. To understand the determinants of how\nthese networks are formed in the first place, we analyze a unique dataset of\nbusiness cards exchanges among a sample of over 240,000 users of the\nmulti-platform contact management and professional social networking tool for\nindividuals Eight. We develop a structural model of network formation with\nstrategic interactions, and we estimate users' payoffs that depend on the\ncomposition of business relationships, as well as indirect business\ninteractions. We allow heterogeneity of users in both observable and\nunobservable characteristics to affect how relationships form and are\nmaintained. The model's stationary equilibrium delivers a likelihood that is a\nmixture of exponential random graph models that we can characterize in\nclosed-form. We overcome several econometric and computational challenges in\nestimation, by exploiting a two-step estimation procedure, variational\napproximations and minorization-maximization methods. Our algorithm is\nscalable, highly parallelizable and makes efficient use of computer memory to\nallow estimation in massive networks. We show that users payoffs display\nhomophily in several dimensions, e.g. location; furthermore, users unobservable\ncharacteristics also display homophily.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:33:50 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 09:50:46 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Dahbura", "Juan Nelson Mart\u00ednez", ""], ["Komatsu", "Shota", ""], ["Nishida", "Takanori", ""], ["Mele", "Angelo", ""]]}, {"id": "2105.12720", "submitter": "Denis Talbot", "authors": "Awa Diop, Caroline Sirois, Jason Robert Guertin and Denis Talbot", "title": "Marginal structural models with Latent Class Growth Modeling of\n  Treatment Trajectories", "comments": "12 pages, 2 figures et 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a real-life setting, little is known regarding the effectiveness of\nstatins for primary prevention among older adults, and analysis of\nobservational data can add crucial information on the benefits of actual\npatterns of use. Latent class growth models (LCGM) are increasingly proposed as\na solution to summarize the observed longitudinal treatment in a few distinct\ngroups. When combined with standard approaches like Cox proportional hazards\nmodels, LCGM can fail to control time-dependent confounding bias because of\ntime-varying covariates that have a double role of confounders and mediators.\nWe propose to use LCGM to classify individuals into a few latent classes based\non their medication adherence pattern, then choose a working marginal\nstructural model (MSM) that relates the outcome to these groups. The parameter\nof interest is nonparametrically defined as the projection of the true MSM onto\nthe chosen working model. The combination of LCGM with MSM is a convenient way\nto describe treatment adherence and can effectively control time-dependent\nconfounding. Simulation studies were used to illustrate our approach and\ncompare it with unadjusted, baseline covariates-adjusted, time-varying\ncovariates adjusted and inverse probability of trajectory groups weighting\nadjusted models. We found that our proposed approach yielded estimators with\nlittle or no bias.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 17:56:35 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Diop", "Awa", ""], ["Sirois", "Caroline", ""], ["Guertin", "Jason Robert", ""], ["Talbot", "Denis", ""]]}, {"id": "2105.12730", "submitter": "Aaron King", "authors": "Aaron A. King, Qianying Lin, Edward L. Ionides", "title": "Markov Genealogy Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR q-bio.PE q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a family of genealogy-valued Markov processes that are induced\nby a continuous-time Markov population process. We derive exact expressions for\nthe likelihood of a given genealogy conditional on the history of the\nunderlying population process. These lead to a version of the nonlinear\nfiltering equation, which can be used to design efficient Monte Carlo inference\nalgorithms. Existing full-information approaches for phylodynamic inference are\nspecial cases of the theory.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 10:36:09 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["King", "Aaron A.", ""], ["Lin", "Qianying", ""], ["Ionides", "Edward L.", ""]]}, {"id": "2105.12852", "submitter": "Marco Berrettini", "authors": "Marco Berrettini, Giuliano Galimberti, Saverio Ranciati and Thomas\n  Brendan Murphy", "title": "Flexible Bayesian modelling of concomitant covariate effects in mixture\n  models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mixture models provide a useful tool to account for unobserved heterogeneity\nand are at the basis of many model-based clustering methods. In order to gain\nadditional flexibility, some model parameters can be expressed as functions of\nconcomitant covariates. In particular, component weights of the mixture can be\nlinked to the covariates through a multinomial logistic regression model, where\neach component weight is a function of the linear predictor involving one or\nmore covariates. The proposed contribution extends this approach by replacing\nthe linear predictor, used for the component weights, with an additive\nstructure, where each term is a smooth function of the covariates considered.\nAn estimation procedure within the Bayesian paradigm is suggested. In\nparticular, a data augmentation scheme based on differenced random utility\nmodels is exploited, and smoothness of the covariate effects is controlled by\nsuitable choices for the prior distributions of the spline coefficients. The\nperformance of the proposed methodology is investigated via simulation\nexperiments, and an application to an original dataset about UK parliamentary\nvotes on Brexit is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 26 May 2021 21:22:16 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Berrettini", "Marco", ""], ["Galimberti", "Giuliano", ""], ["Ranciati", "Saverio", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "2105.12893", "submitter": "Yuanlu Bai", "authors": "Yuanlu Bai and Tucker Balch and Haoxian Chen and Danial Dervovic and\n  Henry Lam and Svitlana Vyetrenko", "title": "Calibrating Over-Parametrized Simulation Models: A Framework via\n  Eligibility Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CE cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic simulation aims to compute output performance for complex models\nthat lack analytical tractability. To ensure accurate prediction, the model\nneeds to be calibrated and validated against real data. Conventional methods\napproach these tasks by assessing the model-data match via simple hypothesis\ntests or distance minimization in an ad hoc fashion, but they can encounter\nchallenges arising from non-identifiability and high dimensionality. In this\npaper, we investigate a framework to develop calibration schemes that satisfy\nrigorous frequentist statistical guarantees, via a basic notion that we call\neligibility set designed to bypass non-identifiability via a set-based\nestimation. We investigate a feature extraction-then-aggregation approach to\nconstruct these sets that target at multivariate outputs. We demonstrate our\nmethodology on several numerical examples, including an application to\ncalibration of a limit order book market simulator (ABIDES).\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 00:59:29 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Bai", "Yuanlu", ""], ["Balch", "Tucker", ""], ["Chen", "Haoxian", ""], ["Dervovic", "Danial", ""], ["Lam", "Henry", ""], ["Vyetrenko", "Svitlana", ""]]}, {"id": "2105.12894", "submitter": "Chaofan Huang", "authors": "Chaofan Huang, Simin Ma, Shihao Yang", "title": "MAGI-X: Manifold-Constrained Gaussian Process Inference for Unknown\n  System Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary differential equations (ODEs), commonly used to characterize the\ndynamic systems, are difficult to propose in closed-form for many complicated\nscientific applications, even with the help of domain expert. We propose a fast\nand accurate data-driven method, MAGI-X, to learn the unknown dynamic from the\nobservation data in a non-parametric fashion, without the need of any domain\nknowledge. Unlike the existing methods that mainly rely on the costly numerical\nintegration, MAGI-X utilizes the powerful functional approximator of neural\nnetwork to learn the unknown nonlinear dynamic within the MAnifold-constrained\nGaussian process Inference (MAGI) framework that completely circumvents the\nnumerical integration. Comparing against the state-of-the-art methods on three\nrealistic examples, MAGI-X achieves competitive accuracy in both fitting and\nforecasting while only taking a fraction of computational time. Moreover,\nMAGI-X provides practical solution for the inference of partial observed\nsystems, which no previous method is able to handle.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 01:01:40 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 03:28:33 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Huang", "Chaofan", ""], ["Ma", "Simin", ""], ["Yang", "Shihao", ""]]}, {"id": "2105.12901", "submitter": "Sarah Pirikahu", "authors": "Sarah Pirikahu, Geoffrey Jones and Martin Hazelton", "title": "Bayesian Inference for Population Attributable Measures from\n  Under-identified Models", "comments": "26 pages, 5 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Population attributable risk (PAR) is used in epidemiology to predict the\nimpact of removing a risk factor from the population. Until recently, no\nstandard approach for calculating confidence intervals or the variance for PAR\nwas available in the literature. Pirikahu et al. (2016) outlined a fully\nBayesian approach to provide credible intervals for the PAR from a\ncross-sectional study, where the data was presented in the form of a 2 x 2\ntable. However, extensions to cater for other frequently used study designs\nwere not provided. In this paper we provide methodology to calculate credible\nintervals for the PAR for case-control and cohort studies. Additionally, we\nextend the cross-sectional example to allow for the incorporation of\nuncertainty that arises when an imperfect diagnostic test is used. In all these\nsituations the model becomes over-parameterised, or non-identifiable, which can\nresult in standard \"off-the-shelf\" Markov chain Monte Carlo updaters taking a\nlong time to converge or even failing altogether. We adapt an importance\nsampling methodology to overcome this problem, and propose some novel MCMC\nsamplers that take into consideration the shape of the posterior ridge to aid\nin the convergence of the Markov chain.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 01:32:07 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Pirikahu", "Sarah", ""], ["Jones", "Geoffrey", ""], ["Hazelton", "Martin", ""]]}, {"id": "2105.12921", "submitter": "Yukun Liu", "authors": "Hairu Wang, Zhiping Lu and Yukun Liu", "title": "Score test for missing at random or not", "comments": "22 pages, 4 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Missing data are frequently encountered in various disciplines and can be\ndivided into three categories: missing completely at random (MCAR), missing at\nrandom (MAR) and missing not at random (MNAR). Valid statistical approaches to\nmissing data depend crucially on correct identification of the underlying\nmissingness mechanism. Although the problem of testing whether this mechanism\nis MCAR or MAR has been extensively studied, there has been very little\nresearch on testing MAR versus MNAR.A critical challenge that is faced when\ndealing with this problem is the issue of model identification under MNAR. In\nthis paper, under a logistic model for the missing probability, we develop two\nscore tests for the problem of whether the missingness mechanism is MAR or MNAR\nunder a parametric model and a semiparametric location model on the regression\nfunction. The score tests require only parameter estimation under the null MAR\nassumption, which completely circumvents the identification issue. Our\nsimulations and analysis of human immunodeficiency virus data show that the\nscore tests have well-controlled type I errors and desirable powers.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 03:02:37 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Wang", "Hairu", ""], ["Lu", "Zhiping", ""], ["Liu", "Yukun", ""]]}, {"id": "2105.13008", "submitter": "Tao Li", "authors": "Zixuan Han, Tao Li, Jinhong You", "title": "Individual Heterogeneity Learning in Distributional Data Response\n  Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many complex applications, data heterogeneity and homogeneity exist\nsimultaneously. Ignoring either one will result in incorrect statistical\ninference. In addition, coping with complex data that are non-Euclidean becomes\nmore common. To address these issues we consider a distributional data response\nadditive model in which the response is a distributional density function and\nthe individual effect curves are homogeneous within a group but heterogeneous\nacross groups, the covariates capturing the variation share common additive\nbivariate functions. A transformation approach is first utilized to map density\nfunctions into a linear space. We then apply the B-spline series approximating\nmethod to estimate the unknown subject-specific and additive bivariate\nfunctions, and identify the latent group structures by hierarchical\nagglomerative clustering (HAC) algorithm. Our method is demonstrated to\nidentify the true latent group structures with probability approaching one. To\nimprove the efficiency, we further construct the backfitted local linear\nestimators for grouped structures and additive bivariate functions in\npost-grouping model. We establish the asymptotic properties of the resultant\nestimators including the convergence rates, asymptotic distributions and the\npost-grouping oracle efficiency. The performance of the proposed method is\nillustrated by simulation studies and empirical analysis with some interesting\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 08:52:33 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Han", "Zixuan", ""], ["Li", "Tao", ""], ["You", "Jinhong", ""]]}, {"id": "2105.13059", "submitter": "Jeremie Coullon", "authors": "Jeremie Coullon, Leah South, Christopher Nemeth", "title": "Stochastic Gradient MCMC with Multi-Armed Bandit Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic gradient Markov chain Monte Carlo (SGMCMC) is a popular class of\nalgorithms for scalable Bayesian inference. However, these algorithms include\nhyperparameters such as step size or batch size that influence the accuracy of\nestimators based on the obtained samples. As a result, these hyperparameters\nmust be tuned by the practitioner and currently no principled and automated way\nto tune them exists. Standard MCMC tuning methods based on acceptance rates\ncannot be used for SGMCMC, thus requiring alternative tools and diagnostics. We\npropose a novel bandit-based algorithm that tunes SGMCMC hyperparameters to\nmaximize the accuracy of the posterior approximation by minimizing the kernel\nStein discrepancy (KSD). We provide theoretical results supporting this\napproach and assess alternative metrics to KSD. We support our results with\nexperiments on both simulated and real datasets, and find that this method is\npractical for a wide range of application areas.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 11:00:31 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 13:49:38 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Coullon", "Jeremie", ""], ["South", "Leah", ""], ["Nemeth", "Christopher", ""]]}, {"id": "2105.13080", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Wenceslao Gonz\\'alez-Manteiga, Rosa M. Crujeiras, Eduardo\n  Garc\\'ia-Portugu\\'es", "title": "A review of goodness-of-fit tests for models involving functional data", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A sizable amount of goodness-of-fit tests involving functional data have\nappeared in the last decade. We provide a relatively compact revision of most\nof these contributions, within the independent and identically distributed\nframework, by reviewing goodness-of-fit tests for distribution and regression\nmodels with functional predictor and either scalar or functional response.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 12:07:47 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["Crujeiras", "Rosa M.", ""], ["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""]]}, {"id": "2105.13081", "submitter": "Wagner Barreto-Souza", "authors": "Raanju R. Sundararajan and Wagner Barreto-Souza", "title": "Student-t Stochastic Volatility Model With Composite Likelihood\n  EM-Algorithm", "comments": "Paper submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new robust stochastic volatility (SV) model having Student-t marginals is\nproposed. Our process is defined through a linear normal regression model\ndriven by a latent gamma process that controls temporal dependence. This gamma\nprocess is strategically chosen to enable us to find an explicit expression for\nthe pairwise joint density function of the Student-t response process. With\nthis at hand, we propose a composite likelihood (CL) based inference for our\nmodel, which can be straightforwardly implemented with a low computational\ncost. This is a remarkable feature of our Student-t SV process over existing SV\nmodels in the literature that involve computationally heavy algorithms for\nestimating parameters. Aiming at a precise estimation of the parameters related\nto the latent process, we propose a CL Expectation-Maximization algorithm and\ndiscuss a bootstrap approach to obtain standard errors. The finite-sample\nperformance of our composite likelihood methods is assessed through Monte Carlo\nsimulations. The methodology is motivated by an empirical application in the\nfinancial market. We analyze the relationship, across multiple time periods,\nbetween various US sector Exchange-Traded Funds returns and individual\ncompanies' stock price returns based on our novel Student-t model. This\nrelationship is further utilized in selecting optimal financial portfolios.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 12:09:11 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Sundararajan", "Raanju R.", ""], ["Barreto-Souza", "Wagner", ""]]}, {"id": "2105.13148", "submitter": "Xiang Meng", "authors": "Xiang Meng, Jonathan Huang", "title": "Doubly robust, machine learning effect estimation in real-world clinical\n  sciences: A practical evaluation of performance in molecular epidemiology\n  cohort settings", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern efficient estimators such as AIPW and TMLE facilitate the application\nof flexible, non-parametric machine learning algorithms to improve treatment\nand outcome model fit, allowing for some model misspecification while still\nmaintaining desired bias and variance properties. Recent simulation work has\npointed to essential conditions for effective application including: the need\nfor cross-fitting, using of a broad library of well-tuned, flexible learners,\nand sufficiently large sample sizes. In these settings,cross-fit, doubly robust\nestimators fit with machine learning appear to be clearly superior to\nconventional alternatives. However, commonly simulated conditions differ in\nimportant ways from settings in which these estimators may be most useful,\nnamely in high-dimensional, observational settings where: costs of measurements\nlimit sample size, high numbers of covariates may only contain a subset of true\nconfounders, and where model misspecification may include the omission of\nessential biological interactions. In such settings, computationally-intensive\nand challenging to optimize cross-fit, ensemble learning-based estimators may\nhave less of a practical advantage. We present extensive simulation results\ndrawing data on 331 covariates from 1178 subjects of a multi-omic, longitudinal\nbirth cohort while fixing treatment and outcome effects. We fit models under\nvarious conditions including under- and over- (e.g. excess orthogonal\ncovariates) specification, and missing interactions using both state-of-the-art\nand less-computationally intensive (e.g. singly-fit,parametric) estimators. In\nreal data structures, we find in nearly every scenario (e.g. model\nmisspecification, single- or cross-fit- estimators), that efficient estimators\nfit with parametric learner out perform those that include non-parametric\nlearners on the basis of bias and coverage.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 13:50:27 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 07:11:27 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Meng", "Xiang", ""], ["Huang", "Jonathan", ""]]}, {"id": "2105.13346", "submitter": "Joshua Agterberg", "authors": "Joshua Agterberg, Zachary Lubberts, and Carey Priebe", "title": "Entrywise Estimation of Singular Vectors of Low-Rank Matrices with\n  Heteroskedasticity and Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.OC math.SP stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an estimator for the singular vectors of high-dimensional low-rank\nmatrices corrupted by additive subgaussian noise, where the noise matrix is\nallowed to have dependence within rows and heteroskedasticity between them. We\nprove finite-sample $\\ell_{2,\\infty}$ bounds and a Berry-Esseen theorem for the\nindividual entries of the estimator, and we apply these results to\nhigh-dimensional mixture models. Our Berry-Esseen theorem clearly shows the\ngeometric relationship between the signal matrix, the covariance structure of\nthe noise, and the distribution of the errors in the singular vector estimation\ntask. These results are illustrated in numerical simulations. Unlike previous\nresults of this type, which rely on assumptions of gaussianity or independence\nbetween the entries of the additive noise, handling the dependence between\nentries in the proofs of these results requires careful leave-one-out analysis\nand conditioning arguments. Our results depend only on the signal-to-noise\nratio, the sample size, and the spectral properties of the signal matrix.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 17:51:50 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 13:29:54 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Agterberg", "Joshua", ""], ["Lubberts", "Zachary", ""], ["Priebe", "Carey", ""]]}, {"id": "2105.13407", "submitter": "Yan Sun", "authors": "Yan Sun, Shihao Yang", "title": "Manifold-constrained Gaussian process inference for time-varying\n  parameters in dynamic systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of parameters in ordinary differential equations (ODEs) is an\nimportant and challenging task when modeling dynamic systems in biomedical\nresearch and other scientific areas, especially with the presence of\ntime-varying parameters. This article proposes a fast and accurate method,\nTVMAGI (Time-Varying MAnifold-constrained Gaussian process Inference), to\nestimate both time-constant and time-varying parameters in the ODE using noisy\nand sparse observation data. TVMAGI imposes a Gaussian process model over the\ntime series of system components as well as time-varying parameters, and\nrestricts the derivative process to satisfy ODE conditions. Consequently,\nTVMAGI completely bypasses numerical integration and achieves substantial\nsavings in computation time. By incorporating the ODE structures through\nmanifold constraints, TVMAGI enjoys a principled statistical construction under\nthe Bayesian paradigm, which further enables it to handle systems with missing\ndata or unobserved components. The Gaussian process prior also alleviates the\nidentifiability issue often associated with the time-varying parameters in ODE.\nUnlike existing approaches, TVMAGI assumes no specific linearity of the ODE\nstructure, and can be applied to general nonlinear systems. We demonstrate the\nrobustness and efficiency of our method through three simulation examples,\nincluding an infectious disease compartmental model.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 19:19:20 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Sun", "Yan", ""], ["Yang", "Shihao", ""]]}, {"id": "2105.13419", "submitter": "Henry Lam", "authors": "Henry Lam", "title": "On the Impossibility of Statistically Improving Empirical Optimization:\n  A Second-Order Stochastic Dominance Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the underlying probability distribution in a stochastic optimization is\nobserved only through data, various data-driven formulations have been studied\nto obtain approximate optimal solutions. We show that no such formulations can,\nin a sense, theoretically improve the statistical quality of the solution\nobtained from empirical optimization. We argue this by proving that the\nfirst-order behavior of the optimality gap against the oracle best solution,\nwhich includes both the bias and variance, for any data-driven solution is\nsecond-order stochastically dominated by empirical optimization, as long as\nsuitable smoothness holds with respect to the underlying distribution. We\ndemonstrate this impossibility of improvement in a range of examples including\nregularized optimization, distributionally robust optimization, parametric\noptimization and Bayesian generalizations. We also discuss the connections of\nour results to semiparametric statistical inference and other perspectives in\nthe data-driven optimization literature.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 19:46:24 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Lam", "Henry", ""]]}, {"id": "2105.13469", "submitter": "Max Westphal", "authors": "Max Westphal, Antonia Zapf", "title": "Statistical Inference for Diagnostic Test Accuracy Studies with Multiple\n  Comparisons", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diagnostic accuracy studies assess sensitivity and specificity of a new index\ntest in relation to an established comparator or the reference standard. The\ndevelopment and selection of the index test is usually assumed to be conducted\nprior to the accuracy study. In practice, this is often violated, for instance\nif the choice of the (apparently) best biomarker, model or cutpoint is based on\nthe same data that is used later for validation purposes. In this work, we\ninvestigate several multiple comparison procedures which provide family-wise\nerror rate control for the emerging multiple testing problem. Due to the nature\nof the co-primary hypothesis problem, conventional approaches for multiplicity\nadjustment are too conservative for the specific problem and thus need to be\nadapted. In an extensive simulation study, five multiple comparison procedures\nare compared with regards to statistical error rates in least-favorable and\nrealistic scenarios. This covers parametric and nonparamtric methods and one\nBayesian approach. All methods have been implemented in the new open-source R\npackage DTAmc which allows to reproduce all simulation results. Based on our\nnumerical results, we conclude that the parametric approaches (maxT,\nBonferroni) are easy to apply but can have inflated type I error rates for\nsmall sample sizes. The two investigated Bootstrap procedures, in particular\nthe so-called pairs Bootstrap, allow for a family-wise error rate control in\nfinite samples and in addition have a competitive statistical power.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:09:42 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Westphal", "Max", ""], ["Zapf", "Antonia", ""]]}, {"id": "2105.13476", "submitter": "Marco Antonio Pinto Orellana", "authors": "Marco A. Pinto Orellana and Peyman Mirtaheri and Hugo L. Hammer", "title": "The Complex-Pole Filter Representation (COFRE) for spectral modeling of\n  fNIRS signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM physics.med-ph stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The complex-pole frequency representation (COFRE) is introduced in this paper\nas a new approach for spectrum modeling in biomedical signals. Our method\nallows us to estimate the spectral power density at precise frequencies using\nan array of narrow band-pass filters with single complex poles. Closed-form\nexpressions for the frequency resolution and transient time response of the\nproposed filters have also been formulated. In addition, COFRE filters have a\nconstant time and space complexity allowing their use in real-time\nenvironments. Our model was applied to identify frequency markers that\ncharacterize tinnitus in very-low-frequency oscillations within functional\nnear-infrared spectroscopy (fNIRS) signals. We examined data from six patients\nwith subjective tinnitus and seven healthy participants as a control group. A\nsignificant decrease in the spectrum power was observed in tinnitus patients in\nthe left temporal lobe. In particular, we identified several tinnitus\nsignatures in the spectral hemodynamic information, including (a.) a\nsignificant spectrum difference in one specific harmonic in the\nmetabolic/endothelial frequency region, at 7mHz, for both chromophores and\nhemispheres; and (b.) a significant differences in the range 30-50mHz in the\nneurogenic/myogenic band.\n", "versions": [{"version": "v1", "created": "Thu, 13 May 2021 16:42:00 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Orellana", "Marco A. Pinto", ""], ["Mirtaheri", "Peyman", ""], ["Hammer", "Hugo L.", ""]]}, {"id": "2105.13483", "submitter": "Matteo Guardiani", "authors": "Matteo Guardiani, Philipp Frank, Andrija Kosti\\'c, Gordian Edenhofer,\n  Jakob Roth, Berit Uhlmann, Torsten En{\\ss}lin", "title": "Non-parametric Bayesian Causal Modeling of the SARS-CoV-2 Viral Load\n  Distribution vs. Patient's Age", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The viral load of patients infected with SARS-CoV-2 varies on logarithmic\nscales and possibly with age. Controversial claims have been made in the\nliterature regarding whether the viral load distribution actually depends on\nthe age of the patients. Such a dependence would have implications for the\nCOVID-19 spreading mechanism, the age-dependent immune system reaction, and\nthus for policymaking. We hereby develop a method to analyze viral-load\ndistribution data as a function of the patients' age within a flexible,\nnon-parametric, hierarchical, Bayesian, and causal model. This method can be\napplied to other contexts as well, and for this purpose, it is made freely\navailable. The developed reconstruction method also allows testing for bias in\nthe data. This could be due to, e.g., bias in patient-testing and data\ncollection or systematic errors in the measurement of the viral load. We\nperform these tests by calculating the Bayesian evidence for each implied\npossible causal direction. When applying these tests to publicly available age\nand SARS-CoV-2 viral load data, we find a statistically significant increase in\nthe viral load with age, but only for one of the two analyzed datasets. If we\nconsider this dataset, and based on the current understanding of viral load's\nimpact on patients' infectivity, we expect a non-negligible difference in the\ninfectivity of different age groups. This difference is nonetheless too small\nto justify considering any age group as noninfectious.\n", "versions": [{"version": "v1", "created": "Thu, 27 May 2021 22:35:02 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Guardiani", "Matteo", ""], ["Frank", "Philipp", ""], ["Kosti\u0107", "Andrija", ""], ["Edenhofer", "Gordian", ""], ["Roth", "Jakob", ""], ["Uhlmann", "Berit", ""], ["En\u00dflin", "Torsten", ""]]}, {"id": "2105.13581", "submitter": "Giovanni Maria Merola", "authors": "Giovanni Maria Merola", "title": "Sparse Principal Components Analysis: a Tutorial", "comments": "26 pages, preprint subimitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The topic of this tutorial is Least Squares Sparse Principal Components\nAnalysis (LS SPCA) which is a simple method for computing approximated\nPrincipal Components which are combinations of only a few of the observed\nvariables. Analogously to Principal Components, these components are\nuncorrelated and sequentially best approximate the dataset. The derivation of\nLS SPCA is intuitive for anyone familiar with linear regression. Since LS SPCA\nis based on a different optimality from other SPCA methods and does not suffer\nfrom their serious drawbacks. I will demonstrate on two datasets how useful and\nparsimonious sparse PCs can be computed. An R package for computing LS SPCA is\navailable for download.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 04:23:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Merola", "Giovanni Maria", ""]]}, {"id": "2105.13584", "submitter": "Mohammad Arashi", "authors": "Jarod Smith, Mohammad Arashi, Andriette Bekker", "title": "Empowering Differential Networks Using Bayesian Analysis", "comments": "21 pages, 5 Figures, 7 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differential networks (DN) are important tools for modeling the changes in\nconditional dependencies between multiple samples. A Bayesian approach for\nestimating DNs, from the classical viewpoint, is introduced with a\ncomputationally efficient threshold selection for graphical model\ndetermination. The algorithm separately estimates the precision matrices of the\nDN using the Bayesian adaptive graphical lasso procedure. Synthetic experiments\nillustrate that the Bayesian DN performs exceptionally well in numerical\naccuracy and graphical structure determination in comparison to\nstate-of-the-art methods. The proposed method is applied to South African\nCOVID-$19$ data to investigate the change in DN structure between various\nphases of the pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 04:46:49 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Smith", "Jarod", ""], ["Arashi", "Mohammad", ""], ["Bekker", "Andriette", ""]]}, {"id": "2105.13627", "submitter": "Nicolas Hernandez", "authors": "Nicol\\'as Hern\\'andez, Jairo Cugliari and Julien Jacques", "title": "Simultaneous predictive bands for functional time series using minimum\n  entropy sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.FA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Functional Time Series are sequences of dependent random elements taking\nvalues on some functional space. Most of the research on this domain is focused\non producing a predictor able to forecast the value of the next function having\nobserved a part of the sequence. For this, the Autoregresive Hilbertian process\nis a suitable framework. We address here the problem of constructing\nsimultaneous predictive confidence bands for a stationary functional time\nseries. The method is based on an entropy measure for stochastic processes, in\nparticular functional time series. To construct predictive bands we use a\nfunctional bootstrap procedure that allow us to estimate the prediction law\nthrough the use of pseudo-predictions. Each pseudo-realisation is then\nprojected into a space of finite dimension, associated to a functional basis.\nWe use Reproducing Kernel Hilbert Spaces (RKHS) to represent the functions,\nconsidering then the basis associated to the reproducing kernel. Using a simple\ndecision rule, we classify the points on the projected space among those\nbelonging to the minimum entropy set and those that do not. We push back the\nminimum entropy set to the functional space and construct a band using the\nregularity property of the RKHS. The proposed methodology is illustrated\nthrough artificial and real-world data sets.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 07:05:22 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2021 14:59:30 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Hern\u00e1ndez", "Nicol\u00e1s", ""], ["Cugliari", "Jairo", ""], ["Jacques", "Julien", ""]]}, {"id": "2105.13716", "submitter": "Monica Musio", "authors": "Francesco Bertolino, Monica Musio, Walter Racugno and Laura Ventura", "title": "A new Bayesian discrepancy measure", "comments": "20 pages 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Bayesian Discrepancy Test (BDT) is proposed to evaluate the distance of a\ngiven hypothesis with respect to the available information (prior law and\ndata). The proposed measure of evidence has properties of consistency and\ninvariance. After having presented the similarities and differences between the\nBDT and other Bayesian tests, we proceed with the analysis of some\nmultiparametric case studies, showing the properties of the BDT. Among them\nconceptual and interpretative simplicity, possibility of dealing with complex\ncase studies.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 10:27:44 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Bertolino", "Francesco", ""], ["Musio", "Monica", ""], ["Racugno", "Walter", ""], ["Ventura", "Laura", ""]]}, {"id": "2105.13747", "submitter": "Swarnadip Ghosh", "authors": "Swarnadip Ghosh, Trevor Hastie and Art B. Owen", "title": "Scalable logistic regression with crossed random effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The cost of both generalized least squares (GLS) and Gibbs sampling in a\ncrossed random effects model can easily grow faster than $N^{3/2}$ for $N$\nobservations. Ghosh et al. (2020) develop a backfitting algorithm that reduces\nthe cost to $O(N)$. Here we extend that method to a generalized linear mixed\nmodel for logistic regression. We use backfitting within an iteratively\nreweighted penalized least square algorithm. The specific approach is a version\nof penalized quasi-likelihood due to Schall (1991). A straightforward version\nof Schall's algorithm would also cost more than $N^{3/2}$ because it requires\nthe trace of the inverse of a large matrix. We approximate that quantity at\ncost $O(N)$ and prove that this substitution makes an asymptotically negligible\ndifference. Our backfitting algorithm also collapses the fixed effect with one\nrandom effect at a time in a way that is analogous to the collapsed Gibbs\nsampler of Papaspiliopoulos et al. (2020). We use a symmetric operator that\nfacilitates efficient covariance computation. We illustrate our method on a\nreal dataset from Stitch Fix. By properly accounting for crossed random effects\nwe show that a naive logistic regression could underestimate sampling variances\nby several hundred fold.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 11:32:00 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ghosh", "Swarnadip", ""], ["Hastie", "Trevor", ""], ["Owen", "Art B.", ""]]}, {"id": "2105.13786", "submitter": "Harald Baayen", "authors": "R. Harald Baayen (1), Matteo Fasiolo (2), Simon Wood (3), and Yu-Ying\n  Chuang (1) ((1) Eberhard-Karls University Tuebingen (2) University of Bristol\n  (3) University of Edinburgh)", "title": "A note on the modeling of the effects of experimental time in\n  psycholinguistic experiments", "comments": "24 pages, 6 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Thul et al. (2020) called attention to problems that arise when chronometric\nexperiments implementing specific factorial designs are analysed with the\ngeneralized additive mixed model (henceforth GAMM), using factor smooths to\ncapture trial-to-trial dependencies. From a series of simulations using sine\nwaves representing such dependencies, Thul et al. (2020) draw the conclusion\nthat GAMMs are inappropriate for between-subject designs. They argue that\neffects of experimental time can be safely ignored as noise in statistical\nanalyses when using linear mixed models (LMM).\n  We address the questions raised by Thul et al. (2020), who clearly\ndemonstrated that problems can arise when using factor smooths in combination\nwith factorial designs. We show that the problem they reported does not arise\nwhen using by-smooths. Furthermore, we have traced a bug in the implementation\nof factor smooths in the mgcv package, which will have been removed from\nversion 1.8-36 onwards.\n  To illustrate that GAMMs now produce correct estimates, we report a series of\nsimulation studies implementing by-subject longitudinal effects. Simulations\nincluded both sinusoid time-varying effects (following Thul et al. 2020) and\nrandom longitudinal effects. The maximal LMM emerges as slightly conservative\ncompared to GAMMs, and GAMMs provide estimated coefficients that are less\nvariable across simulation runs. We also report analyses of two experimental\ndatasets in which time-varying effects interact with predictors of theoretical\ninterest. We conclude that GAMMs are an excellent and reliable tool for\nunderstanding chronometric data with time-varying effects, for both blocked and\nunblocked experimental designs.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 12:47:37 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Baayen", "R. Harald", ""], ["Fasiolo", "Matteo", ""], ["Wood", "Simon", ""], ["Chuang", "Yu-Ying", ""]]}, {"id": "2105.13817", "submitter": "Marco Scutari", "authors": "Marco Scutari and Manuel Proissl", "title": "Achieving Fairness with a Simple Ridge Penalty", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a fair linear regression model subject to a user-defined level of\nfairness can be achieved by solving a non-convex quadratic programming\noptimisation problem with quadratic constraints. In this work we propose an\nalternative, more flexible approach to this task that enforces a user-defined\nlevel of fairness by means of a ridge penalty. Our proposal addresses three\nlimitations of the former approach: it produces regression coefficient\nestimates that are more intuitive to interpret; it is mathematically simpler,\nwith a solution that is partly in closed form; and it is easier to extend\nbeyond linear regression. We evaluate both approaches empirically on five\ndifferent data sets, and we find that our proposal provides better goodness of\nfit and better predictive accuracy while being equally effective at achieving\nthe desired fairness level. In addition we highlight a source of bias in the\noriginal experimental evaluation of the non-convex quadratic approach, and we\ndiscuss how our proposal can be extended to a wide range of models.\n", "versions": [{"version": "v1", "created": "Tue, 18 May 2021 15:43:57 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Scutari", "Marco", ""], ["Proissl", "Manuel", ""]]}, {"id": "2105.13952", "submitter": "Yue Wu", "authors": "Yue Wu, Ted Spaide, Kenji Nakamichi, Russell Van Gelder, Aaron Lee", "title": "Generalized Permutation Framework for Testing Model Variable\n  Significance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A common problem in machine learning is determining if a variable\nsignificantly contributes to a model's prediction performance. This problem is\naggravated for datasets, such as gene expression datasets, that suffer the\nworst case of dimensionality: a low number of observations along with a high\nnumber of possible explanatory variables. In such scenarios, traditional\nmethods for testing variable statistical significance or constructing variable\nconfidence intervals do not apply. To address these problems, we developed a\nnovel permutation framework for testing the significance of variables in\nsupervised models. Our permutation framework has three main advantages. First,\nit is non-parametric and does not rely on distributional assumptions or\nasymptotic results. Second, it not only ranks model variables in terms of\nrelative importance, but also tests for statistical significance of each\nvariable. Third, it can test for the significance of the interaction between\nmodel variables. We applied this permutation framework to multi-class\nclassification of the Iris flower dataset and of brain regions in RNA\nexpression data, and using this framework showed variable-level statistical\nsignificance and interactions.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 16:19:43 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Wu", "Yue", ""], ["Spaide", "Ted", ""], ["Nakamichi", "Kenji", ""], ["Van Gelder", "Russell", ""], ["Lee", "Aaron", ""]]}, {"id": "2105.14045", "submitter": "Peter Hoff", "authors": "Peter Hoff", "title": "Bayes-optimal prediction with frequentist coverage control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article illustrates how indirect or prior information can be optimally\nused to construct a prediction region that maintains a target frequentist\ncoverage rate. If the indirect information is accurate, the volume of the\nprediction region is lower on average than that of other regions with the same\ncoverage rate. Even if the indirect information is inaccurate, the resulting\nregion still maintains the target coverage rate. Such a prediction region can\nbe constructed for models that have a complete sufficient statistic, which\nincludes many widely-used parametric and nonparametric models. Particular\nexamples include a Bayes-optimal conformal prediction procedure that maintains\na constant coverage rate across distributions in a nonparametric model, as well\nas a prediction procedure for the normal linear regression model that can\nutilize a regularizing prior distribution, yet maintain a frequentist coverage\nrate that is constant as a function of the model parameters and explanatory\nvariables. No results in this article rely on asymptotic approximations.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 18:23:10 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hoff", "Peter", ""]]}, {"id": "2105.14081", "submitter": "Indeewara Perera", "authors": "Giuseppe Cavaliere, Indeewara Perera and Anders Rahbek", "title": "Specification tests for GARCH processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper develops tests for the correct specification of the conditional\nvariance function in GARCH models when the true parameter may lie on the\nboundary of the parameter space. The test statistics considered are of\nKolmogorov-Smirnov and Cram\\'{e}r-von Mises type, and are based on a certain\nempirical process marked by centered squared residuals. The limiting\ndistributions of the test statistics are not free from (unknown) nuisance\nparameters, and hence critical values cannot be tabulated. A novel bootstrap\nprocedure is proposed to implement the tests; it is shown to be asymptotically\nvalid under general conditions, irrespective of the presence of nuisance\nparameters on the boundary. The proposed bootstrap approach is based on\nshrinking of the parameter estimates used to generate the bootstrap sample\ntoward the boundary of the parameter space at a proper rate. It is simple to\nimplement and fast in applications, as the associated test statistics have\nsimple closed form expressions. A simulation study demonstrates that the new\ntests: (i) have excellent finite sample behavior in terms of empirical\nrejection probabilities under the null as well as under the alternative; (ii)\nprovide a useful complement to existing procedures based on Ljung-Box type\napproaches. Two data examples are considered to illustrate the tests.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 19:50:02 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Cavaliere", "Giuseppe", ""], ["Perera", "Indeewara", ""], ["Rahbek", "Anders", ""]]}, {"id": "2105.14093", "submitter": "Yan Liu", "authors": "Yan Liu, Yuguo Chen", "title": "Variational Inference for Latent Space Models for Dynamic Networks", "comments": "50 pages", "journal-ref": null, "doi": "10.5705/ss.202020.0506", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent space models are popular for analyzing dynamic network data. We\npropose a variational approach to estimate the model parameters as well as the\nlatent positions of the nodes in the network. The variational approach is much\nfaster than Markov chain Monte Carlo algorithms, and is able to handle large\nnetworks. Theoretical properties of the variational Bayes risk of the proposed\nprocedure are provided. We apply the variational method and latent space model\nto simulated data as well as real data to demonstrate its performance.\n", "versions": [{"version": "v1", "created": "Fri, 28 May 2021 20:25:07 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Liu", "Yan", ""], ["Chen", "Yuguo", ""]]}, {"id": "2105.14193", "submitter": "Larry Lacey", "authors": "Laurence F Lacey", "title": "Characterization of the probability and information entropy of a process\n  with an exponentially increasing sample space and its application to the\n  Broad Money Supply", "comments": "26 pages, 17 figures, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST econ.EM math.PR stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There is a random variable (X) with a determined outcome (i.e., X = x0),\np(x0) = 1. Consider x0 to have a discrete uniform distribution over the integer\ninterval [1, s], where the size of the sample space (s) = 1, in the initial\nstate, such that p(x0) = 1. What is the probability of x0 and the associated\ninformation entropy (H), as s increases exponentially? If the sample space\nexpansion occurs at an exponential rate (rate constant = lambda) with time (t)\nand applying time scaling, such that T = lambda x t, gives: p(x0|T)=exp(-T) and\nH(T)=T. The characterization has also been extended to include exponential\nexpansion by means of simultaneous, independent processes, as well as the more\ngeneral multi-exponential case. The methodology was applied to the expansion of\nthe broad money supply of US$ over the period 2001-2019, as a real-world\nexample. At any given time, the information entropy is related to the rate at\nwhich the sample space is expanding. In the context of the expansion of the\nbroad money supply, the information entropy could be considered to be related\nto the \"velocity\" of the expansion of the money supply.\n", "versions": [{"version": "v1", "created": "Fri, 14 May 2021 14:31:54 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lacey", "Laurence F", ""]]}, {"id": "2105.14222", "submitter": "Panos Toulis", "authors": "Panos Toulis, Jacob Bean", "title": "Randomization Inference of Periodicity in Unequally Spaced Time Series\n  with Application to Exoplanet Detection", "comments": "8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The estimation of periodicity is a fundamental task in many scientific areas\nof study. Existing methods rely on theoretical assumptions that the observation\ntimes have equal or i.i.d. spacings, and that common estimators, such as the\nperiodogram peak, are consistent and asymptotically normal. In practice,\nhowever, these assumptions are unrealistic as observation times usually exhibit\ndeterministic patterns -- e.g., the nightly observation cycle in astronomy --\nthat imprint nuisance periodicities in the data. These nuisance signals also\naffect the finite-sample distribution of estimators, which can substantially\ndeviate from normality. Here, we propose a set identification method, fusing\nideas from randomization inference and partial identification. In particular,\nwe develop a sharp test for any periodicity value, and then invert the test to\nbuild a confidence set. This approach is appropriate here because the\nconstruction of confidence sets does not rely on assumptions of regular or\nwell-behaved asymptotics. Notably, our inference is valid in finite samples\nwhen our method is fully implemented, while it can be asymptotically valid\nunder an approximate implementation designed to ease computation. Empirically,\nwe validate our method in exoplanet detection using radial velocity data. In\nthis context, our method correctly identifies the periodicity of the confirmed\nexoplanets in our sample. For some other, yet unconfirmed detections, we show\nthat the statistical evidence is weak, which illustrates the failure of\ntraditional statistical techniques. Last but not least, our method offers a\nconstructive way to resolve these identification issues via improved\nobservation designs. In exoplanet detection, these designs suggest meaningful\nimprovements in identifying periodicity even when a moderate amount of\nrandomization is introduced in scheduling radial velocity measurements.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 05:55:39 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Toulis", "Panos", ""], ["Bean", "Jacob", ""]]}, {"id": "2105.14328", "submitter": "Ye Tian", "authors": "Ye Tian and Yang Feng", "title": "Transfer Learning under High-dimensional Generalized Linear Models", "comments": "52 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the transfer learning problem under high-dimensional\ngeneralized linear models (GLMs), which aim to improve the fit on target data\nby borrowing information from useful source data. Given which sources to\ntransfer, we propose an oracle algorithm and derive its $\\ell_2$-estimation\nerror bounds. The theoretical analysis shows that under certain conditions,\nwhen the target and source are sufficiently close to each other, the estimation\nerror bound could be improved over that of the classical penalized estimator\nusing only target data. When we don't know which sources to transfer, an\nalgorithm-free transferable source detection approach is introduced to detect\ninformative sources. The detection consistency is proved under the\nhigh-dimensional GLM transfer learning setting. Extensive simulations and a\nreal-data experiment verify the effectiveness of our algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 15:39:43 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tian", "Ye", ""], ["Feng", "Yang", ""]]}, {"id": "2105.14348", "submitter": "Liyan Xie", "authors": "Liyan Xie, Rui Gao, and Yao Xie", "title": "Robust Hypothesis Testing with Wasserstein Uncertainty Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a data-driven robust hypothesis test where the optimal test will\nminimize the worst-case performance regarding distributions that are close to\nthe empirical distributions with respect to the Wasserstein distance. This\nleads to a new non-parametric hypothesis testing framework based on\ndistributionally robust optimization, which is more robust when there are\nlimited samples for one or both hypotheses. Such a scenario often arises from\napplications such as health care, online change-point detection, and anomaly\ndetection. We study the computational and statistical properties of the\nproposed test by presenting a tractable convex reformulation of the original\ninfinite-dimensional variational problem exploiting Wasserstein's properties\nand characterizing the radii selection for the uncertainty sets. We also\ndemonstrate the good performance of our method on synthetic and real data.\n", "versions": [{"version": "v1", "created": "Sat, 29 May 2021 18:16:29 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xie", "Liyan", ""], ["Gao", "Rui", ""], ["Xie", "Yao", ""]]}, {"id": "2105.14395", "submitter": "Sanvesh Srivastava", "authors": "Chunlei Wang and Sanvesh Srivastava", "title": "Divide-and-Conquer Bayesian Inference in Hidden Markov Models", "comments": "51 pages and 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Divide-and-conquer Bayesian methods consist of three steps: dividing the data\ninto smaller computationally manageable subsets, running a sampling algorithm\nin parallel on all the subsets, and combining parameter draws from all the\nsubsets. The combined parameter draws are used for efficient posterior\ninference in massive data settings. A major restriction of existing\ndivide-and-conquer methods is that their first two steps assume that the\nobservations are independent. We address this problem by developing a\ndivide-and-conquer method for Bayesian inference in parametric hidden Markov\nmodels, where the state space is known and finite. Our main contributions are\ntwo-fold. First, after partitioning the data into smaller blocks of consecutive\nobservations, we modify the likelihood for performing posterior computations on\nthe subsets such that the posterior variances of the subset and true posterior\ndistributions have the same asymptotic order. Second, if the number of subsets\nis chosen appropriately depending on the mixing properties of the hidden Markov\nchain, then we show that the subset posterior distributions defined using the\nmodified likelihood are asymptotically normal as the subset sample size tends\nto infinity. The latter result also implies that we can use any existing\ncombination algorithm in the third step. We show that the combined posterior\ndistribution obtained using one such algorithm is close to the true posterior\ndistribution in 1-Wasserstein distance under widely used regularity\nassumptions. Our numerical results show that the proposed method provides an\naccurate approximation of the true posterior distribution than its competitors\nin diverse simulation studies and a real data analysis.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 00:16:18 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Chunlei", ""], ["Srivastava", "Sanvesh", ""]]}, {"id": "2105.14502", "submitter": "Arun Kumar", "authors": "Monika Singh Dhull and Arun Kumar", "title": "Normal Inverse Gaussian Autoregressive Model Using EM Algorithm", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, normal inverse Gaussian (NIG) autoregressive model is\nintroduced. The parameters of the model are estimated using Expectation\nMaximization (EM) algorithm. The efficacy of the EM algorithm is shown using\nsimulated and real world financial data. It is shown that NIG autoregressive\nmodel fit very well the considered financial data and hence could be useful in\nmodeling of various real life time-series data.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 11:18:18 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 10:39:49 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Dhull", "Monika Singh", ""], ["Kumar", "Arun", ""]]}, {"id": "2105.14577", "submitter": "Arun Kuchibhotla", "authors": "Arun Kumar Kuchibhotla, Sivaraman Balakrishnan, Larry Wasserman", "title": "The HulC: Confidence Regions from Convex Hulls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and analyze the HulC, an intuitive and general method for\nconstructing confidence sets using the convex hull of estimates constructed\nfrom subsets of the data. Unlike classical methods which are based on\nestimating the (limiting) distribution of an estimator, the HulC is often\nsimpler to use and effectively bypasses this step. In comparison to the\nbootstrap, the HulC requires fewer regularity conditions and succeeds in many\nexamples where the bootstrap provably fails. Unlike subsampling, the HulC does\nnot require knowledge of the rate of convergence of the estimators on which it\nis based. The validity of the HulC requires knowledge of the (asymptotic)\nmedian-bias of the estimators. We further analyze a variant of our basic\nmethod, called the Adaptive HulC, which is fully data-driven and estimates the\nmedian-bias using subsampling. We show that the Adaptive HulC retains the\naforementioned strengths of the HulC. In certain cases where the underlying\nestimators are pathologically asymmetric the HulC and Adaptive HulC can fail to\nprovide useful confidence sets. We propose a final variant, the Unimodal HulC,\nwhich can salvage the situation in cases where the distribution of the\nunderlying estimator is (asymptotically) unimodal. We discuss these methods in\nthe context of several challenging inferential problems which arise in\nparametric, semi-parametric, and non-parametric inference. Although our focus\nis on validity under weak regularity conditions, we also provide some general\nresults on the width of the HulC confidence sets, showing that in many cases\nthe HulC confidence sets have near-optimal width.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 16:21:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""], ["Balakrishnan", "Sivaraman", ""], ["Wasserman", "Larry", ""]]}, {"id": "2105.14590", "submitter": "Ehsan Zamanzade", "authors": "Zeinab Akbari Ghamsari, Ehsan Zamanzade and Majid Asadi", "title": "Statistical Inference from Partially Nominated Sets: An Application to\n  Estimating the Prevalence of Osteoporosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on drawing statistical inference based on a novel variant\nof maxima or minima nomination sampling (NS) designs. These sampling designs\nare useful for obtaining more representative sample units from the tails of the\npopulation distribution using the available auxiliary ranking information.\nHowever, one common difficulty in performing NS in practice is that the\nresearcher cannot obtain a nominated sample unless he/she uniquely determines\nthe sample unit with the highest or the lowest rank in each set. To overcome\nthis problem, a variant of NS which is called partial nomination sampling is\nproposed in which the researcher is allowed to declare that two or more units\nare tied in the ranks whenever he/she cannot find with high confidence the\nsample unit with the highest or the lowest rank with high confidence. Based on\nthis sampling design, two asymptotically unbiased estimators are developed for\nthe cumulative distribution function, which are obtained using maximum\nlikelihood and moment-based approaches, and their asymptotic normality is\nproved. Several numerical studies have shown that the developed estimators have\nhigher relative efficiencies than their counterpart in simple random sampling\nin analyzing either the upper or the lower tail of the parent distribution. The\nprocedures that we are developed are then implemented on a real dataset from\nthe Third National Health and Nutrition Examination Survey (NHANES III) to\nestimate the prevalence of osteoporosis among adult women aged 50 and over. It\nis shown that in some certain circumstances, the techniques that we have\ndeveloped require only one-third of the sample size needed in SRS to achieve\nthe desired precision. This results in a considerable reduction of time and\ncost compared to the standard SRS method.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 17:46:18 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ghamsari", "Zeinab Akbari", ""], ["Zamanzade", "Ehsan", ""], ["Asadi", "Majid", ""]]}, {"id": "2105.14647", "submitter": "Lin Wang", "authors": "Lin Wang, Jake Elmstedt, Weng Kee Wong, Hongquan Xu", "title": "Orthogonal Subsampling for Big Data Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The dramatic growth of big datasets presents a new challenge to data storage\nand analysis. Data reduction, or subsampling, that extracts useful information\nfrom datasets is a crucial step in big data analysis. We propose an orthogonal\nsubsampling (OSS) approach for big data with a focus on linear regression\nmodels. The approach is inspired by the fact that an orthogonal array of two\nlevels provides the best experimental design for linear regression models in\nthe sense that it minimizes the average variance of the estimated parameters\nand provides the best predictions. The merits of OSS are three-fold: (i) it is\neasy to implement and fast; (ii) it is suitable for distributed parallel\ncomputing and ensures the subsamples selected in different batches have no\ncommon data points; and (iii) it outperforms existing methods in minimizing the\nmean squared errors of the estimated parameters and maximizing the efficiencies\nof the selected subsamples. Theoretical results and extensive numerical results\nshow that the OSS approach is superior to existing subsampling approaches. It\nis also more robust to the presence of interactions among covariates and, when\nthey do exist, OSS provides more precise estimates of the interaction effects\nthan existing methods. The advantages of OSS are also illustrated through\nanalysis of real data.\n", "versions": [{"version": "v1", "created": "Sun, 30 May 2021 23:00:00 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Wang", "Lin", ""], ["Elmstedt", "Jake", ""], ["Wong", "Weng Kee", ""], ["Xu", "Hongquan", ""]]}, {"id": "2105.14705", "submitter": "Jiannan Lu", "authors": "Alex Deng, Jiannan Lu and Wen Qin", "title": "The equivalence of the Delta method and the cluster-robust variance\n  estimator for the analysis of clustered randomized experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It often happens that the same problem presents itself to different\ncommunities and the solutions proposed or adopted by those communities are\ndifferent. We take the case of the variance estimation of the population\naverage treatment effect in cluster-randomized experiments. The econometrics\nliterature promotes the cluster-robust variance estimator (Athey and Imbens,\n2017), which can be dated back to the study of linear regression with clustered\nresiduals (Liang and Zeger, 1986). The A/B testing or online experimentation\nliterature promotes the delta method (Kohavi et al., 2010, Deng et al., 2017,\n2018), which tackles the variance estimation of the ATE estimator directly\nusing large sample theory. The two methods are seemly different as the former\nbegins with a regression setting at the individual unit level and the latter is\nsemi-parametric with only i.i.d. assumptions on the clusters. Both methods are\nwidely used in practice. It begs the question for their connection and\ncomparison. In this paper we prove they are equivalent and in the canonical\nimplementation they should give exactly the same result.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 04:53:07 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Deng", "Alex", ""], ["Lu", "Jiannan", ""], ["Qin", "Wen", ""]]}, {"id": "2105.14752", "submitter": "Yichong Zhang", "authors": "Liang Jiang, Peter C.B. Phillips, Yubo Tao, and Yichong Zhang", "title": "Regression-Adjusted Estimation of Quantile Treatment Effects under\n  Covariate-Adaptive Randomizations", "comments": "110 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Datasets from field experiments with covariate-adaptive randomizations (CARs)\nusually contain extra baseline covariates in addition to the strata indicators.\nWe propose to incorporate these extra covariates via auxiliary regressions in\nthe estimation and inference of unconditional QTEs under CARs. We establish the\nconsistency, limiting distribution, and validity of the multiplier bootstrap of\nthe regression-adjusted QTE estimator. The auxiliary regression may be\nestimated parametrically, nonparametrically, or via regularization when the\ndata are high-dimensional. Even when the auxiliary regression is misspecified,\nthe proposed bootstrap inferential procedure still achieves the nominal\nrejection probability in the limit under the null. When the auxiliary\nregression is correctly specified, the regression-adjusted estimator achieves\nthe minimum asymptotic variance. We also derive the optimal pseudo true values\nfor the potentially misspecified parametric model that minimize the asymptotic\nvariance of the corresponding QTE estimator. We demonstrate the finite sample\nperformance of the new estimation and inferential methods using simulations and\nprovide an empirical application to a well-known dataset in education.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 07:33:31 GMT"}, {"version": "v2", "created": "Sat, 3 Jul 2021 12:33:07 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Jiang", "Liang", ""], ["Phillips", "Peter C. B.", ""], ["Tao", "Yubo", ""], ["Zhang", "Yichong", ""]]}, {"id": "2105.14877", "submitter": "Thanh Vinh Vo", "authors": "Thanh Vinh Vo, Pengfei Wei, Trong Nghia Hoang, Tze-Yun Leong", "title": "Adaptive Multi-Source Causal Inference", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data scarcity is a tremendous challenge in causal effect estimation. In this\npaper, we propose to exploit additional data sources to facilitate estimating\ncausal effects in the target population. Specifically, we leverage additional\nsource datasets which share similar causal mechanisms with the target\nobservations to help infer causal effects of the target population. We propose\nthree levels of knowledge transfer, through modelling the outcomes, treatments,\nand confounders. To achieve consistent positive transfer, we introduce\nlearnable parametric transfer factors to adaptively control the transfer\nstrength, and thus achieving a fair and balanced knowledge transfer between the\nsources and the target. The proposed method can infer causal effects in the\ntarget population without prior knowledge of data discrepancy between the\nadditional data sources and the target. Experiments on both synthetic and\nreal-world datasets show the effectiveness of the proposed method as compared\nwith recent baselines.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 11:02:37 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Vo", "Thanh Vinh", ""], ["Wei", "Pengfei", ""], ["Hoang", "Trong Nghia", ""], ["Leong", "Tze-Yun", ""]]}, {"id": "2105.14957", "submitter": "Chancellor Johnstone", "authors": "Chancellor Johnstone", "title": "Conformal Uncertainty Sets for Robust Optimization", "comments": "20 pages, 8 figures, submitted to COPA 2021, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-making under uncertainty is hugely important for any decisions\nsensitive to perturbations in observed data. One method of incorporating\nuncertainty into making optimal decisions is through robust optimization, which\nminimizes the worst-case scenario over some uncertainty set. We explore\nMahalanobis distance as a novel function for multi-target regression and the\nconstruction of joint prediction regions. We also connect conformal prediction\nregions to robust optimization, providing finite sample valid and conservative\nuncertainty sets, aptly named conformal uncertainty sets. We compare the\ncoverage and efficiency of the conformal prediction regions generated with\nMahalanobis distance to other conformal prediction regions. We also construct a\nsmall robust optimization example to compare conformal uncertainty sets to\nthose constructed under the assumption of normality.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 13:42:24 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Johnstone", "Chancellor", ""]]}, {"id": "2105.15000", "submitter": "Hang Zhou", "authors": "Hang Zhou, Zhenhua Lin and Fang Yao", "title": "Intrinsic Wasserstein Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a framework of canonical correlation analysis for\ndistribution-valued functional data within the geometry of Wasserstein spaces.\nSpecifically, we formulate an intrinsic concept of correlation between random\ndistributions, propose estimation methods based on functional principal\ncomponent analysis (FPCA) and Tikhonov regularization, respectively, for the\ncorrelation and its corresponding weight functions, and establish the minimax\nconvergence rates of the estimators. The key idea is to extend the framework of\ntensor Hilbert spaces to distribution-valued functional data to overcome the\nchallenging issue raised by nonlinearity of Wasserstein spaces. The\nfinite-sample performance of the proposed estimators is illustrated via\nsimulation studies, and the practical merit is demonstrated via a study on the\nassociation of distributions of brain activities between two brain regions.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 14:32:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zhou", "Hang", ""], ["Lin", "Zhenhua", ""], ["Yao", "Fang", ""]]}, {"id": "2105.15036", "submitter": "Khue-Dung Dang", "authors": "Khue-Dung Dang and Luca Maestrini", "title": "Fitting Structural Equation Models via Variational Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural equation models are commonly used to capture the structural\nrelationship between sets of observed and unobservable variables. In Bayesian\nsettings, fitting and inference for these models are typically performed via\nMarkov chain Monte Carlo methods that may be computationally intensive,\nespecially for models with a large number of manifest variables or complex\nstructures. Variational approximations can be a fast alternative; however they\nhave not been adequately explored for this class of models. We develop a mean\nfield variational Bayes approach for fitting basic structural equation models.\nWe show that this variational approximation method can provide reliable\ninference while being significantly faster than Markov chain Monte Carlo.\nClassical mean field variational Bayes may underestimate the true posterior\nvariance, therefore we propose and study bootstrap to overcome this issue. We\ndiscuss different inference strategies based on bootstrap and demonstrate how\nthese can considerably improve the accuracy of the variational approximation\nthrough real and simulated examples.\n", "versions": [{"version": "v1", "created": "Mon, 31 May 2021 15:18:09 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Dang", "Khue-Dung", ""], ["Maestrini", "Luca", ""]]}]