[{"id": "1301.0118", "submitter": "Aris Spanos", "authors": "Aris Spanos", "title": "The Two Envelope Problem: a Paradox or Fallacious Reasoning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The primary objective of this note is to revisit the two envelope problem and\npropose a simple resolution. It is argued that the paradox arises from the\nambiguity associated with the money content $x of the chosen envelope. When X=x\nis observed it is not know which one of the two events, X={\\theta} or\nX=2{\\theta}, has occurred. Moreover, the money in the other envelope Y is not\nindependent of X; when one contains {\\theta} the other contains 2{\\theta}. By\ntaking these important features of the problem into account, the paradox\ndisappears.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2013 18:44:59 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["Spanos", "Aris", ""]]}, {"id": "1301.0148", "submitter": "Dimitris Kugiumtzis", "authors": "Maria Papapetrou and Dimitris Kugiumtzis", "title": "Markov Chain Order estimation with Conditional Mutual Information", "comments": "16 pages, 3 figures; M. Papapetrou, D. Kugiumtzis, Markov chain order\n  estimation with conditional mutual information, Physica A: Statistical\n  Mechanics and its Applications, Available online 26 December 2012, ISSN\n  0378-4371", "journal-ref": null, "doi": "10.1016/j.physa.2012.12.017.", "report-no": null, "categories": "physics.data-an cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Conditional Mutual Information (CMI) for the estimation of\nthe Markov chain order. For a Markov chain of $K$ symbols, we define CMI of\norder $m$, $I_c(m)$, as the mutual information of two variables in the chain\nbeing $m$ time steps apart, conditioning on the intermediate variables of the\nchain. We find approximate analytic significance limits based on the estimation\nbias of CMI and develop a randomization significance test of $I_c(m)$, where\nthe randomized symbol sequences are formed by random permutation of the\ncomponents of the original symbol sequence. The significance test is applied\nfor increasing $m$ and the Markov chain order is estimated by the last order\nfor which the null hypothesis is rejected. We present the appropriateness of\nCMI-testing on Monte Carlo simulations and compare it to the Akaike and\nBayesian information criteria, the maximal fluctuation method (Peres-Shields\nestimator) and a likelihood ratio test for increasing orders using\n$\\phi$-divergence. The order criterion of CMI-testing turns out to be superior\nfor orders larger than one, but its effectiveness for large orders depends on\ndata availability. In view of the results from the simulations, we interpret\nthe estimated orders by the CMI-testing and the other criteria on genes and\nintergenic regions of DNA chains.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jan 2013 23:54:15 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["Papapetrou", "Maria", ""], ["Kugiumtzis", "Dimitris", ""]]}, {"id": "1301.0211", "submitter": "Jon Wellner", "authors": "Jon Wellner, Tong Zhang", "title": "Introduction to the Special Issue on Sparsity and Regularization Methods", "comments": "Published in at http://dx.doi.org/10.1214/12-STS409 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 4, 447-449", "doi": "10.1214/12-STS409", "report-no": "IMS-STS-STS409", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional statistical inference considers relatively small data sets and\nthe corresponding theoretical analysis focuses on the asymptotic behavior of a\nstatistical estimator when the number of samples approaches infinity. However,\nmany data sets encountered in modern applications have dimensionality\nsignificantly larger than the number of training data available, and for such\nproblems the classical statistical tools become inadequate. In order to analyze\nhigh-dimensional data, new statistical methodology and the corresponding theory\nhave to be developed.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2013 11:24:17 GMT"}], "update_date": "2013-01-03", "authors_parsed": [["Wellner", "Jon", ""], ["Zhang", "Tong", ""]]}, {"id": "1301.0413", "submitter": "Ernie Esser", "authors": "Ernie Esser, Yifei Lou, Jack Xin", "title": "A Method for Finding Structured Sparse Solutions to Non-negative Least\n  Squares Problems with Applications", "comments": "38 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Demixing problems in many areas such as hyperspectral imaging and\ndifferential optical absorption spectroscopy (DOAS) often require finding\nsparse nonnegative linear combinations of dictionary elements that match\nobserved data. We show how aspects of these problems, such as misalignment of\nDOAS references and uncertainty in hyperspectral endmembers, can be modeled by\nexpanding the dictionary with grouped elements and imposing a structured\nsparsity assumption that the combinations within each group should be sparse or\neven 1-sparse. If the dictionary is highly coherent, it is difficult to obtain\ngood solutions using convex or greedy methods, such as non-negative least\nsquares (NNLS) or orthogonal matching pursuit. We use penalties related to the\nHoyer measure, which is the ratio of the $l_1$ and $l_2$ norms, as sparsity\npenalties to be added to the objective in NNLS-type models. For solving the\nresulting nonconvex models, we propose a scaled gradient projection algorithm\nthat requires solving a sequence of strongly convex quadratic programs. We\ndiscuss its close connections to convex splitting methods and difference of\nconvex programming. We also present promising numerical results for example\nDOAS analysis and hyperspectral demixing problems.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2013 10:09:41 GMT"}], "update_date": "2013-01-04", "authors_parsed": [["Esser", "Ernie", ""], ["Lou", "Yifei", ""], ["Xin", "Jack", ""]]}, {"id": "1301.0463", "submitter": "Francisco Javier Rubio Mr.", "authors": "F. J. Rubio, and Adam M. Johansen", "title": "A Simple Approach to Maximum Intractable Likelihood Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian Computation (ABC) can be viewed as an analytic\napproximation of an intractable likelihood coupled with an elementary\nsimulation step. Such a view, combined with a suitable instrumental prior\ndistribution permits maximum-likelihood (or maximum-a-posteriori) inference to\nbe conducted, approximately, using essentially the same techniques. An\nelementary approach to this problem which simply obtains a nonparametric\napproximation of the likelihood surface which is then used as a smooth proxy\nfor the likelihood in a subsequent maximisation step is developed here and the\nconvergence of this class of algorithms is characterised theoretically. The use\nof non-sufficient summary statistics in this context is considered. Applying\nthe proposed method to four problems demonstrates good performance. The\nproposed approach provides an alternative for approximating the maximum\nlikelihood estimator (MLE) in complex scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2013 14:21:23 GMT"}], "update_date": "2013-01-04", "authors_parsed": [["Rubio", "F. J.", ""], ["Johansen", "Adam M.", ""]]}, {"id": "1301.0533", "submitter": "Matthias Troffaes", "authors": "Matthias C. M. Troffaes, Gero Walter, Dana Kelly", "title": "A robust Bayesian approach to modelling epistemic uncertainty in\n  common-cause failure models", "comments": "16 pages", "journal-ref": "Reliability Engineering and System Safety 125 (2014) 13-21", "doi": "10.1016/j.ress.2013.05.022", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a standard Bayesian approach to the alpha-factor model for common-cause\nfailure, a precise Dirichlet prior distribution models epistemic uncertainty in\nthe alpha-factors. This Dirichlet prior is then updated with observed data to\nobtain a posterior distribution, which forms the basis for further inferences.\n  In this paper, we adapt the imprecise Dirichlet model of Walley to represent\nepistemic uncertainty in the alpha-factors. In this approach, epistemic\nuncertainty is expressed more cautiously via lower and upper expectations for\neach alpha-factor, along with a learning parameter which determines how quickly\nthe model learns from observed data. For this application, we focus on\nelicitation of the learning parameter, and find that values in the range of 1\nto 10 seem reasonable. The approach is compared with Kelly and Atwood's\nminimally informative Dirichlet prior for the alpha-factor model, which\nincorporated precise mean values for the alpha-factors, but which was otherwise\nquite diffuse.\n  Next, we explore the use of a set of Gamma priors to model epistemic\nuncertainty in the marginal failure rate, expressed via a lower and upper\nexpectation for this rate, again along with a learning parameter. As zero\ncounts are generally less of an issue here, we find that the choice of this\nlearning parameter is less crucial.\n  Finally, we demonstrate how both epistemic uncertainty models can be combined\nto arrive at lower and upper expectations for all common-cause failure rates.\nThereby, we effectively provide a full sensitivity analysis of common-cause\nfailure rates, properly reflecting epistemic uncertainty of the analyst on all\nlevels of the common-cause failure model.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2013 19:41:18 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Troffaes", "Matthias C. M.", ""], ["Walter", "Gero", ""], ["Kelly", "Dana", ""]]}, {"id": "1301.0550", "submitter": "Ayesha R. Ali", "authors": "Ayesha R. Ali, Thomas S. Richardson", "title": "Markov Equivalence Classes for Maximal Ancestral Graphs", "comments": "Appears in Proceedings of the Eighteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2002)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2002-PG-1-9", "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ancestral graphs are a class of graphs that encode conditional independence\nrelations arising in DAG models with latent and selection variables,\ncorresponding to marginalization and conditioning. However, for any ancestral\ngraph, there may be several other graphs to which it is Markov equivalent. We\nintroduce a simple representation of a Markov equivalence class of ancestral\ngraphs, thereby facilitating model search. \\ More specifically, we define a\njoin operation on ancestral graphs which will associate a unique graph with a\nMarkov equivalence class. We also extend the separation criterion for ancestral\ngraphs (which is an extension of d-separation) and provide a proof of the\npairwise Markov property for joined ancestral graphs.\n", "versions": [{"version": "v1", "created": "Wed, 12 Dec 2012 15:55:00 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Ali", "Ayesha R.", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1301.0681", "submitter": "Abhishek Bhattacharya Dr.", "authors": "Abhishek Bhattacharya", "title": "Nonparametric Bayes Classification via Learning of Affine Subspaces", "comments": "Presented at the ISBA Regional Meeting and International\n  Workshop/Conference on Bayesian Theory and Applications (IWCBTA) organized by\n  the DST Centre for Interdisciplinary Mathematical Sciences, Banaras Hindu\n  University, India on January 10, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this presentation is to build an efficient non-parametric Bayes\nclassifier in the presence of large numbers of predictors. When analyzing such\ndata, parametric models are often too inflexible while non-parametric\nprocedures tend to be non-robust because of insufficient data on these high\ndimensional spaces. When dealing with these types of data, it is often the case\nthat most of the variability tends to lie along a few directions, or more\ngenerally along a much smaller dimensional subspace of the feature space. Hence\na class of regression models is proposed that flexibly learn about this\nsubspace while simultaneously performing dimension reduction in classification.\nThis methodology, allows the cell probabilities to vary non-parametrically\nbased on a few coordinates expressed as linear combinations of the predictors.\nAlso, as opposed to many black-box methods for dimensionality reduction, the\nproposed model is appealing in having clearly interpretable and identifiable\nparameters which provide insight into which predictors are important in\ndetermining accurate classification boundaries. Gibbs sampling methods are\ndeveloped for posterior computations. The estimated cell probabilities are\ntheoretically shown to be consistent, and real data applications are included\nto support the findings.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2013 07:19:23 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Bhattacharya", "Abhishek", ""]]}, {"id": "1301.0741", "submitter": "Giuseppe Arbia", "authors": "Giuseppe Arbia", "title": "A bivariate marginal likelihood specification of spatial econometric\n  modeling of very large datasets", "comments": "Paper presented at the 7th Camp Econometrics conference, Cooperstown,\n  April 13-15, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a bivariate marginal likelihood specification of spatial\neconometrics models that simplifies the derivation of the log-likelihood and\nleads to a closed form expression for the estimation of the parameters. With\nrespect to the more traditional specifications of spatial autoregressive\nmodels, our method avoids the arbitrariness of the specification of a weight\nmatrix, presents analytical and computational advantages and provides\ninteresting interpretative insights. We establish small sample and asymptotic\nproperties of the estimators and we derive the associated Fisher information\nmatrix needed in confidence interval estimation and hypothesis testing.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jan 2013 15:02:48 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Arbia", "Giuseppe", ""]]}, {"id": "1301.0877", "submitter": "Wei Gao", "authors": "Wei Gao, Ping Shing Chan, Hon Keung Tony Ng and Xiaolei Lu", "title": "Efficient Computational Algorithm for Optimal Allocation in Regression\n  Models", "comments": "17 pages and 2 tables, accepted by Journal of Computational and\n  Applied Mathematics in 2013 Journal of Computational and Applied Mathematics\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we discuss the optimal allocation problem in an experiment\nwhen a regression model is used for statistical analysis. Monotonic convergence\nfor a general class of multiplicative algorithms for $D$-optimality has been\ndiscussed in the literature. Here, we provide an alternate proof of the\nmonotonic convergence for $D$-criterion with a simple computational algorithm\nand furthermore show it converges to the $D$-optimality. We also discuss an\nalgorithm as well as a conjecture of the monotonic convergence for\n$A$-criterion. Monte Carlo simulations are used to demonstrate the reliability,\nefficiency and usefulness of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jan 2013 06:39:29 GMT"}, {"version": "v2", "created": "Fri, 25 Oct 2013 15:18:13 GMT"}], "update_date": "2013-10-28", "authors_parsed": [["Gao", "Wei", ""], ["Chan", "Ping Shing", ""], ["Ng", "Hon Keung Tony", ""], ["Lu", "Xiaolei", ""]]}, {"id": "1301.0993", "submitter": "Georgiy Shevchenko", "authors": "Marco Dozzi, Yuliya Mishura and Georgiy Shevchenko", "title": "Asymptotic behavior of mixed power variations and statistical estimation\n  in mixed models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain results on both weak and almost sure asymptotic behaviour of power\nvariations of a linear combination of independent Wiener process and fractional\nBrownian motion. These results are used to construct strongly consistent\nparameter estimators in mixed models.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2013 11:39:12 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2013 06:20:51 GMT"}], "update_date": "2013-06-20", "authors_parsed": [["Dozzi", "Marco", ""], ["Mishura", "Yuliya", ""], ["Shevchenko", "Georgiy", ""]]}, {"id": "1301.1184", "submitter": "Tucker S. McElroy", "authors": "Tucker S. McElroy, Scott H. Holan", "title": "A Conversation with David Findley", "comments": "Published in at http://dx.doi.org/10.1214/12-STS388 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 4, 594-606", "doi": "10.1214/12-STS388", "report-no": "IMS-STS-STS388", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  David Findley was born in Washington, DC on December 27, 1940. After\nattending high school in Lyndon, Kentucky, he earned a B.S. (1962) and M.A.\n(1963) in mathematics from the University of Cincinnati. He then lived in\nGermany, studying functional analysis under Gottfried K\\\"{o}the, obtaining a\nPh.D. from the University of Frankfurt in 1967. Returning to the United States,\nhe served as a mathematics professor at the University of Cincinnati until\n1975. Having transitioned from pure mathematics to statistical time series\nanalysis, Findley took a new academic position at the University of Tulsa,\nduring which time he interacted frequently with the nearby research\nlaboratories of major oil companies and consulted regularly for Cities Service\nOil Company (now Citgo). In 1980 he was invited to lead the seasonal adjustment\nresearch effort at the U.S. Census Bureau, and eventually rose to be a Senior\nMathematical Statistician before his retirement in 2009. In 1966 he married\nMary Virginia Baker, and they currently live in Washington, DC. David Findley\nhas published more than 40 journal articles and book chapters, as well as\ndozens of technical reports and conference proceedings, many of which are\nheavily cited and influential. He has also published two edited volumes (1978\nand 1981) that have had a substantial impact on the field of time series\nanalysis. Numerous honors and awards have accrued to him, including ASA Fellow\n(1987), the Julius Shiskin award (1996) and the U.S. Department of Commerce\nGold Medal (1997).\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2013 13:19:33 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["McElroy", "Tucker S.", ""], ["Holan", "Scott H.", ""]]}, {"id": "1301.1208", "submitter": "Mark Tygert", "authors": "William Perkins, Mark Tygert, and Rachel Ward", "title": "Significance testing without truth", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach to significance testing proposes to decide whether the\ngiven hypothesized statistical model is likely to be true (or false).\nStatistical decision theory provides a basis for this approach by requiring\nevery significance test to make a decision about the truth of the\nhypothesis/model under consideration. Unfortunately, many interesting and\nuseful models are obviously false (that is, not exactly true) even before\nconsidering any data. Fortunately, in practice a significance test need only\ngauge the consistency (or inconsistency) of the observed data with the assumed\nhypothesis/model -- without enquiring as to whether the assumption is likely to\nbe true (or false), or whether some alternative is likely to be true (or\nfalse). In this practical formulation, a significance test rejects a\nhypothesis/model only if the observed data is highly improbable when\ncalculating the probability while assuming the hypothesis being tested; the\nsignificance test only gauges whether the observed data likely invalidates the\nassumed hypothesis, and cannot decide that the assumption -- however\nunmistakably false -- is likely to be false a priori, without any data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2013 14:40:38 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Perkins", "William", ""], ["Tygert", "Mark", ""], ["Ward", "Rachel", ""]]}, {"id": "1301.1273", "submitter": "Louis Lyons", "authors": "Louis Lyons", "title": "Bayes and Frequentism: a Particle Physicist's perspective", "comments": null, "journal-ref": null, "doi": "10.1080/00107514.2012.756312", "report-no": null, "categories": "physics.data-an hep-ex stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In almost every scientific field, an experiment involves collecting data and\nthen analysing it. The analysis stage will often consist in trying to extract\nsome physical parameter and estimating its uncertainty; this is known as\nParameter Determination. An example would be the determination of the mass of\nthe top quark, from data collected from high energy proton-proton collisions. A\ndifferent aim is to choose between two possible hypotheses. For example, are\ndata on the recession speed s of distant galaxies proportional to their\ndistance d, or do they fit better to a model where the expansion of the\nUniverse is accelerating?\n  There are two fundamental approaches to such statistical analyses - Bayesian\nand Frequentist. This article discusses the way they differ in their approach\nto probability, and then goes on to consider how this affects the way they deal\nwith Parameter Determination and Hypothesis Testing. The examples are taken\nfrom every-day life and from Particle Physics.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2013 17:28:37 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Lyons", "Louis", ""]]}, {"id": "1301.1305", "submitter": "Forrest Crawford", "authors": "Forrest W. Crawford and Marc A. Suchard", "title": "Birth-death processes", "comments": "This review replaces an earlier version that focused exclusively on\n  integrals of birth-death processes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many important stochastic counting models can be written as general\nbirth-death processes (BDPs). BDPs are continuous-time Markov chains on the\nnon-negative integers and can be used to easily parameterize a rich variety of\nprobability distributions. Although the theoretical properties of general BDPs\nare well understood, traditionally statistical work on BDPs has been limited to\nthe simple linear (Kendall) process, which arises in ecology and evolutionary\napplications. Aside from a few simple cases, it remains impossible to find\nanalytic expressions for the likelihood of a discretely-observed BDP, and\ncomputational difficulties have hindered development of tools for statistical\ninference. But the gap between BDP theory and practical methods for estimation\nhas narrowed in recent years. There are now robust methods for evaluating\nlikelihoods for realizations of BDPs: finite-time transition, first passage,\nequilibrium probabilities, and distributions of summary statistics that arise\ncommonly in applications. Recent work has also exploited the connection between\ncontinuously- and discretely-observed BDPs to derive EM algorithms for maximum\nlikelihood estimation. Likelihood-based inference for previously intractable\nBDPs is much easier than previously thought and regression approaches analogous\nto Poisson regression are straightforward to derive. In this review, we outline\nthe basic mathematical theory for BDPs and demonstrate new tools for\nstatistical inference using data from BDPs. We give six examples of BDPs and\nderive EM algorithms to fit their parameters by maximum likelihood. We show how\nto compute the distribution of integral summary statistics and give an example\napplication to the total cost of an epidemic. Finally, we suggest future\ndirections for innovation in this important class of stochastic processes.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2013 19:08:06 GMT"}, {"version": "v2", "created": "Fri, 25 Jul 2014 18:46:52 GMT"}], "update_date": "2014-07-28", "authors_parsed": [["Crawford", "Forrest W.", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1301.1505", "submitter": "Francesca Greselin", "authors": "Francesca Greselin and Salvatore Ingrassia", "title": "Maximum likelihood estimation in constrained parameter spaces for\n  mixtures of factor analyzers", "comments": "21 pages, 7 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of factor analyzers are becoming more and more popular in the area\nof model based clustering of high-dimensional data. According to the likelihood\napproach in data modeling, it is well known that the unconstrained\nlog-likelihood function may present spurious maxima and singularities and this\nis due to specific patterns of the estimated covariance structure, when their\ndeterminant approaches 0. To reduce such drawbacks, in this paper we introduce\na procedure for the parameter estimation of mixtures of factor analyzers, which\nmaximizes the likelihood function in a constrained parameter space. We then\nanalyze and measure its performance, compared to the usual non-constrained\napproach, via some simulations and applications to real data sets.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2013 12:10:57 GMT"}], "update_date": "2013-01-09", "authors_parsed": [["Greselin", "Francesca", ""], ["Ingrassia", "Salvatore", ""]]}, {"id": "1301.2053", "submitter": "Vakili Kaveh", "authors": "Kaveh Vakili and Eric Schmitt", "title": "Finding Multivariate Outliers With FastPCS", "comments": "21 pages 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Projection Congruent Subset (PCS) Outlyingness is a new index of\nmultivariate outlyingness obtained by considering univariate projections of the\ndata. Like many other outlier detection procedures, PCS searches for a subset\nwhich minimizes a criterion. The difference is that the new criterion was\ndesigned to be insensitive to the outliers. PCS is supported by FastPCS, a fast\nand affine equivariant algorithm which we also detail. Both an extensive\nsimulation study and a real data application from the field of engineering show\nthat FastPCS performs better than its competitors.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 08:47:03 GMT"}, {"version": "v10", "created": "Wed, 17 Jul 2013 19:40:24 GMT"}, {"version": "v11", "created": "Thu, 18 Jul 2013 06:25:41 GMT"}, {"version": "v12", "created": "Wed, 31 Jul 2013 12:53:44 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2013 22:49:56 GMT"}, {"version": "v3", "created": "Sat, 2 Feb 2013 00:15:04 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2013 15:57:17 GMT"}, {"version": "v5", "created": "Mon, 15 Apr 2013 12:54:45 GMT"}, {"version": "v6", "created": "Tue, 16 Apr 2013 21:07:37 GMT"}, {"version": "v7", "created": "Fri, 19 Apr 2013 09:22:03 GMT"}, {"version": "v8", "created": "Wed, 19 Jun 2013 15:55:57 GMT"}, {"version": "v9", "created": "Tue, 25 Jun 2013 11:40:00 GMT"}], "update_date": "2013-08-01", "authors_parsed": [["Vakili", "Kaveh", ""], ["Schmitt", "Eric", ""]]}, {"id": "1301.2079", "submitter": "Guobin Fang", "authors": "Guobin Fang, Kani Chen, Bo Zhang", "title": "Estimation of Dynamic Mixed Double Factors Model in High Dimensional\n  Panel Data", "comments": "38 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this article is to develop the dimension reduction techniques\nin panel data analysis when the number of individuals and indicators is large.\nWe use Principal Component Analysis (PCA) method to represent large number of\nindicators by minority common factors in the factor models. We propose the\nDynamic Mixed Double Factor Model (DMDFM for short) to re ect cross section and\ntime series correlation with interactive factor structure. DMDFM not only\nreduce the dimension of indicators but also consider the time series and cross\nsection mixed effect. Different from other models, mixed factor model have two\nstyles of common factors. The regressors factors re flect common trend and\nreduce the dimension, error components factors re ect difference and weak\ncorrelation of individuals. The results of Monte Carlo simulation show that\nGeneralized Method of Moments (GMM) estimators have good unbiasedness and\nconsistency. Simulation also shows that the DMDFM can improve prediction power\nof the models effectively.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 10:44:18 GMT"}, {"version": "v2", "created": "Sat, 7 Jan 2017 12:53:16 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Fang", "Guobin", ""], ["Chen", "Kani", ""], ["Zhang", "Bo", ""]]}, {"id": "1301.2093", "submitter": "Marco Scutari", "authors": "Marco Scutari and Ian Mackay and David J. Balding", "title": "Improving the Efficiency of Genomic Selection", "comments": "17 pages, 5 figures", "journal-ref": "Statistical Applications in Genetics and Molecular Biology 2013,\n  12(4), 517-527", "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate two approaches to increase the efficiency of phenotypic\nprediction from genome-wide markers, which is a key step for genomic selection\n(GS) in plant and animal breeding. The first approach is feature selection\nbased on Markov blankets, which provide a theoretically-sound framework for\nidentifying non-informative markers. Fitting GS models using only the\ninformative markers results in simpler models, which may allow cost savings\nfrom reduced genotyping. We show that this is accompanied by no loss, and\npossibly a small gain, in predictive power for four GS models: partial least\nsquares (PLS), ridge regression, LASSO and elastic net. The second approach is\nthe choice of kinship coefficients for genomic best linear unbiased prediction\n(GBLUP). We compare kinships based on different combinations of centring and\nscaling of marker genotypes, and a newly proposed kinship measure that adjusts\nfor linkage disequilibrium (LD).\n  We illustrate the use of both approaches and examine their performances using\nthree real-world data sets from plant and animal genetics. We find that elastic\nnet with feature selection and GBLUP using LD-adjusted kinships performed\nsimilarly well, and were the best-performing methods in our study.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 11:38:59 GMT"}, {"version": "v2", "created": "Sat, 1 Jun 2013 19:08:45 GMT"}], "update_date": "2013-09-09", "authors_parsed": [["Scutari", "Marco", ""], ["Mackay", "Ian", ""], ["Balding", "David J.", ""]]}, {"id": "1301.2167", "submitter": "Isabella Gollini", "authors": "Isabella Gollini and Thomas Brendan Murphy", "title": "Mixture of Latent Trait Analyzers for Model-Based Clustering of\n  Categorical Data", "comments": "Accepted to appear in Statistics and Computing; Main paper and\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering methods for continuous data are well established and\ncommonly used in a wide range of applications. However, model-based clustering\nmethods for categorical data are less standard. Latent class analysis is a\ncommonly used method for model-based clustering of binary data and/or\ncategorical data, but due to an assumed local independence structure there may\nnot be a correspondence between the estimated latent classes and groups in the\npopulation of interest. The mixture of latent trait analyzers model extends\nlatent class analysis by assuming a model for the categorical response\nvariables that depends on both a categorical latent class and a continuous\nlatent trait variable; the discrete latent class accommodates group structure\nand the continuous latent trait accommodates dependence within these groups.\nFitting the mixture of latent trait analyzers model is potentially difficult\nbecause the likelihood function involves an integral that cannot be evaluated\nanalytically. We develop a variational approach for fitting the mixture of\nlatent trait models and this provides an efficient model fitting strategy. The\nmixture of latent trait analyzers model is demonstrated on the analysis of data\nfrom the National Long Term Care Survey (NLTCS) and voting in the U.S.\nCongress. The model is shown to yield intuitive clustering results and it gives\na much better fit than either latent class analysis or latent trait analysis\nalone.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 15:45:18 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2013 18:00:46 GMT"}], "update_date": "2013-02-20", "authors_parsed": [["Gollini", "Isabella", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1301.2194", "submitter": "Steven Hill", "authors": "Steven M. Hill and Sach Mukherjee", "title": "Network-based clustering with mixtures of L1-penalized Gaussian\n  graphical models: an empirical investigation", "comments": "A version of this work also appears in the first author's PhD Thesis\n  (Sparse Graphical Models for Cancer Signalling, University of Warwick, 2012),\n  which can be accessed at http://wrap.warwick.ac.uk/id/eprint/49626", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, multivariate samples may harbor previously unrecognized\nheterogeneity at the level of conditional independence or network structure.\nFor example, in cancer biology, disease subtypes may differ with respect to\nsubtype-specific interplay between molecular components. Then, both subtype\ndiscovery and estimation of subtype-specific networks present important and\nrelated challenges. To enable such analyses, we put forward a mixture model\nwhose components are sparse Gaussian graphical models. This brings together\nmodel-based clustering and graphical modeling to permit simultaneous estimation\nof cluster assignments and cluster-specific networks. We carry out estimation\nwithin an L1-penalized framework, and investigate several specific penalization\nregimes. We present empirical results on simulated data and provide general\nrecommendations for the formulation and use of mixtures of L1-penalized\nGaussian graphical models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 17:23:11 GMT"}], "update_date": "2013-01-11", "authors_parsed": [["Hill", "Steven M.", ""], ["Mukherjee", "Sach", ""]]}, {"id": "1301.2258", "submitter": "Blai Bonet", "authors": "Blai Bonet", "title": "Instrumentality Tests Revisited", "comments": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2001-PG-48-55", "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An instrument is a random variable thatallows the identification of\nparameters inlinear models when the error terms arenot uncorrelated.It is a\npopular method used in economicsand the social sciences that reduces theproblem\nof identification to the problemof finding the appropriate instruments.Few\nyears ago, Pearl introduced a necessarytest for instruments that allows the\nresearcher to discard those candidatesthat fail the test.In this paper, we make\na detailed study of Pearl's test and the general model forinstruments. The\nresults of this studyinclude a novel interpretation of Pearl'stest, a general\ntheory of instrumentaltests, and an affirmative answer to aprevious conjecture.\nWe also presentnew instrumentality tests for the casesof discrete and\ncontinuous variables.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:22:44 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Bonet", "Blai", ""]]}, {"id": "1301.2261", "submitter": "Tianjiao Chu", "authors": "Tianjiao Chu, Richard Scheines, Peter L. Spirtes", "title": "Semi-Instrumental Variables: A Test for Instrument Admissibility", "comments": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2001-PG-83-90", "categories": "stat.ME cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a causal graphical model, an instrument for a variable X and its effect Y\nis a random variable that is a cause of X and independent of all the causes of\nY except X. (Pearl (1995), Spirtes et al (2000)). Instrumental variables can be\nused to estimate how the distribution of an effect will respond to a\nmanipulation of its causes, even in the presence of unmeasured common causes\n(confounders). In typical instrumental variable estimation, instruments are\nchosen based on domain knowledge. There is currently no statistical test for\nvalidating a variable as an instrument. In this paper, we introduce the concept\nof semi-instrument, which generalizes the concept of instrument. We show that\nin the framework of additive models, under certain conditions, we can test\nwhether a variable is semi-instrumental. Moreover, adding some distribution\nassumptions, we can test whether two semi-instruments are instrumental. We give\nalgorithms to estimate the p-value that a random variable is semi-instrumental,\nand the p-value that two semi-instruments are both instrumental. These\nalgorithms can be used to test the experts' choice of instruments, or to\nidentify instruments automatically.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:22:57 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Chu", "Tianjiao", ""], ["Scheines", "Richard", ""], ["Spirtes", "Peter L.", ""]]}, {"id": "1301.2300", "submitter": "Judea Pearl", "authors": "Judea Pearl", "title": "Direct and Indirect Effects", "comments": "Appears in Proceedings of the Seventeenth Conference on Uncertainty\n  in Artificial Intelligence (UAI2001)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2001-PG-411-420", "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The direct effect of one eventon another can be defined and measured\nbyholding constant all intermediate variables between the two.Indirect effects\npresent conceptual andpractical difficulties (in nonlinear models), because\nthey cannot be isolated by holding certain variablesconstant. This paper shows\na way of defining any path-specific effectthat does not invoke blocking the\nremainingpaths.This permits the assessment of a more naturaltype of direct and\nindirect effects, one thatis applicable in both linear and nonlinear models.\nThe paper establishesconditions under which such assessments can be estimated\nconsistentlyfrom experimental and nonexperimental data,and thus extends\npath-analytic techniques tononlinear and nonparametric models.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2013 16:25:47 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Pearl", "Judea", ""]]}, {"id": "1301.2410", "submitter": "Dimitris Kugiumtzis", "authors": "Ioannis Vlachos and Dimitris Kugiumtzis", "title": "Backward-in-Time Selection of the Order of Dynamic Regression Prediction\n  Model", "comments": "42 pages, 3 figures, accepted for publication in the Journal of\n  Forecasting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the optimal structure of dynamic regression models used in\nmultivariate time series prediction and propose a scheme to form the lagged\nvariable structure called Backward-in-Time Selection (BTS) that takes into\naccount feedback and multi-collinearity, often present in multivariate time\nseries. We compare BTS to other known methods, also in conjunction with\nregularization techniques used for the estimation of model parameters, namely\nprincipal components, partial least squares and ridge regression estimation.\nThe predictive efficiency of the different models is assessed by means of Monte\nCarlo simulations for different settings of feedback and multi-collinearity.\nThe results show that BTS has consistently good prediction performance while\nother popular methods have varying and often inferior performance. The\nprediction performance of BTS was also found the best when tested on human\nelectroencephalograms of an epileptic seizure, and to the prediction of returns\nof indices of world financial markets.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 08:04:51 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Vlachos", "Ioannis", ""], ["Kugiumtzis", "Dimitris", ""]]}, {"id": "1301.2534", "submitter": "Alice Cleynen", "authors": "Alice Cleynen and Emilie Lebarbier", "title": "Segmentation of the Poisson and negative binomial rate models: a\n  penalized estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the segmentation problem of Poisson and negative binomial (i.e.\noverdispersed Poisson) rate distributions. In segmentation, an important issue\nremains the choice of the number of segments. To this end, we propose a\npenalized log-likelihood estimator where the penalty function is constructed in\na non-asymptotic context following the works of L. Birg\\'e and P. Massart. The\nresulting estimator is proved to satisfy an oracle inequality. The performances\nof our criterion is assessed using simulated and real datasets in the RNA-seq\ndata analysis context.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 16:27:48 GMT"}, {"version": "v2", "created": "Sun, 17 Mar 2013 11:02:03 GMT"}], "update_date": "2013-03-19", "authors_parsed": [["Cleynen", "Alice", ""], ["Lebarbier", "Emilie", ""]]}, {"id": "1301.2550", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Eduardo Garc\\'ia-Portugu\\'es, Ana M. G. Barros, Rosa M. Crujeiras,\n  Wenceslao Gonz\\'alez-Manteiga, J. M. C. Pereira", "title": "A test for directional-linear independence, with applications to\n  wildfire orientation and size", "comments": "19 pages, 4 figures, 3 tables", "journal-ref": "Stochastic Environmental Research and Risk Assessment,\n  28(5):1261-1275, 2014", "doi": "10.1007/s00477-013-0819-6", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The relation between wildfire orientation and size is analyzed by means of a\nnonparametric test for directional-linear independence. The test statistic is\ndesigned for assessing the independence between two random variables of\ndifferent nature, specifically directional (fire orientation, circular or\nspherical, as particular cases) and linear (fire size measured as burnt area,\nscalar), based on a directional-linear nonparametric kernel density estimator.\nIn order to apply the proposed methodology in practice, a resampling procedure\nbased on permutations and bootstrap is provided. The finite sample performance\nof the test is assessed by a simulation study, comparing its behavior with\nother classical tests for the circular-linear case. Finally, the test is\napplied to analyze wildfire data from Portugal.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2013 17:00:18 GMT"}, {"version": "v2", "created": "Mon, 14 Jan 2013 10:33:17 GMT"}, {"version": "v3", "created": "Fri, 14 Jun 2013 11:50:00 GMT"}, {"version": "v4", "created": "Wed, 26 Feb 2014 11:24:57 GMT"}, {"version": "v5", "created": "Sun, 28 Sep 2014 03:25:48 GMT"}, {"version": "v6", "created": "Sun, 20 Sep 2020 23:11:12 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Barros", "Ana M. G.", ""], ["Crujeiras", "Rosa M.", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""], ["Pereira", "J. M. C.", ""]]}, {"id": "1301.2699", "submitter": "Peter McCullagh", "authors": "Peter McCullagh and Walter Dempsey", "title": "Survival models and health sequences", "comments": "21 pages, 3 figs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical investigations focusing on patient survival often generate not only a\nfailure time for each patient but also a sequence of measurements on patient\nhealth at annual or semi-annual check-ups while the patient remains alive. Such\na sequence of random length accompanied by a survival time is called a survival\nprocess. Ordinarily robust health is associated with longer survival, so the\ntwo parts of a survival process cannot be assumed independent. This paper is\nconcerned with a general technique---time reversal---for constructing\nstatistical models for survival processes. A revival model is a regression\nmodel in the sense that it incorporates covariate and treatment effects into\nboth the distribution of survival times and the joint distribution of health\noutcomes. It also allows individual health outcomes to be used clinically for\npredicting the subsequent survival time.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jan 2013 16:53:33 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2013 15:10:22 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2016 09:58:33 GMT"}], "update_date": "2016-01-18", "authors_parsed": [["McCullagh", "Peter", ""], ["Dempsey", "Walter", ""]]}, {"id": "1301.2917", "submitter": "Nial Friel", "authors": "Nial Friel", "title": "Evidence and Bayes factor estimation for Gibbs random fields", "comments": "21 pages (Appeared in Journal of Computational and Graphical\n  Statistics) (Minor typos corrected)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs random fields play an important role in statistics. However they are\ncomplicated to work with due to an intractability of the likelihood function\nand there has been much work devoted to finding computational algorithms to\nallow Bayesian inference to be conducted for such so-called doubly intractable\ndistributions. This paper extends this work and addresses the issue of\nestimating the evidence and Bayes factor for such models. The approach which we\ndevelop is shown to yield good performance.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 10:41:12 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2013 15:27:41 GMT"}, {"version": "v3", "created": "Fri, 28 Mar 2014 21:14:26 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Friel", "Nial", ""]]}, {"id": "1301.2975", "submitter": "Theodore  Kypraios", "authors": "Simon R. White, Theodore Kypraios, Simon P. Preston", "title": "Fast Approximate Bayesian Computation for discretely observed Markov\n  models using a factorised posterior distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern statistical applications involve inference for complicated\nstochastic models for which the likelihood function is difficult or even\nimpossible to calculate, and hence conventional likelihood-based inferential\nechniques cannot be used. In such settings, Bayesian inference can be performed\nusing Approximate Bayesian Computation (ABC). However, in spite of many recent\ndevelopments to ABC methodology, in many applications the computational cost of\nABC necessitates the choice of summary statistics and tolerances that can\npotentially severely bias the estimate of the posterior.\n  We propose a new \"piecewise\" ABC approach suitable for discretely observed\nMarkov models that involves writing the posterior density of the parameters as\na product of factors, each a function of only a subset of the data, and then\nusing ABC within each factor. The approach has the advantage of side-stepping\nthe need to choose a summary statistic and it enables a stringent tolerance to\nbe set, making the posterior \"less approximate\". We investigate two methods for\nestimating the posterior density based on ABC samples for each of the factors:\nthe first is to use a Gaussian approximation for each factor, and the second is\nto use a kernel density estimate. Both methods have their merits. The Gaussian\napproximation is simple, fast, and probably adequate for many applications. On\nthe other hand, using instead a kernel density estimate has the benefit of\nconsistently estimating the true ABC posterior as the number of ABC samples\ntends to infinity. We illustrate the piecewise ABC approach for three examples;\nin each case, the approach enables \"exact matching\" between simulations and\ndata and offers fast and accurate inference.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 13:53:07 GMT"}, {"version": "v2", "created": "Tue, 28 May 2013 16:04:42 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["White", "Simon R.", ""], ["Kypraios", "Theodore", ""], ["Preston", "Simon P.", ""]]}, {"id": "1301.2983", "submitter": "Julian Faraway", "authors": "Julian J. Faraway", "title": "Does Data Splitting Improve Prediction?", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-014-9522-9", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data splitting divides data into two parts. One part is reserved for model\nselection. In some applications, the second part is used for model validation\nbut we use this part for estimating the parameters of the chosen model. We\nfocus on the problem of constructing reliable predictive distributions for\nfuture observed values. We judge the predictive performance using log scoring.\nWe compare the full data strategy with the data splitting strategy for\nprediction. We show how the full data score can be decomposed into model\nselection, parameter estimation and data reuse costs. Data splitting is\npreferred when data reuse costs are high. We investigate the relative\nperformance of the strategies in four simulation scenarios. We introduce a\nhybrid estimator called SAFE that uses one part for model selection but both\nparts for estimation. We discuss the choice to use a split data analysis versus\na full data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 14:09:49 GMT"}, {"version": "v2", "created": "Thu, 21 Feb 2013 15:06:02 GMT"}], "update_date": "2016-01-20", "authors_parsed": [["Faraway", "Julian J.", ""]]}, {"id": "1301.3156", "submitter": "Martin Weinberg", "authors": "Martin D. Weinberg and Ilsang Yoon and Neal Katz", "title": "A remarkably simple and accurate method for computing the Bayes Factor\n  from a Markov chain Monte Carlo Simulation of the Posterior Distribution in\n  high dimension", "comments": "14 pages, 3 figures, submitted to Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weinberg (2012) described a constructive algorithm for computing the marginal\nlikelihood, Z, from a Markov chain simulation of the posterior distribution.\nIts key point is: the choice of an integration subdomain that eliminates\nsubvolumes with poor sampling owing to low tail-values of posterior\nprobability. Conversely, this same idea may be used to choose the subdomain\nthat optimizes the accuracy of Z. Here, we explore using the simulated\ndistribution to define a small region of high posterior probability, followed\nby a numerical integration of the sample in the selected region using the\nvolume tessellation algorithm described in Weinberg (2012). Even more promising\nis the resampling of this small region followed by a naive Monte Carlo\nintegration. The new enhanced algorithm is computationally trivial and leads to\na dramatic improvement in accuracy. For example, this application of the new\nalgorithm to a four-component mixture with random locations in 16 dimensions\nyields accurate evaluation of Z with 5% errors. This enables Bayes-factor model\nselection for real-world problems that have been infeasible with previous\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 21:10:59 GMT"}], "update_date": "2013-01-16", "authors_parsed": [["Weinberg", "Martin D.", ""], ["Yoon", "Ilsang", ""], ["Katz", "Neal", ""]]}, {"id": "1301.3166", "submitter": "Scott Sisson", "authors": "D. Prangle and M. G. B. Blum and G. Popovic and S. A. Sisson", "title": "Diagnostic tools of approximate Bayesian computation using the coverage\n  property", "comments": "Figures 8-13 are Supplementary Information Figures S1-S6", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is an approach for sampling from an\napproximate posterior distribution in the presence of a computationally\nintractable likelihood function. A common implementation is based on simulating\nmodel, parameter and dataset triples, (m,\\theta,y), from the prior, and then\naccepting as samples from the approximate posterior, those pairs (m,\\theta) for\nwhich y, or a summary of y, is \"close\" to the observed data. Closeness is\ntypically determined though a distance measure and a kernel scale parameter,\n\\epsilon. Appropriate choice of \\epsilon is important to producing a good\nquality approximation. This paper proposes diagnostic tools for the choice of\n\\epsilon based on assessing the coverage property, which asserts that credible\nintervals have the correct coverage levels. We provide theoretical results on\ncoverage for both model and parameter inference, and adapt these into\ndiagnostics for the ABC context. We re-analyse a study on human demographic\nhistory to determine whether the adopted posterior approximation was\nappropriate. R code implementing the proposed methodology is freely available\nin the package \"abc.\"\n", "versions": [{"version": "v1", "created": "Mon, 14 Jan 2013 21:47:58 GMT"}], "update_date": "2013-01-16", "authors_parsed": [["Prangle", "D.", ""], ["Blum", "M. G. B.", ""], ["Popovic", "G.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1301.3451", "submitter": "Fanghu Dong", "authors": "Fanghu Dong", "title": "Eigenstructure of Maximum Likelihood from Counts Data", "comments": "The current article contains premature results which is refined in\n  another published articles. I want to withdraw it in order to reduce the\n  confusion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The MLE (Maximum Likelihood Estimate) for a multinomial model is proportional\nto the data. We call such estimate an eigenestimate and the relationship of it\nto the data as the eigenstructure. When the multinomial model is generalized to\ndeal with data arise from incomplete or censored categorical counts, we would\nnaturally look for this eigenstructure between MLE and data. The paper finds\nthe algebraic representation of the eigenstructure (put as Eqn (2.1)), with\nwhich the intuition is visualized geometrically (Figures 2.2 and 4.3) and\nelaborated in a theory (Section 4). The eigenestimate constructed from the\neigenstructure must be a stationary point of the likelihood, a result proved in\nTheorem 4.42. On the bridge between the algebraic definition of Eqn (2.1) and\nthe Proof of Theorem 4.42, we have exploited an elementary inequality (Lemma\n3.1) that governs the primitive cases, defined the thick objects of fragment\nand slice which can be assembled like mechanical parts (Definition 4.1), proved\na few intermediary results that help build up the intuition (Section 4),\nconjectured the universal existence of an eigenestimate (Conjecture 4.32),\nestablished a criterion for boundary regularity (Criterion 4.37), and paved way\n(the Trivial Slicing Algorithm (TSA)) for the derivation of the Weaver\nalgorithms (Section 5) that finds the eigenestimate by using it to reconstruct\nthe observed counts through the eigenstructure, the reconstruction is iterative\nbut derivative-free and matrix-inversion-free. As new addition to the current\nbody of algorithmic methods, the Weaver algorithms craftily tighten threads\nthat are weaved on a rectangular grid (Figure 2.3), and is one incarnation of\nthe TSA. Finally, we put our method in the context of some existing methods\n(Section 6). Softwares are pseudocoded and put online. Visit\nhttp://hku.hk/jdong/eigenstruct2013a.html for demonstrations and download.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 18:50:42 GMT"}, {"version": "v2", "created": "Sun, 22 Mar 2015 13:27:06 GMT"}, {"version": "v3", "created": "Mon, 1 Jan 2018 03:46:45 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Dong", "Fanghu", ""]]}, {"id": "1301.3473", "submitter": "Ivan Kojadinovic", "authors": "L. Bordes, I. Kojadinovic and P. Vandekerkhove", "title": "Semiparametric estimation of a two-component mixture of linear\n  regressions in which one component is known", "comments": "43 pages, 4 figures, 5 tables", "journal-ref": "Electronic Journal of Statistics 7, pages 2603-2644, 2013", "doi": "10.1214/13-EJS858", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new estimation method for the two-component mixture model introduced in\n\\cite{Van13} is proposed. This model consists of a two-component mixture of\nlinear regressions in which one component is entirely known while the\nproportion, the slope, the intercept and the error distribution of the other\ncomponent are unknown. In spite of good performance for datasets of reasonable\nsize, the method proposed in \\cite{Van13} suffers from a serious drawback when\nthe sample size becomes large as it is based on the optimization of a contrast\nfunction whose pointwise computation requires O(n^2) operations. The range of\napplicability of the method derived in this work is substantially larger as it\nrelies on a method-of-moments estimator free of tuning parameters whose\ncomputation requires O(n) operations. From a theoretical perspective, the\nasymptotic normality of both the estimator of the Euclidean parameter vector\nand of the semiparametric estimator of the c.d.f.\\ of the error is proved under\nweak conditions not involving zero-symmetry assumptions. In addition, an\napproximate confidence band for the c.d.f.\\ of the error can be computed using\na weighted bootstrap whose asymptotic validity is proved. The finite-sample\nperformance of the resulting estimation procedure is studied under various\nscenarios through Monte Carlo experiments. The proposed method is illustrated\non three real datasets of size $n=150$, 51 and 176,343, respectively. Two\nextensions of the considered model are discussed in the final section: a model\nwith an additional scale parameter for the first component, and a model with\nmore than one explanatory variable.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jan 2013 20:14:04 GMT"}, {"version": "v2", "created": "Fri, 19 Jul 2013 17:31:11 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Bordes", "L.", ""], ["Kojadinovic", "I.", ""], ["Vandekerkhove", "P.", ""]]}, {"id": "1301.3558", "submitter": "Heng Peng", "authors": "Tao Huang, Heng Peng and Kun Zhang", "title": "Model Selection for Gaussian Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with an important issue in finite mixture modelling,\nthe selection of the number of mixing components. We propose a new penalized\nlikelihood method for model selection of finite multivariate Gaussian mixture\nmodels. The proposed method is shown to be statistically consistent in\ndetermining of the number of components. A modified EM algorithm is developed\nto simultaneously select the number of components and to estimate the mixing\nweights, i.e. the mixing probabilities, and unknown parameters of Gaussian\ndistributions. Simulations and a real data analysis are presented to illustrate\nthe performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 02:17:58 GMT"}], "update_date": "2013-01-17", "authors_parsed": [["Huang", "Tao", ""], ["Peng", "Heng", ""], ["Zhang", "Kun", ""]]}, {"id": "1301.3759", "submitter": "Isabella Gollini", "authors": "Isabella Gollini and Thomas Brendan Murphy", "title": "Joint Modelling of Multiple Network Views", "comments": "Main paper and Supplementary material: 37 (27 + 10) pages, 20 (16 +\n  4) figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent space models (LSM) for network data were introduced by Hoff et al.\n(2002) under the basic assumption that each node of the network has an unknown\nposition in a D-dimensional Euclidean latent space: generally the smaller the\ndistance between two nodes in the latent space, the greater their probability\nof being connected. In this paper we propose a variational Bayes approach to\nestimate the intractable posterior of the LSM.\n  In many cases, different network views on the same set of nodes are\navailable. It can therefore be useful to build a model able to jointly\nsummarise the information given by all the network views. For this purpose, we\nintroduce the latent space joint model (LSJM) that merges the information given\nby multiple network views assuming that the probability of a node being\nconnected with other nodes in each network view is explained by a unique latent\nvariable. This model is demonstrated on the analysis of two datasets: the\nexcerpt of 50 girls from `Teenage Friends and Lifestyle Study' data at three\ntime points and the Saccharomyces cerevisiae genetic and physical\nprotein-protein interactions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 17:27:31 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2013 13:15:59 GMT"}, {"version": "v3", "created": "Thu, 25 Sep 2014 09:32:40 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Gollini", "Isabella", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1301.3863", "submitter": "Soren Hojsgaard", "authors": "Soren Hojsgaard", "title": "YGGDRASIL - A Statistical Package for Learning Split Models", "comments": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2000)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2000-PG-274-281", "categories": "cs.AI cs.MS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two main objectives of this paper. The first is to present a\nstatistical framework for models with context specific independence structures,\ni.e., conditional independences holding only for sepcific values of the\nconditioning variables. This framework is constituted by the class of split\nmodels. Split models are extension of graphical models for contigency tables\nand allow for a more sophisticiated modelling than graphical models. The\ntreatment of split models include estimation, representation and a Markov\nproperty for reading off those independencies holding in a specific context.\nThe second objective is to present a software package named YGGDRASIL which is\ndesigned for statistical inference in split models, i.e., for learning such\nmodels on the basis of data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 15:50:42 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Hojsgaard", "Soren", ""]]}, {"id": "1301.3902", "submitter": "David M Williamson", "authors": "David M. Williamson, Russell Almond, Robert Mislevy", "title": "Model Criticism of Bayesian Networks with Latent Variables", "comments": "Appears in Proceedings of the Sixteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI2000)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2000-PG-634-643", "categories": "cs.AI stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of Bayesian networks (BNs) to cognitive assessment and\nintelligent tutoring systems poses new challenges for model construction. When\ncognitive task analyses suggest constructing a BN with several latent\nvariables, empirical model criticism of the latent structure becomes both\ncritical and complex. This paper introduces a methodology for criticizing\nmodels both globally (a BN in its entirety) and locally (observable nodes), and\nexplores its value in identifying several kinds of misfit: node errors, edge\nerrors, state errors, and prior probability errors in the latent structure. The\nresults suggest the indices have potential for detecting model misfit and\nassisting in locating problematic components of the model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 15:53:20 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Williamson", "David M.", ""], ["Almond", "Russell", ""], ["Mislevy", "Robert", ""]]}, {"id": "1301.3933", "submitter": "Andrew Jaffe", "authors": "Andrew E. Jaffe, John D. Storey, Hongkai Ji and Jeffrey T. Leek", "title": "Gene set bagging for estimating replicability of gene set analyses", "comments": "3 Figures", "journal-ref": null, "doi": "10.1186/1471-2105-14-360", "report-no": null, "categories": "stat.ME q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Significance analysis plays a major role in identifying and\nranking genes, transcription factor binding sites, DNA methylation regions, and\nother high-throughput features for association with disease. We propose a new\napproach, called gene set bagging, for measuring the stability of ranking\nprocedures using predefined gene sets. Gene set bagging involves resampling the\noriginal high-throughput data, performing gene-set analysis on the resampled\ndata, and confirming that biological categories replicate. This procedure can\nbe thought of as bootstrapping gene-set analysis and can be used to determine\nwhich are the most reproducible gene sets. Results: Here we apply this approach\nto two common genomics applications: gene expression and DNA methylation. Even\nwith state-of-the-art statistical ranking procedures, significant categories in\na gene set enrichment analysis may be unstable when subjected to resampling.\nConclusions: We demonstrate that gene lists are not necessarily stable, and\ntherefore additional steps like gene set bagging can improve biological\ninference of gene set analysis.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 22:02:22 GMT"}, {"version": "v2", "created": "Fri, 18 Jan 2013 01:38:48 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Jaffe", "Andrew E.", ""], ["Storey", "John D.", ""], ["Ji", "Hongkai", ""], ["Leek", "Jeffrey T.", ""]]}, {"id": "1301.3947", "submitter": "Hilary Parker", "authors": "Hilary S. Parker, H\\'ector Corrada Bravo and Jeffrey T. Leek", "title": "Removing batch effects for prediction problems with frozen surrogate\n  variable analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch effects are responsible for the failure of promising genomic prognos-\ntic signatures, major ambiguities in published genomic results, and retractions\nof widely-publicized findings. Batch effect corrections have been developed to\nre- move these artifacts, but they are designed to be used in population\nstudies. But genomic technologies are beginning to be used in clinical\napplications where sam- ples are analyzed one at a time for diagnostic,\nprognostic, and predictive applica- tions. There are currently no batch\ncorrection methods that have been developed specifically for prediction. In\nthis paper, we propose an new method called frozen surrogate variable analysis\n(fSVA) that borrows strength from a training set for individual sample batch\ncorrection. We show that fSVA improves prediction ac- curacy in simulations and\nin public genomic studies. fSVA is available as part of the sva Bioconductor\npackage.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 23:16:02 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Parker", "Hilary S.", ""], ["Bravo", "H\u00e9ctor Corrada", ""], ["Leek", "Jeffrey T.", ""]]}, {"id": "1301.3949", "submitter": "Marianna Pensky", "authors": "Daniela De Canditiis, Marianna Pensky and Patrick J. Wolfe", "title": "De-noising procedures for frame operators", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present paper provides a comprehensive study of de-noising properties of\nframes and, in particular, tight frames, which constitute one of the most\npopular tools in contemporary signal processing. The objective of the paper is\nto bridge the existing gap between mathematical and statistical theories on one\nhand and engineering practice on the other and explore how one can take\nadvantage of a specific structure of a frame in contrast to an arbitrary\ncollection of vectors or an orthonormal basis. For both the general and the\ntight frames, the paper presents a set of practically implementable de-noising\ntechniques which take frame induced correlation structures into account. These\nresults are supplemented by an examination of the case when the frame is\nconstructed as a collection of orthonormal bases. In particular,\nrecommendations are given for aggregation of the estimators at the stage of\nframe coefficients. The paper is concluded by a finite sample simulation study\nwhich confirms that taking frame structure and frame induced correlations into\naccount indeed improves de-noising precision.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2013 23:42:42 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["De Canditiis", "Daniela", ""], ["Pensky", "Marianna", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1301.4167", "submitter": "Ekkehard  Glimm", "authors": "Ekkehard Glimm and J\\\"urgen L\\\"auter", "title": "Some Notes on Blinded Sample Size Re-Estimation", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note investigates a number of scenarios in which unadjusted testing\nfollowing a blinded sample size re-estimation leads to type I error violations.\nFor superiority testing, this occurs in certain small-sample borderline cases.\nWe discuss a number of alternative approaches that keep the type I error rate.\nThe paper also gives a reason why the type I error inflation in the superiority\ncontext might have been missed in previous publications and investigates why it\nis more marked in case of non-inferiority testing.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 17:35:00 GMT"}], "update_date": "2013-01-18", "authors_parsed": [["Glimm", "Ekkehard", ""], ["L\u00e4uter", "J\u00fcrgen", ""]]}, {"id": "1301.4240", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "Hypothesis Testing in High-Dimensional Regression under the Gaussian\n  Random Design Model: Asymptotic Theory", "comments": "63 pages, 10 figures, 11 tables, Section 5 and Theorem 4.5 are added.\n  Other modifications to improve presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider linear regression in the high-dimensional regime where the number\nof observations $n$ is smaller than the number of parameters $p$. A very\nsuccessful approach in this setting uses $\\ell_1$-penalized least squares\n(a.k.a. the Lasso) to search for a subset of $s_0< n$ parameters that best\nexplain the data, while setting the other parameters to zero. Considerable\namount of work has been devoted to characterizing the estimation and model\nselection problems within this approach.\n  In this paper we consider instead the fundamental, but far less understood,\nquestion of \\emph{statistical significance}. More precisely, we address the\nproblem of computing p-values for single regression coefficients.\n  On one hand, we develop a general upper bound on the minimax power of tests\nwith a given significance level. On the other, we prove that this upper bound\nis (nearly) achievable through a practical procedure in the case of random\ndesign matrices with independent entries. Our approach is based on a debiasing\nof the Lasso estimator. The analysis builds on a rigorous characterization of\nthe asymptotic distribution of the Lasso estimator and its debiased version.\nOur result holds for optimal sample size, i.e., when $n$ is at least on the\norder of $s_0 \\log(p/s_0)$.\n  We generalize our approach to random design matrices with i.i.d. Gaussian\nrows $x_i\\sim N(0,\\Sigma)$. In this case we prove that a similar distributional\ncharacterization (termed `standard distributional limit') holds for $n$ much\nlarger than $s_0(\\log p)^2$.\n  Finally, we show that for optimal sample size, $n$ being at least of order\n$s_0 \\log(p/s_0)$, the standard distributional limit for general Gaussian\ndesigns can be derived from the replica heuristics in statistical physics.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2013 21:16:49 GMT"}, {"version": "v2", "created": "Fri, 24 May 2013 07:02:40 GMT"}, {"version": "v3", "created": "Tue, 4 Feb 2014 04:05:41 GMT"}], "update_date": "2014-02-05", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1301.4291", "submitter": "Roberto D. Pascual-Marqui", "authors": "Roberto D. Pascual-Marqui, Dietrich Lehmann, Kieko Kochi, Toshihiko\n  Kinoshita, Naoto Yamada", "title": "A measure of association between vectors based on \"similarity\n  covariance\"", "comments": "Pre-print; Technical report; 2013-01-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The \"maximum similarity correlation\" definition introduced in this study is\nmotivated by the seminal work of Szekely et al on \"distance covariance\" (Ann.\nStatist. 2007, 35: 2769-2794; Ann. Appl. Stat. 2009, 3: 1236-1265). Instead of\nusing Euclidean distances \"d\" as in Szekely et al, we use \"similarity\", which\ncan be defined as \"exp(-d/s)\", where the scaling parameter s>0 controls how\nrapidly the similarity falls off with distance. Scale parameters are chosen by\nmaximizing the similarity correlation. The motivation for using \"similarity\"\noriginates in spectral clustering theory (see e.g. Ng et al 2001, Advances in\nNeural Information Processing Systems 14: 849-856). We show that a particular\nform of similarity correlation is asymptotically equivalent to distance\ncorrelation for large values of the scale parameter. Furthermore, we extend\nsimilarity correlation to coherence between complex valued vectors, including\nits partitioning into real and imaginary contributions. Several toy examples\nare used for comparing distance and similarity correlations. For instance,\npoints on a noiseless straight line give distance and similarity correlation\nvalues equal to 1; but points on a noiseless circle produces near zero distance\ncorrelation (dCorr=0.02) while the similarity correlation is distinctly non\nzero (sCorr=0.36). In distinction to the distance approach, similarity gives\nmore importance to small distances, which emphasizes the local properties of\nfunctional relations. This paper represents a preliminary empirical study,\nshowing that the novel similarity association has some distinct practical\nadvantages over distance based association.For the sake of reproducible\nresearch, the software code implementing all methods here (using lazarus\nfree-pascal \"www.lazarus.freepascal.org\"), including all test data, are freely\navailable at: \"sites.google.com/site/pascualmarqui/home/similaritycovariance\".\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2013 04:06:38 GMT"}, {"version": "v2", "created": "Mon, 21 Jan 2013 06:58:31 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2013 11:43:53 GMT"}, {"version": "v4", "created": "Mon, 4 Feb 2013 07:21:20 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Pascual-Marqui", "Roberto D.", ""], ["Lehmann", "Dietrich", ""], ["Kochi", "Kieko", ""], ["Kinoshita", "Toshihiko", ""], ["Yamada", "Naoto", ""]]}, {"id": "1301.4292", "submitter": "Mohammad Jafari Jozani", "authors": "Mohammad Jafari Jozani and Jafar Ahmadi", "title": "On uncertainty and information properties of ranked set samples", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranked set sampling is a sampling design which has a wide range of\napplications in industrial statistics, and environmental and ecological\nstudies, etc.. It is well known that ranked set samples provide more Fisher\ninformation than simple random samples of the same size about the unknown\nparameters of the underlying distribution in parametric inferences. In this\npaper, we consider the uncertainty and information content of ranked set\nsamples in both perfect and imperfect ranking scenarios in terms of Shannon\nentropy, R\\'enyi and Kullback-Leibler (KL) information measures. It is proved\nthat under these information measures, ranked set sampling design performs\nbetter than its simple random sampling counterpart of the same size. The\ninformation content is also a monotone function of the set size in ranked set\nsampling. Moreover, the effect of ranking error on the information content of\nthe data is investigated.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2013 04:21:45 GMT"}], "update_date": "2013-01-21", "authors_parsed": [["Jozani", "Mohammad Jafari", ""], ["Ahmadi", "Jafar", ""]]}, {"id": "1301.4628", "submitter": "Mohammad Jafari Jozani", "authors": "Mohammad Jafari Jozani and Nahid Jafari Tabrizi", "title": "Intrinsic posterior regret gamma-minimax estimation for the exponential\n  family of distributions", "comments": "16 pages", "journal-ref": null, "doi": "10.1214/13-EJS828", "report-no": null, "categories": "math.ST stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, it is desired to have estimates that are invariant under\nreparameterization. The invariance property of the estimators helps to\nformulate a unified solution to the underlying estimation problem. In robust\nBayesian analysis, a frequent criticism is that the optimal estimators are not\ninvariant under smooth reparameterizations. This paper considers the problem of\nposterior regret gamma-minimax (PRGM) estimation of the natural parameter of\nthe exponential family of distributions under intrinsic loss functions. We show\nthat under the class of Jeffrey's Conjugate Prior (JCP) distributions, PRGM\nestimators are invariant to smooth one-to-one reparameterizations. We apply our\nresults to several distributions and different classes of JCP, as well as the\nusual conjugate prior distributions. We observe that, in many cases, invariant\nPRGM estimators in the class of JCP distributions can be obtained by some\nmodifications of PRGM estimators in the usual class of conjugate priors.\n  Moreover, when the class of priors are convex or dependant on a\nhyper-parameter belonging to a connected set, we show that the PRGM estimator\nunder the intrinsic loss function could be Bayes with respect to a prior\ndistribution in the original prior class. Theoretical results are supplemented\nwith several examples and illustrations.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2013 05:11:40 GMT"}], "update_date": "2014-04-24", "authors_parsed": [["Jozani", "Mohammad Jafari", ""], ["Tabrizi", "Nahid Jafari", ""]]}, {"id": "1301.4649", "submitter": "Julie Josse", "authors": "Marie Verbanck and Julie Josse and Fran\\c{c}ois Husson", "title": "Regularised PCA to denoise and visualise data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a well-established method commonly used\nto explore and visualise data. A classical PCA model is the fixed effect model\nwhere data are generated as a fixed structure of low rank corrupted by noise.\nUnder this model, PCA does not provide the best recovery of the underlying\nsignal in terms of mean squared error. Following the same principle as in ridge\nregression, we propose a regularised version of PCA that boils down to\nthreshold the singular values. Each singular value is multiplied by a term\nwhich can be seen as the ratio of the signal variance over the total variance\nof the associated dimension. The regularised term is analytically derived using\nasymptotic results and can also be justified from a Bayesian treatment of the\nmodel. Regularised PCA provides promising results in terms of the recovery of\nthe true signal and the graphical outputs in comparison with classical PCA and\nwith a soft thresholding estimation strategy. The gap between PCA and\nregularised PCA is all the more important that data are noisy.\n", "versions": [{"version": "v1", "created": "Sun, 20 Jan 2013 10:49:48 GMT"}, {"version": "v2", "created": "Fri, 10 May 2013 01:16:17 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Verbanck", "Marie", ""], ["Josse", "Julie", ""], ["Husson", "Fran\u00e7ois", ""]]}, {"id": "1301.4950", "submitter": "Yun Yang", "authors": "Yun Yang and David B. Dunson", "title": "Bayesian Conditional Tensor Factorizations for High-Dimensional\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many application areas, data are collected on a categorical response and\nhigh-dimensional categorical predictors, with the goals being to build a\nparsimonious model for classification while doing inferences on the important\npredictors. In settings such as genomics, there can be complex interactions\namong the predictors. By using a carefully-structured Tucker factorization, we\ndefine a model that can characterize any conditional probability, while\nfacilitating variable selection and modeling of higher-order interactions.\nFollowing a Bayesian approach, we propose a Markov chain Monte Carlo algorithm\nfor posterior computation accommodating uncertainty in the predictors to be\nincluded. Under near sparsity assumptions, the posterior distribution for the\nconditional probability is shown to achieve close to the parametric rate of\ncontraction even in ultra high-dimensional settings. The methods are\nillustrated using simulation examples and biomedical applications.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 18:32:48 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Yang", "Yun", ""], ["Dunson", "David B.", ""]]}, {"id": "1301.4954", "submitter": "Xiao Wang", "authors": "Xiao Wang and David Ruppert", "title": "Optimal Prediction in an Additive Functional Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The functional generalized additive model (FGAM) provides a more flexible\nnonlinear functional regression model than the well-studied functional linear\nregression model. This paper restricts attention to the FGAM with identity link\nand additive errors, which we will call the additive functional model, a\ngeneralization of the functional linear model. This paper studies the minimax\nrate of convergence of predictions from the additive functional model in the\nframework of reproducing kernel Hilbert space. It is shown that the optimal\nrate is determined by the decay rate of the eigenvalues of a specific kernel\nfunction, which in turn is determined by the reproducing kernel and the joint\ndistribution of any two points in the random predictor function. For the\nspecial case of the functional linear model, this kernel function is jointly\ndetermined by the covariance function of the predictor function and the\nreproducing kernel. The easily implementable roughness-regularized predictor is\nshown to achieve the optimal rate of convergence. Numerical studies are carried\nout to illustrate the merits of the predictor. Our simulations and real data\nexamples demonstrate a competitive performance against the existing approach.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 18:40:08 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Wang", "Xiao", ""], ["Ruppert", "David", ""]]}, {"id": "1301.4968", "submitter": "Chris Fleming", "authors": "C. H. Fleming, J. M. Calabrese", "title": "On the estimators of autocorrelation model parameters", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of autocorrelations and spectral densities is of fundamental\nimportance in many fields of science, from identifying pulsar signals in\nastronomy to measuring heart beats in medicine. In circumstances where one is\ninterested in specific autocorrelation functions that do not fit into any\nsimple families of models, such as auto-regressive moving average (ARMA),\nestimating model parameters is generally approached in one of two ways: by\nfitting the model autocorrelation function to a non-parameteric autocorrelation\nestimate via regression analysis or by fitting the model autocorrelation\nfunction directly to the data via maximum likelihood. Prior literature suggests\nthat variogram regression yields parameter estimates of comparable quality to\nmaximum likelihood. In this letter we demonstrate that, as sample size is\nincreases, the accuracy of the maximum-likelihood estimates (MLE) ultimately\nimproves by orders of magnitude beyond that of variogram regression. For\nrelatively continuous and Gaussian processes, this improvement can occur for\nsample sizes of less than 100. Moreover, even where the accuracy of these\nmethods is comparable, the MLE remains almost universally better and, more\ncritically, variogram regression does not provide reliable confidence\nintervals. Inaccurate regression parameter estimates are typically accompanied\nby underestimated standard errors, whereas likelihood provides reliable\nconfidence intervals.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jan 2013 19:23:55 GMT"}], "update_date": "2013-01-22", "authors_parsed": [["Fleming", "C. H.", ""], ["Calabrese", "J. M.", ""]]}, {"id": "1301.5157", "submitter": "Leonard Rogers", "authors": "L. C. G. Rogers", "title": "Least-action filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to estimating a hidden process in a\ncontinuous-time setting, where the hidden process is a diffusion. The approach\nis simply to minimize the negative log-likelihood of the hidden path, where the\nlikelihood is expressed relative to Wiener measure. This negative\nlog-likelihood is the action integral of the path, which we minimize by\ncalculus of variations. We then perform an asymptotic maximum-likelihood\nanalysis to understand better how the actual path is distributed around the\nleast-action path; it turns out that the actual path can be expressed\n(approximately) as the sum of the least-action path and a zero-mean Gaussian\nprocess which can be specified quite explicitly. Numerical solution of the ODEs\nwhich arise from the calculus of variations is often feasible, but is\ncomplicated by the shooting nature of the problem, and the possibility that we\nhave found a local but not global minimum. We analyze the situations when this\nhappens, and provide effective numerical methods for studying this. We also\nshow how the methodology works in a situation where the hidden positive\ndiffusion acts as the random intensity of a point process which is observed;\nhere too it is possible to estimate the hidden process.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2013 11:47:46 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Rogers", "L. C. G.", ""]]}, {"id": "1301.5390", "submitter": "Christopher Paciorek", "authors": "Mariel M. Finucane, Christopher J. Paciorek, Gretchen A. Stevens, and\n  Majid Ezzati", "title": "Semiparametric Bayesian Density Estimation with Disparate Data Sources:\n  A Meta-Analysis of Global Childhood Undernutrition", "comments": "41 total pages, 6 figures, 1 table", "journal-ref": "Journal of the American Statistical Association (2015) 110:\n  889-901", "doi": "10.1080/01621459.2014.937487", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undernutrition, resulting in restricted growth, and quantified here using\nheight-for-age z-scores, is an important contributor to childhood morbidity and\nmortality. Since all levels of mild, moderate and severe undernutrition are of\nclinical and public health importance, it is of interest to estimate the shape\nof the z-scores' distributions.\n  We present a finite normal mixture model that uses data on 4.3 million\nchildren to make annual country-specific estimates of these distributions for\nunder-5-year-old children in the world's 141 low- and middle-income countries\nbetween 1985 and 2011. We incorporate both individual-level data when\navailable, as well as aggregated summary statistics from studies whose\nindividual-level data could not be obtained. We place a hierarchical Bayesian\nprobit stick-breaking model on the mixture weights. The model allows for\nnonlinear changes in time, and it borrows strength in time, in covariates, and\nwithin and across regional country clusters to make estimates where data are\nuncertain, sparse, or missing.\n  This work addresses three important problems that often arise in the fields\nof public health surveillance and global health monitoring. First, data are\nalways incomplete. Second, different data sources commonly use different\nreporting metrics. Last, distributions, and especially their tails, are often\nof substantive interest.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jan 2013 03:13:54 GMT"}, {"version": "v2", "created": "Sat, 14 Dec 2013 16:22:37 GMT"}, {"version": "v3", "created": "Sun, 29 Jun 2014 00:48:35 GMT"}], "update_date": "2015-12-08", "authors_parsed": [["Finucane", "Mariel M.", ""], ["Paciorek", "Christopher J.", ""], ["Stevens", "Gretchen A.", ""], ["Ezzati", "Majid", ""]]}, {"id": "1301.5701", "submitter": "Yasin Yilmaz", "authors": "Yasin Yilmaz, George V. Moustakides, and Xiaodong Wang", "title": "Sequential and Decentralized Estimation of Linear Regression Parameters\n  in Wireless Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT math.OC math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential estimation of a vector of linear regression coefficients is\nconsidered under both centralized and decentralized setups. In sequential\nestimation, the number of observations used for estimation is determined by the\nobserved samples, hence is random, as opposed to fixed-sample-size estimation.\nSpecifically, after receiving a new sample, if a target accuracy level is\nreached, we stop and estimate using the samples collected so far; otherwise we\ncontinue to receive another sample. It is known that finding an optimum\nsequential estimator, which minimizes the average sample number for a given\ntarget accuracy level, is an intractable problem with a general stopping rule\nthat depends on the complete observation history. By properly restricting the\nsearch space to stopping rules that depend on a specific subset of the complete\nobservation history, we derive the optimum sequential estimator in the\ncentralized case via optimal stopping theory. However, finding the optimum\nstopping rule in this case requires numerical computations that {\\em\nquadratically} scales with the number of parameters to be estimated. For the\ndecentralized setup with stringent energy constraints, under an alternative\nproblem formulation that is conditional on the observed regressors, we first\nderive a simple optimum scheme whose computational complexity is {\\em constant}\nwith respect to the number of parameters. Then, following this simple optimum\nscheme we propose a decentralized sequential estimator whose computational\ncomplexity and energy consumption scales {\\em linearly} with the number of\nparameters. Specifically, in the proposed decentralized scheme a\nclose-to-optimum average stopping time performance is achieved by infrequently\ntransmitting a single pulse with very short duration.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2013 05:10:13 GMT"}, {"version": "v2", "created": "Fri, 10 May 2013 11:15:30 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2014 22:03:28 GMT"}, {"version": "v4", "created": "Wed, 17 Dec 2014 09:11:05 GMT"}], "update_date": "2014-12-18", "authors_parsed": [["Yilmaz", "Yasin", ""], ["Moustakides", "George V.", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1301.5927", "submitter": "Thordis Thorarinsdottir", "authors": "Thordis L. Thorarinsdottir, Tilmann Gneiting, Nadine Gissibl", "title": "Using proper divergence functions to evaluate climate models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been argued persuasively that, in order to evaluate climate models,\nthe probability distributions of model output need to be compared to the\ncorresponding empirical distributions of observed data. Distance measures\nbetween probability distributions, also called divergence functions, can be\nused for this purpose. We contend that divergence functions ought to be proper,\nin the sense that acting on modelers' true beliefs is an optimal strategy.\nScore divergences that derive from proper scoring rules are proper, with the\nintegrated quadratic distance and the Kullback-Leibler divergence being\nparticularly attractive choices. Other commonly used divergences fail to be\nproper. In an illustration, we evaluate and rank simulations from fifteen\nclimate models for temperature extremes in a comparison to re-analysis data.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2013 22:37:43 GMT"}, {"version": "v2", "created": "Tue, 16 Jul 2013 09:30:09 GMT"}], "update_date": "2013-07-17", "authors_parsed": [["Thorarinsdottir", "Thordis L.", ""], ["Gneiting", "Tilmann", ""], ["Gissibl", "Nadine", ""]]}, {"id": "1301.6206", "submitter": "Yasin Yilmaz", "authors": "Yasin Yilmaz, George V. Moustakides, and Xiaodong Wang", "title": "Optimal Sequential Joint Detection and Estimation", "comments": "This paper has been withdrawn by the authors. Please see\n  arXiv:1302.6058", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn by the authors. Please see arXiv:1302.6058. We\nconsider the sequential joint detection and estimation problem. Minimizing the\naverage stopping time subject to a combination of detection and estimation\nconstraints we obtain the optimal triplet of stopping time, detector and\nestimator. In the joint detection and estimation problem the primary goal is to\ndetect and estimate together, as opposed to the conventional testing of\ncomposite hypotheses where the primary goal is to detect only. For the first\ntime in the literature we develop optimal solution to the sequential joint\ndetection and estimation problem. In the sequential version of the problem,\ndifferent from the fixed sample size version, optimal stopping time is also\nsought, complicating the solution considerably.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2013 03:02:46 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2013 06:53:41 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2013 20:10:01 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Yilmaz", "Yasin", ""], ["Moustakides", "George V.", ""], ["Wang", "Xiaodong", ""]]}, {"id": "1301.6270", "submitter": "Yawen Xu", "authors": "Yawen Xu, Xin Gao and Xiaogang Wang", "title": "Nonparametric Clustering of Mixed Data Using Modified Chi-square Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a non-parametric method to cluster mixed data containing both\ncontinuous and discrete random variables. The product space of continuous and\ncategorical sample spaces is approximated locally by analyzing neighborhoods\nwith cluster patterns. Detection of cluster patterns on the product space is\ndetermined by using a modified Chi-square test. The proposed method does not\nimpose a global distance function which could be difficult to specify in\npractice. Results from simulation studies have shown that our proposed methods\nout-performed the benchmark method, AutoClass, for various settings.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2013 16:43:52 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Xu", "Yawen", ""], ["Gao", "Xin", ""], ["Wang", "Xiaogang", ""]]}, {"id": "1301.6278", "submitter": "Aris Spanos", "authors": "Aris Spanos", "title": "Revisiting the Neyman-Scott model: an Inconsistent MLE or an Ill-defined\n  Model?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neyman and Scott (1948) model is widely used to demonstrate a serious\nweakness of the Maximum Likelihood (ML) method: it can give rise to\ninconsistent estimators. The primary objective of this paper is to revisit this\nexample with a view to demonstrate that the culprit for the inconsistent\nestimation is not the ML method but an ill-defined statistical model. It is\nalso shown that a simple recasting of this model renders it well-defined and\nthe ML method gives rise to consistent and asymptotically efficient estimators.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2013 18:37:59 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Spanos", "Aris", ""]]}, {"id": "1301.6376", "submitter": "Georg Mainik", "authors": "Jan Beran, Georg Mainik", "title": "On estimating extremal dependence structures by parametric spectral\n  measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of extreme value copulas is often required in situations where\navailable data are sparse. Parametric methods may then be the preferred\napproach. A possible way of defining parametric families that are simple and,\nat the same time, cover a large variety of multivariate extremal dependence\nstructures is to build models based on spectral measures. This approach is\nconsidered here. Parametric families of spectral measures are defined as convex\nhulls of suitable basis elements, and parameters are estimated by projecting an\ninitial nonparametric estimator on these finite-dimensional spaces. Asymptotic\ndistributions are derived for the estimated parameters and the resulting\nestimates of the spectral measure and the extreme value copula. Finite sample\nproperties are illustrated by a simulation study.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jan 2013 17:05:56 GMT"}], "update_date": "2013-01-29", "authors_parsed": [["Beran", "Jan", ""], ["Mainik", "Georg", ""]]}, {"id": "1301.6450", "submitter": "Ewan Cameron", "authors": "Ewan Cameron, Anthony Pettitt", "title": "Recursive Pathways to Marginal Likelihood Estimation with\n  Prior-Sensitivity Analysis", "comments": "Published in at http://dx.doi.org/10.1214/13-STS465 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 3, 397-419", "doi": "10.1214/13-STS465", "report-no": "IMS-STS-STS465", "categories": "stat.ME astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the utility to computational Bayesian analyses of a particular\nfamily of recursive marginal likelihood estimators characterized by the\n(equivalent) algorithms known as \"biased sampling\" or \"reverse logistic\nregression\" in the statistics literature and \"the density of states\" in\nphysics. Through a pair of numerical examples (including mixture modeling of\nthe well-known galaxy data set) we highlight the remarkable diversity of\nsampling schemes amenable to such recursive normalization, as well as the\nnotable efficiency of the resulting pseudo-mixture distributions for gauging\nprior sensitivity in the Bayesian model selection context. Our key theoretical\ncontributions are to introduce a novel heuristic (\"thermodynamic integration\nvia importance sampling\") for qualifying the role of the bridging sequence in\nthis procedure and to reveal various connections between these recursive\nestimators and the nested sampling technique.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 05:16:52 GMT"}, {"version": "v2", "created": "Mon, 28 Oct 2013 08:35:55 GMT"}, {"version": "v3", "created": "Wed, 15 Oct 2014 09:24:08 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Cameron", "Ewan", ""], ["Pettitt", "Anthony", ""]]}, {"id": "1301.6494", "submitter": "Ilenia  Epifani", "authors": "I. Epifani, L. Ladelli, A. Pievatolo", "title": "Bayesian estimation for a parametric Markov Renewal model applied to\n  seismic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a complete methodology for Bayesian inference on a\nsemi-Markov process, from the elicitation of the prior distribution, to the\ncomputation of posterior summaries, including a guidance for its JAGS\nimplementation. The holding times (conditional on the transition between two\ngiven states) are assumed to be Weibull-distributed. We examine the elicitation\nof the joint prior density of the shape and scale parameters of the Weibull\ndistributions, deriving a specific class of priors in a natural way, along with\na method for the determination of hyperparameters based on ``learning data''\nand moment existence conditions. This framework is applied to data of\nearthquakes of three types of severity (low, medium and high size) that\noccurred in the central Northern Apennines in Italy and collected by the\n\\cite{CPTI04} catalogue. Assumptions on two types of energy accumulation and\nrelease mechanisms are evaluated.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 09:49:00 GMT"}, {"version": "v2", "created": "Wed, 30 Jan 2013 08:42:32 GMT"}, {"version": "v3", "created": "Fri, 4 Apr 2014 19:20:49 GMT"}], "update_date": "2014-04-07", "authors_parsed": [["Epifani", "I.", ""], ["Ladelli", "L.", ""], ["Pievatolo", "A.", ""]]}, {"id": "1301.6554", "submitter": "Cristina Savin", "authors": "J\\'ozsef Fiser, M\\'at\\'e Lengyel, Cristina Savin, Gerg\\\"o Orb\\'an,\n  Pietro Berkes", "title": "How (not) to assess the importance of correlations for the matching of\n  spontaneous and evoked activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  A comment on `Population rate dynamics and multineuron firing patterns in\nsensory cortex' by Okun et al. Journal of Neuroscience 32(48):17108-17119, 2012\nand our response to the corresponding reply by Okun et al's (arXiv, 2013).\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 14:22:50 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2013 16:30:01 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2013 17:47:40 GMT"}, {"version": "v4", "created": "Fri, 8 Feb 2013 13:39:33 GMT"}, {"version": "v5", "created": "Wed, 27 Mar 2013 10:35:33 GMT"}], "update_date": "2013-03-28", "authors_parsed": [["Fiser", "J\u00f3zsef", ""], ["Lengyel", "M\u00e1t\u00e9", ""], ["Savin", "Cristina", ""], ["Orb\u00e1n", "Gerg\u00f6", ""], ["Berkes", "Pietro", ""]]}, {"id": "1301.6624", "submitter": "Robin J. Evans", "authors": "Robin J. Evans, Thomas S. Richardson", "title": "Markovian acyclic directed mixed graphs for discrete data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1206 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 4, 1452-1482", "doi": "10.1214/14-AOS1206", "report-no": "IMS-AOS-AOS1206", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acyclic directed mixed graphs (ADMGs) are graphs that contain directed\n($\\rightarrow$) and bidirected ($\\leftrightarrow$) edges, subject to the\nconstraint that there are no cycles of directed edges. Such graphs may be used\nto represent the conditional independence structure induced by a DAG model\ncontaining hidden variables on its observed margin. The Markovian model\nassociated with an ADMG is simply the set of distributions obeying the global\nMarkov property, given via a simple path criterion (m-separation). We first\npresent a factorization criterion characterizing the Markovian model that\ngeneralizes the well-known recursive factorization for DAGs. For the case of\nfinite discrete random variables, we also provide a parameterization of the\nmodel in terms of simple conditional probabilities, and characterize its\nvariation dependence. We show that the induced models are smooth. Consequently,\nMarkovian ADMG models for discrete variables are curved exponential families of\ndistributions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2013 17:49:56 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2013 10:27:00 GMT"}, {"version": "v3", "created": "Wed, 23 Apr 2014 14:17:08 GMT"}, {"version": "v4", "created": "Thu, 14 Aug 2014 06:12:21 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Evans", "Robin J.", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1301.7026", "submitter": "Nicola Lunardon", "authors": "Nicola Lunardon", "title": "Prepivoting composite score statistics by weighted bootstrap iteration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The role played by the composite analogue of the log likelihood ratio in\nhypothesis testing and in setting confidence regions is not as prominent as it\nis in the canonical likelihood setting, since its asymptotic distribution\ndepends on the unknown parameter. Approximate pivots based on the composite log\nlikelihood ratio can be derived by using asymptotic arguments. However, the\nactual distribution of such pivots may differ considerably from the asymptotic\nreference, leading to tests and confidence regions whose levels are distant\nfrom the nominal ones. The use of bootstrap rather than asymptotic\ndistributions in the composite likelihood framework is explored. Prepivoted\ntests and confidence sets based on a suitable statistic turn out to be accurate\nand computationally appealing inferential tools.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2013 19:16:38 GMT"}], "update_date": "2013-01-30", "authors_parsed": [["Lunardon", "Nicola", ""]]}, {"id": "1301.7118", "submitter": "Yixin Fang", "authors": "Yixin Fang, Junhui Wang, and Wei Sun", "title": "A note on selection stability: combining stability and prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, many regularized procedures have been proposed for variable\nselection in linear regression, but their performance depends on the tuning\nparameter selection. Here a criterion for the tuning parameter selection is\nproposed, which combines the strength of both stability selection and\ncross-validation and therefore is referred as the prediction and stability\nselection (PASS). The selection consistency is established assuming the data\ngenerating model is a subset of the full model, and the small sample\nperformance is demonstrated through some simulation studies where the\nassumption is either held or violated.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2013 01:19:12 GMT"}], "update_date": "2013-01-31", "authors_parsed": [["Fang", "Yixin", ""], ["Wang", "Junhui", ""], ["Sun", "Wei", ""]]}, {"id": "1301.7161", "submitter": "Richard Lockhart", "authors": "Richard Lockhart, Jonathan Taylor, Ryan J. Tibshirani, Robert\n  Tibshirani", "title": "A significance test for the lasso", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1175 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 2, 413-468", "doi": "10.1214/13-AOS1175", "report-no": "IMS-AOS-AOS1175", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the sparse linear regression setting, we consider testing the significance\nof the predictor variable that enters the current lasso model, in the sequence\nof models visited along the lasso solution path. We propose a simple test\nstatistic based on lasso fitted values, called the covariance test statistic,\nand show that when the true model is linear, this statistic has an\n$\\operatorname {Exp}(1)$ asymptotic distribution under the null hypothesis (the\nnull being that all truly active variables are contained in the current lasso\nmodel). Our proof of this result for the special case of the first predictor to\nenter the model (i.e., testing for a single significant predictor variable\nagainst the global null) requires only weak assumptions on the predictor matrix\n$X$. On the other hand, our proof for a general step in the lasso path places\nfurther technical assumptions on $X$ and the generative model, but still allows\nfor the important high-dimensional case $p>n$, and does not necessarily require\nthat the current lasso model achieves perfect recovery of the truly active\nvariables. Of course, for testing the significance of an additional variable\nbetween two nested linear models, one typically uses the chi-squared test,\ncomparing the drop in residual sum of squares (RSS) to a $\\chi^2_1$\ndistribution. But when this additional variable is not fixed, and has been\nchosen adaptively or greedily, this test is no longer appropriate: adaptivity\nmakes the drop in RSS stochastically much larger than $\\chi^2_1$ under the null\nhypothesis. Our analysis explicitly accounts for adaptivity, as it must, since\nthe lasso builds an adaptive sequence of linear models as the tuning parameter\n$\\lambda$ decreases. In this analysis, shrinkage plays a key role: though\nadditional variables are chosen adaptively, the coefficients of lasso active\nvariables are shrunken due to the $\\ell_1$ penalty. Therefore, the test\nstatistic (which is based on lasso fitted values) is in a sense balanced by\nthese two opposing properties - adaptivity and shrinkage - and its null\ndistribution is tractable and asymptotically $\\operatorname {Exp}(1)$.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2013 08:35:55 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2013 21:07:34 GMT"}, {"version": "v3", "created": "Mon, 26 May 2014 05:22:25 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Lockhart", "Richard", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Ryan J.", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1301.7212", "submitter": "Hannes Sieling", "authors": "Klaus Frick, Axel Munk and Hannes Sieling", "title": "Multiscale Change-Point Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new estimator SMUCE (simultaneous multiscale change-point\nestimator) for the change-point problem in exponential family regression. An\nunknown step function is estimated by minimizing the number of change-points\nover the acceptance region of a multiscale test at a level \\alpha. The\nprobability of overestimating the true number of change-points K is controlled\nby the asymptotic null distribution of the multiscale test statistic. Further,\nwe derive exponential bounds for the probability of underestimating K. By\nbalancing these quantities, \\alpha will be chosen such that the probability of\ncorrectly estimating K is maximized. All results are even non-asymptotic for\nthe normal case. Based on the aforementioned bounds, we construct\nasymptotically honest confidence sets for the unknown step function and its\nchange-points. At the same time, we obtain exponential bounds for estimating\nthe change-point locations which for example yield the minimax rate O(1/n) up\nto a log term. Finally, SMUCE asymptotically achieves the optimal detection\nrate of vanishing signals. We illustrate how dynamic programming techniques can\nbe employed for efficient computation of estimators and confidence regions. The\nperformance of the proposed multiscale approach is illustrated by simulations\nand in two cutting-edge applications from genetic engineering and photoemission\nspectroscopy.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2013 13:02:44 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2013 12:33:50 GMT"}, {"version": "v3", "created": "Mon, 12 Aug 2013 09:36:05 GMT"}], "update_date": "2013-08-13", "authors_parsed": [["Frick", "Klaus", ""], ["Munk", "Axel", ""], ["Sieling", "Hannes", ""]]}, {"id": "1301.7377", "submitter": "Clark Glymour", "authors": "Clark Glymour", "title": "Psychological and Normative Theories of Causal Power and the\n  Probabilities of Causes", "comments": "Appears in Proceedings of the Fourteenth Conference on Uncertainty in\n  Artificial Intelligence (UAI1998)", "journal-ref": null, "doi": null, "report-no": "UAI-P-1998-PG-166-172", "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper (1)shows that the best supported current psychological theory\n(Cheng, 1997) of how human subjects judge the causal power or influence of\nvariations in presence or absence of one feature on another, given data on\ntheir covariation, tacitly uses a Bayes network which is either a noisy or gate\n(for causes that promote the effect) or a noisy and gate (for causes that\ninhibit the effect); (2)generalizes Chengs theory to arbitrary acyclic networks\nof noisy or and noisy and gates; (3)gives various sufficient conditions for the\nestimation of the parameters in such networks when there are independent,\nunobserved causes; (4)distinguishes direct causal influence of one feature on\nanother (influence along a path with one edge) from total influence (influence\nalong all paths from one variable to another) and gives sufficient conditions\nfor estimating each when there are unobserved causes of the outcome variable;\n(5)describes the relation between Cheng models and a simplified version of the\nRubin framework for representing causal relations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2013 15:03:57 GMT"}], "update_date": "2013-02-01", "authors_parsed": [["Glymour", "Clark", ""]]}, {"id": "1301.7703", "submitter": "George Karabatsos Ph.D.", "authors": "George Karabatsos, Elizabeth Talbott, and Stephen G. Walker", "title": "A Bayesian Nonparametric Meta-Analysis Model", "comments": "2 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a meta-analysis, it is important to specify a model that adequately\ndescribes the effect-size distribution of the underlying population of studies.\nThe conventional normal fixed-effect and normal random-effects models assume a\nnormal effect-size population distribution, conditionally on parameters and\ncovariates. For estimating the mean overall effect size, such models may be\nadequate, but for prediction they surely are not if the effect size\ndistribution exhibits non-normal behavior. To address this issue, we propose a\nBayesian nonparametric meta-analysis model, which can describe a wider range of\neffect-size distributions, including unimodal symmetric distributions, as well\nas skewed and more multimodal distributions. We demonstrate our model through\nthe analysis of real meta-analytic data arising from behavioral-genetic\nresearch. We compare the predictive performance of the Bayesian nonparametric\nmodel against various conventional and more modern normal fixed-effects and\nrandom-effects models.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2013 17:53:50 GMT"}, {"version": "v2", "created": "Fri, 1 Feb 2013 02:44:39 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2013 21:31:40 GMT"}], "update_date": "2013-10-21", "authors_parsed": [["Karabatsos", "George", ""], ["Talbott", "Elizabeth", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1301.7745", "submitter": "Justin Kinney", "authors": "Justin B. Kinney, Gurinder S. Atwal", "title": "Equitability, mutual information, and the maximal information\n  coefficient", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.1309933111", "report-no": null, "categories": "q-bio.QM math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reshef et al. recently proposed a new statistical measure, the \"maximal\ninformation coefficient\" (MIC), for quantifying arbitrary dependencies between\npairs of stochastic quantities. MIC is based on mutual information, a\nfundamental quantity in information theory that is widely understood to serve\nthis need. MIC, however, is not an estimate of mutual information. Indeed, it\nwas claimed that MIC possesses a desirable mathematical property called\n\"equitability\" that mutual information lacks. This was not proven; instead it\nwas argued solely through the analysis of simulated data. Here we show that\nthis claim, in fact, is incorrect. First we offer mathematical proof that no\n(non-trivial) dependence measure satisfies the definition of equitability\nproposed by Reshef et al.. We then propose a self-consistent and more general\ndefinition of equitability that follows naturally from the Data Processing\nInequality. Mutual information satisfies this new definition of equitability\nwhile MIC does not. Finally, we show that the simulation evidence offered by\nReshef et al. was artifactual. We conclude that estimating mutual information\nis not only practical for many real-world applications, but also provides a\nnatural solution to the problem of quantifying associations in large data sets.\n", "versions": [{"version": "v1", "created": "Thu, 31 Jan 2013 20:44:28 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Kinney", "Justin B.", ""], ["Atwal", "Gurinder S.", ""]]}]