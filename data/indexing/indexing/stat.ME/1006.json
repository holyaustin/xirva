[{"id": "1006.0042", "submitter": "Mark Tygert", "authors": "William Perkins, Mark Tygert, and Rachel Ward", "title": "Computing the confidence levels for a root-mean-square test of\n  goodness-of-fit", "comments": "19 pages, 8 figures, 3 tables", "journal-ref": "Applied Mathematics and Computation, 217 (22): 9072-9084, 2011", "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classic chi-squared statistic for testing goodness-of-fit has long been a\ncornerstone of modern statistical practice. The statistic consists of a sum in\nwhich each summand involves division by the probability associated with the\ncorresponding bin in the distribution being tested for goodness-of-fit.\nTypically this division should precipitate rebinning to uniformize the\nprobabilities associated with the bins, in order to make the test reasonably\npowerful. With the now widespread availability of computers, there is no longer\nany need for this. The present paper provides efficient black-box algorithms\nfor calculating the asymptotic confidence levels of a variant on the classic\nchi-squared test which omits the problematic division. In many circumstances,\nit is also feasible to compute the exact confidence levels via Monte Carlo\nsimulation.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2010 00:42:24 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2010 14:37:23 GMT"}, {"version": "v3", "created": "Tue, 9 Nov 2010 19:10:29 GMT"}, {"version": "v4", "created": "Thu, 2 Dec 2010 19:25:34 GMT"}, {"version": "v5", "created": "Mon, 6 Dec 2010 19:59:29 GMT"}, {"version": "v6", "created": "Wed, 12 Jan 2011 17:19:00 GMT"}, {"version": "v7", "created": "Mon, 7 Mar 2011 20:35:09 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Perkins", "William", ""], ["Tygert", "Mark", ""], ["Ward", "Rachel", ""]]}, {"id": "1006.0554", "submitter": "Christian P. Robert", "authors": "Nicolas Chopin (CREST, Paris), Alessandra Iacobucci (Paris-Dauphine),\n  Jean-Michel Marin (I3M, Montpellier 2), Kerrie Mengersen (QUT), Christian P.\n  Robert (Paris-Dauphine and CREST), Robin Ryder (Paris-Dauphine and CREST),\n  and Christian Sch\\\"afer (Paris-Dauphine and CREST)", "title": "On Particle Learning", "comments": "14 pages, 9 figures, discussions on the invited paper of Lopes,\n  Carvalho, Johannes, and Polson, for the Ninth Valencia International Meeting\n  on Bayesian Statistics, held in Benidorm, Spain, on June 3-8, 2010. To appear\n  in Bayesian Statistics 9, Oxford University Press (except for the final\n  discussion)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document is the aggregation of six discussions of Lopes et al. (2010)\nthat we submitted to the proceedings of the Ninth Valencia Meeting, held in\nBenidorm, Spain, on June 3-8, 2010, in conjunction with Hedibert Lopes' talk at\nthis meeting, and of a further discussion of the rejoinder by Lopes et al.\n(2010). The main point in those discussions is the potential for degeneracy in\nthe particle learning methodology, related with the exponential forgetting of\nthe past simulations. We illustrate in particular the resulting difficulties in\nthe case of mixtures.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2010 05:08:16 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2010 16:42:16 GMT"}, {"version": "v3", "created": "Fri, 19 Nov 2010 08:32:36 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Chopin", "Nicolas", "", "CREST, Paris"], ["Iacobucci", "Alessandra", "", "Paris-Dauphine"], ["Marin", "Jean-Michel", "", "I3M, Montpellier 2"], ["Mengersen", "Kerrie", "", "QUT"], ["Robert", "Christian P.", "", "Paris-Dauphine and CREST"], ["Ryder", "Robin", "", "Paris-Dauphine and CREST"], ["Sch\u00e4fer", "Christian", "", "Paris-Dauphine and CREST"]]}, {"id": "1006.0621", "submitter": "Silvia Pandolfi Miss", "authors": "S. Pandolfi, F. Bartolucci, N. Friel", "title": "A generalized Multiple-try Metropolis version of the Reversible Jump\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Reversible Jump algorithm is one of the most widely used Markov chain\nMonte Carlo algorithms for Bayesian estimation and model selection. A\ngeneralized multiple-try version of this algorithm is proposed. The algorithm\nis based on drawing several proposals at each step and randomly choosing one of\nthem on the basis of weights (selection probabilities) that may be arbitrary\nchosen. Among the possible choices, a method is employed which is based on\nselection probabilities depending on a quadratic approximation of the posterior\ndistribution. Moreover, the implementation of the proposed algorithm for\nchallenging model selection problems, in which the quadratic approximation is\nnot feasible, is considered. The resulting algorithm leads to a gain in\nefficiency with respect to the Reversible Jump algorithm, and also in terms of\ncomputational effort. The performance of this approach is illustrated for real\nexamples involving a logistic regression model and a latent class model.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jun 2010 11:38:08 GMT"}, {"version": "v2", "created": "Fri, 11 Oct 2013 13:56:24 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Pandolfi", "S.", ""], ["Bartolucci", "F.", ""], ["Friel", "N.", ""]]}, {"id": "1006.1030", "submitter": "Andrej Kastrin", "authors": "Andrej Kastrin, Borut Peterlin", "title": "Rasch-based high-dimensionality data reduction and class prediction with\n  applications to microarray gene expression data", "comments": null, "journal-ref": "Expert Systems with Applications, 2010;37(7):5178-5185", "doi": "10.1016/j.eswa.2009.12.074", "report-no": null, "categories": "cs.AI stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class prediction is an important application of microarray gene expression\ndata analysis. The high-dimensionality of microarray data, where number of\ngenes (variables) is very large compared to the number of samples (obser-\nvations), makes the application of many prediction techniques (e.g., logistic\nregression, discriminant analysis) difficult. An efficient way to solve this\nprob- lem is by using dimension reduction statistical techniques. Increasingly\nused in psychology-related applications, Rasch model (RM) provides an appealing\nframework for handling high-dimensional microarray data. In this paper, we\nstudy the potential of RM-based modeling in dimensionality reduction with\nbinarized microarray gene expression data and investigate its prediction ac-\ncuracy in the context of class prediction using linear discriminant analysis.\nTwo different publicly available microarray data sets are used to illustrate a\ngeneral framework of the approach. Performance of the proposed method is\nassessed by re-randomization scheme using principal component analysis (PCA) as\na benchmark method. Our results show that RM-based dimension reduction is as\neffective as PCA-based dimension reduction. The method is general and can be\napplied to the other high-dimensional data problems.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2010 08:27:29 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kastrin", "Andrej", ""], ["Peterlin", "Borut", ""]]}, {"id": "1006.1062", "submitter": "Ryan Adams", "authors": "Ryan Prescott Adams, Zoubin Ghahramani, Michael I. Jordan", "title": "Tree-Structured Stick Breaking Processes for Hierarchical Data", "comments": "16 pages, 5 figures, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data are naturally modeled by an unobserved hierarchical structure. In\nthis paper we propose a flexible nonparametric prior over unknown data\nhierarchies. The approach uses nested stick-breaking processes to allow for\ntrees of unbounded width and depth, where data can live at any node and are\ninfinitely exchangeable. One can view our model as providing infinite mixtures\nwhere the components have a dependency structure corresponding to an\nevolutionary diffusion down a tree. By using a stick-breaking approach, we can\napply Markov chain Monte Carlo methods based on slice sampling to perform\nBayesian inference and simulate from the posterior distribution on trees. We\napply our method to hierarchical clustering of images and topic modeling of\ntext data.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2010 18:52:13 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Adams", "Ryan Prescott", ""], ["Ghahramani", "Zoubin", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1006.1146", "submitter": "Z. John Daye", "authors": "X. Jessie Jeng And Z. John Daye", "title": "Sparse covariance thresholding for high-dimensional variable selection", "comments": "To appear in Statistica Sinica", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensions, many variable selection methods, such as the lasso, are\noften limited by excessive variability and rank deficiency of the sample\ncovariance matrix. Covariance sparsity is a natural phenomenon in\nhigh-dimensional applications, such as microarray analysis, image processing,\netc., in which a large number of predictors are independent or weakly\ncorrelated. In this paper, we propose the covariance-thresholded lasso, a new\nclass of regression methods that can utilize covariance sparsity to improve\nvariable selection. We establish theoretical results, under the random design\nsetting, that relate covariance sparsity to variable selection. Real-data and\nsimulation examples indicate that our method can be useful in improving\nvariable selection performances.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2010 22:51:30 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Daye", "X. Jessie Jeng And Z. John", ""]]}, {"id": "1006.1350", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Zoubin Ghahramani", "title": "Copula Processes", "comments": "11 pages, 1 table, 1 figure. Submitted for publication. Since last\n  version: minor edits and reformatting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR q-fin.CP q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a copula process which describes the dependencies between\narbitrarily many random variables independently of their marginal\ndistributions. As an example, we develop a stochastic volatility model,\nGaussian Copula Process Volatility (GCPV), to predict the latent standard\ndeviations of a sequence of random variables. To make predictions we use\nBayesian inference, with the Laplace approximation, and with Markov chain Monte\nCarlo as an alternative. We find both methods comparable. We also find our\nmodel can outperform GARCH on simulated and financial data. And unlike GARCH,\nGCPV can easily handle missing data, incorporate covariates other than time,\nand model a rich class of covariance structures.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 19:59:50 GMT"}, {"version": "v2", "created": "Wed, 9 Jun 2010 18:17:41 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1006.1514", "submitter": "Stephen Leslie", "authors": "Peter Donnelly and Stephen Leslie", "title": "The coalescent and its descendants", "comments": "34 pages. To appear in Bingham, N. H., and Goldie, C. M. (eds),\n  Probability and Mathematical Genetics: Papers in Honour of Sir John Kingman.\n  London Math. Soc. Lecture Note Series vol. 378. Cambridge: Cambridge Univ.\n  Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coalescent revolutionised theoretical population genetics, simplifying,\nor making possible for the first time, many analyses, proofs, and derivations,\nand offering crucial insights about the way in which the structure of data in\nsamples from populations depends on the demographic history of the population.\nHowever statistical inference under the coalescent model is extremely\nchallenging, effectively because no explicit expressions are available for key\nsampling probabilities. This led initially to approximation of these\nprobabilities by ingenious application of modern computationally-intensive\nstatistical methods. A key breakthrough occurred when Li and Stephens\nintroduced a different model, similar in spirit to the coalescent, for which\nefficient calculations are feasible. In turn, the Li and Stephens model has\nchanged statistical inference for the wealth of data now available which\ndocuments molecular genetic variation within populations. We briefly review the\ncoalescent and associated measure-valued diffusions, describe the Li and\nStephens model, and introduce and apply a generalisation of it for inference of\npopulation structure in the presence of linkage disequilibrium.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 10:19:32 GMT"}], "update_date": "2010-06-09", "authors_parsed": [["Donnelly", "Peter", ""], ["Leslie", "Stephen", ""]]}, {"id": "1006.1567", "submitter": "Gabriel Lang", "authors": "Gabriel Lang and Eric Marcon", "title": "Testing randomness of spatial point patterns with the Ripley statistic", "comments": "26 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregation patterns are often visually detected in sets of location data.\nThese clusters may be the result of interesting dynamics or the effect of pure\nrandomness. We build an asymptotically Gaussian test for the hypothesis of\nrandomness corresponding to a Poisson point process. We first compute the exact\nfirst and second moment of the Ripley K-statistic under the homogeneous Poisson\npoint process model. Then we prove the asymptotic normality of a vector of such\nstatistics for different scales and compute its covariance matrix. From these\nresults, we derive a test statistic that is chi-square distributed. By a\nMonte-Carlo study, we check that the test is numerically tractable even for\nlarge data sets and also correct when only a hundred of points are observed.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 14:24:37 GMT"}], "update_date": "2010-06-09", "authors_parsed": [["Lang", "Gabriel", ""], ["Marcon", "Eric", ""]]}, {"id": "1006.1572", "submitter": "Hailin Sang", "authors": "Magda Peligrad and Hailin Sang", "title": "Asymptotic Properties of Self-Normalized Linear Processes with Long\n  Memory", "comments": "23 pages, To appear in Econometric Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the convergence to fractional Brownian motion for long\nmemory time series having independent innovations with infinite second moment.\nFor the sake of applications we derive the self-normalized version of this\ntheorem. The study is motivated by models arising in economical applications\nwhere often the linear processes have long memory, and the innovations have\nheavy tails.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 14:39:13 GMT"}, {"version": "v2", "created": "Mon, 25 Oct 2010 20:23:03 GMT"}, {"version": "v3", "created": "Tue, 31 May 2011 15:14:41 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Peligrad", "Magda", ""], ["Sang", "Hailin", ""]]}, {"id": "1006.1860", "submitter": "Jan C. Neddermeyer", "authors": "Rainer Dahlhaus and Jan C. Neddermeyer", "title": "On-line Spot Volatility-Estimation and Decomposition with Nonlinear\n  Market Microstructure Noise Models", "comments": "10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A technique for on-line estimation of spot volatility for high-frequency data\nis developed. The algorithm works directly on the transaction data and updates\nthe volatility estimate immediately after the occurrence of a new transaction.\nFurthermore, a nonlinear market microstructure noise model is proposed that\nreproduces several stylized facts of high-frequency data. A computationally\nefficient particle filter is used that allows for the approximation of the\nunknown efficient prices and, in combination with a recursive EM algorithm, for\nthe estimation of the volatility curve. We neither assume that the transaction\ntimes are equidistant nor do we use interpolated prices. We also make a\ndistinction between volatility per time unit and volatility per transaction and\nprovide estimators for both. More precisely we use a model with random time\nchange where spot volatility is decomposed into spot volatility per transaction\ntimes the trading intensity - thus highlighting the influence of trading\nintensity on volatility.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 17:33:56 GMT"}, {"version": "v2", "created": "Mon, 18 Apr 2011 19:27:37 GMT"}, {"version": "v3", "created": "Mon, 13 Aug 2012 18:45:28 GMT"}, {"version": "v4", "created": "Sun, 13 Jan 2013 21:14:28 GMT"}], "update_date": "2013-01-15", "authors_parsed": [["Dahlhaus", "Rainer", ""], ["Neddermeyer", "Jan C.", ""]]}, {"id": "1006.1914", "submitter": "Robert Kohn", "authors": "Michael Pitt, Ralph Silva, Paolo Giordani and Robert Kohn", "title": "Auxiliary Particle filtering within adaptive Metropolis-Hastings\n  Sampling", "comments": "32 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our article deals with Bayesian inference for a general state space model\nwith the simulated likelihood computed by the particle filter. We show\nempirically that the partially or fully adapted particle filters can be much\nmore efficient than the standard particle, especially when the signal to noise\nratio is high. This is especially important because using the particle filter\nwithin MCMC sampling is O(T^2), where T is the sample size. We also show that\nan adaptive independent proposal for the unknown parameters based on a mixture\nof normals can be much more efficient than the usual optimal random walk\nmethods because the simulated likelihood is not continuous in the parameters\nand the cost of constructing a good adaptive proposal is negligible compared to\nthe cost of evaluating the simulated likelihood. Independent \\MH proposals are\nalso attractive because they are easy to run in parallel on multiple\nprocessors. The article also shows that the proposed \\aimh sampler converges to\nthe posterior distribution. We also show that the marginal likelihood of any\nstate space model can be obtained in an efficient and unbiased manner by using\nthe \\pf making model comparison straightforward. Obtaining the marginal\nlikelihood is often difficult using other methods. Finally, we prove that the\nsimulated likelihood obtained by the auxiliary particle filter is unbiased.\nThis result is fundamental to using the particle for MCMC sampling and is first\nobtained in a more abstract and difficult setting by Del Moral (2004). However,\nour proof is direct and will make the result accessible to readers.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 22:07:29 GMT"}], "update_date": "2010-06-11", "authors_parsed": [["Pitt", "Michael", ""], ["Silva", "Ralph", ""], ["Giordani", "Paolo", ""], ["Kohn", "Robert", ""]]}, {"id": "1006.2070", "submitter": "Grigory Temnov Dr", "authors": "Lev B. Klebanov and Grigory Temnov", "title": "Characterization of a subclass of Tweedie distributions by a property of\n  generalized stability", "comments": "Research paper, 15 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of distributions originating from an exponential family\nand having a property related to the strict stability property. A\ncharacteristic function representation for this family is obtained and its\nproperties are investigated. The proposed class relates to stable distributions\nand includes Inverse Gaussian distribution and Levy distribution as special\ncases. Due to its origin, the proposed distribution has a sufficient statistic.\nBesides, it combines stability property at lower scales with an exponential\ndecay of the distribution's tail and has an additional flexibility due to the\nconvenient parametrization. Apart from the basic model, certain generalizations\nare considered, including the one related to geometric stable distributions.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2010 15:56:06 GMT"}], "update_date": "2010-06-11", "authors_parsed": [["Klebanov", "Lev B.", ""], ["Temnov", "Grigory", ""]]}, {"id": "1006.2165", "submitter": "Marc Deisenroth", "authors": "Marc Peter Deisenroth and Henrik Ohlsson", "title": "A Probabilistic Perspective on Gaussian Filtering and Smoothing", "comments": "14 pages. Extended version of conference paper (ACC 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.RO cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general probabilistic perspective on Gaussian filtering and\nsmoothing. This allows us to show that common approaches to Gaussian\nfiltering/smoothing can be distinguished solely by their methods of\ncomputing/approximating the means and covariances of joint probabilities. This\nimplies that novel filters and smoothers can be derived straightforwardly by\nproviding methods for computing these moments. Based on this insight, we derive\nthe cubature Kalman smoother and propose a novel robust filtering and smoothing\nalgorithm based on Gibbs sampling.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2010 22:23:23 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2010 00:14:05 GMT"}, {"version": "v3", "created": "Mon, 9 Aug 2010 01:36:08 GMT"}, {"version": "v4", "created": "Mon, 4 Apr 2011 20:54:13 GMT"}, {"version": "v5", "created": "Wed, 8 Jun 2011 06:15:34 GMT"}], "update_date": "2011-06-09", "authors_parsed": [["Deisenroth", "Marc Peter", ""], ["Ohlsson", "Henrik", ""]]}, {"id": "1006.2300", "submitter": "Gael Varoquaux", "authors": "G. Varoquaux (INRIA Saclay - Ile de France, LNAO), S. Sadaghiani\n  (LCogn), P. Pinel (LCogn), A. Kleinschmidt (LCogn), J. B. Poline (LNAO), B.\n  Thirion (INRIA Saclay - Ile de France, LNAO)", "title": "A group model for stable multi-subject ICA on fMRI datasets", "comments": null, "journal-ref": "NeuroImage 2010;51(1):288-99", "doi": "10.1016/j.neuroimage.2010.02.010", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial Independent Component Analysis (ICA) is an increasingly used\ndata-driven method to analyze functional Magnetic Resonance Imaging (fMRI)\ndata. To date, it has been used to extract sets of mutually correlated brain\nregions without prior information on the time course of these regions. Some of\nthese sets of regions, interpreted as functional networks, have recently been\nused to provide markers of brain diseases and open the road to paradigm-free\npopulation comparisons. Such group studies raise the question of modeling\nsubject variability within ICA: how can the patterns representative of a group\nbe modeled and estimated via ICA for reliable inter-group comparisons? In this\npaper, we propose a hierarchical model for patterns in multi-subject fMRI\ndatasets, akin to mixed-effect group models used in linear-model-based\nanalysis. We introduce an estimation procedure, CanICA (Canonical ICA), based\non i) probabilistic dimension reduction of the individual data, ii) canonical\ncorrelation analysis to identify a data subspace common to the group iii)\nICA-based pattern extraction. In addition, we introduce a procedure based on\ncross-validation to quantify the stability of ICA patterns at the level of the\ngroup. We compare our method with state-of-the-art multi-subject fMRI ICA\nmethods and show that the features extracted using our procedure are more\nreproducible at the group level on two datasets of 12 healthy controls: a\nresting-state and a functional localizer study.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2010 13:29:05 GMT"}], "update_date": "2011-02-08", "authors_parsed": [["Varoquaux", "G.", "", "INRIA Saclay - Ile de France, LNAO"], ["Sadaghiani", "S.", "", "LCogn"], ["Pinel", "P.", "", "LCogn"], ["Kleinschmidt", "A.", "", "LCogn"], ["Poline", "J. B.", "", "LNAO"], ["Thirion", "B.", "", "INRIA Saclay - Ile de France, LNAO"]]}, {"id": "1006.2592", "submitter": "Yiyuan She", "authors": "Yiyuan She and Art B. Owen", "title": "Outlier Detection Using Nonconvex Penalized Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the outlier detection problem from the point of view of\npenalized regressions. Our regression model adds one mean shift parameter for\neach of the $n$ data points. We then apply a regularization favoring a sparse\nvector of mean shift parameters. The usual $L_1$ penalty yields a convex\ncriterion, but we find that it fails to deliver a robust estimator. The $L_1$\npenalty corresponds to soft thresholding. We introduce a thresholding (denoted\nby $\\Theta$) based iterative procedure for outlier detection ($\\Theta$-IPOD). A\nversion based on hard thresholding correctly identifies outliers on some hard\ntest problems. We find that $\\Theta$-IPOD is much faster than iteratively\nreweighted least squares for large data because each iteration costs at most\n$O(np)$ (and sometimes much less) avoiding an $O(np^2)$ least squares estimate.\nWe describe the connection between $\\Theta$-IPOD and $M$-estimators. Our\nproposed method has one tuning parameter with which to both identify outliers\nand estimate regression coefficients. A data-dependent choice can be made based\non BIC. The tuned $\\Theta$-IPOD shows outstanding performance in identifying\noutliers in various situations in comparison to other existing approaches. This\nmethodology extends to high-dimensional modeling with $p\\gg n$, if both the\ncoefficient vector and the outlier pattern are sparse.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2010 02:51:41 GMT"}, {"version": "v2", "created": "Thu, 30 Sep 2010 19:04:02 GMT"}, {"version": "v3", "created": "Mon, 17 Oct 2011 02:23:15 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["She", "Yiyuan", ""], ["Owen", "Art B.", ""]]}, {"id": "1006.2871", "submitter": "Nengfeng Zhou", "authors": "Nengfeng Zhou and Ji Zhu", "title": "Group Variable Selection via a Hierarchical Lasso and Its Oracle\n  Property", "comments": "43 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many engineering and scientific applications, prediction variables are\ngrouped, for example, in biological applications where assayed genes or\nproteins can be grouped by biological roles or biological pathways. Common\nstatistical analysis methods such as ANOVA, factor analysis, and functional\nmodeling with basis sets also exhibit natural variable groupings. Existing\nsuccessful group variable selection methods such as Antoniadis and Fan (2001),\nYuan and Lin (2006) and Zhao, Rocha and Yu (2009) have the limitation of\nselecting variables in an \"all-in-all-out\" fashion, i.e., when one variable in\na group is selected, all other variables in the same group are also selected.\nIn many real problems, however, we may want to keep the flexibility of\nselecting variables within a group, such as in gene-set selection. In this\npaper, we develop a new group variable selection method that not only removes\nunimportant groups effectively, but also keeps the flexibility of selecting\nvariables within a group. We also show that the new method offers the potential\nfor achieving the theoretical \"oracle\" property as in Fan and Li (2001) and Fan\nand Peng (2004).\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 01:06:30 GMT"}, {"version": "v2", "created": "Mon, 9 Aug 2010 16:52:19 GMT"}], "update_date": "2010-08-10", "authors_parsed": [["Zhou", "Nengfeng", ""], ["Zhu", "Ji", ""]]}, {"id": "1006.2940", "submitter": "Zhou Fang", "authors": "Zhou Fang and Nicolai Meinshausen", "title": "LASSO ISOtone for High Dimensional Additive Isotonic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive isotonic regression attempts to determine the relationship between a\nmulti-dimensional observation variable and a response, under the constraint\nthat the estimate is the additive sum of univariate component effects that are\nmonotonically increasing. In this article, we present a new method for such\nregression called LASSO Isotone (LISO). LISO adapts ideas from sparse linear\nmodelling to additive isotonic regression. Thus, it is viable in many\nsituations with high dimensional predictor variables, where selection of\nsignificant versus insignificant variables are required. We suggest an\nalgorithm involving a modification of the backfitting algorithm CPAV. We give a\nnumerical convergence result, and finally examine some of its properties\nthrough simulations. We also suggest some possible extensions that improve\nperformance, and allow calculation to be carried out when the direction of the\nmonotonicity is unknown.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 09:52:14 GMT"}], "update_date": "2010-06-16", "authors_parsed": [["Fang", "Zhou", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1006.3342", "submitter": "Hugh Miller Dr", "authors": "Hugh Miller and Peter Hall", "title": "Local polynomial regression and variable selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for incorporating variable selection into local\npolynomial regression. This can improve the accuracy of the regression by\nextending the bandwidth in directions corresponding to those variables judged\nto be are unimportant. It also increases our understanding of the dataset by\nhighlighting areas where these variables are redundant. The approach has the\npotential to effect complete variable removal as well as perform partial\nremoval when a variable redundancy applies only to particular regions of the\ndata. We define a nonparametric oracle property and show that this is more than\nsatisfied by our approach under asymptotic analysis. The usefulness of the\nmethod is demonstrated through simulated and real data numerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2010 23:03:08 GMT"}], "update_date": "2010-06-18", "authors_parsed": [["Miller", "Hugh", ""], ["Hall", "Peter", ""]]}, {"id": "1006.3436", "submitter": "Konstantin Usevich", "authors": "Konstantin Usevich", "title": "On signal and extraneous roots in Singular Spectrum Analysis", "comments": "24 pages, 7 figures", "journal-ref": "Statistics and Its Interface, 2010, Vol. 3(3):281-295", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present paper we study properties of roots of characteristic\npolynomials for the linear recurrent formulae (LRF) that govern time series. We\nalso investigate how the values of these roots affect Singular Spectrum\nAnalysis implications, in what concerns separation of components, SSA\nforecasting and related signal parameter estimation methods. The roots of the\ncharacteristic polynomial for an LRF comprise the signal roots, which determine\nthe structure of the time series, and extraneous roots. We show how the\nseparability of two time series can be characterized in terms of their signal\nroots. All possible cases of exact separability are enumerated. We also examine\nproperties of extraneous roots of the LRF used in SSA forecasting algorithms,\nwhich is equivalent to the Min-Norm vector in subspace-based estimation\nmethods. We apply recent theoretical results for orthogonal polynomials on the\nunit circle, which enable us to precisely describe the asymptotic distribution\nof extraneous roots relative to the position of the signal roots.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jun 2010 11:47:50 GMT"}], "update_date": "2013-03-08", "authors_parsed": [["Usevich", "Konstantin", ""]]}, {"id": "1006.3690", "submitter": "Yanan Fan Dr", "authors": "P. H. Garthwaite and Y. Fan and S. A. Sisson", "title": "Adaptive Optimal Scaling of Metropolis-Hastings Algorithms Using the\n  Robbins-Monro Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adaptive method for the automatic scaling of Random-Walk\nMetropolis-Hastings algorithms, which quickly and robustly identifies the\nscaling factor that yields a specified overall sampler acceptance probability.\nOur method relies on the use of the Robbins-Monro search process, whose\nperformance is determined by an unknown steplength constant. We give a very\nsimple estimator of this constant for proposal distributions that are\nunivariate or multivariate normal, together with a sampling algorithm for\nautomating the method. The effectiveness of the algorithm is demonstrated with\nboth simulated and real data examples. This approach could be implemented as a\nuseful component in more complex adaptive Markov chain Monte Carlo algorithms,\nor as part of automated software packages.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 13:22:24 GMT"}], "update_date": "2010-06-21", "authors_parsed": [["Garthwaite", "P. H.", ""], ["Fan", "Y.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1006.3707", "submitter": "Wolfgang Waltenberger", "authors": "Rudolf Fr\\\"uhwirth and Wolfgang Waltenberger", "title": "Redescending M-estimators and Deterministic Annealing, with Applications\n  to Robust Regression and Tail Index Estimation", "comments": null, "journal-ref": "Austrian Journal of Statistics, Volume 37 (2008), Number 3 & 4,\n  301-317", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new type of redescending M-estimators is constructed, based on data\naugmentation with an unspecified outlier model. Necessary and sufficient\nconditions for the convergence of the resulting estimators to the Hubertype\nskipped mean are derived. By introducing a temperature parameter the concept of\ndeterministic annealing can be applied, making the estimator insensitive to the\nstarting point of the iteration. The properties of the annealing M-estimator as\na function of the temperature are explored. Finally, two applications are\npresented. The first one is the robust estimation of interaction vertices in\nexperimental particle physics, including outlier detection. The second one is\nthe estimation of the tail index of a distribution from a sample using robust\nregression diagnostics.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 14:34:12 GMT"}], "update_date": "2010-06-21", "authors_parsed": [["Fr\u00fchwirth", "Rudolf", ""], ["Waltenberger", "Wolfgang", ""]]}, {"id": "1006.3718", "submitter": "Vasiliy Krivtsov", "authors": "Mark Kaminskiy, Vasiliy Krivtsov", "title": "G1-Renewal Process as Repairable System Model", "comments": "11 pages, 1 table, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers a point process model with a monotonically decreasing or\nincreasing ROCOF and the underlying distributions from the location-scale\nfamily, known as the geometric process (Lam, 1988). In terms of repairable\nsystem reliability analysis, the process is capable of modeling various\nrestoration types including \"better-than-new\", i.e., the one not covered by the\npopular G-Renewal model (Kijima & Sumita, 1986). The distinctive property of\nthe process is that the times between successive events are obtained from the\nunderlying distributions as the scale parameter of each is monotonically\ndecreasing or increasing. The paper discusses properties and maximum likelihood\nestimation of the model for the case of the Exponential and Weibull underlying\ndistributions.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 15:19:21 GMT"}, {"version": "v2", "created": "Tue, 26 Oct 2010 19:36:20 GMT"}], "update_date": "2010-10-27", "authors_parsed": [["Kaminskiy", "Mark", ""], ["Krivtsov", "Vasiliy", ""]]}, {"id": "1006.3764", "submitter": "Erik Sauleau A", "authors": "Erik A. Sauleau, Valentina Mameli and Monica Musio", "title": "Using Integrated Nested Laplace Approximation for Modeling Spatial\n  Healthcare Utilization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, spatial and spatio-temporal modeling have become an\nimportant area of research in many fields (epidemiology, environmental studies,\ndisease mapping). In this work we propose different spatial models to study\nhospital recruitment, including some potentially explicative variables.\nInterest is on the distribution per geographical unit of the ratio between the\nnumber of patients living in this geographical unit and the population in the\nsame unit. Models considered are within the framework of Bayesian Latent\nGaussian models. Our response variable is assumed to follow a binomial\ndistribution, with logit link, whose parameters are the population in the\ngeographical unit and the corresponding relative risk. The structured additive\npredictor accounts for effects of various covariates in an additive way,\nincluding smoothing functions of the covariates (for example spatial effect),\nlinear effect of covariates. To approximate posterior marginals, which not\navailable in closed form, we use integrated nested Laplace approximations\n(INLA), recently proposed for approximate Bayesian inference in latent Gaussian\nmodels. INLA has the advantage of giving very accurate approximations and being\nfaster than McMC methods when the number of parameters does not exceed 6 (as it\nis in our case). Model comparisons are assessed using DIC criterion.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 18:08:42 GMT"}], "update_date": "2010-06-21", "authors_parsed": [["Sauleau", "Erik A.", ""], ["Mameli", "Valentina", ""], ["Musio", "Monica", ""]]}, {"id": "1006.3854", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universite Paris-Dauphine)", "title": "About incoherent inference", "comments": "This is an early draft for a letter to PNAS, reflecting only my\n  opinions on Templeton's 2010 PNAS paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Templeton (2010), the Approximate Bayesian Computation (ABC) algorithm\n(see, e.g., Pritchard et al., 1999, Beaumont et al., 2002, Marjoram et al.,\n2003, Ratmann et al., 2009) is criticised on mathematical and logical grounds:\n\"the [Bayesian] inference is mathematically incorrect and formally illogical\".\nSince those criticisms turn out to be bearing on Bayesian foundations rather\nthan on the computational methodology they are primarily directed at, we\nendeavour to point out in this note the statistical errors and inconsistencies\nin Templeton (2010), refering to Beaumont et al. (2010) for a reply that is\nbroader in scope since it also covers the phylogenetic aspects of nested clade\nversus a model-based approach.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2010 10:31:49 GMT"}], "update_date": "2010-06-22", "authors_parsed": [["Robert", "Christian P.", "", "Universite Paris-Dauphine"]]}, {"id": "1006.3901", "submitter": "Fabian Wauthier", "authors": "Fabian L. Wauthier and Michael I. Jordan", "title": "Heavy-Tailed Processes for Selective Shrinkage", "comments": "10 pages, 4 figures, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heavy-tailed distributions are frequently used to enhance the robustness of\nregression and classification methods to outliers in output space. Often,\nhowever, we are confronted with \"outliers\" in input space, which are isolated\nobservations in sparsely populated regions. We show that heavy-tailed\nstochastic processes (which we construct from Gaussian processes via a copula),\ncan be used to improve robustness of regression and classification estimators\nto such outliers by selectively shrinking them more strongly in sparse regions\nthan in dense regions. We carry out a theoretical analysis to show that\nselective shrinkage occurs, provided the marginals of the heavy-tailed process\nhave sufficiently heavy tails. The analysis is complemented by experiments on\nbiological data which indicate significant improvements of estimates in sparse\nregions while producing competitive results in dense regions.\n", "versions": [{"version": "v1", "created": "Sat, 19 Jun 2010 23:34:53 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2010 09:09:01 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Wauthier", "Fabian L.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1006.3972", "submitter": "John Lafferty", "authors": "Han Liu, Xi Chen, John Lafferty and Larry Wasserman", "title": "Graph-Valued Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphical models encode in a graph $G$ the dependency structure of\na random vector $Y$. In many applications, it is of interest to model $Y$ given\nanother random vector $X$ as input. We refer to the problem of estimating the\ngraph $G(x)$ of $Y$ conditioned on $X=x$ as ``graph-valued regression.'' In\nthis paper, we propose a semiparametric method for estimating $G(x)$ that\nbuilds a tree on the $X$ space just as in CART (classification and regression\ntrees), but at each leaf of the tree estimates a graph. We call the method\n``Graph-optimized CART,'' or Go-CART. We study the theoretical properties of\nGo-CART using dyadic partitioning trees, establishing oracle inequalities on\nrisk minimization and tree partition consistency. We also demonstrate the\napplication of Go-CART to a meteorological dataset, showing how graph-valued\nregression can provide a useful tool for analyzing complex data.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2010 00:56:37 GMT"}], "update_date": "2010-06-22", "authors_parsed": [["Liu", "Han", ""], ["Chen", "Xi", ""], ["Lafferty", "John", ""], ["Wasserman", "Larry", ""]]}, {"id": "1006.4432", "submitter": "Grace Chiu", "authors": "Grace S. Chiu and Anton H. Westveld", "title": "A Statistical Social Network Model for Consumption Data in Food Webs", "comments": "On 2013-09-05, a revised version entitled \"A Statistical Social\n  Network Model for Consumption Data in Trophic Food Webs\" was accepted for\n  publication in the upcoming Special Issue \"Statistical Methods for Ecology\"\n  in the journal Statistical Methodology", "journal-ref": null, "doi": "10.1016/j.stamet.2013.09.001", "report-no": null, "categories": "stat.ME q-bio.PE stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We adapt existing statistical modeling techniques for social networks to\nstudy consumption data observed in trophic food webs. These data describe the\nfeeding volume (non-negative) among organisms grouped into nodes, called\ntrophic species, that form the food web. Model complexity arises due to the\nextensive amount of zeros in the data, as each node in the web is predator/prey\nto only a small number of other trophic species. Many of the zeros are regarded\nas structural (non-random) in the context of feeding behavior. The presence of\nbasal prey and top predator nodes (those who never consume and those who are\nnever consumed, with probability 1) creates additional complexity to the\nstatistical modeling. We develop a special statistical social network model to\naccount for such network features. The model is applied to two empirical food\nwebs; focus is on the web for which the population size of seals is of concern\nto various commercial fisheries.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 07:15:44 GMT"}, {"version": "v2", "created": "Tue, 24 Aug 2010 06:58:56 GMT"}, {"version": "v3", "created": "Fri, 12 Jul 2013 15:49:59 GMT"}, {"version": "v4", "created": "Fri, 6 Sep 2013 04:49:23 GMT"}], "update_date": "2013-09-26", "authors_parsed": [["Chiu", "Grace S.", ""], ["Westveld", "Anton H.", ""]]}, {"id": "1006.4642", "submitter": "Badri Padhukasahasram", "authors": "Badri Padhukasahasram, Eran Halperin, Jennifer Wessel, Daryl Thomas,\n  Elana Silver, Heather Trumbower, Michele Cargill, Dietrich Stephan", "title": "Presymptomatic risk assessment for chronic non-communicable diseases", "comments": "Plos ONE paper. Previous version was withdrawn to be updated by the\n  journal's pdf version", "journal-ref": "2010 PLoS ONE 5(12): e14338. doi:10.1371/journal.pone.0014338", "doi": "10.1371/journal.pone.0014338", "report-no": null, "categories": "q-bio.GN q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The prevalence of common chronic non-communicable diseases (CNCDs) far\novershadows the prevalence of both monogenic and infectious diseases combined.\nAll CNCDs, also called complex genetic diseases, have a heritable genetic\ncomponent that can be used for pre-symptomatic risk assessment. Common single\nnucleotide polymorphisms (SNPs) that tag risk haplotypes across the genome\ncurrently account for a non-trivial portion of the germ-line genetic risk and\nwe will likely continue to identify the remaining missing heritability in the\nform of rare variants, copy number variants and epigenetic modifications. Here,\nwe describe a novel measure for calculating the lifetime risk of a disease,\ncalled the genetic composite index (GCI), and demonstrate its predictive value\nas a clinical classifier. The GCI only considers summary statistics of the\neffects of genetic variation and hence does not require the results of\nlarge-scale studies simultaneously assessing multiple risk factors. Combining\nGCI scores with environmental risk information provides an additional tool for\nclinical decision-making. The GCI can be populated with heritable risk\ninformation of any type, and thus represents a framework for CNCD\npre-symptomatic risk assessment that can be populated as additional risk\ninformation is identified through next-generation technologies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 20:48:49 GMT"}, {"version": "v2", "created": "Tue, 26 Oct 2010 06:46:47 GMT"}, {"version": "v3", "created": "Tue, 4 Jan 2011 21:56:29 GMT"}, {"version": "v4", "created": "Fri, 24 Jun 2011 19:43:36 GMT"}, {"version": "v5", "created": "Tue, 28 Jun 2011 06:37:41 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Padhukasahasram", "Badri", ""], ["Halperin", "Eran", ""], ["Wessel", "Jennifer", ""], ["Thomas", "Daryl", ""], ["Silver", "Elana", ""], ["Trumbower", "Heather", ""], ["Cargill", "Michele", ""], ["Stephan", "Dietrich", ""]]}, {"id": "1006.4801", "submitter": "Soosan Beheshti", "authors": "Soosan Beheshti, Masoud Hashemi, Xiao-Ping Zhang, and Nima Nikvand", "title": "Noise Invalidation Denoising", "comments": "9 pages, journal submission", "journal-ref": null, "doi": "10.1109/TSP.2010.2074199", "report-no": null, "categories": "stat.ME cs.CV math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A denoising technique based on noise invalidation is proposed. The adaptive\napproach derives a noise signature from the noise order statistics and utilizes\nthe signature to denoise the data. The novelty of this approach is in\npresenting a general-purpose denoising in the sense that it does not need to\nemploy any particular assumption on the structure of the noise-free signal,\nsuch as data smoothness or sparsity of the coefficients. An advantage of the\nmethod is in denoising the corrupted data in any complete basis transformation\n(orthogonal or non-orthogonal). Experimental results show that the proposed\nmethod, called Noise Invalidation Denoising (NIDe), outperforms existing\ndenoising approaches in terms of Mean Square Error (MSE).\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2010 14:23:09 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Beheshti", "Soosan", ""], ["Hashemi", "Masoud", ""], ["Zhang", "Xiao-Ping", ""], ["Nikvand", "Nima", ""]]}, {"id": "1006.4818", "submitter": "Namrata Vaswani", "authors": "Namrata Vaswani", "title": "Stability (over time) of Modified-CS and LS-CS for Recursive Causal\n  Sparse Reconstruction", "comments": "15 pages, one figure with four rows", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we obtain sufficient conditions for the ``stability\" of our\nrecently proposed algorithms, modified-CS (for noisy measurements) and Least\nSquares CS-residual (LS-CS), designed for recursive reconstruction of sparse\nsignal sequences from noisy measurements. By ``stability\" we mean that the\nnumber of misses from the current support estimate and the number of extras in\nit remain bounded by a time-invariant value at all times. The concept is\nmeaningful only if the bound is small compared to the current signal support\nsize. A direct corollary is that the reconstruction errors are also bounded by\na time-invariant and small value.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2010 15:45:34 GMT"}], "update_date": "2010-06-25", "authors_parsed": [["Vaswani", "Namrata", ""]]}, {"id": "1006.4837", "submitter": "Krista Gile", "authors": "Krista J. Gile", "title": "Improved Inference for Respondent-Driven Sampling Data with Application\n  to HIV Prevalence Estimation", "comments": "36 pages, 12 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling is a form of link-tracing network sampling, which\nis widely used to study hard-to-reach populations, often to estimate population\nproportions. Previous treatments of this process have used a with-replacement\napproximation, which we show induces bias in estimates for large sample\nfractions and differential network connectedness by characteristic of interest.\n  We present a treatment of respondent-driven sampling as a successive sampling\nprocess. Unlike existing representations, our approach respects the essential\nwithout-replacement feature of the process, while converging to an existing\nwith-replacement representation for small sample fractions, and to the sample\nmean for a full-population sample.\n  We present a successive-sampling based estimator for population means based\non respondent-driven sampling data, and demonstrate its superior performance\nwhen the size of the hidden population is known. We present sensitivity\nanalyses for unknown population sizes. In addition, we note that like other\nexisting estimators, our new estimator is subject to bias induced by the\nselection of the initial sample. Using data collected among three populations\nin two countries, we illustrate the application of this approach to populations\nwith varying characteristics. We conclude that the successive sampling\nestimator improves on existing estimators, and can also be used as a diagnostic\ntool when population size is not known.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2010 17:10:22 GMT"}], "update_date": "2010-06-25", "authors_parsed": [["Gile", "Krista J.", ""]]}, {"id": "1006.5060", "submitter": "Xiaohui Xie", "authors": "Gui-Bo Ye and Xiaohui Xie", "title": "Learning sparse gradients for variable selection and dimension reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection and dimension reduction are two commonly adopted\napproaches for high-dimensional data analysis, but have traditionally been\ntreated separately. Here we propose an integrated approach, called sparse\ngradient learning (SGL), for variable selection and dimension reduction via\nlearning the gradients of the prediction function directly from samples. By\nimposing a sparsity constraint on the gradients, variable selection is achieved\nby selecting variables corresponding to non-zero partial derivatives, and\neffective dimensions are extracted based on the eigenvectors of the derived\nsparse empirical gradient covariance matrix. An error analysis is given for the\nconvergence of the estimated gradients to the true ones in both the Euclidean\nand the manifold setting. We also develop an efficient forward-backward\nsplitting algorithm to solve the SGL problem, making the framework practically\nscalable for medium or large datasets. The utility of SGL for variable\nselection and feature extraction is explicitly given and illustrated on\nartificial data as well as real-world examples. The main advantages of our\nmethod include variable selection for both linear and nonlinear predictions,\neffective dimension reduction with sparse loadings, and an efficient algorithm\nfor large p, small n problems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 20:27:00 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2010 05:06:43 GMT"}], "update_date": "2010-07-02", "authors_parsed": [["Ye", "Gui-Bo", ""], ["Xie", "Xiaohui", ""]]}, {"id": "1006.5831", "submitter": "Eric Laber", "authors": "Eric B. Laber and Min Qian and Dan J. Lizotte, William E. Pelham and\n  Susan A. Murphy", "title": "Statistical Inference in Dynamic Treatment Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic treatment regimes are of growing interest across the clinical\nsciences as these regimes provide one way to operationalize and thus inform\nsequential personalized clinical decision making. A dynamic treatment regime is\na sequence of decision rules, with a decision rule per stage of clinical\nintervention; each decision rule maps up-to-date patient information to a\nrecommended treatment. We briefly review a variety of approaches for using data\nto construct the decision rules. We then review an interesting challenge, that\nof nonregularity that often arises in this area. By nonregularity, we mean the\nparameters indexing the optimal dynamic treatment regime are nonsmooth\nfunctionals of the underlying generative distribution.\n  A consequence is that no regular or asymptotically unbiased estimator of\nthese parameters exists. Nonregularity arises in inference for parameters in\nthe optimal dynamic treatment regime; we illustrate the effect of nonregularity\non asymptotic bias and via sensitivity of asymptotic, limiting, distributions\nto local perturbations. We propose and evaluate a locally consistent Adaptive\nConfidence Interval (ACI) for the parameters of the optimal dynamic treatment\nregime. We use data from the Adaptive Interventions for Children with ADHD\nstudy as an illustrative example. We conclude by highlighting and discussing\nemerging theoretical problems in this area.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 11:10:09 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2011 21:00:14 GMT"}, {"version": "v3", "created": "Tue, 26 Nov 2013 16:54:55 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Laber", "Eric B.", ""], ["Qian", "Min", ""], ["Lizotte", "Dan J.", ""], ["Pelham", "William E.", ""], ["Murphy", "Susan A.", ""]]}]