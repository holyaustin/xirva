[{"id": "1607.00021", "submitter": "Jacob Bien", "authors": "Jacob Bien", "title": "The Simulator: An Engine to Streamline Simulations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulator is an R package that streamlines the process of performing\nsimulations by creating a common infrastructure that can be easily used and\nreused across projects. Methodological statisticians routinely write\nsimulations to compare their methods to preexisting ones. While developing\nideas, there is a temptation to write \"quick and dirty\" simulations to try out\nideas. This approach of rapid prototyping is useful but can sometimes backfire\nif bugs are introduced. Using the simulator allows one to remove the \"dirty\"\nwithout sacrificing the \"quick.\" Coding is quick because the statistician\nfocuses exclusively on those aspects of the simulation that are specific to the\nparticular paper being written. Code written with the simulator is succinct,\nhighly readable, and easily shared with others. The modular nature of\nsimulations written with the simulator promotes code reusability, which saves\ntime and facilitates reproducibility. The syntax of the simulator leads to\nsimulation code that is easily human-readable. Other benefits of using the\nsimulator include the ability to \"step in\" to a simulation and change one\naspect without having to rerun the entire simulation from scratch, the\nstraightforward integration of parallel computing into simulations, and the\nability to rapidly generate plots, tables, and reports with minimal effort.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 20:04:45 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Bien", "Jacob", ""]]}, {"id": "1607.00098", "submitter": "Longhai Li", "authors": "Lai Jiang, Longhai Li, Weixin Yao", "title": "Fully Bayesian Classification with Heavy-tailed Priors for Selection in\n  High-dimensional Features with Grouping Structure", "comments": "31 pages", "journal-ref": "Sci Rep 10, 9747 (2020)", "doi": "10.1038/s41598-020-66466-z", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is demanded in many modern scientific research problems\nthat use high-dimensional data. A typical example is to find the most useful\ngenes that are related to a certain disease (eg, cancer) from high-dimensional\ngene expressions. The expressions of genes have grouping structures, for\nexample, a group of co-regulated genes that have similar biological functions\ntend to have similar expressions. Many statistical methods have been proposed\nto take the grouping structure into consideration in feature selection,\nincluding group LASSO, supervised group LASSO, and regression on group\nrepresentatives. In this paper, we propose a fully Bayesian Robit regression\nmethod with heavy-tailed (sparsity) priors (shortened by FBRHT) for selecting\nfeatures with grouping structure. The main features of FBRHT include that it\ndiscards more aggressively unrelated features than LASSO, and it can make\nfeature selection within groups automatically without a pre-specified grouping\nstructure. In this paper, we use simulated and real datasets to demonstrate\nthat the predictive power of the sparse feature subsets selected by FBRHT are\ncomparable with other much larger feature subsets selected by LASSO, group\nLASSO, supervised group LASSO, penalized logistic regression and random forest,\nand that the succinct feature subsets selected by FBRHT have significantly\nbetter predictive power than the feature subsets of the same size taken from\nthe top features selected by the aforementioned methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 02:28:22 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 17:15:50 GMT"}, {"version": "v3", "created": "Fri, 12 May 2017 05:14:36 GMT"}], "update_date": "2020-06-18", "authors_parsed": [["Jiang", "Lai", ""], ["Li", "Longhai", ""], ["Yao", "Weixin", ""]]}, {"id": "1607.00393", "submitter": "David Kaplan", "authors": "David M. Kaplan (University of Missouri), Longhao Zhuo (Bank of\n  America)", "title": "Frequentist size of Bayesian inequality tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian and frequentist criteria are fundamentally different, but often\nposterior and sampling distributions are asymptotically equivalent (e.g.,\nGaussian). For the corresponding limit experiment, we characterize the\nfrequentist size of a certain Bayesian hypothesis test of (possibly nonlinear)\ninequalities. If the null hypothesis is that the (possibly\ninfinite-dimensional) parameter lies in a certain half-space, then the Bayesian\ntest's size is $\\alpha$; if the null hypothesis is a subset of a half-space,\nthen size is above $\\alpha$ (sometimes strictly); and in other cases, size may\nbe above, below, or equal to $\\alpha$. Two examples illustrate our results:\ntesting stochastic dominance and testing curvature of a translog cost function.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 20:02:05 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 20:40:33 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 16:44:26 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Kaplan", "David M.", "", "University of Missouri"], ["Zhuo", "Longhao", "", "Bank of\n  America"]]}, {"id": "1607.00515", "submitter": "Alnur Ali", "authors": "Alnur Ali, J. Zico Kolter, Ryan J. Tibshirani", "title": "The Multiple Quantile Graphical Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Multiple Quantile Graphical Model (MQGM), which extends the\nneighborhood selection approach of Meinshausen and Buhlmann for learning sparse\ngraphical models. The latter is defined by the basic subproblem of modeling the\nconditional mean of one variable as a sparse function of all others. Our\napproach models a set of conditional quantiles of one variable as a sparse\nfunction of all others, and hence offers a much richer, more expressive class\nof conditional distribution estimates. We establish that, under suitable\nregularity conditions, the MQGM identifies the exact conditional independencies\nwith probability tending to one as the problem size grows, even outside of the\nusual homoskedastic Gaussian data model. We develop an efficient algorithm for\nfitting the MQGM using the alternating direction method of multipliers. We also\ndescribe a strategy for sampling from the joint distribution that underlies the\nMQGM estimate. Lastly, we present detailed experiments that demonstrate the\nflexibility and effectiveness of the MQGM in modeling hetereoskedastic\nnon-Gaussian data.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 14:40:48 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2016 22:15:41 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Ali", "Alnur", ""], ["Kolter", "J. Zico", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1607.00698", "submitter": "Susan Athey", "authors": "Susan Athey and Guido Imbens", "title": "The Econometrics of Randomized Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this review, we present econometric and statistical methods for analyzing\nrandomized experiments. For basic experiments we stress randomization-based\ninference as opposed to sampling-based inference. In randomization-based\ninference, uncertainty in estimates arises naturally from the random assignment\nof the treatments, rather than from hypothesized sampling from a large\npopulation. We show how this perspective relates to regression analyses for\nrandomized experiments. We discuss the analyses of stratified, paired, and\nclustered randomized experiments, and we stress the general efficiency gains\nfrom stratification. We also discuss complications in randomized experiments\nsuch as non-compliance. In the presence of non-compliance we contrast\nintention-to-treat analyses with instrumental variables analyses allowing for\ngeneral treatment effect heterogeneity. We consider in detail estimation and\ninference for heterogeneous treatment effects in settings with (possibly many)\ncovariates. These methods allow researchers to explore heterogeneity by\nidentifying subpopulations with different treatment effects while maintaining\nthe ability to construct valid confidence intervals. We also discuss optimal\nassignment to treatment based on covariates in such settings. Finally, we\ndiscuss estimation and inference in experiments in settings with interactions\nbetween units, both in general network settings and in settings where the\npopulation is partitioned into groups with all interactions contained within\nthese groups.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 22:57:14 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Athey", "Susan", ""], ["Imbens", "Guido", ""]]}, {"id": "1607.00699", "submitter": "Susan Athey", "authors": "Susan Athey and Guido Imbens", "title": "The State of Applied Econometrics - Causality and Policy Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we discuss recent developments in econometrics that we view as\nimportant for empirical researchers working on policy evaluation questions. We\nfocus on three main areas, where in each case we highlight recommendations for\napplied work. First, we discuss new research on identification strategies in\nprogram evaluation, with particular focus on synthetic control methods,\nregression discontinuity, external validity, and the causal interpretation of\nregression methods. Second, we discuss various forms of supplementary analyses\nto make the identification strategies more credible. These include placebo\nanalyses as well as sensitivity and robustness analyses. Third, we discuss\nrecent advances in machine learning methods for causal effects. These advances\ninclude methods to adjust for differences between treated and control units in\nhigh-dimensional settings, and methods for identifying and estimating\nheterogeneous treatment effects.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 23:08:26 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Athey", "Susan", ""], ["Imbens", "Guido", ""]]}, {"id": "1607.00743", "submitter": "Miles Lopes", "authors": "Miles E. Lopes", "title": "A Residual Bootstrap for High-Dimensional Regression with Near Low-Rank\n  Designs", "comments": "The main text of this paper was published at NIPS 2014. Proofs are\n  included here in the appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the residual bootstrap (RB) method in the context of\nhigh-dimensional linear regression. Specifically, we analyze the distributional\napproximation of linear contrasts $c^{\\top} (\\hat{\\beta}_{\\rho}-\\beta)$, where\n$\\hat{\\beta}_{\\rho}$ is a ridge-regression estimator. When regression\ncoefficients are estimated via least squares, classical results show that RB\nconsistently approximates the laws of contrasts, provided that $p\\ll n$, where\nthe design matrix is of size $n\\times p$. Up to now, relatively little work has\nconsidered how additional structure in the linear model may extend the validity\nof RB to the setting where $p/n\\asymp 1$. In this setting, we propose a version\nof RB that resamples residuals obtained from ridge regression. Our main\nstructural assumption on the design matrix is that it is nearly low rank --- in\nthe sense that its singular values decay according to a power-law profile.\nUnder a few extra technical assumptions, we derive a simple criterion for\nensuring that RB consistently approximates the law of a given contrast. We then\nspecialize this result to study confidence intervals for mean response values\n$X_i^{\\top} \\beta$, where $X_i^{\\top}$ is the $i$th row of the design. More\nprecisely, we show that conditionally on a Gaussian design with near low-rank\nstructure, RB simultaneously approximates all of the laws\n$X_i^{\\top}(\\hat{\\beta}_{\\rho}-\\beta)$, $i=1,\\dots,n$. This result is also\nnotable as it imposes no sparsity assumptions on $\\beta$. Furthermore, since\nour consistency results are formulated in terms of the Mallows (Kantorovich)\nmetric, the existence of a limiting distribution is not required.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 05:50:19 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Lopes", "Miles E.", ""]]}, {"id": "1607.00882", "submitter": "Anna Gottard", "authors": "Roberto Colombi, Sabrina Giordano, Anna Gottard, Maria Iannario", "title": "Modelling Ordinal Responses with Uncertainty: a Hierarchical Marginal\n  Model with Latent Uncertainty components", "comments": null, "journal-ref": "Scandinavian Journal of Statistics, 2019, 46:595-620", "doi": "10.1111/sjos.12366", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In responding to rating questions, an individual may give answers either\naccording to his/her knowledge/awareness or to his/her level of\nindecision/uncertainty, typically driven by a response style. As ignoring this\ndual behaviour may lead to misleading results, we define a multivariate model\nfor ordinal rating responses, by introducing, for every item, a binary latent\nvariable that discriminates aware from uncertain responses. Some independence\nassumptions among latent and observable variables characterize the uncertain\nbehaviour and make the model easier to interpret. Uncertain responses are\nmodelled by specifying probability distributions that can depict different\nresponse styles characterizing the uncertain raters. A marginal parametrization\nallows a simple and direct interpretation of the parameters in terms of\nassociation among aware responses and their dependence on explanatory factors.\nThe effectiveness of the proposed model is attested through an application to\nreal data and supported by a Monte Carlo study.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 13:37:22 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 11:49:46 GMT"}, {"version": "v3", "created": "Tue, 6 Mar 2018 09:33:24 GMT"}, {"version": "v4", "created": "Wed, 16 May 2018 09:16:17 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Colombi", "Roberto", ""], ["Giordano", "Sabrina", ""], ["Gottard", "Anna", ""], ["Iannario", "Maria", ""]]}, {"id": "1607.01073", "submitter": "So Young Park", "authors": "So Young Park, Ana-Maria Staicu, Luo Xiao, Ciprian Crainiceanu", "title": "Simple fixed-effects inference for complex functional models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose simple inferential approaches for the fixed effects in complex\nfunctional mixed effects models. We estimate the fixed effects under the\nindependence of functional residuals assumption and then bootstrap independent\nunits (e.g. subjects) to estimate the variability of and conduct inference in\nthe form of hypothesis testing on the fixed effects parameters. Simulations\nshow excellent coverage probability of the confidence intervals and size of\ntests. Methods are motivated by and applied to the Baltimore Longitudinal Study\nof Aging (BLSA), though they are applicable to other studies that collect\ncorrelated functional data.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 23:27:04 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Park", "So Young", ""], ["Staicu", "Ana-Maria", ""], ["Xiao", "Luo", ""], ["Crainiceanu", "Ciprian", ""]]}, {"id": "1607.01145", "submitter": "Kei Hirose", "authors": "Kei Hirose and Yoshikazu Terada", "title": "Simple structure estimation via prenet penalization", "comments": "47 pages, 7 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a $prenet$ ($pr$oduct $e$lastic $net$), which is a new\npenalization method for factor analysis models. The penalty is based on the\nproduct of a pair of elements in each row of the loading matrix. The prenet not\nonly shrinks some of the factor loadings toward exactly zero, but also enhances\nthe simplicity of the loading matrix, which plays an important role in the\ninterpretation of the common factors. In particular, with a large amount of\nprenet penalization, the estimated loading matrix possesses a perfect simple\nstructure, which is known as a desirable structure in terms of the simplicity\nof the loading matrix. Furthermore, the perfect simple structure estimation via\nthe prenet turns out to be a generalization of the $k$-means clustering of\nvariables. On the other hand, a mild amount of the penalization approximates a\nloading matrix estimated by the quartimin rotation, one of the most commonly\nused oblique rotation techniques. Thus, the proposed penalty bridges a gap\nbetween the perfect simple structure and the quartimin rotation. Monte Carlo\nsimulation is conducted to investigate the performance of the proposed\nprocedure. Three real data analyses are given to illustrate the usefulness of\nour penalty.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 08:42:24 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 13:47:09 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 03:31:43 GMT"}], "update_date": "2018-04-19", "authors_parsed": [["Hirose", "Kei", ""], ["Terada", "Yoshikazu", ""]]}, {"id": "1607.01179", "submitter": "Carlos Matran", "authors": "E. del Barrio, J.A. Cuesta-Albertos, C. Matr\\'an and A. Mayo-\\'Iscar", "title": "Robust clustering tools based on optimal transportation", "comments": null, "journal-ref": "Statistics and Computing, (2019), 29, 139-160 The final\n  publication is available at link.springer.com", "doi": "10.1007/s11222-018-9800-z", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A robust clustering method for probabilities in Wasserstein space is\nintroduced. This new \"trimmed $k$-barycenters\" approach relies on recent\nresults on barycenters in Wasserstein space that allow intensive computation,\nas required by clustering algorithms. The possibility of trimming the most\ndiscrepant distributions results in a gain in stability and robustness, highly\nconvenient in this setting. As a remarkable application we consider a\nparallelized estimation setup in which each of $m$ units processes a portion of\nthe data, producing an estimate of $k$-features, encoded as $k$ probabilities.\nWe prove that the trimmed $k$-barycenter of the $m\\times k$ estimates produces\na consistent aggregation. We illustrate the methodology with simulated and real\ndata examples. These include clustering populations by age distributions and\nanalysis of cytometric data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 10:23:29 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 07:39:22 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["del Barrio", "E.", ""], ["Cuesta-Albertos", "J. A.", ""], ["Matr\u00e1n", "C.", ""], ["Mayo-\u00cdscar", "A.", ""]]}, {"id": "1607.01192", "submitter": "Michael Muma", "authors": "Michael Muma and Abdelhak M. Zoubir", "title": "Bounded Influence Propagation {\\tau}-Estimation: A New Robust Method for\n  ARMA Model Estimation", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2016.2634539", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new robust and statistically efficient estimator for ARMA models called the\nbounded influence propagation (BIP) {\\tau}-estimator is proposed. The estimator\nincorporates an auxiliary model, which prevents the propagation of outliers.\nStrong consistency and asymptotic normality of the estimator for ARMA models\nthat are driven by independently and identically distributed (iid) innovations\nwith symmetric distributions are established. To analyze the infinitesimal\neffect of outliers on the estimator, the influence function is derived and\ncomputed explicitly for an AR(1) model with additive outliers. To obtain\nestimates for the AR(p) model, a robust Durbin-Levinson type and a\nforward-backward algorithm are proposed. An iterative algorithm to robustly\nobtain ARMA(p,q) parameter estimates is also presented. The problem of finding\na robust initialization is addressed, which for orders p+q>2 is a non-trivial\nmatter. Numerical experiments are conducted to compare the finite sample\nperformance of the proposed estimator to existing robust methodologies for\ndifferent types of outliers both in terms of average and of worst-case\nperformance, as measured by the maximum bias curve. To illustrate the practical\napplicability of the proposed estimator, a real-data example of outlier\ncleaning for R-R interval plots derived from electrocardiographic (ECG) data is\nconsidered. The proposed estimator is not limited to biomedical applications,\nbut is also useful in any real-world problem whose observations can be modeled\nas an ARMA process disturbed by outliers or impulsive noise.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 11:07:02 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 09:44:53 GMT"}, {"version": "v3", "created": "Thu, 20 Oct 2016 08:57:00 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Muma", "Michael", ""], ["Zoubir", "Abdelhak M.", ""]]}, {"id": "1607.01367", "submitter": "Sacha Epskamp", "authors": "Sacha Epskamp and Eiko I. Fried", "title": "A Tutorial on Regularized Partial Correlation Networks", "comments": "In press in Psychological Methods (DOI: 10.1037/met0000167)", "journal-ref": null, "doi": "10.1037/met0000167", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen an emergence of network modeling applied to moods,\nattitudes, and problems in the realm of psychology. In this framework,\npsychological variables are understood to directly affect each other rather\nthan being caused by an unobserved latent entity. In this tutorial, we\nintroduce the reader to estimating the most popular network model for\npsychological data: the partial correlation network. We describe how\nregularization techniques can be used to efficiently estimate a parsimonious\nand interpretable network structure in psychological data. We show how to\nperform these analyses in R and demonstrate the method in an empirical example\non post-traumatic stress disorder data. In addition, we discuss the effect of\nthe hyperparameter that needs to be manually set by the researcher, how to\nhandle non-normal data, how to determine the required sample size for a network\nanalysis, and provide a checklist with potential solutions for problems that\ncan arise when estimating regularized partial correlation networks.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 18:55:19 GMT"}, {"version": "v2", "created": "Tue, 13 Sep 2016 20:01:32 GMT"}, {"version": "v3", "created": "Sun, 25 Sep 2016 10:24:17 GMT"}, {"version": "v4", "created": "Mon, 3 Oct 2016 15:34:25 GMT"}, {"version": "v5", "created": "Sun, 30 Apr 2017 18:29:41 GMT"}, {"version": "v6", "created": "Wed, 28 Jun 2017 16:15:15 GMT"}, {"version": "v7", "created": "Mon, 11 Sep 2017 20:46:08 GMT"}, {"version": "v8", "created": "Thu, 14 Sep 2017 11:09:41 GMT"}, {"version": "v9", "created": "Fri, 1 Dec 2017 13:35:57 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Epskamp", "Sacha", ""], ["Fried", "Eiko I.", ""]]}, {"id": "1607.01631", "submitter": "Mike West", "authors": "Kaoru Irie and Mike West", "title": "Bayesian emulation for optimization in multi-step portfolio decisions", "comments": "24 pages, 7 figures, 2 tables", "journal-ref": null, "doi": "10.1214/18-BA1105", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the Bayesian emulation approach to computational solution of\nmulti-step portfolio studies in financial time series. \"Bayesian emulation for\ndecisions\" involves mapping the technical structure of a decision analysis\nproblem to that of Bayesian inference in a purely synthetic \"emulating\"\nstatistical model. This provides access to standard posterior analytic,\nsimulation and optimization methods that yield indirect solutions of the\ndecision problem. We develop this in time series portfolio analysis using\nclasses of economically and psychologically relevant multi-step ahead portfolio\nutility functions. Studies with multivariate currency, commodity and stock\nindex time series illustrate the approach and show some of the practical\nutility and benefits of the Bayesian emulation methodology.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 14:21:41 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Irie", "Kaoru", ""], ["West", "Mike", ""]]}, {"id": "1607.01833", "submitter": "Lek-Heng Lim", "authors": "Lek-Heng Lim, Ken Sze-Wai Wong, Ke Ye", "title": "Numerical algorithms on the affine Grassmannian", "comments": "18 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The affine Grassmannian is a noncompact smooth manifold that parameterizes\nall affine subspaces of a fixed dimension. It is a natural generalization of\nEuclidean space, points being zero-dimensional affine subspaces. We will\nrealize the affine Grassmannian as a matrix manifold and extend Riemannian\noptimization algorithms including steepest descent, Newton method, and\nconjugate gradient, to real-valued functions on the affine Grassmannian. Like\ntheir counterparts for the Grassmannian, these algorithms are in the style of\nEdelman--Arias--Smith --- they rely only on standard numerical linear algebra\nand are readily computable.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 22:33:43 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 08:40:51 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2018 16:41:24 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Lim", "Lek-Heng", ""], ["Wong", "Ken Sze-Wai", ""], ["Ye", "Ke", ""]]}, {"id": "1607.01853", "submitter": "Fang Han", "authors": "Fang Han, Sheng Xu, and Wen-Xin Zhou", "title": "On Gaussian Comparison Inequality and Its Application to Spectral\n  Analysis of Large Random Matrices", "comments": "to appear in Bernoulli", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Chernozhukov, Chetverikov, and Kato [Ann. Statist. 42 (2014)\n1564--1597] developed a new Gaussian comparison inequality for approximating\nthe suprema of empirical processes. This paper exploits this technique to\ndevise sharp inference on spectra of large random matrices. In particular, we\nshow that two long-standing problems in random matrix theory can be solved: (i)\nsimple bootstrap inference on sample eigenvalues when true eigenvalues are\ntied; (ii) conducting two-sample Roy's covariance test in high dimensions. To\nestablish the asymptotic results, a generalized $\\epsilon$-net argument\nregarding the matrix rescaled spectral norm and several new empirical process\nbounds are developed and of independent interest.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 02:09:30 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 18:54:26 GMT"}, {"version": "v3", "created": "Fri, 26 May 2017 18:39:43 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Han", "Fang", ""], ["Xu", "Sheng", ""], ["Zhou", "Wen-Xin", ""]]}, {"id": "1607.01881", "submitter": "Alessio Spantini", "authors": "Alessio Spantini, Tiangang Cui, Karen Willcox, Luis Tenorio, Youssef\n  Marzouk", "title": "Goal-oriented optimal approximations of Bayesian linear inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose optimal dimensionality reduction techniques for the solution of\ngoal-oriented linear-Gaussian inverse problems, where the quantity of interest\n(QoI) is a function of the inversion parameters. These approximations are\nsuitable for large-scale applications. In particular, we study the\napproximation of the posterior covariance of the QoI as a low-rank negative\nupdate of its prior covariance, and prove optimality of this update with\nrespect to the natural geodesic distance on the manifold of symmetric positive\ndefinite matrices. Assuming exact knowledge of the posterior mean of the QoI,\nthe optimality results extend to optimality in distribution with respect to the\nKullback-Leibler divergence and the Hellinger distance between the associated\ndistributions. We also propose approximation of the posterior mean of the QoI\nas a low-rank linear function of the data, and prove optimality of this\napproximation with respect to a weighted Bayes risk. Both of these optimal\napproximations avoid the explicit computation of the full posterior\ndistribution of the parameters and instead focus on directions that are well\ninformed by the data and relevant to the QoI. These directions stem from a\nbalance among all the components of the goal-oriented inverse problem: prior\ninformation, forward model, measurement noise, and ultimate goals. We\nillustrate the theory using a high-dimensional inverse problem in heat\ntransfer.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 06:36:31 GMT"}, {"version": "v2", "created": "Tue, 14 Mar 2017 23:12:42 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Spantini", "Alessio", ""], ["Cui", "Tiangang", ""], ["Willcox", "Karen", ""], ["Tenorio", "Luis", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1607.02017", "submitter": "Gilles Nisol", "authors": "Siegfried H\\\"ormann and Piotr Kokoszka and Gilles Nisol", "title": "Detection of periodicity in functional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive several tests for the presence of a periodic component in a time\nseries of functions. We consider both the traditional setting in which the\nperiodic functional signal is contaminated by functional white noise, and a\nmore general setting of a contaminating process which is weakly dependent.\nSeveral forms of the periodic component are considered. Our tests are motivated\nby the likelihood principle and fall into two broad categories, which we term\nmultivariate and fully functional. Overall, for the functional series that\nmotivate this research, the fully functional tests exhibit a superior balance\nof size and power. Asymptotic null distributions of all tests are derived and\ntheir consistency is established. Their finite sample performance is examined\nand compared by numerical studies and application to pollution data.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 13:58:31 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["H\u00f6rmann", "Siegfried", ""], ["Kokoszka", "Piotr", ""], ["Nisol", "Gilles", ""]]}, {"id": "1607.02325", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli and Nial Friel", "title": "Optimal Bayesian estimators for latent variable cluster models", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cluster analysis interest lies in probabilistically capturing partitions\nof individuals, items or observations into groups, such that those belonging to\nthe same group share similar attributes or relational profiles. Bayesian\nposterior samples for the latent allocation variables can be effectively\nobtained in a wide range of clustering models, including finite mixtures,\ninfinite mixtures, hidden Markov models and block models for networks. However,\ndue to the categorical nature of the clustering variables and the lack of\nscalable algorithms, summary tools that can interpret such samples are not\navailable. We adopt a Bayesian decision theoretic approach to define an\noptimality criterion for clusterings, and propose a fast and\ncontext-independent greedy algorithm to find the best allocations. One\nimportant facet of our approach is that the optimal number of groups is\nautomatically selected, thereby solving the clustering and the model-choice\nproblems at the same time. We consider several loss functions to compare\npartitions, and show that our approach can accommodate a wide range of cases.\nFinally, we illustrate our approach on a variety of real-data applications for\nthree different clustering models: Gaussian finite mixtures, stochastic block\nmodels and latent block models for networks.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 11:42:37 GMT"}, {"version": "v2", "created": "Wed, 22 Mar 2017 17:13:27 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Rastelli", "Riccardo", ""], ["Friel", "Nial", ""]]}, {"id": "1607.02328", "submitter": "Age Smilde", "authors": "Age K. Smilde, Ingrid Mage, Tormod Naes, Thomas Hankemeier, Mirjam A.\n  Lips, Henk A.L. Kiers, Evrim Acar and Rasmus Bro", "title": "Common and Distinct Components in Data Fusion", "comments": "50 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many areas of science multiple sets of data are collected pertaining to\nthe same system. Examples are food products which are characterized by\ndifferent sets of variables, bio-processes which are on-line sampled with\ndifferent instruments, or biological systems of which different genomics\nmeasurements are obtained. Data fusion is concerned with analyzing such sets of\ndata simultaneously to arrive at a global view of the system under study. One\nof the upcoming areas of data fusion is exploring whether the data sets have\nsomething in common or not. This gives insight into common and distinct\nvariation in each data set, thereby facilitating understanding the\nrelationships between the data sets. Unfortunately, research on methods to\ndistinguish common and distinct components is fragmented, both in terminology\nas well as in methods: there is no common ground which hampers comparing\nmethods and understanding their relative merits. This paper provides a unifying\nframework for this subfield of data fusion by using rigorous arguments from\nlinear algebra. The most frequently used methods for distinguishing common and\ndistinct components are explained in this framework and some practical examples\nare given of these methods in the areas of (medical) biology and food science.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 11:56:21 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Smilde", "Age K.", ""], ["Mage", "Ingrid", ""], ["Naes", "Tormod", ""], ["Hankemeier", "Thomas", ""], ["Lips", "Mirjam A.", ""], ["Kiers", "Henk A. L.", ""], ["Acar", "Evrim", ""], ["Bro", "Rasmus", ""]]}, {"id": "1607.02516", "submitter": "Johan Westerborn Alenl\\\"ov", "authors": "Johan Alenl\\\"ov and Arnaud Doucet and Fredrik Lindsten", "title": "Pseudo-Marginal Hamiltonian Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference in the presence of an intractable likelihood function is\ncomputationally challenging. When following a Markov chain Monte Carlo (MCMC)\napproach to approximate the posterior distribution in this context, one\ntypically either uses MCMC schemes which target the joint posterior of the\nparameters and some auxiliary latent variables, or pseudo-marginal\nMetropolis--Hastings (MH) schemes. The latter mimic a MH algorithm targeting\nthe marginal posterior of the parameters by approximating unbiasedly the\nintractable likelihood. However, in scenarios where the parameters and\nauxiliary variables are strongly correlated under the posterior and/or this\nposterior is multimodal, Gibbs sampling or Hamiltonian Monte Carlo (HMC) will\nperform poorly and the pseudo-marginal MH algorithm, as any other MH scheme,\nwill be inefficient for high dimensional parameters. We propose here an\noriginal MCMC algorithm, termed pseudo-marginal HMC, which combines the\nadvantages of both HMC and pseudo-marginal schemes. Specifically, the\npseudo-marginal HMC method is controlled by a precision parameter N,\ncontrolling the approximation of the likelihood and, for any N, it samples the\nmarginal posterior of the parameters. Additionally, as N tends to infinity, its\nsample trajectories and acceptance probability converge to those of an ideal,\nbut intractable, HMC algorithm which would have access to the marginal\nposterior of parameters and its gradient. We demonstrate through experiments\nthat pseudo-marginal HMC can outperform significantly both standard HMC and\npseudo-marginal MH schemes.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 20:06:43 GMT"}, {"version": "v2", "created": "Wed, 2 Oct 2019 08:50:23 GMT"}], "update_date": "2019-10-03", "authors_parsed": [["Alenl\u00f6v", "Johan", ""], ["Doucet", "Arnaud", ""], ["Lindsten", "Fredrik", ""]]}, {"id": "1607.02566", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy, Scott A. Lorch, Dylan S. Small", "title": "Robust causal inference with continuous instruments using the local\n  instrumental variable curve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instrumental variables are commonly used to estimate effects of a treatment\nafflicted by unmeasured confounding, and in practice instruments are often\ncontinuous (e.g., measures of distance, or treatment preference). However,\navailable methods for continuous instruments have important limitations: they\neither require restrictive parametric assumptions for identification, or else\nrely on modeling both the outcome and treatment process well (and require\nmodeling effect modification by all adjustment covariates). In this work we\ndevelop the first semiparametric doubly robust estimators of the local\ninstrumental variable effect curve, i.e., the effect among those who would take\ntreatment for instrument values above some threshold and not below. In addition\nto being robust to misspecification of either the instrument or\ntreatment/outcome processes, our approach also incorporates information about\nthe instrument mechanism and allows for flexible data-adaptive estimation of\neffect modification. We discuss asymptotic properties under weak conditions,\nand use the methods to study infant mortality effects of neonatal intensive\ncare units with high versus low technical capacity, using travel time as an\ninstrument.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 04:04:07 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 01:08:51 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 15:26:20 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Kennedy", "Edward H.", ""], ["Lorch", "Scott A.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1607.02631", "submitter": "Linbo Wang", "authors": "Eric J. Tchetgen Tchetgen, Linbo Wang, BaoLuo Sun", "title": "Discrete Choice Models for Nonmonotone Nonignorable Missing Data:\n  Identification and Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonmonotone missing data arise routinely in empirical studies of social and\nhealth sciences, and when ignored, can induce selection bias and loss of\nefficiency. In practice, it is common to account for nonresponse under a\nmissing-at-random assumption which although convenient, is rarely appropriate\nwhen nonresponse is nonmonotone. Likelihood and Bayesian missing data\nmethodologies often require specification of a parametric model for the full\ndata law, thus a priori ruling out any prospect for semiparametric inference.\nIn this paper, we propose an all-purpose approach which delivers semiparametric\ninferences when missing data are nonmonotone and not at random. The approach is\nbased on a discrete choice model (DCM) as a means to generate a large class of\nnonmonotone nonresponse mechanisms that are nonignorable. Sufficient conditions\nfor nonparametric identification are given, and a general framework for fully\nparametric and semiparametric inference under an arbitrary DCM is proposed.\nSpecial consideration is given to the case of logit discrete choice nonresponse\nmodel (LDCM) for which we describe generalizations of inverse-probability\nweighting, pattern-mixture estimation, doubly robust estimation and multiply\nrobust estimation.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 16:27:50 GMT"}, {"version": "v2", "created": "Sun, 1 Jan 2017 21:23:49 GMT"}, {"version": "v3", "created": "Wed, 19 Jul 2017 13:14:25 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Tchetgen", "Eric J. Tchetgen", ""], ["Wang", "Linbo", ""], ["Sun", "BaoLuo", ""]]}, {"id": "1607.02633", "submitter": "Umberto Picchini", "authors": "Umberto Picchini and Julie Lyng Forman", "title": "Bayesian inference for stochastic differential equation mixed effects\n  models of a tumor xenography study", "comments": "Minor revision: posterior predictive checks for BSL have ben updated\n  (both theory and results). Code on GitHub has ben revised accordingly", "journal-ref": "Journal of the Royal Statistical Society: Series C (Applied\n  Statistics), 2019", "doi": "10.1111/rssc.12347", "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider Bayesian inference for stochastic differential equation mixed\neffects models (SDEMEMs) exemplifying tumor response to treatment and regrowth\nin mice. We produce an extensive study on how a SDEMEM can be fitted using both\nexact inference based on pseudo-marginal MCMC and approximate inference via\nBayesian synthetic likelihoods (BSL). We investigate a two-compartments SDEMEM,\nthese corresponding to the fractions of tumor cells killed by and survived to a\ntreatment, respectively. Case study data considers a tumor xenography study\nwith two treatment groups and one control, each containing 5-8 mice. Results\nfrom the case study and from simulations indicate that the SDEMEM is able to\nreproduce the observed growth patterns and that BSL is a robust tool for\ninference in SDEMEMs. Finally, we compare the fit of the SDEMEM to a similar\nordinary differential equation model. Due to small sample sizes, strong prior\ninformation is needed to identify all model parameters in the SDEMEM and it\ncannot be determined which of the two models is the better in terms of\npredicting tumor growth curves. In a simulation study we find that with a\nsample of 17 mice per group BSL is able to identify all model parameters and\ndistinguish treatment groups.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 16:30:33 GMT"}, {"version": "v2", "created": "Sat, 10 Dec 2016 16:03:46 GMT"}, {"version": "v3", "created": "Tue, 10 Oct 2017 17:16:32 GMT"}, {"version": "v4", "created": "Mon, 1 Oct 2018 17:19:19 GMT"}, {"version": "v5", "created": "Sun, 17 Feb 2019 10:51:35 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Picchini", "Umberto", ""], ["Forman", "Julie Lyng", ""]]}, {"id": "1607.02649", "submitter": "Ping Li", "authors": "Martin Slawski and Ping Li", "title": "Linear signal recovery from $b$-bit-quantized linear measurements:\n  precise analysis of the trade-off between bit depth and number of\n  measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a high-dimensional structured signal\nfrom independent Gaussian linear measurements each of which is quantized to $b$\nbits. Our interest is in linear approaches to signal recovery, where \"linear\"\nmeans that non-linearity resulting from quantization is ignored and the\nobservations are treated as if they arose from a linear measurement model.\nSpecifically, the focus is on a generalization of a method for one-bit\nobservations due to Plan and Vershynin [\\emph{IEEE~Trans. Inform. Theory,\n\\textbf{59} (2013), 482--494}]. At the heart of the present paper is a precise\ncharacterization of the optimal trade-off between the number of measurements\n$m$ and the bit depth per measurement $b$ given a total budget of $B = m \\cdot\nb$ bits when the goal is to minimize the $\\ell_2$-error in estimating the\nsignal. It turns out that the choice $b = 1$ is optimal for estimating the unit\nvector (direction) corresponding to the signal for any level of additive\nGaussian noise before quantization as well as for a specific model of\nadversarial noise, while the choice $b = 2$ is optimal for estimating the\ndirection and the norm (scale) of the signal. Moreover, Lloyd-Max quantization\nis shown to be an optimal quantization scheme w.r.t. $\\ell_2$-estimation error.\nOur analysis is corroborated by numerical experiments showing nearly perfect\nagreement with our theoretical predictions. The paper is complemented by an\nempirical comparison to alternative methods of signal recovery taking the\nnon-linearity resulting from quantization into account. The results of that\ncomparison point to a regime change depending on the noise level: in a\nlow-noise setting, linear signal recovery falls short of more sophisticated\ncompetitors while being competitive in moderate- and high-noise settings.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 19:49:14 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Slawski", "Martin", ""], ["Li", "Ping", ""]]}, {"id": "1607.02655", "submitter": "Mike West", "authors": "Xi Chen, Kaoru Irie, David Banks, Robert Haslinger, Jewell Thomas and\n  Mike West", "title": "Scalable Bayesian modeling, monitoring and analysis of dynamic network\n  flow data", "comments": "29 pages, 16 figures", "journal-ref": null, "doi": "10.1080/01621459.2017.1345742", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic flow count data in networks arise in many applications, such as\nautomobile or aviation transportation, certain directed social network\ncontexts, and Internet studies. Using an example of Internet browser traffic\nflow through site-segments of an international news website, we present\nBayesian analyses of two linked classes of models which, in tandem, allow fast,\nscalable and interpretable Bayesian inference. We first develop flexible\nstate-space models for streaming count data, able to adaptively characterize\nand quantify network dynamics efficiently in real-time. We then use these\nmodels as emulators of more structured, time-varying gravity models that allow\nformal dissection of network dynamics. This yields interpretable inferences on\ntraffic flow characteristics, and on dynamics in interactions among network\nnodes. Bayesian monitoring theory defines a strategy for sequential model\nassessment and adaptation in cases when network flow data deviates from\nmodel-based predictions. Exploratory and sequential monitoring analyses of\nevolving traffic on a network of web site-segments in e-commerce demonstrate\nthe utility of this coupled Bayesian emulation approach to analysis of\nstreaming network count data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 20:11:20 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Chen", "Xi", ""], ["Irie", "Kaoru", ""], ["Banks", "David", ""], ["Haslinger", "Robert", ""], ["Thomas", "Jewell", ""], ["West", "Mike", ""]]}, {"id": "1607.02675", "submitter": "Bowei Yan", "authors": "Bowei Yan and Purnamrita Sarkar", "title": "Covariate Regularized Community Detection in Sparse Graphs", "comments": "27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate community detection in networks in the presence\nof node covariates. In many instances, covariates and networks individually\nonly give a partial view of the cluster structure. One needs to jointly infer\nthe full cluster structure by considering both. In statistics, an emerging body\nof work has been focused on combining information from both the edges in the\nnetwork and the node covariates to infer community memberships. However, so far\nthe theoretical guarantees have been established in the dense regime, where the\nnetwork can lead to perfect clustering under a broad parameter regime, and\nhence the role of covariates is often not clear. In this paper, we examine\nsparse networks in conjunction with finite dimensional sub-gaussian mixtures as\ncovariates under moderate separation conditions. In this setting each\nindividual source can only cluster a non-vanishing fraction of nodes correctly.\nWe propose a simple optimization framework which provably improves clustering\naccuracy when the two sources carry partial information about the cluster\nmemberships, and hence perform poorly on their own. Our optimization problem\ncan be solved using scalable convex optimization algorithms. Using a variety of\nsimulated and real data examples, we show that the proposed method outperforms\nother existing methodology.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 01:07:16 GMT"}, {"version": "v2", "created": "Sat, 22 Oct 2016 16:59:02 GMT"}, {"version": "v3", "created": "Sat, 3 Dec 2016 06:51:51 GMT"}, {"version": "v4", "created": "Wed, 25 Apr 2018 14:14:03 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Yan", "Bowei", ""], ["Sarkar", "Purnamrita", ""]]}, {"id": "1607.02745", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo", "title": "How to use the functional empirical process for deriving asymptotic laws\n  for functions of the sample", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The functional empirical process is a very powerful tool for deriving\nasymptotic laws for almost any kind of statistics whenever we know how to\nexpress them into functions of the sample. Since this method seems to be\napplied more and more in the very recent future, this paper is intended to\nprovide a complete but short description and justification of the method and to\nillustrate it with a non-trivial example using bivariate data. It may also\nserve for citation whithout repeating the arguments.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 13:19:27 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 12:42:20 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Lo", "Gane Samb", ""]]}, {"id": "1607.02788", "submitter": "Youssef Marzouk", "authors": "Patrick Conrad, Andrew Davis, Youssef Marzouk, Natesh Pillai, Aaron\n  Smith", "title": "Parallel local approximation MCMC for expensive models", "comments": "34 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing Bayesian inference via Markov chain Monte Carlo (MCMC) can be\nexceedingly expensive when posterior evaluations invoke the evaluation of a\ncomputationally expensive model, such as a system of partial differential\nequations. In recent work [Conrad et al. JASA 2016, arXiv:1402.1694], we\ndescribed a framework for constructing and refining local approximations of\nsuch models during an MCMC simulation. These posterior--adapted approximations\nharness regularity of the model to reduce the computational cost of inference\nwhile preserving asymptotic exactness of the Markov chain. Here we describe two\nextensions of that work. First, we prove that samplers running in parallel can\ncollaboratively construct a shared posterior approximation while ensuring\nergodicity of each associated chain, providing a novel opportunity for\nexploiting parallel computation in MCMC. Second, focusing on the\nMetropolis--adjusted Langevin algorithm, we describe how a proposal\ndistribution can successfully employ gradients and other relevant information\nextracted from the approximation. We investigate the practical performance of\nour strategies using two challenging inference problems, the first in\nsubsurface hydrology and the second in glaciology. Using local approximations\nconstructed via parallel chains, we successfully reduce the run time needed to\ncharacterize the posterior distributions in these problems from days to hours\nand from months to days, respectively, dramatically improving the tractability\nof Bayesian inference.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 21:46:43 GMT"}, {"version": "v2", "created": "Mon, 25 Dec 2017 21:50:54 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Conrad", "Patrick", ""], ["Davis", "Andrew", ""], ["Marzouk", "Youssef", ""], ["Pillai", "Natesh", ""], ["Smith", "Aaron", ""]]}, {"id": "1607.02804", "submitter": "Chao Deng", "authors": "Chao Deng, Timothy Daley, Peter Calabrese, Jie Ren, Andrew D. Smith", "title": "Estimating the number of species to attain sufficient representation in\n  a random sample", "comments": "19 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical problem of using an initial sample to estimate the number of\nspecies in a larger sample has found important applications in fields far\nremoved from ecology. Here we address the general problem of estimating the\nnumber of species that will be represented by at least a number r of\nobservations in a future sample. The number r indicates species with sufficient\nobservations, which are commonly used as a necessary condition for any robust\nstatistical inference. We derive a procedure to construct consistent estimators\nthat apply universally for a given population: once constructed, they can be\nevaluated as a simple function of r. Our approach is based on a relation\nbetween the number of species represented at least r times and the higher\nderivatives of the expected number of species discovered per unit of time.\nCombining this relation with a rational function approximation, we propose\nnonparametric estimators that are accurate for both large values of r and\nlong-range extrapolations. We further show that our estimators retain\nasymptotic behaviors that are essential for applications on large-scale\ndatasets. We evaluate the performance of this approach by both simulation and\nreal data applications for inferences of the vocabulary of Shakespeare and\nDickens, the topology of a Twitter social network, and molecular diversity in\nDNA sequencing data.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 01:43:15 GMT"}, {"version": "v2", "created": "Fri, 19 Aug 2016 20:11:23 GMT"}, {"version": "v3", "created": "Tue, 15 May 2018 09:02:43 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Deng", "Chao", ""], ["Daley", "Timothy", ""], ["Calabrese", "Peter", ""], ["Ren", "Jie", ""], ["Smith", "Andrew D.", ""]]}, {"id": "1607.02883", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh and Magne Thoresen", "title": "Non-Concave Penalization in Linear Mixed-Effects Models and Regularized\n  Selection of Fixed Effects", "comments": "25 pages, under review", "journal-ref": "AStA Advances in Statistical Analysis (2018), Volume 102, Issue 2,\n  pp 179--210", "doi": "10.1007/s10182-017-0298-z", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-effect models are very popular for analyzing data with a hierarchical\nstructure, e.g. repeated observations within subjects in a longitudinal design,\npatients nested within centers in a multicenter design. However, recently, due\nto the medical advances, the number of fixed effect covariates collected from\neach patient can be quite large, e.g. data on gene expressions of each patient,\nand all of these variables are not necessarily important for the outcome. So,\nit is very important to choose the relevant covariates correctly for obtaining\nthe optimal inference for the overall study. On the other hand, the relevant\nrandom effects will often be low-dimensional and pre-specified. In this paper,\nwe consider regularized selection of important fixed effect variables in linear\nmixed-effects models along with maximum penalized likelihood estimation of both\nfixed and random effect parameters based on general non-concave penalties.\nAsymptotic and variable selection consistency with oracle properties are proved\nfor low-dimensional cases as well as for high-dimensionality of non-polynomial\norder of sample size (number of parameters is much larger than sample size). We\nalso provide a suitable computationally efficient algorithm for implementation.\nAdditionally, all the theoretical results are proved for a general non-convex\noptimization problem that applies to several important situations well beyond\nthe mixed model set-up (like finite mixture of regressions etc.) illustrating\nthe huge range of applicability of our proposal.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 10:07:37 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Ghosh", "Abhik", ""], ["Thoresen", "Magne", ""]]}, {"id": "1607.02993", "submitter": "Gonzalo  Garc\\'ia-Donato", "authors": "James O. Berger, Gonzalo Garcia-Donato, Miguel A. Martinez-Beneito and\n  Victor Pe\\~na", "title": "Bayesian variable selection in high dimensional problems without\n  assumptions on prior model probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of variable selection in linear models when $p$, the\nnumber of potential regressors, may exceed (and perhaps substantially) the\nsample size $n$ (which is possibly small).\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 15:11:27 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Berger", "James O.", ""], ["Garcia-Donato", "Gonzalo", ""], ["Martinez-Beneito", "Miguel A.", ""], ["Pe\u00f1a", "Victor", ""]]}, {"id": "1607.03045", "submitter": "Alexander Franks", "authors": "Alexander Franks and Peter Hoff", "title": "Shared Subspace Models for Multi-Group Covariance Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a model-based method for evaluating heterogeneity among several p\nx p covariance matrices in the large p, small n setting. This is done by\nassuming a spiked covariance model for each group and sharing information about\nthe space spanned by the group-level eigenvectors. We use an empirical Bayes\nmethod to identify a low-dimensional subspace which explains variation across\nall groups and use an MCMC algorithm to estimate the posterior uncertainty of\neigenvectors and eigenvalues on this subspace. The implementation and utility\nof our model is illustrated with analyses of high-dimensional multivariate gene\nexpression.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 17:21:13 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 21:34:55 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 21:54:39 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2019 17:59:37 GMT"}], "update_date": "2019-10-22", "authors_parsed": [["Franks", "Alexander", ""], ["Hoff", "Peter", ""]]}, {"id": "1607.03080", "submitter": "Deukwoo Kwon", "authors": "Deukwoo Kwon and Isildinha M. Reis", "title": "Approximate Bayesian computation (ABC) coupled with Bayesian model\n  averaging method for estimating mean and standard deviation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: We proposed approximate Bayesian computation with single\ndistribution selection (ABC-SD) for estimating mean and standard deviation from\nother reported summary statistics. The ABC-SD generates pseudo data from a\nsingle parametric distribution thought to be the true distribution of\nunderlying study data. This single distribution is either an educated guess, or\nit is selected via model selection using posterior probability criterion for\ntesting two or more candidate distributions. Further analysis indicated that\nwhen model selection is used, posterior model probabilities are sensitive to\nthe prior distribution(s) for parameter(s) and dependable on the type of\nreported summary statistics. Method: We propose ABC with Bayesian model\naveraging (ABC-BMA) methodology to estimate mean and standard deviation based\non various sets of other summary statistics reported in published studies. We\nconduct a Monte Carlo simulation study to compare the new proposed ABC-BMA\nmethod with our previous ABC-SD method. Results: In the estimation of standard\ndeviation, ABC-BMA has smaller average relative errors (AREs) than that of\nABC-SD for normal, lognormal, beta, and exponential distributions. For Weibull\ndistribution, ARE of ABC-BMA is larger than that of ABC-SD but <0.05 in small\nsample sizes and moves toward zero as sample size increases. When underlying\ndistribution is highly skewed and available summary statistics are only\nquartiles and sample size, ABC-BMA is recommended but it should be used with\ncaution. Comparison of mean estimation between ABC-BMA and ABC-SD shows similar\npatterns of results as for standard deviation estimation. Conclusion: ABC-BMA\nis easy to implement and it performs even better than our previous ABC-SD\nmethod for estimation of mean and standard deviation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 19:19:57 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Kwon", "Deukwoo", ""], ["Reis", "Isildinha M.", ""]]}, {"id": "1607.03197", "submitter": "BaoLuo Sun", "authors": "BaoLuo Sun, Lan Liu, Wang Miao, Kathleen Wirth, James Robins and Eric\n  Tchetgen Tchetgen", "title": "Semiparametric Estimation with Data Missing Not at Random Using an\n  Instrumental Variable", "comments": null, "journal-ref": "Statistica Sinica 28 (2018), 1965-1983", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data occur frequently in empirical studies in health and social\nsciences, often compromising our ability to make accurate inferences. An\noutcome is said to be missing not at random (MNAR) if, conditional on the\nobserved variables, the missing data mechanism still depends on the unobserved\noutcome. In such settings, identification is generally not possible without\nimposing additional assumptions. Identification is sometimes possible, however,\nif an instrumental variable (IV) is observed for all subjects which satisfies\nthe exclusion restriction that the IV affects the missingness process without\ndirectly influencing the outcome. In this paper, we provide necessary and\nsufficient conditions for nonparametric identification of the full data\ndistribution under MNAR with the aid of an IV. In addition, we give sufficient\nidentification conditions that are more straightforward to verify in practice.\nFor inference, we focus on estimation of a population outcome mean, for which\nwe develop a suite of semiparametric estimators that extend methods previously\ndeveloped for data missing at random. Specifically, we propose inverse\nprobability weighted estimation, outcome regression-based estimation and doubly\nrobust estimation of the mean of an outcome subject to MNAR. For illustration,\nthe methods are used to account for selection bias induced by HIV testing\nrefusal in the evaluation of HIV seroprevalence in Mochudi, Botswana, using\ninterviewer characteristics such as gender, age and years of experience as IVs.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 23:33:44 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 08:35:00 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Sun", "BaoLuo", ""], ["Liu", "Lan", ""], ["Miao", "Wang", ""], ["Wirth", "Kathleen", ""], ["Robins", "James", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "1607.03302", "submitter": "Alberto Llera", "authors": "A. Llera and C. F. Beckmann", "title": "Bayesian estimators of the Gamma distribution", "comments": "arXiv admin note: substantial text overlap with arXiv:1605.01019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce two Bayesian estimators for learning the\nparameters of the Gamma distribution. The first algorithm uses a well known\nunnormalized conjugate prior for the Gamma shape and the second one uses a\nnon-linear approximation to the likelihood and a prior on the shape that is\nconjugate to the approximated likelihood. In both cases use the Laplace\napproximation to compute the required expectations. We perform a theoretical\ncomparison between maximum like- lihood and the presented Bayesian algorithms\nthat allow us to provide non-informative parameter values for the priors hyper\nparameters. We also provide a numerical comparison using synthetic data. The\nintroduction of these novel Bayesian estimators open the possibility of\nincluding Gamma distributions into more complex Bayesian structures, e.g.\nvariational Bayesian mixture models.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 10:38:55 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Llera", "A.", ""], ["Beckmann", "C. F.", ""]]}, {"id": "1607.03675", "submitter": "Ege Rubak", "authors": "Jesper M{\\o}ller, Morten Nielsen, Emilio Porcu, Ege Rubak", "title": "Determinantal point process models on the sphere", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider determinantal point processes on the $d$-dimensional unit sphere\n$\\mathbb S^d$. These are finite point processes exhibiting repulsiveness and\nwith moment properties determined by a certain determinant whose entries are\nspecified by a so-called kernel which we assume is a complex covariance\nfunction defined on $\\mathbb S^d\\times\\mathbb S^d$. We review the appealing\nproperties of such processes, including their specific moment properties,\ndensity expressions and simulation procedures. Particularly, we characterize\nand construct isotropic DPPs models on $\\mathbb{S}^d$, where it becomes\nessential to specify the eigenvalues and eigenfunctions in a spectral\nrepresentation for the kernel, and we figure out how repulsive isotropic DPPs\ncan be. Moreover, we discuss the shortcomings of adapting existing models for\nisotropic covariance functions and consider strategies for developing new\nmodels, including a useful spectral approach.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 11:01:26 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["M\u00f8ller", "Jesper", ""], ["Nielsen", "Morten", ""], ["Porcu", "Emilio", ""], ["Rubak", "Ege", ""]]}, {"id": "1607.03698", "submitter": "Vitaliy Oryshchenko", "authors": "Vitaliy Oryshchenko", "title": "Indirect Maximum Entropy Bandwidth", "comments": "19 pages, 7 figures; 12 pages of supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method of bandwidth selection in kernel estimation\nof density and distribution functions motivated by the connection between\nmaximisation of the entropy of probability integral transforms and maximum\nlikelihood in classical parametric models. The proposed estimators are designed\nto indirectly maximise the entropy of the leave-one-out kernel estimates of a\ndistribution function, which are the analogues of the parametric probability\nintegral transforms.\n  The estimators based on minimisation of the Cramer-von Mises discrepancy,\nnear-solution of the moment-based estimating equations, and inversion of the\nNeyman smooth test statistic are discussed and their performance compared in a\nsimulation study. The bandwidth minimising the Anderson-Darling statistic is\nfound to perform reliably for a variety of distribution shapes and can be\nrecommended in practice.\n  The results will also be of interest to anyone analysing the cross-validation\nbandwidths based on leave-one-out estimates or evaluation of nonparametric\ndensity forecasts.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 12:32:48 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Oryshchenko", "Vitaliy", ""]]}, {"id": "1607.03717", "submitter": "Shujie Ma", "authors": "Shujie Ma, Jian Huang, Zhiwei Zhang and Mingming Liu", "title": "Exploration of heterogeneous treatment effects via concave fusion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Understanding treatment heterogeneity is essential to the development of\nprecision medicine, which seeks to tailor medical treatments to subgroups of\npatients with similar characteristics. One of the challenges to achieve this\ngoal is that we usually do not have a priori knowledge of the grouping\ninformation of patients with respect to treatment. To address this problem, we\nconsider a heterogeneous regression model by assuming that the coefficient for\ntreatment variables are subject-dependent and belong to different subgroups\nwith unknown grouping information. We develop a concave fusion penalized method\nfor automatically estimating the grouping structure and the subgroup-specific\ntreatment effects, and derive an alternating direction method of multipliers\nalgorithm for its implementation. We also study the theoretical properties of\nthe proposed method and show that under suitable conditions there exists a\nlocal minimizer that equals the oracle least squares estimator with a priori\nknowledge of the true grouping information with high probability. This provides\ntheoretical support for making statistical inference about the\nsubgroup-specific treatment effects based on the proposed method. We evaluate\nthe performance of the proposed method by simulation studies and illustrate its\napplication by analyzing the data from the AIDS Clinical Trials Group Study.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 13:15:58 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 22:29:12 GMT"}, {"version": "v3", "created": "Mon, 19 Aug 2019 23:56:26 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Ma", "Shujie", ""], ["Huang", "Jian", ""], ["Zhang", "Zhiwei", ""], ["Liu", "Mingming", ""]]}, {"id": "1607.03752", "submitter": "Joydeep Chowdhury", "authors": "Joydeep Chowdhury and Probal Chaudhuri", "title": "Nonparametric Depth and Quantile Regression for Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate nonparametric regression methods based on spatial depth and\nquantiles when the response and the covariate are both functions. As in\nclassical quantile regression for finite dimensional data, regression\ntechniques developed here provide insight into the influence of the functional\ncovariate on different parts, like the center as well as the tails, of the\nconditional distribution of the functional response. Depth and quantile based\nnonparametric regressions are useful to detect heteroscedasticity in functional\nregression. We derive the asymptotic behaviour of nonparametric depth and\nquantile regression estimates, which depend on the small ball probabilities in\nthe covariate space. Our nonparametric regression procedures are used to\nanalyse a dataset about the influence of per capita GDP on saving rates for 125\ncountries, and another dataset on the effects of per capita net disposable\nincome on the sale of cigarettes in some states in the US.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 14:20:22 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 14:36:43 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Chowdhury", "Joydeep", ""], ["Chaudhuri", "Probal", ""]]}, {"id": "1607.03775", "submitter": "Marine Dufournet", "authors": "Marine Dufournet, Emilie Lanoy, Jean-Louis Martin, Vivian Viallon", "title": "Causal inference to detect selection bias in road safety epidemiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of road safety, it is common to use responsibility analyses to\nassess the effect of a given factor on the risk of being responsible for an\naccident, among drivers involved in an accident only. Even if this design is\nnow widely adopted in the field, the question of selection bias is often\nraised. The structural Causal Model framework now provides valuable tools to\nassess causal effects from observational data and identify selection bias. In\nthis article, we briefly review recent results regarding the recoverability of\ncausal effects from selection biased data, and apply them to the case of\nresponsibility analyses. Our objective is to formally determine whether causal\neffects can be unbiasedly estimated through this type of analyses, when\navailable data are restricted to severe accidents, as it is commonly the case\nin practice. However, because speed has a direct effect on the severity of the\naccident, we show that causal odds-ratios are not estimable from responsibility\nanalyses. We present numerical results to illustrate our argument, the\nmagnitude of the bias and to discuss recent results from real data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 14:46:46 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 08:39:57 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 13:18:35 GMT"}, {"version": "v4", "created": "Mon, 7 May 2018 15:41:02 GMT"}, {"version": "v5", "created": "Sun, 14 Oct 2018 18:00:33 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Dufournet", "Marine", ""], ["Lanoy", "Emilie", ""], ["Martin", "Jean-Louis", ""], ["Viallon", "Vivian", ""]]}, {"id": "1607.03954", "submitter": "Charles Matthews", "authors": "Charles Matthews and Jonathan Weare and Benedict Leimkuhler", "title": "Ensemble preconditioning for Markov chain Monte Carlo simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe parallel Markov chain Monte Carlo methods that propagate a\ncollective ensemble of paths, with local covariance information calculated from\nneighboring replicas. The use of collective dynamics eliminates multiplicative\nnoise and stabilizes the dynamics thus providing a practical approach to\ndifficult anisotropic sampling problems in high dimensions. Numerical\nexperiments with model problems demonstrate that dramatic potential speedups,\ncompared to various alternative schemes, are attainable.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 23:12:05 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Matthews", "Charles", ""], ["Weare", "Jonathan", ""], ["Leimkuhler", "Benedict", ""]]}, {"id": "1607.03975", "submitter": "Eric Strobl", "authors": "Eric V. Strobl, Peter L. Spirtes, Shyam Visweswaran", "title": "Estimating and Controlling the False Discovery Rate for the PC Algorithm\n  Using Edge-Specific P-Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The PC algorithm allows investigators to estimate a complete partially\ndirected acyclic graph (CPDAG) from a finite dataset, but few groups have\ninvestigated strategies for estimating and controlling the false discovery rate\n(FDR) of the edges in the CPDAG. In this paper, we introduce PC with p-values\n(PC-p), a fast algorithm which robustly computes edge-specific p-values and\nthen estimates and controls the FDR across the edges. PC-p specifically uses\nthe p-values returned by many conditional independence tests to upper bound the\np-values of more complex edge-specific hypothesis tests. The algorithm then\nestimates and controls the FDR using the bounded p-values and the\nBenjamini-Yekutieli FDR procedure. Modifications to the original PC algorithm\nalso help PC-p accurately compute the upper bounds despite non-zero Type II\nerror rates. Experiments show that PC-p yields more accurate FDR estimation and\ncontrol across the edges in a variety of CPDAGs compared to alternative\nmethods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 01:34:57 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 03:43:48 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Strobl", "Eric V.", ""], ["Spirtes", "Peter L.", ""], ["Visweswaran", "Shyam", ""]]}, {"id": "1607.04039", "submitter": "Timothy NeCamp", "authors": "Timothy NeCamp, Amy Kilbourne, Daniel Almirall", "title": "Comparing cluster-level dynamic treatment regimens using sequential,\n  multiple assignment, randomized trials: Regression estimation and sample size\n  considerations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster-level dynamic treatment regimens can be used to guide sequential,\nintervention or treatment decision-making at the cluster level in order to\nimprove outcomes at the individual or patient-level. In a cluster-level DTR,\nthe intervention or treatment is potentially adapted and re-adapted over time\nbased on changes in the cluster that could be impacted by prior intervention,\nincluding based on aggregate measures of the individuals or patients that\ncomprise it. Cluster-randomized sequential multiple assignment randomized\ntrials (SMARTs) can be used to answer multiple open questions preventing\nscientists from developing high-quality cluster-level DTRs. In a\ncluster-randomized SMART, sequential randomizations occur at the cluster level\nand outcomes are at the individual level. This manuscript makes two\ncontributions to the design and analysis of cluster-randomized SMARTs: First, a\nweighted least squares regression approach is proposed for comparing the mean\nof a patient-level outcome between the cluster-level DTRs embedded in a SMART.\nThe regression approach facilitates the use of baseline covariates which is\noften critical in the analysis of cluster-level trials. Second, sample size\ncalculators are derived for two common cluster-randomized SMART designs for use\nwhen the primary aim is a between-DTR comparison of the mean of a continuous\npatient-level outcome. The methods are motivated by the Adaptive Implementation\nof Effective Programs Trial, which is, to our knowledge, the first-ever\ncluster-randomized SMART in psychiatry.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 09:06:21 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["NeCamp", "Timothy", ""], ["Kilbourne", "Amy", ""], ["Almirall", "Daniel", ""]]}, {"id": "1607.04204", "submitter": "Jing Lei", "authors": "Jing Lei, Anne-Sophie Charest, Aleksandra Slavkovic, Adam Smith,\n  Stephen Fienberg", "title": "Differentially Private Model Selection with Penalized and Constrained\n  Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical disclosure control, the goal of data analysis is twofold: The\nreleased information must provide accurate and useful statistics about the\nunderlying population of interest, while minimizing the potential for an\nindividual record to be identified. In recent years, the notion of differential\nprivacy has received much attention in theoretical computer science, machine\nlearning, and statistics. It provides a rigorous and strong notion of\nprotection for individuals' sensitive information. A fundamental question is\nhow to incorporate differential privacy into traditional statistical inference\nprocedures. In this paper we study model selection in multivariate linear\nregression under the constraint of differential privacy. We show that model\nselection procedures based on penalized least squares or likelihood can be made\ndifferentially private by a combination of regularization and randomization,\nand propose two algorithms to do so. We show that our private procedures are\nconsistent under essentially the same conditions as the corresponding\nnon-private procedures. We also find that under differential privacy, the\nprocedure becomes more sensitive to the tuning parameters. We illustrate and\nevaluate our method using simulation studies and two real data examples.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 16:51:05 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Lei", "Jing", ""], ["Charest", "Anne-Sophie", ""], ["Slavkovic", "Aleksandra", ""], ["Smith", "Adam", ""], ["Fienberg", "Stephen", ""]]}, {"id": "1607.04209", "submitter": "Kirstin Early Kirstin Early", "authors": "Kirstin Early, Jennifer Mankoff, Stephen E. Fienberg", "title": "Dynamic Question Ordering in Online Surveys", "comments": "In submission to the Journal of Official Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online surveys have the potential to support adaptive questions, where later\nquestions depend on earlier responses. Past work has taken a rule-based\napproach, uniformly across all respondents. We envision a richer interpretation\nof adaptive questions, which we call dynamic question ordering (DQO), where\nquestion order is personalized. Such an approach could increase engagement, and\ntherefore response rate, as well as imputation quality. We present a DQO\nframework to improve survey completion and imputation. In the general\nsurvey-taking setting, we want to maximize survey completion, and so we focus\non ordering questions to engage the respondent and collect hopefully all\ninformation, or at least the information that most characterizes the\nrespondent, for accurate imputations. In another scenario, our goal is to\nprovide a personalized prediction. Since it is possible to give reasonable\npredictions with only a subset of questions, we are not concerned with\nmotivating users to answer all questions. Instead, we want to order questions\nto get information that reduces prediction uncertainty, while not being too\nburdensome. We illustrate this framework with an example of providing energy\nestimates to prospective tenants. We also discuss DQO for national surveys and\nconsider connections between our statistics-based question-ordering approach\nand cognitive survey methodology.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 17:03:24 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Early", "Kirstin", ""], ["Mankoff", "Jennifer", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1607.04522", "submitter": "Maria Lucia Parrella", "authors": "Maria Lucia Parrella", "title": "Modelling high-dimensional time series efficiently by means of\n  constrained spatio--temporal models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many econometric analyses involve spatio--temporal data. A considerable\namount of literature has addressed spatio--temporal models, with Spatial\nDynamic Panel Data (SDPD) being widely investigated and applied. In real data\napplications, checking the validity of the theoretical assumptions underlying\nthe SDPD models is essential but sometimes difficult. At other times, the\nassumptions are clearly violated. For example, the spatial matrix is assumed to\nbe known but it may actually be unknown and needs to be estimated. In such\ncases, the performance of the SDPD model's estimator is generally affected.\nMotivated by such considerations, we propose a new model (called stationary\nSDPD) and a new estimation procedure based on simple and clear assumptions that\ncan be easily checked with real data. The new model is highly adaptive, and the\nestimation procedure has a rate of convergence that is not affected by the\ndimension of the time series (under general assumptions), notwithstanding the\nrelatively high number of parameters to be estimated. The new model may be used\nto represent a wide class of multivariate time series, not necessarily\nspatio-temporal. So, it can be used as a valid alternative to vector\nautoregressive (VAR) models with two immediate advantages: i) a faster rate of\nconvergence of the estimation procedure and ii) the possibility of estimating\nthe model even when the dimension is higher than the time series length,\novercoming the curse of dimensionality typical of the VAR models. The\nsimulation study shows that the new estimation procedure performs well compared\nwith the classic alternative procedure, even when the spatial matrix is unknown\nand therefore estimated.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 14:24:09 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Parrella", "Maria Lucia", ""]]}, {"id": "1607.04532", "submitter": "Gregor Kastner", "authors": "Florian Huber, Gregor Kastner, Martin Feldkircher", "title": "Should I stay or should I go? A latent threshold approach to large-scale\n  mixture innovation models", "comments": null, "journal-ref": "Journal of Applied Econometrics 34(5), 621-640 (2019)", "doi": "10.1002/jae.2680", "report-no": null, "categories": "stat.ME econ.EM stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a straightforward algorithm to carry out inference in\nlarge time-varying parameter vector autoregressions (TVP-VARs) with mixture\ninnovation components for each coefficient in the system. We significantly\ndecrease the computational burden by approximating the latent indicators that\ndrive the time-variation in the coefficients with a latent threshold process\nthat depends on the absolute size of the shocks. The merits of our approach are\nillustrated with two applications. First, we forecast the US term structure of\ninterest rates and demonstrate forecast gains of the proposed mixture\ninnovation model relative to other benchmark models. Second, we apply our\napproach to US macroeconomic data and find significant evidence for\ntime-varying effects of a monetary policy tightening.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 14:42:32 GMT"}, {"version": "v2", "created": "Sun, 11 Sep 2016 16:28:16 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 12:25:18 GMT"}, {"version": "v4", "created": "Fri, 12 Jan 2018 15:18:18 GMT"}, {"version": "v5", "created": "Thu, 26 Jul 2018 05:38:46 GMT"}], "update_date": "2019-08-07", "authors_parsed": [["Huber", "Florian", ""], ["Kastner", "Gregor", ""], ["Feldkircher", "Martin", ""]]}, {"id": "1607.04796", "submitter": "Fabrizio Leisen", "authors": "Fabrizio Leisen, Juan Miguel Marin and Cristiano Villa", "title": "Objective Bayesian modelling of insurance risks with the skewed\n  Student-t distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insurance risks data typically exhibit skewed behaviour. In this paper, we\npropose a Bayesian approach to capture the main features of these datasets.\nThis work extends the methodology introduced in Villa and Walker (2014a) by\nconsidering an extra parameter which captures the skewness of the data. In\nparticular, a skewed Student-t distribution is considered. Two datasets are\nanalysed: the Danish fire losses and the US indemnity loss. The analysis is\ncarried with an objective Bayesian approach. For the discrete parameter\nrepresenting the number of the degrees of freedom, we adopt a novel prior\nrecently introduced in Villa and Walker (2014b).\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 19:23:59 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Leisen", "Fabrizio", ""], ["Marin", "Juan Miguel", ""], ["Villa", "Cristiano", ""]]}, {"id": "1607.04848", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo", "title": "A note on the asymptotic normality of sums of extreme values", "comments": "12", "journal-ref": "Journal of Statistical Planning and Inference, 22, 1989, pp.\n  127-136", "doi": "10.1016/0378-3758(89)90071-2", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_1$, $X_2$,... be a sequence of independent random variables with\ncommon distribution function $F$ in the domain of attraction of a Gumbel\nextreme value distribution and for each integer $n\\geq 1$, let $X_{1,n} \\leq\n... X_{n,n}$ denote the order statistics based on the first $n$ of these random\nvariables. Along with related results it is shown that for any sequence of\npositive integers $k_n \\rightarrow +\\infty$ and $k_{n}/n \\rightarrow 0$ as $n\n\\rightarrow 0$ the sum of the upper $k_n$ extreme values\n$X_{n-k_{n},n}+...+X_{n,n}$, when properly centered and normalized, converges\nin distribution to a standard normal random variable $N(0, 1)$. These results\nconstitute an extension of results by S. Cs\\\"{o}rg\\H{o} and D.M. Mason (1985).\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 10:46:59 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Lo", "Gane Samb", ""]]}, {"id": "1607.04993", "submitter": "Matthieu Wilhelm", "authors": "Matthieu Wilhelm and Yves Till\\'e and Lionel Qualit\\'e", "title": "Quasi-Systematic Sampling From a Continuous Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A specific family of point processes are introduced that allow to select\nsamples for the purpose of estimating the mean or the integral of a function of\na real variable. These processes, called quasi-systematic processes, depend on\na tuning parameter $r>0$ that permits to control the likeliness of jointly\nselecting neighbor units in a same sample. When $r$ is large, units that are\nclose tend to not be selected together and samples are well spread. When $r$\ntends to infinity, the sampling design is close to systematic sampling. For all\n$r > 0$, the first and second-order unit inclusion densities are positive,\nallowing for unbiased estimators of variance.\n  Algorithms to generate these sampling processes for any positive real value\nof $r$ are presented. When $r$ is large, the estimator of variance is unstable.\nIt follows that $r$ must be chosen by the practitioner as a trade-off between\nan accurate estimation of the target parameter and an accurate estimation of\nthe variance of the parameter estimator. The method's advantages are\nillustrated with a set of simulations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 09:52:07 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Wilhelm", "Matthieu", ""], ["Till\u00e9", "Yves", ""], ["Qualit\u00e9", "Lionel", ""]]}, {"id": "1607.05042", "submitter": "Carlo Sguera", "authors": "Carlo Sguera and Rosa E. Lillo", "title": "An empirical comparison of global and local functional depths", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A functional data depth provides a center-outward ordering criterion which\nallows the definition of measures such as median, trimmed means, central\nregions or ranks in a functional framework. A functional data depth can be\nglobal or local. With global depths, the degree of centrality of a curve $x$\ndepends equally on the rest of the sample observations, while with local\ndepths, the contribution of each observation in defining the degree of\ncentrality of $x$ decreases as the distance from $x$ increases. We empirically\ncompare the global and the local approaches to the functional depth problem\nfocusing on three global and two local functional depths. First, we consider\ntwo real data sets and show that global and local depths may provide different\ninsights. Second, we use simulated data to show when we should expect\ndifferences between a global and a local approach to the functional depth\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 12:29:20 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 07:23:05 GMT"}, {"version": "v3", "created": "Thu, 5 Jul 2018 11:16:44 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Sguera", "Carlo", ""], ["Lillo", "Rosa E.", ""]]}, {"id": "1607.05051", "submitter": "Ryan Martin", "authors": "Ryan Martin", "title": "False confidence, non-additive beliefs, and valid statistical inference", "comments": "60 pages, 12 figures. Comments welcome at\n  https://www.researchers.one/article/2019-02-1", "journal-ref": "International Journal of Approximate Reasoning, 2019, volume 113,\n  pages 39--73", "doi": "10.1016/j.ijar.2019.06.005", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistics has made tremendous advances since the times of Fisher, Neyman,\nJeffreys, and others, but the fundamental and practically relevant questions\nabout probability and inference that puzzled our founding fathers remain\nunanswered. To bridge this gap, I propose to look beyond the two dominating\nschools of thought and ask the following three questions: what do scientists\nneed out of statistics, do the existing frameworks meet these needs, and, if\nnot, how to fill the void? To the first question, I contend that scientists\nseek to convert their data, posited statistical model, etc., into calibrated\ndegrees of belief about quantities of interest. To the second question, I argue\nthat any framework that returns additive beliefs, i.e., probabilities,\nnecessarily suffers from {\\em false confidence}---certain false hypotheses tend\nto be assigned high probability---and, therefore, risks systematic bias. This\nreveals the fundamental importance of {\\em non-additive beliefs} in the context\nof statistical inference. But non-additivity alone is not enough so, to the\nthird question, I offer a sufficient condition, called {\\em validity}, for\navoiding false confidence, and present a framework, based on random sets and\nbelief functions, that provably meets this condition. Finally, I discuss\ncharacterizations of p-values and confidence intervals in terms of valid\nnon-additive beliefs, which imply that users of these classical procedures are\nalready following the proposed framework without knowing it.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 12:56:13 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 03:53:41 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 20:43:33 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Martin", "Ryan", ""]]}, {"id": "1607.05169", "submitter": "Xiaogang Su", "authors": "Xiaogang Su, Juanjuan Fan, Richard A. Levine, Martha E. Nunn, and\n  Chih-Ling Tsai", "title": "Sparse Estimation of Generalized Linear Models (GLM) via Approximated\n  Information Criteria", "comments": "23 pages, 3 figures", "journal-ref": "Statistica Sinica, 28: 1561-1581, 2018", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new sparse estimation method, termed MIC (Minimum approximated\nInformation Criterion), for generalized linear models (GLM) in fixed\ndimensions. What is essentially involved in MIC is the approximation of the\n$\\ell_0$-norm with a continuous unit dent function. Besides, a\nreparameterization step is devised to enforce sparsity in parameter estimates\nwhile maintaining the smoothness of the objective function. MIC yields superior\nperformance in sparse estimation by optimizing the approximated information\ncriterion without reducing the search space and is computationally advantageous\nsince no selection of tuning parameters is required. Moreover, the\nreparameterization tactic leads to valid significance testing results that are\nfree of post-selection inference. We explore the asymptotic properties of MIC\nand illustrate its usage with both simulated experiments and empirical\nexamples.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:35:50 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Su", "Xiaogang", ""], ["Fan", "Juanjuan", ""], ["Levine", "Richard A.", ""], ["Nunn", "Martha E.", ""], ["Tsai", "Chih-Ling", ""]]}, {"id": "1607.05184", "submitter": "Ansgar Steland", "authors": "Ansgar Steland", "title": "On the Accuracy of Fixed Sampled and Fixed Width Confidence Intervals\n  Based on the Vertically Weighted Averages", "comments": null, "journal-ref": "Journal of Statistical Theory and Practice, 2017 - 11(3) 375-392,", "doi": "10.1080/15598608.2016.1263809", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertically weighted averages perform a bilateral filtering of data, in order\nto preserve fine details of the underlying signal, especially discontinuities\nsuch as jumps (in dimension one) or edges (in dimension two). In homogeneous\nregions of the domain the procedure smoothes the data by averaging nearby data\npoints to reduce the noise, whereas in inhomogenous regions the neighboring\npoints are only taken into account when their value is close to the current\none. This results in a denoised reconstruction or estimate of the true signal\nwithout blurring finer details.\n  This paper addresses the lack of results about the construction and\nevaluation of confidence intervals based on the vertically weighted average,\nwhich is required for a proper statistical evaluation of its estimation\naccuracy. Based on recent results we discuss and investigate in greater detail\nfixed sample as well as fixed width (conditional) confidence intervals\nconstructed from this estimator. The fixed width approach allows to specify\nexplicitly the estimator's accuracy and determines a random sample size to\nensure the required coverage probability. This also fixes to some extent the\ninherent property of the vertically weighted average that its variability is\nhigher in low-density regions than in high-density regions. To estimate the\nvariances required to construct the procedures, we rely on resampling\ntechniques, especially the bootstrap and the jackknife.\n  Extensive Monte Carlo simulations show that the proposed confidence intervals\nare reliable in terms of their coverage probabilities for a wide range of\nparameter settings.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:56:53 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 14:56:25 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Steland", "Ansgar", ""]]}, {"id": "1607.05424", "submitter": "Holger Dette", "authors": "Frank Bretz, Kathrin M\\\"ollenhoff, Holger Dette, Wei Liu, Matthias\n  Trampisch", "title": "Assessing the similarity of dose response and target doses in two\n  non-overlapping subgroups", "comments": "Keywords and Phrases: equivalence testing, multiregional trial,\n  target dose estimation, subgroup analyses", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two problems that are attracting increasing attention in clinical\ndose finding studies. First, we assess the similarity of two non-linear\nregression models for two non-overlapping subgroups of patients over a\nrestricted covariate space. To this end, we derive a confidence interval for\nthe maximum difference between the two given models. If this confidence\ninterval excludes the equivalence margins, similarity of dose response can be\nclaimed. Second, we address the problem of demonstrating the similarity of two\ntarget doses for two non-overlapping subgroups, using again a confidence\ninterval based approach. We illustrate the proposed methods with a real case\nstudy and investigate their operating characteristics (coverage probabilities,\nType I error rates, power) via simulation.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 06:45:12 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 17:48:30 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Bretz", "Frank", ""], ["M\u00f6llenhoff", "Kathrin", ""], ["Dette", "Holger", ""], ["Liu", "Wei", ""], ["Trampisch", "Matthias", ""]]}, {"id": "1607.05455", "submitter": "Lutz Duembgen", "authors": "Lutz Duembgen and David E. Tyler", "title": "Geodesic Convexity and Regularized Scatter Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As observed by Auderset et al. (2005) and Wiesel (2012), viewing covariance\nmatrices as elements of a Riemannian manifold and using the concept of geodesic\nconvexity provide useful tools for studying M-estimators of multivariate\nscatter. In this paper, we begin with a mathematically rigorous self-contained\noverview of Riemannian geometry on the space of symmetric positive definite\nmatrices and of the notion of geodesic convexity. The overview contains both a\nreview as well as new results. In particular, we introduce and utilize first\nand second order Taylor expansions with respect to geodesic parametrizations.\nThis enables us to give sufficient conditions for a function to be geodesically\nconvex. In addition, we introduce the concept of geodesic coercivity, which is\nimportant in establishing the existence of a minimum to a geodesic convex\nfunction. We also develop a general partial Newton algorithm for minimizing\nsmooth and strictly geodesically convex functions. We then use these results to\ngenerate a fairly complete picture of the existence, uniqueness and computation\nof regularized M-estimators of scatter defined using additive geodescially\nconvex penalty terms. Various such penalties are demonstrated which shrink an\nestimator towards the identity matrix or multiples of the identity matrix.\nFinally, we propose a cross-validation method for choosing the scaling\nparameter for the penalty function, and illustrate our results using a\nnumerical example.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 08:33:41 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 05:04:23 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Duembgen", "Lutz", ""], ["Tyler", "David E.", ""]]}, {"id": "1607.05858", "submitter": "Roberto Molinari Mr", "authors": "St\\'ephane Guerrier and Roberto Molinari", "title": "Wavelet Variance for Random Fields: an M-Estimation Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general M-estimation framework for inference on the wavelet\nvariance. This framework generalizes the results on the scale-wise properties\nof the standard estimator and extends them to deliver the joint asymptotic\nproperties of the estimated wavelet variance vector. Moreover, this is achieved\nby extending the estimation of the wavelet variance to multidimensional random\nfields and by stating the necessary conditions for these properties to hold\nwhen the size of the wavelet variance vector goes to infinity with the sample\nsize. Finally, these results generally hold when using bounded estimating\nfunctions thereby delivering a robust framework for the estimation of this\nquantity which improves over existing methods both in terms of asymptotic\nproperties and in terms of its finite sample performance. The proposed\nestimator is investigated in simulation studies and different applications\nhighlighting its good properties.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 08:20:07 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Molinari", "Roberto", ""]]}, {"id": "1607.05861", "submitter": "Roberto Molinari Mr", "authors": "St\\'ephane Guerrier and Roberto Molinari", "title": "Fast and Robust Parametric Estimation for Time Series and Spatial Models", "comments": "arXiv admin note: substantial text overlap with arXiv:1512.09325", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new framework for robust estimation and inference on\nsecond-order stationary time series and random fields. This framework is based\non the Generalized Method of Wavelet Moments which uses the wavelet variance to\nachieve parameter estimation for complex models. Using an M-estimator of the\nwavelet variance, this method can be made robust therefore allowing to estimate\nthe parameters of a wide range of time series and spatial models when the data\nsuffers from outliers or different forms of contamination. The paper presents a\nseries of simulation studies as well as a range of applications where this new\napproach can be considered as a computationally efficient, numerically stable\nand robust method which performs at least as well as existing methods in\nbounding the influence of outliers on the estimation procedure.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 08:40:19 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Molinari", "Roberto", ""]]}, {"id": "1607.05874", "submitter": "Johannes Klepsch", "authors": "Johannes Klepsch and Claudia Kl\\\"uppelberg", "title": "An Innovations Algorithm for the prediction of functional linear\n  processes", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When observations are curves over some natural time interval, the field of\nfunctional data analysis comes into play. Functional linear processes account\nfor temporal dependence in the data. The prediction problem for functional\nlinear processes has been solved theoretically, but the focus for applications\nhas been on functional autoregressive processes. We propose a new\ncomputationally tractable linear predictor for functional linear processes. It\nis based on an application of the Multivariate Innovations Algorithm to\nfinite-dimensional subprocesses of increasing dimension of the\ninfinite-dimensional functional linear process. We investigate the behavior of\nthe predictor for increasing sample size. We show that, depending on the decay\nrate of the eigenvalues of the covariance and the spectral density operator,\nthe resulting predictor converges with a certain rate to the theoretically best\nlinear predictor.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 09:10:19 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Klepsch", "Johannes", ""], ["Kl\u00fcppelberg", "Claudia", ""]]}, {"id": "1607.06051", "submitter": "Dingdong Yi", "authors": "Xinran Li, Dingdong Yi, Jun S. Liu", "title": "Bayesian Analysis of Rank Data with Covariates and Heterogeneous Rankers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data in the form of ranking lists are frequently encountered, and combining\nranking results from different sources can potentially generate a better\nranking list and help understand behaviors of the rankers. Of interest here are\nthe rank data under the following settings: (i) covariate information available\nfor the ranked entities; (ii) rankers of varying qualities or having different\nopinions; and (iii) incomplete ranking lists for non-overlapping subgroups. We\nreview some key ideas built around the Thurstone model family by researchers in\nthe past few decades and provide a unifying approach for Bayesian Analysis of\nRank data with Covariates (BARC) and its extensions in handling heterogeneous\nrankers. With this Bayesian framework, we can study rankers' varying quality,\ncluster rankers' heterogeneous opinions, and measure the corresponding\nuncertainties. To enable an efficient Bayesian inference, we advocate a\nparameter-expanded Gibbs sampler to sample from the target posterior\ndistribution. The posterior samples also result in a Bayesian aggregated\nranking list, with credible intervals quantifying its uncertainty. We\ninvestigate and compare performances of the proposed methods and other rank\naggregation methods in both simulation studies and two real-data examples.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 18:23:00 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 20:45:02 GMT"}, {"version": "v3", "created": "Fri, 17 Jul 2020 02:50:16 GMT"}], "update_date": "2020-07-20", "authors_parsed": [["Li", "Xinran", ""], ["Yi", "Dingdong", ""], ["Liu", "Jun S.", ""]]}, {"id": "1607.06158", "submitter": "Konstantinos Spiliopoulos", "authors": "Andrew Papanicolaou, Konstantinos Spiliopoulos", "title": "Dimension Reduction in Statistical Estimation of Partially Observed\n  Multiscale Processes", "comments": "SIAM Journal of Uncertainty Quantification, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST q-fin.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider partially observed multiscale diffusion models that are specified\nup to an unknown vector parameter. We establish for a very general class of\ntest functions that the filter of the original model converges to a filter of\nreduced dimension. Then, this result is used to justify statistical estimation\nfor the unknown parameters of interest based on the model of reduced dimension\nbut using the original available data. This allows to learn the unknown\nparameters of interest while working in lower dimensions, as opposed to working\nwith the original high dimensional system. Simulation studies support and\nillustrate the theoretical results.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 00:14:31 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 20:06:55 GMT"}, {"version": "v3", "created": "Sun, 26 Nov 2017 18:02:11 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Papanicolaou", "Andrew", ""], ["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1607.06163", "submitter": "David Frazier", "authors": "David T. Frazier and Eric Renault", "title": "Indirect Inference With(Out) Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-fin.EC stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indirect Inference (I-I) estimation of structural parameters $\\theta$\n{{requires matching observed and simulated statistics, which are most often\ngenerated using an auxiliary model that depends on instrumental parameters\n$\\beta$.}} {The estimators of the instrumental parameters will encapsulate} the\nstatistical information used for inference about the structural parameters. As\nsuch, artificially constraining these parameters may restrict the ability of\nthe auxiliary model to accurately replicate features in the structural data,\nwhich may lead to a range of issues, such as, a loss of identification.\nHowever, in certain situations the parameters $\\beta$ naturally come with a set\nof $q$ restrictions. Examples include settings where $\\beta$ must be estimated\nsubject to $q$ possibly strict inequality constraints $g(\\beta) > 0$, such as,\nwhen I-I is based on GARCH auxiliary models. In these settings we propose a\nnovel I-I approach that uses appropriately modified unconstrained auxiliary\nstatistics, which are simple to compute and always exists. We state the\nrelevant asymptotic theory for this I-I approach without constraints and show\nthat it can be reinterpreted as a standard implementation of I-I through a\nproperly modified binding function. Several examples that have featured in the\nliterature illustrate our approach.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 00:57:10 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 06:18:18 GMT"}, {"version": "v3", "created": "Tue, 20 Aug 2019 10:11:32 GMT"}], "update_date": "2019-08-21", "authors_parsed": [["Frazier", "David T.", ""], ["Renault", "Eric", ""]]}, {"id": "1607.06358", "submitter": "Ian Vernon Dr", "authors": "Ian Vernon and Junli Liu and Michael Goldstein and James Rowe and Jen\n  Topping and Keith Lindsey", "title": "Bayesian uncertainty analysis for complex systems biology models:\n  emulation, global parameter searches and evaluation of gene functions", "comments": "26 pages, 13 figures. Version accepted by BMC systems biology", "journal-ref": "BMC Systems Biology (2018), 12(1)", "doi": "10.1186/s12918-017-0484-3", "report-no": null, "categories": "q-bio.MN q-bio.CB q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Many mathematical models have now been employed across every area\nof systems biology. These models increasingly involve large numbers of unknown\nparameters, have complex structure which can result in substantial evaluation\ntime relative to the needs of the analysis, and need to be compared to observed\ndata. The correct analysis of such models usually requires a global parameter\nsearch, over a high dimensional parameter space, that incorporates and respects\nthe most important sources of uncertainty. This can be an extremely difficult\ntask, but it is essential for any meaningful inference or prediction to be made\nabout any biological system. It hence represents a fundamental challenge for\nthe whole of systems biology.\n  Results: Bayesian statistical methodology for the uncertainty analysis of\ncomplex models is introduced, which is designed to address the high dimensional\nglobal parameter search problem. Bayesian emulators that mimic the systems\nbiology model but which are extremely fast to evaluate are embedded within an\niterative history match: an efficient method to search high dimensional spaces\nwithin a more formal statistical setting, while incorporating major sources of\nuncertainty. The approach is demonstrated via application to two models of\nhormonal crosstalk in Arabidopsis root development, which have 32 rate\nparameters, for which we identify the sets of rate parameter values that lead\nto acceptable matches to observed trend data. The biological consequences of\nthe resulting comparison, including the evaluation of gene functions, are\ndescribed.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 15:10:57 GMT"}, {"version": "v2", "created": "Fri, 12 Jan 2018 11:36:41 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Vernon", "Ian", ""], ["Liu", "Junli", ""], ["Goldstein", "Michael", ""], ["Rowe", "James", ""], ["Topping", "Jen", ""], ["Lindsey", "Keith", ""]]}, {"id": "1607.06565", "submitter": "Edward McFowland Iii", "authors": "Edward McFowland III and Cosma Rohilla Shalizi", "title": "Estimating Causal Peer Influence in Homophilous Social Networks by\n  Inferring Latent Locations", "comments": "35 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social influence cannot be identified from purely observational data on\nsocial networks, because such influence is generically confounded with latent\nhomophily, i.e., with a node's network partners being informative about the\nnode's attributes and therefore its behavior. If the network grows according to\neither a latent community (stochastic block) model, or a continuous latent\nspace model, then latent homophilous attributes can be consistently estimated\nfrom the global pattern of social ties. We show that, for common versions of\nthose two network models, these estimates are so informative that controlling\nfor estimated attributes allows for asymptotically unbiased and consistent\nestimation of social-influence effects in linear models. In particular, the\nbias shrinks at a rate which directly reflects how much information the network\nprovides about the latent attributes. These are the first results on the\nconsistent non-experimental estimation of social-influence effects in the\npresence of latent homophily, and we discuss the prospects for generalizing\nthem.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 06:22:23 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 04:17:46 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 16:49:28 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["McFowland", "Edward", "III"], ["Shalizi", "Cosma Rohilla", ""]]}, {"id": "1607.06763", "submitter": "Matthias Raess", "authors": "Matthias Raess", "title": "Exploring elastic net and multivariate regression", "comments": "Methodology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When it comes to datasets with a tremendous amount of predictors, variable\nreduction techniques such as PCA or FA are often used. In this paper, the\nelastic net, which lies in between the LASSO method and ridge regression, is\nused as a variable reduction technique followed by further analysis with\nmultivariate regression. Specifically, a messy only dataset is used to show how\nit can be 'tidied' up and broken down into sensible subsets using the\naforementioned method.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 17:43:56 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Raess", "Matthias", ""]]}, {"id": "1607.06779", "submitter": "Robert J. B. Goudie", "authors": "Robert J. B. Goudie, Anne M. Presanis, David Lunn, Daniela De Angelis\n  and Lorenz Wernisch", "title": "Joining and splitting models with Markov melding", "comments": null, "journal-ref": null, "doi": "10.1214/18-BA1104", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysing multiple evidence sources is often feasible only via a modular\napproach, with separate submodels specified for smaller components of the\navailable evidence. Here we introduce a generic framework that enables fully\nBayesian analysis in this setting. We propose a generic method for forming a\nsuitable joint model when joining submodels, and a convenient computational\nalgorithm for fitting this joint model in stages, rather than as a single,\nmonolithic model. The approach also enables splitting of large joint models\ninto smaller submodels, allowing inference for the original joint model to be\nconducted via our multi-stage algorithm. We motivate and demonstrate our\napproach through two examples: joining components of an evidence synthesis of\nA/H1N1 influenza, and splitting a large ecology model.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 18:12:30 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 17:40:12 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 20:46:30 GMT"}], "update_date": "2018-04-18", "authors_parsed": [["Goudie", "Robert J. B.", ""], ["Presanis", "Anne M.", ""], ["Lunn", "David", ""], ["De Angelis", "Daniela", ""], ["Wernisch", "Lorenz", ""]]}, {"id": "1607.06801", "submitter": "Stefan Wager", "authors": "Stefan Wager, Wenfei Du, Jonathan Taylor and Robert Tibshirani", "title": "High-dimensional regression adjustments in randomized experiments", "comments": "To appear in the Proceedings of the National Academy of Sciences. The\n  present draft does not reflect final copyediting by the PNAS staff", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of treatment effect estimation in randomized experiments\nwith high-dimensional covariate information, and show that essentially any\nrisk-consistent regression adjustment can be used to obtain efficient estimates\nof the average treatment effect. Our results considerably extend the range of\nsettings where high-dimensional regression adjustments are guaranteed to\nprovide valid inference about the population average treatment effect. We then\npropose cross-estimation, a simple method for obtaining finite-sample-unbiased\ntreatment effect estimates that leverages high-dimensional regression\nadjustments. Our method can be used when the regression model is estimated\nusing the lasso, the elastic net, subset selection, etc. Finally, we extend our\nanalysis to allow for adaptive specification search via cross-validation, and\nflexible non-parametric regression adjustments with machine learning methods\nsuch as random forests or neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 19:38:46 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 23:35:46 GMT"}, {"version": "v3", "created": "Thu, 27 Oct 2016 21:36:45 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Wager", "Stefan", ""], ["Du", "Wenfei", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1607.06802", "submitter": "Brahimi Brahim", "authors": "Chine Amel, Benatia Fateh and Brahimi Brahim", "title": "Trimmed L-moments For Estimation Multi-parameter Archimedean Copulas", "comments": "This paper has been withdrawn by the author due to a crucial\n  modification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trimmed L-moments, were introduced by Elamir and Seheult(2003) to proposed a\nnew estimation method for multi-parameter distributions when the mean doesn't\nexist or for heavy tailed distribution where the L-moments method which\nproposed by Hosking (1990) is not valid because the absence of theoretical\nL-moments. In this paper a new estimation method based on trimmed L-moments of\nmulti-parameter copulas is proposed with a simulation study. The consistency\nand the asymptotic normality of the new estimator also established.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 19:41:40 GMT"}, {"version": "v2", "created": "Sun, 4 Sep 2016 18:36:03 GMT"}], "update_date": "2016-09-06", "authors_parsed": [["Amel", "Chine", ""], ["Fateh", "Benatia", ""], ["Brahim", "Brahimi", ""]]}, {"id": "1607.06849", "submitter": "Yang Ni", "authors": "Yang Ni and Yuan Ji and Peter Mueller", "title": "Reciprocal Graphical Models for Integrative Gene Regulatory Network\n  Analysis", "comments": "20 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constructing gene regulatory networks is a fundamental task in systems\nbiology. We introduce a Gaussian reciprocal graphical model for inference about\ngene regulatory relationships by integrating mRNA gene expression and DNA level\ninformation including copy number and methylation. Data integration allows for\ninference on the directionality of certain regulatory relationships, which\nwould be otherwise indistinguishable due to Markov equivalence. Efficient\ninference is developed based on simultaneous equation models. Bayesian model\nselection techniques are adopted to estimate the graph structure. We illustrate\nour approach by simulations and two applications in ZODIAC pairwise gene\ninteraction analysis and colon adenocarcinoma pathway analysis.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 21:51:22 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Ni", "Yang", ""], ["Ji", "Yuan", ""], ["Mueller", "Peter", ""]]}, {"id": "1607.06903", "submitter": "Christian P. Robert", "authors": "David T. Frazier (Monash University), Gael M. Martin (Monash\n  University), Christian P. Robert (Universit\\'e Paris-Dauphine PSL and\n  University of Warwick, UK), and Judith Rousseau (University of Oxford, UK)", "title": "Asymptotic Properties of Approximate Bayesian Computation", "comments": "This 31 pages paper is a revised version of the paper, including\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation allows for statistical analysis in models\nwith intractable likelihoods. In this paper we consider the asymptotic\nbehaviour of the posterior distribution obtained by this method. We give\ngeneral results on the rate at which the posterior distribution concentrates on\nsets containing the true parameter, its limiting shape, and the asymptotic\ndistribution of the posterior mean. These results hold under given rates for\nthe tolerance used within the method, mild regularity conditions on the summary\nstatistics, and a condition linked to identification of the true parameters.\nImplications for practitioners are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 09:05:00 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2016 06:37:26 GMT"}, {"version": "v3", "created": "Mon, 26 Jun 2017 09:18:42 GMT"}, {"version": "v4", "created": "Tue, 8 May 2018 15:22:41 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Frazier", "David T.", "", "Monash University"], ["Martin", "Gael M.", "", "Monash\n  University"], ["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine PSL and\n  University of Warwick, UK"], ["Rousseau", "Judith", "", "University of Oxford, UK"]]}, {"id": "1607.06976", "submitter": "Sihai Zhao", "authors": "Sihai Dave Zhao", "title": "Integrative genetic risk prediction using nonparametric empirical Bayes\n  classification", "comments": null, "journal-ref": null, "doi": "10.1111/biom.12619", "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic risk prediction is an important component of individualized medicine,\nbut prediction accuracies remain low for many complex diseases. A fundamental\nlimitation is the sample sizes of the studies on which the prediction\nalgorithms are trained. One way to increase the effective sample size is to\nintegrate information from previously existing studies. However, it can be\ndifficult to find existing data that examine the target disease of interest,\nespecially if that disease is rare or poorly studied. Furthermore,\nindividual-level genotype data from these auxiliary studies are typically\ndifficult to obtain. This paper proposes a new approach to integrative genetic\nrisk prediction of complex diseases with binary phenotypes. It accommodates\npossible heterogeneity in the genetic etiologies of the target and auxiliary\ndiseases using a tuning parameter-free nonparametric empirical Bayes procedure,\nand can be trained using only auxiliary summary statistics. Simulation studies\nshow that the proposed method can provide superior predictive accuracy relative\nto non-integrative as well as integrative classifiers. The method is applied to\na recent study of pediatric autoimmune diseases, where it substantially reduces\nprediction error for certain target/auxiliary disease combinations. The\nproposed method is implemented in the R package ssa.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 22:11:28 GMT"}, {"version": "v2", "created": "Thu, 25 Aug 2016 23:40:47 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Zhao", "Sihai Dave", ""]]}, {"id": "1607.07002", "submitter": "Feifei Wang", "authors": "Feifei Wang, Jian Wang, Alan E. Gelfand and Fan Li", "title": "Disease Mapping with Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disease mapping focuses on learning about areal units presenting high\nrelative risk. Disease mapping models for disease counts specify Poisson\nregressions in relative risks compared with the expected counts. These models\ntypically incorporate spatial random effects to accomplish spatial smoothing.\nFitting of these models customarily computes expected disease counts via\ninternal standardization. This places the data on both sides of the model,\ni.e., the counts are on the left side but they are also used to obtain the\nexpected counts on the right side. As a result, these internally standardized\nmodels are incoherent and not generative; probabilistically, they could not\nproduce the observed data. Here, we argue for adopting the direct generative\nmodel for disease counts. We model disease incidence instead of relative risks,\nusing a generalized logistic regression. We extract relative risks post model\nfitting. We also extend the generative model to dynamic settings. We compare\nthe generative models with internally standardized models through simulated\ndatasets and a well-examined lung cancer morbidity data in Ohio. Each model is\na spatial smoother and they smooth the data similarly with regard to relative\nrisks. However, the generative models tend to provide tighter credible\nintervals. Since the generative specification is no more difficult to fit, is\ncoherent, and is at least as good inferentially, we suggest it should be the\nmodel of choice for spatial disease mapping.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 05:41:45 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Wang", "Feifei", ""], ["Wang", "Jian", ""], ["Gelfand", "Alan E.", ""], ["Li", "Fan", ""]]}, {"id": "1607.07028", "submitter": "Matthias Schmid", "authors": "Leonie Weinhold, Simone Wahl, Matthias Schmid", "title": "A Statistical Model for the Analysis of Beta Values in DNA Methylation\n  Studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: The analysis of DNA methylation is a key component in the\ndevelopment of personalized treatment approaches. A common way to measure DNA\nmethylation is the calculation of beta values, which are bounded variables of\nthe form M = (M + U) that are generated by Illumina's 450k BeadChip array. The\nstatistical analysis of beta values is considered to be challenging, as\ntraditional methods for the analysis of bounded variables, such as M-value\nregression and beta regression, are based on regularity assumptions that are\noften too strong to adequately describe the distribution of beta values.\nResults: We develop a statistical model for the analysis of beta values that is\nderived from a bivariate gamma distribution for the signal intensities M and U.\nBy allowing for possible correlations between M and U, the proposed model\nexplicitly takes into account the data-generating process underlying the\ncalculation of beta values. Conclusion: The proposed model can be used to\nimprove the identification of associations between beta values and covariates\nsuch as clinical variables and lifestyle factors in epigenome-wide association\nstudies. It is as easy to apply to a sample of beta values as beta regression\nand M-value regression.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 11:30:50 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Weinhold", "Leonie", ""], ["Wahl", "Simone", ""], ["Schmid", "Matthias", ""]]}, {"id": "1607.07083", "submitter": "Matthias Eckardt", "authors": "Matthias Eckardt", "title": "Graphical modelling of multivariate spatial point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel graphical model, termed the spatial dependence\ngraph model, which captures the global dependence structure of different events\nthat occur randomly in space. In the spatial dependence graph model, the edge\nset is identified by using the conditional partial spectral coherence. Thereby,\nnodes are related to the components of a multivariate spatial point process and\nedges express orthogonality relation between the single components. This paper\nintroduces an efficient approach towards pattern analysis of highly structured\nand high dimensional spatial point processes. Unlike all previous methods, our\nnew model permits the simultaneous analysis of all multivariate conditional\ninterrelations. The potential of our new technique to investigate multivariate\nstructural relations is illustrated using data on forest stands in Lansing\nWoods as well as monthly data on crimes committed in the City of London.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 19:30:11 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Eckardt", "Matthias", ""]]}, {"id": "1607.07203", "submitter": "Kyoungjae Lee", "authors": "Sarat C. Dass, Jaeyong Lee, Kyoungjae Lee and Jonghun Park", "title": "Laplace based approximate posterior inference for differential equation\n  models", "comments": null, "journal-ref": null, "doi": "10.1007/s11222-016-9647-0", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary differential equations are arguably the most popular and useful\nmathematical tool for describing physical and biological processes in the real\nworld. Often, these physical and biological processes are observed with errors,\nin which case the most natural way to model such data is via regression where\nthe mean function is defined by an ordinary differential equation believed to\nprovide an understanding of the underlying process. These regression based\ndynamical models are called differential equation models. Parameter inference\nfrom differential equation models poses computational challenges mainly due to\nthe fact that analytic solutions to most differential equations are not\navailable. In this paper, we propose an approximation method for obtaining the\nposterior distribution of parameters in differential equation models. The\napproximation is done in two steps. In the first step, the solution of a\ndifferential equation is approximated by the general one-step method which is a\nclass of numerical methods for ordinary differential equations including the\nEuler and the Runge-Kutta procedures; in the second step, nuisance parameters\nare marginalized using Laplace approximation. The proposed Laplace approximated\nposterior gives a computationally fast alternative to the full Bayesian\ncomputational scheme (such as Markov Chain Monte Carlo) and produces more\naccurate and stable estimators than the popular smoothing methods (called\ncollocation methods) based on frequentist procedures. For a theoretical support\nof the proposed method, we prove that the Laplace approximated posterior\nconverges to the actual posterior under certain conditions and analyze the\nrelation between the order of numerical error and its Laplace approximation.\nThe proposed method is tested on simulated data sets and compared with the\nother existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 11:06:12 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Dass", "Sarat C.", ""], ["Lee", "Jaeyong", ""], ["Lee", "Kyoungjae", ""], ["Park", "Jonghun", ""]]}, {"id": "1607.07330", "submitter": "Kevin Xu", "authors": "Ruthwik R. Junuthula, Kevin S. Xu, and Vijay K. Devabhaktuni", "title": "Evaluating Link Prediction Accuracy on Dynamic Networks with Added and\n  Removed Edges", "comments": "To appear in Proceedings of SocialCom 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of predicting future relationships in a social network, known as\nlink prediction, has been studied extensively in the literature. Many link\nprediction methods have been proposed, ranging from common neighbors to\nprobabilistic models. Recent work by Yang et al. has highlighted several\nchallenges in evaluating link prediction accuracy. In dynamic networks where\nedges are both added and removed over time, the link prediction problem is more\ncomplex and involves predicting both newly added and newly removed edges. This\nresults in new challenges in the evaluation of dynamic link prediction methods,\nand the recommendations provided by Yang et al. are no longer applicable,\nbecause they do not address edge removal. In this paper, we investigate several\nmetrics currently used for evaluating accuracies of dynamic link prediction\nmethods and demonstrate why they can be misleading in many cases. We provide\nseveral recommendations on evaluating dynamic link prediction accuracy,\nincluding separation into two categories of evaluation. Finally we propose a\nunified metric to characterize link prediction accuracy effectively using a\nsingle number.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 16:00:32 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Junuthula", "Ruthwik R.", ""], ["Xu", "Kevin S.", ""], ["Devabhaktuni", "Vijay K.", ""]]}, {"id": "1607.07414", "submitter": "Ihab Sraj", "authors": "Ihab Sraj and Kyle T. Mandli and Omar M. Knio and Clint N. Dawson and\n  Ibrahim Hoteit", "title": "Quantifying Uncertainties in Fault Slip Distribution during the T\\=ohoku\n  Tsunami using Polynomial Chaos", "comments": null, "journal-ref": null, "doi": "10.1007/s10236-017-1105-9", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An efficient method for inferring Manning's $n$ coefficients using water\nsurface elevation data was presented in Sraj et al. (2014) focusing on a test\ncase based on data collected during the $T\\=ohoku$ earthquake and tsunami.\nPolynomial chaos expansions were used to build an inexpensive surrogate for the\nnumerical model Geoclaw, which were then used to perform a sensitivity analysis\nin addition to the inversion. In this paper, a new analysis is performed with\nthe goal of inferring the fault slip distribution of the $T\\=ohoku$ earthquake\nusing a similar problem setup. The same approach to constructing the PC\nsurrogate did not lead to a converging expansion, however an alternative\napproach based on Basis-Pursuit DeNoising was found to be suitable. Our result\nshows that the fault slip distribution can be inferred using water surface\nelevation data whereas the inferred values minimizes the error between\nobservations and the numerical model. The numerical approach and the resulting\ninversion are presented in this work.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 19:09:41 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 19:50:56 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Sraj", "Ihab", ""], ["Mandli", "Kyle T.", ""], ["Knio", "Omar M.", ""], ["Dawson", "Clint N.", ""], ["Hoteit", "Ibrahim", ""]]}, {"id": "1607.07423", "submitter": "Arin Chaudhuri", "authors": "Deovrat Kakde, Sergriy Peredriy, Arin Chaudhuri, Anya Mcguirk", "title": "A Non-Parametric Control Chart For High Frequency Multivariate Data", "comments": null, "journal-ref": null, "doi": "10.1109/RAM.2017.7889786", "report-no": null, "categories": "cs.LG stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support Vector Data Description (SVDD) is a machine learning technique used\nfor single class classification and outlier detection. SVDD based K-chart was\nfirst introduced by Sun and Tsung for monitoring multivariate processes when\nunderlying distribution of process parameters or quality characteristics depart\nfrom Normality. The method first trains a SVDD model on data obtained from\nstable or in-control operations of the process to obtain a threshold $R^2$ and\nkernel center a. For each new observation, its Kernel distance from the Kernel\ncenter a is calculated. The kernel distance is compared against the threshold\n$R^2$ to determine if the observation is within the control limits. The\nnon-parametric K-chart provides an attractive alternative to the traditional\ncontrol charts such as the Hotelling's $T^2$ charts when distribution of the\nunderlying multivariate data is either non-normal or is unknown. But there are\nchallenges when K-chart is deployed in practice. The K-chart requires\ncalculating kernel distance of each new observation but there are no guidelines\non how to interpret the kernel distance plot and infer about shifts in process\nmean or changes in process variation. This limits the application of K-charts\nin big-data applications such as equipment health monitoring, where\nobservations are generated at a very high frequency. In this scenario, the\nanalyst using the K-chart is inundated with kernel distance results at a very\nhigh frequency, generally without any recourse for detecting presence of any\nassignable causes of variation. We propose a new SVDD based control chart,\ncalled as $K_T$ chart, which addresses challenges encountered when using\nK-chart for big-data applications. The $K_T$ charts can be used to\nsimultaneously track process variation and central tendency. We illustrate the\nsuccessful use of $K_T$ chart using the Tennessee Eastman process data.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 19:40:55 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2016 22:23:47 GMT"}, {"version": "v3", "created": "Fri, 29 Jul 2016 20:31:54 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Kakde", "Deovrat", ""], ["Peredriy", "Sergriy", ""], ["Chaudhuri", "Arin", ""], ["Mcguirk", "Anya", ""]]}, {"id": "1607.07521", "submitter": "Shu Yang", "authors": "Shu Yang", "title": "Propensity score weighting for causal inference with multi-stage\n  clustered data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score weighting is a tool for causal inference to adjust for\nmeasured confounders. Survey data are often collected under complex sampling\ndesigns such as multistage cluster sampling, which presents challenges for\npropensity score modeling and estimation. In addition, for clustered data,\nthere may also be unobserved cluster effects related to both the treatment and\nthe outcome. When such unmeasured confounders exist and are omitted in the\npropensity score model, the subsequent propensity score adjustment will be\nbiased. We propose a calibrated propensity score weighting adjustment for\nmulti-stage clustered data in the presence of unmeasured cluster-level\nconfounders. The propensity score is calibrated to balance design-weighted\ncovariate distributions and cluster effects between treatment groups. In\nparticular, we consider a growing number of calibration constraints increasing\nwith the number of clusters, which is necessary for removing asymptotic bias\nthat is associated with the unobserved cluster-level confounders. We show that\nour estimator is robust in the sense that the estimator is consistent without\ncorrect specification of the propensity score model. We extend the results to\nthe multiple treatments case. In simulation studies we show that the proposed\nestimator is superior to other competitors. We estimate the effect of School\nBody Mass Index Screening on prevalence of overweight and obesity for\nelementary schools in Pennsylvania.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 02:14:58 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Yang", "Shu", ""]]}, {"id": "1607.07745", "submitter": "Arin Chaudhuri", "authors": "Deovrat Kakde, Arin Chaudhuri", "title": "Leveraging Unstructured Data to Detect Emerging Reliability Issues", "comments": null, "journal-ref": null, "doi": "10.1109/RAMS.2015.7105093", "report-no": null, "categories": "cs.AI stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured data refers to information that does not have a predefined data\nmodel or is not organized in a pre-defined manner. Loosely speaking,\nunstructured data refers to text data that is generated by humans. In\nafter-sales service businesses, there are two main sources of unstructured\ndata: customer complaints, which generally describe symptoms, and technician\ncomments, which outline diagnostics and treatment information. A legitimate\ncustomer complaint can eventually be tracked to a failure or a claim. However,\nthere is a delay between the time of a customer complaint and the time of a\nfailure or a claim. A proactive strategy aimed at analyzing customer complaints\nfor symptoms can help service providers detect reliability problems in advance\nand initiate corrective actions such as recalls. This paper introduces\nessential text mining concepts in the context of reliability analysis and a\nmethod to detect emerging reliability issues. The application of the method is\nillustrated using a case study.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:19:12 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Kakde", "Deovrat", ""], ["Chaudhuri", "Arin", ""]]}, {"id": "1607.07771", "submitter": "Hyunphil Choi", "authors": "Hyunphil Choi, Matthew Reimherr", "title": "A Geometric Approach to Confidence Regions and Bands for Functional\n  Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis, FDA, is now a well established discipline of\nstatistics, with its core concepts and perspectives in place. Despite this,\nthere are still fundamental statistical questions which have received\nrelatively little attention. One of these is the systematic construction of\nconfidence regions for functional parameters. This work is concerned with\ndeveloping, understanding, and visualizing such regions. We provide a general\nstrategy for constructing confidence regions in a real separable Hilbert space\nusing hyper-ellipsoids and hyper-rectangles. We then propose specific\nimplementations which work especially well in practice. They provide powerful\nhypothesis tests and useful visualization tools without using any simulation.\nWe also demonstrate the negative result that nearly all regions, including our\nown, have zero-coverage when working with empirical covariances. To overcome\nthis challenge we propose a new paradigm for evaluating confidence regions by\nshowing that the distance between an estimated region and the desired region\n(with proper coverage) tends to zero faster than the regions shrink to a point.\nWe call this phenomena ghosting and refer to the empirical regions as ghost\nregions. We illustrate the proposed methods in a simulation study and an\napplication to fractional anisotropy tract profile data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:59:50 GMT"}, {"version": "v2", "created": "Wed, 10 Aug 2016 15:51:27 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Choi", "Hyunphil", ""], ["Reimherr", "Matthew", ""]]}, {"id": "1607.07834", "submitter": "Rui Sun", "authors": "Rui Sun, Haoyi Weng, Inchi Hu, Junfeng Guo, William K.K. Wu, Benny\n  Chung-Ying Zee, Maggie Haitian Wang", "title": "A W-test collapsing method for rare variant testing with applications to\n  exome sequencing data of hypertensive disorder", "comments": "18 pages, 1 figure, 4 tables. Genetic Epidemiology accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advancement in sequencing technology enables the study of association between\ncomplex disorders and rare variants with low minor allele frequencies. One of\nthe major challenges in rare variant testing is lack of statistical power of\ntraditional testing methods due to extremely low variances of single nucleotide\npolymorphisms. In this paper, we introduce a W-test collapsing method that\nevaluates the distributional differences in cases and controls using a combined\nlog of odds ratio. The proposed method is compared with the Weighted-Sum\nStatistic and Sequence Kernel Association Test using simulation data sets and\nshowed better performances and faster computing speed. In the study of real\nnext generation sequencing data set of hypertensive disorder, we identified\ngenes of interesting biological functions that are associated to metabolism\ndisorder and inflammation, which include the MACROD1, NLRP7, AGK, PAK6 and\nAPBB1. The W-test collapsing method offers a fast, effective and alternative\nway for rare variants association analysis.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 18:32:30 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Sun", "Rui", ""], ["Weng", "Haoyi", ""], ["Hu", "Inchi", ""], ["Guo", "Junfeng", ""], ["Wu", "William K. K.", ""], ["Zee", "Benny Chung-Ying", ""], ["Wang", "Maggie Haitian", ""]]}, {"id": "1607.07925", "submitter": "Alexandre B. Simas", "authors": "Andr\\'ea V. Rocha and Alexandre B. Simas", "title": "Asymptotic adjustments of Pearson residuals in exponential family\n  nonlinear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we define a set of corrected Pearson residuals for continuous\nexponential family nonlinear models that have the same distribution as the true\nPearson residuals up to order $\\mathcal{O}(n^{-1})$, where $n$ is the sample\nsize. Furthermore, we also introduce a new modification of the Pearson\nresiduals, which we call PCA Pearson residuals, that are approximately\nuncorrelated. These PCA residuals are new even for the generalized linear\nmodels. The numerical results show that the PCA residuals are approximately\nnormally distributed, thus improving previous results by Simas and Cordeiro\n(2009). These numerical results also show that the corrected Pearson residuals\napproximately follow the same distribution as the true residuals, which is a\nconsiderable improvement with respect to the Pearson residuals and also extends\nthe previous work by Cordeiro and Simas (2009).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 00:52:47 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Rocha", "Andr\u00e9a V.", ""], ["Simas", "Alexandre B.", ""]]}, {"id": "1607.07926", "submitter": "Alexandre B. Simas", "authors": "Andr\\'ea V. Rocha, Evelina Shamarova and Alexandre B. Simas", "title": "Improved residuals for linear regression models under heteroskedasticity\n  of unknown form", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce a new residual for normal linear models that are\nsuitable for situations in which we are dealing with heteroskedasticity of\nunknown form, they are referred to by principal component analysis (PCA)\nresiduals. These residuals are obtained through a linear transformation of the\nordinary residuals, by means of a spectral analysis on a\nheteroskedasticity-consistent estimator of the covariance matrix. The resulting\nresiduals are independent and normally distributed. These residuals provide a\nsimple way to check several assumptions that underlie the normal linear\nregression model, as well as model adequacy. Since they are independent and\nnormally distributed, one may apply several results on independent random\nvariables directly to these residuals. Finally, we provide an application to\nreal data to illustrate the usefulness of our residuals.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 00:55:25 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 04:58:30 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Rocha", "Andr\u00e9a V.", ""], ["Shamarova", "Evelina", ""], ["Simas", "Alexandre B.", ""]]}, {"id": "1607.07974", "submitter": "Michail Tsagris", "authors": "Michail Tsagris, Simon Preston and Andrew T.A. Wood", "title": "Nonparametric hypothesis testing for equality of means on the simplex", "comments": "This is a preprint of the article to be published by Taylor & Francis\n  Group in Journal of Statistical Computation and Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of data that lie on the simplex, we investigate use of\nempirical and exponential empirical likelihood, and Hotelling and James\nstatistics, to test the null hypothesis of equal population means based on two\nindependent samples. We perform an extensive numerical study using data\nsimulated from various distributions on the simplex. The results, taken\ntogether with practical considerations regarding implementation, support the\nuse of bootstrap-calibrated James statistic.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 06:39:12 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2016 12:15:06 GMT"}], "update_date": "2016-08-05", "authors_parsed": [["Tsagris", "Michail", ""], ["Preston", "Simon", ""], ["Wood", "Andrew T. A.", ""]]}, {"id": "1607.08033", "submitter": "Maria Lucia Parrella", "authors": "Francesco Giordano and Maria Lucia Parrella", "title": "Efficient nonparametric estimation and inference for the volatility\n  function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last decades there has been increasing interest in modeling the\nvolatility of financial data. Several parametric models have been proposed to\nthis aim, starting from ARCH, GARCH and their variants, but often it is hard to\nevaluate which one is the most suitable for the analyzed financial data. In\nthis paper we focus on nonparametric analysis of the volatility function for\nmixing processes. Our approach encompasses many parametric frameworks and\nsupplies several tools which can be used to give evidence against or in favor\nof a specific parametric model: nonparametric function estimation, confidence\nbands and test for symmetry. Another contribution of this paper is to give an\nalternative representation of the GARCH(1,1) model in terms of a\nNonparametric-ARCH(1) model, which avoids the use of the lagged volatility, so\nthat a more precise and more informative News Impact Function can be estimated\nby our procedure. We prove the consistency of the proposed method and\ninvestigate its empirical performance on synthetic and real datasets.\nSurprisingly, for finite sample size, the simulation results show a better\nperformance of our nonparametric estimator compared with the MLE estimator of a\nGARCH(1,1) model, even in the case of correct specification of the model.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 11:04:07 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Giordano", "Francesco", ""], ["Parrella", "Maria Lucia", ""]]}, {"id": "1607.08096", "submitter": "S\\'andor Baran", "authors": "S\\'andor Baran and Sebastian Lerch", "title": "Combining predictive distributions for statistical post-processing of\n  ensemble forecasts", "comments": "33 pages, 8 figures, 2 tables", "journal-ref": "International Journal of Forecasting 2018, 34, 477--496", "doi": "10.1016/j.ijforecast.2018.01.005", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical post-processing techniques are now widely used to correct\nsystematic biases and errors in calibration of ensemble forecasts obtained from\nmultiple runs of numerical weather prediction models. A standard approach is\nthe ensemble model output statistics (EMOS) method, a distributional regression\napproach where the forecast distribution is given by a single parametric law\nwith parameters depending on the ensemble members. Choosing an appropriate\nparametric family for the weather variable of interest is a critical, however,\noften non-trivial task, and has been the focus of much recent research. In this\narticle, we assess the merits of combining predictive distributions from\nmultiple EMOS models based on different parametric families. In four case\nstudies with wind speed and precipitation forecasts from two ensemble\nprediction systems, we study whether state of the art forecast combination\nmethods are able to improve forecast skill.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 14:08:16 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 07:15:50 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 09:11:25 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Baran", "S\u00e1ndor", ""], ["Lerch", "Sebastian", ""]]}, {"id": "1607.08169", "submitter": "Gianluca Baio", "authors": "Sara Geneletti and Federico Ricciardi and Aidan O'Keeffe and Gianluca\n  Baio", "title": "Bayesian modelling for binary outcomes in the Regression Discontinuity\n  Design", "comments": "29 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Regression Discontinuity (RD) design is a quasi-experimental design which\nemulates a randomised study by exploiting situations where treatment is\nassigned according to a continuous variable as is common in many drug treatment\nguidelines. The RD design literature focuses principally on continuous\noutcomes. In this paper we exploit the link between the RD design and\ninstrumental variables to obtain a causal effect estimator, the risk ratio for\nthe treated (RRT), for the RD design when the outcome is binary. Occasionally\nthe RRT estimator can give negative lower confindence bounds. In the Bayesian\nframework we impose prior constraints that prevent this from happening. This is\nnovel and cannot be easily reproduced in a frequentist framework. We compare\nour estimators to those based on estimating equation and generalized methods of\nmoments methods. Based on extensive simulations our methods compare favourably\nwith both methods. We apply our method on a real example to estimate the effect\nof statins on the probability of Low-density Lipoprotein (LDL) cholesterol\nlevels reaching recommended levels.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 16:18:15 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Geneletti", "Sara", ""], ["Ricciardi", "Federico", ""], ["O'Keeffe", "Aidan", ""], ["Baio", "Gianluca", ""]]}, {"id": "1607.08211", "submitter": "Rina Foygel Barber", "authors": "Fan Yang, Rina Foygel Barber, Prateek Jain, and John Lafferty", "title": "Selective Inference for Group-Sparse Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop tools for selective inference in the setting of group sparsity,\nincluding the construction of confidence intervals and p-values for testing\nselected groups of variables. Our main technical result gives the precise\ndistribution of the magnitude of the projection of the data onto a given\nsubspace, and enables us to develop inference procedures for a broad class of\ngroup-sparse selection methods, including the group lasso, iterative hard\nthresholding, and forward stepwise regression. We give numerical results to\nillustrate these tools on simulated data and on health record data.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 18:39:26 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Yang", "Fan", ""], ["Barber", "Rina Foygel", ""], ["Jain", "Prateek", ""], ["Lafferty", "John", ""]]}, {"id": "1607.08255", "submitter": "Maria Xose Rodriguez-Alvarez", "authors": "Mar\\'ia Xos\\'e Rodr\\'iguez-\\'Alvarez, Martin P. Boer, Fred A. van\n  Eeuwijk and Paul H. C. Eilers", "title": "Spatial Models for Field Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aim of the analysis of agricultural field trials is to obtain\ngood predictions for genotypic performance, by correcting for spatial effects.\nIn practice these corrections turn out to be complicated, since there can be\ndifferent types of spatial effects; those due to management interventions\napplied to the field plots and those due to various kinds of erratic spatial\ntrends. This paper presents models for field trials in which the random spatial\ncomponent consists of tensor product Penalized splines (P-splines). A special\nANOVA-type reformulation leads to five smooth additive spatial components,\nwhich form the basis of a mixed model with five unknown variance components. On\ntop of this spatial field, effects of genotypes, blocks, replicates, and/or\nother sources of spatial variation are described by a mixed model in a standard\nway. We show the relation between several definitions of heritability and the\neffective dimension or the effective degrees of freedom associated to the\ngenetic component. The approach is illustrated with large-scale field trial\nexperiments. An R-package is provided.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 20:04:47 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Rodr\u00edguez-\u00c1lvarez", "Mar\u00eda Xos\u00e9", ""], ["Boer", "Martin P.", ""], ["van Eeuwijk", "Fred A.", ""], ["Eilers", "Paul H. C.", ""]]}, {"id": "1607.08274", "submitter": "Hang Kim", "authors": "Hang J. Kim, Steven N. MacEachern, Yoonsuh Jung", "title": "Bandwidth Selection for Kernel Density Estimation with a Markov Chain\n  Monte Carlo Sample", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Carlo samplers produce dependent streams of variates drawn\nfrom the limiting distribution of the Markov chain. With this as motivation, we\nintroduce novel univariate kernel density estimators which are appropriate for\nthe stationary sequences of dependent variates. We modify the asymptotic mean\nintegrated squared error criterion to account for dependence and find that the\nmodified criterion suggests data-driven adjustments to standard bandwidth\nselection methods. Simulation studies show that our proposed methods find\nbandwidths close to the optimal value while standard methods lead to smaller\nbandwidths and hence to undersmoothed density estimates. Empirically, the\nproposed methods have considerably smaller integrated mean squared error than\ndo standard methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 22:14:41 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Kim", "Hang J.", ""], ["MacEachern", "Steven N.", ""], ["Jung", "Yoonsuh", ""]]}, {"id": "1607.08288", "submitter": "Amy Willis", "authors": "Amy Willis", "title": "Confidence sets for phylogenetic trees", "comments": "Final version accepted to the Journal of the American Statistical\n  Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring evolutionary histories (phylogenetic trees) has important\napplications in biology, criminology and public health. However, phylogenetic\ntrees are complex mathematical objects that reside in a non-Euclidean space,\nwhich complicates their analysis. While our mathematical, algorithmic, and\nprobabilistic understanding of phylogenies in their metric space is mature,\nrigorous inferential infrastructure is as yet undeveloped. In this manuscript\nwe unify recent computational and probabilistic advances to construct\ntree--valued confidence sets. The procedure accounts for both centre and\nmultiple directions of tree--valued variability. We draw on block replicates to\nimprove testing, identifying the best supported most recent ancestor of the\nZika virus, and formally testing the hypothesis that a Floridian dentist with\nAIDS infected two of his patients with HIV. The method illustrates connections\nbetween variability in Euclidean and tree space, opening phylogenetic tree\nanalysis to techniques available in the multivariate Euclidean setting.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 00:30:25 GMT"}, {"version": "v2", "created": "Fri, 13 Oct 2017 01:18:13 GMT"}], "update_date": "2017-10-16", "authors_parsed": [["Willis", "Amy", ""]]}, {"id": "1607.08372", "submitter": "Denis Allard", "authors": "Denis Marcotte and Denis Allard", "title": "Half-tapering strategy for conditional simulation with large datasets", "comments": "39 pages, 2 Tables and 11 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian conditional realizations are routinely used for risk assessment and\nplanning in a variety of Earth sciences applications. Conditional realizations\ncan be obtained by first creating unconditional realizations that are then\npost-conditioned by kriging. Many efficient algorithms are available for the\nfirst step, so the bottleneck resides in the second step. Instead of doing the\nconditional simulations with the desired covariance (F approach) or with a\ntapered covariance (T approach), we propose to use the taper covariance only in\nthe conditioning step (Half-Taper or HT approach). This enables to speed up the\ncomputations and to reduce memory requirements for the conditioning step but\nalso to keep the right short scale variations in the realizations. A criterion\nbased on mean square error of the simulation is derived to help anticipate the\nsimilarity of HT to F. Moreover, an index is used to predict the sparsity of\nthe kriging matrix for the conditioning step. Some guides for the choice of the\ntaper function are discussed. The distributions of a series of 1D, 2D and 3D\nscalar response functions are compared for F, T and HT approaches. The\ndistributions obtained indicate a much better similarity to F with HT than with\nT.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 09:21:52 GMT"}, {"version": "v2", "created": "Fri, 27 Jan 2017 15:48:34 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 14:13:32 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Marcotte", "Denis", ""], ["Allard", "Denis", ""]]}, {"id": "1607.08554", "submitter": "Fang Liu", "authors": "Fang Liu", "title": "Statistical Properties of Sanitized Results from Differentially Private\n  Laplace Mechanism with Univariate Bounding Constraints", "comments": null, "journal-ref": "Transactions on Data Privacy, 2019, 12(3): 169 - 195", "doi": null, "report-no": null, "categories": "stat.ME cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protection of individual privacy is a common concern when releasing and\nsharing data and information. Differential privacy (DP) formalizes privacy in\nprobabilistic terms without making assumptions about the background knowledge\nof data intruders, and thus provides a robust concept for privacy protection.\nPractical applications of DP involve development of differentially private\nmechanisms to generate sanitized results at a pre-specified privacy budget. For\nthe sanitization of statistics with publicly known bounds such as proportions\nand correlation coefficients, the bounding constraints will need to be\nincorporated in the differentially private mechanisms. There has been little\nwork on examining the consequences of the bounding constraints on the accuracy\nof sanitized results and the statistical inferences of the population\nparameters based on the sanitized results. In this paper, we formalize the\ndifferentially private truncated and boundary inflated truncated (BIT)\nprocedures for releasing statistics with publicly known bounding constraints.\nThe impacts of the truncated and BIT Laplace procedures on the statistical\naccuracy and validity of sanitized statistics are evaluated both theoretically\nand empirically via simulation studies.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 18:13:25 GMT"}, {"version": "v2", "created": "Mon, 31 Oct 2016 17:41:23 GMT"}, {"version": "v3", "created": "Sun, 13 Aug 2017 04:22:25 GMT"}, {"version": "v4", "created": "Mon, 12 Mar 2018 01:39:21 GMT"}, {"version": "v5", "created": "Sun, 26 May 2019 20:08:44 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Liu", "Fang", ""]]}, {"id": "1607.08685", "submitter": "Shinsuke Koyama", "authors": "Shinsuke Koyama", "title": "Projection-based filtering for stochastic reaction networks", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study concerns online inference (i.e., filtering) on the state of\nreaction networks, conditioned on noisy and partial measurements. The\ndifficulty in deriving the equation that the conditional probability\ndistribution of the state satisfies stems from the fact that the master\nequation, which governs the evolution of the reaction networks, is analytically\nintractable. The linear noise approximation (LNA) technique, which is widely\nused in the analysis of reaction networks, has recently been applied to develop\napproximate inference. Here, we apply the projection method to derive\napproximate filters, and compare them to a filter based on the LNA numerically\nin their filtering performance. We also contrast the projection method with\nmoment-closure techniques in terms of approximating the evolution of stochastic\nreaction networks.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 04:45:06 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 08:14:35 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Koyama", "Shinsuke", ""]]}, {"id": "1607.08712", "submitter": "Samrat Mukhopadhyay", "authors": "Samrat Mukhopadhyay, Prateek Vashishtha and, Mrityunjoy Chakraborty", "title": "Signal Recovery in Uncorrelated and Correlated Dictionaries Using\n  Orthogonal Least Squares", "comments": "18 Pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.NA math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though the method of least squares has been used for a long time in solving\nsignal processing problems, in the recent field of sparse recovery from\ncompressed measurements, this method has not been given much attention. In this\npaper we show that a method in the least squares family, known in the\nliterature as Orthogonal Least Squares (OLS), adapted for compressed recovery\nproblems, has competitive recovery performance and computation complexity, that\nmakes it a suitable alternative to popular greedy methods like Orthogonal\nMatching Pursuit (OMP). We show that with a slight modification, OLS can\nexactly recover a $K$-sparse signal, embedded in an $N$ dimensional space\n($K<<N$) in $M=\\mathcal{O}(K\\log (N/K))$ no of measurements with Gaussian\ndictionaries. We also show that OLS can be easily implemented in such a way\nthat it requires $\\mathcal{O}(KMN)$ no of floating point operations similar to\nthat of OMP. In this paper performance of OLS is also studied with sensing\nmatrices with correlated dictionary, in which algorithms like OMP does not\nexhibit good recovery performance. We study the recovery performance of OLS in\na specific dictionary called \\emph{generalized hybrid dictionary}, which is\nshown to be a correlated dictionary, and show numerically that OLS has is far\nsuperior to OMP in these kind of dictionaries in terms of recovery performance.\nFinally we provide analytical justifications that corroborate the findings in\nthe numerical illustrations.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 07:42:09 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Mukhopadhyay", "Samrat", ""], ["and", "Prateek Vashishtha", ""], ["Chakraborty", "Mrityunjoy", ""]]}, {"id": "1607.08799", "submitter": "Yunpeng Li", "authors": "Yunpeng Li and Mark Coates", "title": "Particle Filtering with Invertible Particle Flow", "comments": null, "journal-ref": "IEEE Transactions on Signal Processing, Vol. 65, No. 15, pp.\n  4102-4116 (August 1, 2017)", "doi": "10.1109/TSP.2017.2703684", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge when designing particle filters in high-dimensional state\nspaces is the construction of a proposal distribution that is close to the\nposterior distribution. Recent advances in particle flow filters provide a\npromising avenue to avoid weight degeneracy; particles drawn from the prior\ndistribution are migrated in the state-space to the posterior distribution by\nsolving partial differential equations. Numerous particle flow filters have\nbeen proposed based on different assumptions concerning the flow dynamics.\nApproximations are needed in the implementation of all of these filters; as a\nresult the articles do not exactly match a sample drawn from the desired\nposterior distribution. Past efforts to correct the discrepancies involve\nexpensive calculations of importance weights. In this paper, we present new\nfilters which incorporate deterministic particle flows into an encompassing\nparticle filter framework. The valuable theoretical guarantees concerning\nparticle filter performance still apply, but we can exploit the attractive\nperformance of the particle flow methods. The filters we describe involve a\ncomputationally efficient weight update step, arising because the embedded\nparticle flows we design possess an invertible mapping property. We evaluate\nthe proposed particle flow particle filters' performance through numerical\nsimulations of a challenging multi-target multi-sensor tracking scenario and\ncomplex high-dimensional filtering examples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 13:20:23 GMT"}, {"version": "v2", "created": "Sat, 13 Aug 2016 18:27:49 GMT"}, {"version": "v3", "created": "Fri, 14 Oct 2016 02:28:24 GMT"}, {"version": "v4", "created": "Thu, 9 Mar 2017 03:27:23 GMT"}, {"version": "v5", "created": "Wed, 28 Jun 2017 20:27:51 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Li", "Yunpeng", ""], ["Coates", "Mark", ""]]}, {"id": "1607.08882", "submitter": "Daniel Nevo", "authors": "Daniel Nevo, Reiko Nishihara, Shuji Ogino and Molin Wang", "title": "The competing risks Cox model with and without auxiliary case covariates\n  under weaker or no missing-at-random cause of failure", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the analysis of time-to-event data with multiple causes using a competing\nrisks Cox model, often the cause of failure is unknown for some of the cases.\nThe probability of a missing cause is typically assumed to be independent of\nthe cause given the time of the event and covariates measured before the event\noccurred. In practice, however, the underlying missing-at-random assumption\ndoes not necessarily hold. Motivated by colorectal cancer subtype analysis, we\ndevelop semiparametric methods to conduct valid analysis, first when additional\nauxiliary variables are available for cases only. We consider a weaker\nmissing-at-random assumption, with missing pattern depending on the observed\nquantities, which include the auxiliary covariates. Overlooking these\ncovariates will potentially result in biased estimates. We use an informative\nlikelihood approach that will yield consistent estimates even when the\nunderlying model for missing cause of failure is misspecified. We then consider\na method to conduct valid statistical analysis when there are no auxiliary\ncovariates in the not missing-at-random scenario. The superiority of our\nmethods in finite samples is demonstrated by simulation study results. We\nillustrate the use of our method in an analysis of colorectal cancer data from\nthe Nurses' Health Study cohort, where, apparently, the traditional\nmissing-at-random assumption fails to hold for particular molecular subtypes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 18:19:58 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Nevo", "Daniel", ""], ["Nishihara", "Reiko", ""], ["Ogino", "Shuji", ""], ["Wang", "Molin", ""]]}]