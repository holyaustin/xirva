[{"id": "1403.0048", "submitter": "Qiming Huang", "authors": "Qiming Huang and Yu Zhu", "title": "Model-Free Sure Screening via Maximum Correlation", "comments": "38 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of screening features in an ultrahigh-dimensional\nsetting. Using maximum correlation, we develop a novel procedure called MC-SIS\nfor feature screening, and show that MC-SIS possesses the sure screen property\nwithout imposing model or distributional assumptions on the response and\npredictor variables. Therefore, MC-SIS is a model-free sure independence\nscreening method as in contrast with some other existing model-based sure\nindependence screening methods in the literature. Simulation examples and a\nreal data application are used to demonstrate the performance of MC-SIS as well\nas to compare MC-SIS with other existing sure screening methods. The results\nshow that MC-SIS outperforms those methods when their model assumptions are\nviolated, and it remains competitive when the model assumptions hold.\n", "versions": [{"version": "v1", "created": "Sat, 1 Mar 2014 05:31:22 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2015 01:26:24 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Huang", "Qiming", ""], ["Zhu", "Yu", ""]]}, {"id": "1403.0201", "submitter": "Anirvan Chakraborty Mr.", "authors": "Anirvan Chakraborty and Probal Chaudhuri", "title": "A Wilcoxon-Mann-Whitney type test for infinite dimensional data", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "Report No.: R5/2013, Stat. Math. Unit", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wilcoxon-Mann-Whitney test is a robust competitor of the t-test in the\nunivariate setting. For finite dimensional multivariate data, several\nextensions of the Wilcoxon-Mann-Whitney test have been shown to have better\nperformance than Hotelling's $T^{2}$ test for many non-Gaussian distributions\nof the data. In this paper, we study a Wilcoxon-Mann-Whitney type test based on\nspatial ranks for data in infinite dimensional spaces. We demonstrate the\nperformance of this test using some real and simulated datasets. We also\ninvestigate the asymptotic properties of the proposed test and compare the test\nwith a wide range of competing tests.\n", "versions": [{"version": "v1", "created": "Sun, 2 Mar 2014 12:11:05 GMT"}], "update_date": "2014-03-04", "authors_parsed": [["Chakraborty", "Anirvan", ""], ["Chaudhuri", "Probal", ""]]}, {"id": "1403.0330", "submitter": "Abhijit Mandal", "authors": "Ayanendranath Basu, Abhijit Mandal, Nirian Martin, Leandro Pardo", "title": "Testing Composite Hypothesis based on the Density Power Divergence", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": "10.1007/s13571-017-0143-0", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In any parametric inference problem, the robustness of the procedure is a\nreal concern. A procedure which retains a high degree of efficiency under the\nmodel and simultaneously provides stable inference under data contamination is\npreferable in any practical situation over another procedure which achieves its\nefficiency at the cost of robustness or vice versa. The density power\ndivergence family of Basu et al. (1998) provides a flexible class of\ndivergences where the adjustment between efficiency and robustness is\ncontrolled by a single parameter $\\beta$. In this paper we consider general\ntests of parametric hypotheses based on the density power divergence. We\nestablish the asymptotic null distribution of the test statistic and explore\nits asymptotic power function. Numerical results illustrate the performance of\nthe theory developed.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 07:30:56 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 00:18:33 GMT"}, {"version": "v3", "created": "Sat, 26 Dec 2015 23:31:23 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Basu", "Ayanendranath", ""], ["Mandal", "Abhijit", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1403.0625", "submitter": "Weining Shen", "authors": "Weining Shen and Subhashis Ghosal", "title": "Adaptive Bayesian procedures using random series priors", "comments": "arXiv admin note: substantial text overlap with arXiv:1204.4238", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a prior for nonparametric Bayesian estimation which uses finite\nrandom series with a random number of terms. The prior is constructed through\ndistributions on the number of basis functions and the associated coefficients.\nWe derive a general result on adaptive posterior convergence rates for all\nsmoothness levels of the function in the true model by constructing an\nappropriate \"sieve\" and applying the general theory of posterior convergence\nrates. We apply this general result on several statistical problems such as\nsignal processing, density estimation, various nonparametric regressions,\nclassification, spectral density estimation, functional regression etc. The\nprior can be viewed as an alternative to the commonly used Gaussian process\nprior, but properties of the posterior distribution can be analyzed by\nrelatively simpler techniques and in many cases allows a simpler approach to\ncomputation without using Markov chain Monte-Carlo (MCMC) methods. A simulation\nstudy is conducted to show that the accuracy of the Bayesian estimators based\non the random series prior and the Gaussian process prior are comparable. We\napply the method on two interesting data sets on functional regression.\n", "versions": [{"version": "v1", "created": "Mon, 3 Mar 2014 22:53:41 GMT"}, {"version": "v2", "created": "Sat, 7 Feb 2015 06:21:26 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Shen", "Weining", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1403.0735", "submitter": "Isma\\\"{e}l Castillo", "authors": "Isma\\\"el Castillo, Johannes Schmidt-Hieber, Aad van der Vaart", "title": "Bayesian linear regression with sparse priors", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1334 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 5, 1986-2018", "doi": "10.1214/15-AOS1334", "report-no": "IMS-AOS-AOS1334", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study full Bayesian procedures for high-dimensional linear regression\nunder sparsity constraints. The prior is a mixture of point masses at zero and\ncontinuous distributions. Under compatibility conditions on the design matrix,\nthe posterior distribution is shown to contract at the optimal rate for\nrecovery of the unknown sparse vector, and to give optimal prediction of the\nresponse vector. It is also shown to select the correct sparse model, or at\nleast the coefficients that are significantly different from zero. The\nasymptotic shape of the posterior distribution is characterized and employed to\nthe construction and study of credible sets for uncertainty quantification.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 10:39:31 GMT"}, {"version": "v2", "created": "Sun, 29 Mar 2015 09:08:31 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 09:04:57 GMT"}], "update_date": "2015-10-15", "authors_parsed": [["Castillo", "Isma\u00ebl", ""], ["Schmidt-Hieber", "Johannes", ""], ["van der Vaart", "Aad", ""]]}, {"id": "1403.0848", "submitter": "Andreas Joseph Mr.", "authors": "Andreas Joseph, Irena Vodenska, Eugene Stanley, Guanrong Chen", "title": "Netconomics: Novel Forecasting Techniques from the Combination of Big\n  Data, Network Science and Economics", "comments": "18 pages, 8 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.GN physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of the network theoretic approach with recently available\nabundant economic data leads to the development of novel analytic and\ncomputational tools for modelling and forecasting key economic indicators. The\nmain idea is to introduce a topological component into the analysis, taking\ninto account consistently all higher-order interactions. We present three basic\nmethodologies to demonstrate different approaches to harness the resulting\nnetwork gain. First, a multiple linear regression optimisation algorithm is\nused to generate a relational network between individual components of national\nbalance of payment accounts. This model describes annual statistics with a high\naccuracy and delivers good forecasts for the majority of indicators. Second, an\nearly-warning mechanism for global financial crises is presented, which\ncombines network measures with standard economic indicators. From the analysis\nof the cross-border portfolio investment network of long-term debt securities,\nthe proliferation of a wide range of over-the-counter-traded financial\nderivative products, such as credit default swaps, can be described in terms of\ngross-market values and notional outstanding amounts, which are associated with\nincreased levels of market interdependence and systemic risk. Third,\nconsidering the flow-network of goods traded between G-20 economies, network\nstatistics provide better proxies for key economic measures than conventional\nindicators. For example, it is shown that a country's gate-keeping potential,\nas a measure for local power, projects its annual change of GDP generally far\nbetter than the volume of its imports or exports.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 16:40:39 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Joseph", "Andreas", ""], ["Vodenska", "Irena", ""], ["Stanley", "Eugene", ""], ["Chen", "Guanrong", ""]]}, {"id": "1403.0873", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J Kir\\'aly and Louis Theran", "title": "Matroid Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.DM cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algebraic combinatorial method for solving large sparse linear\nsystems of equations locally - that is, a method which can compute single\nevaluations of the signal without computing the whole signal. The method scales\nonly in the sparsity of the system and not in its size, and allows to provide\nerror estimates for any solution method. At the heart of our approach is the\nso-called regression matroid, a combinatorial object associated to sparsity\npatterns, which allows to replace inversion of the large matrix with the\ninversion of a kernel matrix that is constant size. We show that our method\nprovides the best linear unbiased estimator (BLUE) for this setting and the\nminimum variance unbiased estimator (MVUE) under Gaussian noise assumptions,\nand furthermore we show that the size of the kernel matrix which is to be\ninverted can be traded off with accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 17:54:37 GMT"}], "update_date": "2014-03-05", "authors_parsed": [["Kir\u00e1ly", "Franz J", ""], ["Theran", "Louis", ""]]}, {"id": "1403.0904", "submitter": "Wessel van Wieringen", "authors": "Wessel N. van Wieringen, Carel F.W. Peeters", "title": "Ridge Estimation of Inverse Covariance Matrices from High-Dimensional\n  Data", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, 103 (2016): 284-303", "doi": "10.1016/j.csda.2016.05.012", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study ridge estimation of the precision matrix in the high-dimensional\nsetting where the number of variables is large relative to the sample size. We\nfirst review two archetypal ridge estimators and note that their utilized\npenalties do not coincide with common ridge penalties. Subsequently, starting\nfrom a common ridge penalty, analytic expressions are derived for two\nalternative ridge estimators of the precision matrix. The alternative\nestimators are compared to the archetypes with regard to eigenvalue shrinkage\nand risk. The alternatives are also compared to the graphical lasso within the\ncontext of graphical modeling. The comparisons may give reason to prefer the\nproposed alternative estimators.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 19:09:14 GMT"}, {"version": "v2", "created": "Thu, 14 Aug 2014 08:01:31 GMT"}, {"version": "v3", "created": "Thu, 24 Sep 2015 07:29:23 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["van Wieringen", "Wessel N.", ""], ["Peeters", "Carel F. W.", ""]]}, {"id": "1403.0921", "submitter": "Kevin Xu", "authors": "Kevin S. Xu and Alfred O. Hero III", "title": "Dynamic stochastic blockmodels for time-evolving social networks", "comments": "To appear in Journal of Selected Topics in Signal Processing special\n  issue: Signal Processing for Social Networks", "journal-ref": "IEEE Journal of Selected Topics in Signal Processing 8 (2014)\n  552-562", "doi": "10.1109/JSTSP.2014.2310294", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant efforts have gone into the development of statistical models for\nanalyzing data in the form of networks, such as social networks. Most existing\nwork has focused on modeling static networks, which represent either a single\ntime snapshot or an aggregate view over time. There has been recent interest in\nstatistical modeling of dynamic networks, which are observed at multiple points\nin time and offer a richer representation of many complex phenomena. In this\npaper, we present a state-space model for dynamic networks that extends the\nwell-known stochastic blockmodel for static networks to the dynamic setting. We\nfit the model in a near-optimal manner using an extended Kalman filter (EKF)\naugmented with a local search. We demonstrate that the EKF-based algorithm\nperforms competitively with a state-of-the-art algorithm based on Markov chain\nMonte Carlo sampling but is significantly less computationally demanding.\n", "versions": [{"version": "v1", "created": "Tue, 4 Mar 2014 19:54:07 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Xu", "Kevin S.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1403.0998", "submitter": "Mingyu Tang", "authors": "Mingyu Tang and Mark Schervish", "title": "Hierarchical Semi-parametric Duration Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research attempts to model the stochastic process of trades in a limit\norder book market as a marked point process. We propose a semi-parametric model\nfor the conditional distribution given the past, attempting to capture the\neffect of the recent past in a nonparametric way and the effect of the more\ndistant past using a parametric time series model. Our framework provides more\nflexibility than the most commonly used family of models, known as\nAutoregressive Conditional Duration (ACD), in terms of the shape of the density\nof durations and in the form of dependence across time. We also propose an\nonline learning algorithm for intraday trends that vary from day to day. This\nallows us both to do prediction of future trade times and to incorporate the\neffects of additional explanatory variables. In this paper, we show that the\nframework works better than the ACD family both in the sense of prediction\nlog-likelihood and according to various diagnostic tests using data from the\nNew York Stock Exchange. In general, the framework can be used both to estimate\nthe intensity of a point process, and to estimate a the joint density of a time\nseries.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 03:57:26 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Tang", "Mingyu", ""], ["Schervish", "Mark", ""]]}, {"id": "1403.1102", "submitter": "Rajesh  Singh", "authors": "Hemant K. Verma, R. D. Singh and Rajesh Singh", "title": "Some Improved Estimators In Systematic Sampling Under Non-response", "comments": "13 pages, 1 table. arXiv admin note: text overlap with\n  arXiv:1306.6157", "journal-ref": "nat. acad. sci. let., 37(1)-91-95, (2014)", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we have considered the problem of estimating the population\nmean in systematic sampling using information on an auxiliary variable in\npresence of non response. Some modified ratio, product and difference type\nestimators in systematic sampling have been suggested and their properties are\nstudied. The expressions of mean squared errors (MSEs) up to the first order of\napproximation are derived. An empirical study is carried out to judge the best\nestimator out of the suggested estimators.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 12:54:49 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Verma", "Hemant K.", ""], ["Singh", "R. D.", ""], ["Singh", "Rajesh", ""]]}, {"id": "1403.1124", "submitter": "Juha Karvanen", "authors": "Juha Karvanen", "title": "Estimating complex causal effects from incomplete observational data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the major advances taken in causal modeling, causality is still an\nunfamiliar topic for many statisticians. In this paper, it is demonstrated from\nthe beginning to the end how causal effects can be estimated from observational\ndata assuming that the causal structure is known. To make the problem more\nchallenging, the causal effects are highly nonlinear and the data are missing\nat random. The tools used in the estimation include causal models with design,\ncausal calculus, multiple imputation and generalized additive models. The main\nmessage is that a trained statistician can estimate causal effects by\njudiciously combining existing tools.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 13:40:29 GMT"}, {"version": "v2", "created": "Wed, 2 Jul 2014 08:12:09 GMT"}], "update_date": "2014-07-03", "authors_parsed": [["Karvanen", "Juha", ""]]}, {"id": "1403.1158", "submitter": "Pavlo Mozharovskyi", "authors": "Karl Mosler and Pavlo Mozharovskyi", "title": "Fast DD-classification of functional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A fast nonparametric procedure for classifying functional data is introduced.\nIt consists of a two-step transformation of the original data plus a classifier\noperating on a low-dimensional hypercube. The functional data are first mapped\ninto a finite-dimensional location-slope space and then transformed by a\nmultivariate depth function into the $DD$-plot, which is a subset of the unit\nhypercube. This transformation yields a new notion of depth for functional\ndata. Three alternative depth functions are employed for this, as well as two\nrules for the final classification on $[0,1]^q$. The resulting classifier has\nto be cross-validated over a small range of parameters only, which is\nrestricted by a Vapnik-Cervonenkis bound. The entire methodology does not\ninvolve smoothing techniques, is completely nonparametric and allows to achieve\nBayes optimality under standard distributional settings. It is robust,\nefficiently computable, and has been implemented in an R environment.\nApplicability of the new approach is demonstrated by simulations as well as a\nbenchmark study.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 15:28:51 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 12:22:35 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2016 19:19:41 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["Mosler", "Karl", ""], ["Mozharovskyi", "Pavlo", ""]]}, {"id": "1403.1239", "submitter": "Elizabeth L. Ogburn", "authors": "Elizabeth L. Ogburn, Tyler J. VanderWeele", "title": "Causal Diagrams for Interference", "comments": "Published in at http://dx.doi.org/10.1214/14-STS501 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 559-578", "doi": "10.1214/14-STS501", "report-no": "IMS-STS-STS501", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term \"interference\" has been used to describe any setting in which one\nsubject's exposure may affect another subject's outcome. We use causal diagrams\nto distinguish among three causal mechanisms that give rise to interference.\nThe first causal mechanism by which interference can operate is a direct causal\neffect of one individual's treatment on another individual's outcome; we call\nthis direct interference. Interference by contagion is present when one\nindividual's outcome may affect the outcomes of other individuals with whom he\ncomes into contact. Then giving treatment to the first individual could have an\nindirect effect on others through the treated individual's outcome. The third\npathway by which interference may operate is allocational interference.\nTreatment in this case allocates individuals to groups; through interactions\nwithin a group, individuals may affect one another's outcomes in any number of\nways. In many settings, more than one type of interference will be present\nsimultaneously. The causal effects of interest differ according to which types\nof interference are present, as do the conditions under which causal effects\nare identifiable. Using causal diagrams for interference, we describe these\ndifferences, give criteria for the identification of important causal effects,\nand discuss applications to infectious diseases.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 20:00:39 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 17:52:12 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 08:11:59 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Ogburn", "Elizabeth L.", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "1403.1241", "submitter": "Elizabeth Ogburn", "authors": "Elizabeth L. Ogburn, Tyler J. VanderWeele", "title": "Vaccines, Contagion, and Social Networks", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the causal effect that one individual's treatment may have on\nanother individual's outcome when the outcome is contagious, with specific\napplication to the effect of vaccination on an infectious disease outcome. The\neffect of one individual's vaccination on another's outcome can be decomposed\ninto two different causal effects, called the \"infectiousness\" and \"contagion\"\neffects. We present identifying assumptions and estimation or testing\nprocedures for infectiousness and contagion effects in two different settings:\n(1) using data sampled from independent groups of observations, and (2) using\ndata collected from a single interdependent social network. The methods that we\npropose for social network data require fitting generalized linear models\n(GLMs). GLMs and other statistical models that require independence across\nsubjects have been used widely to estimate causal effects in social network\ndata, but, because the subjects in networks are presumably not independent, the\nuse of such models is generally invalid, resulting in inference that is\nexpected to be anticonservative. We introduce a way to ensure that GLM\nresiduals are uncorrelated across subjects despite the fact that outcomes are\nnon-independent. This simultaneously demonstrates the possibility of using GLMs\nand related statistical models for network data and highlights their\nlimitations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 20:09:32 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Ogburn", "Elizabeth L.", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "1403.1249", "submitter": "Antonino Abbruzzo AA", "authors": "Antonino Abbruzzo, Ivan Vuja\\v{c}i\\'c, Ernst Wit, Angelo M. Mineo", "title": "Generalized information criterion for model selection in penalized\n  graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an estimator of the relative directed distance between\nan estimated model and the true model, based on the Kulback-Leibler divergence\nand is motivated by the generalized information criterion proposed by Konishi\nand Kitagawa. This estimator can be used to select model in penalized Gaussian\ncopula graphical models. The use of this estimator is not feasible for\nhigh-dimensional cases. However, we derive an efficient way to compute this\nestimator which is feasible for the latter class of problems. Moreover, this\nestimator is, generally, appropriate for several penalties such as lasso,\nadaptive lasso and smoothly clipped absolute deviation penalty. Simulations\nshow that the method performs similarly to KL oracle estimator and it also\nimproves BIC performance in terms of support recovery of the graph.\nSpecifically, we compare our method with Akaike information criterion, Bayesian\ninformation criterion and cross validation for band, sparse and dense network\nstructures.\n", "versions": [{"version": "v1", "created": "Wed, 5 Mar 2014 20:53:47 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Abbruzzo", "Antonino", ""], ["Vuja\u010di\u0107", "Ivan", ""], ["Wit", "Ernst", ""], ["Mineo", "Angelo M.", ""]]}, {"id": "1403.1345", "submitter": "Yun Yang", "authors": "Yun Yang and David B. Dunson", "title": "Minimax Optimal Bayesian Aggregation", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is generally believed that ensemble approaches, which combine multiple\nalgorithms or models, can outperform any single algorithm at machine learning\ntasks, such as prediction. In this paper, we propose Bayesian convex and linear\naggregation approaches motivated by regression applications. We show that the\nproposed approach is minimax optimal when the true data-generating model is a\nconvex or linear combination of models in the list. Moreover, the method can\nadapt to sparsity structure in which certain models should receive zero\nweights, and the method is tuning parameter free unlike competitors. More\ngenerally, under an M-open view when the truth falls outside the space of all\nconvex/linear combinations, our theory suggests that the posterior measure\ntends to concentrate on the best approximation of the truth at the minimax\nrate. We illustrate the method through simulation studies and several\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 04:57:38 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Yang", "Yun", ""], ["Dunson", "David B.", ""]]}, {"id": "1403.1389", "submitter": "Stephan Huckemann", "authors": "Alexander Hartmann, Stephan Huckemann, J\\\"orn Dannemann, Oskar\n  Laitenberger, Claudia Geisler, Alexander Egner and Axel Munk", "title": "Drift Estimation in Sparse Sequential Dynamic Imaging: with Application\n  to Nanoscale Fluorescence Microscopy", "comments": "43 pages 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in many modern superresolution fluorescence microscopy\ntechniques at the nanoscale lies in the correct alignment of long sequences of\nsparse but spatially and temporally highly resolved images. This is caused by\nthe temporal drift of the protein structure, e.g. due to temporal thermal\ninhomogeneity of the object of interest or its supporting area during the\nobservation process. We develop a simple semiparametric model for drift\ncorrection in SMS microscopy. Then we propose an M-estimator for the drift and\nshow its asymptotic normality. This is used to correct the final image and it\nis shown that this purely statistical method is competitive with state of the\nart calibration techniques which require to incorporate fiducial markers into\nthe specimen. Moreover, a simple bootstrap algorithm allows to quantify the\nprecision of the drift estimate and its effect on the final image estimation.\nWe argue that purely statistical drift correction is even more robust than\nfiducial tracking rendering the latter superfluous in many applications. The\npracticability of our method is demonstrated by a simulation study and by an\nSMS application. This serves as a prototype for many other typical imaging\ntechniques where sparse observations with highly temporal resolution are\nblurred by motion of the object to be reconstructed.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 09:53:52 GMT"}, {"version": "v2", "created": "Sun, 21 Dec 2014 22:07:01 GMT"}], "update_date": "2014-12-23", "authors_parsed": [["Hartmann", "Alexander", ""], ["Huckemann", "Stephan", ""], ["Dannemann", "J\u00f6rn", ""], ["Laitenberger", "Oskar", ""], ["Geisler", "Claudia", ""], ["Egner", "Alexander", ""], ["Munk", "Axel", ""]]}, {"id": "1403.1395", "submitter": "Abhijit Mandal", "authors": "Ayanendranath Basu, Abhijit Mandal, Nirian Martin, Leandro Pardo", "title": "Robust Tests for the Equality of Two Normal Means based on the Density\n  Power Divergence", "comments": "20 pages, 9 figures", "journal-ref": "Metrika, 2015, 78 (5), 611-634", "doi": "10.1007/s00184-014-0518-4", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical techniques are used in all branches of science to determine the\nfeasibility of quantitative hypotheses. One of the most basic applications of\nstatistical techniques in comparative analysis is the test of equality of two\npopulation means, generally performed under the assumption of normality. In\nmedical studies, for example, we often need to compare the effects of two\ndifferent drugs, treatments or preconditions on the resulting outcome. The most\ncommonly used test in this connection is the two sample $t$-test for the\nequality of means, performed under the assumption of equality of variances. It\nis a very useful tool, which is widely used by practitioners of all disciplines\nand has many optimality properties under the model. However, the test has one\nmajor drawback; it is highly sensitive to deviations from the ideal conditions,\nand may perform miserably under model misspecification and the presence of\noutliers. In this paper we present a robust test for the two sample hypothesis\nbased on the density power divergence measure (Basu et al., 1998), and show\nthat it can be a great alternative to the ordinary two sample $t$-test. The\nasymptotic properties of the proposed tests are rigorously established in the\npaper, and their performances are explored through simulations and real data\nanalysis.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 10:29:06 GMT"}, {"version": "v2", "created": "Tue, 25 Nov 2014 21:14:47 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Basu", "Ayanendranath", ""], ["Mandal", "Abhijit", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1403.1452", "submitter": "Andreas Mayr", "authors": "Andreas Mayr, Harald Binder, Olaf Gefeller and Matthias Schmid", "title": "The Evolution of Boosting Algorithms - From Machine Learning to\n  Statistical Modelling", "comments": null, "journal-ref": "Methods Inf Med 2014; 53(6): 419-427", "doi": "10.3414/ME13-01-0122", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of boosting emerged from the field of machine learning. The basic\nidea is to boost the accuracy of a weak classifying tool by combining various\ninstances into a more accurate prediction. This general concept was later\nadapted to the field of statistical modelling. This review article attempts to\nhighlight this evolution of boosting algorithms from machine learning to\nstatistical modelling. We describe the AdaBoost algorithm for classification as\nwell as the two most prominent statistical boosting approaches, gradient\nboosting and likelihood-based boosting. Although both appraoches are typically\ntreated separately in the literature, they share the same methodological roots\nand follow the same fundamental concepts. Compared to the initial machine\nlearning algorithms, which must be seen as black-box prediction schemes,\nstatistical boosting result in statistical models which offer a\nstraight-forward interpretation. We highlight the methodological background and\npresent the most common software implementations. Worked out examples and\ncorresponding R code can be found in the Appendix.\n", "versions": [{"version": "v1", "created": "Thu, 6 Mar 2014 14:24:21 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 14:15:43 GMT"}, {"version": "v3", "created": "Tue, 18 Nov 2014 18:00:14 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Mayr", "Andreas", ""], ["Binder", "Harald", ""], ["Gefeller", "Olaf", ""], ["Schmid", "Matthias", ""]]}, {"id": "1403.1638", "submitter": "Linglong Kong", "authors": "Linglong Kong and Douglas P. Wiens", "title": "Model-Robust Designs for Quantile Regression", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2014.969427", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We give methods for the construction of designs for linear models, when the\npurpose of the investigation is the estimation of the conditional quantile\nfunction and the estimation method is quantile regression. The designs are\nrobust against misspecified response functions, and against unanticipated\nheteroscedasticity. The methods are illustrated by example, and in a case study\nin which they are applied to growth charts.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 02:53:10 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 22:11:20 GMT"}], "update_date": "2014-12-01", "authors_parsed": [["Kong", "Linglong", ""], ["Wiens", "Douglas P.", ""]]}, {"id": "1403.1680", "submitter": "Debasish Roy", "authors": "Saikat Sarkar, Debasish Roy and Ram Mohan Vasu", "title": "Evolutionary global optimization posed as a randomly perturbed\n  martingale problem and applied to parameter recovery of chaotic oscillators", "comments": "35 pages, 2 figures; being submitted to Physica D", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new global stochastic search, guided mainly through derivative-free\ndirectional information computable from the sample statistical moments of the\ndesign variables within a Monte Carlo setup, is proposed. The search is aided\nby imparting to a directional update term, which parallels the conventional\nGateaux derivative used in a local search for the extrema of smooth cost\nfunctionals, additional layers of random perturbations referred to as\n'coalescence' and 'scrambling'. A selection scheme, constituting yet another\navenue for random perturbation, completes the global search. The\ndirection-driven nature of the search is manifest in the local extremization\nand coalescence components, which are posed as martingale problems that yield\ngain-like update terms upon discretization. As anticipated and numerically\ndemonstrated, to a limited extent, against the problem of parameter recovery\ngiven the chaotic response histories of a couple of nonlinear oscillators, the\nproposed method apparently provides for a more rational, more accurate and\nfaster alternative to most available evolutionary schemes, prominently the\nparticle swarm optimization.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 08:25:38 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Sarkar", "Saikat", ""], ["Roy", "Debasish", ""], ["Vasu", "Ram Mohan", ""]]}, {"id": "1403.1692", "submitter": "Andreas Mayr", "authors": "Andreas Mayr, Harald Binder, Olaf Gefeller and Matthias Schmid", "title": "Extending Statistical Boosting - An Overview of Recent Methodological\n  Developments", "comments": null, "journal-ref": "Methods Inf Med 2014; 53(6): 428-435", "doi": "10.3414/ME13-01-0123", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Boosting algorithms to simultaneously estimate and select predictor effects\nin statistical models have gained substantial interest during the last decade.\nThis review article aims to highlight recent methodological developments\nregarding boosting algorithms for statistical modelling especially focusing on\ntopics relevant for biomedical research. We suggest a unified framework for\ngradient boosting and likelihood-based boosting (statistical boosting) which\nhave been addressed strictly separated in the literature up to now. Statistical\nboosting algorithms have been adapted to carry out unbiased variable selection\nand automated model choice during the fitting process and can nowadays be\napplied in almost any possible type of regression setting in combination with a\nlarge amount of different types of predictor effects. The methodological\ndevelopments on statistical boosting during the last ten years can be grouped\ninto three different lines of research: (i) efforts to ensure variable\nselection leading to sparser models, (ii) developments regarding different\ntypes of predictor effects and their selection (model choice), (iii) approaches\nto extend the statistical boosting framework to new regression settings.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 09:14:20 GMT"}, {"version": "v2", "created": "Mon, 28 Apr 2014 14:16:17 GMT"}, {"version": "v3", "created": "Tue, 18 Nov 2014 18:03:26 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Mayr", "Andreas", ""], ["Binder", "Harald", ""], ["Gefeller", "Olaf", ""], ["Schmid", "Matthias", ""]]}, {"id": "1403.1806", "submitter": "Gianluca Baio", "authors": "Sara Geneletti, Aidan G. O'Keeffe, Linda D. Sharples, Sylvia\n  Richardson, Gianluca Baio", "title": "Bayesian regression discontinuity designs: Incorporating clinical\n  knowledge in the causal analysis of primary care data", "comments": "21 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The regression discontinuity (RD) design is a quasi-experimental design that\nestimates the causal effects of a treatment by exploiting naturally occurring\ntreatment rules. It can be applied in any context where a particular treatment\nor intervention is administered according to a pre-specified rule linked to a\ncontinuous variable. Such thresholds are common in primary care drug\nprescription where the RD design can be used to estimate the causal effect of\nmedication in the general population. Such results can then be contrasted to\nthose obtained from randomised controlled trials (RCTs) and inform prescription\npolicy and guidelines based on a more realistic and less expensive context. In\nthis paper we focus on statins, a class of cholesterol-lowering drugs, however,\nthe methodology can be applied to many other drugs provided these are\nprescribed in accordance to pre-determined guidelines. NHS guidelines state\nthat statins should be prescribed to patients with 10 year cardiovascular\ndisease risk scores in excess of 20%. If we consider patients whose scores are\nclose to this threshold we find that there is an element of random variation in\nboth the risk score itself and its measurement. We can thus consider the\nthreshold a randomising device assigning the prescription to units just above\nthe threshold and withholds it from those just below. Thus we are effectively\nreplicating the conditions of an RCT in the area around the threshold, removing\nor at least mitigating confounding. We frame the RD design in the language of\nconditional independence which clarifies the assumptions necessary to apply it\nto data, and which makes the links with instrumental variables clear. We also\nhave context specific knowledge about the expected sizes of the effects of\nstatin prescription and are thus able to incorporate this into Bayesian models\nby formulating informative priors on our causal parameters.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 16:56:35 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Geneletti", "Sara", ""], ["O'Keeffe", "Aidan G.", ""], ["Sharples", "Linda D.", ""], ["Richardson", "Sylvia", ""], ["Baio", "Gianluca", ""]]}, {"id": "1403.1830", "submitter": "Fuqi Chen", "authors": "Fuqi Chen and S\\'ev\\'erien Nkurunziza", "title": "A short note on model selection by LASSO methods in a change-point model", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In Ciuperca (2012) (Ciuperca. Model selection by LASSO methods in a\nchange-point model, Stat. Papers, 2012; (in press)), the author considered a\nlinear regression model with multiple change-points occurring at unknown times.\nIn particular, the author studied the asymptotic properties of the LASSO-type\nand of the adaptive LASSO estimators. While the established results seem\ninteresting, we point out some major errors in proof of the most important\nresult of the quoted paper. Further, we present a corrected result and proof.\n", "versions": [{"version": "v1", "created": "Fri, 7 Mar 2014 18:15:31 GMT"}], "update_date": "2014-03-10", "authors_parsed": [["Chen", "Fuqi", ""], ["Nkurunziza", "S\u00e9v\u00e9rien", ""]]}, {"id": "1403.1913", "submitter": "Han Lin Shang", "authors": "Han Lin Shang", "title": "Bayesian bandwidth estimation for a nonparametric functional regression\n  model with mixed types of regressors and unknown error density", "comments": null, "journal-ref": "Journal of Nonparametric Statistics, 2014, 26(3), 599-615", "doi": "10.1080/10485252.2014.916806", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the issue of bandwidth estimation in a nonparametric\nfunctional regression model with function-valued, continuous real-valued and\ndiscrete-valued regressors under the framework of unknown error density.\nExtending from the recent work of Shang (2013, Computational Statistics & Data\nAnalysis), we approximate the unknown error density by a kernel density\nestimator of residuals, where the regression function is estimated by the\nfunctional Nadaraya-Watson estimator that admits mixed types of regressors. We\nderive a kernel likelihood and posterior density for the bandwidth parameters\nunder the kernel-form error density, and put forward a Bayesian bandwidth\nestimation approach that can simultaneously estimate the bandwidths. Simulation\nstudies demonstrated the estimation accuracy of the regression function and\nerror density for the proposed Bayesian approach. Illustrated by a spectroscopy\ndata set in the food quality control, we applied the proposed Bayesian approach\nto select the optimal bandwidths in a nonparametric functional regression model\nwith mixed types of regressors.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 01:30:41 GMT"}], "update_date": "2016-06-20", "authors_parsed": [["Shang", "Han Lin", ""]]}, {"id": "1403.1922", "submitter": "Tingni Sun", "authors": "Zhuang Ma, Zongming Ma and Tingni Sun", "title": "Adaptive Estimation in Two-way Sparse Reduced-rank Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of estimating a large coefficient matrix in a\nmultiple response linear regression model when the coefficient matrix could be\nboth of low rank and sparse in the sense that most nonzero entries concentrate\non a few rows and columns. We are especially interested in the high dimensional\nsettings where the number of predictors and/or response variables can be much\nlarger than the number of observations. We propose a new estimation scheme,\nwhich achieves competitive numerical performance and at the same time allows\nfast computation. Moreover, we show that (a slight variant of) the proposed\nestimator achieves near optimal non-asymptotic minimax rates of estimation\nunder a collection of squared Schatten norm losses simultaneously by providing\nboth the error bounds for the estimator and minimax lower bounds. The\neffectiveness of the proposed algorithm is also demonstrated on an \\textit{in\nvivo} calcium imaging dataset.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 02:45:17 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 21:28:56 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Ma", "Zhuang", ""], ["Ma", "Zongming", ""], ["Sun", "Tingni", ""]]}, {"id": "1403.1958", "submitter": "Souhil Chakar", "authors": "Souhil Chakar, \\'Emilie Lebarbier, C\\'eline L\\'evy-Leduc, St\\'ephane\n  Robin", "title": "A robust approach for estimating change-points in the mean of an AR(1)\n  process", "comments": "42 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of multiple change-point estimation in the mean of a\nGaussian AR(1) process. Taking into account the dependence structure does not\nallow us to use the dynamic programming algorithm, which is the only algorithm\ngiving the optimal solution in the independent case. We propose a robust\nestimator of the autocorrelation parameter, which is consistent and satisfies a\ncentral limit theorem. Then, we propose to follow the classical inference\napproach, by plugging this estimator in the criteria used for change-points\nestimation. We show that the asymptotic properties of these estimators are the\nsame as those of the classical estimators in the independent framework. The\nsame plug-in approach is then used to approximate the modified BIC and choose\nthe number of segments. This method is implemented in the R package AR1seg and\nis available from the Comprehensive R Archive Network (CRAN). This package is\nused in the simulation section in which we show that for finite sample sizes\ntaking into account the dependence structure improves the statistical\nperformance of the change-point estimators and of the selection criterion.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 10:14:29 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 20:22:00 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Chakar", "Souhil", ""], ["Lebarbier", "\u00c9milie", ""], ["L\u00e9vy-Leduc", "C\u00e9line", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1403.1975", "submitter": "Anna Kiriliouk", "authors": "John Einmahl and Anna Kiriliouk and Andrea Krajina and Johan Segers", "title": "An M-estimator of spatial tail dependence", "comments": "25 pages; major revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tail dependence models for distributions attracted to a max-stable law are\nfitted using observations above a high threshold. To cope with spatial,\nhigh-dimensional data, a rank-based M-estimator is proposed relying on\nbivariate margins only. A data-driven weight matrix is used to minimize the\nasymptotic variance. Empirical process arguments show that the estimator is\nconsistent and asymptotically normal. Its finite-sample performance is assessed\nin simulation experiments involving popular max-stable processes perturbed with\nadditive noise. An analysis of wind speed data from the Netherlands illustrates\nthe method.\n", "versions": [{"version": "v1", "created": "Sat, 8 Mar 2014 15:14:18 GMT"}, {"version": "v2", "created": "Fri, 9 Jan 2015 10:40:22 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Einmahl", "John", ""], ["Kiriliouk", "Anna", ""], ["Krajina", "Andrea", ""], ["Segers", "Johan", ""]]}, {"id": "1403.2272", "submitter": "Daniele Durante", "authors": "Daniele Durante and David B. Dunson", "title": "Bayesian dynamic financial networks with time-varying predictors", "comments": null, "journal-ref": "Statistics & Probability Letters (2014). 93, 19-26", "doi": "10.1016/j.spl.2014.06.015", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian nonparametric model including time-varying predictors\nin dynamic network inference. The model is applied to infer the dependence\nstructure among financial markets during the global financial crisis,\nestimating effects of verbal and material cooperation efforts. We interestingly\nlearn contagion effects, with increasing influence of verbal relations during\nthe financial crisis and opposite results during the United States housing\nbubble.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 16:17:24 GMT"}], "update_date": "2014-07-08", "authors_parsed": [["Durante", "Daniele", ""], ["Dunson", "David B.", ""]]}, {"id": "1403.2285", "submitter": "Brian Franczak C.", "authors": "Brian C. Franczak, Cristina Tortora, Ryan P. Browne and Paul D.\n  McNicholas", "title": "Unsupervised Learning via Mixtures of Skewed Distributions with\n  Hypercube Contours", "comments": null, "journal-ref": null, "doi": "10.1016/j.patrec.2015.02.011", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models whose components have skewed hypercube contours are developed\nvia a generalization of the multivariate shifted asymmetric Laplace density.\nSpecifically, we develop mixtures of multiple scaled shifted asymmetric Laplace\ndistributions. The component densities have two unique features: they include a\nmultivariate weight function, and the marginal distributions are also\nasymmetric Laplace. We use these mixtures of multiple scaled shifted asymmetric\nLaplace distributions for clustering applications, but they could equally well\nbe used in the supervised or semi-supervised paradigms. The\nexpectation-maximization algorithm is used for parameter estimation and the\nBayesian information criterion is used for model selection. Simulated and real\ndata sets are used to illustrate the approach and, in some cases, to visualize\nthe skewed hypercube structure of the components.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 16:11:23 GMT"}, {"version": "v2", "created": "Thu, 27 Mar 2014 17:33:31 GMT"}, {"version": "v3", "created": "Wed, 30 Apr 2014 16:14:51 GMT"}, {"version": "v4", "created": "Fri, 16 May 2014 16:46:01 GMT"}, {"version": "v5", "created": "Wed, 17 Sep 2014 19:27:06 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Franczak", "Brian C.", ""], ["Tortora", "Cristina", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1403.2310", "submitter": "Qing Zhou", "authors": "Jiaying Gu, Fei Fu, and Qing Zhou", "title": "Penalized Estimation of Directed Acyclic Graphs From Discrete Data", "comments": "To appear in Statistics and Computing", "journal-ref": null, "doi": "10.1007/s11222-018-9801-y", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks, with structure given by a directed acyclic graph (DAG),\nare a popular class of graphical models. However, learning Bayesian networks\nfrom discrete or categorical data is particularly challenging, due to the large\nparameter space and the difficulty in searching for a sparse structure. In this\narticle, we develop a maximum penalized likelihood method to tackle this\nproblem. Instead of the commonly used multinomial distribution, we model the\nconditional distribution of a node given its parents by multi-logit regression,\nin which an edge is parameterized by a set of coefficient vectors with dummy\nvariables encoding the levels of a node. To obtain a sparse DAG, a group norm\npenalty is employed, and a blockwise coordinate descent algorithm is developed\nto maximize the penalized likelihood subject to the acyclicity constraint of a\nDAG. When interventional data are available, our method constructs a causal\nnetwork, in which a directed edge represents a causal relation. We apply our\nmethod to various simulated and real data sets. The results show that our\nmethod is very competitive, compared to many existing methods, in DAG\nestimation from both interventional and high-dimensional observational data.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 17:26:40 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 16:49:21 GMT"}, {"version": "v3", "created": "Wed, 4 May 2016 18:40:58 GMT"}, {"version": "v4", "created": "Fri, 2 Feb 2018 19:49:44 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Gu", "Jiaying", ""], ["Fu", "Fei", ""], ["Zhou", "Qing", ""]]}, {"id": "1403.2332", "submitter": "Paul McNicholas", "authors": "Cristina Tortora, Brian C. Franczak, Ryan P. Browne and Paul D.\n  McNicholas", "title": "A Mixture of Coalesced Generalized Hyperbolic Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixture of multiple scaled generalized hyperbolic distributions (MMSGHDs)\nis introduced. Then, a coalesced generalized hyperbolic distribution (CGHD) is\ndeveloped by joining a generalized hyperbolic distribution with a multiple\nscaled generalized hyperbolic distribution. After detailing the development of\nthe MMSGHDs, which arises via implementation of a multi-dimensional weight\nfunction, the density of the mixture of CGHDs is developed. A parameter\nestimation scheme is developed using the ever-expanding class of MM algorithms\nand the Bayesian information criterion is used for model selection. The issue\nof cluster convexity is examined and a special case of the MMSGHDs is developed\nthat is guaranteed to have convex clusters. These approaches are illustrated\nand compared using simulated and real data. The identifiability of the MMSGHDs\nand the mixture of CGHDs is discussed in an appendix.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 18:25:17 GMT"}, {"version": "v2", "created": "Fri, 28 Mar 2014 01:27:52 GMT"}, {"version": "v3", "created": "Thu, 22 May 2014 16:14:17 GMT"}, {"version": "v4", "created": "Thu, 11 Dec 2014 22:11:49 GMT"}, {"version": "v5", "created": "Mon, 15 Dec 2014 14:46:10 GMT"}, {"version": "v6", "created": "Thu, 4 Jun 2015 22:17:14 GMT"}, {"version": "v7", "created": "Mon, 6 Nov 2017 19:53:30 GMT"}, {"version": "v8", "created": "Sat, 27 Oct 2018 19:36:13 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Tortora", "Cristina", ""], ["Franczak", "Brian C.", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1403.2363", "submitter": "Ottmar Cronie", "authors": "Ottmar Cronie and Jorge Mateu", "title": "Spatio-temporal c\\`adl\\`ag functional marked point processes: Unifying\n  spatio-temporal frameworks", "comments": "47 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper defines the class of c\\`adl\\`ag functional marked point processes\n(CFMPPs). These are (spatio-temporal) point processes marked by random elements\nwhich take values in a c\\`adl\\`ag function space, i.e. the marks are given by\nc\\`adl\\`ag stochastic processes. We generalise notions of marked\n(spatio-temporal) point processes and indicate how this class, in a sensible\nway, connects the point process framework with the random fields framework. We\nalso show how they can be used to construct a class of spatio-temporal Boolean\nmodels, how to construct different classes of these models by choosing specific\nmark functions, and how c\\`adl\\`ag functional marked Cox processes have a\ndouble connection to random fields. We also discuss finite CFMPPs, purely\ntemporally well-defined CFMPPs and Markov CFMPPs. Furthermore, we define\ncharacteristics such as product densities, Palm distributions and conditional\nintensities, in order to develop statistical inference tools such as likelihood\nestimation schemes.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 19:45:51 GMT"}], "update_date": "2014-03-11", "authors_parsed": [["Cronie", "Ottmar", ""], ["Mateu", "Jorge", ""]]}, {"id": "1403.2397", "submitter": "Li Ma", "authors": "Li Ma", "title": "Scalable Bayesian model averaging through local information propagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that a probabilistic version of the classical forward-stepwise\nvariable inclusion procedure can serve as a general data-augmentation scheme\nfor model space distributions in (generalized) linear models. This latent\nvariable representation takes the form of a Markov process, thereby allowing\ninformation propagation algorithms to be applied for sampling from model space\nposteriors. In particular, we propose a sequential Monte Carlo method for\nachieving effective unbiased Bayesian model averaging in high-dimensional\nproblems, utilizing proposal distributions constructed using local information\npropagation. We illustrate our method---called LIPS for local information\npropagation based sampling---through real and simulated examples with\ndimensionality ranging from 15 to 1,000, and compare its performance in\nestimating posterior inclusion probabilities and in out-of-sample prediction to\nthose of several other methods---namely, MCMC, BAS, iBMA, and LASSO. In\naddition, we show that the latent variable representation can also serve as a\nmodeling tool for specifying model space priors that account for knowledge\nregarding model complexity and conditional inclusion relationships.\n", "versions": [{"version": "v1", "created": "Mon, 10 Mar 2014 20:11:01 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 00:14:46 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Ma", "Li", ""]]}, {"id": "1403.2695", "submitter": "Weining Shen", "authors": "Weining Shen, Subhashis Ghosal", "title": "Adaptive Bayesian density regression for high-dimensional data", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ663 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 1, 396-420", "doi": "10.3150/14-BEJ663", "report-no": "IMS-BEJ-BEJ663", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Density regression provides a flexible strategy for modeling the distribution\nof a response variable $Y$ given predictors $\\mathbf{X}=(X_1,\\ldots,X_p)$ by\nletting that the conditional density of $Y$ given $\\mathbf{X}$ as a completely\nunknown function and allowing its shape to change with the value of\n$\\mathbf{X}$. The number of predictors $p$ may be very large, possibly much\nlarger than the number of observations $n$, but the conditional density is\nassumed to depend only on a much smaller number of predictors, which are\nunknown. In addition to estimation, the goal is also to select the important\npredictors which actually affect the true conditional density. We consider a\nnonparametric Bayesian approach to density regression by constructing a random\nseries prior based on tensor products of spline functions. The proposed prior\nalso incorporates the issue of variable selection. We show that the posterior\ndistribution of the conditional density contracts adaptively at the truth\nnearly at the optimal oracle rate, determined by the unknown sparsity and\nsmoothness levels, even in the ultra high-dimensional settings where $p$\nincreases exponentially with $n$. The result is also extended to the\nanisotropic case where the degree of smoothness can vary in different\ndirections, and both random and deterministic predictors are considered. We\nalso propose a technique to calculate posterior moments of the conditional\ndensity function without requiring Markov chain Monte Carlo methods.\n", "versions": [{"version": "v1", "created": "Tue, 11 Mar 2014 19:02:49 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 12:54:38 GMT"}], "update_date": "2016-01-07", "authors_parsed": [["Shen", "Weining", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1403.2940", "submitter": "Robert B. Gramacy", "authors": "Timothy Graves, Robert B. Gramacy, Christian Franzke, Nicholas Watkins", "title": "Efficient Bayesian inference for long memory processes", "comments": "33 pages, 12 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In forecasting problems it is important to know whether or not recent events\nrepresent a regime change (low long-term predictive potential), or rather a\nlocal manifestation of longer term effects (potentially higher predictive\npotential). Mathematically, a key question is about whether the underlying\nstochastic process exhibits \"memory\", and if so whether the memory is \"long\" in\na precise sense. Being able to detect or rule out such effects can have a\nprofound impact on speculative investment (e.g., in financial markets) and\ninform public policy (e.g., characterising the size and timescales of the earth\nsystem's response to the anthropogenic CO2 perturbation). Most previous work on\ninference of long memory effects is frequentist in nature. Here we provide a\nsystematic treatment of Bayesian inference for long memory processes via the\nAutoregressive Fractional Integrated Moving Average (ARFIMA) model. In\nparticular, we provide a new approximate likelihood for efficient parameter\ninference, and show how nuisance parameters (e.g., short memory effects) can be\nintegrated over in order to focus on long memory parameters and hypothesis\ntesting more directly than ever before. We illustrate our new methodology on\nboth synthetic and observational data, with favorable comparison to the\nstandard estimators.\n", "versions": [{"version": "v1", "created": "Wed, 12 Mar 2014 14:14:21 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 01:31:46 GMT"}], "update_date": "2014-07-09", "authors_parsed": [["Graves", "Timothy", ""], ["Gramacy", "Robert B.", ""], ["Franzke", "Christian", ""], ["Watkins", "Nicholas", ""]]}, {"id": "1403.3231", "submitter": "Dimitris Kugiumtzis", "authors": "Dimitris Kugiumtzis, Efthimia Bora-Senta", "title": "Simulation of Multivariate Non-Gaussian Autoregressive Time Series with\n  Given Autocovariance and Marginals", "comments": "21 pages, 6 figures, accepted in Simulation Modelling Practice and\n  Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A semi-analytic method is proposed for the generation of realizations of a\nmultivariate process of a given linear correlation structure and marginal\ndistribution. This is an extension of a similar method for univariate\nprocesses, transforming the autocorrelation of the non-Gaussian process to that\nof a Gaussian process based on a piece-wise linear marginal transform from\nnon-Gaussian to Gaussian marginal. The extension to multivariate processes\ninvolves the derivation of the autocorrelation matrix from the marginal\ntransforms, which determines the generating vector autoregressive process. The\neffectiveness of the approach is demonstrated on systems designed under\ndifferent scenarios of autocovariance and marginals.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 10:59:34 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Kugiumtzis", "Dimitris", ""], ["Bora-Senta", "Efthimia", ""]]}, {"id": "1403.3359", "submitter": "Menelaos Karanasos", "authors": "Menelaos Karanasos, Alexandros Paraskevopoulos and Stavros Dafnos", "title": "The fundamental properties of time varying AR models with non stochastic\n  coefficients", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper examines the problem of representing the dynamics of low order\nautoregressive (AR) models with time varying (TV) coefficients. The existing\nliterature computes the forecasts of the series from a recursion relation.\nInstead, we provide the linearly independent solutions to TV-AR models. Our\nsolution formulas enable us to derive the fundamental properties of these\nprocesses, and obtain explicit expressions for the optimal predictors. We\nillustrate our methodology and results with a few classic examples amenable to\ntime varying treatment, e.g, periodic, cyclical, and AR models subject to\nmultiple structural breaks.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 18:37:27 GMT"}], "update_date": "2014-03-14", "authors_parsed": [["Karanasos", "Menelaos", ""], ["Paraskevopoulos", "Alexandros", ""], ["Dafnos", "Stavros", ""]]}, {"id": "1403.3457", "submitter": "Yuekai Sun", "authors": "Yuekai Sun and Jonathan E. Taylor", "title": "Valid post-correction inference for censored regression problems", "comments": "20 pages, 6 figues", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-step estimators often called upon to fit censored regression models in\nmany areas of science and engineering. Since censoring incurs a bias in the\nnaive least-squares fit, a two-step estimator first estimates the bias and then\nfits a corrected linear model. We develop a framework for performing valid\n/post-correction inference/ with two-step estimators. By exploiting recent\nresults on post-selection inference, we obtain valid confidence intervals and\nsignificance tests for the fitted coefficients.\n", "versions": [{"version": "v1", "created": "Thu, 13 Mar 2014 23:07:38 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Sun", "Yuekai", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1403.3500", "submitter": "Tobias Michael Erhardt", "authors": "Tobias Michael Erhardt, Claudia Czado and Ulf Schepsmeier", "title": "R-vine Models for Spatial Time Series with an Application to Daily Mean\n  Temperature", "comments": "28 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an extension of R-vine copula models for the purpose of spatial\ndependency modeling and model based prediction at unobserved locations. The\nnewly derived spatial R-vine model combines the flexibility of vine copulas\nwith the classical geostatistical idea of modeling spatial dependencies by\nmeans of the distances between the variable locations. In particular the model\nis able to capture non-Gaussian spatial dependencies. For the purpose of model\ndevelopment and as an illustration we consider daily mean temperature data\nobserved at 54 monitoring stations in Germany. We identify a relationship\nbetween the vine copula parameters and the station distances and exploit it in\norder to reduce the huge number of parameters needed to parametrize a\n54-dimensional R-vine model needed to fit the data. The new distance based\nmodel parametrization results in a distinct reduction in the number of\nparameters and makes parameter estimation and prediction at unobserved\nlocations feasible. The prediction capabilities are validated using adequate\nscoring techniques, showing a better performance of the spatial R-vine copula\nmodel compared to a Gaussian spatial model.\n", "versions": [{"version": "v1", "created": "Fri, 14 Mar 2014 08:35:13 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Erhardt", "Tobias Michael", ""], ["Czado", "Claudia", ""], ["Schepsmeier", "Ulf", ""]]}, {"id": "1403.3805", "submitter": "Holger Dette", "authors": "Holger Dette, Yuri Grigoriev", "title": "$E$-optimal designs for second-order response surface models", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1241 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 4, 1635-1656", "doi": "10.1214/14-AOS1241", "report-no": "IMS-AOS-AOS1241", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  $E$-optimal experimental designs for a second-order response surface model\nwith $k\\geq1$ predictors are investigated. If the design space is the\n$k$-dimensional unit cube, Galil and Kiefer [J. Statist. Plann. Inference 1\n(1977a) 121-132] determined optimal designs in a restricted class of designs\n(defined by the multiplicity of the minimal eigenvalue) and stated their\nuniversal optimality as a conjecture. In this paper, we prove this claim and\nshow that these designs are in fact $E$-optimal in the class of all approximate\ndesigns. Moreover, if the design space is the unit ball, $E$-optimal designs\nhave not been found so far and we also provide a complete solution to this\noptimal design problem. The main difficulty in the construction of $E$-optimal\ndesigns for the second-order response surface model consists in the fact that\nfor the multiplicity of the minimum eigenvalue of the \"optimal information\nmatrix\" is larger than one (in contrast to the case $k=1$) and as a consequence\nthe corresponding optimality criterion is not differentiable at the optimal\nsolution. These difficulties are solved by considering nonlinear Chebyshev\napproximation problems, which arise from a corresponding equivalence theorem.\nThe extremal polynomials which solve these Chebyshev problems are constructed\nexplicitly leading to a complete solution of the corresponding $E$-optimal\ndesign problems.\n", "versions": [{"version": "v1", "created": "Sat, 15 Mar 2014 13:49:56 GMT"}, {"version": "v2", "created": "Fri, 29 Aug 2014 10:06:37 GMT"}], "update_date": "2014-09-01", "authors_parsed": [["Dette", "Holger", ""], ["Grigoriev", "Yuri", ""]]}, {"id": "1403.3808", "submitter": "Florian Heinrichs", "authors": "Michael Vogt, Holger Dette", "title": "Detecting Gradual Changes in Locally Stationary Processes", "comments": "Key words: Local stationarity; empirical processes; measure of\n  time-variation, gradual changes. AMS 2010 subject classifications: 62G05,\n  62G20, 62M10", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide range of applications, the stochastic properties of the observed\ntime series change over time. The changes often occur gradually rather than\nabruptly: the prop- erties are (approximately) constant for some time and then\nslowly start to change. In such situations, it is frequently of interest to\nlocate the time point where the properties start to vary. In contrast to the\nanalysis of abrupt changes, methods for detecting smooth or gradual change\npoints are less developed and often require strong paramet- ric assumptions. In\nthis paper, we develop a fully nonparametric method to estimate a smooth change\npoint in a locally stationary framework. We set up a general procedure which\nallows to deal with a wide variety of stochastic properties including the mean,\n(auto)covariances and higher-order moments. The theoretical part of the paper\nestab- lishes the convergence rate of the new estimator. In addition, we\nexamine its finite sample performance by means of a simulation study and\nillustrate the methodology by applications to temperature and financial return\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 15 Mar 2014 14:31:30 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Vogt", "Michael", ""], ["Dette", "Holger", ""]]}, {"id": "1403.4035", "submitter": "B{\\l}a\\.zej Miasojedow", "authors": "Blazej Miasojedow, Wojciech Niemiro, John Noble, Krzysztof Opalski", "title": "Metropolis-type algorithms for Continuous Time Bayesian Networks", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Metropolis-Hastings Markov chain Monte Carlo (MCMC) algorithm\nfor detecting hidden variables in a continuous time Bayesian network (CTBN),\nwhich uses reversible jumps in the sense defined by (Green 1995). In common\nwith several Monte Carlo algorithms, one of the most recent and important by\n(Rao and Teh 2013), our algorithm exploits uniformization techniques under\nwhich a continuous time Markov process can be represented as a marked Poisson\nprocess. We exploit this in a novel way. We show that our MCMC algorithm can be\nmore efficient than those of likelihood weighting type, as in (Nodelman et al.\n2003) and (Fan et al. 2010) and that our algorithm broadens the class of\nimportant examples that can be treated effectively.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 09:19:50 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Miasojedow", "Blazej", ""], ["Niemiro", "Wojciech", ""], ["Noble", "John", ""], ["Opalski", "Krzysztof", ""]]}, {"id": "1403.4135", "submitter": "Giuliano Galimberti", "authors": "Giuliano Galimberti, Elena Scardovi, Gabriele Soffritti", "title": "Using mixtures in seemingly unrelated linear regression models with\n  non-normal errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seemingly unrelated linear regression models are introduced in which the\ndistribution of the errors is a finite mixture of Gaussian components.\nIdentifiability conditions are provided. The score vector and the Hessian\nmatrix are derived. Parameter estimation is performed using the maximum\nlikelihood method and an Expectation-Maximisation algorithm is developed. The\nusefulness of the proposed methods and a numerical evaluation of their\nproperties are illustrated through the analysis of a real dataset.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 15:46:51 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Galimberti", "Giuliano", ""], ["Scardovi", "Elena", ""], ["Soffritti", "Gabriele", ""]]}, {"id": "1403.4138", "submitter": "Xin Zhang", "authors": "R. Dennis Cook and Xin Zhang", "title": "Algorithms for envelope estimation", "comments": "30 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Envelopes were recently proposed as methods for reducing estimative variation\nin multivariate linear regression. Estimation of an envelope usually involves\noptimization over Grassmann manifolds. We propose a fast and widely applicable\none-dimensional (1D) algorithm for estimating an envelope in general. We reveal\nan important structural property of envelopes that facilitates our algorithm,\nand we prove both Fisher consistency and root-n-consistency of the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 15:51:48 GMT"}], "update_date": "2014-03-18", "authors_parsed": [["Cook", "R. Dennis", ""], ["Zhang", "Xin", ""]]}, {"id": "1403.4290", "submitter": "Tiangang Cui", "authors": "Tiangang Cui, Youssef M. Marzouk and Karen E. Willcox", "title": "Data-Driven Model Reduction for the Bayesian Solution of Inverse\n  Problems", "comments": null, "journal-ref": "International Journal for Numerical Methods in Engineering, 102\n  (5), 966-990 (2015)", "doi": "10.1002/nme.4748", "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the major challenges in the Bayesian solution of inverse problems\ngoverned by partial differential equations (PDEs) is the computational cost of\nrepeatedly evaluating numerical PDE models, as required by Markov chain Monte\nCarlo (MCMC) methods for posterior sampling. This paper proposes a data-driven\nprojection-based model reduction technique to reduce this computational cost.\nThe proposed technique has two distinctive features. First, the model reduction\nstrategy is tailored to inverse problems: the snapshots used to construct the\nreduced-order model are computed adaptively from the posterior distribution.\nPosterior exploration and model reduction are thus pursued simultaneously.\nSecond, to avoid repeated evaluations of the full-scale numerical model as in a\nstandard MCMC method, we couple the full-scale model and the reduced-order\nmodel together in the MCMC algorithm. This maintains accurate inference while\nreducing its overall computational cost. In numerical experiments considering\nsteady-state flow in a porous medium, the data-driven reduced-order model\nachieves better accuracy than a reduced-order model constructed using the\nclassical approach. It also improves posterior sampling efficiency by several\norders of magnitude compared to a standard MCMC method.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 22:21:38 GMT"}, {"version": "v2", "created": "Tue, 15 Jul 2014 04:02:51 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Cui", "Tiangang", ""], ["Marzouk", "Youssef M.", ""], ["Willcox", "Karen E.", ""]]}, {"id": "1403.4296", "submitter": "Claus Ekstr{\\o}m", "authors": "Kasper Brink-Jensen and Claus Thorn Ekstr{\\o}m", "title": "Inference for feature selection using the Lasso with high-dimensional\n  data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression models such as the Lasso have proved useful for variable\nselection in many fields - especially for situations with high-dimensional data\nwhere the numbers of predictors far exceeds the number of observations. These\nmethods identify and rank variables of importance but do not generally provide\nany inference of the selected variables. Thus, the variables selected might be\nthe \"most important\" but need not be significant. We propose a significance\ntest for the selection found by the Lasso. We introduce a procedure that\ncomputes inference and p-values for features chosen by the Lasso. This method\nrephrases the null hypothesis and uses a randomization approach which ensures\nthat the error rate is controlled even for small samples. We demonstrate the\nability of the algorithm to compute $p$-values of the expected magnitude with\nsimulated data using a multitude of scenarios that involve various effects\nstrengths and correlation between predictors. The algorithm is also applied to\na prostate cancer dataset that has been analyzed in recent papers on the\nsubject. The proposed method is found to provide a powerful way to make\ninference for feature selection even for small samples and when the number of\npredictors are several orders of magnitude larger than the number of\nobservations. The algorithm is implemented in the MESS package in R and is\nfreely available.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 22:36:52 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Brink-Jensen", "Kasper", ""], ["Ekstr\u00f8m", "Claus Thorn", ""]]}, {"id": "1403.4429", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universite Paris-Dauphine and University of\n  Warwick)", "title": "Des specificites de l'approche bayesienne et de ses justifications en\n  statistique inferentielle", "comments": "19 pages, in French, to appear in a philosophy of science volume on\n  Bayesian statistics edited by editions Materiologiques", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This book chapter (written in French) is a review of the foundations of the\nBayesian approach to statistical inference, relating to its historical roots\nand some philosophical arguments, as well as a short presentation of its\npractical implementation.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 12:25:12 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Robert", "Christian P.", "", "Universite Paris-Dauphine and University of\n  Warwick"]]}, {"id": "1403.4630", "submitter": "Daniel Simpson", "authors": "Daniel P. Simpson, H{\\aa}vard Rue, Thiago G. Martins, Andrea Riebler,\n  and Sigrunn H. S{\\o}rbye", "title": "Penalising model component complexity: A principled, practical approach\n  to constructing priors", "comments": "Major revision of previous version. Includes a beefed up literature\n  review and new desiderata for hierarchical priors. Removes (for space) the\n  Cox proportional hazard model and the section on hyperparameters for Gaussian\n  random fields", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new concept for constructing prior\ndistributions. We exploit the natural nested structure inherent to many model\ncomponents, which defines the model component to be a flexible extension of a\nbase model. Proper priors are defined to penalise the complexity induced by\ndeviating from the simpler base model and are formulated after the input of a\nuser-defined scaling parameter for that model component, both in the univariate\nand the multivariate case. These priors are invariant to reparameterisations,\nhave a natural connection to Jeffreys' priors, are designed to support Occam's\nrazor and seem to have excellent robustness properties, all which are highly\ndesirable and allow us to use this approach to define default prior\ndistributions. Through examples and theoretical results, we demonstrate the\nappropriateness of this approach and how it can be applied in various\nsituations.\n", "versions": [{"version": "v1", "created": "Tue, 18 Mar 2014 22:11:06 GMT"}, {"version": "v2", "created": "Sat, 5 Apr 2014 14:29:00 GMT"}, {"version": "v3", "created": "Wed, 26 Nov 2014 22:37:55 GMT"}, {"version": "v4", "created": "Thu, 6 Aug 2015 11:20:27 GMT"}], "update_date": "2015-08-07", "authors_parsed": [["Simpson", "Daniel P.", ""], ["Rue", "H\u00e5vard", ""], ["Martins", "Thiago G.", ""], ["Riebler", "Andrea", ""], ["S\u00f8rbye", "Sigrunn H.", ""]]}, {"id": "1403.4680", "submitter": "Tiangang Cui", "authors": "Tiangang Cui, James Martin, Youssef M. Marzouk, Antti Solonen and\n  Alessio Spantini", "title": "Likelihood-informed dimension reduction for nonlinear inverse problems", "comments": null, "journal-ref": "Inverse Problems, 30, 114015 (2014)", "doi": "10.1088/0266-5611/30/11/114015", "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intrinsic dimensionality of an inverse problem is affected by prior\ninformation, the accuracy and number of observations, and the smoothing\nproperties of the forward operator. From a Bayesian perspective, changes from\nthe prior to the posterior may, in many problems, be confined to a relatively\nlow-dimensional subspace of the parameter space. We present a dimension\nreduction approach that defines and identifies such a subspace, called the\n\"likelihood-informed subspace\" (LIS), by characterizing the relative influences\nof the prior and the likelihood over the support of the posterior distribution.\nThis identification enables new and more efficient computational methods for\nBayesian inference with nonlinear forward models and Gaussian priors. In\nparticular, we approximate the posterior distribution as the product of a\nlower-dimensional posterior defined on the LIS and the prior distribution\nmarginalized onto the complementary subspace. Markov chain Monte Carlo sampling\ncan then proceed in lower dimensions, with significant gains in computational\nefficiency. We also introduce a Rao-Blackwellization strategy that\nde-randomizes Monte Carlo estimates of posterior expectations for additional\nvariance reduction. We demonstrate the efficiency of our methods using two\nnumerical examples: inference of permeability in a groundwater system governed\nby an elliptic PDE, and an atmospheric remote sensing problem based on Global\nOzone Monitoring System (GOMOS) observations.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 03:22:21 GMT"}, {"version": "v2", "created": "Tue, 15 Jul 2014 04:50:31 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Cui", "Tiangang", ""], ["Martin", "James", ""], ["Marzouk", "Youssef M.", ""], ["Solonen", "Antti", ""], ["Spantini", "Alessio", ""]]}, {"id": "1403.4698", "submitter": "Xi Luo", "authors": "Xi Luo", "title": "A Hierarchical Graphical Model for Big Inverse Covariance Estimation\n  with an Application to fMRI", "comments": "An R package of the proposed method will be publicly available on\n  CRAN. This paper has been presented orally at Yale University on Feburary 18,\n  2014, and at the Eastern North American Region Meeting of the International\n  Biometric Society on March 18, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brain networks has attracted the interests of many neuroscientists. From\nfunctional MRI (fMRI) data, statistical tools have been developed to recover\nbrain networks. However, the dimensionality of whole-brain fMRI, usually in\nhundreds of thousands, challenges the applicability of these methods. We\ndevelop a hierarchical graphical model (HGM) to remediate this difficulty. This\nmodel introduces a hidden layer of networks based on sparse Gaussian graphical\nmodels, and the observed data are sampled from individual network nodes. In\nfMRI, the network layer models the underlying signals of different brain\nfunctional units, and how these units directly interact with each other. The\nintroduction of this hierarchical structure not only provides a formal and\ninterpretable approach, but also enables efficient computation for inferring\nbig networks with hundreds of thousands of nodes. Based on the conditional\nconvexity of our formulation, we develop an alternating update algorithm to\ncompute the HGM model parameters simultaneously. The effectiveness of this\napproach is demonstrated on simulated data and a real dataset from a stop/go\nfMRI experiment.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 05:43:43 GMT"}, {"version": "v2", "created": "Mon, 7 Apr 2014 18:49:10 GMT"}], "update_date": "2014-04-08", "authors_parsed": [["Luo", "Xi", ""]]}, {"id": "1403.4803", "submitter": "Menelaos Karanasos", "authors": "Menelaos Karanasos, Alexandros Paraskevopoulos, Stavros Dafnos", "title": "A univariate time varying analysis of periodic ARMA processes", "comments": "26 pages, no figures. arXiv admin note: text overlap with\n  arXiv:1403.3359", "journal-ref": null, "doi": null, "report-no": "26 pages", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach for studying the periodic ARMA model with coefficients\nthat vary over the seasons is to express it in a vector form. In this paper we\nintroduce an alternative method which views the periodic formulation as a time\nvarying univariate process and obviates the need for vector analysis. The\nspecification, interpretation, and solution of a periodic ARMA process enable\nus to formulate a forecasting method which avoids recursion and allows us to\nobtain analytic expressions of the optimal predictors. Our results on periodic\nmodels are general, analogous to those for stationary specifications, and place\nthe former on the same computational basis as the latter.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 13:59:35 GMT"}], "update_date": "2014-03-20", "authors_parsed": [["Karanasos", "Menelaos", ""], ["Paraskevopoulos", "Alexandros", ""], ["Dafnos", "Stavros", ""]]}, {"id": "1403.4890", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy, Genetha A. Gray, Sebastien Le Digabel, Herbert K.H.\n  Lee, Pritam Ranjan, Garth Wells, Stefan M. Wild", "title": "Modeling an Augmented Lagrangian for Blackbox Constrained Optimization", "comments": "22 Pages, 2 additional supplementary, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained blackbox optimization is a difficult problem, with most\napproaches coming from the mathematical programming literature. The statistical\nliterature is sparse, especially in addressing problems with nontrivial\nconstraints. This situation is unfortunate because statistical methods have\nmany attractive properties: global scope, handling noisy objectives,\nsensitivity analysis, and so forth. To narrow that gap, we propose a\ncombination of response surface modeling, expected improvement, and the\naugmented Lagrangian numerical optimization framework. This hybrid approach\nallows the statistical model to think globally and the augmented Lagrangian to\nact locally. We focus on problems where the constraints are the primary\nbottleneck, requiring expensive simulation to evaluate and substantial modeling\neffort to map out. In that context, our hybridization presents a simple yet\neffective solution that allows existing objective-oriented statistical\napproaches, like those based on Gaussian process surrogates and expected\nimprovement heuristics, to be applied to the constrained setting with minor\nmodification. This work is motivated by a challenging, real-data benchmark\nproblem from hydrology where, even with a simple linear objective function,\nlearning a nontrivial valid region complicates the search for a global minimum.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 17:36:37 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 15:37:55 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2015 18:41:10 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Gray", "Genetha A.", ""], ["Digabel", "Sebastien Le", ""], ["Lee", "Herbert K. H.", ""], ["Ranjan", "Pritam", ""], ["Wells", "Garth", ""], ["Wild", "Stefan M.", ""]]}, {"id": "1403.5065", "submitter": "Dario Gasbarra", "authors": "Dario Gasbarra, Jia Liu, Juha Railavo", "title": "Data augmentation in Rician noise model and Bayesian Diffusion Tensor\n  Imaging", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mapping white matter tracts is an essential step towards understanding brain\nfunction. Diffusion Magnetic Resonance Imaging (dMRI) is the only noninvasive\ntechnique which can detect in vivo anisotropies in the 3-dimensional diffusion\nof water molecules, which correspond to nervous fibers in the living brain. In\nthis process, spectral data from the displacement distribution of water\nmolecules is collected by a magnetic resonance scanner. From the statistical\npoint of view, inverting the Fourier transform from such sparse and noisy\nspectral measurements leads to a non-linear regression problem. Diffusion\ntensor imaging (DTI) is the simplest modeling approach postulating a Gaussian\ndisplacement distribution at each volume element (voxel). Typically the\ninference is based on a linearized log-normal regression model that can fit the\nspectral data at low frequencies. However such approximation fails to fit the\nhigh frequency measurements which contain information about the details of the\ndisplacement distribution but have a low signal to noise ratio. In this paper,\nwe directly work with the Rice noise model and cover the full range of\n$b$-values. Using data augmentation to represent the likelihood, we reduce the\nnon-linear regression problem to the framework of generalized linear models.\nThen we construct a Bayesian hierarchical model in order to perform\nsimultaneously estimation and regularization of the tensor field. Finally the\nBayesian paradigm is implemented by using Markov chain Monte Carlo.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 08:37:14 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Gasbarra", "Dario", ""], ["Liu", "Jia", ""], ["Railavo", "Juha", ""]]}, {"id": "1403.5105", "submitter": "Xinjun Gan", "authors": "Xinjun Gan, Gang Wei, Jie Zhang, Qi Zhang", "title": "The factorization and simulation for fundamental solution of Cauchy\n  problem", "comments": "This paper has been withdrawn by the author due to a crucial sign\n  error in simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we demonstrate the simulation of fundamental solution for the\nparabolic equation by the relationship with Ito diffusion. The factorization\nand Monte Carlo methods of the fundamental solution are considered. With the\nfact that the fundamental solution can be written as a product of the\ntransition function and the expectation of a bridge path integral, we give an\nnovel and efficient algorithm to simulate the fundamental solution by\nimportance sampling method, especially for dealing with the multi-dimensional\ncase.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 11:44:29 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 13:55:34 GMT"}, {"version": "v3", "created": "Fri, 4 Jul 2014 14:52:08 GMT"}], "update_date": "2014-07-07", "authors_parsed": [["Gan", "Xinjun", ""], ["Wei", "Gang", ""], ["Zhang", "Jie", ""], ["Zhang", "Qi", ""]]}, {"id": "1403.5118", "submitter": "Robin  Lovelace Dr", "authors": "Robin Lovelace, Nick Malleson, Kirk Harland and Mark Birkin", "title": "Geotagged tweets to inform a spatial interaction model: a case study of\n  museums", "comments": "A concise version of this article was submitted to GISRUK2014\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the potential of volunteered geographical information\nfrom social media for informing geographical models of behavior, based on a\ncase study of museums in Yorkshire, UK. A spatial interaction model of visitors\nto 15 museums from 179 administrative zones is constructed to test this\npotential. The main input dataset comprises geo-tagged messages harvested using\nthe Twitter Streaming Application Programming Interface (API), filtered,\nanalyzed and aggregated to allow direct comparison with the model's output.\nComparison between model output and tweet information allowed the calibration\nof model parameters to optimize the fit between flows to museums inferred from\ntweets and flow matrices generated by the spatial interaction model. We\nconclude that volunteered geographic information from social media sites have\ngreat potential for informing geographical models of behavior, especially if\nthe volume of geo-tagged social media messages continues to increase. However,\nwe caution that volunteered geographical information from social media has some\nmajor limitations so should be used only as a supplement to more consistent\ndata sources or when official datasets are unavailable.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 12:48:24 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Lovelace", "Robin", ""], ["Malleson", "Nick", ""], ["Harland", "Kirk", ""], ["Birkin", "Mark", ""]]}, {"id": "1403.5150", "submitter": "Karthik Bharath", "authors": "Sebastian Kurtek and Karthik Bharath", "title": "Bayes Sensitivity with Fisher-Rao Metric", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a geometric framework to assess sensitivity of Bayesian procedures\nto modeling assumptions based on the nonparametric Fisher-Rao metric. While the\nframework is general in spirit, the focus of this article is restricted to\nmetric-based diagnosis under two settings: assessing local and global\nrobustness in Bayesian procedures to perturbations of the likelihood and prior,\nand identification of influential observations. The approach is based on the\nsquare-root representation of densities which enables one to compute geodesics\nand geodesic distances in analytical form, facilitating the definition of\nnaturally calibrated local and global discrepancy measures. An important\nfeature of our approach is the definition of a geometric\n$\\epsilon$-contamination class of sampling distributions and priors via\nintrinsic analysis on the space of probability density functions. We showcase\nthe applicability of our framework on several simulated toy datasets as well as\nin real data settings for generalized mixed effects models, directional data\nand shape data.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 14:38:59 GMT"}, {"version": "v2", "created": "Fri, 25 Apr 2014 14:38:45 GMT"}], "update_date": "2014-04-28", "authors_parsed": [["Kurtek", "Sebastian", ""], ["Bharath", "Karthik", ""]]}, {"id": "1403.5196", "submitter": "Ben Youngman", "authors": "Jeremy E. Oakley and Benjamin D. Youngman", "title": "Calibration of Complex Computer Simulators using Likelihood Emulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We calibrate a Natural History Model, which is a class of computer simulator\nused in the health industry, and here has been used to characterise bowel\ncancer incidence for the UK. The simulator tracks the development of bowel\ncancer in a sample of people, and its output mostly stratifies bowel cancer\noccurrence by patient age and bowel cancer type. Its output relies on 25\nunknown inputs, which we are required to calibrate. In order to do this we must\naddress that not only is the output count data, but it is also stochastic, due\nto the simulation procedure.\n  We cannot feasibly achieve calibration of the simulator using Monte Carlo\nmethods alone, as it is of `moderate' computational expense. To achieve a\nreliable calibration, we must also specify its discrepancy: how, when\ncalibrated, it differs from reality. We propose a method for calibration that\ncombines a statistical emulator for the likelihood function with importance\nsampling. The emulator provides an interim sample of inputs at which the\nsimulator is run, from which the likelihood is calculated. Importance sampling\nis then used to re-weight the inputs and provide a final sample of calibrated\ninputs. Re-calculating the importance weights incurs little computational cost,\nand so we can easily investigate how different discrepancy specifications\naffect calibration.\n", "versions": [{"version": "v1", "created": "Thu, 20 Mar 2014 16:40:52 GMT"}, {"version": "v2", "created": "Tue, 28 Oct 2014 10:24:15 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Oakley", "Jeremy E.", ""], ["Youngman", "Benjamin D.", ""]]}, {"id": "1403.5250", "submitter": "Hitesh Chhinkaniwala", "authors": "Hitesh Chhinkaniwala, Sanjay Garg", "title": "Privacy Gain Based Multi-Iterative k-Anonymization to Protect\n  Respondents Privacy", "comments": "8 pages", "journal-ref": "IFRSA International Journal Of Computing, Vol 3, issue 2, April\n  2013, pp. 85-92", "doi": null, "report-no": null, "categories": "stat.ME cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Huge volume of data from domain specific applications such as medical,\nfinancial, telephone, shopping records and individuals are regularly generated.\nSharing of these data is proved to be beneficial for data mining application.\nSince data mining often involves data that contains personally identifiable\ninformation and therefore releasing such data may result in privacy breaches.\nOn one hand such data is an important asset to business decision making by\nanalyzing it. On the other hand data privacy concerns may prevent data owners\nfrom sharing information for data analysis. In order to share data while\npreserving privacy, data owner must come up with a solution which achieves the\ndual goal of privacy preservation as well as accuracy of data mining task\nmainly clustering and classification. Privacy Preserving Data Publishing (PPDP)\nis a study of eliminating privacy threats like linkage attack while preserving\ndata utility by anonymizing data set before publishing. Proposed work is an\nextension to k-anonymization where Privacy Gain (PrGain) has been computed for\nselective anonymization for set of tuples. Classification and clustering\ncharacteristics of original data and anonymized data using proposed algorithm\nhave been evaluated in terms of information loss, execution time, and privacy\nachieved. Algorithm has been processed against standard data sets and analysis\nshows that values for sensitive attributes are being preserved with minimal\ninformation loss.\n", "versions": [{"version": "v1", "created": "Wed, 19 Mar 2014 16:50:52 GMT"}], "update_date": "2014-03-21", "authors_parsed": [["Chhinkaniwala", "Hitesh", ""], ["Garg", "Sanjay", ""]]}, {"id": "1403.5496", "submitter": "Nial Friel", "authors": "P. Alquier, N. Friel, R. Everitt and A. Boland", "title": "Noisy Monte Carlo: Convergence of Markov chains with approximate\n  transition kernels", "comments": "This version: results extended to non-uniformly ergodic Markov chains", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo algorithms often aim to draw from a distribution $\\pi$ by\nsimulating a Markov chain with transition kernel $P$ such that $\\pi$ is\ninvariant under $P$. However, there are many situations for which it is\nimpractical or impossible to draw from the transition kernel $P$. For instance,\nthis is the case with massive datasets, where is it prohibitively expensive to\ncalculate the likelihood and is also the case for intractable likelihood models\narising from, for example, Gibbs random fields, such as those found in spatial\nstatistics and network analysis. A natural approach in these cases is to\nreplace $P$ by an approximation $\\hat{P}$. Using theory from the stability of\nMarkov chains we explore a variety of situations where it is possible to\nquantify how 'close' the chain given by the transition kernel $\\hat{P}$ is to\nthe chain given by $P$. We apply these results to several examples from spatial\nstatistics and network analysis.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 15:38:31 GMT"}, {"version": "v2", "created": "Mon, 24 Mar 2014 12:34:36 GMT"}, {"version": "v3", "created": "Tue, 15 Apr 2014 15:46:25 GMT"}], "update_date": "2014-04-16", "authors_parsed": [["Alquier", "P.", ""], ["Friel", "N.", ""], ["Everitt", "R.", ""], ["Boland", "A.", ""]]}, {"id": "1403.5536", "submitter": "James M. Flegal", "authors": "Lei Gong and James M. Flegal", "title": "A practical sequential stopping rule for high-dimensional MCMC and its\n  application to spatial-temporal Bayesian models", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A current challenge for many Bayesian analyses is determining when to\nterminate high-dimensional Markov chain Monte Carlo simulations. To this end,\nwe propose using an automated sequential stopping procedure that terminates the\nsimulation when the computational uncertainty is small relative to the\nposterior uncertainty. Such a stopping rule has previously been shown to work\nwell in settings with posteriors of moderate dimension. In this paper, we\nillustrate its utility in high-dimensional simulations while overcoming some\ncurrent computational issues. Further, we investigate the relationship between\nthe stopping rule and effective sample size. As examples, we consider two\ncomplex Bayesian analyses on spatially and temporally correlated datasets. The\nfirst involves a dynamic space-time model on weather station data and the\nsecond a spatial variable selection model on fMRI brain imaging data. Our\nresults show the sequential stopping rule is easy to implement, provides\nuncertainty estimates, and performs well in high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 18:40:26 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Gong", "Lei", ""], ["Flegal", "James M.", ""]]}, {"id": "1403.5537", "submitter": "Alexandre Janon", "authors": "Yohann De Castro (LM-Orsay), Alexandre Janon (LM-Orsay, - M\\'ethodes\n  d'Analyse Stochastique des Codes et Traitements Num\\'eriques)", "title": "Randomized pick-freeze for sparse Sobol indices estimation in high\n  dimension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article investigates a new procedure to estimate the influence of each\nvariable of a given function defined on a high-dimensional space. More\nprecisely, we are concerned with describing a function of a large number $p$ of\nparameters that depends only on a small number $s$ of them. Our proposed method\nis an unconstrained $\\ell_{1}$-minimization based on the Sobol's method. We\nprove that, with only $\\mathcal O(s\\log p)$ evaluations of $f$, one can find\nwhich are the relevant parameters.\n", "versions": [{"version": "v1", "created": "Fri, 21 Mar 2014 18:41:09 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["De Castro", "Yohann", "", "LM-Orsay"], ["Janon", "Alexandre", "", "LM-Orsay, - M\u00e9thodes\n  d'Analyse Stochastique des Codes et Traitements Num\u00e9riques"]]}, {"id": "1403.5609", "submitter": "Zhigen Zhao", "authors": "Li He, Sanat K. Sarkar, Zhigen Zhao", "title": "Capturing the Severity of Type II Errors in High-Dimensional Multiple\n  Testing", "comments": "19 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The severity of type II errors is frequently ignored when deriving a multiple\ntesting procedure, even though utilizing it properly can greatly help in making\ncorrect decisions. This paper puts forward a theory behind developing a\nmultiple testing procedure that can incorporate the type II error severity and\nis optimal in the sense of minimizing a measure of false non-discoveries among\nall procedures controlling a measure of false discoveries. The theory is\ndeveloped under a general model allowing arbitrary dependence by taking a\ncompound decision theoretic approach to multiple testing with a loss function\nincorporating the type II error severity. We present this optimal procedure in\nits oracle form and offer numerical evidence of its superior performance over\nrelevant competitors.\n", "versions": [{"version": "v1", "created": "Sat, 22 Mar 2014 03:52:24 GMT"}], "update_date": "2014-03-25", "authors_parsed": [["He", "Li", ""], ["Sarkar", "Sanat K.", ""], ["Zhao", "Zhigen", ""]]}, {"id": "1403.6095", "submitter": "Irina Gaynanova", "authors": "Irina Gaynanova, James G. Booth and Martin T. Wells", "title": "Simultaneous sparse estimation of canonical vectors in the p>>N setting", "comments": "Accepted to JASA, 2015", "journal-ref": "Journal of the American Statistical Association 2016, Vol. 111,\n  No. 514, 696-706", "doi": "10.1080/01621459.2015.1034318", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the problem of sparse estimation of canonical vectors\nin linear discriminant analysis when $p\\gg N$. Several methods have been\nproposed in the literature that estimate one canonical vector in the two-group\ncase. However, $G-1$ canonical vectors can be considered if the number of\ngroups is $G$. In the multi-group context, it is common to estimate canonical\nvectors in a sequential fashion. Moreover, separate prior estimation of the\ncovariance structure is often required. We propose a novel methodology for\ndirect estimation of canonical vectors. In contrast to existing techniques, the\nproposed method estimates all canonical vectors at once, performs variable\nselection across all the vectors and comes with theoretical guarantees on the\nvariable selection and classification consistency. First, we highlight the fact\nthat in the $N>p$ setting the canonical vectors can be expressed in a closed\nform up to an orthogonal transformation. Secondly, we propose an extension of\nthis form to the $p\\gg N$ setting and achieve feature selection by using a\ngroup penalty. The resulting optimization problem is convex and can be solved\nusing a block-coordinate descent algorithm. The practical performance of the\nmethod is evaluated through simulation studies as well as real data\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 19:28:37 GMT"}, {"version": "v2", "created": "Tue, 1 Apr 2014 14:11:02 GMT"}, {"version": "v3", "created": "Tue, 4 Nov 2014 15:50:04 GMT"}, {"version": "v4", "created": "Thu, 30 Apr 2015 18:44:27 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gaynanova", "Irina", ""], ["Booth", "James G.", ""], ["Wells", "Martin T.", ""]]}, {"id": "1403.6168", "submitter": "Julien  Chiquet Dr.", "authors": "Julien Chiquet and Tristan Mary-Huard and St\\'ephane Robin", "title": "Structured Regularization for conditional Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional Gaussian graphical models (cGGM) are a recent reparametrization\nof the multivariate linear regression model which explicitly exhibits $i)$ the\npartial covariances between the predictors and the responses, and $ii)$ the\npartial covariances between the responses themselves. Such models are\nparticularly suitable for interpretability since partial covariances describe\nstrong relationships between variables. In this framework, we propose a\nregularization scheme to enhance the learning strategy of the model by driving\nthe selection of the relevant input features by prior structural information.\nIt comes with an efficient alternating optimization procedure which is\nguaranteed to converge to the global minimum. On top of showing competitive\nperformance on artificial and real datasets, our method demonstrates\ncapabilities for fine interpretation of its parameters, as illustrated on three\nhigh-dimensional datasets from spectroscopy, genetics, and genomics.\n", "versions": [{"version": "v1", "created": "Mon, 24 Mar 2014 22:07:06 GMT"}, {"version": "v2", "created": "Thu, 25 Sep 2014 08:58:52 GMT"}], "update_date": "2014-09-26", "authors_parsed": [["Chiquet", "Julien", ""], ["Mary-Huard", "Tristan", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1403.6206", "submitter": "Luke Prendergast", "authors": "Luke A. Prendergast and Alexandra L. Garnham", "title": "Simple response and predictor transformations to adjust for symmetric\n  dependency in dimension reduction for visualization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the regression setting, dimension reduction allows for complicated\nregression structures to be detected via visualization in a low-dimension\nframework. However, some popular dimension reduction methodologies fail to\nachieve this aim when faced with a problem often referred to as symmetric\ndependency. In this paper we show how vastly superior results can be achieved\nwhen carrying out response and predictor transformations for methods such as\nleast squares and Sliced Inverse Regression. These transformations are simple\nto implement and utilize estimates from other dimension reduction methods that\nare not faced with the symmetric dependency problem. We highlight the\neffectiveness of our approach via simulation and an example. Furthermore, we\nshow that ordinary least squares can effectively detect multiple dimension\nreduction directions. Methods robust to extreme response values are also\nconsidered.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 01:28:14 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Prendergast", "Luke A.", ""], ["Garnham", "Alexandra L.", ""]]}, {"id": "1403.6212", "submitter": "Yiyuan She", "authors": "Yiyuan She", "title": "Selective Factor Extraction in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies simultaneous feature selection and extraction in\nsupervised and unsupervised learning. We propose and investigate selective\nreduced rank regression for constructing optimal explanatory factors from a\nparsimonious subset of input features. The proposed estimators enjoy sharp\noracle inequalities, and with a predictive information criterion for model\nselection, they adapt to unknown sparsity by controlling both rank and row\nsupport of the coefficient matrix. A class of algorithms is developed that can\naccommodate various convex and nonconvex sparsity-inducing penalties, and can\nbe used for rank-constrained variable screening in high-dimensional\nmultivariate data. The paper also showcases applications in macroeconomics and\ncomputer vision to demonstrate how low-dimensional data structures can be\neffectively captured by joint variable selection and projection.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 02:40:41 GMT"}, {"version": "v2", "created": "Sun, 7 Dec 2014 01:25:39 GMT"}, {"version": "v3", "created": "Thu, 20 Oct 2016 05:35:29 GMT"}, {"version": "v4", "created": "Tue, 25 Oct 2016 22:51:03 GMT"}], "update_date": "2016-10-27", "authors_parsed": [["She", "Yiyuan", ""]]}, {"id": "1403.6295", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh", "title": "Asymptotic Properties of Minimum S-Divergence Estimator for Discrete\n  Models", "comments": "Under review, 24 pages", "journal-ref": "Sankhya - The Indian Journal of Statistics, 2015, 77(2), 380 - 407", "doi": "10.1007/s13171-014-0063-2", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust inference based on the minimization of statistical divergences has\nproved to be a useful alternative to the classical techniques based on maximum\nlikelihood and related methods. Recently Ghosh et al. (2013) proposed a general\nclass of divergence measures, namely the S-Divergence Family and discussed its\nusefulness in robust parametric estimation through some numerical\nillustrations. In this present paper, we develop the asymptotic properties of\nthe proposed minimum S-Divergence estimators under discrete models.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 10:58:01 GMT"}, {"version": "v2", "created": "Wed, 25 Jun 2014 10:43:55 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Ghosh", "Abhik", ""]]}, {"id": "1403.6304", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh, Ayanendranath Basu", "title": "Estimation of Multivariate Location and Covariance using the S\n  -Hellinger Distance", "comments": "This paper has been withdrawn by the author due to a crucial error in\n  the simulation section", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a generalization of the Hellinger distance which we call\nthe S -Hellinger distance; this general family connects the Hellinger distance\nsmoothly with the $L_2$-divergence by a tuning parameter $\\alpha$ and is indeed\na subfamily of the S -Divergence family of Ghosh et al. (2013 a, b). We use\nthis general divergence in the context of estimating the location and\ncovariances under (continuous) multivariate models and show that the proposed\nminimum S -Hellinger distance estimator is affine equivariant, asymptotically\nconsistent and have high breakdown point under suitable conditions. We also\nillustrate its performance through an extensive simulation study which show\nthat the proposed estimators give more robust estimator than the minimum\nHellinger distance estimator for the location and correlation parameters under\ndifferent types of contamination with the contamination proportion being as\nhigh as 20%.\n", "versions": [{"version": "v1", "created": "Tue, 25 Mar 2014 11:20:57 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 06:31:47 GMT"}], "update_date": "2014-12-08", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1403.6496", "submitter": "X. San Liang", "authors": "X. San Liang", "title": "Causality between time series", "comments": "Presented at the 18th Conference on Atmospheric and Oceanic Fluid\n  Dynamics, 13-17 June 2011, Spokane, WA, under the title \"Information Flow and\n  Causality within Atmosphere-Ocean Systems\". PPT slides are available from the\n  online conference archives", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two time series, can one tell, in a rigorous and quantitative way, the\ncause and effect between them? Based on a recently rigorized physical notion\nnamely information flow, we arrive at a concise formula and give this\nchallenging question, which is of wide concern in different disciplines, a\npositive answer. Here causality is measured by the time rate of change of\ninformation flowing from one series, say, X2, to another, X1. The measure is\nasymmetric between the two parties and, particularly, if the process underlying\nX1 does not depend on X2, then the resulting causality from X2 to X1 vanishes.\nThe formula is tight in form, involving only the commonly used statistics,\nsample covariances. It has been validated with touchstone series purportedly\ngenerated with one-way causality. It has also been applied to the investigation\nof real world problems; an example presented here is the cause-effect relation\nbetween two climate modes, El Ni\\~no and Indian Ocean Dipole, which have been\nlinked to the hazards in far flung regions of the globe, with important results\nthat would otherwise be difficult, if not impossible, to obtain.\n", "versions": [{"version": "v1", "created": "Mon, 17 Mar 2014 01:16:10 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Liang", "X. San", ""]]}, {"id": "1403.6573", "submitter": "Yermek Kapushev", "authors": "Mikhail Belyaev, Evgeny Burnaev and Yermek Kapushev", "title": "Exact Inference for Gaussian Process Regression in case of Big Data with\n  the Cartesian Product Structure", "comments": "10 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximation algorithms are widely used in many engineering problems. To\nobtain a data set for approximation a factorial design of experiments is often\nused. In such case the size of the data set can be very large. Therefore, one\nof the most popular algorithms for approximation - Gaussian Process regression\n- can be hardly applied due to its computational complexity. In this paper a\nnew approach for Gaussian Process regression in case of factorial design of\nexperiments is proposed. It allows to efficiently compute exact inference and\nhandle large multidimensional data sets. The proposed algorithm provides fast\nand accurate approximation and also handles anisotropic data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 05:56:17 GMT"}, {"version": "v2", "created": "Thu, 3 Jul 2014 13:46:10 GMT"}], "update_date": "2014-07-04", "authors_parsed": [["Belyaev", "Mikhail", ""], ["Burnaev", "Evgeny", ""], ["Kapushev", "Yermek", ""]]}, {"id": "1403.6585", "submitter": "Isambi Mbalawata", "authors": "Isambi S. Mbalawata and Simo S\\\"arkk\\\"a", "title": "Moment Conditions for Convergence of Particle Filters with Unbounded\n  Importance Weights", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we derive moment conditions for particle filter importance\nweights, which ensure that the particle filter estimates of the expectations of\nbounded Borel functions converge in mean square and $L^4$ sense, and that the\nempirical measure of the particle filter converges weakly to the true filtering\nmeasure. The result extends the previously derived conditions by not requiring\nthe boundedness of the importance weights, but only boundedness of second or\nfourth order moments. We show that the boundedness of the second order moments\nof the weights implies the convergence of the estimates bounded functions in\nthe mean square sense, and the $L^4$ convergence as well as the empirical\nmeasure convergence are assured by the boundedness of the fourth order moments\nof the weights. We also present an example class of models and importance\ndistributions where the moment conditions hold, but the boundedness does not.\nThe unboundedness in these models is caused by point-singularities in the\nweights which still leave the weight moments bounded. We show by using\nsimulated data that the particle filter for this kind of model also performs\nwell in practice.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 08:34:10 GMT"}, {"version": "v2", "created": "Thu, 12 Jun 2014 05:34:41 GMT"}, {"version": "v3", "created": "Mon, 18 Aug 2014 06:09:03 GMT"}], "update_date": "2014-08-20", "authors_parsed": [["Mbalawata", "Isambi S.", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1403.6606", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh and Ayanendranath Basu", "title": "Robust Estimation in Generalised Linear Models : The Density Power\n  Divergence Approach", "comments": "Pre-print, 22 pages", "journal-ref": "TEST June 2016, Volume 25, Issue 2, pp 269-290", "doi": "10.1007/s11749-015-0445-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalised linear model (GLM) is a very important tool for analysing\nreal data in biology, sociology, agriculture, engineering and many other\napplication domain where the relationship between the response and explanatory\nvariables may not be linear or the distributions may not be normal in all the\ncases. However, quite often such real data contain a significant number of\noutliers in relation to the standard parametric model used in the analysis; in\nsuch cases the classical maximum likelihood estimator may fail to produce\nreasonable estimators and related inference could be unreliable. In this paper,\nwe develop a robust estimation procedure for the generalised linear models that\ncan generate robust estimators with little loss in efficiency. We will also\nexplore two particular cases of the generalised linear model in details --\nPoisson regression for count data and logistic regression for binary data --\nwhich are widely applied in real life experiments. We will also illustrate the\nperformance of the proposed estimators through several interesting data\nexamples\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 09:50:12 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1403.6744", "submitter": "Rui Zhang", "authors": "Rui Zhang Kwun Chuen Gary Chan", "title": "A marginalizable frailty model for correlated right-censored data", "comments": "46 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a flexible individual frailty model for clustered right-censored\ndata, in which covariate effects can be marginally interpreted as log failure\nodds ratios. Flexible correlation structures can be imposed by introducing\nmultivariate exponential distributed frailties, constructed from a set of\nmultivariate Gaussian random variables. Finite and infinite dimensional\nparameters are consistently estimated by maximizing a composite contributing\nmarginal likelihood and a consistent estimate for their asymptotic covariance\nis proposed. Parameter estimation is implemented through a hybrid\nexpectation-maximum algorithm. Simulations and an analysis of the Rats study\nwere carried out to demonstrate our method.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 16:46:51 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Chan", "Rui Zhang Kwun Chuen Gary", ""]]}, {"id": "1403.6752", "submitter": "Jana Jankova", "authors": "Jana Jankova, Sara van de Geer", "title": "Confidence intervals for high-dimensional inverse covariance estimation", "comments": "26 pages", "journal-ref": "Electronic Journal of Statistics 2015, Vol. 9, No. 1, 1205 - 1229", "doi": "10.1214/15-EJS1031", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose methodology for statistical inference for low-dimensional\nparameters of sparse precision matrices in a high-dimensional setting. Our\nmethod leads to a non-sparse estimator of the precision matrix whose entries\nhave a Gaussian limiting distribution. Asymptotic properties of the novel\nestimator are analyzed for the case of sub-Gaussian observations under a\nsparsity assumption on the entries of the true precision matrix and regularity\nconditions. Thresholding the de-sparsified estimator gives guarantees for edge\nselection in the associated graphical model. Performance of the proposed method\nis illustrated in a simulation study.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 17:08:09 GMT"}, {"version": "v2", "created": "Tue, 11 Aug 2015 20:16:33 GMT"}], "update_date": "2015-08-13", "authors_parsed": [["Jankova", "Jana", ""], ["van de Geer", "Sara", ""]]}, {"id": "1403.6804", "submitter": "Wan Yang", "authors": "Wan Yang, Jeffrey Shaman", "title": "A simple modification for improving inference of non-linear dynamical\n  systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle and ensemble filters are increasingly utilized for inference,\noptimization, and forecast; however, both filtering methods use discrete\ndistributions to simulate continuous state space, a drawback that can lead to\ndegraded performance for non-linear dynamical systems. Here we propose a simple\nmodification, applicable to both particle and ensemble filters, that\ncompensates for this problem. The method randomly replaces one or more model\nvariables or parameters within a fraction of simulated trajectories at each\nfiltering cycle. This modification, termed space re-probing, expands the state\nspace covered by the filter through the introduction of outlying trajectories.\nWe apply the space re-probing modification to three particle filters and three\nensemble filters, and use these modified filters to model and forecast\ninfluenza epidemics. For both filter types, the space re-probing improves\nsimulation of influenza epidemic curves and the prediction of influenza\noutbreak peak timing. Further, as fewer particles are needed for the particle\nfilters, the proposed modification reduces the computational cost of these\nfilters.\n", "versions": [{"version": "v1", "created": "Wed, 26 Mar 2014 19:27:34 GMT"}], "update_date": "2014-03-27", "authors_parsed": [["Yang", "Wan", ""], ["Shaman", "Jeffrey", ""]]}, {"id": "1403.7001", "submitter": "Steven Gustafson Ph.D.", "authors": "Steven C. Gustafson and Leno M. Pedrotti", "title": "Spaghetti prediction: A robust method for forecasting short time series", "comments": "3 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel method for predicting time series is described and demonstrated. This\nmethod inputs time series data points and outputs multiple \"spaghetti\"\nfunctions from which predictions can be made. Spaghetti prediction has\ndesirable properties that are not realized by classic autoregression, moving\naverage, spline, Gaussian process, and other methods. It is particularly\nappropriate for short time series because it allows asymmetric prediction\ndistributions and produces prediction functions which are robust in that they\nuse multiple independent models.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 12:44:32 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Gustafson", "Steven C.", ""], ["Pedrotti", "Leno M.", ""]]}, {"id": "1403.7118", "submitter": "Benjamin Hofner", "authors": "Benjamin Hofner and Thomas Kneib and Torsten Hothorn", "title": "A Unified Framework of Constrained Regression", "comments": "This is a preliminary version of the manuscript. The final\n  publication is available at\n  http://link.springer.com/article/10.1007/s11222-014-9520-y", "journal-ref": null, "doi": "10.1007/s11222-014-9520-y", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized additive models (GAMs) play an important role in modeling and\nunderstanding complex relationships in modern applied statistics. They allow\nfor flexible, data-driven estimation of covariate effects. Yet researchers\noften have a priori knowledge of certain effects, which might be monotonic or\nperiodic (cyclic) or should fulfill boundary conditions. We propose a unified\nframework to incorporate these constraints for both univariate and bivariate\neffect estimates and for varying coefficients. As the framework is based on\ncomponent-wise boosting methods, variables can be selected intrinsically, and\neffects can be estimated for a wide range of different distributional\nassumptions. Bootstrap confidence intervals for the effect estimates are\nderived to assess the models. We present three case studies from environmental\nsciences to illustrate the proposed seamless modeling framework. All discussed\nconstrained effect estimates are implemented in the comprehensive R package\nmboost for model-based boosting.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 16:28:00 GMT"}, {"version": "v2", "created": "Tue, 30 Sep 2014 18:15:49 GMT"}, {"version": "v3", "created": "Wed, 1 Oct 2014 07:51:05 GMT"}, {"version": "v4", "created": "Fri, 7 Nov 2014 10:18:12 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Hofner", "Benjamin", ""], ["Kneib", "Thomas", ""], ["Hothorn", "Torsten", ""]]}, {"id": "1403.7134", "submitter": "Donatello Telesca", "authors": "Yafeng Zhang and Donatello Telesca", "title": "Joint Clustering and Registration of Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Curve registration and clustering are fundamental tools in the analysis of\nfunctional data. While several methods have been developed and explored for\neither task individually, limited work has been done to infer functional\nclusters and register curves simultaneously. We propose a hierarchical model\nfor joint curve clustering and registration. Our proposal combines a Dirichlet\nprocess mixture model for clustering of common shapes, with a reproducing\nkernel representation of phase variability for registration. We show how\ninference can be carried out applying standard posterior simulation algorithms\nand compare our method to several alternatives in both engineered data and a\nbenchmark analysis of the Berkeley growth data. We conclude our investigation\nwith an application to time course gene expression.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 17:17:25 GMT"}], "update_date": "2014-03-28", "authors_parsed": [["Zhang", "Yafeng", ""], ["Telesca", "Donatello", ""]]}, {"id": "1403.7249", "submitter": "Minh Tang", "authors": "Minh Tang, Avanti Athreya, Daniel L. Sussman, Vince Lyzinski, and\n  Carey E. Priebe", "title": "A semiparametric two-sample hypothesis testing problem for random dot\n  product graphs", "comments": "44 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two-sample hypothesis testing for random graphs arises naturally in\nneuroscience, social networks, and machine learning. In this paper, we consider\na semiparametric problem of two-sample hypothesis testing for a class of latent\nposition random graphs. We formulate a notion of consistency in this context\nand propose a valid test for the hypothesis that two finite-dimensional random\ndot product graphs on a common vertex set have the same generating latent\npositions or have generating latent positions that are scaled or diagonal\ntransformations of one another. Our test statistic is a function of a spectral\ndecomposition of the adjacency matrix for each graph and our test procedure is\nconsistent across a broad range of alternatives. We apply our test procedure to\nreal biological data: in a test-retest data set of neural connectome graphs, we\nare able to distinguish between scans from different subjects; and in the {\\em\nC.elegans} connectome, we are able to distinguish between chemical and\nelectrical networks. The latter example is a concrete demonstration that our\ntest can have power even for small sample sizes. We conclude by discussing the\nrelationship between our test procedure and generalized likelihood ratio tests.\n", "versions": [{"version": "v1", "created": "Thu, 27 Mar 2014 23:44:42 GMT"}, {"version": "v2", "created": "Tue, 9 Sep 2014 14:18:40 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 14:57:04 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Tang", "Minh", ""], ["Athreya", "Avanti", ""], ["Sussman", "Daniel L.", ""], ["Lyzinski", "Vince", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1403.7354", "submitter": "Enkelejd Hashorva", "authors": "Krzysztof Debicki, Enkelejd Hashorva, Lanpeng Ji, Chengxiu Ling", "title": "Extremes of Order Statistics of Stationary Processes", "comments": "20 pages, revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\{X_i(t),t\\ge0\\}, 1\\le i\\le n$ be independent copies of a stationary\nprocess $\\{X(t), t\\ge0\\}$. For given positive constants $u,T$, define the set\nof $r$th conjunctions $ C_{r,T,u}:= \\{t\\in [0,T]: X_{r:n}(t) > u\\}$ with\n$X_{r:n}(t)$ the $r$th largest order statistics of $X_1(t), \\ldots , X_n(t),\nt\\ge 0$. In numerous applications such as brain mapping and digital\ncommunication systems, of interest is the approximation of the probability that\nthe set of conjunctions $C_{r,T,u}$ is not empty. Imposing the Albin's\nconditions on $X$, in this paper we obtain an exact asymptotic expansion of\nthis probability as $u$ tends to infinity. Further, we establish the tail\nasymptotics of the supremum of a generalized skew-Gaussian process and a Gumbel\nlimit theorem for the minimum order statistics of stationary Gaussian\nprocesses. As a by-product we derive a version of Li and Shao's normal\ncomparison lemma for the minimum and the maximum of Gaussian random vectors.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 12:10:34 GMT"}, {"version": "v2", "created": "Wed, 6 Aug 2014 16:25:38 GMT"}], "update_date": "2014-08-07", "authors_parsed": [["Debicki", "Krzysztof", ""], ["Hashorva", "Enkelejd", ""], ["Ji", "Lanpeng", ""], ["Ling", "Chengxiu", ""]]}, {"id": "1403.7406", "submitter": "Ragnhild C. Noven", "authors": "Ragnhild C. Noven, Almut E. D. Veraart, Axel Gandy", "title": "A L\\'evy-driven rainfall model with applications to futures pricing", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parsimonious stochastic model for characterising the\ndistributional and temporal properties of rainfall. The model is based on an\nintegrated Ornstein-Uhlenbeck process driven by the Hougaard L\\'evy process. We\nderive properties of this process and propose an extended model which\ngeneralises the Ornstein-Uhlenbeck process to the class of continuous-time ARMA\n(CARMA) processes. The model is illustrated by fitting it to empirical rainfall\ndata on both daily and hourly time scales. It is shown that the model is\nsufficiently flexible to capture important features of the rainfall process\nacross locations and time scales. Finally we study an application to the\npricing of rainfall derivatives which introduces the market price of risk via\nthe Esscher transform. We first give a result specifying the risk-neutral\nexpectation of a general moving average process. Then we illustrate the pricing\nmethod by calculating futures prices based on empirical daily rainfall data,\nwhere the rainfall process is specified by our model.\n", "versions": [{"version": "v1", "created": "Fri, 28 Mar 2014 15:05:08 GMT"}, {"version": "v2", "created": "Sat, 24 Jan 2015 17:16:34 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Noven", "Ragnhild C.", ""], ["Veraart", "Almut E. D.", ""], ["Gandy", "Axel", ""]]}, {"id": "1403.7589", "submitter": "Ryan Martin", "authors": "Ryan Martin and Rama Lingham", "title": "Prior-free probabilistic prediction of future observations", "comments": "21 pages, 3 figures, 2 tables", "journal-ref": "Technometrics, 2016, Vol. 58, Number 2, pages 225--235", "doi": "10.1080/00401706.2015.1017116", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction of future observations is a fundamental problem in statistics.\nHere we present a general approach based on the recently developed inferential\nmodel (IM) framework. We employ an IM-based technique to marginalize out the\nunknown parameters, yielding prior-free probabilistic prediction of future\nobservables. Verifiable sufficient conditions are given for validity of our IM\nfor prediction, and a variety of examples demonstrate the proposed method's\nperformance. Thanks to its generality and ease of implementation, we expect\nthat our IM-based method for prediction will be a useful tool for\npractitioners.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 04:27:55 GMT"}, {"version": "v2", "created": "Sat, 12 Apr 2014 13:39:57 GMT"}, {"version": "v3", "created": "Wed, 10 Dec 2014 13:03:58 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Martin", "Ryan", ""], ["Lingham", "Rama", ""]]}, {"id": "1403.7616", "submitter": "Abhijit Mandal", "authors": "Ayanendranath Basu, Abhijit Mandal, Nirian Martin, Leandro Pardo", "title": "Generalized Wald-type Tests based on Minimum Density Power Divergence\n  Estimators", "comments": "26 pages, 10 figures. arXiv admin note: substantial text overlap with\n  arXiv:1403.0330", "journal-ref": "Statistics, Volume 50, 2016 - Issue 1", "doi": "10.1080/02331888.2015.1016435", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In testing of hypothesis the robustness of the tests is an important concern.\nGenerally, the maximum likelihood based tests are most efficient under standard\nregularity conditions, but they are highly non-robust even under small\ndeviations from the assumed conditions. In this paper we have proposed\ngeneralized Wald-type tests based on minimum density power divergence\nestimators for parametric hypotheses. This method avoids the use of\nnonparametric density estimation and the bandwidth selection. The trade-off\nbetween efficiency and robustness is controlled by a tuning parameter $\\beta$.\nThe asymptotic distributions of the test statistics are chi-square with\nappropriate degrees of freedom. The performance of the proposed tests are\nexplored through simulations and real data analysis.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 10:38:17 GMT"}, {"version": "v2", "created": "Sun, 8 Feb 2015 20:39:08 GMT"}, {"version": "v3", "created": "Sat, 7 Mar 2015 06:45:27 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Basu", "Ayanendranath", ""], ["Mandal", "Abhijit", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1403.7635", "submitter": "Daniel Vogel", "authors": "Alexander D\\\"urre, Daniel Vogel, Roland Fried", "title": "Spatial Sign Correlation", "comments": "20 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new robust correlation estimator based on the spatial sign covariance\nmatrix (SSCM) is proposed. We derive its asymptotic distribution and influence\nfunction at elliptical distributions. Finite sample and robustness properties\nare studied and compared to other robust correlation estimators by means of\nnumerical simulations.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 13:33:17 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["D\u00fcrre", "Alexander", ""], ["Vogel", "Daniel", ""], ["Fried", "Roland", ""]]}, {"id": "1403.7672", "submitter": "Veerabhadran Baladandayuthapani", "authors": "Veerabhadran Baladandayuthapani, Rajesh Talluri, Yuan Ji, Kevin R.\n  Coombes, Yiling Lu, Bryan T. Hennessy, Michael A. Davies, Bani K. Mallick", "title": "Bayesian sparse graphical models for classification with application to\n  protein expression data", "comments": "Published in at http://dx.doi.org/10.1214/14-AOAS722 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2014, Vol. 8, No. 3, 1443-1468", "doi": "10.1214/14-AOAS722", "report-no": "IMS-AOAS-AOAS722", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reverse-phase protein array (RPPA) analysis is a powerful, relatively new\nplatform that allows for high-throughput, quantitative analysis of protein\nnetworks. One of the challenges that currently limit the potential of this\ntechnology is the lack of methods that allow for accurate data modeling and\nidentification of related networks and samples. Such models may improve the\naccuracy of biological sample classification based on patterns of protein\nnetwork activation and provide insight into the distinct biological\nrelationships underlying different types of cancer. Motivated by RPPA data, we\npropose a Bayesian sparse graphical modeling approach that uses selection\npriors on the conditional relationships in the presence of class information.\nThe novelty of our Bayesian model lies in the ability to draw information from\nthe network data as well as from the associated categorical outcome in a\nunified hierarchical model for classification. In addition, our method allows\nfor intuitive integration of a priori network information directly in the model\nand allows for posterior inference on the network topologies both within and\nbetween classes. Applying our methodology to an RPPA data set generated from\npanels of human breast cancer and ovarian cancer cell lines, we demonstrate\nthat the model is able to distinguish the different cancer cell types more\naccurately than several existing models and to identify differential regulation\nof components of a critical signaling network (the PI3K-AKT pathway) between\nthese two types of cancer. This approach represents a powerful new tool that\ncan be used to improve our understanding of protein networks in cancer.\n", "versions": [{"version": "v1", "created": "Sat, 29 Mar 2014 21:25:13 GMT"}, {"version": "v2", "created": "Fri, 21 Nov 2014 14:14:07 GMT"}], "update_date": "2014-11-24", "authors_parsed": [["Baladandayuthapani", "Veerabhadran", ""], ["Talluri", "Rajesh", ""], ["Ji", "Yuan", ""], ["Coombes", "Kevin R.", ""], ["Lu", "Yiling", ""], ["Hennessy", "Bryan T.", ""], ["Davies", "Michael A.", ""], ["Mallick", "Bani K.", ""]]}, {"id": "1403.7701", "submitter": "Qing Mai", "authors": "Qing Mai, Hui Zou", "title": "The fused Kolmogorov filter: A nonparametric model-free screening method", "comments": "Published at http://dx.doi.org/10.1214/14-AOS1303 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 4, 1471-1497", "doi": "10.1214/14-AOS1303", "report-no": "IMS-AOS-AOS1303", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new model-free screening method called the fused Kolmogorov filter is\nproposed for high-dimensional data analysis. This new method is fully\nnonparametric and can work with many types of covariates and response\nvariables, including continuous, discrete and categorical variables. We apply\nthe fused Kolmogorov filter to deal with variable screening problems emerging\nfrom a wide range of applications, such as multiclass classification,\nnonparametric regression and Poisson regression, among others. It is shown that\nthe fused Kolmogorov filter enjoys the sure screening property under weak\nregularity conditions that are much milder than those required for many\nexisting nonparametric screening methods. In particular, the fused Kolmogorov\nfilter can still be powerful when covariates are strongly dependent on each\nother. We further demonstrate the superior performance of the fused Kolmogorov\nfilter over existing screening methods by simulations and real data examples.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 04:20:16 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2015 16:41:02 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2015 07:19:29 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Mai", "Qing", ""], ["Zou", "Hui", ""]]}, {"id": "1403.7711", "submitter": "Alexandros Beskos", "authors": "Alexandros Beskos", "title": "A Stable Manifold MCMC Method for High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine two important recent advancements of MCMC algorithms: first,\nmethods utilizing the intrinsic manifold structure of the parameter space;\nthen, algorithms effective for targets in infinite-dimensions with the critical\nproperty that their mixing time is robust to mesh refinement.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 08:07:56 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Beskos", "Alexandros", ""]]}, {"id": "1403.7812", "submitter": "Rui Zhang", "authors": "Rui Zhang, Kwun Chuen Gary Chan", "title": "Marginalizable conditional model for clustered ordinal data", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a flexible parametric mixed effects model for correlated binary\ndata, with parameters that can be directly interpreted as marginal odds ratios.\nThis leads to a robust estimation equation with an optimal weighting matrix\nbeing the inverse of a genuine model-based covariance matrix. Flexible\ncorrelation structures can be imposed by correlated random effects, and\ncorrelation parameters can be estimated by solving a composite likelihood score\nfunction. Marginal parameters are consistently estimated even when the\nconditional parametric model is misspecified, and the robust estimation\nprocedure has low estimation efficiency loss compared to the maximum likelihood\nestimation under a correct model specification. Simulations, analyses of the\nMadras longitudinal schizophrenia study and British social attributes panel\nsurvey were carried out to demonstrate our method.\n", "versions": [{"version": "v1", "created": "Sun, 30 Mar 2014 21:16:25 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Zhang", "Rui", ""], ["Chan", "Kwun Chuen Gary", ""]]}, {"id": "1403.7853", "submitter": "Yanxun Xu", "authors": "Yanxun Xu and Yuan Ji", "title": "A Latent Gaussian Process Model with Application to Monitoring Clinical\n  Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many clinical trials treatments need to be repeatedly applied as diseases\nrelapse frequently after remission over a long period of time (e.g., 35 weeks).\nMost research in statistics focuses on the overall trial design, such as sample\nsize and power calculation, or on the data analysis after trials are completed.\nLittle is done to improve the efficiency of trial monitoring, such as early\ntermination of trials due to futility. The challenge faced in such trial\nmonitoring is mostly caused by the need to properly model repeated outcomes\nfrom patients. We propose a Bayesian trial monitoring scheme for clinical\ntrials with repeated and potentially cyclic binary outcomes. We construct a\nlatent Gaussian process (LGP) to model discrete longitudinal data in those\ntrials. LGP describes the underlying latent process that gives rise to the\nobserved longitudinal binary outcomes. The posterior consistency property of\nthe proposed model is studied. Posterior inference is conducted with a hybrid\nMonte Carlo algorithm. Simulation studies are conducted under various clinical\nscenarios, and a case study is reported based on a real-life trial.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 02:37:07 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Xu", "Yanxun", ""], ["Ji", "Yuan", ""]]}, {"id": "1403.7890", "submitter": "Xiangyu Chang", "authors": "Xiangyu Chang, Yu Wang, Rongjian Li, Zongben Xu", "title": "Sparse K-Means with $\\ell_{\\infty}/\\ell_0$ Penalty for High-Dimensional\n  Data Clustering", "comments": "36 pages, 4 figures, Present the paper at ICSA 2013", "journal-ref": "Statistica Sinica 28 (2018)1265-1284", "doi": null, "report-no": "SS-2015-0261", "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse clustering, which aims to find a proper partition of an extremely\nhigh-dimensional data set with redundant noise features, has been attracted\nmore and more interests in recent years. The existing studies commonly solve\nthe problem in a framework of maximizing the weighted feature contributions\nsubject to a $\\ell_2/\\ell_1$ penalty. Nevertheless, this framework has two\nserious drawbacks: One is that the solution of the framework unavoidably\ninvolves a considerable portion of redundant noise features in many situations,\nand the other is that the framework neither offers intuitive explanations on\nwhy this framework can select relevant features nor leads to any theoretical\nguarantee for feature selection consistency.\n  In this article, we attempt to overcome those drawbacks through developing a\nnew sparse clustering framework which uses a $\\ell_{\\infty}/\\ell_0$ penalty.\nFirst, we introduce new concepts on optimal partitions and noise features for\nthe high-dimensional data clustering problems, based on which the previously\nknown framework can be intuitively explained in principle. Then, we apply the\nsuggested $\\ell_{\\infty}/\\ell_0$ framework to formulate a new sparse k-means\nmodel with the $\\ell_{\\infty}/\\ell_0$ penalty ($\\ell_0$-k-means for short). We\npropose an efficient iterative algorithm for solving the $\\ell_0$-k-means. To\ndeeply understand the behavior of $\\ell_0$-k-means, we prove that the solution\nyielded by the $\\ell_0$-k-means algorithm has feature selection consistency\nwhenever the data matrix is generated from a high-dimensional Gaussian mixture\nmodel. Finally, we provide experiments with both synthetic data and the Allen\nDeveloping Mouse Brain Atlas data to support that the proposed $\\ell_0$-k-means\nexhibits better noise feature detection capacity over the previously known\nsparse k-means with the $\\ell_2/\\ell_1$ penalty ($\\ell_1$-k-means for short).\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 07:18:55 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Chang", "Xiangyu", ""], ["Wang", "Yu", ""], ["Li", "Rongjian", ""], ["Xu", "Zongben", ""]]}, {"id": "1403.7957", "submitter": "Sam Livingstone", "authors": "Samuel Livingstone and Mark Girolami", "title": "Information-geometric Markov Chain Monte Carlo methods using Diffusions", "comments": "22 pages, 2 figures", "journal-ref": null, "doi": "10.3390/e16063074", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work incorporating geometric ideas in Markov chain Monte Carlo is\nreviewed in order to highlight these advances and their possible application in\na range of domains beyond Statistics. A full exposition of Markov chains and\ntheir use in Monte Carlo simulation for Statistical inference and molecular\ndynamics is provided, with particular emphasis on methods based on Langevin\ndiffusions. After this geometric concepts in Markov chain Monte Carlo are\nintroduced. A full derivation of the Langevin diffusion on a Riemannian\nmanifold is given, together with a discussion of appropriate Riemannian metric\nchoice for different problems. A survey of applications is provided, and some\nopen questions are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 11:35:00 GMT"}, {"version": "v2", "created": "Tue, 1 Apr 2014 09:58:17 GMT"}, {"version": "v3", "created": "Fri, 18 Apr 2014 11:48:19 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Livingstone", "Samuel", ""], ["Girolami", "Mark", ""]]}, {"id": "1403.8120", "submitter": "Florian Heinrichs", "authors": "Holger Dette, Dominik Wied", "title": "Detecting relevant changes in time series models", "comments": "Keywords: change-point analysis, CUSUM, relevant changes, precise\n  hypotheses, strong mixing, weak convergence under the alternative AMS Subject\n  Classification: 62M10, 62F05, 62G10", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the literature on change-point analysis by means of hypothesis\ntesting considers hypotheses of the form H0 : \\theta_1 = \\theta_2 vs. H1 :\n\\theta_1 != \\theta_2, where \\theta_1 and \\theta_2 denote parameters of the\nprocess before and after a change point. This paper takes a different\nperspective and investigates the null hypotheses of no relevant changes, i.e.\nH0 : ||\\theta_1 - \\theta_2|| ? \\leq \\Delta?, where || \\cdot || is an\nappropriate norm. This formulation of the testing problem is motivated by the\nfact that in many applications a modification of the statistical analysis might\nnot be necessary, if the difference between the parameters before and after the\nchange-point is small. A general approach to problems of this type is developed\nwhich is based on the CUSUM principle. For the asymptotic analysis weak\nconvergence of the sequential empirical process has to be established under the\nalternative of non-stationarity, and it is shown that the resulting test\nstatistic is asymptotically normal distributed. Several applications of the\nmethodology are given including tests for relevant changes in the mean,\nvariance, parameter in a linear regression model and distribution function\namong others. The finite sample properties of the new tests are investigated by\nmeans of a simulation study and illustrated by analyzing a data example from\neconomics.\n", "versions": [{"version": "v1", "created": "Mon, 31 Mar 2014 18:34:35 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Dette", "Holger", ""], ["Wied", "Dominik", ""]]}]