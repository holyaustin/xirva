[{"id": "2001.00299", "submitter": "Mohsen Sadatsafavi", "authors": "Mohsen Sadatsafavi, Mohammad Ali Mansournia, Paul Gustafson", "title": "Concentration of Benefit index: A threshold-free summary metric for\n  quantifying the capacity of covariates to yield efficient treatment rules", "comments": "This submission was intended to be an update of the previous work\n  (arXiv:1901.05124) and not a new record. As such, the authors decided to\n  withdraw this record and will update arXiv:1901.05124 with an identical copy", "journal-ref": null, "doi": "10.1002/sim.8481", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When data on treatment assignment, outcomes, and covariates from a randomized\ntrial are available, a question of interest is to what extent covariates can be\nused to optimize treatment decisions. Statistical hypothesis testing of\ncovariate-by-treatment interaction is ill-suited for this purpose. The\napplication of decision theory results in treatment rules that compare the\nexpected benefit of treatment given the patient's covariates against a\ntreatment threshold. However, determining treatment threshold is often\ncontext-specific, and any given threshold might seem arbitrary when the overall\ncapacity towards predicting treatment benefit is of concern. We propose the\nConcentration of Benefit index (Cb), a threshold-free metric that quantifies\nthe combined performance of covariates towards finding individuals who will\nbenefit the most from treatment. The construct of the proposed index is\ncomparing expected treatment outcomes with and without knowledge of covariates\nwhen one of a two randomly selected patients are to be treated. We show that\nthe resulting index can also be expressed in terms of the integrated efficiency\nof individualized treatment decision over the entire range of treatment\nthresholds. We propose parametric and semi-parametric estimators, the latter\nbeing suitable for out-of-sample validation and correction for optimism. We\nused data from a clinical trial to demonstrate the calculations in a\nstep-by-step fashion, and have provided the R code for implementation\n(https://github.com/msadatsafavi/txBenefit). The proposed index has intuitive\nand theoretically sound interpretation and can be estimated with relative ease\nfor a wide class of regression models. Beyond the conceptual developments,\nvarious aspects of estimation and inference for such a metric need to be\npursued in future research.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 02:38:52 GMT"}, {"version": "v2", "created": "Tue, 7 Jan 2020 18:43:46 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Sadatsafavi", "Mohsen", ""], ["Mansournia", "Mohammad Ali", ""], ["Gustafson", "Paul", ""]]}, {"id": "2001.00412", "submitter": "Moritz N. Lang", "authors": "Moritz N. Lang, Lisa Schlosser, Torsten Hothorn, Georg J. Mayr, Reto\n  Stauffer, Achim Zeileis", "title": "Circular Regression Trees and Forests with an Application to\n  Probabilistic Wind Direction Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While circular data occur in a wide range of scientific fields, the\nmethodology for distributional modeling and probabilistic forecasting of\ncircular response variables is rather limited. Most of the existing methods are\nbuilt on the framework of generalized linear and additive models, which are\noften challenging to optimize and to interpret. Therefore, we suggest circular\nregression trees and random forests as an intuitive alternative approach that\nis relatively easy to fit. Building on previous ideas for trees modeling\ncircular means, we suggest a distributional approach for both trees and forests\nyielding probabilistic forecasts based on the von Mises distribution. The\nresulting tree-based models simplify the estimation process by using the\navailable covariates for partitioning the data into sufficiently homogeneous\nsubgroups so that a simple von Mises distribution without further covariates\ncan be fitted to the circular response in each subgroup. These circular\nregression trees are straightforward to interpret, can capture nonlinear\neffects and interactions, and automatically select the relevant covariates that\nare associated with either location and/or scale changes in the von Mises\ndistribution. Combining an ensemble of circular regression trees to a circular\nregression forest yields a local adaptive likelihood estimator for the von\nMises distribution that can regularize and smooth the covariate effects. The\nnew methods are evaluated in a case study on probabilistic wind direction\nforecasting at two Austrian airports, considering other common approaches as a\nbenchmark.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 12:36:26 GMT"}], "update_date": "2020-01-03", "authors_parsed": [["Lang", "Moritz N.", ""], ["Schlosser", "Lisa", ""], ["Hothorn", "Torsten", ""], ["Mayr", "Georg J.", ""], ["Stauffer", "Reto", ""], ["Zeileis", "Achim", ""]]}, {"id": "2001.00419", "submitter": "Weichi Wu", "authors": "Holger Dette, Weichi Wu", "title": "Prediction in locally stationary time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop an estimator for the high-dimensional covariance matrix of a\nlocally stationary process with a smoothly varying trend and use this statistic\nto derive consistent predictors in non-stationary time series. In contrast to\nthe currently available methods for this problem the predictor developed here\ndoes not rely on fitting an autoregressive model and does not require a\nvanishing trend. The finite sample properties of the new methodology are\nillustrated by means of a simulation study and a financial indices study.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 13:03:14 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2020 00:13:00 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Dette", "Holger", ""], ["Wu", "Weichi", ""]]}, {"id": "2001.00672", "submitter": "Kerry Sun", "authors": "Kerry Sun and Demoz Gebre-Egziabher", "title": "A Two-Stage Batch Algorithm for Nonlinear Static Parameter Estimation", "comments": "Accepted by AIAA Journal of Guidance, Control and Dynamics, Dec 2019", "journal-ref": null, "doi": "10.2514/1.G004713", "report-no": null, "categories": "eess.SP cs.SY eess.SY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-stage batch estimation algorithm for solving a class of nonlinear,\nstatic parameter estimation problems that appear in aerospace engineering\napplications is proposed. It is shown how these problems can be recast into a\nform suitable for the proposed two-stage estimation process. In the first\nstage, linear least squares is used to obtain a subset of the unknown\nparameters (set 1), while a residual sampling procedure is used for selecting\ninitial values for the rest of the parameters (set 2). In the second stage,\ndepending on the uniqueness of the local minimum, either only the parameters in\nthe second set need to be re-estimated, or all the parameters will have to be\nre-estimated simultaneously, by a nonlinear constrained optimization. The\nestimates from the first stage are used as initial conditions for the second\nstage optimizer. It is shown that this approach alleviates the sensitivity to\ninitial conditions and minimizes the likelihood of converging to an incorrect\nlocal minimum of the nonlinear cost function. An error bound analysis is\npresented to show that the first stage can be solved in such a way that the\ntotal cost function will be driven to the optimal cost, and the difference has\nan upper bound. Two tutorial examples are used to show how to implement this\nestimator and compare its performance to other similar nonlinear estimators.\nFinally, the estimator is used on a 5-hole Pitot tube calibration problem using\nflight test data collected from a small Unmanned Aerial Vehicle (UAV) which\ncannot be easily solved with single-stage methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 00:02:42 GMT"}], "update_date": "2020-02-18", "authors_parsed": [["Sun", "Kerry", ""], ["Gebre-Egziabher", "Demoz", ""]]}, {"id": "2001.00719", "submitter": "Xiwei Tang", "authors": "Xiwei Tang and Lexin Li", "title": "Multivariate Temporal Point Process Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point process modeling is gaining increasing attention, as point process type\ndata are emerging in numerous scientific applications. In this article,\nmotivated by a neuronal spike trains study, we propose a novel point process\nregression model, where both the response and the predictor can be a\nhigh-dimensional point process. We model the predictor effects through the\nconditional intensities using a set of basis transferring functions in a\nconvolutional fashion. We organize the corresponding transferring coefficients\nin the form of a three-way tensor, then impose the low-rank, sparsity, and\nsubgroup structures on this coefficient tensor. These structures help reduce\nthe dimensionality, integrate information across different individual\nprocesses, and facilitate the interpretation. We develop a highly scalable\noptimization algorithm for parameter estimation. We derive the large sample\nerror bound for the recovered coefficient tensor, and establish the subgroup\nidentification consistency, while allowing the dimension of the multivariate\npoint process to diverge. We demonstrate the efficacy of our method through\nboth simulations and a cross-area neuronal spike trains analysis in a sensory\ncortex study.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 04:28:01 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 19:38:00 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Tang", "Xiwei", ""], ["Li", "Lexin", ""]]}, {"id": "2001.00727", "submitter": "Genshiro Kitagawa", "authors": "Genshiro Kitagawa (The University of Tokyo)", "title": "Pearson chi^2-divergence Approach to Gaussian Mixture Reduction and its\n  Application to Gaussian-sum Filter and Smoother", "comments": "20 pages, 8 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": "Meiji University MIMS-RPB Statistics & Data Science Series (SDS-12)", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gaussian mixture distribution is important in various statistical\nproblems. In particular it is used in the Gaussian-sum filter and smoother for\nlinear state-space model with non-Gaussian noise inputs. However, for this\nmethod to be practical, an efficient method of reducing the number of Gaussian\ncomponents is necessary. In this paper, we show that a closed form expression\nof Pearson chi^2-divergence can be obtained and it can apply to the\ndetermination of the pair of two Gaussian components in sequential reduction of\nGaussian components. By numerical examples for one dimensional and two\ndimensional distribution models, it will be shown that in most cases the\nproposed criterion performed almost equally as the Kullback-Libler divergence,\nfor which computationally costly numerical integration is necessary.\nApplication to Gaussian-sum filtering and smoothing is also shown.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 05:24:31 GMT"}], "update_date": "2020-01-06", "authors_parsed": [["Kitagawa", "Genshiro", "", "The University of Tokyo"]]}, {"id": "2001.00811", "submitter": "Georgia Papacharalampous", "authors": "Georgia Papacharalampous, Hristos Tyralis", "title": "Hydrological time series forecasting using simple combinations: Big data\n  testing and investigations on one-year ahead river flow predictability", "comments": null, "journal-ref": "Journal of Hydrology 590 (2020) 125205", "doi": "10.1016/j.jhydrol.2020.125205", "report-no": null, "categories": "stat.AP cs.LG stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Delivering useful hydrological forecasts is critical for urban and\nagricultural water management, hydropower generation, flood protection and\nmanagement, drought mitigation and alleviation, and river basin planning and\nmanagement, among others. In this work, we present and appraise a new simple\nand flexible methodology for hydrological time series forecasting. This\nmethodology relies on (a) at least two individual forecasting methods and (b)\nthe median combiner of forecasts. The appraisal is made by using a big dataset\nconsisted of 90-year-long mean annual river flow time series from approximately\n600 stations. Covering large parts of North America and Europe, these stations\nrepresent various climate and catchment characteristics, and thus can\ncollectively support benchmarking. Five individual forecasting methods and 26\nvariants of the introduced methodology are applied to each time series. The\napplication is made in one-step ahead forecasting mode. The individual methods\nare the last-observation benchmark, simple exponential smoothing, complex\nexponential smoothing, automatic autoregressive fractionally integrated moving\naverage (ARFIMA) and Facebook's Prophet, while the 26 variants are defined by\nall the possible combinations (per two, three, four or five) of the five\nafore-mentioned methods. The new methodology is identified as well-performing\nin the long run, especially when more than two individual forecasting methods\nare combined within its framework. Moreover, the possibility of case-informed\nintegrations of diverse hydrological forecasting methods within systematic\nframeworks is algorithmically investigated and discussed. The related\ninvestigations encompass linear regression analyses, which aim at finding\ninterpretable relationships between the values of a representative forecasting\nperformance metric and the values of selected river flow statistics...\n", "versions": [{"version": "v1", "created": "Thu, 2 Jan 2020 18:45:43 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2020 16:58:31 GMT"}], "update_date": "2020-08-19", "authors_parsed": [["Papacharalampous", "Georgia", ""], ["Tyralis", "Hristos", ""]]}, {"id": "2001.00915", "submitter": "Xianzheng Huang", "authors": "Dewei Wang, Xichen Mou, Xiang Li, and Xianzheng Huang", "title": "Local polynomial regression for pooled response data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose local polynomial estimators for the conditional mean of a\ncontinuous response when only pooled response data are collected under\ndifferent pooling designs. Asymptotic properties of these estimators are\ninvestigated and compared. Extensive simulation studies are carried out to\ncompare finite sample performance of the proposed estimators under various\nmodel settings and pooling strategies. We apply the proposed local polynomial\nregression methods to two real-life applications to illustrate practical\nimplementation and performance of the estimators for the mean function.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 18:05:46 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 14:40:17 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Wang", "Dewei", ""], ["Mou", "Xichen", ""], ["Li", "Xiang", ""], ["Huang", "Xianzheng", ""]]}, {"id": "2001.00944", "submitter": "Lidia Sacchetto", "authors": "Mauro Gasparini and Lidia Sacchetto", "title": "On the definition of a concentration function relevant to the ROC curve", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a reader's reaction to a recent paper by E. Schechtman and G.\nSchechtman (Metron, 2019) about the correct definition of a concentration\nfunction for the diagnostic, i.e. supervised classification, problem. We\npropose and motivate a different definition and refer to the relevant\nliterature.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 15:56:45 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Gasparini", "Mauro", ""], ["Sacchetto", "Lidia", ""]]}, {"id": "2001.00980", "submitter": "M{\\aa}ns Magnusson", "authors": "M{\\aa}ns Magnusson, Michael Riis Andersen, Johan Jonasson, and Aki\n  Vehtari", "title": "Leave-One-Out Cross-Validation for Bayesian Model Comparison in Large\n  Data", "comments": null, "journal-ref": "Proceedings of the 23rd International Conference on Artificial\n  Intelligence and Statistics (AISTATS), PMLR 108:341-351, 2020", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, new methods for model assessment, based on subsampling and\nposterior approximations, have been proposed for scaling leave-one-out\ncross-validation (LOO) to large datasets. Although these methods work well for\nestimating predictive performance for individual models, they are less powerful\nin model comparison. We propose an efficient method for estimating differences\nin predictive performance by combining fast approximate LOO surrogates with\nexact LOO subsampling using the difference estimator and supply proofs with\nregards to scaling characteristics. The resulting approach can be orders of\nmagnitude more efficient than previous approaches, as well as being better\nsuited to model comparison.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jan 2020 20:47:45 GMT"}], "update_date": "2020-08-12", "authors_parsed": [["Magnusson", "M\u00e5ns", ""], ["Andersen", "Michael Riis", ""], ["Jonasson", "Johan", ""], ["Vehtari", "Aki", ""]]}, {"id": "2001.01095", "submitter": "Cencheng Shen", "authors": "Cencheng Shen", "title": "High-Dimensional Independence Testing and Maximum Marginal Correlation", "comments": "20 pages, 5 page appendix, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of universally consistent dependence measures have been recently\nproposed for testing independence, such as distance correlation, kernel\ncorrelation, multiscale graph correlation, etc. They provide a satisfactory\nsolution for dependence testing in low-dimensions, but often exhibit decreasing\npower for high-dimensional data, a phenomenon that has been recognized but\nremains mostly unchartered. In this paper, we aim to better understand the\nhigh-dimensional testing scenarios and explore a procedure that is robust\nagainst increasing dimension. To that end, we propose the maximum marginal\ncorrelation method and characterize high-dimensional dependence structures via\nthe notion of dependent dimensions. We prove that the maximum method can be\nvalid and universally consistent for testing high-dimensional dependence under\nregularity conditions, and demonstrate when and how the maximum method may\noutperform other methods. The methodology can be implemented by most existing\ndependence measures, has a superior testing power in a variety of common\nhigh-dimensional settings, and is computationally efficient for big data\nanalysis when using the distance correlation chi-square test.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 16:21:50 GMT"}], "update_date": "2020-01-07", "authors_parsed": [["Shen", "Cencheng", ""]]}, {"id": "2001.01116", "submitter": "Zijian Zeng", "authors": "Zijian Zeng, Meng Li", "title": "Bayesian Median Autoregression for Robust Time Series Forecasting", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijforecast.2020.11.002", "report-no": null, "categories": "stat.AP econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian median autoregressive (BayesMAR) model for time series\nforecasting. The proposed method utilizes time-varying quantile regression at\nthe median, favorably inheriting the robustness of median regression in\ncontrast to the widely used mean-based methods. Motivated by a working Laplace\nlikelihood approach in Bayesian quantile regression, BayesMAR adopts a\nparametric model bearing the same structure as autoregressive models by\naltering the Gaussian error to Laplace, leading to a simple, robust, and\ninterpretable modeling strategy for time series forecasting. We estimate model\nparameters by Markov chain Monte Carlo. Bayesian model averaging is used to\naccount for model uncertainty, including the uncertainty in the autoregressive\norder, in addition to a Bayesian model selection approach. The proposed methods\nare illustrated using simulations and real data applications. An application to\nU.S. macroeconomic data forecasting shows that BayesMAR leads to favorable and\noften superior predictive performance compared to the selected mean-based\nalternatives under various loss functions that encompass both point and\nprobabilistic forecasts. The proposed methods are generic and can be used to\ncomplement a rich class of methods that build on autoregressive models.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 19:44:33 GMT"}, {"version": "v2", "created": "Sat, 5 Dec 2020 15:07:07 GMT"}], "update_date": "2020-12-08", "authors_parsed": [["Zeng", "Zijian", ""], ["Li", "Meng", ""]]}, {"id": "2001.01130", "submitter": "Adam Kashlak", "authors": "Adam B Kashlak, Sergii Myroshnychenko, Susanna Spektor", "title": "Analytic Permutation Testing via Kahane--Khintchine Inequalities", "comments": "35 pages, 11 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.FA math.PR stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The permutation test is a versatile type of exact nonparametric significance\ntest that requires drastically fewer assumptions than similar parametric tests\nby considering the distribution of a test statistic over a discrete group of\ndistributionally invariant transformations. The main downfall of the\npermutation test is the high computational cost of running such a test making\nthis approach laborious for complex data and experimental designs and\ncompletely infeasible in any application requiring speedy results. We rectify\nthis problem through application of Kahane--Khintchine-type inequalities under\na weak dependence condition and thus propose a computation free permutation\ntest---i.e. a permutation-less permutation test. This general framework is\nstudied within both commutative and non-commutative Banach spaces. We further\nimprove these Kahane-Khintchine-type bounds via a transformation based on the\nincomplete beta function and Talagrand's concentration inequality. For\n$k$-sample testing, we extend the theory presented for Rademacher sums to\nweakly dependent Rademacher chaoses making use of modified decoupling\ninequalities. We test this methodology on classic functional data sets\nincluding the Berkeley growth curves and the phoneme dataset. We also consider\nhypothesis testing on speech samples under two experimental designs: the Latin\nsquare and the complete randomized block design.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 21:24:50 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2020 18:44:41 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Kashlak", "Adam B", ""], ["Myroshnychenko", "Sergii", ""], ["Spektor", "Susanna", ""]]}, {"id": "2001.01434", "submitter": "Jinfeng Xu", "authors": "Jinfeng Xu, Zhiliang Ying and Na Zhao", "title": "Scalable Estimation and Inference with Large-scale or Online Survival\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of data collection and aggregation technologies in\nmany scientific disciplines, it is becoming increasingly ubiquitous to conduct\nlarge-scale or online regression to analyze real-world data and unveil\nreal-world evidence. In such applications, it is often numerically challenging\nor sometimes infeasible to store the entire dataset in memory. Consequently,\nclassical batch-based estimation methods that involve the entire dataset are\nless attractive or no longer applicable. Instead, recursive estimation methods\nsuch as stochastic gradient descent that process data points sequentially are\nmore appealing, exhibiting both numerical convenience and memory efficiency. In\nthis paper, for scalable estimation of large or online survival data, we\npropose a stochastic gradient descent method which recursively updates the\nestimates in an online manner as data points arrive sequentially in streams.\nTheoretical results such as asymptotic normality and estimation efficiency are\nestablished to justify its validity. Furthermore, to quantify the uncertainty\nassociated with the proposed stochastic gradient descent estimator and\nfacilitate statistical inference, we develop a scalable resampling strategy\nthat specifically caters to the large-scale or online setting. Simulation\nstudies and a real data application are also provided to assess its performance\nand illustrate its practical utility.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 08:28:50 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 03:47:29 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Xu", "Jinfeng", ""], ["Ying", "Zhiliang", ""], ["Zhao", "Na", ""]]}, {"id": "2001.01466", "submitter": "Jesse Hemerik", "authors": "Jesse Hemerik, Magne Thoresen and Livio Finos", "title": "Permutation testing in high-dimensional linear models: an empirical\n  investigation", "comments": "Accepted for publication in Journal of Statistical Computation and\n  Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Permutation testing in linear models, where the number of nuisance\ncoefficients is smaller than the sample size, is a well-studied topic. The\ncommon approach of such tests is to permute residuals after regressing on the\nnuisance covariates. Permutation-based tests are valuable in particular because\nthey can be highly robust to violations of the standard linear model, such as\nnon-normality and heteroscedasticity. Moreover, in some cases they can be\ncombined with existing, powerful permutation-based multiple testing methods.\nHere, we propose permutation tests for models where the number of nuisance\ncoefficients exceeds the sample size. The performance of the novel tests is\ninvestigated with simulations. In a wide range of simulation scenarios our\nproposed permutation methods provided appropriate type I error rate control,\nunlike some competing tests, while having good power.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 10:20:22 GMT"}, {"version": "v2", "created": "Thu, 8 Oct 2020 10:41:37 GMT"}], "update_date": "2020-10-09", "authors_parsed": [["Hemerik", "Jesse", ""], ["Thoresen", "Magne", ""], ["Finos", "Livio", ""]]}, {"id": "2001.01541", "submitter": "Ningning Xu", "authors": "Ningning Xu, Aldo Solari and Jelle Goeman", "title": "Pathway Testing in Metabolomics with Globaltest, Allowing Post Hoc\n  Choice of Pathways", "comments": "33 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Globaltest is a powerful test for the global null hypothesis that there\nis no association between a group of features and a response of interest, which\nis popular in pathway testing in metabolomics. Evaluating multiple pathways,\nhowever, requires multiple testing correction. In this paper, we propose a\nmultiple testing method, based on closed testing, specifically designed for the\nGlobaltest. The proposed method controls the family-wise error rate\nsimultaneously over all possible feature sets, and therefore allows post hoc\ninference, i.e. the researcher may choose the pathway database after seeing the\ndata without jeopardizing error control. To circumvent the exponential\ncomputation time of closed testing, we derive a novel shortcut that allows\nexact closed testing to be performed on the scale of metabolomics data. An R\npackage ctgt is available on CRAN. We illustrate the shortcut on several\nmetabolomics data examples.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 13:06:28 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 12:25:15 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Xu", "Ningning", ""], ["Solari", "Aldo", ""], ["Goeman", "Jelle", ""]]}, {"id": "2001.01676", "submitter": "Beniamino Hadj-Amar", "authors": "Beniamino Hadj-Amar, B\\\"arbel Finkenst\\\"adt, Mark Fiecas, and Robert\n  Huckstepp", "title": "Identifying the Recurrence of Sleep Apnea Using a Harmonic Hidden Markov\n  Model", "comments": null, "journal-ref": "Annals of Applied Statistics, 2021", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to model time-varying periodic and oscillatory processes by means\nof a hidden Markov model where the states are defined through the spectral\nproperties of a periodic regime. The number of states is unknown along with the\nrelevant periodicities, the role and number of which may vary across states. We\naddress this inference problem by a Bayesian nonparametric hidden Markov model\nassuming a sticky hierarchical Dirichlet process for the switching dynamics\nbetween different states while the periodicities characterizing each state are\nexplored by means of a trans-dimensional Markov chain Monte Carlo sampling\nstep. We develop the full Bayesian inference algorithm and illustrate the use\nof our proposed methodology for different simulation studies as well as an\napplication related to respiratory research which focuses on the detection of\napnea instances in human breathing traces.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jan 2020 17:34:23 GMT"}, {"version": "v2", "created": "Wed, 15 Jan 2020 16:37:43 GMT"}, {"version": "v3", "created": "Fri, 5 Jun 2020 08:17:07 GMT"}, {"version": "v4", "created": "Thu, 18 Mar 2021 13:01:58 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Hadj-Amar", "Beniamino", ""], ["Finkenst\u00e4dt", "B\u00e4rbel", ""], ["Fiecas", "Mark", ""], ["Huckstepp", "Robert", ""]]}, {"id": "2001.01890", "submitter": "Elynn Chen", "authors": "Elynn Y. Chen, Jianqing Fan, Ellen Li", "title": "Statistical Inference for High-Dimensional Matrix-Variate Factor Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the estimation and inference of the low-rank components\nin high-dimensional matrix-variate factor models, where each dimension of the\nmatrix-variates ($p \\times q$) is comparable to or greater than the number of\nobservations ($T$). We propose an estimation method called $\\alpha$-PCA that\npreserves the matrix structure and aggregates mean and contemporary covariance\nthrough a hyper-parameter $\\alpha$. We develop an inferential theory,\nestablishing consistency, the rate of convergence, and the limiting\ndistributions, under general conditions that allow for correlations across\ntime, rows, or columns of the noise. We show both theoretical and empirical\nmethods of choosing the best $\\alpha$, depending on the use-case criteria.\nSimulation results demonstrate the adequacy of the asymptotic results in\napproximating the finite sample properties. The $\\alpha$-PCA compares favorably\nwith the existing ones. Finally, we illustrate its applications with a real\nnumeric data set and two real image data sets. In all applications, the\nproposed estimation procedure outperforms previous methods in the power of\nvariance explanation using out-of-sample 10-fold cross-validation.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 05:03:47 GMT"}, {"version": "v2", "created": "Wed, 21 Oct 2020 21:01:34 GMT"}], "update_date": "2020-10-23", "authors_parsed": [["Chen", "Elynn Y.", ""], ["Fan", "Jianqing", ""], ["Li", "Ellen", ""]]}, {"id": "2001.02121", "submitter": "Alexander M\\\"arz", "authors": "Alexander M\\\"arz", "title": "CatBoostLSS -- An extension of CatBoost to probabilistic forecasting", "comments": "CatBoost, Distributional Modelling, Expectile Regression, GAMLSS,\n  Probabilistic Forecast, Statistical Machine Learning, Uncertainty\n  Quantification. arXiv admin note: substantial text overlap with\n  arXiv:1907.03178", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework of CatBoost that predicts the entire conditional\ndistribution of a univariate response variable. In particular, CatBoostLSS\nmodels all moments of a parametric distribution (i.e., mean, location, scale\nand shape [LSS]) instead of the conditional mean only. Choosing from a wide\nrange of continuous, discrete and mixed discrete-continuous distributions,\nmodelling and predicting the entire conditional distribution greatly enhances\nthe flexibility of CatBoost, as it allows to gain insight into the data\ngenerating process, as well as to create probabilistic forecasts from which\nprediction intervals and quantiles of interest can be derived. We present both\na simulation study and real-world examples that demonstrate the benefits of our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jan 2020 15:42:44 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["M\u00e4rz", "Alexander", ""]]}, {"id": "2001.02168", "submitter": "Chris Sherlock Dr.", "authors": "Chris Sherlock and Andrew Golightly", "title": "Exact Bayesian inference for discretely observed Markov Jump Processes\n  using finite rate matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new methodologies for Bayesian inference on the rate parameters of\na discretely observed continuous-time Markov jump processes with a countably\ninfinite state space. The usual method of choice for inference, particle Markov\nchain Monte Carlo (particle MCMC), struggles when the observation noise is\nsmall. We consider the most challenging regime of exact observations and\nprovide two new methodologies for inference in this case: the minimal extended\nstate space algorithm (MESA) and the nearly minimal extended state space\nalgorithm (nMESA). By extending the Markov chain Monte Carlo state space, both\nMESA and nMESA use the exponentiation of finite rate matrices to perform exact\nBayesian inference on the Markov jump process even though its state space is\ncountably infinite. Numerical experiments show improvements over particle MCMC\nof between a factor of three and several orders of magnitude.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2019 15:58:52 GMT"}], "update_date": "2020-01-08", "authors_parsed": [["Sherlock", "Chris", ""], ["Golightly", "Andrew", ""]]}, {"id": "2001.02229", "submitter": "Rahul Roy", "authors": "Rahul Roy, Subir Kumar Bhandari", "title": "Asymptotically optimal test for dependent multiple testing set up", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the behaviour of dependent test statistics for\ntesting of multiple hypothesis . To keep simplicity, we have considered a\nmixture normal model with equicorrelated correlation set up. With a simple\nlinear transformation,the test statistics were decomposed into independent\ncomponents, which, when conditioned appropriately generated independent\nvariables. These were used to construct conditional tests , which were shown to\nbe asymptotically optimal with power as large as that obtained using N.P.Lemma.\nWe have pursued extensive simulation to support the claim.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 09:49:24 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Roy", "Rahul", ""], ["Bhandari", "Subir Kumar", ""]]}, {"id": "2001.02250", "submitter": "Yuan Yan", "authors": "Yuan Yan, Hsin-Cheng Huang, Marc G. Genton", "title": "Vector Autoregressive Models with Spatially Structured Coefficients for\n  Time Series on a Spatial Grid", "comments": null, "journal-ref": "Journal of Agricultural, Biological and Environmental Statistics,\n  2021", "doi": "10.1007/s13253-021-00444-4", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a parsimonious spatiotemporal model for time series data on a\nspatial grid. Our model is capable of dealing with high-dimensional time series\ndata that may be collected at hundreds of locations and capturing the spatial\nnon-stationarity. In essence, our model is a vector autoregressive model that\nutilizes the spatial structure to achieve parsimony of autoregressive matrices\nat two levels. The first level ensures the sparsity of the autoregressive\nmatrices using a lagged-neighborhood scheme. The second level performs a\nspatial clustering of the non-zero autoregressive coefficients such that nearby\nlocations share similar coefficients. This model is interpretable and can be\nused to identify geographical subregions, within each of which, the time series\nshare similar dynamical behavior with homogeneous autoregressive coefficients.\nThe model parameters are obtained using the penalized maximum likelihood with\nan adaptive fused Lasso penalty. The estimation procedure is easy to implement\nand can be tailored to the need of a modeler. We illustrate the performance of\nthe proposed estimation algorithm in a simulation study. We apply our model to\na wind speed time series dataset generated from a climate model over Saudi\nArabia to illustrate its usefulness. Limitations and possible extensions of our\nmethod are also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 19:11:14 GMT"}, {"version": "v2", "created": "Sun, 28 Feb 2021 17:54:56 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yan", "Yuan", ""], ["Huang", "Hsin-Cheng", ""], ["Genton", "Marc G.", ""]]}, {"id": "2001.02285", "submitter": "Adam Groce", "authors": "Wenxin Du, Canyon Foot, Monica Moniot, Andrew Bray, Adam Groce", "title": "Differentially Private Confidence Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence intervals for the population mean of normally distributed data are\nsome of the most standard statistical outputs one might want from a database.\nIn this work we give practical differentially private algorithms for this task.\nWe provide five algorithms and then compare them to each other and to prior\nwork. We give concrete, experimental analysis of their accuracy and find that\nour algorithms provide much more accurate confidence intervals than prior work.\nFor example, in one setting (with {\\epsilon} = 0.1 and n = 2782) our algorithm\nyields an interval that is only 1/15th the size of the standard set by prior\nwork.\n", "versions": [{"version": "v1", "created": "Tue, 7 Jan 2020 21:49:32 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Du", "Wenxin", ""], ["Foot", "Canyon", ""], ["Moniot", "Monica", ""], ["Bray", "Andrew", ""], ["Groce", "Adam", ""]]}, {"id": "2001.02342", "submitter": "Han Lin Shang", "authors": "Ufuk Beyaztas, Han Lin Shang, Abdel-Salam G. Abdel-Salam", "title": "Functional linear models for interval-valued data", "comments": "26 pages, 8 figures, to be published at Communications in Statistics\n  - Simulation and Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregation of large databases in a specific format is a frequently used\nprocess to make the data easily manageable. Interval-valued data is one of the\ndata types that is generated by such an aggregation process. Using traditional\nmethods to analyze interval-valued data results in loss of information, and\nthus, several interval-valued data models have been proposed to gather reliable\ninformation from such data types. On the other hand, recent technological\ndevelopments have led to high dimensional and complex data in many application\nareas, which may not be analyzed by traditional techniques. Functional data\nanalysis is one of the most commonly used techniques to analyze such complex\ndatasets. While the functional extensions of much traditional statistical\ntechniques are available, the functional form of the interval-valued data has\nnot been studied well. This paper introduces the functional forms of some\nwell-known regression models that take interval-valued data. The proposed\nmethods are based on the function-on-function regression model, where both the\nresponse and predictor/s are functional. Through several Monte Carlo\nsimulations and empirical data analysis, the finite sample performance of the\nproposed methods is evaluated and compared with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 02:44:12 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Beyaztas", "Ufuk", ""], ["Shang", "Han Lin", ""], ["Abdel-Salam", "Abdel-Salam G.", ""]]}, {"id": "2001.02393", "submitter": "Hugo Lewi Hammer Dr.", "authors": "Hugo Lewi Hammer, Anis Yazidi and H{\\aa}vard Rue", "title": "Estimating Tukey Depth Using Incremental Quantile Estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of depth represents methods to measure how deep an arbitrary\npoint is positioned in a dataset and can be seen as the opposite of\noutlyingness. It has proved very useful and a wide range of methods have been\ndeveloped based on the concept.\n  To address the well-known computational challenges associated with the depth\nconcept, we suggest to estimate Tukey depth contours using recently developed\nincremental quantile estimators. The suggested algorithm can estimate depth\ncontours when the dataset in known in advance, but also recursively update and\neven track Tukey depth contours for dynamically varying data stream\ndistributions. Tracking was demonstrated in a real-life data example where\nchanges in human activity was detected in real-time from accelerometer\nobservations.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 06:39:39 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Hammer", "Hugo Lewi", ""], ["Yazidi", "Anis", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "2001.02430", "submitter": "Subhajit Dutta Dr.", "authors": "Sarbojit Roy, Soham Sarkar and Subhajit Dutta", "title": "On a Generalization of the Average Distance Classifier", "comments": "Short version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high dimension, low sample size (HDLSS)settings, the simple average\ndistance classifier based on the Euclidean distance performs poorly if\ndifferences between the locations get masked by the scale differences. To\nrectify this issue, modifications to the average distance classifier was\nproposed by Chan and Hall (2009). However, the existing classifiers cannot\ndiscriminate when the populations differ in other aspects than locations and\nscales. In this article, we propose some simple transformations of the average\ndistance classifier to tackle this issue. The resulting classifiers perform\nquite well even when the underlying populations have the same location and\nscale. The high-dimensional behaviour of the proposed classifiers is studied\ntheoretically. Numerical experiments with a variety of simulated as well as\nreal data sets exhibit the usefulness of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 10:00:55 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Roy", "Sarbojit", ""], ["Sarkar", "Soham", ""], ["Dutta", "Subhajit", ""]]}, {"id": "2001.02466", "submitter": "Zheng Zhao", "authors": "Zheng Zhao, Toni Karvonen, Roland Hostettler, Simo S\\\"arkk\\\"a", "title": "Taylor Moment Expansion for Continuous-Discrete Gaussian Filtering and\n  Smoothing", "comments": "Submitted to IEEE Transactions on Automatic Control. Code is\n  available at (once accepted for publication)\n  https://github.com/zgbkdlm/TME-filter-smoother", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper is concerned with non-linear Gaussian filtering and smoothing in\ncontinuous-discrete state-space models, where the dynamic model is formulated\nas an It\\^{o} stochastic differential equation (SDE), and the measurements are\nobtained at discrete time instants. We propose novel Taylor moment expansion\n(TME) Gaussian filter and smoother which approximate the moments of the SDE\nwith a temporal Taylor expansion. Differently from classical linearisation or\nIt\\^{o}--Taylor approaches, the Taylor expansion is formed for the moment\nfunctions directly and in time variable, not by using a Taylor expansion on the\nnon-linear functions in the model. We analyse the theoretical properties,\nincluding the positive definiteness of the covariance estimate and stability of\nthe TME Gaussian filter and smoother. By numerical experiments, we demonstrate\nthat the proposed TME Gaussian filter and smoother significantly outperform the\nstate-of-the-art methods in terms of estimation accuracy and numerical\nstability.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 11:59:59 GMT"}], "update_date": "2020-08-13", "authors_parsed": [["Zhao", "Zheng", ""], ["Karvonen", "Toni", ""], ["Hostettler", "Roland", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "2001.02488", "submitter": "Marc Ditzhaus", "authors": "Marc Ditzhaus and Daniel Gaigall", "title": "Testing marginal homogeneity in Hilbert spaces with applications to\n  stock market returns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers a paired data framework and discuss the question of\nmarginal homogeneity of bivariate high dimensional or functional data. The\nrelated testing problem can be endowed into a more general setting for paired\nrandom variables taking values in a general Hilbert space. To address this\nproblem, a Cramer-von-Mises type test statistic is applied and a bootstrap\nprocedure is suggested to obtain critical values and finally a consistent test.\nThe desired properties of a bootstrap test can be derived, that are asymptotic\nexactness under the null hypothesis and consistency under alternatives.\nSimulations show the quality of the test in the finite sample case. A possible\napplication is the comparison of two possibly dependent stock market returns on\nthe basis of functional data. The approach is demonstrated on the basis of\nhistorical data for different stock market indices.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 13:04:52 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 06:44:15 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ditzhaus", "Marc", ""], ["Gaigall", "Daniel", ""]]}, {"id": "2001.02673", "submitter": "Xianzheng Huang", "authors": "Xianzheng Huang and Haiming Zhou", "title": "Conditional density estimation with covariate measurement error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider estimating the density of a response conditioning on an\nerror-prone covariate. Motivated by two existing kernel density estimators in\nthe absence of covariate measurement error, we propose a method to correct the\nexisting estimators for measurement error. Asymptotic properties of the\nresultant estimators under different types of measurement error distributions\nare derived. Moreover, we adjust bandwidths readily available from existing\nbandwidth selection methods developed for error-free data to obtain bandwidths\nfor the new estimators. Extensive simulation studies are carried out to compare\nthe proposed estimators with naive estimators that ignore measurement error,\nwhich also provide empirical evidence for the effectiveness of the proposed\nbandwidth selection methods. A real-life data example is used to illustrate\nimplementation of these methods under practical scenarios. An R package, lpme,\nis developed for implementing all considered methods, which we demonstrate via\nan R code example in Appendix H.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 18:57:33 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Huang", "Xianzheng", ""], ["Zhou", "Haiming", ""]]}, {"id": "2001.02719", "submitter": "Erin Gabriel", "authors": "Erin E Gabriel", "title": "A note on Horvitz-Thompson estimators for rare subgroup analysis in the\n  presence of interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When there is interference, a subject's outcome depends on the treatment of\nothers and treatment effects may take on several different forms. This\nsituation arises often, particularly in vaccine evaluation. In settings where\ninterference is likely, two-stage cluster randomized trials have been suggested\nas a means of estimating some of the causal contrast of interest. Working in\nthe finite population setting to investigate rare and unplanned subgroup\nanalyses using some of the estimators that have been suggested in the\nliterature, include Horvitz-Thompson, Hajek, and what might be called the\nnatural extension of the marginal estimators suggested in Hudgens and Halloran\n2008. I define the estimands of interest conditional on individual, group and\nboth individual and group baseline variables, giving unbiased Horvitz-Thompson\nstyle estimates for each. I also provide variance estimators for several\nestimators. I show that the Horvitz-Thompson (HT) type estimators are always\nunbiased provided at least one subject within the group or population, whatever\nthe level of interest for the estimator, is in the subgroup of interest. This\nis not true of the \"natural\" or the Hajek style estimators, which will often be\nundefined for rare subgroups.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 20:01:31 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Gabriel", "Erin E", ""]]}, {"id": "2001.02883", "submitter": "Matthew Nunes", "authors": "Aaron Lowther and Paul Fearnhead and Matthew Nunes and Kjeld Jensen", "title": "Semi-automated simultaneous predictor selection for Regression-SARIMA\n  models", "comments": "18 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciding which predictors to use plays an integral role in deriving\nstatistical models in a wide range of applications. Motivated by the challenges\nof predicting events across a telecommunications network, we propose a\nsemi-automated, joint model-fitting and predictor selection procedure for\nlinear regression models. Our approach can model and account for serial\ncorrelation in the regression residuals, produces sparse and interpretable\nmodels and can be used to jointly select models for a group of related\nresponses. This is achieved through fitting linear models under constraints on\nthe number of non-zero coefficients using a generalisation of a recently\ndeveloped Mixed Integer Quadratic Optimisation approach. The resultant models\nfrom our approach achieve better predictive performance on the motivating\ntelecommunications data than methods currently used by industry.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 08:38:59 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Lowther", "Aaron", ""], ["Fearnhead", "Paul", ""], ["Nunes", "Matthew", ""], ["Jensen", "Kjeld", ""]]}, {"id": "2001.03013", "submitter": "Elvan Ceyhan", "authors": "Elvan Ceyhan", "title": "Domination Number of an Interval Catch Digraph Family and its use for\n  Testing Uniformity", "comments": "48 pages, 14 Figures, and 2 Tables. The corresponding article is to\n  appear in the Journal \"Statistics\". arXiv admin note: text overlap with\n  arXiv:1003.5362", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a special type of interval catch digraph (ICD) family for\none-dimensional data in a randomized setting and propose its use for testing\nuniformity. These ICDs are defined with an expansion and a centrality\nparameter, hence we will refer to this ICD as parameterized ICD (PICD). We\nderive the exact (and asymptotic) distribution of the domination number of this\nPICD family when its vertices are from a uniform (and non-uniform) distribution\nin one dimension for the entire range of the parameters; thereby determine the\nparameters for which the asymptotic distribution is non-degenerate. We observe\njumps (from degeneracy to non-degeneracy or from a non-degenerate distribution\nto another) in the asymptotic distribution of the domination number at certain\nparameter combinations. We use the domination number for testing uniformity of\ndata in real line, prove its consistency against certain alternatives, and\ncompare it with two commonly used tests and three recently proposed tests in\nliterature and also arc density of this ICD and of another ICD family in terms\nof size and power. Based on our extensive Monte Carlo simulations, we\ndemonstrate that domination number of our PICD has higher power for certain\ntypes of deviations from uniformity compared to other tests.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jan 2020 18:44:36 GMT"}], "update_date": "2020-01-10", "authors_parsed": [["Ceyhan", "Elvan", ""]]}, {"id": "2001.03231", "submitter": "Mariah Schrum", "authors": "Mariah L. Schrum, Michael Johnson, Muyleng Ghuy, Matthew C. Gombolay", "title": "Four Years in Review: Statistical Practices of Likert Scales in\n  Human-Robot Interaction Studies", "comments": null, "journal-ref": null, "doi": "10.1145/3319502.3378178", "report-no": null, "categories": "cs.HC cs.RO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots become more prevalent, the importance of the field of human-robot\ninteraction (HRI) grows accordingly. As such, we should endeavor to employ the\nbest statistical practices. Likert scales are commonly used metrics in HRI to\nmeasure perceptions and attitudes. Due to misinformation or honest mistakes,\nmost HRI researchers do not adopt best practices when analyzing Likert data. We\nconduct a review of psychometric literature to determine the current standard\nfor Likert scale design and analysis. Next, we conduct a survey of four years\nof the International Conference on Human-Robot Interaction (2016 through 2019)\nand report on incorrect statistical practices and design of Likert scales.\nDuring these years, only 3 of the 110 papers applied proper statistical testing\nto correctly-designed Likert scales. Our analysis suggests there are areas for\nmeaningful improvement in the design and testing of Likert scales. Lastly, we\nprovide recommendations to improve the accuracy of conclusions drawn from\nLikert data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 21:45:25 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 01:50:53 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Schrum", "Mariah L.", ""], ["Johnson", "Michael", ""], ["Ghuy", "Muyleng", ""], ["Gombolay", "Matthew C.", ""]]}, {"id": "2001.03258", "submitter": "Min Qian", "authors": "Xinyu Hu, Min Qian, Bin Cheng and Ying Kuen Cheung", "title": "Personalized Policy Learning using Longitudinal Mobile Health Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the personalized policy learning problem using longitudinal mobile\nhealth application usage data. Personalized policy represents a paradigm shift\nfrom developing a single policy that may prescribe personalized decisions by\ntailoring. Specifically, we aim to develop the best policy, one per user, based\non estimating random effects under generalized linear mixed model. With many\nrandom effects, we consider new estimation method and penalized objective to\ncircumvent high-dimension integrals for marginal likelihood approximation. We\nestablish consistency and optimality of our method with endogenous app usage.\nWe apply our method to develop personalized push (\"prompt\") schedules in 294\napp users, with a goal to maximize the prompt response rate given past app\nusage and other contextual factors. We found the best push schedule given the\nsame covariates varied among the users, thus calling for personalized policies.\nUsing the estimated personalized policies would have achieved a mean prompt\nresponse rate of 23% in these users at 16 weeks or later: this is a remarkable\nimprovement on the observed rate (11%), while the literature suggests 3%-15%\nuser engagement at 3 months after download. The proposed method compares\nfavorably to existing estimation methods including using the R function \"glmer\"\nin a simulation study.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 23:34:41 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Hu", "Xinyu", ""], ["Qian", "Min", ""], ["Cheng", "Bin", ""], ["Cheung", "Ying Kuen", ""]]}, {"id": "2001.03259", "submitter": "Jae-Kwang Kim", "authors": "Shu Yang and Jae Kwang Kim", "title": "Statistical Data Integration in Survey Sampling: A Review", "comments": "Submitted to Japanese Journal of Statistics and Data Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite population inference is a central goal in survey sampling. Probability\nsampling is the main statistical approach to finite population inference.\nChallenges arise due to high cost and increasing non-response rates. Data\nintegration provides a timely solution by leveraging multiple data sources to\nprovide more robust and efficient inference than using any single data source\nalone. The technique for data integration varies depending on types of samples\nand available information to be combined. This article provides a systematic\nreview of data integration techniques for combining probability samples,\nprobability and non-probability samples, and probability and big data samples.\nWe discuss a wide range of integration methods such as generalized least\nsquares, calibration weighting, inverse probability weighting, mass imputation\nand doubly robust methods. Finally, we highlight important questions for future\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 23:38:38 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Yang", "Shu", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "2001.03267", "submitter": "Dino Sejdinovic", "authors": "Dino Sejdinovic", "title": "Discussion of \"Functional Models for Time-Varying Random Objects'' by\n  Dubey and M\\\"uller", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discussion focuses on metric covariance, a new association measure\nbetween paired random objects in a metric space, developed by Dubey and\nM\\\"uller, and on its relationship with other similar concepts which have\npreviously appeared in the literature, including distance covariance by\nSz\\'ekely et al, as well as its generalisations which rely on the formalism of\nreproducing kernel Hilbert spaces (RKHS).\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 01:05:28 GMT"}], "update_date": "2020-01-13", "authors_parsed": [["Sejdinovic", "Dino", ""]]}, {"id": "2001.03658", "submitter": "Han Lin Shang", "authors": "Han Lin Shang and Steven Haberman", "title": "Forecasting multiple functional time series in a group structure: an\n  application to mortality", "comments": "23 pages, 6 figures, 4 tables, to be published in ASTIN Bulletin: The\n  Journal of the IAA. arXiv admin note: substantial text overlap with\n  arXiv:1705.08001", "journal-ref": "ASTIN Bulletin: The Journal of the IAA, 2020, 50(2), 357-379", "doi": "10.1017/asb.2020.3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When modeling sub-national mortality rates, we should consider three\nfeatures: (1) how to incorporate any possible correlation among sub-populations\nto potentially improve forecast accuracy through multi-population joint\nmodeling; (2) how to reconcile sub-national mortality forecasts so that they\naggregate adequately across various levels of a group structure; (3) among the\nforecast reconciliation methods, how to combine their forecasts to achieve\nimproved forecast accuracy. To address these issues, we introduce an extension\nof grouped univariate functional time series method. We first consider a\nmultivariate functional time series method to jointly forecast multiple related\nseries. We then evaluate the impact and benefit of using forecast combinations\namong the forecast reconciliation methods. Using the Japanese regional\nage-specific mortality rates, we investigate one-step-ahead to 15-step-ahead\npoint and interval forecast accuracies of our proposed extension and make\nrecommendations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 20:33:01 GMT"}], "update_date": "2020-09-22", "authors_parsed": [["Shang", "Han Lin", ""], ["Haberman", "Steven", ""]]}, {"id": "2001.03709", "submitter": "Micol Federica Tresoldi", "authors": "Peter McCullagh and Micol Federica Tresoldi", "title": "A likelihood analysis of quantile-matching transformations", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile matching is a strictly monotone transformation that sends the\nobserved response values $\\{y_1, . . . , y_n\\}$ to the quantiles of a given\ntarget distribution. A likelihood based criterion is developed for comparing\none target distribution with another in a linear-model setting.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 04:15:52 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["McCullagh", "Peter", ""], ["Tresoldi", "Micol Federica", ""]]}, {"id": "2001.03719", "submitter": "Setareh Ranjbar", "authors": "Setareh Ranjbar and Nicola Salvati and Barbara Pacini", "title": "Causal Inferences in Small Area Estimation", "comments": "33 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  When doing impact evaluation and making causal inferences, it is important to\nacknowledge the heterogeneity of the treatment effects for different domains\n(geographic, socio-demographic, or socio-economic). If the domain of interest\nis small with regards to its sample size (or even zero in some cases), then the\nevaluator has entered the small area estimation (SAE) dilemma.\n  Based on the modification of the Inverse Propensity Weighting estimator and\nthe traditional small area predictors, the paper proposes a new methodology to\nestimate area specific average treatment effects for unplanned domains. By\nmeans of these methods we can also provide a map of policy impacts, that can\nhelp to better target the treatment group(s). We develop analytical Mean\nSquared Error (MSE) estimators of the proposed predictors. An extensive\nsimulation analysis, also based on real data, shows that the proposed\ntechniques in most cases lead to more efficient estimators.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 06:46:40 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 11:34:02 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ranjbar", "Setareh", ""], ["Salvati", "Nicola", ""], ["Pacini", "Barbara", ""]]}, {"id": "2001.03786", "submitter": "Ioannis Kosmidis", "authors": "Ioannis Kosmidis, Nicola Lunardon", "title": "Empirical bias-reducing adjustments to estimating functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We develop a novel, general framework for reduced-bias $M$-estimation from\nasymptotically unbiased estimating functions. The framework relies on an\nempirical approximation of the bias by a function of derivatives of estimating\nfunction contributions. Reduced-bias $M$-estimation operates either implicitly,\nby solving empirically adjusted estimating equations, or explicitly, by\nsubtracting the estimated bias from the original $M$-estimates, and applies to\nmodels that are partially- or fully-specified, with either likelihoods or other\nsurrogate objectives. Automatic differentiation can be used to abstract away\nthe only algebra required to implement reduced-bias $M$-estimation. As a\nresult, the bias reduction methods we introduce have markedly broader\napplicability and more straightforward implementation than other established\nbias-reduction methods that require resampling or evaluation of expectations of\nproducts of log-likelihood derivatives. If $M$-estimation is by maximizing an\nobjective, then there always exists a bias-reducing penalized objective. That\npenalized objective relates closely to information criteria for model\nselection, and can be further enhanced with plug-in penalties to deliver\nreduced-bias $M$-estimates with extra properties, like finiteness in models for\ncategorical data. The reduced-bias $M$-estimators have the same asymptotic\ndistribution as the original $M$-estimators, and, hence, standard procedures\nfor inference and model selection apply unaltered with the improved estimates.\nWe demonstrate and assess the properties of reduced-bias $M$-estimation in\nwell-used, prominent modelling settings of varying complexity.\n", "versions": [{"version": "v1", "created": "Sat, 11 Jan 2020 19:19:36 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 09:00:59 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kosmidis", "Ioannis", ""], ["Lunardon", "Nicola", ""]]}, {"id": "2001.03985", "submitter": "Luigi Acerbi", "authors": "Bas van Opheusden, Luigi Acerbi and Wei Ji Ma", "title": "Unbiased and Efficient Log-Likelihood Estimation with Inverse Binomial\n  Sampling", "comments": "Bas van Opheusden and Luigi Acerbi contributed equally to this work", "journal-ref": null, "doi": "10.1371/journal.pcbi.1008483", "report-no": null, "categories": "cs.LG q-bio.NC q-bio.QM stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fate of scientific hypotheses often relies on the ability of a\ncomputational model to explain the data, quantified in modern statistical\napproaches by the likelihood function. The log-likelihood is the key element\nfor parameter estimation and model evaluation. However, the log-likelihood of\ncomplex models in fields such as computational biology and neuroscience is\noften intractable to compute analytically or numerically. In those cases,\nresearchers can often only estimate the log-likelihood by comparing observed\ndata with synthetic observations generated by model simulations. Standard\ntechniques to approximate the likelihood via simulation either use summary\nstatistics of the data or are at risk of producing severe biases in the\nestimate. Here, we explore another method, inverse binomial sampling (IBS),\nwhich can estimate the log-likelihood of an entire data set efficiently and\nwithout bias. For each observation, IBS draws samples from the simulator model\nuntil one matches the observation. The log-likelihood estimate is then a\nfunction of the number of samples drawn. The variance of this estimator is\nuniformly bounded, achieves the minimum variance for an unbiased estimator, and\nwe can compute calibrated estimates of the variance. We provide theoretical\narguments in favor of IBS and an empirical assessment of the method for\nmaximum-likelihood estimation with simulation-based models. As case studies, we\ntake three model-fitting problems of increasing complexity from computational\nand cognitive neuroscience. In all problems, IBS generally produces lower error\nin the estimated parameters and maximum log-likelihood values than alternative\nsampling methods with the same average number of samples. Our results\ndemonstrate the potential of IBS as a practical, robust, and easy to implement\nmethod for log-likelihood evaluation when exact techniques are not available.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 19:51:35 GMT"}, {"version": "v2", "created": "Wed, 7 Oct 2020 19:24:28 GMT"}, {"version": "v3", "created": "Tue, 27 Oct 2020 20:08:25 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["van Opheusden", "Bas", ""], ["Acerbi", "Luigi", ""], ["Ma", "Wei Ji", ""]]}, {"id": "2001.03988", "submitter": "Meimei Liu", "authors": "Meimei Liu and David B. Dunson", "title": "Domain Adaptive Bootstrap Aggregating", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When there is a distributional shift between data used to train a predictive\nalgorithm and current data, performance can suffer. This is known as the domain\nadaptation problem. Bootstrap aggregating, or bagging, is a popular method for\nimproving stability of predictive algorithms, while reducing variance and\nprotecting against over-fitting. This article proposes a domain adaptive\nbagging method coupled with a new iterative nearest neighbor sampler. The key\nidea is to draw bootstrap samples from the training data in such a manner that\ntheir distribution equals that of new testing data. The proposed approach\nprovides a general ensemble framework that can be applied to arbitrary\nclassifiers. We further modify the method to allow anomalous samples in the\ntest data corresponding to outliers in the training data. Theoretical support\nis provided, and the approach is compared to alternatives in simulations and\nreal data applications.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 20:02:58 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2020 04:51:55 GMT"}], "update_date": "2020-06-17", "authors_parsed": [["Liu", "Meimei", ""], ["Dunson", "David B.", ""]]}, {"id": "2001.04013", "submitter": "Yongqiang Tang Dr.", "authors": "Yongqiang Tang", "title": "Notes on Exact Power Calculations for t Tests and Analysis of Covariance", "comments": "5 pages", "journal-ref": "Statistics in Biopharmaceutical Research 2020", "doi": "10.1080/19466315.2019.1707110", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tang derived the exact power formulae for t tests and analysis of covariance\n(ANCOVA) in superiority, noninferiority and equivalence trials. The power\ncalculation in equivalence trials can be simplified by using Owen's Q function,\nwhich is available in standard statistical software. We extend the exact power\ndetermination method for ANCOVA to unstratified and stratified multi-arm\nrandomized trials. The method is applied to the design of multi-arm trials and\ngold standard noninferiority trials.\n", "versions": [{"version": "v1", "created": "Sun, 12 Jan 2020 23:48:31 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Tang", "Yongqiang", ""]]}, {"id": "2001.04026", "submitter": "Zijian Liu", "authors": "Zijian Liu, Chunbo Luo, Shuai Li, Peng Ren and Geyong Min", "title": "Fractional order graph neural network", "comments": "There are serious mistakes in the article and it needs to be\n  retracted and corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes fractional order graph neural networks (FGNNs), optimized\nby the approximation strategy to address the challenges of local optimum of\nclassic and fractional graph neural networks which are specialised at\naggregating information from the feature and adjacent matrices of connected\nnodes and their neighbours to solve learning tasks on non-Euclidean data such\nas graphs. Meanwhile the approximate calculation of fractional order gradients\nalso overcomes the high computational complexity of fractional order\nderivations. We further prove that such an approximation is feasible and the\nFGNN is unbiased towards global optimization solution. Extensive experiments on\ncitation networks show that FGNN achieves great advantage over baseline models\nwhen selected appropriate fractional order.\n", "versions": [{"version": "v1", "created": "Sun, 5 Jan 2020 11:55:55 GMT"}, {"version": "v2", "created": "Mon, 4 Jan 2021 08:30:29 GMT"}, {"version": "v3", "created": "Tue, 6 Jul 2021 06:23:18 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Liu", "Zijian", ""], ["Luo", "Chunbo", ""], ["Li", "Shuai", ""], ["Ren", "Peng", ""], ["Min", "Geyong", ""]]}, {"id": "2001.04040", "submitter": "Wei Li", "authors": "Wei Li, Chunchen Liu, Zhi Geng and John Murray", "title": "Causal Mediation Analysis with Multiple Treatments and Latent\n  Confounders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis is used to evaluate direct and indirect causal\neffects of a treatment on an outcome of interest through an intermediate\nvariable or a mediator.It is difficult to identify the direct and indirect\ncausal effects because the mediator cannot be randomly assigned in many real\napplications. In this article, we consider a causal model including latent\nconfounders between the mediator and the outcome. We present sufficient\nconditions for identifying the direct and indirect effects and propose an\napproach for estimating them. The performance of the proposed approach is\nevaluated by simulation studies. Finally, we apply the approach to a data set\nof the customer loyalty survey by a telecom company.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 02:53:23 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Li", "Wei", ""], ["Liu", "Chunchen", ""], ["Geng", "Zhi", ""], ["Murray", "John", ""]]}, {"id": "2001.04214", "submitter": "Haotian Xu", "authors": "St\\'ephane Guerrier, Roberto Molinari, Maria-Pia Victoria-Feser,\n  Haotian Xu", "title": "Robust Two-Step Wavelet-Based Inference for Time Series Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex time series models such as (the sum of) ARMA$(p,q)$ models with\nadditional noise, random walks, rounding errors and/or drifts are increasingly\nused for data analysis in fields such as biology, ecology, engineering and\neconomics where the length of the observed signals can be extremely large.\nPerforming inference on and/or prediction from these models can be highly\nchallenging for several reasons: (i) the data may contain outliers that can\nadversely affect the estimation procedure; (ii) the computational complexity\ncan become prohibitive when models include more than just a few parameters\nand/or the time series are large; (iii) model building and/or selection adds\nanother layer of (computational) complexity to the previous task; and (iv)\nsolutions that address (i), (ii) and (iii) simultaneously do not exist in\npractice. For this reason, this paper aims at jointly addressing these\nchallenges by proposing a general framework for robust two-step estimation\nbased on a bounded influence M-estimator of the wavelet variance. In this\nperspective, we first develop the conditions for the joint asymptotic normality\nof the latter estimator thereby providing the necessary tools to perform\n(direct) inference for scale-based analysis of signals. Taking advantage of the\nmodel-independent weights of this first-step estimator that are computed only\nonce, we then develop the asymptotic properties of two-step robust estimators\nusing the framework of the Generalized Method of Wavelet Moments (GMWM), hence\ndefining the Robust GMWM (RGMWM) that we then use for robust model estimation\nand inference in a computationally efficient manner even for large time series.\nSimulation studies illustrate the good finite sample performance of the RGMWM\nestimator and applied examples highlight the practical relevance of the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 13:11:51 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Guerrier", "St\u00e9phane", ""], ["Molinari", "Roberto", ""], ["Victoria-Feser", "Maria-Pia", ""], ["Xu", "Haotian", ""]]}, {"id": "2001.04324", "submitter": "Takuya Ishihara", "authors": "Takuya Ishihara", "title": "Panel Data Quantile Regression for Treatment Effect Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we develop a novel estimation method of the quantile treatment\neffects (QTE) under the rank invariance and rank stationarity assumptions.\nIshihara (2020) explores identification of the nonseparable panel data model\nunder these assumptions and propose a parametric estimation based on the\nminimum distance method. However, the minimum distance estimation using this\nprocess is computationally demanding when the dimensionality of covariates is\nlarge. To overcome this problem, we propose a two-step estimation method based\non the quantile regression and minimum distance method. We then show\nconsistency and asymptotic normality of our estimator. Monte Carlo studies\nindicate that our estimator performs well in finite samples. Last, we present\ntwo empirical illustrations, to estimate the distributional effects of\ninsurance provision on household production and of TV watching on child\ncognitive development.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 15:05:52 GMT"}, {"version": "v2", "created": "Sun, 11 Oct 2020 09:55:16 GMT"}], "update_date": "2020-10-13", "authors_parsed": [["Ishihara", "Takuya", ""]]}, {"id": "2001.04343", "submitter": "F. William Townes", "authors": "F. William Townes", "title": "Review of Probability Distributions for Modeling Count Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count data take on non-negative integer values and are challenging to\nproperly analyze using standard linear-Gaussian methods such as linear\nregression and principal components analysis. Generalized linear models enable\ndirect modeling of counts in a regression context using distributions such as\nthe Poisson and negative binomial. When counts contain only relative\ninformation, multinomial or Dirichlet-multinomial models can be more\nappropriate. We review some of the fundamental connections between multinomial\nand count models from probability theory, providing detailed proofs. These\nrelationships are useful for methods development in applications such as topic\nmodeling of text data and genomics.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jan 2020 18:28:19 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Townes", "F. William", ""]]}, {"id": "2001.04444", "submitter": "Lee McDaniel", "authors": "Lee S. McDaniel, Jonathan S. Schildcrout, Enrique F. Schisterman, Paul\n  J. Rathouz", "title": "Generalized Linear Models for Longitudinal Data with Biased Sampling\n  Designs: A Sequential Offsetted Regressions Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biased sampling designs can be highly efficient when studying rare (binary)\nor low variability (continuous) endpoints. We consider longitudinal data\nsettings in which the probability of being sampled depends on a repeatedly\nmeasured response through an outcome-related, auxiliary variable. Such\nauxiliary variable- or outcome-dependent sampling improves observed response\nand possibly exposure variability over random sampling, {even though} the\nauxiliary variable is not of scientific interest. {For analysis,} we propose a\ngeneralized linear model based approach using a sequence of two offsetted\nregressions. The first estimates the relationship of the auxiliary variable to\nresponse and covariate data using an offsetted logistic regression model. The\noffset hinges on the (assumed) known ratio of sampling probabilities for\ndifferent values of the auxiliary variable. Results from the auxiliary model\nare used to estimate observation-specific probabilities of being sampled\nconditional on the response and covariates, and these probabilities are then\nused to account for bias in the second, target population model. We provide\nasymptotic standard errors accounting for uncertainty in the estimation of the\nauxiliary model, and perform simulation studies demonstrating substantial bias\nreduction, correct coverage probability, and improved design efficiency over\nsimple random sampling designs. We illustrate the approaches with two examples.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jan 2020 18:26:05 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["McDaniel", "Lee S.", ""], ["Schildcrout", "Jonathan S.", ""], ["Schisterman", "Enrique F.", ""], ["Rathouz", "Paul J.", ""]]}, {"id": "2001.04629", "submitter": "Fei Xue", "authors": "Fei Xue, Yanqing Zhang, Wenzhuo Zhou, Haoda Fu, Annie Qu", "title": "Multicategory Angle-based Learning for Estimating Optimal Dynamic\n  Treatment Regimes with Censored Data", "comments": "35 pages, 11 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An optimal dynamic treatment regime (DTR) consists of a sequence of decision\nrules in maximizing long-term benefits, which is applicable for chronic\ndiseases such as HIV infection or cancer. In this paper, we develop a novel\nangle-based approach to search the optimal DTR under a multicategory treatment\nframework for survival data. The proposed method targets maximization the\nconditional survival function of patients following a DTR. In contrast to most\nexisting approaches which are designed to maximize the expected survival time\nunder a binary treatment framework, the proposed method solves the\nmulticategory treatment problem given multiple stages for censored data.\nSpecifically, the proposed method obtains the optimal DTR via integrating\nestimations of decision rules at multiple stages into a single multicategory\nclassification algorithm without imposing additional constraints, which is also\nmore computationally efficient and robust. In theory, we establish Fisher\nconsistency of the proposed method under regularity conditions. Our numerical\nstudies show that the proposed method outperforms competing methods in terms of\nmaximizing the conditional survival function. We apply the proposed method to\ntwo real datasets: Framingham heart study data and acquired immunodeficiency\nsyndrome (AIDS) clinical data.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 05:19:15 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Xue", "Fei", ""], ["Zhang", "Yanqing", ""], ["Zhou", "Wenzhuo", ""], ["Fu", "Haoda", ""], ["Qu", "Annie", ""]]}, {"id": "2001.04639", "submitter": "Chiwoo Park", "authors": "Chiwoo Park, David J. Borth, Nicholas S. Wilson, Chad N. Hunter, and\n  Fritz J. Friedersdorf", "title": "Robust Gaussian Process Regression with a Bias Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach to a robust Gaussian process (GP)\nregression. Most existing approaches replace an outlier-prone Gaussian\nlikelihood with a non-Gaussian likelihood induced from a heavy tail\ndistribution, such as the Laplace distribution and Student-t distribution.\nHowever, the use of a non-Gaussian likelihood would incur the need for a\ncomputationally expensive Bayesian approximate computation in the posterior\ninferences. The proposed approach models an outlier as a noisy and biased\nobservation of an unknown regression function, and accordingly, the likelihood\ncontains bias terms to explain the degree of deviations from the regression\nfunction. We entail how the biases can be estimated accurately with other\nhyperparameters by a regularized maximum likelihood estimation. Conditioned on\nthe bias estimates, the robust GP regression can be reduced to a standard GP\nregression problem with analytical forms of the predictive mean and variance\nestimates. Therefore, the proposed approach is simple and very computationally\nattractive. It also gives a very robust and accurate GP estimate for many\ntested scenarios. For the numerical evaluation, we perform a comprehensive\nsimulation study to evaluate the proposed approach with the comparison to the\nexisting robust GP approaches under various simulated scenarios of different\noutlier proportions and different noise levels. The approach is applied to data\nfrom two measurement systems, where the predictors are based on robust\nenvironmental parameter measurements and the response variables utilize more\ncomplex chemical sensing methods that contain a certain percentage of outliers.\nThe utility of the measurement systems and value of the environmental data are\nimproved through the computationally efficient GP regression and bias model.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 06:21:51 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Park", "Chiwoo", ""], ["Borth", "David J.", ""], ["Wilson", "Nicholas S.", ""], ["Hunter", "Chad N.", ""], ["Friedersdorf", "Fritz J.", ""]]}, {"id": "2001.04660", "submitter": "Israel Mart\\'inez Hern\\'andez", "authors": "Israel Mart\\'inez-Hern\\'andez and Marc G. Genton", "title": "Nonparametric Trend Estimation in Functional Time Series with\n  Application to Annual Mortality Rates", "comments": "added new simulation studies", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Here, we address the problem of trend estimation for functional time series.\nExisting contributions either deal with detecting a functional trend or\nassuming a simple model. They consider neither the estimation of a general\nfunctional trend nor the analysis of functional time series with a functional\ntrend component. Similarly to univariate time series, we propose an alternative\nmethodology to analyze functional time series, taking into account a functional\ntrend component. We propose to estimate the functional trend by using a tensor\nproduct surface that is easy to implement, to interpret, and allows to control\nthe smoothness properties of the estimator. Through a Monte Carlo study, we\nsimulate different scenarios of functional processes to show that our estimator\naccurately identifies the functional trend component. We also show that the\ndependency structure of the estimated stationary time series component is not\nsignificantly affected by the error approximation of the functional trend\ncomponent. We apply our methodology to annual mortality rates in France.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 08:17:33 GMT"}, {"version": "v2", "created": "Fri, 21 Aug 2020 15:38:32 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["Mart\u00ednez-Hern\u00e1ndez", "Israel", ""], ["Genton", "Marc G.", ""]]}, {"id": "2001.04847", "submitter": "Anita Nandi", "authors": "Anita K. Nandi, Tim C. D. Lucas, Rohan Arambepola, Peter Gething,\n  Daniel J. Weiss", "title": "disaggregation: An R Package for Bayesian Spatial Disaggregation\n  Modelling", "comments": "16 pages, 5 figures, submitted to Journal of Statistical Software", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Disaggregation modelling, or downscaling, has become an important discipline\nin epidemiology. Surveillance data, aggregated over large regions, is becoming\nmore common, leading to an increasing demand for modelling frameworks that can\ndeal with this data to understand spatial patterns. Disaggregation regression\nmodels use response data aggregated over large heterogenous regions to make\npredictions at fine-scale over the region by using fine-scale covariates to\ninform the heterogeneity. This paper presents the R package disaggregation,\nwhich provides functionality to streamline the process of running a\ndisaggregation model for fine-scale predictions.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jan 2020 16:00:16 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Nandi", "Anita K.", ""], ["Lucas", "Tim C. D.", ""], ["Arambepola", "Rohan", ""], ["Gething", "Peter", ""], ["Weiss", "Daniel J.", ""]]}, {"id": "2001.04867", "submitter": "Alban Moor", "authors": "Davide La Vecchia, Alban Moor, Olivier Scaillet", "title": "A Higher-Order Correct Fast Moving-Average Bootstrap for Dependent Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and implement a novel fast bootstrap for dependent data. Our\nscheme is based on the i.i.d. resampling of the smoothed moment indicators. We\ncharacterize the class of parametric and semi-parametric estimation problems\nfor which the method is valid. We show the asymptotic refinements of the\nproposed procedure, proving that it is higher-order correct under mild\nassumptions on the time series, the estimating functions, and the smoothing\nkernel. We illustrate the applicability and the advantages of our procedure for\nGeneralized Empirical Likelihood estimation. As a by-product, our fast\nbootstrap provides higher-order correct asymptotic confidence distributions.\nMonte Carlo simulations on an autoregressive conditional duration model provide\nnumerical evidence that the novel bootstrap yields higher-order accurate\nconfidence intervals. A real-data application on dynamics of trading volume of\nstocks illustrates the advantage of our method over the routinely-applied\nfirst-order asymptotic theory, when the underlying distribution of the test\nstatistic is skewed or fat-tailed.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 16:11:22 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["La Vecchia", "Davide", ""], ["Moor", "Alban", ""], ["Scaillet", "Olivier", ""]]}, {"id": "2001.04938", "submitter": "Swati Chandna", "authors": "Swati Chandna, Pierre-Andre Maugis", "title": "Nonparametric regression for multiple heterogeneous networks", "comments": "26 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonparametric methods for the setting where multiple distinct\nnetworks are observed on the same set of nodes. Such samples may arise in the\nform of replicated networks drawn from a common distribution, or in the form of\nheterogeneous networks, with the network generating process varying from one\nnetwork to another, e.g.~dynamic and cross-sectional networks. Nonparametric\nmethods for undirected networks have focused on estimation of the graphon\nmodel. While the graphon model accounts for nodal heterogeneity, it does not\naccount for network heterogeneity, a feature specific to applications where\nmultiple networks are observed. To address this setting of multiple networks,\nwe propose a multi-graphon model which allows node-level as well as\nnetwork-level heterogeneity. We show how information from multiple networks can\nbe leveraged to enable estimation of the multi-graphon via standard\nnonparametric regression techniques, e.g. kernel regression, orthogonal series\nestimation. We study theoretical properties of the proposed estimator\nestablishing recovery of the latent nodal positions up to negligible error, and\nconvergence of the multi-graphon estimator to the normal distribution. Finite\nsample performance are investigated in a simulation study and application to\ntwo real-world networks---a dynamic contact network of ants and a collection of\nstructural brain networks from different subjects---illustrate the utility of\nour approach.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 17:54:59 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Chandna", "Swati", ""], ["Maugis", "Pierre-Andre", ""]]}, {"id": "2001.04968", "submitter": "Ying Liu", "authors": "Ying Liu, Bowei Yan, Kathleen Merikangas, and Haochang Shou", "title": "Graph-Fused Multivariate Regression via Total Variation Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the Graph-Fused Multivariate Regression (GFMR) via\nTotal Variation regularization, a novel method for estimating the association\nbetween a one-dimensional or multidimensional array outcome and scalar\npredictors. While we were motivated by data from neuroimaging and physical\nactivity tracking, the methodology is designed and presented in a generalizable\nformat and is applicable to many other areas of scientific research. The\nestimator is the solution of a penalized regression problem where the objective\nis the sum of square error plus a total variation (TV) regularization on the\npredicted mean across all subjects. We propose an algorithm for parameter\nestimation, which is efficient and scalable in a distributed computing\nplatform. Proof of the algorithm convergence is provided, and the statistical\nconsistency of the estimator is presented via an oracle inequality. We present\n1D and 2D simulation results and demonstrate that GFMR outperforms existing\nmethods in most cases. We also demonstrate the general applicability of the\nmethod by two real data examples, including the analysis of the 1D\naccelerometry subsample of a large community-based study for mood disorders and\nthe analysis of the 3D MRI data from the attention-deficient/hyperactive\ndeficient (ADHD) 200 consortium.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 18:56:47 GMT"}], "update_date": "2020-01-15", "authors_parsed": [["Liu", "Ying", ""], ["Yan", "Bowei", ""], ["Merikangas", "Kathleen", ""], ["Shou", "Haochang", ""]]}, {"id": "2001.05034", "submitter": "Youssef Aboutaleb", "authors": "Youssef M Aboutaleb, Mazen Danaf, Yifei Xie, and Moshe Ben-Akiva", "title": "Sparse Covariance Estimation in Logit Mixture Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new data-driven methodology for estimating sparse\ncovariance matrices of the random coefficients in logit mixture models.\nResearchers typically specify covariance matrices in logit mixture models under\none of two extreme assumptions: either an unrestricted full covariance matrix\n(allowing correlations between all random coefficients), or a restricted\ndiagonal matrix (allowing no correlations at all). Our objective is to find\noptimal subsets of correlated coefficients for which we estimate covariances.\nWe propose a new estimator, called MISC, that uses a mixed-integer optimization\n(MIO) program to find an optimal block diagonal structure specification for the\ncovariance matrix, corresponding to subsets of correlated coefficients, for any\ndesired sparsity level using Markov Chain Monte Carlo (MCMC) posterior draws\nfrom the unrestricted full covariance matrix. The optimal sparsity level of the\ncovariance matrix is determined using out-of-sample validation. We demonstrate\nthe ability of MISC to correctly recover the true covariance structure from\nsynthetic data. In an empirical illustration using a stated preference survey\non modes of transportation, we use MISC to obtain a sparse covariance matrix\nindicating how preferences for attributes are related to one another.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jan 2020 20:19:15 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Aboutaleb", "Youssef M", ""], ["Danaf", "Mazen", ""], ["Xie", "Yifei", ""], ["Ben-Akiva", "Moshe", ""]]}, {"id": "2001.05099", "submitter": "Vladimir Minin", "authors": "Jonathan Fintzi, Jon Wakefield, Vladimir N. Minin", "title": "A linear noise approximation for stochastic epidemic models fit to\n  partially observed incidence counts", "comments": "67 pages, 22 pages of main text, the rest are appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic epidemic models (SEMs) fit to incidence data are critical to\nelucidating outbreak dynamics, shaping response strategies, and preparing for\nfuture epidemics. SEMs typically represent counts of individuals in discrete\ninfection states using Markov jump processes (MJPs), but are computationally\nchallenging as imperfect surveillance, lack of subject-level information, and\ntemporal coarseness of the data obscure the true epidemic. Analytic integration\nover the latent epidemic process is impossible, and integration via Markov\nchain Monte Carlo (MCMC) is cumbersome due to the dimensionality and\ndiscreteness of the latent state space. Simulation-based computational\napproaches can address the intractability of the MJP likelihood, but are\nnumerically fragile and prohibitively expensive for complex models. A linear\nnoise approximation (LNA) that approximates the MJP transition density with a\nGaussian density has been explored for analyzing prevalence data in\nlarge-population settings, but requires modification for analyzing incidence\ncounts without assuming that the data are normally distributed. We demonstrate\nhow to reparameterize SEMs to appropriately analyze incidence data, and fold\nthe LNA into a data augmentation MCMC framework that outperforms deterministic\nmethods, statistically, and simulation-based methods, computationally. Our\nframework is computationally robust when the model dynamics are complex and\napplies to a broad class of SEMs. We evaluate our method in simulations that\nreflect Ebola, influenza, and SARS-CoV-2 dynamics, and apply our method to\nnational surveillance counts from the 2013--2015 West Africa Ebola outbreak.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 01:51:09 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 05:59:08 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Fintzi", "Jonathan", ""], ["Wakefield", "Jon", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "2001.05126", "submitter": "Albert Vexler", "authors": "Albert Vexler", "title": "Valid p-Values and Expectations of p-Values Revisited", "comments": "Annals of the Institute of Statistical Mathematics. (2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A storm of favorable or critical publications regarding p-values-based\nprocedures has been observed in both the theoretical and applied literature. We\nfocus on valid definitions of p-values in the scenarios when composite null\nmodels are in effect. A valid p-value (VpV) statistic can be used to make a\nprefixed level-decision. In this context, Kolmogorov Smirnov goodness-of-fit\ntests and the normal two sample problem are considered. In particular, we\nexamine an issue regarding the goodness-of-fit testability based on a single\nobservation. This article exemplifies constructions of new test procedures,\nadvocating practical reasons to implement VpV-based mechanisms. The VpV\nframework induces an extension of the conventional expected p-value (EPV) tool\nfor measuring the performance of a test. Associating the EPV concept with the\nreceiver operating characteristic (ROC) curve methodology, a well-established\nbiostatistical approach, we propose a Youden index based optimality principle\nto derive critical values of decision making procedures. In these terms, the\nsignificance level alpha=0.05 can be suggested, in many situations. In light of\nan ROC curve analysis, we introduce partial EPVs to characterize properties of\ntests including their unbiasedness. We also provide the intrinsic relationship\nbetween the Bayes Factor (BF) test statistic and the BF of test statistics.\n  Keywords: AUC; Bayes Factor; Expected p-value; Kolmogorov Smirnov tests;\nLikelihood ratio; Nuisance parameters; P-value; ROC curve; Pooled data; Single\nobservation; Type I error rate; Youden index\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 04:19:40 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Vexler", "Albert", ""]]}, {"id": "2001.05204", "submitter": "Ansgar Steland", "authors": "Nils Mause and Ansgar Steland", "title": "Detecting Changes in the Second Moment Structure of High-Dimensional\n  Sensor-Type Data in a $K$-Sample Setting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The $K$ sample problem for high-dimensional vector time series is studied,\nespecially focusing on sensor data streams, in order to analyze the second\nmoment structure and detect changes across samples and/or across variables\ncumulated sum (CUSUM) statistics of bilinear forms of the sample covariance\nmatrix. In this model $K$ independent vector time series\n$\\mathbf{Y}_{T,1},\\dots,\\mathbf{Y}_{T,K}$ are observed over a time span $ [0,T]\n$, which may correspond to $K$ sensors (locations) yielding $d$-dimensional\ndata as well as $K$ locations where $d$ sensors emit univariate data. Unequal\nsample sizes are considered as arising when the sampling rate of the sensors\ndiffers. We provide large sample approximations and two related change-point\nstatistics, a sums of squares and a pooled variance statistic. The resulting\nprocedures are investigated by simulations and illustrated by analyzing a real\ndata set.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 10:00:32 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Mause", "Nils", ""], ["Steland", "Ansgar", ""]]}, {"id": "2001.05241", "submitter": "Thomas Grundy", "authors": "Thomas Grundy and Rebecca Killick and Gueorgui Mihaylov", "title": "High-Dimensional Changepoint Detection via a Geometrically Inspired\n  Mapping", "comments": null, "journal-ref": "Statistics and Computing (2020)", "doi": "10.1007/s11222-020-09940-y", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional changepoint analysis is a growing area of research and has\napplications in a wide range of fields. The aim is to accurately and\nefficiently detect changepoints in time series data when both the number of\ntime points and dimensions grow large. Existing methods typically aggregate or\nproject the data to a smaller number of dimensions; usually one. We present a\nhigh-dimensional changepoint detection method that takes inspiration from\ngeometry to map a high-dimensional time series to two dimensions. We show\ntheoretically and through simulation that if the input series is Gaussian then\nthe mappings preserve the Gaussianity of the data. Applying univariate\nchangepoint detection methods to both mapped series allows the detection of\nchangepoints that correspond to changes in the mean and variance of the\noriginal time series. We demonstrate that this approach outperforms the current\nstate-of-the-art multivariate changepoint methods in terms of accuracy of\ndetected changepoints and computational efficiency. We conclude with\napplications from genetics and finance.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 11:15:48 GMT"}], "update_date": "2020-04-01", "authors_parsed": [["Grundy", "Thomas", ""], ["Killick", "Rebecca", ""], ["Mihaylov", "Gueorgui", ""]]}, {"id": "2001.05260", "submitter": "Silvia D'Angelo", "authors": "Silvia D'Angelo, Marco Alf\\`o and Michael Fop", "title": "Model-based Clustering for Multivariate Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data are relational data recorded among a group of individuals, the\nnodes. Multiple relations observed among the same set of nodes may be\nrepresented by means of different networks, using a so-called multidimensional\nnetwork, or multiplex. We propose a latent space model for network data that\nenables clustering of the nodes in a latent space, with clusters in this space\ncorresponding to communities of nodes. The clustering structure is modelled\nusing an infinite mixture distribution framework, which allows to perform joint\ninference on the number of clusters and the cluster parameters. The method is\ntested on simulated data experiments and is shown in application to a\nmultivariate network among students.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 12:17:30 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["D'Angelo", "Silvia", ""], ["Alf\u00f2", "Marco", ""], ["Fop", "Michael", ""]]}, {"id": "2001.05344", "submitter": "Ted Westling", "authors": "Ted Westling", "title": "Nonparametric tests of the causal null with non-discrete exposures", "comments": "Major updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many scientific studies, it is of interest to determine whether an\nexposure has a causal effect on an outcome. In observational studies, this is a\nchallenging task due to the presence of confounding variables that affect both\nthe exposure and the outcome. Many methods have been developed to test for the\npresence of a causal effect when all such confounding variables are observed\nand when the exposure of interest is discrete. In this article, we propose a\nclass of nonparametric tests of the null hypothesis that there is no average\ncausal effect of an arbitrary univariate exposure on an outcome in the presence\nof observed confounding. Our tests apply to discrete, continuous, and mixed\ndiscrete-continuous exposures. We demonstrate that our proposed tests are\ndoubly-robust consistent, that they have correct asymptotic type I error if\nboth nuisance parameters involved in the problem are estimated at fast enough\nrates, and that they have power to detect local alternatives approaching the\nnull at the rate $n^{-1/2}$. We study the performance of our tests in numerical\nstudies, and use them to test for the presence of a causal effect of smoking on\nbirthweight among smoking mothers.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 14:34:35 GMT"}, {"version": "v2", "created": "Tue, 6 Oct 2020 00:59:33 GMT"}], "update_date": "2020-10-07", "authors_parsed": [["Westling", "Ted", ""]]}, {"id": "2001.05360", "submitter": "Jacob Fiksel", "authors": "Jacob Fiksel, Abhirup Datta, Agbessi Amouzou and Scott Zeger", "title": "Generalized Bayes Quantification Learning under Dataset Shift", "comments": "59 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantification learning is the task of prevalence estimation for a test\npopulation using predictions from a classifier trained on a different\npopulation. Quantification methods assume that the sensitivities and\nspecificities of the classifier are either perfect or transportable from the\ntraining to the test population. These assumptions are inappropriate in the\npresence of dataset shift, when the misclassification rates in the training\npopulation are not representative of those for the test population.\nQuantification under dataset shift has been addressed only for single-class\n(categorical) predictions and assuming perfect knowledge of the true labels on\na small subset of the test population. We propose generalized Bayes\nquantification learning (GBQL) that uses the entire compositional predictions\nfrom probabilistic classifiers and allows for uncertainty in true class labels\nfor the limited labeled test data. Instead of positing a full model, we use a\nmodel-free Bayesian estimating equation approach to compositional data based\nonly on a first-moment assumption. The idea will be useful in Bayesian\ncompositional data analysis in general as it is robust to different generating\nmechanisms for compositional data and includes categorical outputs as a special\ncase. We show how our method yields existing quantification approaches as\nspecial cases. Extension to an ensemble GBQL that uses predictions from\nmultiple classifiers yielding inference robust to inclusion of a poor\nclassifier is discussed. We outline a fast and efficient Gibbs sampler using a\nrounding and coarsening approximation to the loss functions. We also establish\nposterior consistency, asymptotic normality and valid coverage of interval\nestimates from GBQL, as well as finite sample posterior concentration rate.\nEmpirical performance of GBQL is demonstrated through simulations and analysis\nof real data with evident dataset shift.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 15:04:06 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2021 01:31:44 GMT"}], "update_date": "2021-02-18", "authors_parsed": [["Fiksel", "Jacob", ""], ["Datta", "Abhirup", ""], ["Amouzou", "Agbessi", ""], ["Zeger", "Scott", ""]]}, {"id": "2001.05444", "submitter": "Stephanie Zonszein", "authors": "Peter M. Aronow (1), Dean Eckles (2), Cyrus Samii (3), Stephanie\n  Zonszein (3) ((1) Yale, (2) MIT, (3) NYU)", "title": "Spillover Effects in Experimental Data", "comments": "Forthcoming in Advances in Experimental Political Science, J. N.\n  Druckman and D. P. Green, eds. Cambridge University Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present current methods for estimating treatment effects and spillover\neffects under \"interference\", a term which covers a broad class of situations\nin which a unit's outcome depends not only on treatments received by that unit,\nbut also on treatments received by other units. To the extent that units react\nto each other, interact, or otherwise transmit effects of treatments, valid\ninference requires that we account for such interference, which is a departure\nfrom the traditional assumption that units' outcomes are affected only by their\nown treatment assignment. Interference and associated spillovers may be a\nnuisance or they may be of substantive interest to the researcher. In this\nchapter, we focus on interference in the context of randomized experiments. We\nreview methods for when interference happens in a general network setting. We\nthen consider the special case where interference is contained within a\nhierarchical structure. Finally, we discuss the relationship between\ninterference and contagion. We use the interference R package and simulated\ndata to illustrate key points. We consider efficient designs that allow for\nestimation of the treatment and spillover effects and discuss recent empirical\nstudies that try to capture such effects.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 17:37:58 GMT"}], "update_date": "2020-01-16", "authors_parsed": [["Aronow", "Peter M.", "", "Yale"], ["Eckles", "Dean", "", "MIT"], ["Samii", "Cyrus", "", "NYU"], ["Zonszein", "Stephanie", "", "NYU"]]}, {"id": "2001.05513", "submitter": "Thomas Berrett", "authors": "Thomas B. Berrett, Ioannis Kontoyiannis, Richard J. Samworth", "title": "Optimal rates for independence testing via $U$-statistic permutation\n  tests", "comments": "58 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of independence testing given independent and\nidentically distributed pairs taking values in a $\\sigma$-finite, separable\nmeasure space. Defining a natural measure of dependence $D(f)$ as the squared\n$L^2$-distance between a joint density $f$ and the product of its marginals, we\nfirst show that there is no valid test of independence that is uniformly\nconsistent against alternatives of the form $\\{f: D(f) \\geq \\rho^2 \\}$. We\ntherefore restrict attention to alternatives that impose additional\nSobolev-type smoothness constraints, and define a permutation test based on a\nbasis expansion and a $U$-statistic estimator of $D(f)$ that we prove is\nminimax optimal in terms of its separation rates in many instances. Finally,\nfor the case of a Fourier basis on $[0,1]^2$, we provide an approximation to\nthe power function that offers several additional insights. Our methodology is\nimplemented in the R package USP.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 19:04:23 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2020 11:50:28 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Kontoyiannis", "Ioannis", ""], ["Samworth", "Richard J.", ""]]}, {"id": "2001.05520", "submitter": "Philip White", "authors": "Philip A. White, Durban G. Keeler, Summer Rupper", "title": "Hierarchical Integrated Spatial Process Modeling of Monotone West\n  Antarctic Snow Density Curves", "comments": null, "journal-ref": "the Annals of Applied Statistics 2021, Vol. 15, No. 2, 556-571", "doi": "10.1214/21-AOAS1443", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Snow density estimates below the surface, used with airplane-acquired\nice-penetrating radar measurements, give a site-specific history of snow water\naccumulation. Because it is infeasible to drill snow cores across all of\nAntarctica to measure snow density and because it is critical to understand how\nclimatic changes are affecting the world's largest freshwater reservoir, we\ndevelop methods that enable snow density estimation with uncertainty in regions\nwhere snow cores have not been drilled.\n  In inland West Antarctica, snow density increases monotonically as a function\nof depth, except for possible micro-scale variability or measurement error, and\nit cannot exceed the density of ice. We present a novel class of integrated\nspatial process models that allow interpolation of monotone snow density\ncurves. For computational feasibility, we construct the space-depth process\nthrough kernel convolutions of log-Gaussian spatial processes. We discuss model\ncomparison, model fitting, and prediction. Using this model, we extend\nestimates of snow density beyond the depth of the original core and estimate\nsnow density curves where snow cores have not been drilled. Along flight lines\nwith ice-penetrating radar, we use interpolated snow density curves to estimate\nrecent water accumulation and find predominantly decreasing water accumulation\nover recent decades.\n", "versions": [{"version": "v1", "created": "Wed, 15 Jan 2020 19:21:59 GMT"}, {"version": "v2", "created": "Fri, 17 Jan 2020 15:32:22 GMT"}, {"version": "v3", "created": "Tue, 25 Aug 2020 21:15:38 GMT"}, {"version": "v4", "created": "Mon, 19 Jul 2021 16:32:16 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["White", "Philip A.", ""], ["Keeler", "Durban G.", ""], ["Rupper", "Summer", ""]]}, {"id": "2001.05602", "submitter": "Qiong Zhang", "authors": "Ye Chen, Qiong Zhang, Mingyang Li, Wenjun Cai", "title": "Sequential Selection for Accelerated Life Testing via Approximate\n  Bayesian Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accelerated life testing (ALT) is typically used to assess the reliability of\nmaterial's lifetime under desired stress levels. Recent advances in material\nengineering have made a variety of material alternatives readily available. To\nidentify the most reliable material setting with efficient experimental design,\na sequential test planning strategy is preferred. To guarantee a tractable\nstatistical mechanism for information collection and update, we develop\nexplicit model parameter update formulas via approximate Bayesian inference.\nTheories show that our explicit update formulas give consistent parameter\nestimates. Simulation study and a case study show that the proposed sequential\nselection approach can significantly improve the probability of identifying the\nmaterial alternative with best reliability performance over other design\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 01:05:46 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Chen", "Ye", ""], ["Zhang", "Qiong", ""], ["Li", "Mingyang", ""], ["Cai", "Wenjun", ""]]}, {"id": "2001.05729", "submitter": "Marco Stefanucci", "authors": "Marco Stefanucci, Antonio Canale", "title": "Multiscale stick-breaking mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of multiscale stick-breaking mixture models for\nBayesian nonparametric density estimation. The Bayesian nonparametric\nliterature is dominated by single scale methods, exception made for P\\`olya\ntrees and allied approaches. Our proposal is based on a mixture specification\nexploiting an infinitely-deep binary tree of random weights that grows\naccording to a multiscale generalization of a large class of stick-breaking\nprocesses; this multiscale stick-breaking is paired with specific stochastic\nprocesses generating sequences of parameters that induce stochastically ordered\nkernel functions. Properties of this family of multiscale stick-breaking\nmixtures are described. Focusing on a Gaussian specification, a Markov Chain\nMontecarlo algorithm for posterior computation is introduced. The performance\nof the method is illustrated analyzing both synthetic and real data sets. The\nmethod is well-suited for data living in $\\mathbb{R}$ and is able to detect\ndensities with varying degree of smoothness and local features.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 10:21:45 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Stefanucci", "Marco", ""], ["Canale", "Antonio", ""]]}, {"id": "2001.05889", "submitter": "Sebastiano Grazzi", "authors": "Joris Bierkens, Sebastiano Grazzi, Frank van der Meulen and Moritz\n  Schauer", "title": "A piecewise deterministic Monte Carlo method for diffusion bridges", "comments": "31 pages, 11 figures. Implementation at\n  https://github.com/SebaGraz/ZZDiffusionBridge/", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the use of the Zig-Zag sampler to the problem of sampling\nconditional diffusion processes (diffusion bridges). The Zig-Zag sampler is a\nrejection-free sampling scheme based on a non-reversible continuous piecewise\ndeterministic Markov process. Similar to the L\\'evy-Ciesielski construction of\na Brownian motion, we expand the diffusion path in a truncated Faber-Schauder\nbasis. The coefficients within the basis are sampled using a Zig-Zag sampler. A\nkey innovation is the use of the fully local Algorithm for the Zig-Zag sampler\nthat allows to exploit the sparsity structure implied by the dependency graph\nof the coefficients and by the subsampling technique to reduce the complexity\nof the algorithm. We illustrate the performance of the proposed methods in a\nnumber of examples.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 15:29:28 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2020 07:24:26 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 09:02:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bierkens", "Joris", ""], ["Grazzi", "Sebastiano", ""], ["van der Meulen", "Frank", ""], ["Schauer", "Moritz", ""]]}, {"id": "2001.05965", "submitter": "Adam Sykulski Dr", "authors": "Adam M. Sykulski, Sofia C. Olhede, Hanna M. Sykulska-Lawrence", "title": "The Elliptical Ornstein-Uhlenbeck Process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.EP eess.SP physics.data-an stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the elliptical Ornstein-Uhlenbeck (OU) process, which is a\ngeneralisation of the well-known univariate OU process to bivariate time\nseries. This process maps out elliptical stochastic oscillations over time in\nthe complex plane, which are observed in many applications of coupled bivariate\ntime series. The appeal of the model is that elliptical oscillations are\ngenerated using one simple first order SDE, whereas alternative models require\nmore complicated vectorised or higher order SDE representations. The second\nuseful feature is that parameter estimation can be performed robustly in the\nfrequency domain using only the modelled and observed power spectral density,\nwithout having to model and compute cross spectra of individual time series\ncomponents. We determine properties of the model including the conditions for\nstationarity, and the geometrical structure of the elliptical oscillations. We\ndemonstrate the utility of the model by measuring periodic and elliptical\nproperties of Earth's polar motion.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 17:53:47 GMT"}, {"version": "v2", "created": "Wed, 26 Aug 2020 20:49:30 GMT"}, {"version": "v3", "created": "Mon, 28 Dec 2020 23:56:49 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Sykulski", "Adam M.", ""], ["Olhede", "Sofia C.", ""], ["Sykulska-Lawrence", "Hanna M.", ""]]}, {"id": "2001.06027", "submitter": "David Benkeser", "authors": "David Benkeser", "title": "Nonparametric inference for interventional effects with multiple\n  mediators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the pathways whereby an intervention has an effect on an\noutcome is a common scientific goal. A rich body of literature provides various\ndecompositions of the total intervention effect into pathway specific effects.\nInterventional direct and indirect effects provide one such decomposition.\nExisting estimators of these effects are based on parametric models with\nconfidence interval estimation facilitated via the nonparametric bootstrap. We\nprovide theory that allows for more flexible, possibly machine learning-based,\nestimation techniques to be considered. In particular, we establish weak\nconvergence results that facilitate the construction of closed-form confidence\nintervals and hypothesis tests. Finally, we demonstrate multiple robustness\nproperties of the proposed estimators. Simulations show that inference based on\nlarge-sample theory has adequate small-sample performance. Our work thus\nprovides a means of leveraging modern statistical learning techniques in\nestimation of interventional mediation effects.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 19:05:00 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Benkeser", "David", ""]]}, {"id": "2001.06049", "submitter": "Yunshu Zhang", "authors": "Shu Yang, Yunshu Zhang", "title": "Multiply robust matching estimators of average and quantile treatment\n  effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Propensity score matching has been a long-standing tradition for handling\nconfounding in causal inference, however requiring stringent model assumptions.\nIn this article, we propose double score matching(DSM) for general causal\nestimands utilizing two balancing scores including the propensity score and\nprognostic score. To gain the protection of possible model misspecification, we\nposit multiple candidate models for each score. We show that the de-biasing DSM\nestimator achieves the multiple robustness property in that it is consistent\nfor the true causal estimand if any model of the propensity score or prognostic\nscore is correct.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jan 2020 19:40:44 GMT"}, {"version": "v2", "created": "Sun, 5 Jul 2020 15:24:46 GMT"}], "update_date": "2020-07-07", "authors_parsed": [["Yang", "Shu", ""], ["Zhang", "Yunshu", ""]]}, {"id": "2001.06118", "submitter": "Florian Gunsilius", "authors": "Florian Gunsilius", "title": "Distributional synthetic controls", "comments": "keywords: causal inference; deformable templates; Fr\\'echet mean;\n  geodesic convex hull; heterogeneous treatment effects; optimal\n  transportation; synthetic controls; Wasserstein barycenter", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a generalization of the widely-used synthetic\ncontrols estimator for evaluating causal effects of policy changes. The\nproposed method can be applied to settings with individual-level- or functional\ndata and provides a geometrically faithful estimate of the entire\ncounterfactual distribution or functional parameter of interest. The technical\ncontribution is the development of a tensor-valued linear regression approach\nto efficiently compute the estimator in practice. It works as soon as the\ntarget distribution is absolutely continuous, but is also applicable in many\nsettings where the target is discrete. The method can be applied to repeated\ncross-sections or panel data and works with as little as a single pre-treatment\nperiod. We introduce novel identification results by showing that any synthetic\ncontrols method, classical or our generalization, provides the correct\ncounterfactual for causal models that are essentially affine in the unobserved\nheterogeneity. We also show that the optimal weights and the whole\ncounterfactual distribution can be consistently estimated from data using this\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 00:12:02 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 14:18:32 GMT"}, {"version": "v3", "created": "Tue, 31 Mar 2020 14:44:41 GMT"}, {"version": "v4", "created": "Wed, 10 Feb 2021 15:17:54 GMT"}], "update_date": "2021-02-11", "authors_parsed": [["Gunsilius", "Florian", ""]]}, {"id": "2001.06125", "submitter": "Anthony Scotina", "authors": "Anthony D. Scotina, Andrew R. Zullo, Robert J. Smith, Roee Gutman", "title": "Approximate Bayesian Bootstrap Procedures to Estimate Multilevel\n  Treatment Effects in Observational Studies with Application to Type 2\n  Diabetes Treatment Regimens", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized clinical trials are considered the gold standard for estimating\ncausal effects. Nevertheless, in studies that are aimed at examining adverse\neffects of interventions, such trials are often impractical because of ethical\nand financial considerations. In observational studies, matching on the\ngeneralized propensity scores was proposed as a possible solution to estimate\nthe treatment effects of multiple interventions. However, the derivation of\npoint and interval estimates for these matching procedures can become complex\nwith non-continuous or censored outcomes. We propose a novel Approximate\nBayesian Bootstrap algorithm that result in statistically valid point and\ninterval estimates of the treatment effects with categorical outcomes. The\nprocedure relies on the estimated generalized propensity scores and multiply\nimputes the unobserved potential outcomes for each unit. In addition, we\ndescribe a corresponding interpretable sensitivity analysis to examine the\nunconfoundedness assumption. We apply this approach to examines the\ncardiovascular safety of common, real-world anti-diabetic treatment regimens\nfor Type 2 diabetes mellitus in a large observational database.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 01:00:18 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Scotina", "Anthony D.", ""], ["Zullo", "Andrew R.", ""], ["Smith", "Robert J.", ""], ["Gutman", "Roee", ""]]}, {"id": "2001.06168", "submitter": "Jeevan Jankar", "authors": "Jeevan Jankar, Abhyuday Mandal, Jie Yang", "title": "Optimal Crossover Designs for Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify locally $D$-optimal crossover designs for generalized linear\nmodels. We use generalized estimating equations to estimate the model\nparameters along with their variances. To capture the dependency among the\nobservations coming from the same subject, we propose six different correlation\nstructures. We identify the optimal allocations of units for different\nsequences of treatments. For two-treatment crossover designs, we show via\nsimulations that the optimal allocations are reasonably robust to different\nchoices of the correlation structures. We discuss a real example of multiple\ntreatment crossover experiments using Latin square designs. Using a simulation\nstudy, we show that a two-stage design with our locally $D$-optimal design at\nthe second stage is more efficient than the uniform design, especially when the\nresponses from the same subject are correlated.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 06:41:57 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Jankar", "Jeevan", ""], ["Mandal", "Abhyuday", ""], ["Yang", "Jie", ""]]}, {"id": "2001.06194", "submitter": "Zhen Yu", "authors": "Ping Zhou, Zhen Yu, Jingyi Ma, Maozai Tian, and Ye Fan", "title": "Communication-Efficient Distributed Estimator for Generalized Linear\n  Models with a Diverging Number of Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed statistical inference has recently attracted immense attention.\nThe asymptotic efficiency of the maximum likelihood estimator (MLE), the\none-step MLE, and the aggregated estimating equation estimator are established\nfor generalized linear models under the \"large $n$, diverging $p_n$\" framework,\nwhere the dimension of the covariates $p_n$ grows to infinity at a polynomial\nrate $o(n^\\alpha)$ for some $0<\\alpha<1$. Then a novel method is proposed to\nobtain an asymptotically efficient estimator for large-scale distributed data\nby two rounds of communication. In this novel method, the assumption on the\nnumber of servers is more relaxed and thus practical for real-world\napplications. Simulations and a case study demonstrate the satisfactory\nfinite-sample performance of the proposed estimators.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 08:51:11 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2020 17:25:05 GMT"}], "update_date": "2020-08-14", "authors_parsed": [["Zhou", "Ping", ""], ["Yu", "Zhen", ""], ["Ma", "Jingyi", ""], ["Tian", "Maozai", ""], ["Fan", "Ye", ""]]}, {"id": "2001.06208", "submitter": "Jonas Peters", "authors": "Jonas Peters and Stefan Bauer and Niklas Pfister", "title": "Causal models for dynamical systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A probabilistic model describes a system in its observational state. In many\nsituations, however, we are interested in the system's response under\ninterventions. The class of structural causal models provides a language that\nallows us to model the behaviour under interventions. It can been taken as a\nstarting point to answer a plethora of causal questions, including the\nidentification of causal effects or causal structure learning. In this chapter,\nwe provide a natural and straight-forward extension of this concept to\ndynamical systems, focusing on continuous time models. In particular, we\nintroduce two types of causal kinetic models that differ in how the randomness\nenters into the model: it may either be considered as observational noise or as\nsystematic driving noise. In both cases, we define interventions and therefore\nprovide a possible starting point for causal inference. In this sense, the book\nchapter provides more questions than answers. The focus of the proposed causal\nkinetic models lies on the dynamics themselves rather than corresponding\nstationary distributions, for example. We believe that this is beneficial when\nthe aim is to model the full time evolution of the system and data are measured\nat different time points. Under this focus, it is natural to consider\ninterventions in the differential equations themselves.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 09:21:58 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Peters", "Jonas", ""], ["Bauer", "Stefan", ""], ["Pfister", "Niklas", ""]]}, {"id": "2001.06281", "submitter": "Stefan T\\\"ubbicke", "authors": "Stefan T\\\"ubbicke", "title": "Entropy Balancing for Continuous Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces entropy balancing for continuous treatments (EBCT) by\nextending the original entropy balancing methodology of Hainm\\\"uller (2012). In\norder to estimate balancing weights, the proposed approach solves a globally\nconvex constrained optimization problem. EBCT weights reliably eradicate\nPearson correlations between covariates and the continuous treatment variable.\nThis is the case even when other methods based on the generalized propensity\nscore tend to yield insufficient balance due to strong selection into different\ntreatment intensities. Moreover, the optimization procedure is more successful\nin avoiding extreme weights attached to a single unit. Extensive Monte-Carlo\nsimulations show that treatment effect estimates using EBCT display similar or\nlower bias and uniformly lower root mean squared error. These properties make\nEBCT an attractive method for the evaluation of continuous treatments.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 12:56:17 GMT"}, {"version": "v2", "created": "Thu, 28 May 2020 14:07:41 GMT"}], "update_date": "2020-05-29", "authors_parsed": [["T\u00fcbbicke", "Stefan", ""]]}, {"id": "2001.06384", "submitter": "Seongyong Park Mr.", "authors": "Seongyong Park and Shujaat Khan", "title": "GSSMD: New metric for robust and interpretable assay quality assessment\n  and hit selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.IT math.IT q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the high-throughput screening (HTS) campaigns, the Z'-factor and strictly\nstandardized mean difference (SSMD) are commonly used to assess the quality of\nassays and to select hits. However, these measures are vulnerable to outliers\nand their performances are highly sensitive to background distributions. Here,\nwe propose an alternative measure for assay quality assessment and hit\nselection. The proposed method is a non-parametric generalized variant of SSMD\n(GSSMD). In this paper, we have shown that the proposed method provides more\nrobust and intuitive way of assay quality assessment and hit selection.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 15:45:23 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2020 15:40:34 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Park", "Seongyong", ""], ["Khan", "Shujaat", ""]]}, {"id": "2001.06465", "submitter": "James Scott Mr", "authors": "Axel Gandy and James Scott", "title": "Unit Testing for MCMC and other Monte Carlo Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose approaches for testing implementations of Markov Chain Monte Carlo\nmethods as well as of general Monte Carlo methods. Based on statistical\nhypothesis tests, these approaches can be used in a unit testing framework to,\nfor example, check if individual steps in a Gibbs sampler or a reversible jump\nMCMC have the desired invariant distribution. Two exact tests for assessing\nwhether a given Markov chain has a specified invariant distribution are\ndiscussed. These and other tests of Monte Carlo methods can be embedded into a\nsequential method that allows low expected effort if the simulation shows the\ndesired behavior and high power if it does not. Moreover, the false rejection\nprobability can be kept arbitrarily low. For general Monte Carlo methods, this\nallows testing, for example, if a sampler has a specified distribution or if a\nsampler produces samples with the desired mean. The methods have been\nimplemented in the R-package MCUnit.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 18:29:10 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Gandy", "Axel", ""], ["Scott", "James", ""]]}, {"id": "2001.06477", "submitter": "Jonathan Bradley", "authors": "Hou-Cheng Yang and Jonathan R. Bradley", "title": "Bayesian Inference for Big Spatial Data Using Non-stationary Spectral\n  Simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is increasingly understood that the assumption of stationarity is\nunrealistic for many spatial processes. In this article, we combine dimension\nexpansion with a spectral method to model big non-stationary spatial fields in\na computationally efficient manner. Specifically, we use Mejia and\nRodriguez-Iturbe (1974)'s spectral simulation approach to simulate a spatial\nprocess with a covariogram at locations that have an expanded dimension. We\nintroduce Bayesian hierarchical modelling to dimension expansion, which\noriginally has only been modeled using a method of moments approach. In\nparticular, we simulate from the posterior distribution using a collapsed Gibbs\nsampler. Our method is both full rank and non-stationary, and can be applied to\nbig spatial data because it does not involve storing and inverting large\ncovariance matrices. Additionally, we have fewer parameters than many other\nnon-stationary spatial models. We demonstrate the wide applicability of our\napproach using a simulation study, and an application using ozone data obtained\nfrom the National Aeronautics and Space Administration (NASA).\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 18:55:06 GMT"}], "update_date": "2020-01-20", "authors_parsed": [["Yang", "Hou-Cheng", ""], ["Bradley", "Jonathan R.", ""]]}, {"id": "2001.06483", "submitter": "Liangyuan Hu", "authors": "Liangyuan Hu, Chenyang Gu, Michael Lopez, Jiayi Ji, and Juan\n  Wisnivesky", "title": "Estimation of Causal Effects of Multiple Treatments in Observational\n  Studies with a Binary Outcome", "comments": "3 figures, 3 tables. arXiv admin note: text overlap with\n  arXiv:1901.04312", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a dearth of robust methods to estimate the causal effects of\nmultiple treatments when the outcome is binary. This paper uses two unique sets\nof simulations to propose and evaluate the use of Bayesian Additive Regression\nTrees (BART) in such settings. First, we compare BART to several approaches\nthat have been proposed for continuous outcomes, including inverse probability\nof treatment weighting (IPTW), targeted maximum likelihood estimator (TMLE),\nvector matching and regression adjustment. Results suggest that under\nconditions of non-linearity and non-additivity of both the treatment assignment\nand outcome generating mechanisms, BART, TMLE and IPTW using generalized\nboosted models (GBM) provide better bias reduction and smaller root mean\nsquared error. BART and TMLE provide more consistent 95 per cent CI coverage\nand better large-sample convergence property. Second, we supply BART with a\nstrategy to identify a common support region for retaining inferential units\nand for avoiding extrapolating over areas of the covariate space where common\nsupport does not exist. BART retains more inferential units than the\ngeneralized propensity score based strategy, and shows lower bias, compared to\nTMLE or GBM, in a variety of scenarios differing by the degree of covariate\noverlap. A case study examining the effects of three surgical approaches for\nnon-small cell lung cancer demonstrates the methods.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 03:49:31 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Hu", "Liangyuan", ""], ["Gu", "Chenyang", ""], ["Lopez", "Michael", ""], ["Ji", "Jiayi", ""], ["Wisnivesky", "Juan", ""]]}, {"id": "2001.06530", "submitter": "Beatriz Galindo-Prieto", "authors": "B. Galindo-Prieto, P. Geladi, J. Trygg", "title": "Multiblock variable influence on orthogonal projections (MB-VIOP) for\n  enhanced interpretation of total, global, local and unique variations in\n  OnPLS models", "comments": "It has 54 pages (including 4 figures, 5 tables, and supporting\n  information). Submitted to peer-reviewed journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For multivariate data analysis involving only two input matrices, the\npreviously published methods for variable influence on projection (e.g.,\nVIPOPLS or VIPO2PLS) are widely used for variable selection purposes, including\n(i) variable importance assessment, (ii) dimensionality reduction of big data\nand (iii) interpretation enhancement of PLS, OPLS and O2PLS models. For\nmultiblock analysis, the OnPLS models find relationships among multiple data\nmatrices by calculating latent variables; however, a method for improving the\ninterpretation of these latent variables by assessing the importance of the\ninput variables was not available up to now.\n  A method for variable selection in multiblock analysis, called multiblock\nvariable influence on orthogonal projections (MB-VIOP) is explained in this\npaper. MB-VIOP is a model based variable selection method that uses the data\nmatrices, the scores and the normalized loadings of an OnPLS model in order to\nsort the input variables of more than two data matrices according to their\nimportance for both simplification and interpretation of the total multiblock\nmodel, and also of the unique, local and global model components separately.\nMB-VIOP has been tested using three multiblock datasets.\n  MB-VIOP assesses the variable importance in any type of data. MB-VIOP\nconnects the input variables of different data matrices according to their\nrelevance for the interpretation of each latent variable, yielding enhanced\ninterpretability for each OnPLS model component. Besides, MB-VIOP can deal with\nstrong overlapping of types of variation, as well as with many data blocks with\nvery different dimensionality. The ability of MB-VIOP for generating\ndimensionality reduced models with high interpretability makes this method\nideal for big data mining, multi-omics data integration and any study that\nrequires exploration and interpretation of large streams of data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 20:54:50 GMT"}, {"version": "v2", "created": "Sat, 6 Feb 2021 01:08:46 GMT"}], "update_date": "2021-02-09", "authors_parsed": [["Galindo-Prieto", "B.", ""], ["Geladi", "P.", ""], ["Trygg", "J.", ""]]}, {"id": "2001.06555", "submitter": "Elizabeth Ogburn", "authors": "Elizabeth L. Ogburn, Ilya Shpitser, Eric J. Tchetgen Tchetgen", "title": "Counterexamples to \"The Blessings of Multiple Causes\" by Wang and Blei", "comments": "This note has been updated (April, 2020) to respond to \"Towards\n  Clarifying the Theory of the Deconfounder\" by Yixin Wang, David M. Blei\n  (arXiv:2003.04948)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note has been updated (April, 2020) to respond to \"Towards Clarifying\nthe Theory of the Deconfounder\" by Yixin Wang, David M. Blei\n(arXiv:2003.04948). This original note, posted in January, 2020, is meant to\ncomplement our previous comment on \"The Blessings of Multiple Causes\" by Wang\nand Blei (2019). We provide a more succinct and transparent explanation of the\nfact that the deconfounder does not control for multi-cause confounding. The\nargument given in Wang and Blei (2019) makes two mistakes: (1) attempting to\ninfer independence conditional on one variable from independence conditional on\na different, unrelated variable, and (2) attempting to infer joint independence\nfrom pairwise independence. We give two simple counterexamples to the\ndeconfounder claim.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jan 2020 23:33:32 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2020 13:24:45 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2020 15:30:13 GMT"}], "update_date": "2020-04-27", "authors_parsed": [["Ogburn", "Elizabeth L.", ""], ["Shpitser", "Ilya", ""], ["Tchetgen", "Eric J. Tchetgen", ""]]}, {"id": "2001.06606", "submitter": "Xiaoming Wang", "authors": "Xiaoming Wang and Sukun Wang", "title": "Insight into bias in time-stratified case-crossover studies", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The use of case-crossover designs has become widespread in epidemiological\nand medical investigations of transient associations. However, the most popular\nreference-select strategy, the time-stratified schema, is not a suitable\nsolution for controlling bias in case-crossover studies. To prove this, we\nconducted a time series decomposition for daily ozone (O3) records; scrutinized\nthe ability of the time-stratified schema on controlling the yearly, monthly\nand weekly time trends; and found it failed on controlling the weekly time\ntrend. Based on this finding, we proposed a new logistic regression approach in\nwhich we did adjustment for the weekly time trend. A comparison between the\ntraditional model and the proposed method was done by simulation. An empirical\nstudy was conducted to explore potential associations between air pollutants\nand AMI hospitalizations. In summary, time-stratified schema provide effective\ncontrol on yearly and monthly time trends but not on weekly time trend.\nTherefore, the estimation from the traditional logistical regression basically\nreveals the effect of weekly time trend, instead of the transient effect. In\ncontrast, the proposed logistic regression with adjustment for weekly time\ntrend can effectively eliminate system bias in case-crossover studies.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 05:43:03 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Wang", "Xiaoming", ""], ["Wang", "Sukun", ""]]}, {"id": "2001.06642", "submitter": "Ben Youngman", "authors": "Benjamin D. Youngman", "title": "Flexible models for nonstationary dependence: Methodology and examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many situations when modelling environmental phenomena for which it\nis not appropriate to assume a stationary dependence structure.\n\\cite{sampson1992} proposed an approach to allowing nonstationarity in\ndependence based on a deformed space: coordinates from original geographic\n\"$G$\" space are mapped to a new dispersion \"$D$\" space in which stationary\ndependence is a reasonable assumption. \\cite{sampson1992} achieve this with two\ndeformation functions, which are chosen as thin plate splines, each\nrepresenting how one of the two coordinates in $D$-space relates to the\noriginal $G$-space coordinates. This works extends the deformation approach,\nand the dimension expansion approach of \\cite{bornn2012}, to a regression-based\nframework in which all dimensions in $D$-space are treated as \"smooths\" as\nfound, for example, in generalized additive models. The framework offers an\nintuitive and user-friendly approach to specifying $D$-space, allows different\nlevels of smoothing for dimensions in $D$-space, and allows objective inference\nfor all model parameters. Furthermore, a numerical approach is proposed to\navoid non-bijective deformations, should they occur, which applies to any\ndeformation. The proposed framework is demonstrated on the solar radiation data\nstudied in \\cite{sampson1992}, and then on an example related to risk analysis,\nwhich culminates in producing simulations of extreme rainfall for part of\nColorado, US.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jan 2020 10:03:54 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Youngman", "Benjamin D.", ""]]}, {"id": "2001.06858", "submitter": "Yuan Wang", "authors": "Ray-Bing Chen, Yuan Wang, C. F. Jeff Wu", "title": "Finding Optimal Points for Expensive Functions Using Adaptive RBF-Based\n  Surrogate Model Via Uncertainty Quantification", "comments": "35 pages, 5 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Global optimization of expensive functions has important applications in\nphysical and computer experiments. It is a challenging problem to develop\nefficient optimization scheme, because each function evaluation can be costly\nand the derivative information of the function is often not available. We\npropose a novel global optimization framework using adaptive Radial Basis\nFunctions (RBF) based surrogate model via uncertainty quantification. The\nframework consists of two iteration steps. It first employs an RBF-based\nBayesian surrogate model to approximate the true function, where the parameters\nof the RBFs can be adaptively estimated and updated each time a new point is\nexplored. Then it utilizes a model-guided selection criterion to identify a new\npoint from a candidate set for function evaluation. The selection criterion\nadopted here is a sample version of the expected improvement (EI) criterion. We\nconduct simulation studies with standard test functions, which show that the\nproposed method has some advantages, especially when the true surface is not\nvery smooth. In addition, we also propose modified approaches to improve the\nsearch performance for identifying global optimal points and to deal with the\nhigher dimension scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jan 2020 16:15:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Chen", "Ray-Bing", ""], ["Wang", "Yuan", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "2001.07042", "submitter": "Arjun Seshadri", "authors": "Arjun Seshadri, Johan Ugander", "title": "Fundamental Limits of Testing the Independence of Irrelevant\n  Alternatives in Discrete Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Multinomial Logit (MNL) model and the axiom it satisfies, the\nIndependence of Irrelevant Alternatives (IIA), are together the most widely\nused tools of discrete choice. The MNL model serves as the workhorse model for\na variety of fields, but is also widely criticized, with a large body of\nexperimental literature claiming to document real-world settings where IIA\nfails to hold. Statistical tests of IIA as a modelling assumption have been the\nsubject of many practical tests focusing on specific deviations from IIA over\nthe past several decades, but the formal size properties of hypothesis testing\nIIA are still not well understood. In this work we replace some of the\nambiguity in this literature with rigorous pessimism, demonstrating that any\ngeneral test for IIA with low worst-case error would require a number of\nsamples exponential in the number of alternatives of the choice problem. A\nmajor benefit of our analysis over previous work is that it lies entirely in\nthe finite-sample domain, a feature crucial to understanding the behavior of\ntests in the common data-poor settings of discrete choice. Our lower bounds are\nstructure-dependent, and as a potential cause for optimism, we find that if one\nrestricts the test of IIA to violations that can occur in a specific collection\nof choice sets (e.g., pairs), one obtains structure-dependent lower bounds that\nare much less pessimistic. Our analysis of this testing problem is unorthodox\nin being highly combinatorial, counting Eulerian orientations of cycle\ndecompositions of a particular bipartite graph constructed from a data set of\nchoices. By identifying fundamental relationships between the comparison\nstructure of a given testing problem and its sample efficiency, we hope these\nrelationships will help lay the groundwork for a rigorous rethinking of the IIA\ntesting problem as well as other testing problems in discrete choice.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 10:15:28 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Seshadri", "Arjun", ""], ["Ugander", "Johan", ""]]}, {"id": "2001.07064", "submitter": "Qiyang Han", "authors": "Hang Deng, Qiyang Han, Cun-Hui Zhang", "title": "Confidence intervals for multiple isotonic regression and other monotone\n  models", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing pointwise confidence intervals in the\nmultiple isotonic regression model. Recently, [HZ19] obtained a pointwise limit\ndistribution theory for the so-called block max-min and min-max estimators\n[FLN17] in this model, but inference remains a difficult problem due to the\nnuisance parameter in the limit distribution that involves multiple unknown\npartial derivatives of the true regression function.\n  In this paper, we show that this difficult nuisance parameter can be\neffectively eliminated by taking advantage of information beyond point\nestimates in the block max-min and min-max estimators. Formally, let\n$\\hat{u}(x_0)$ (resp. $\\hat{v}(x_0)$) be the maximizing lower-left (resp.\nminimizing upper-right) vertex in the block max-min (resp. min-max) estimator,\nand $\\hat{f}_n$ be the average of the block max-min and min-max estimators. If\nall (first-order) partial derivatives of $f_0$ are non-vanishing at $x_0$, then\nthe following pivotal limit distribution theory holds: $$\n\\sqrt{n_{\\hat{u},\\hat{v}}(x_0)}\\big(\\hat{f}_n(x_0)-f_0(x_0)\\big)\\rightsquigarrow\n\\sigma\\cdot \\mathbb{L}_{1_d}. $$ Here $n_{\\hat{u},\\hat{v}}(x_0)$ is the number\nof design points in the block $[\\hat{u}(x_0),\\hat{v}(x_0)]$, $\\sigma$ is the\nstandard deviation of the errors, and $\\mathbb{L}_{1_d}$ is a universal limit\ndistribution free of nuisance parameters. This immediately yields confidence\nintervals for $f_0(x_0)$ with asymptotically exact confidence level and oracle\nlength. Notably, the construction of the confidence intervals, even new in the\nunivariate setting, requires no more efforts than performing an isotonic\nregression for once using the block max-min and min-max estimators, and can be\neasily adapted to other common monotone models. Extensive simulations are\ncarried out to support our theory.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 11:32:12 GMT"}, {"version": "v2", "created": "Wed, 30 Sep 2020 17:12:03 GMT"}], "update_date": "2020-10-01", "authors_parsed": [["Deng", "Hang", ""], ["Han", "Qiyang", ""], ["Zhang", "Cun-Hui", ""]]}, {"id": "2001.07147", "submitter": "Wen Wei Loh", "authors": "Wen Wei Loh, Beatrijs Moerkerke, Tom Loeys, Stijn Vansteelandt", "title": "Non-linear Mediation Analysis with High-dimensional Mediators whose\n  Causal Structure is Unknown", "comments": "30 pages, 2 figures, 3 tables", "journal-ref": null, "doi": "10.1111/biom.13402", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With multiple potential mediators on the causal pathway from a treatment to\nan outcome, we consider the problem of decomposing the effects along multiple\npossible causal path(s) through each distinct mediator. Under Pearl's\npath-specific effects framework (Pearl, 2001; Avin et al., 2005), such\nfine-grained decompositions necessitate stringent assumptions, such as\ncorrectly specifying the causal structure among the mediators, and there being\nno unobserved confounding among the mediators. In contrast, interventional\ndirect and indirect effects for multiple mediators (Vansteelandt and Daniel,\n2017) can be identified under much weaker conditions, while providing\nscientifically relevant causal interpretations. Nonetheless, current estimation\napproaches require (correctly) specifying a model for the joint mediator\ndistribution, which can be difficult when there is a high-dimensional set of\npossibly continuous and non-continuous mediators. In this article, we avoid the\nneed to model this distribution, by developing a definition of interventional\neffects previously suggested by VanderWeele and Tchetgen Tchetgen (2017) for\nlongitudinal mediation. We propose a novel estimation strategy that uses\nnon-parametric estimates of the (counterfactual) mediator distributions.\nNon-continuous outcomes can be accommodated using non-linear outcome models.\nEstimation proceeds via Monte Carlo integration. The procedure is illustrated\nusing publicly available genomic data (Huang and Pan, 2016) to assess the\ncausal effect of a microRNA expression on the three-month mortality of brain\ncancer patients that is potentially mediated by expression values of multiple\ngenes.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 15:37:49 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2020 14:52:26 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Loh", "Wen Wei", ""], ["Moerkerke", "Beatrijs", ""], ["Loeys", "Tom", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2001.07160", "submitter": "Leigh Shlomovich", "authors": "Leigh Shlomovich, Edward Cohen, Niall Adams, Lekha Patel", "title": "A Monte Carlo EM Algorithm for the Parameter Estimation of Aggregated\n  Hawkes Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key difficulty that arises from real event data is imprecision in the\nrecording of event time-stamps. In many cases, retaining event times with a\nhigh precision is expensive due to the sheer volume of activity. Combined with\npractical limits on the accuracy of measurements, aggregated data is common. In\norder to use point processes to model such event data, tools for handling\nparameter estimation are essential. Here we consider parameter estimation of\nthe Hawkes process, a type of self-exciting point process that has found\napplication in the modeling of financial stock markets, earthquakes and social\nmedia cascades. We develop a novel optimization approach to parameter\nestimation of aggregated Hawkes processes using a Monte Carlo\nExpectation-Maximization (MC-EM) algorithm. Through a detailed simulation\nstudy, we demonstrate that existing methods are capable of producing severely\nbiased and highly variable parameter estimates and that our novel MC-EM method\nsignificantly outperforms them in all studied circumstances. These results\nhighlight the importance of correct handling of aggregated data.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 16:17:00 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Shlomovich", "Leigh", ""], ["Cohen", "Edward", ""], ["Adams", "Niall", ""], ["Patel", "Lekha", ""]]}, {"id": "2001.07256", "submitter": "Spencer Woody", "authors": "Spencer Woody, Carlos M. Carvalho and Jared S. Murray", "title": "Bayesian inference for treatment effects under nested subsets of\n  controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When constructing a model to estimate the causal effect of a treatment, it is\nnecessary to control for other factors which may have confounding effects.\nBecause the ignorability assumption is not testable, however, it is usually\nunclear which set of controls is appropriate, and effect estimation is\ngenerally sensitive to this choice. A common approach in this case is to fit\nseveral models, each with a different set of controls, but it is difficult to\nreconcile inference under the multiple resulting posterior distributions for\nthe treatment effect. Therefore we propose a two-stage approach to measure the\nsensitivity of effect estimation with respect to control specification. In the\nfirst stage, a model is fit with all available controls using a prior carefully\nselected to adjust for confounding. In the second stage, posterior\ndistributions are calculated for the treatment effect under nested sets of\ncontrols by propagating posterior uncertainty in the original model. We\ndemonstrate how our approach can be used to detect the most significant\nconfounders in a dataset, and apply it in a sensitivity analysis of an\nobservational study measuring the effect of legalized abortion on crime rates.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jan 2020 21:20:24 GMT"}, {"version": "v2", "created": "Mon, 27 Jan 2020 22:53:02 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Woody", "Spencer", ""], ["Carvalho", "Carlos M.", ""], ["Murray", "Jared S.", ""]]}, {"id": "2001.07383", "submitter": "Subhajit Dutta Dr.", "authors": "Soumya Das, Subhajit Dutta, Radhenduhska Srivastava", "title": "On Construction of Higher Order Kernels Using Fourier Transforms and\n  Covariance Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show that a suitably chosen covariance function of a\ncontinuous time, second order stationary stochastic process can be viewed as a\nsymmetric higher order kernel. This leads to the construction of a higher order\nkernel by choosing an appropriate covariance function. An optimal choice of the\nconstructed higher order kernel that partially minimizes the mean integrated\nsquare error of the kernel density estimator is also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 08:32:55 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Das", "Soumya", ""], ["Dutta", "Subhajit", ""], ["Srivastava", "Radhenduhska", ""]]}, {"id": "2001.07623", "submitter": "David Miller", "authors": "David L Miller, Richard Glennie, Andrew E Seaton", "title": "Understanding the stochastic partial differential equation approach to\n  smoothing", "comments": "23 pages, 4 figures. JABES (2019)", "journal-ref": null, "doi": "10.1007/s13253-019-00377-z", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Correlation and smoothness are terms used to describe a wide variety of\nrandom quantities. In time, space, and many other domains, they both imply the\nsame idea: quantities that occur closer together are more similar than those\nfurther apart. Two popular statistical models that represent this idea are\nbasis-penalty smoothers (Wood, 2017) and stochastic partial differential\nequations (SPDE) (Lindgren et al., 2011). In this paper, we discuss how the\nSPDE can be interpreted as a smoothing penalty and can be fitted using the R\npackage mgcv, allowing practitioners with existing knowledge of smoothing\npenalties to better understand the implementation and theory behind the SPDE\napproach.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 15:57:15 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2020 14:39:32 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Miller", "David L", ""], ["Glennie", "Richard", ""], ["Seaton", "Andrew E", ""]]}, {"id": "2001.07624", "submitter": "Glen Martin Dr", "authors": "Glen P. Martin, Matthew Sperrin, Kym I.E. Snell, Iain Buchan and\n  Richard D. Riley", "title": "Clinical Prediction Models to Predict the Risk of Multiple Binary\n  Outcomes: a comparison of approaches", "comments": "34 pages, 2 tables and 5 figures", "journal-ref": null, "doi": "10.1002/sim.8787", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical prediction models (CPMs) are used to predict clinically relevant\noutcomes or events. Typically, prognostic CPMs are derived to predict the risk\nof a single future outcome. However, with rising emphasis on the prediction of\nmulti-morbidity, there is growing need for CPMs to simultaneously predict risks\nfor each of multiple future outcomes. A common approach to multi-outcome risk\nprediction is to derive a CPM for each outcome separately, then multiply the\npredicted risks. This approach is only valid if the outcomes are conditionally\nindependent given the covariates, and it fails to exploit the potential\nrelationships between the outcomes. This paper outlines several approaches that\ncould be used to develop prognostic CPMs for multiple outcomes. We consider\nfour methods, ranging in complexity and assumed conditional independence\nassumptions: namely, probabilistic classifier chain, multinomial logistic\nregression, multivariate logistic regression, and a Bayesian probit model.\nThese are compared with methods that rely on conditional independence: separate\nunivariate CPMs and stacked regression. Employing a simulation study and\nreal-world example via the MIMIC-III database, we illustrate that CPMs for\njoint risk prediction of multiple outcomes should only be derived using methods\nthat model the residual correlation between outcomes. In such a situation, our\nresults suggest that probabilistic classification chains, multinomial logistic\nregression or the Bayesian probit model are all appropriate choices. We call\ninto question the development of CPMs for each outcome in isolation when\nmultiple correlated or structurally related outcomes are of interest and\nrecommend more holistic risk prediction.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 15:57:21 GMT"}], "update_date": "2020-10-29", "authors_parsed": [["Martin", "Glen P.", ""], ["Sperrin", "Matthew", ""], ["Snell", "Kym I. E.", ""], ["Buchan", "Iain", ""], ["Riley", "Richard D.", ""]]}, {"id": "2001.07773", "submitter": "Damjan Krstajic", "authors": "Damjan Krstajic", "title": "Missed opportunities in large scale comparison of QSAR and conformal\n  prediction methods and their applications in drug discovery", "comments": null, "journal-ref": "J Cheminform 11, 65 (2019)", "doi": "10.1186/s13321-019-0387-y", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Bosc et al. (J Cheminform 11(1): 4, 2019), published an article\ndescribing a case study that directly compares conformal predictions with\ntraditional QSAR methods for large-scale predictions of target-ligand binding.\nWe consider this study to be very important. Unfortunately, we have found\nseveral issues in the authors' approach as well as in the presentation of their\nfindings.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 21:03:35 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Krstajic", "Damjan", ""]]}, {"id": "2001.07778", "submitter": "Hugo Maruri-Aguilar", "authors": "Hugo Maruri-Aguilar and Simon Lunagomez", "title": "Lasso for hierarchical polynomial models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a polynomial regression model, the divisibility conditions implicit in\npolynomial hierarchy give way to a natural construction of constraints for the\nmodel parameters. We use this principle to derive versions of strong and weak\nhierarchy and to extend existing work in the literature, which at the moment is\nonly concerned with models of degree two. We discuss how to estimate parameters\nin lasso using standard quadratic programming techniques and apply our proposal\nto both simulated data and examples from the literature. The proposed\nmethodology compares favorably with existing techniques in terms of low\nvalidation error and model size.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jan 2020 21:26:24 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Maruri-Aguilar", "Hugo", ""], ["Lunagomez", "Simon", ""]]}, {"id": "2001.07835", "submitter": "Zhimei Ren", "authors": "Zhimei Ren, Emmanuel Cand\\`es", "title": "Knockoffs with Side Information", "comments": "29 pages, 9 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of assessing the importance of multiple variables or\nfactors from a dataset when side information is available. In principle, using\nside information can allow the statistician to pay attention to variables with\na greater potential, which in turn, may lead to more discoveries. We introduce\nan adaptive knockoff filter, which generalizes the knockoff procedure (Barber\nand Cand\\`es, 2015; Cand\\`es et al., 2018) in that it uses both the data at\nhand and side information to adaptively order the variables under study and\nfocus on those that are most promising. Adaptive knockoffs controls the\nfinite-sample false discovery rate (FDR) and we demonstrate its power by\ncomparing it with other structured multiple testing methods. We also apply our\nmethodology to real genetic data in order to find associations between genetic\nvariants and various phenotypes such as Crohn's disease and lipid levels. Here,\nadaptive knockoffs makes more discoveries than reported in previous studies on\nthe same datasets.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 01:09:36 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Ren", "Zhimei", ""], ["Cand\u00e8s", "Emmanuel", ""]]}, {"id": "2001.07859", "submitter": "Christopher Urban", "authors": "Christopher J. Urban and Daniel J. Bauer", "title": "A Deep Learning Algorithm for High-Dimensional Exploratory Item Factor\n  Analysis", "comments": "30 pages; 12 figures; accepted for publication in Psychometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal maximum likelihood (MML) estimation is the preferred approach to\nfitting item response theory models in psychometrics due to the MML estimator's\nconsistency, normality, and efficiency as the sample size tends to infinity.\nHowever, state-of-the-art MML estimation procedures such as the\nMetropolis-Hastings Robbins-Monro (MH-RM) algorithm as well as approximate MML\nestimation procedures such as variational inference (VI) are computationally\ntime-consuming when the sample size and the number of latent factors are very\nlarge. In this work, we investigate a deep learning-based VI algorithm for\nexploratory item factor analysis (IFA) that is computationally fast even in\nlarge data sets with many latent factors. The proposed approach applies a deep\nartificial neural network model called an importance-weighted autoencoder\n(IWAE) for exploratory IFA. The IWAE approximates the MML estimator using an\nimportance sampling technique wherein increasing the number of\nimportance-weighted (IW) samples drawn during fitting improves the\napproximation, typically at the cost of decreased computational efficiency. We\nprovide a real data application that recovers results aligning with\npsychological theory across random starts. Via simulation studies, we show that\nthe IWAE yields more accurate estimates as either the sample size or the number\nof IW samples increases (although factor correlation and intercepts estimates\nexhibit some bias) and obtains similar results to MH-RM in less time. Our\nsimulations also suggest that the proposed approach performs similarly to and\nis potentially faster than constrained joint maximum likelihood estimation, a\nfast procedure that is consistent when the sample size and the number of items\nsimultaneously tend to infinity.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 03:02:34 GMT"}, {"version": "v2", "created": "Mon, 14 Sep 2020 00:21:19 GMT"}, {"version": "v3", "created": "Mon, 25 Jan 2021 18:24:46 GMT"}, {"version": "v4", "created": "Thu, 4 Feb 2021 17:29:22 GMT"}], "update_date": "2021-02-05", "authors_parsed": [["Urban", "Christopher J.", ""], ["Bauer", "Daniel J.", ""]]}, {"id": "2001.08038", "submitter": "Andrew Manderson", "authors": "Andrew A. Manderson and Robert J. B. Goudie", "title": "A numerically stable algorithm for integrating Bayesian models using\n  Markov melding", "comments": "18 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When statistical analyses consider multiple data sources, Markov melding\nprovides a method for combining the source-specific Bayesian models. Models\noften contain different quantities of information due to variation in the\nrichness of model-specific data, or availability of model-specific prior\ninformation. We show that this can make the multi-stage Markov chain Monte\nCarlo sampler employed by Markov melding unstable and unreliable. We propose a\nrobust multi-stage algorithm that estimates the required prior marginal\nself-density ratios using weighted samples, dramatically improving accuracy in\nthe tails of the distribution, thus stabilising the algorithm and providing\nreliable inference. We demonstrate our approach using an evidence synthesis for\ninferring HIV prevalence, and an evidence synthesis of A/H1N1 influenza.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 14:45:22 GMT"}], "update_date": "2020-01-23", "authors_parsed": [["Manderson", "Andrew A.", ""], ["Goudie", "Robert J. B.", ""]]}, {"id": "2001.08089", "submitter": "Jakob Dambon", "authors": "Jakob A. Dambon (1 and 2), Fabio Sigrist (2), Reinhard Furrer (1) ((1)\n  University of Zurich, (2) Lucerne University of Applied Sciences and Arts)", "title": "Maximum Likelihood Estimation of Spatially Varying Coefficient Models\n  for Large Data with an Application to Real Estate Price Prediction", "comments": "revision: 35 pages, 14 figures, typo in likelihood corrected, DOI\n  added", "journal-ref": null, "doi": "10.1016/j.spasta.2020.100470", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression models for spatial data, it is often assumed that the marginal\neffects of covariates on the response are constant over space. In practice,\nthis assumption might often be questionable. In this article, we show how a\nGaussian process-based spatially varying coefficient (SVC) model can be\nestimated using maximum likelihood estimation (MLE). In addition, we present an\napproach that scales to large data by applying covariance tapering. We compare\nour methodology to existing methods such as a Bayesian approach using the\nstochastic partial differential equation (SPDE) link, geographically weighted\nregression (GWR), and eigenvector spatial filtering (ESF) in both a simulation\nstudy and an application where the goal is to predict prices of real estate\napartments in Switzerland. The results from both the simulation study and\napplication show that the MLE approach results in increased predictive accuracy\nand more precise estimates. Since we use a model-based approach, we can also\nprovide predictive variances. In contrast to existing model-based approaches,\nour method scales better to data where both the number of spatial points is\nlarge and the number of spatially varying covariates is moderately-sized, e.g.,\nabove ten.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 15:46:29 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 11:06:00 GMT"}, {"version": "v3", "created": "Fri, 24 Jan 2020 07:31:48 GMT"}, {"version": "v4", "created": "Fri, 31 Jul 2020 14:20:51 GMT"}, {"version": "v5", "created": "Thu, 12 Nov 2020 16:33:17 GMT"}], "update_date": "2020-11-13", "authors_parsed": [["Dambon", "Jakob A.", "", "1 and 2"], ["Sigrist", "Fabio", ""], ["Furrer", "Reinhard", ""]]}, {"id": "2001.08090", "submitter": "Romain Bey", "authors": "R. Bey, R. Goussault, M. Benchoufi, R. Porcher", "title": "Stratified cross-validation for unbiased and privacy-preserving\n  federated learning", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": "10.1093/jamia/ocaa096", "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale collections of electronic records constitute both an opportunity\nfor the development of more accurate prediction models and a threat for\nprivacy. To limit privacy exposure new privacy-enhancing techniques are\nemerging such as federated learning which enables large-scale data analysis\nwhile avoiding the centralization of records in a unique database that would\nrepresent a critical point of failure. Although promising regarding privacy\nprotection, federated learning prevents using some data-cleaning algorithms\nthus inducing new biases. In this work we focus on the recurrent problem of\nduplicated records that, if not handled properly, may cause over-optimistic\nestimations of a model's performances. We introduce and discuss stratified\ncross-validation, a validation methodology that leverages stratification\ntechniques to prevent data leakage in federated learning settings without\nrelying on demanding deduplication algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 15:49:34 GMT"}, {"version": "v2", "created": "Thu, 23 Jan 2020 08:43:26 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Bey", "R.", ""], ["Goussault", "R.", ""], ["Benchoufi", "M.", ""], ["Porcher", "R.", ""]]}, {"id": "2001.08327", "submitter": "Himel Mallick", "authors": "Himel Mallick, Rahim Alhamzawi, Erina Paul, Vladimir Svetnik", "title": "The Reciprocal Bayesian LASSO", "comments": "35 pages, 2 figures, 4 tables; includes revised simulation and real\n  data analysis results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reciprocal LASSO (rLASSO) regularization employs a decreasing penalty\nfunction as opposed to conventional penalization approaches that use increasing\npenalties on the coefficients, leading to stronger parsimony and superior model\nselection relative to traditional shrinkage methods. Here we consider a fully\nBayesian formulation of the rLASSO problem, which is based on the observation\nthat the rLASSO estimate for linear regression parameters can be interpreted as\na Bayesian posterior mode estimate when the regression parameters are assigned\nindependent inverse Laplace priors. Bayesian inference from this posterior is\npossible using an expanded hierarchy motivated by a scale mixture of double\nPareto or truncated normal distributions. On simulated and real datasets, we\nshow that the Bayesian formulation outperforms its classical cousin in\nestimation, prediction, and variable selection across a wide range of scenarios\nwhile offering the advantage of posterior inference. Finally, we discuss other\nvariants of this new approach and provide a unified framework for variable\nselection using flexible reciprocal penalties. All methods described in this\npaper are publicly available as an R package at:\nhttps://github.com/himelmallick/BayesRecipe.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 01:21:59 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 18:33:20 GMT"}, {"version": "v3", "created": "Sat, 29 May 2021 11:06:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Mallick", "Himel", ""], ["Alhamzawi", "Rahim", ""], ["Paul", "Erina", ""], ["Svetnik", "Vladimir", ""]]}, {"id": "2001.08336", "submitter": "Yang Chen", "authors": "Yang Chen, Ruobin Gong, Min-ge Xie", "title": "Geometric Conditions for the Discrepant Posterior Phenomenon and\n  Connections to Simpson's Paradox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrepant posterior phenomenon (DPP) is a counterintuitive phenomenon\nthat occurs in the Bayesian analysis of multivariate parameters. It refers to\nwhen an estimate of a marginal parameter obtained from the posterior is more\nextreme than both of those obtained using either the prior or the likelihood\nalone. Inferential claims that exhibit DPP defy intuition, and the phenomenon\ncan be surprisingly ubiquitous in well-behaved Bayesian models. Using point\nestimation as an example, we derive conditions under which the DPP occurs in\nBayesian models with exponential quadratic likelihoods, including Gaussian\nmodels and those with local asymptotic normality property, with conjugate\nmultivariate Gaussian priors. We also examine the DPP for the Binomial model,\nin which the posterior mean is not a linear combination of that of the prior\nand the likelihood. We provide an intuitive geometric interpretation of the\nphenomenon and show that there exists a non-trivial space of marginal\ndirections such that the DPP occurs. We further relate the phenomenon to the\nSimpson's paradox and discover their deep-rooted connection that is associated\nwith marginalization. We also draw connections with Bayesian computational\nalgorithms when difficult geometry exists. Theoretical results are complemented\nby numerical illustrations. Scenarios covered in this study have implications\nfor parameterization, sensitivity analysis, and prior choice for Bayesian\nmodeling.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 01:46:59 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Chen", "Yang", ""], ["Gong", "Ruobin", ""], ["Xie", "Min-ge", ""]]}, {"id": "2001.08363", "submitter": "Aaron Molstad", "authors": "Aaron J. Molstad, Wei Sun, Li Hsu", "title": "A covariance-enhanced approach to multi-tissue joint eQTL mapping with\n  application to transcriptome-wide association studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcriptome-wide association studies based on genetically predicted gene\nexpression have the potential to identify novel regions associated with various\ncomplex traits. It has been shown that incorporating expression quantitative\ntrait loci (eQTLs) corresponding to multiple tissue types can improve power for\nassociation studies involving complex etiology. In this article, we propose a\nnew multivariate response linear regression model and method for predicting\ngene expression in multiple tissues simultaneously. Unlike existing methods for\nmulti-tissue joint eQTL mapping, our approach incorporates tissue-tissue\nexpression correlation, which allows us to more efficiently handle missing\nexpression measurements and more accurately predict gene expression using a\nweighted summation of eQTL genotypes. We show through simulation studies that\nour approach performs better than the existing methods in many scenarios. We\nuse our method to estimate eQTL weights for 29 tissues collected by GTEx, and\nshow that our approach significantly improves expression prediction accuracy\ncompared to competitors. Using our eQTL weights, we perform a\nmulti-tissue-based S-MultiXcan transcriptome-wide association study and show\nthat our method leads to more discoveries in novel regions and more discoveries\noverall than the existing methods. Estimated eQTL weights are available for\ndownload online at github.com/ajmolstad/MTeQTLResults.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 04:09:43 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Molstad", "Aaron J.", ""], ["Sun", "Wei", ""], ["Hsu", "Li", ""]]}, {"id": "2001.08431", "submitter": "Thomas Yee", "authors": "Thomas William Yee", "title": "On the Hauck-Donner Effect in Wald Tests: Detection, Tipping Points, and\n  Parameter Space Characterization", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wald test remains ubiquitous in statistical practice despite shortcomings\nsuch as its inaccuracy in small samples and lack of invariance under\nreparameterization. This paper develops on another but lesser-known shortcoming\ncalled the Hauck--Donner effect (HDE) whereby a Wald test statistic is not\nmonotonely increasing as a function of increasing distance between the\nparameter estimate and the null value. Resulting in an upward biased $p$-value\nand loss of power, the aberration can lead to very damaging consequences such\nas in variable selection. The HDE afflicts many types of regression models and\ncorresponds to estimates near the boundary of the parameter space. This article\npresents several new results, and its main contributions are to (i) propose a\nvery general test for detecting the HDE, regardless of its underlying cause;\n(ii) fundamentally characterize the HDE by pairwise ratios of Wald and Rao\nscore and likelihood ratio test statistics for 1-parameter distributions; (iii)\nshow that the parameter space may be partitioned into an interior encased by 5\nHDE severity measures (faint, weak, moderate, strong, extreme); (iv) prove that\na necessary condition for the HDE in a 2 by 2 table is a log odds ratio of at\nleast 2; (v) give some practical guidelines about HDE-free hypothesis testing.\nOverall, practical post-fit tests can now be conducted potentially to any model\nestimated by iteratively reweighted least squares, such as the generalized\nlinear model (GLM) and Vector GLM (VGLM) classes, the latter which encompasses\nmany popular regression models.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 10:15:50 GMT"}], "update_date": "2020-01-24", "authors_parsed": [["Yee", "Thomas William", ""]]}, {"id": "2001.08465", "submitter": "Shonosuke Sugasawa", "authors": "Yasuyuki Hamura, Kaoru Irie and Shonosuke Sugasawa", "title": "Shrinkage with Robustness: Log-Adjusted Priors for Sparse Signals", "comments": "40 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of distributions named log-adjusted shrinkage priors\nfor the analysis of sparse signals, which extends the three parameter beta\npriors by multiplying an additional log-term to their densities. The proposed\nprior has density tails that are heavier than even those of the Cauchy\ndistribution and realizes the tail-robustness of the Bayes estimator, while\nkeeping the strong shrinkage effect on noises. We verify this property via the\nimproved posterior mean squared errors in the tail. An integral representation\nwith latent variables for the new density is available and enables fast and\nsimple Gibbs samplers for the full posterior analysis. Our log-adjusted prior\nis significantly different from existing shrinkage priors with logarithms for\nallowing its further generalization by multiple log-terms in the density. The\nperformance of the proposed priors is investigated through simulation studies\nand data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jan 2020 12:16:09 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2020 13:03:40 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Hamura", "Yasuyuki", ""], ["Irie", "Kaoru", ""], ["Sugasawa", "Shonosuke", ""]]}, {"id": "2001.08971", "submitter": "Wen Wei Loh", "authors": "Wen Wei Loh, Stijn Vansteelandt", "title": "Confounder selection strategies targeting stable treatment effect\n  estimators", "comments": null, "journal-ref": null, "doi": "10.1002/sim.8792", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring the causal effect of a treatment on an outcome in an observational\nstudy requires adjusting for observed baseline confounders to avoid bias.\nHowever, adjusting for all observed baseline covariates, when only a subset are\nconfounders of the effect of interest, is known to yield potentially\ninefficient and unstable estimators of the treatment effect. Furthermore, it\nraises the risk of finite-sample bias and bias due to model misspecification.\nFor these stated reasons, confounder (or covariate) selection is commonly used\nto determine a subset of the available covariates that is sufficient for\nconfounding adjustment. In this article, we propose a confounder selection\nstrategy that focuses on stable estimation of the treatment effect. In\nparticular, when the propensity score model already includes covariates that\nare sufficient to adjust for confounding, then the addition of covariates that\nare associated with either treatment or outcome alone, but not both, should not\nsystematically change the effect estimator. The proposal, therefore, entails\nfirst prioritizing covariates for inclusion in the propensity score model, then\nusing a change-in-estimate approach to select the smallest adjustment set that\nyields a stable effect estimate. The ability of the proposal to correctly\nselect confounders, and to ensure valid inference of the treatment effect\nfollowing data-driven covariate selection, is assessed empirically and compared\nwith existing methods using simulation studies. We demonstrate the procedure\nusing three different publicly available datasets commonly used for causal\ninference.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 12:53:23 GMT"}, {"version": "v2", "created": "Thu, 25 Jun 2020 18:33:44 GMT"}, {"version": "v3", "created": "Sat, 10 Oct 2020 06:46:07 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["Loh", "Wen Wei", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "2001.09036", "submitter": "Rainer Schwabe", "authors": "Ulrike Gra{\\ss}hoff, Heiko Gro{\\ss}mann, Heinz Holling, Rainer Schwabe", "title": "Optimal Design for Probit Choice Models with Dependent Utilities", "comments": null, "journal-ref": "Statistics, Volume 55, 2021, Issue 1, Pages 173-194", "doi": "10.1080/02331888.2021.1888292", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we derive locally D-optimal designs for discrete choice\nexperiments based on multinomial probit models. These models include several\ndiscrete explanatory variables as well as a quantitative one. The commonly used\nmultinomial logit model assumes independent utilities for different choice\noptions. Thus, D-optimal optimal designs for such multinomial logit models may\ncomprise choice sets, e.g., consisting of alternatives which are identical in\nall discrete attributes but different in the quantitative variable. Obviously\nsuch designs are not appropriate for many empirical choice experiments. It will\nbe shown that locally D-optimal designs for multinomial probit models supposing\nindependent utilities consist of counterintuitive choice sets as well. However,\nlocally D-optimal designs for multinomial probit models allowing for dependent\nutilities turn out to be reasonable for analyzing decisions using discrete\nchoice studies.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 14:39:42 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Gra\u00dfhoff", "Ulrike", ""], ["Gro\u00dfmann", "Heiko", ""], ["Holling", "Heinz", ""], ["Schwabe", "Rainer", ""]]}, {"id": "2001.09180", "submitter": "Kabir Chandrasekher", "authors": "Kabir Aladin Chandrasekher, Ahmed El Alaoui, Andrea Montanari", "title": "Imputation for High-Dimensional Linear Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional regression with missing entries in the covariates.\nA common strategy in practice is to \\emph{impute} the missing entries with an\nappropriate substitute and then implement a standard statistical procedure\nacting as if the covariates were fully observed. Recent literature on this\nsubject proposes instead to design a specific, often complicated or non-convex,\nalgorithm tailored to the case of missing covariates. We investigate a simpler\napproach where we fill-in the missing entries with their conditional mean given\nthe observed covariates. We show that this imputation scheme coupled with\nstandard off-the-shelf procedures such as the LASSO and square-root LASSO\nretains the minimax estimation rate in the random-design setting where the\ncovariates are i.i.d.\\ sub-Gaussian. We further show that the square-root LASSO\nremains \\emph{pivotal} in this setting.\n  It is often the case that the conditional expectation cannot be computed\nexactly and must be approximated from data. We study two cases where the\ncovariates either follow an autoregressive (AR) process, or are jointly\nGaussian with sparse precision matrix. We propose tractable estimators for the\nconditional expectation and then perform linear regression via LASSO, and show\nsimilar estimation rates in both cases. We complement our theoretical results\nwith simulations on synthetic and semi-synthetic examples, illustrating not\nonly the sharpness of our bounds, but also the broader utility of this strategy\nbeyond our theoretical assumptions.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 19:54:09 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Chandrasekher", "Kabir Aladin", ""], ["Alaoui", "Ahmed El", ""], ["Montanari", "Andrea", ""]]}, {"id": "2001.09295", "submitter": "Mohammad Arshad Rahman", "authors": "Georges Bresson, Guy Lacroix, Mohammad Arshad Rahman", "title": "Bayesian Panel Quantile Regression for Binary Outcomes with Correlated\n  Random Effects: An Application on Crime Recidivism in Canada", "comments": "36 Pages, 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a Bayesian approach for estimating panel quantile\nregression with binary outcomes in the presence of correlated random effects.\nWe construct a working likelihood using an asymmetric Laplace (AL) error\ndistribution and combine it with suitable prior distributions to obtain the\ncomplete joint posterior distribution. For posterior inference, we propose two\nMarkov chain Monte Carlo (MCMC) algorithms but prefer the algorithm that\nexploits the blocking procedure to produce lower autocorrelation in the MCMC\ndraws. We also explain how to use the MCMC draws to calculate the marginal\neffects, relative risk and odds ratio. The performance of our preferred\nalgorithm is demonstrated in multiple simulation studies and shown to perform\nextremely well. Furthermore, we implement the proposed framework to study crime\nrecidivism in Quebec, a Canadian Province, using a novel data from the\nadministrative correctional files. Our results suggest that the recently\nimplemented \"tough-on-crime\" policy of the Canadian government has been largely\nsuccessful in reducing the probability of repeat offenses in the post-policy\nperiod. Besides, our results support existing findings on crime recidivism and\noffer new insights at various quantiles.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 11:16:30 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Bresson", "Georges", ""], ["Lacroix", "Guy", ""], ["Rahman", "Mohammad Arshad", ""]]}, {"id": "2001.09351", "submitter": "Qian Zhao", "authors": "Qian Zhao, Pragya Sur, Emmanuel J. Cand\\`es", "title": "The Asymptotic Distribution of the MLE in High-dimensional Logistic\n  Models: Arbitrary Covariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distribution of the maximum likelihood estimate (MLE) in\nhigh-dimensional logistic models, extending the recent results from Sur (2019)\nto the case where the Gaussian covariates may have an arbitrary covariance\nstructure. We prove that in the limit of large problems holding the ratio\nbetween the number $p$ of covariates and the sample size $n$ constant, every\nfinite list of MLE coordinates follows a multivariate normal distribution.\nConcretely, the $j$th coordinate $\\hat {\\beta}_j$ of the MLE is asymptotically\nnormally distributed with mean $\\alpha_\\star \\beta_j$ and standard deviation\n$\\sigma_\\star/\\tau_j$; here, $\\beta_j$ is the value of the true regression\ncoefficient, and $\\tau_j$ the standard deviation of the $j$th predictor\nconditional on all the others. The numerical parameters $\\alpha_\\star > 1$ and\n$\\sigma_\\star$ only depend upon the problem dimensionality $p/n$ and the\noverall signal strength, and can be accurately estimated. Our results imply\nthat the MLE's magnitude is biased upwards and that the MLE's standard\ndeviation is greater than that predicted by classical theory. We present a\nseries of experiments on simulated and real data showing excellent agreement\nwith the theory.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jan 2020 19:07:14 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Zhao", "Qian", ""], ["Sur", "Pragya", ""], ["Cand\u00e8s", "Emmanuel J.", ""]]}, {"id": "2001.09404", "submitter": "Nick James", "authors": "Nick James, Max Menzies, Jennifer Chan", "title": "Semi-metric portfolio optimization: a new algorithm reducing\n  simultaneous asset shocks", "comments": "Substantial revisions since v1. Equal contribution from first two\n  authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM q-fin.MF stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method for financial portfolio optimization based\non reducing simultaneous asset shocks across a collection of assets. We apply\nrecently introduced semi-metrics between finite sets to determine the distance\nbetween time series' structural breaks. Then, we build on the classical\nportfolio optimization theory of Markowitz and use this distance between asset\nstructural breaks for our penalty function, rather than portfolio variance. Our\nexperiments are promising: on synthetic data, we show that our proposed method\ndoes indeed diversify among time series with highly similar structural breaks,\nand enjoys advantages over existing metrics between sets. On real data,\nexperiments illustrate that our proposed optimization method produces higher\nrisk-adjusted returns than mean-variance portfolio optimization. Moreover, the\npredictive distribution is superior in every measure analyzed, producing a\nhigher mean, lower standard deviation and lower kurtosis. The main implication\nfor this method in portfolio management is reducing simultaneous asset shocks\nand potentially sharp associated drawdowns during periods of highly similar\nstructural breaks, such as a market crisis.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 05:28:27 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 05:42:46 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["James", "Nick", ""], ["Menzies", "Max", ""], ["Chan", "Jennifer", ""]]}, {"id": "2001.09510", "submitter": "Johan Segers", "authors": "Stefka Asenova and Gildas Mazo and Johan Segers", "title": "Inference on extremal dependence in the domain of attraction of a\n  structured H\\\"usler-Reiss distribution motivated by a Markov tree with latent\n  variables", "comments": "31 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Markov tree is a probabilistic graphical model for a random vector indexed\nby the nodes of an undirected tree encoding conditional independence relations\nbetween variables. One possible limit distribution of partial maxima of samples\nfrom such a Markov tree is a max-stable H\\\"usler-Reiss distribution whose\nparameter matrix inherits its structure from the tree, each edge contributing\none free dependence parameter. Our central assumption is that, upon marginal\nstandardization, the data-generating distribution is in the max-domain of\nattraction of the said H\\\"usler-Reiss distribution, an assumption much weaker\nthan the one that data are generated according to a graphical model. Even if\nsome of the variables are unobservable (latent), we show that the underlying\nmodel parameters are still identifiable if and only if every node corresponding\nto a latent variable has degree at least three. Three estimation procedures,\nbased on the method of moments, maximum composite likelihood, and pairwise\nextremal coefficients, are proposed for usage on multivariate peaks over\nthresholds data when some variables are latent. A typical application is a\nriver network in the form of a tree where, on some locations, no data are\navailable. We illustrate the model and the identifiability criterion on a data\nset of high water levels on the Seine, France, with two latent variables. The\nstructured H\\\"usler-Reiss distribution is found to fit the observed extremal\ndependence patterns well. The parameters being identifiable we are able to\nquantify tail dependence between locations for which there are no data.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jan 2020 20:18:07 GMT"}, {"version": "v2", "created": "Sun, 17 Jan 2021 18:16:23 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Asenova", "Stefka", ""], ["Mazo", "Gildas", ""], ["Segers", "Johan", ""]]}, {"id": "2001.09560", "submitter": "Takahide Yanagi", "authors": "Tadao Hoshino, Takahide Yanagi", "title": "Estimating Marginal Treatment Effects under Unobserved Group\n  Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies endogenous treatment effect models in which individuals\nare classified into unobserved groups based on heterogeneous treatment choice\nrules. Such heterogeneity may arise, for example, when multiple treatment\neligibility criteria and different preference patterns exist. Using a finite\nmixture approach, we propose a marginal treatment effect (MTE) framework in\nwhich the treatment choice and outcome equations can be heterogeneous across\ngroups. Under the availability of valid instrumental variables specific to each\ngroup, we show that the MTE for each group can be separately identified using\nthe local instrumental variable method. Based on our identification result, we\npropose a two-step semiparametric procedure for estimating the group-wise MTE\nparameters. We first estimate the finite-mixture treatment choice model by a\nmaximum likelihood method and then estimate the MTEs using a series\napproximation method. We prove that the proposed MTE estimator is consistent\nand asymptotically normally distributed. We illustrate the usefulness of the\nproposed method with an application to economic returns to college education.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 02:03:23 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2020 05:43:40 GMT"}, {"version": "v3", "created": "Tue, 26 Jan 2021 12:00:08 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Hoshino", "Tadao", ""], ["Yanagi", "Takahide", ""]]}, {"id": "2001.09593", "submitter": "Daniel Fryer Mr", "authors": "Daniel Fryer and Inga Strumke and Hien Nguyen", "title": "Shapley value confidence intervals for attributing variance explained", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coefficient of determination, the $R^2$, is often used to measure the\nvariance explained by an affine combination of multiple explanatory covariates.\nAn attribution of this explanatory contribution to each of the individual\ncovariates is often sought in order to draw inference regarding the importance\nof each covariate with respect to the response phenomenon. A recent method for\nascertaining such an attribution is via the game theoretic Shapley value\ndecomposition of the coefficient of determination. Such a decomposition has the\ndesirable efficiency, monotonicity, and equal treatment properties. Under a\nweak assumption that the joint distribution is pseudo-elliptical, we obtain the\nasymptotic normality of the Shapley values. We then utilize this result in\norder to construct confidence intervals and hypothesis tests regarding such\nquantities. Monte Carlo studies regarding our results are provided. We found\nthat our asymptotic confidence intervals are computationally superior to\ncompeting bootstrap methods and are able to improve upon the performance of\nsuch intervals. In an expository application to Australian real estate price\nmodelling, we employ Shapley value confidence intervals to identify significant\ndifferences between the explanatory contributions of covariates, between\nmodels, which otherwise share approximately the same $R^2$ value. These\ndifferent models are based on real estate data from the same periods in 2019\nand 2020, the latter covering the early stages of the arrival of the novel\ncoronavirus, COVID-19.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 05:53:12 GMT"}, {"version": "v2", "created": "Sat, 8 Aug 2020 05:45:13 GMT"}], "update_date": "2020-08-11", "authors_parsed": [["Fryer", "Daniel", ""], ["Strumke", "Inga", ""], ["Nguyen", "Hien", ""]]}, {"id": "2001.09602", "submitter": "Yasuyuki Hamura", "authors": "Yasuyuki Hamura and Tatsuya Kubokawa", "title": "Bayesian Shrinkage Estimation of Negative Multinomial Parameter Vectors", "comments": "31 pages; the code for numerical computation of the hierarchical\n  Bayes estimator in Section 4 has been corrected; Tables 2, 3, and 4 and the\n  second-to-the-last paragraph of Section 4 have been changed", "journal-ref": null, "doi": "10.1016/j.jmva.2020.104653", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The negative multinomial distribution is a multivariate generalization of the\nnegative binomial distribution. In this paper, we consider the problem of\nestimating an unknown matrix of probabilities on the basis of observations of\nnegative multinomial variables under the standardized squared error loss.\nFirst, a general sufficient condition for a shrinkage estimator to dominate the\nUMVU estimator is derived and an empirical Bayes estimator satisfying the\ncondition is constructed. Next, a hierarchical shrinkage prior is introduced,\nan associated Bayes estimator is shown to dominate the UMVU estimator under\nsome conditions, and some remarks about posterior computation are presented.\nFinally, shrinkage estimators and the UMVU estimator are compared by\nsimulation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 06:42:17 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2020 14:47:17 GMT"}], "update_date": "2020-10-30", "authors_parsed": [["Hamura", "Yasuyuki", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "2001.09834", "submitter": "Kristoffer H. Hellton", "authors": "Kristoffer H. Hellton", "title": "Penalized angular regression for personalized predictions", "comments": "19 pages, 3 figures, 3 tables. Some language updates", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization is becoming an important feature in many predictive\napplications. We introduce a penalized regression method implementing\npersonalization inherently in the penalty. Personalized angle (PAN) regression\nconstructs regression coefficients that are specific to the covariate vector\nfor which one is producing a prediction, thus personalizing the regression\nmodel itself. This is achieved by penalizing the angles in a hyperspherical\nparametrization of the regression coefficients. For an orthogonal design\nmatrix, it is shown that the PAN estimate is the solution to a low-dimensional\neigenvector equation. Using a parametric bootstrap procedure to select the\ntuning parameter, simulations show that PAN regression can outperform ordinary\nleast squares and ridge regression in terms of prediction error. We further\nprove that by combining the PAN penalty with an $L_{2}$ penalty the resulting\nmethod will have uniformly smaller mean squared prediction error than ridge\nregression, asymptotically. Finally, we demonstrate the method in a medical\napplication.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 14:57:06 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2020 17:08:44 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Hellton", "Kristoffer H.", ""]]}, {"id": "2001.09868", "submitter": "Matteo Ruggiero", "authors": "Filippo Ascolani, Antonio Lijoi and Matteo Ruggiero", "title": "Predictive inference with Fleming--Viot-driven dependent Dirichlet\n  processes", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider predictive inference using a class of temporally dependent\nDirichlet processes driven by Fleming--Viot diffusions, which have a natural\nbearing in Bayesian nonparametrics and lend the resulting family of random\nprobability measures to analytical posterior analysis. Formulating the implied\nstatistical model as a hidden Markov model, we fully describe the predictive\ndistribution induced by these Fleming--Viot-driven dependent Dirichlet\nprocesses, for a sequence of observations collected at a certain time given\nanother set of draws collected at several previous times. This is identified as\na mixture of P\\'olya urns, whereby the observations can be values from the\nbaseline distribution or copies of previous draws collected at the same time as\nin the usual P\\`olya urn, or can be sampled from a random subset of the data\ncollected at previous times. We characterise the time-dependent weights of the\nmixture which select such subsets and discuss the asymptotic regimes. We\ndescribe the induced partition by means of a Chinese restaurant process\nmetaphor with a conveyor belt, whereby new customers who do not sit at an\noccupied table open a new table by picking a dish either from the baseline\ndistribution or from a time-varying offer available on the conveyor belt. We\nlay out explicit algorithms for exact and approximate posterior sampling of\nboth observations and partitions, and illustrate our results on predictive\nproblems with synthetic and real data.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 15:45:46 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Ascolani", "Filippo", ""], ["Lijoi", "Antonio", ""], ["Ruggiero", "Matteo", ""]]}, {"id": "2001.09887", "submitter": "Yifan Cui", "authors": "Yifan Cui, Michael R. Kosorok, Erik Sverdrup, Stefan Wager, Ruoqing\n  Zhu", "title": "Estimating heterogeneous treatment effects with right-censored data via\n  causal survival forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forest-based methods have recently gained in popularity for non-parametric\ntreatment effect estimation. Building on this line of work, we introduce causal\nsurvival forests, which can be used to estimate heterogeneous treatment effects\nin a survival and observational setting where outcomes may be right-censored.\nOur approach relies on orthogonal estimating equations to robustly adjust for\nboth censoring and selection effects. In our experiments, we find our approach\nto perform well relative to a number of baselines.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 16:22:05 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 16:35:44 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Cui", "Yifan", ""], ["Kosorok", "Michael R.", ""], ["Sverdrup", "Erik", ""], ["Wager", "Stefan", ""], ["Zhu", "Ruoqing", ""]]}, {"id": "2001.09996", "submitter": "John Addy", "authors": "J W G Addy and J Langhorne", "title": "A Proposed Method for Assessing Cluster Heterogeneity", "comments": "17 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assessing how adequate clusters fit a dataset and finding an optimum number\nof clusters is a difficult process. A membership matrix and the degree of\nmembership matrix is suggested to determine the homogeneity of a cluster fit.\nMaximisation of the ratio of the overall degree of membership at cluster number\nlag 1 is also suggested as a method to optimise the number of clusters in a\ndataset. A threshold factor upon the degree of membership is also suggested for\nhomogeneous clusters. Cluster simulations were given to compare how well the\nproposed method compares against established methods. This method may be\napplied to the output of both hierarchical and k-means clustering.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 17:27:01 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Addy", "J W G", ""], ["Langhorne", "J", ""]]}, {"id": "2001.10142", "submitter": "Mengyan Li", "authors": "Mengyan Li, Runze Li and Yanyuan Ma", "title": "Inference in High-Dimensional Linear Measurement Error Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a high-dimensional linear model with a finite number of covariates\nmeasured with error, we study statistical inference on the parameters\nassociated with the error-prone covariates, and propose a new corrected\ndecorrelated score test and the corresponding one-step estimator. We further\nestablish asymptotic properties of the newly proposed test statistic and the\none-step estimator. Under local alternatives, we show that the limiting\ndistribution of our corrected decorrelated score test statistic is non-central\nnormal. The finite-sample performance of the proposed inference procedure is\nexamined through simulation studies. We further illustrate the proposed\nprocedure via an empirical analysis of a real data example.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 02:06:14 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Li", "Mengyan", ""], ["Li", "Runze", ""], ["Ma", "Yanyuan", ""]]}, {"id": "2001.10168", "submitter": "HaiYing Wang", "authors": "HaiYing Wang and Yanyuan Ma", "title": "Optimal subsampling for quantile regression in big data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate optimal subsampling for quantile regression. We derive the\nasymptotic distribution of a general subsampling estimator and then derive two\nversions of optimal subsampling probabilities. One version minimizes the trace\nof the asymptotic variance-covariance matrix for a linearly transformed\nparameter estimator and the other minimizes that of the original parameter\nestimator. The former does not depend on the densities of the responses given\ncovariates and is easy to implement. Algorithms based on optimal subsampling\nprobabilities are proposed and asymptotic distributions and asymptotic\noptimality of the resulting estimators are established. Furthermore, we propose\nan iterative subsampling procedure based on the optimal subsampling\nprobabilities in the linearly transformed parameter estimation which has great\nscalability to utilize available computational resources. In addition, this\nprocedure yields standard errors for parameter estimators without estimating\nthe densities of the responses given the covariates. We provide numerical\nexamples based on both simulated and real data to illustrate the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 05:03:13 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Wang", "HaiYing", ""], ["Ma", "Yanyuan", ""]]}, {"id": "2001.10200", "submitter": "Martin Seilmayer", "authors": "Martin Seilmayer, Ferran Garcia Gonzalez and Thomas Wondrak", "title": "The Multivariate Extension of the Lomb-Scargle Method", "comments": "to be published", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The common methods of spectral analysis for multivariate (n-dimensional) time\nseries, like discrete Frourier transform (FT) or Wavelet transform, are based\non Fourier series to decompose discrete data into a set of trigonometric model\ncomponents, e. g. amplitude and phase. Applied to discrete data with a finite\nrange several limitations of (time discrete) FT can be observed which are\ncaused by the orthogonality mismatch of the trigonometric basis functions on a\nfinite interval. However, in the general situation of non-equidistant or\nfragmented sampling FT based methods will cause significant errors in the\nparameter estimation. Therefore, the classical Lomb-Scargle method (LSM), which\nis not based on Fourier series, was developed as a statistical tool for one\ndimensional data to circumvent the inconsistent and erroneous parameter\nestimation of FT. The present work deduces LSM for n-dimensional data sets by a\nredefinition of the shifting parameter \\tau, to maintain orthogonality of the\ntrigonometric basis. An analytical derivation shows, that n-D LSM extents the\ntraditional 1D case preserving all the statistical benefits, such as the\nimproved noise rejection. Here, we derive the parameter confidence intervals\nfor LSM and compare it with FT. Applications with ideal test data and\nexperimental data will illustrate and support the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 08:01:54 GMT"}, {"version": "v2", "created": "Fri, 14 Feb 2020 13:29:34 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Seilmayer", "Martin", ""], ["Gonzalez", "Ferran Garcia", ""], ["Wondrak", "Thomas", ""]]}, {"id": "2001.10317", "submitter": "Andrea Meil\\'an-Vila", "authors": "Andrea Meil\\'an-Vila and Mario Francisco-Fern\\'andez and Rosa M.\n  Crujeiras and Agnese Panzera", "title": "Nonparametric multivariate regression estimation for circular responses", "comments": "30 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric estimators of a regression function with circular response and\nRd-valued predictor are considered in this work. Local polynomial type\nestimators are proposed and studied. Expressions for their asymptotic biases\nand variances are derived, and some guidelines to select asymptotically local\noptimal bandwidth matrices are also given. The finite sample behavior of the\nproposed estimators is assessed through simulations and their performance is\nalso illustrated with a real data set.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 13:40:03 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2020 09:06:23 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Meil\u00e1n-Vila", "Andrea", ""], ["Francisco-Fern\u00e1ndez", "Mario", ""], ["Crujeiras", "Rosa M.", ""], ["Panzera", "Agnese", ""]]}, {"id": "2001.10352", "submitter": "Tyler VanderWeele", "authors": "Tyler J. VanderWeele and Charles J. K. Batty", "title": "On the dimensional indeterminacy of one-wave factor analysis under\n  causal effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is shown, with two sets of survey items that separately load on two\ndistinct factors, independent of one another conditional on the past, that if\nit is the case that at least one of the factors causally affects the other,\nthen, in many settings, the process will converge to a factor model in which a\nsingle factor will suffice to capture the structure of the associations among\nthe items. Factor analysis with one wave of data can then not distinguish\nbetween factor models with a single factor versus those with two factors that\nare causally related. Therefore, unless causal relations between factors can be\nruled out a priori, alleged empirical evidence from one-wave factor analysis\nfor a single factor still leaves open the possibilities of a single factor or\nof two factors that causally affect one another. The implications for\ninterpreting the factor structure of psychological scales, such as self-report\nscales for anxiety and depression, are discussed. Some further generalizations\nto an arbitrary number of underlying factors are noted.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 14:27:02 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2020 17:58:01 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2020 22:08:10 GMT"}], "update_date": "2020-09-09", "authors_parsed": [["VanderWeele", "Tyler J.", ""], ["Batty", "Charles J. K.", ""]]}, {"id": "2001.10353", "submitter": "Eric Wolsztynski", "authors": "Eric Wolsztynski", "title": "Statistical Exploration of Relationships Between Routine and Agnostic\n  Features Towards Interpretable Risk Characterization", "comments": "This work was presented at the 2019 IEEE Medical Imaging Conference,\n  2 November, Manchester, UK. 8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As is typical in other fields of application of high throughput systems,\nradiology is faced with the challenge of interpreting increasingly\nsophisticated predictive models such as those derived from radiomics analyses.\nInterpretation may be guided by the learning output from machine learning\nmodels, which may however vary greatly with each technique. Whatever this\noutput model, it will raise some essential questions. How do we interpret the\nprognostic model for clinical implementation? How can we identify potential\ninformation structures within sets of radiomic features, in order to create\nclinically interpretable models? And how can we recombine or exploit potential\nrelationships between features towards improved interpretability? A number of\nstatistical techniques are explored to assess (possibly nonlinear)\nrelationships between radiological features from different angles.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 14:27:09 GMT"}], "update_date": "2020-01-29", "authors_parsed": [["Wolsztynski", "Eric", ""]]}, {"id": "2001.10377", "submitter": "Chaonan Jiang", "authors": "Chaonan Jiang, Davide La Vecchia, Elvezio Ronchetti, Olivier Scaillet", "title": "Saddlepoint approximations for spatial panel data models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop new higher-order asymptotic techniques for the Gaussian maximum\nlikelihood estimator in a spatial panel data model, with fixed effects,\ntime-varying covariates, and spatially correlated errors. Our saddlepoint\ndensity and tail area approximation feature relative error of order\n$O(1/(n(T-1)))$ with $n$ being the cross-sectional dimension and $T$ the\ntime-series dimension. The main theoretical tool is the tilted-Edgeworth\ntechnique in a non-identically distributed setting. The density approximation\nis always non-negative, does not need resampling, and is accurate in the tails.\nMonte Carlo experiments on density approximation and testing in the presence of\nnuisance parameters illustrate the good performance of our approximation over\nfirst-order asymptotics and Edgeworth expansions. An empirical application to\nthe investment-saving relationship in OECD (Organisation for Economic\nCo-operation and Development) countries shows disagreement between testing\nresults based on first-order asymptotics and saddlepoint techniques.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jan 2020 19:40:01 GMT"}, {"version": "v2", "created": "Fri, 31 Jan 2020 14:28:20 GMT"}, {"version": "v3", "created": "Mon, 12 Jul 2021 18:35:54 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Jiang", "Chaonan", ""], ["La Vecchia", "Davide", ""], ["Ronchetti", "Elvezio", ""], ["Scaillet", "Olivier", ""]]}, {"id": "2001.10488", "submitter": "Nassim Nicholas Taleb", "authors": "Nassim Nicholas Taleb", "title": "Statistical Consequences of Fat Tails: Real World Preasymptotics,\n  Epistemology, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT q-fin.RM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monograph investigates the misapplication of conventional statistical\ntechniques to fat tailed distributions and looks for remedies, when possible.\n  Switching from thin tailed to fat tailed distributions requires more than\n\"changing the color of the dress\". Traditional asymptotics deal mainly with\neither n=1 or $n=\\infty$, and the real world is in between, under of the \"laws\nof the medium numbers\" --which vary widely across specific distributions. Both\nthe law of large numbers and the generalized central limit mechanisms operate\nin highly idiosyncratic ways outside the standard Gaussian or Levy-Stable\nbasins of convergence.\n  A few examples:\n  + The sample mean is rarely in line with the population mean, with effect on\n\"naive empiricism\", but can be sometimes be estimated via parametric methods.\n  + The \"empirical distribution\" is rarely empirical.\n  + Parameter uncertainty has compounding effects on statistical metrics.\n  + Dimension reduction (principal components) fails.\n  + Inequality estimators (GINI or quantile contributions) are not additive and\nproduce wrong results.\n  + Many \"biases\" found in psychology become entirely rational under more\nsophisticated probability distributions\n  + Most of the failures of financial economics, econometrics, and behavioral\neconomics can be attributed to using the wrong distributions.\n  This book, the first volume of the Technical Incerto, weaves a narrative\naround published journal articles.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jan 2020 14:45:55 GMT"}, {"version": "v2", "created": "Tue, 22 Sep 2020 18:12:31 GMT"}], "update_date": "2020-09-24", "authors_parsed": [["Taleb", "Nassim Nicholas", ""]]}, {"id": "2001.10577", "submitter": "Julio Stern", "authors": "Julio Michael Stern and Carlos Alberto de Braganca Pereira", "title": "The e-value: A Fully Bayesian Significance Measure for Precise\n  Statistical Hypotheses and its Research Program", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article gives a survey of the e-value, a statistical significance\nmeasure a.k.a. the evidence rendered by observational data, X, in support of a\nstatistical hypothesis, H, or, the other way around, the epistemic value of H\ngiven X. The $e$-value and the accompanying FBST, the Full Bayesian\nSignificance Test, constitute the core of a research program that was started\nat IME-USP, is being developed by over 20 researchers worldwide, and has, so\nfar, been referenced by over 200 publications.\n  The e-value and the FBST comply with the best principles of Bayesian\ninference, including the likelihood principle, complete invariance, asymptotic\nconsistency, etc. Furthermore, they exhibit powerful logic or algebraic\nproperties in situations where one needs to compare or compose distinct\nhypotheses that can be formulated either in the same or in different\nstatistical models. Moreover, they effortlessly accommodate the case of sharp\nor precise hypotheses, a situation where alternative methods often require ad\nhoc and convoluted procedures. Finally, the FBST has outstanding robustness and\nreliability characteristics, outperforming traditional tests of hypotheses in\nmany practical applications of statistical modeling and operations research.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 20:27:07 GMT"}, {"version": "v2", "created": "Thu, 30 Jan 2020 20:59:11 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2020 11:41:13 GMT"}], "update_date": "2020-04-29", "authors_parsed": [["Stern", "Julio Michael", ""], ["Pereira", "Carlos Alberto de Braganca", ""]]}, {"id": "2001.10616", "submitter": "Yuling Jiao", "authors": "Jian Huang, Yuling Jiao, Lican Kang, Jin Liu, Yanyan Liu, Xiliang Lu,\n  and Yuanyuan Yang", "title": "On Newton Screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Screening and working set techniques are important approaches to reducing the\nsize of an optimization problem. They have been widely used in accelerating\nfirst-order methods for solving large-scale sparse learning problems. In this\npaper, we develop a new screening method called Newton screening (NS) which is\na generalized Newton method with a built-in screening mechanism. We derive an\nequivalent KKT system for the Lasso and utilize a generalized Newton method to\nsolve the KKT equations. Based on this KKT system, a built-in working set with\na relatively small size is first determined using the sum of primal and dual\nvariables generated from the previous iteration, then the primal variable is\nupdated by solving a least-squares problem on the working set and the dual\nvariable updated based on a closed-form expression. Moreover, we consider a\nsequential version of Newton screening (SNS) with a warm-start strategy. We\nshow that NS possesses an optimal convergence property in the sense that it\nachieves one-step local convergence. Under certain regularity conditions on the\nfeature matrix, we show that SNS hits a solution with the same signs as the\nunderlying true target and achieves a sharp estimation error bound with high\nprobability. Simulation studies and real data analysis support our theoretical\nresults and demonstrate that SNS is faster and more accurate than several\nstate-of-the-art methods in our comparative studies.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jan 2020 11:25:33 GMT"}, {"version": "v2", "created": "Fri, 7 Feb 2020 16:09:52 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Kang", "Lican", ""], ["Liu", "Jin", ""], ["Liu", "Yanyan", ""], ["Lu", "Xiliang", ""], ["Yang", "Yuanyuan", ""]]}, {"id": "2001.10664", "submitter": "Yang Chen", "authors": "David E Jones, Robert N Trangucci, Yang Chen", "title": "Quantifying Observed Prior Impact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We distinguish two questions (i) how much information does the prior contain?\nand (ii) what is the effect of the prior? Several measures have been proposed\nfor quantifying effective prior sample size, for example Clarke [1996] and\nMorita et al. [2008]. However, these measures typically ignore the likelihood\nfor the inference currently at hand, and therefore address (i) rather than\n(ii). Since in practice (ii) is of great concern, Reimherr et al. [2014]\nintroduced a new class of effective prior sample size measures based on\nprior-likelihood discordance. We take this idea further towards its natural\nBayesian conclusion by proposing measures of effective prior sample size that\nnot only incorporate the general mathematical form of the likelihood but also\nthe specific data at hand. Thus, our measures do not average across datasets\nfrom the working model, but condition on the current observed data.\nConsequently, our measures can be highly variable, but we demonstrate that this\nis because the impact of a prior can be highly variable. Our measures are Bayes\nestimates of meaningful quantities and well communicate the extent to which\ninference is determined by the prior, or framed differently, the amount of\neffort saved due to having prior information. We illustrate our ideas through a\nnumber of examples including a Gaussian conjugate model (continuous\nobservations), a Beta-Binomial model (discrete observations), and a linear\nregression model (two unknown parameters). Future work on further developments\nof the methodology and an application to astronomy are discussed at the end.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 02:15:09 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Jones", "David E", ""], ["Trangucci", "Robert N", ""], ["Chen", "Yang", ""]]}, {"id": "2001.10679", "submitter": "Duzhe Wang", "authors": "Duzhe Wang, Po-Ling Loh", "title": "Adaptive Estimation and Statistical Inference for High-Dimensional\n  Graph-Based Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider adaptive estimation and statistical inference for\nhigh-dimensional graph-based linear models. In our model, the coordinates of\nregression coefficients correspond to an underlying undirected graph.\nFurthermore, the given graph governs the piecewise polynomial structure of the\nregression vector. In the adaptive estimation part, we apply graph-based\nregularization techniques and propose a family of locally adaptive estimators\ncalled the Graph-Piecewise-Polynomial-Lasso. We further study a one-step update\nof the Graph-Piecewise-Polynomial-Lasso for the problem of statistical\ninference. We develop the corresponding theory, which includes the fixed design\nand the sub-Gaussian random design. Finally, we illustrate the superior\nperformance of our approaches by extensive simulation studies and conclude with\nan application to an Arabidopsis thaliana microarray dataset.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 03:25:10 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Wang", "Duzhe", ""], ["Loh", "Po-Ling", ""]]}, {"id": "2001.10782", "submitter": "Hang Liu", "authors": "Hang Liu and Kanchan Mukherjee", "title": "M-estimation in GARCH models without higher order moments", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of M-estimators of the parameters of the GARCH models\nwhich are asymptotically normal under mild assumptions on the moments of the\nunderlying error distribution. Since heavy-tailed error distributions without\nhigher order moments are common in the GARCH modeling of many real financial\ndata, it becomes worthwhile to use such estimators for the time series\ninference instead of the quasi maximum likelihood estimator. We discuss the\nweighted bootstrap approximations of the distributions of M-estimators. Through\nextensive simulations and data analysis, we demonstrate the robustness of the\nM-estimators under heavy-tailed error distributions and the accuracy of the\nbootstrap approximation. In addition to the GARCH~(1, 1) model, we obtain\nextensive computation and simulation results which are useful in the context of\nhigher order models such as GARCH~(2, 1) and GARCH~(1, 2) but have not yet\nreceived sufficient attention in the literature. Finally, we use M-estimators\nfor the analysis of three real financial time series fitted with GARCH~(1, 1)\nor GARCH~(2, 1) models.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 12:42:05 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Liu", "Hang", ""], ["Mukherjee", "Kanchan", ""]]}, {"id": "2001.10892", "submitter": "Rahma Abid", "authors": "Rahma Abid, C\\'elestin C. Kokonendji", "title": "Discriminating between and within (semi)continuous classes of both\n  Tweedie and geometric Tweedie models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In both Tweedie and geometric Tweedie models, the common power parameter\n$p\\notin(0,1)$ works as an automatic distribution selection. It mainly\nseparates two subclasses of semicontinuous ($1<p<2$) and positive continuous\n($p\\geq 2$) distributions. Our paper centers around exploring diagnostic tools\nbased on the maximum likelihood ratio test and minimum Kolmogorov-Smirnov\ndistance methods in order to discriminate very close distributions within each\nsubclass of these two models according to values of $p$. Grounded on the unique\nequality of variation indices, we also discriminate the gamma and geometric\ngamma distributions with $p=2$ in Tweedie and geometric Tweedie families,\nrespectively. Probabilities of correct selection for several combinations of\ndispersion parameters, means and sample sizes are examined by simulations. We\nthus perform a numerical comparison study to assess the discrimination\nprocedures in these subclasses of two families. Finally, semicontinuous\n($1<p\\leq 2$) distributions in the broad sense are significantly more\ndistinguishable than the over-varied continuous ($p>2$) ones; and two datasets\nfor illustration purposes are investigated.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jan 2020 18:26:52 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Abid", "Rahma", ""], ["Kokonendji", "C\u00e9lestin C.", ""]]}, {"id": "2001.10955", "submitter": "Yong He", "authors": "Long Yu and Yong He and Xinsheng Zhang and Ji Zhu", "title": "Network-Assisted Estimation for Large-dimensional Factor Model with\n  Guaranteed Convergence Rate Improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network structure is growing popular for capturing the intrinsic relationship\nbetween large-scale variables. In the paper we propose to improve the\nestimation accuracy for large-dimensional factor model when a network structure\nbetween individuals is observed. To fully excavate the prior network\ninformation, we construct two different penalties to regularize the factor\nloadings and shrink the idiosyncratic errors. Closed-form solutions are\nprovided for the penalized optimization problems. Theoretical results\ndemonstrate that the modified estimators achieve faster convergence rates and\nlower asymptotic mean squared errors when the underlying network structure\namong individuals is correct. An interesting finding is that even if the priori\nnetwork is totally misleading, the proposed estimators perform no worse than\nconventional state-of-art methods. Furthermore, to facilitate the practical\napplication, we propose a data-driven approach to select the tuning parameters,\nwhich is computationally efficient. We also provide an empirical criterion to\ndetermine the number of common factors. Simulation studies and application to\nthe S&P100 weekly return dataset convincingly illustrate the superiority and\nadaptivity of the new approach.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 17:02:19 GMT"}], "update_date": "2020-01-30", "authors_parsed": [["Yu", "Long", ""], ["He", "Yong", ""], ["Zhang", "Xinsheng", ""], ["Zhu", "Ji", ""]]}, {"id": "2001.11130", "submitter": "Max Cytrynbaum", "authors": "Max Cytrynbaum", "title": "Blocked Clusterwise Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent literature in econometrics models unobserved cross-sectional\nheterogeneity in panel data by assigning each cross-sectional unit a\none-dimensional, discrete latent type. Such models have been shown to allow\nestimation and inference by regression clustering methods. This paper is\nmotivated by the finding that the clustered heterogeneity models studied in\nthis literature can be badly misspecified, even when the panel has significant\ndiscrete cross-sectional structure. To address this issue, we generalize\nprevious approaches to discrete unobserved heterogeneity by allowing each unit\nto have multiple, imperfectly-correlated latent variables that describe its\nresponse-type to different covariates. We give inference results for a k-means\nstyle estimator of our model and develop information criteria to jointly select\nthe number clusters for each latent variable. Monte Carlo simulations confirm\nour theoretical results and give intuition about the finite-sample performance\nof estimation and model selection. We also contribute to the theory of\nclustering with an over-specified number of clusters and derive new convergence\nrates for this setting. Our results suggest that over-fitting can be severe in\nk-means style estimators when the number of clusters is over-specified.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jan 2020 23:29:31 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Cytrynbaum", "Max", ""]]}, {"id": "2001.11240", "submitter": "Moritz Berger Dr.", "authors": "Moritz Berger and Matthias Schmid", "title": "Assessing the Calibration of Subdistribution Hazard Models in Discrete\n  Time", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalization performance of a risk prediction model can be evaluated by\nits calibration, which measures the agreement between predicted and observed\noutcomes on external validation data. Here, methods for assessing the\ncalibration of discrete time-to-event models in the presence of competing risks\nare proposed. The methods are designed for the class of discrete\nsubdistribution hazard models, which directly relate the cumulative incidence\nfunction of one event of interest to a set of covariates. Simulation studies\nshow that the methods are strong tools for calibration assessment even in\nscenarios with a high censoring rate and/or a large number of discrete time\npoints. The proposed approaches are illustrated by an analysis of nosocomial\npneumonia.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 10:13:31 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Berger", "Moritz", ""], ["Schmid", "Matthias", ""]]}, {"id": "2001.11365", "submitter": "Cameron Williams", "authors": "Cameron J. Williams, Kevin J. Wilson and Nina Wilson", "title": "A Comparison of Prior Elicitation Aggregation using the Classical Method\n  and SHELF", "comments": null, "journal-ref": null, "doi": "10.1111/rssa.12691", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective Bayesian prior distributions elicited from experts can be\naggregated together to form group priors. This paper compares aggregated priors\nformed by Equal Weight Aggregation, the Classical Method, and the Sheffield\nElicitation Framework to each other and individual expert priors, using an\nexpert elicitation carried out for a clinical trial. Aggregation methods and\nindividual expert prior distributions are compared using proper scoring rules\nto compare the informativeness and calibration of the distributions. The three\naggregation methods outperform the individual experts, and the Sheffield\nElicitation Framework performs best amongst them.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 14:37:22 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2020 15:40:01 GMT"}, {"version": "v3", "created": "Mon, 28 Jun 2021 12:47:28 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Williams", "Cameron J.", ""], ["Wilson", "Kevin J.", ""], ["Wilson", "Nina", ""]]}, {"id": "2001.11425", "submitter": "Fei Ding", "authors": "Fei Ding, Shiyuan He, David E. Jones, Jianhua Z. Huang", "title": "Supervised Functional PCA with Covariate Dependent Mean and Covariance\n  Structure", "comments": "24 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incorporating covariate information into functional data analysis methods can\nsubstantially improve modeling and prediction performance. However, many\nfunctional data analysis methods do not make use of covariate or supervision\ninformation, and those that do often have high computational cost or assume\nthat only the scores are related to covariates, an assumption that is usually\nviolated in practice. In this article, we propose a functional data analysis\nframework that relates both the mean and covariance function to covariate\ninformation. To facilitate modeling and ensure the covariance function is\npositive semi-definite, we represent it using splines and design a map from\nEuclidean space to the symmetric positive semi-definite matrix manifold. Our\nmodel is combined with a roughness penalty to encourage smoothness of the\nestimated functions in both the temporal and covariate domains. We also develop\nan efficient method for fast evaluation of the objective and gradient\nfunctions. Cross-validation is used to choose the tuning parameters. We\ndemonstrate the advantages of our approach through a simulation study and an\nastronomical data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 16:08:35 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Ding", "Fei", ""], ["He", "Shiyuan", ""], ["Jones", "David E.", ""], ["Huang", "Jianhua Z.", ""]]}, {"id": "2001.11548", "submitter": "Damian Brzyski", "authors": "Damian Brzyski, Xixi Hu, Joaquin Goni, Beau Ances, Timothy W.\n  Randolph, Jaroslaw Harezlak", "title": "A Sparsity Inducing Nuclear-Norm Estimator (SpINNEr) for Matrix-Variate\n  Regression in Brain Connectivity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA math.NA math.OC stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical scalar-response regression methods treat covariates as a vector and\nestimate a corresponding vector of regression coefficients. In medical\napplications, however, regressors are often in a form of multi-dimensional\narrays. For example, one may be interested in using MRI imaging to identify\nwhich brain regions are associated with a health outcome. Vectorizing the\ntwo-dimensional image arrays is an unsatisfactory approach since it destroys\nthe inherent spatial structure of the images and can be computationally\nchallenging. We present an alternative approach - regularized matrix regression\n- where the matrix of regression coefficients is defined as a solution to the\nspecific optimization problem. The method, called SParsity Inducing Nuclear\nNorm EstimatoR (SpINNEr), simultaneously imposes two penalty types on the\nregression coefficient matrix---the nuclear norm and the lasso norm---to\nencourage a low rank matrix solution that also has entry-wise sparsity. A\nspecific implementation of the alternating direction method of multipliers\n(ADMM) is used to build a fast and efficient numerical solver. Our simulations\nshow that SpINNEr outperforms other methods in estimation accuracy when the\nresponse-related entries (representing the brain's functional connectivity) are\narranged in well-connected communities. SpINNEr is applied to investigate\nassociations between HIV-related outcomes and functional connectivity in the\nhuman brain.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jan 2020 20:10:53 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Brzyski", "Damian", ""], ["Hu", "Xixi", ""], ["Goni", "Joaquin", ""], ["Ances", "Beau", ""], ["Randolph", "Timothy W.", ""], ["Harezlak", "Jaroslaw", ""]]}, {"id": "2001.11685", "submitter": "Hao Yan", "authors": "Yujie Zhao, Hao Yan, Sarah E. Holte, Roxanne P. Kerani, Yajun Mei", "title": "Rapid Detection of Hot-spot by Tensor Decomposition with Application to\n  Weekly Gonorrhea Data", "comments": null, "journal-ref": "The XIIIth International Workshop on Intelligent Statistical\n  Quality Control, pp 289- 310, Hong Kong, 2019", "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many bio-surveillance and healthcare applications, data sources are\nmeasured from many spatial locations repeatedly over time, say,\ndaily/weekly/monthly. In these applications, we are typically interested in\ndetecting hot-spots, which are defined as some structured outliers that are\nsparse over the spatial domain but persistent over time. In this paper, we\npropose a tensor decomposition method to detect when and where the hot-spots\noccur. Our proposed methods represent the observed raw data as a\nthree-dimensional tensor including a circular time dimension for\ndaily/weekly/monthly patterns, and then decompose the tensor into three\ncomponents: smooth global trend, local hot-spots, and residuals. A combination\nof LASSO and fused LASSO is used to estimate the model parameters, and a CUSUM\nprocedure is applied to detect when and where the hot-spots might occur. The\nusefulness of our proposed methodology is validated through numerical\nsimulation and a real-world dataset in the weekly number of gonorrhea cases\nfrom $2006$ to $2018$ for $50$ states in the United States.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 07:41:16 GMT"}, {"version": "v2", "created": "Tue, 7 Apr 2020 05:57:33 GMT"}], "update_date": "2020-04-08", "authors_parsed": [["Zhao", "Yujie", ""], ["Yan", "Hao", ""], ["Holte", "Sarah E.", ""], ["Kerani", "Roxanne P.", ""], ["Mei", "Yajun", ""]]}, {"id": "2001.11930", "submitter": "Erik Scharw\\\"achter", "authors": "Erik Scharw\\\"achter and Emmanuel M\\\"uller", "title": "Two-Sample Testing for Event Impacts in Time Series", "comments": "SIAM International Conference on Data Mining (SDM 2020) preprint,\n  source code and supplementary material is available at\n  https://github.com/diozaka/eitest", "journal-ref": "Proceedings of the 2020 SIAM International Conference on Data\n  Mining, pp. 10-18", "doi": "10.1137/1.9781611976236.2", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many application domains, time series are monitored to detect extreme\nevents like technical faults, natural disasters, or disease outbreaks.\nUnfortunately, it is often non-trivial to select both a time series that is\ninformative about events and a powerful detection algorithm: detection may fail\nbecause the detection algorithm is not suitable, or because there is no shared\ninformation between the time series and the events of interest. In this work,\nwe thus propose a non-parametric statistical test for shared information\nbetween a time series and a series of observed events. Our test allows\nidentifying time series that carry information on event occurrences without\ncommitting to a specific event detection methodology. In a nutshell, we test\nfor divergences of the value distributions of the time series at increasing\nlags after event occurrences with a multiple two-sample testing approach. In\ncontrast to related tests, our approach is applicable for time series over\narbitrary domains, including multivariate numeric, strings or graphs. We\nperform a large-scale simulation study to show that it outperforms or is on par\nwith related tests on our task for univariate time series. We also demonstrate\nthe real-world applicability of our approach on datasets from social media and\nsmart home environments.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 16:13:02 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Scharw\u00e4chter", "Erik", ""], ["M\u00fcller", "Emmanuel", ""]]}, {"id": "2001.11945", "submitter": "Sifan Liu", "authors": "Sifan Liu, Regina Liu and Min-ge Xie", "title": "p-Value as the Strength of Evidence Measured by Confidence Distribution", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of p-value is a fundamental concept in statistical inference and\nhas been widely used for reporting outcomes of hypothesis tests. However,\np-value is often misinterpreted, misused or miscommunicated in practice. Part\nof the issue is that existing definitions of p-value are often derived from\nconstructions under specific settings, and a general definition that directly\nreflects the evidence of the null hypothesis is not yet available. In this\narticle, we first propose a general and rigorous definition of p-value that\nfulfills two performance-based characteristics. The performance-based\ndefinition subsumes all existing construction-based definitions of the p-value,\nand justifies their interpretations. The paper further presents a specific\napproach based on confidence distribution to formulate and calculate p-values.\nThis specific way of computing p values has two main advantages. First, it is\napplicable for a wide range of hypothesis testing problems, including the\nstandard one- and two-sided tests, tests with interval-type null,\nintersection-union tests, multivariate tests and so on. Second, it can\nnaturally lead to a coherent interpretation of p-value as evidence in support\nof the null hypothesis, as well as a meaningful measure of degree of such\nsupport. In particular, it places a meaning of a large p-value, e.g. p-value of\n0.8 has more support than 0.5. Numerical examples are used to illustrate the\nwide applicability and computational feasibility of our approach. We show that\nour proposal is effective and can be applied broadly, without further\nconsideration of the form/size of the null space. As for existing testing\nmethods, the solutions have not been available or cannot be easily obtained.\n", "versions": [{"version": "v1", "created": "Fri, 31 Jan 2020 16:43:30 GMT"}], "update_date": "2020-02-03", "authors_parsed": [["Liu", "Sifan", ""], ["Liu", "Regina", ""], ["Xie", "Min-ge", ""]]}]