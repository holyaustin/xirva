[{"id": "1501.00049", "submitter": "Ning Hao", "authors": "Ning Hao, Yang Feng and Hao Helen Zhang", "title": "Model Selection for High Dimensional Quadratic Regression via\n  Regularization", "comments": "37 pages, 1 figure with supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quadratic regression (QR) models naturally extend linear models by\nconsidering interaction effects between the covariates. To conduct model\nselection in QR, it is important to maintain the hierarchical model structure\nbetween main effects and interaction effects. Existing regularization methods\ngenerally achieve this goal by solving complex optimization problems, which\nusually demands high computational cost and hence are not feasible for high\ndimensional data. This paper focuses on scalable regularization methods for\nmodel selection in high dimensional QR. We first consider two-stage\nregularization methods and establish theoretical properties of the two-stage\nLASSO. Then, a new regularization method, called Regularization Algorithm under\nMarginality Principle (RAMP), is proposed to compute a hierarchy-preserving\nregularization solution path efficiently. Both methods are further extended to\nsolve generalized QR models. Numerical results are also shown to demonstrate\nperformance of the methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 00:06:25 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 19:24:23 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Hao", "Ning", ""], ["Feng", "Yang", ""], ["Zhang", "Hao Helen", ""]]}, {"id": "1501.00083", "submitter": "Ofir Harari", "authors": "Ofir Harari and David M. Steinberg", "title": "Variable Selection in Bayesian Semiparametric Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extend existing Bayesian methods for variable selection in\nGaussian process regression, to select both the regression terms and the active\ncovariates in the spatial correlation structure. We then use the estimated\nposterior probabilities to choose between relatively few modes through\ncross-validation, and consequently improve prediction.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 06:21:23 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Harari", "Ofir", ""], ["Steinberg", "David M.", ""]]}, {"id": "1501.00219", "submitter": "Jan Mandel", "authors": "Ivan Kasanick\\'y, Jan Mandel, and Martin Vejmelka", "title": "Spectral diagonal ensemble Kalman filters", "comments": "15 pages, 4 figures", "journal-ref": "Nonlinear Processes in Geophysics, 22, 485-497, 2015", "doi": "10.5194/npg-22-485-2015", "report-no": "UCD CCM Report 325", "categories": "stat.ME physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new type of ensemble Kalman filter is developed, which is based on\nreplacing the sample covariance in the analysis step by its diagonal in a\nspectral basis. It is proved that this technique improves the aproximation of\nthe covariance when the covariance itself is diagonal in the spectral basis, as\nis the case, e.g., for a second-order stationary random field and the Fourier\nbasis. The method is extended by wavelets to the case when the state variables\nare random fields, which are not spatially homogeneous. Efficient\nimplementations by the fast Fourier transform (FFT) and discrete wavelet\ntransform (DWT) are presented for several types of observations, including\nhigh-dimensional data given on a part of the domain, such as radar and\nsatellite images. Computational experiments confirm that the method performs\nwell on the Lorenz 96 problem and the shallow water equations with very small\nensembles and over multiple analysis cycles.\n", "versions": [{"version": "v1", "created": "Wed, 31 Dec 2014 22:41:17 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Kasanick\u00fd", "Ivan", ""], ["Mandel", "Jan", ""], ["Vejmelka", "Martin", ""]]}, {"id": "1501.00264", "submitter": "Antony Overstall", "authors": "Antony Overstall and David Woods", "title": "Bayesian Design of Experiments using Approximate Coordinate Exchange", "comments": "19 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of decision-theoretic Bayesian designs for\nrealistically-complex nonlinear models is computationally challenging, as it\nrequires the optimization of analytically intractable expected utility\nfunctions over high-dimensional design spaces. We provide the most general\nsolution to date for this problem through a novel approximate coordinate\nexchange algorithm. This methodology uses a Gaussian process emulator to\napproximate the expected utility as a function of a single design coordinate in\na series of conditional optimization steps. It has flexibility to address\nproblems for any choice of utility function and for a wide range of statistical\nmodels with different numbers of variables, numbers of runs and randomization\nrestrictions. In contrast to existing approaches to Bayesian design, the method\ncan find multi-variable designs in large numbers of runs without resorting to\nasymptotic approximations to the posterior distribution or expected utility.\nThe methodology is demonstrated on a variety of challenging examples of\npractical importance, including design for pharmacokinetic models and design\nfor mixed models with discrete data. For many of these models, Bayesian designs\nare not currently available. Comparisons are made to results from the\nliterature, and to designs obtained from asymptotic approximations.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jan 2015 09:33:58 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2015 16:03:53 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2016 11:05:25 GMT"}, {"version": "v4", "created": "Thu, 30 Jun 2016 11:28:24 GMT"}], "update_date": "2016-07-01", "authors_parsed": [["Overstall", "Antony", ""], ["Woods", "David", ""]]}, {"id": "1501.00372", "submitter": "Manuel Oviedo de la Fuente", "authors": "Juan A. Cuesta-Albertos, Manuel Febrero-Bande, Manuel Oviedo de la\n  Fuente", "title": "The DD$^G$-classifier in the functional setting", "comments": "29 pages, 6 figures, 6 tables, Supplemental R Code and Data", "journal-ref": "Cuesta-Albertos, J. A., Febrero-Bande, M., & de la Fuente, M. O.\n  (2017). The\\hbox {DD}^ G-classifier in the functional setting. Test, 26(1),\n  119-142", "doi": "10.1007/s11749-016-0502-6", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Maximum Depth was the first attempt to use data depths instead of\nmultivariate raw data to construct a classification rule. Recently, the\nDD-classifier has solved several serious limitations of the Maximum Depth\nclassifier but some issues still remain. This paper is devoted to extending the\nDD-classifier in the following ways: first, to surpass the limitation of the\nDD-classifier when more than two groups are involved. Second to apply regular\nclassification methods (like $k$NN, linear or quadratic classifiers, recursive\npartitioning,...) to DD-plots to obtain useful insights through the diagnostics\nof these methods. And third, to integrate different sources of information\n(data depths or multivariate functional data) in a unified way in the\nclassification procedure. Besides, as the DD-classifier trick is especially\nuseful in the functional framework, an enhanced revision of several functional\ndata depths is done in the paper. A simulation study and applications to some\nclassical real datasets are also provided showing the power of the new\nproposal.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 09:32:18 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2015 15:27:33 GMT"}, {"version": "v3", "created": "Mon, 2 May 2016 14:55:44 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Cuesta-Albertos", "Juan A.", ""], ["Febrero-Bande", "Manuel", ""], ["de la Fuente", "Manuel Oviedo", ""]]}, {"id": "1501.00438", "submitter": "Sebastian Vollmer", "authors": "Sebastian J. Vollmer and Konstantinos C. Zygalakis and and Yee Whye\n  Teh", "title": "(Non-) asymptotic properties of Stochastic Gradient Langevin Dynamics", "comments": "42 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying standard Markov chain Monte Carlo (MCMC) algorithms to large data\nsets is computationally infeasible. The recently proposed stochastic gradient\nLangevin dynamics (SGLD) method circumvents this problem in three ways: it\ngenerates proposed moves using only a subset of the data, it skips the\nMetropolis-Hastings accept-reject step, and it uses sequences of decreasing\nstep sizes. In \\cite{TehThierryVollmerSGLD2014}, we provided the mathematical\nfoundations for the decreasing step size SGLD, including consistency and a\ncentral limit theorem. However, in practice the SGLD is run for a relatively\nsmall number of iterations, and its step size is not decreased to zero. The\npresent article investigates the behaviour of the SGLD with fixed step size. In\nparticular we characterise the asymptotic bias explicitly, along with its\ndependence on the step size and the variance of the stochastic gradient. On\nthat basis a modified SGLD which removes the asymptotic bias due to the\nvariance of the stochastic gradients up to first order in the step size is\nderived. Moreover, we are able to obtain bounds on the finite-time bias,\nvariance and mean squared error (MSE). The theory is illustrated with a\nGaussian toy model for which the bias and the MSE for the estimation of moments\ncan be obtained explicitly. For this toy model we study the gain of the SGLD\nover the standard Euler method in the limit of large data sets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 17:18:56 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 11:00:30 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Vollmer", "Sebastian J.", ""], ["Zygalakis", "Konstantinos C.", ""], ["Teh", "and Yee Whye", ""]]}, {"id": "1501.00478", "submitter": "Anders Bredahl Kock", "authors": "Anders Bredahl Kock and Haihan Tang", "title": "Uniform Inference in High-dimensional Dynamic Panel Data Models", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish oracle inequalities for a version of the Lasso in\nhigh-dimensional fixed effects dynamic panel data models. The inequalities are\nvalid for the coefficients of the dynamic and exogenous regressors. Separate\noracle inequalities are derived for the fixed effects. Next, we show how one\ncan conduct uniformly valid simultaneous inference on the parameters of the\nmodel and construct a uniformly valid estimator of the asymptotic covariance\nmatrix which is robust to conditional heteroskedasticity in the error terms.\nAllowing for conditional heteroskedasticity is important in dynamic models as\nthe conditional error variance may be non-constant over time and depend on the\ncovariates. Furthermore, our procedure allows for inference on high-dimensional\nsubsets of the parameter vector of an increasing cardinality. We show that the\nconfidence bands resulting from our procedure are asymptotically honest and\ncontract at the optimal rate. This rate is different for the fixed effects than\nfor the remaining parts of the parameter vector.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jan 2015 20:37:00 GMT"}, {"version": "v2", "created": "Tue, 6 Jan 2015 12:18:39 GMT"}, {"version": "v3", "created": "Tue, 9 Jun 2015 15:32:38 GMT"}, {"version": "v4", "created": "Mon, 4 Jan 2016 15:57:58 GMT"}], "update_date": "2016-01-05", "authors_parsed": [["Kock", "Anders Bredahl", ""], ["Tang", "Haihan", ""]]}, {"id": "1501.00538", "submitter": "Jialiang Li", "authors": "Ming-Yen Cheng, Toshio Honda, Jialiang Li", "title": "Efficient estimation in semivarying coefficient models for\n  longitudinal/clustered data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In semivarying coefficient models for longitudinal/clustered data, usually of\nprimary interest is usually the parametric component which involves unknown\nconstant coefficients. First, we study semiparametric efficiency bound for\nestimation of the constant coefficients in a general setup. It can be achieved\nby spline regression provided that the within-cluster covariance matrices are\nall known, which is an unrealistic assumption in reality. Thus, we propose an\nadaptive estimator of the constant coefficients when the covariance matrices\nare unknown and depend only on the index random variable, such as time, and\nwhen the link function is the identity function. After preliminary estimation,\nbased on working independence and both spline and local linear regression, we\nestimate the covariance matrices by applying local linear regression to the\nresulting residuals. Then we employ the covariance matrix estimates and spline\nregression to obtain our final estimators of the constant coefficients. The\nproposed estimator achieves the semiparametric efficiency bound under normality\nassumption, and it has the smallest covariance matrix among a class of\nestimators even when normality is violated. We also present results of\nnumerical studies. The simulation results demonstrate that our estimator is\nsuperior to the one based on working independence. When applied to the CD4\ncount data, our method identifies an interesting structure that was not found\nby previous analyses.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2015 07:30:45 GMT"}, {"version": "v2", "created": "Thu, 10 Sep 2015 03:58:55 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2015 08:16:45 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Cheng", "Ming-Yen", ""], ["Honda", "Toshio", ""], ["Li", "Jialiang", ""]]}, {"id": "1501.00592", "submitter": "Necla Gunduz", "authors": "Necla Gunduz and Ernest Fokoue", "title": "Robust Classification of High Dimension Low Sample Size Data", "comments": "17 pages, 29 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robustification of pattern recognition techniques has been the subject of\nintense research in recent years. Despite the multiplicity of papers on the\nsubject, very few articles have deeply explored the topic of robust\nclassification in the high dimension low sample size context. In this work, we\nexplore and compare the predictive performances of robust classification\ntechniques with a special concentration on robust discriminant analysis and\nrobust PCA applied to a wide variety of large $p$ small $n$ data sets. We also\nexplore the performance of random forest by way of comparing and contrasting\nthe differences single model methods and ensemble methods in this context. Our\nwork reveals that Random Forest, although not inherently designed to be robust\nto outliers, substantially outperforms the existing techniques specifically\ndesigned to achieve robustness. Indeed, random forest emerges as the best\npredictively on both real life and simulated data.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2015 18:50:19 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Gunduz", "Necla", ""], ["Fokoue", "Ernest", ""]]}, {"id": "1501.00600", "submitter": "Anna Klimova", "authors": "Anna Klimova and Tam\\'as Rudas", "title": "On the closure of relational models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational models for contingency tables are generalizations of log-linear\nmodels, allowing effects associated with arbitrary subsets of cells in a\npossibly incomplete table, and not necessarily containing the overall effect.\nIn this generality, the MLEs under Poisson and multinomial sampling are not\nalways identical. This paper deals with the theory of maximum likelihood\nestimation in the case when there are observed zeros in the data. A unique MLE\nto such data is shown to always exist in the set of pointwise limits of\nsequences of distributions in the original model. This set is equal to the\nclosure of the original model with respect to the Bregman information\ndivergence. The same variant of iterative scaling may be used to compute the\nMLE in the original model and in its closure.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2015 20:15:16 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 12:30:44 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Klimova", "Anna", ""], ["Rudas", "Tam\u00e1s", ""]]}, {"id": "1501.00925", "submitter": "Justin Yang Mr.", "authors": "Neil Shephard and Justin J. Yang", "title": "Likelihood inference for exponential-trawl processes", "comments": "29 pages, 6 figures, forthcoming in: \"A Fascinating Journey through\n  Probability, Statistics and Applications: In Honour of Ole E.\n  Barndorff-Nielsen's 80th Birthday\", Springer, New York", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integer-valued trawl processes are a class of serially correlated, stationary\nand infinitely divisible processes that Ole E. Barndorff-Nielsen has been\nworking on in recent years. In this Chapter, we provide the first analysis of\nlikelihood inference for trawl processes by focusing on the so-called\nexponential-trawl process, which is also a continuous time hidden Markov\nprocess with countable state space. The core ideas include prediction\ndecomposition, filtering and smoothing, complete-data analysis and EM\nalgorithm. These can be easily scaled up to adapt to more general trawl\nprocesses but with increasing computation efforts.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 17:09:21 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Shephard", "Neil", ""], ["Yang", "Justin J.", ""]]}, {"id": "1501.01150", "submitter": "Yoann Altmann", "authors": "Yoann Altmann and Andrew Wallace and Steve McLaughlin", "title": "Spectral unmixing of Multispectral Lidar signals", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2015.2457401", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a Bayesian approach for spectral unmixing of\nmultispectral Lidar (MSL) data associated with surface reflection from targeted\nsurfaces composed of several known materials. The problem addressed is the\nestimation of the positions and area distribution of each material. In the\nBayesian framework, appropriate prior distributions are assigned to the unknown\nmodel parameters and a Markov chain Monte Carlo method is used to sample the\nresulting posterior distribution. The performance of the proposed algorithm is\nevaluated using synthetic MSL signals, for which single and multi-layered\nmodels are derived. To evaluate the expected estimation performance associated\nwith MSL signal analysis, a Cramer-Rao lower bound associated with model\nconsidered is also derived, and compared with the experimental data. Both the\ntheoretical lower bound and the experimental analysis will be of primary\nassistance in future instrument design.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 11:29:29 GMT"}, {"version": "v2", "created": "Mon, 5 Oct 2015 12:18:07 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Altmann", "Yoann", ""], ["Wallace", "Andrew", ""], ["McLaughlin", "Steve", ""]]}, {"id": "1501.01219", "submitter": "Viktoria \\\"Ollerer", "authors": "Viktoria \\\"Ollerer and Christophe Croux", "title": "Robust high-dimensional precision matrix estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dependency structure of multivariate data can be analyzed using the\ncovariance matrix $\\Sigma$. In many fields the precision matrix $\\Sigma^{-1}$\nis even more informative. As the sample covariance estimator is singular in\nhigh-dimensions, it cannot be used to obtain a precision matrix estimator. A\npopular high-dimensional estimator is the graphical lasso, but it lacks\nrobustness. We consider the high-dimensional independent contamination model.\nHere, even a small percentage of contaminated cells in the data matrix may lead\nto a high percentage of contaminated rows. Downweighting entire observations,\nwhich is done by traditional robust procedures, would then results in a loss of\ninformation. In this paper, we formally prove that replacing the sample\ncovariance matrix in the graphical lasso with an elementwise robust covariance\nmatrix leads to an elementwise robust, sparse precision matrix estimator\ncomputable in high-dimensions. Examples of such elementwise robust covariance\nestimators are given. The final precision matrix estimator is positive\ndefinite, has a high breakdown point under elementwise contamination and can be\ncomputed fast.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 16:18:16 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 11:52:17 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["\u00d6llerer", "Viktoria", ""], ["Croux", "Christophe", ""]]}, {"id": "1501.01231", "submitter": "Ines Wilms", "authors": "Ines Wilms and Christophe Croux", "title": "Sparse canonical correlation analysis from a predictive point of view", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) describes the associations between two\nsets of variables by maximizing the correlation between linear combinations of\nthe variables in each data set. However, in high-dimensional settings where the\nnumber of variables exceeds the sample size or when the variables are highly\ncorrelated, traditional CCA is no longer appropriate. This paper proposes a\nmethod for sparse CCA. Sparse estimation produces linear combinations of only a\nsubset of variables from each data set, thereby increasing the interpretability\nof the canonical variates. We consider the CCA problem from a predictive point\nof view and recast it into a regression framework. By combining an alternating\nregression approach together with a lasso penalty, we induce sparsity in the\ncanonical vectors. We compare the performance with other sparse CCA techniques\nin different simulation settings and illustrate its usefulness on a genomic\ndata set.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 16:44:49 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Wilms", "Ines", ""], ["Croux", "Christophe", ""]]}, {"id": "1501.01233", "submitter": "Ines Wilms", "authors": "Ines Wilms and Christophe Croux", "title": "Robust Sparse Canonical Correlation Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical correlation analysis (CCA) is a multivariate statistical method\nwhich describes the associations between two sets of variables. The objective\nis to find linear combinations of the variables in each data set having maximal\ncorrelation. This paper discusses a method for Robust Sparse CCA. Sparse\nestimation produces canonical vectors with some of their elements estimated as\nexactly zero. As such, their interpretability is improved. We also robustify\nthe method such that it can cope with outliers in the data. To estimate the\ncanonical vectors, we convert the CCA problem into an alternating regression\nframework, and use the sparse Least Trimmed Squares estimator. We illustrate\nthe good performance of the Robust Sparse CCA method in several simulation\nstudies and two real data examples.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 16:53:24 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Wilms", "Ines", ""], ["Croux", "Christophe", ""]]}, {"id": "1501.01234", "submitter": "Alexander Volfovsky", "authors": "Alexander Volfovsky, Edoardo M. Airoldi, Donald B. Rubin", "title": "Causal inference for ordinal outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many outcomes of interest in the social and health sciences, as well as in\nmodern applications in computational social science and experimentation on\nsocial media platforms, are ordinal and do not have a meaningful scale. Causal\nanalyses that leverage this type of data, termed ordinal non-numeric, require\ncareful treatment, as much of the classical potential outcomes literature is\nconcerned with estimation and hypothesis testing for outcomes whose relative\nmagnitudes are well defined. Here, we propose a class of finite population\ncausal estimands that depend on conditional distributions of the potential\noutcomes, and provide an interpretable summary of causal effects when no scale\nis available. We formulate a relaxation of the Fisherian sharp null hypothesis\nof constant effect that accommodates the scale-free nature of ordinal\nnon-numeric data. We develop a Bayesian procedure to estimate the proposed\ncausal estimands that leverages the rank likelihood. We illustrate these\nmethods with an application to educational outcomes in the General Social\nSurvey.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 16:53:33 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Volfovsky", "Alexander", ""], ["Airoldi", "Edoardo M.", ""], ["Rubin", "Donald B.", ""]]}, {"id": "1501.01250", "submitter": "Ines Wilms", "authors": "Ines Wilms and Christophe Croux", "title": "Sparse cointegration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cointegration analysis is used to estimate the long-run equilibrium relations\nbetween several time series. The coefficients of these long-run equilibrium\nrelations are the cointegrating vectors. In this paper, we provide a sparse\nestimator of the cointegrating vectors. The estimation technique is sparse in\nthe sense that some elements of the cointegrating vectors will be estimated as\nzero. For this purpose, we combine a penalized estimation procedure for vector\nautoregressive models with sparse reduced rank regression. The sparse\ncointegration procedure achieves a higher estimation accuracy than the\ntraditional Johansen cointegration approach in settings where the true\ncointegrating vectors have a sparse structure, and/or when the sample size is\nlow compared to the number of time series. We also discuss a criterion to\ndetermine the cointegration rank and we illustrate its good performance in\nseveral simulation settings. In a first empirical application we investigate\nwhether the expectations hypothesis of the term structure of interest rates,\nimplying sparse cointegrating vectors, holds in practice. In a second empirical\napplication we show that forecast performance in high-dimensional systems can\nbe improved by sparsely estimating the cointegration relations.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 17:57:36 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Wilms", "Ines", ""], ["Croux", "Christophe", ""]]}, {"id": "1501.01265", "submitter": "Serena Ng", "authors": "Jean-Jacques Forneron and Serena Ng", "title": "The ABC of Simulation Estimation with Auxiliary Statistics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The frequentist method of simulated minimum distance (SMD) is widely used in\neconomics to estimate complex models with an intractable likelihood. In other\ndisciplines, a Bayesian approach known as Approximate Bayesian Computation\n(ABC) is far more popular. This paper connects these two seemingly related\napproaches to likelihood-free estimation by means of a Reverse Sampler that\nuses both optimization and importance weighting to target the posterior\ndistribution. Its hybrid features enable us to analyze an ABC estimate from the\nperspective of SMD. We show that an ideal ABC estimate can be obtained as a\nweighted average of a sequence of SMD modes, each being the minimizer of the\ndeviations between the data and the model. This contrasts with the SMD, which\nis the mode of the average deviations. Using stochastic expansions, we provide\na general characterization of frequentist estimators and those based on\nBayesian computations including Laplace-type estimators. Their differences are\nillustrated using analytical examples and a simulation study of the dynamic\npanel model.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 19:14:19 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2015 15:22:14 GMT"}, {"version": "v3", "created": "Tue, 1 Dec 2015 16:02:19 GMT"}, {"version": "v4", "created": "Wed, 11 Oct 2017 15:39:23 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Forneron", "Jean-Jacques", ""], ["Ng", "Serena", ""]]}, {"id": "1501.01271", "submitter": "Moritz Jirak", "authors": "Moritz Jirak", "title": "Optimal eigen expansions and uniform bounds", "comments": "corrected some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\bigl\\{X_k\\bigr\\}_{k \\in \\mathbb{Z}} \\in \\mathbb{L}^2(\\mathcal{T})$ be a\nstationary process with associated lag operators ${\\boldsymbol{\\cal C}}_h$.\nUniform asymptotic expansions of the corresponding empirical eigenvalues and\neigenfunctions are established under almost optimal conditions on the lag\noperators in terms of the eigenvalues (spectral gap). In addition, the\nunderlying dependence assumptions are optimal, including both short and long\nmemory processes. This allows us to study the relative maximum deviation of the\nempirical eigenvalues under very general conditions. Among other things, we\nshow convergence to an extreme value distribution, giving rise to the\nconstruction of simultaneous confidence sets. We also discuss how the\nasymptotic expansions transfer to the long-run covariance operator\n${\\boldsymbol{\\cal G}}$ in a general framework.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 20:25:38 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 21:40:16 GMT"}, {"version": "v3", "created": "Thu, 27 Aug 2015 21:32:24 GMT"}, {"version": "v4", "created": "Thu, 3 Sep 2015 20:21:48 GMT"}, {"version": "v5", "created": "Fri, 18 Sep 2015 14:21:14 GMT"}, {"version": "v6", "created": "Fri, 23 Oct 2015 10:16:05 GMT"}, {"version": "v7", "created": "Sun, 14 Feb 2016 21:58:51 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Jirak", "Moritz", ""]]}, {"id": "1501.01332", "submitter": "Jonas Peters", "authors": "Jonas Peters, Peter B\\\"uhlmann, Nicolai Meinshausen", "title": "Causal inference using invariant prediction: identification and\n  confidence intervals", "comments": null, "journal-ref": "Journal of the Royal Statistical Society, Series B (with\n  discussion) 78(5): 947-1012 (2016)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the difference of a prediction that is made with a causal model and a\nnon-causal model? Suppose we intervene on the predictor variables or change the\nwhole environment. The predictions from a causal model will in general work as\nwell under interventions as for observational data. In contrast, predictions\nfrom a non-causal model can potentially be very wrong if we actively intervene\non variables. Here, we propose to exploit this invariance of a prediction under\na causal model for causal inference: given different experimental settings (for\nexample various interventions) we collect all models that do show invariance in\ntheir predictive accuracy across settings and interventions. The causal model\nwill be a member of this set of models with high probability. This approach\nyields valid confidence intervals for the causal relationships in quite general\nscenarios. We examine the example of structural equation models in more detail\nand provide sufficient assumptions under which the set of causal predictors\nbecomes identifiable. We further investigate robustness properties of our\napproach under model misspecification and discuss possible extensions. The\nempirical properties are studied for various data sets, including large-scale\ngene perturbation experiments.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 23:07:35 GMT"}, {"version": "v2", "created": "Thu, 9 Jul 2015 12:13:09 GMT"}, {"version": "v3", "created": "Tue, 24 Nov 2015 08:56:13 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Peters", "Jonas", ""], ["B\u00fchlmann", "Peter", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1501.01366", "submitter": "Georgios Fellouris Dr.", "authors": "Shiyu Wang, Georgios Fellouris and Hua-Hua Chang", "title": "Sequential Design for Computerized Adaptive Testing that Allows for\n  Response Revision", "comments": "32 pages,2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computerized adaptive testing (CAT), items (questions) are selected in\nreal time based on the already observed responses, so that the ability of the\nexaminee can be estimated as accurately as possible. This is typically\nformulated as a non-linear, sequential, experimental design problem with binary\nobservations that correspond to the true or false responses. However, most\nitems in practice are multiple-choice and dichotomous models do not make full\nuse of the available data. Moreover, CAT has been heavily criticized for not\nallowing test-takers to review and revise their answers. In this work, we\npropose a novel CAT design that is based on the polytomous nominal response\nmodel and in which test-takers are allowed to revise their responses at any\ntime during the test. We show that as the number of administered items goes to\ninfinity, the proposed estimator is (i) strongly consistent for any item\nselection and revision strategy and (ii) asymptotically normal when the items\nare selected to maximize the Fisher information at the current ability estimate\nand the number of revisions is smaller than the number of items. We also\npresent the findings of a simulation study that supports our asymptotic\nresults.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 04:07:02 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Wang", "Shiyu", ""], ["Fellouris", "Georgios", ""], ["Chang", "Hua-Hua", ""]]}, {"id": "1501.01432", "submitter": "Kuang Zhou", "authors": "Kuang Zhou (IRISA), Arnaud Martin (IRISA), Quan Pan", "title": "Evidential-EM Algorithm Applied to Progressively Censored Observations", "comments": null, "journal-ref": "15th International Conference on Information Processing and\n  Management of Uncertainty in Knowledge-Based Systems, Jul 2014, Montpellier,\n  France. pp.180 - 189", "doi": "10.1007/978-3-319-08852-5_19", "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidential-EM (E2M) algorithm is an effective approach for computing maximum\nlikelihood estimations under finite mixture models, especially when there is\nuncertain information about data. In this paper we present an extension of the\nE2M method in a particular case of incom-plete data, where the loss of\ninformation is due to both mixture models and censored observations. The prior\nuncertain information is expressed by belief functions, while the\npseudo-likelihood function is derived based on imprecise observations and prior\nknowledge. Then E2M method is evoked to maximize the generalized likelihood\nfunction to obtain the optimal estimation of parameters. Numerical examples\nshow that the proposed method could effectively integrate the uncertain prior\ninfor-mation with the current imprecise knowledge conveyed by the observed\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 10:27:45 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Zhou", "Kuang", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Pan", "Quan", ""]]}, {"id": "1501.01617", "submitter": "Lucy Xia", "authors": "Jianqing Fan, Yang Feng, Lucy Xia", "title": "A Projection Based Conditional Dependence Measure with Applications to\n  High-dimensional Undirected Graphical Models", "comments": "39 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring conditional dependence is an important topic in statistics with\nbroad applications including graphical models. Under a factor model setting, a\nnew conditional dependence measure based on projection is proposed. The\ncorresponding conditional independence test is developed with the asymptotic\nnull distribution unveiled where the number of factors could be\nhigh-dimensional. It is also shown that the new test has control over the\nasymptotic significance level and can be calculated efficiently. A generic\nmethod for building dependency graphs without Gaussian assumption using the new\ntest is elaborated. Numerical results and real data analysis show the\nsuperiority of the new method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 20:43:14 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2015 20:08:35 GMT"}, {"version": "v3", "created": "Wed, 23 Nov 2016 04:50:55 GMT"}, {"version": "v4", "created": "Tue, 14 Feb 2017 20:33:54 GMT"}, {"version": "v5", "created": "Fri, 11 Jan 2019 07:36:17 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Fan", "Jianqing", ""], ["Feng", "Yang", ""], ["Xia", "Lucy", ""]]}, {"id": "1501.01665", "submitter": "Benjamin Taylor", "authors": "Benjamin M. Taylor", "title": "Auxiliary Variable Markov Chain Monte Carlo for Spatial Survival and\n  Geostatistical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article was motivated by the desire to improve Markov chain Monte Carlo\nmethods for spatial survival models in which the locations of individuals in\nspace are known. For a dataset comprising information on n individuals,\nstandard methods of MCMC-based inference involve computing the inverse of an n\nby n matrix at each iteration. However with a judicious choice of auxiliary\nvariables on a regular grid with m prediction points it will be shown how to\nfit an essentially equivalent model but with a substantially reduced\ncomputational cost. For a fixed output grid, the computational cost of the new\nmethod is reduced from O(n^3) to O(n); the cost of increasing the output grid\nsize being O(m\\log m). Furthermore, the new method simultaneously solves the\nproblem of spatial prediction of functions of the latent field, which for\nstandard methods usually presents a further computational challenge. We apply\nthe new method to a spatial survival dataset previously analysed in Henderson\net. al 2002 and show how the new method can be applied to spatial and\nspatiotemporal geostatistical datasets with the same computational benefits.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 22:05:27 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Taylor", "Benjamin M.", ""]]}, {"id": "1501.01756", "submitter": "Florencia Leonardi", "authors": "Bruno M. de Castro and Florencia Leonardi", "title": "A model selection approach for multiple sequence segmentation and\n  dimensionality reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the problem of segmenting $n$ aligned random\nsequences of equal length $m$, into a finite number of independent blocks. We\npropose to use a penalized maximum likelihood criterion to infer simultaneously\nthe number of points of independence as well as the position of each one of\nthese points. We show how to compute the estimator efficiently by means of a\ndynamic programming algorithm with time complexity $O(m^2n)$. We also propose\nanother algorithm, called hierarchical algorithm, that provides an\napproximation to the estimator when the sample size increases and runs in time\n$O(mn)$. Our main theoretical result is the proof of almost sure consistency of\nthe estimator and the convergence of the hierarchical algorithm when the sample\nsize $n$ grows to infinity. We illustrate the convergence of these algorithms\nthrough some simulation examples and we apply the method to a real protein\nsequence alignment of Ebola Virus.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 08:04:00 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["de Castro", "Bruno M.", ""], ["Leonardi", "Florencia", ""]]}, {"id": "1501.01763", "submitter": "Jianfeng Yao", "authors": "Zhaoyuan Li and Jianfeng Yao", "title": "On Two Simple and Effective Procedures for High Dimensional\n  Classification of General Populations", "comments": "5 figures; 22 pages. To appear in \"Statistical Papers\"", "journal-ref": "Statistical Papers 57(2):381-405, April 2016", "doi": "10.1007/s00362-015-0660-8", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we generalize two criteria, the determinant-based and\ntrace-based criteria proposed by Saranadasa (1993), to general populations for\nhigh dimensional classification. These two criteria compare some distances\nbetween a new observation and several different known groups. The\ndeterminant-based criterion performs well for correlated variables by\nintegrating the covariance structure and is competitive to many other existing\nrules. The criterion however requires the measurement dimension be smaller than\nthe sample size. The trace-based criterion in contrast, is an independence rule\nand effective in the \"large dimension-small sample size\" scenario. An appealing\nproperty of these two criteria is that their implementation is straightforward\nand there is no need for preliminary variable selection or use of turning\nparameters. Their asymptotic misclassification probabilities are derived using\nthe theory of large dimensional random matrices. Their competitive performances\nare illustrated by intensive Monte Carlo experiments and a real data analysis.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 08:26:49 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Li", "Zhaoyuan", ""], ["Yao", "Jianfeng", ""]]}, {"id": "1501.01774", "submitter": "Holger Dette", "authors": "Holger Dette, Andrey Pepelyshev, Anatoly Zhigljavsky", "title": "Optimal designs in regression with correlated errors", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the problem of determining optimal designs for\nregression models, when the observations are dependent and taken on an\ninterval. A complete solution of this challenging optimal design problem is\ngiven for a broad class of regression models and covariance kernels.\n  We propose a class of estimators which are only slightly more complicated\nthan the ordinary least-squares estimators. We then demonstrate that we can\ndesign the experiments, such that asymptotically the new estimators achieve the\nsame precision as the best linear unbiased estimator computed for the whole\ntrajectory of the process. As a by-product we derive explicit expressions for\nthe BLUE in the continuous time model and analytic expressions for the optimal\ndesigns in a wide class of regression models. We also demonstrate that for a\nfinite number of observations the precision of the proposed procedure, which\nincludes the estimator and design, is very close to the best achievable. The\nresults are illustrated on a few numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 09:19:44 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 09:25:46 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Dette", "Holger", ""], ["Pepelyshev", "Andrey", ""], ["Zhigljavsky", "Anatoly", ""]]}, {"id": "1501.01840", "submitter": "Ryan Martin", "authors": "Nick Syring and Ryan Martin", "title": "Gibbs posterior inference on the minimum clinically important difference", "comments": "17 pages, 1 figure, 2 tables", "journal-ref": "Journal of Statistical Planning and Inference, volume 187, pages\n  67--77, 2017", "doi": "10.1016/j.jspi.2017.03.001", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IIt is known that a statistically significant treatment may not be clinically\nsignificant. A quantity that can be used to assess clinical significance is\ncalled the minimum clinically important difference (MCID), and inference on the\nMCID is an important and challenging problem. Modeling for the purpose of\ninference on the MCID is non-trivial, and concerns about bias from a\nmisspecified parametric model or inefficiency from a nonparametric model\nmotivate an alternative approach to balance robustness and efficiency. In\nparticular, a recently proposed representation of the MCID as the minimizer of\na suitable risk function makes it possible to construct a Gibbs posterior\ndistribution for the MCID without specifying a model. We establish the\nposterior convergence rate and show, numerically, that an appropriately scaled\nversion of this Gibbs posterior yields interval estimates for the MCID which\nare both valid and efficient even for relatively small sample sizes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 13:37:50 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2015 18:36:22 GMT"}, {"version": "v3", "created": "Sat, 3 Sep 2016 17:23:29 GMT"}, {"version": "v4", "created": "Mon, 26 Jun 2017 19:19:13 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Syring", "Nick", ""], ["Martin", "Ryan", ""]]}, {"id": "1501.01847", "submitter": "Catia  Scricciolo", "authors": "Catia Scricciolo", "title": "Empirical Bayes conditional density estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of nonparametric estimation of the conditional density of a\nresponse, given a vector of explanatory variables, is classical and of\nprominent importance in many prediction problems since the conditional density\nprovides a more comprehensive description of the association between the\nresponse and the predictor than, for instance, does the regression function.\nThe problem has applications across different fields like economy, actuarial\nsciences and medicine. We investigate empirical Bayes estimation of conditional\ndensities establishing that an automatic data-driven selection of the prior\nhyper-parameters in infinite mixtures of Gaussian kernels, with\npredictor-dependent mixing weights, can lead to estimators whose performance is\non par with that of frequentist estimators in being minimax-optimal (up to\nlogarithmic factors) rate adaptive over classes of locally H\\\"older smooth\nconditional densities and in performing an adaptive dimension reduction if the\nresponse is independent of (some of) the explanatory variables which,\ncontaining no information about the response, are irrelevant to the purpose of\nestimating its conditional density.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 13:56:57 GMT"}, {"version": "v2", "created": "Mon, 20 Apr 2015 10:13:16 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Scricciolo", "Catia", ""]]}, {"id": "1501.01859", "submitter": "Carlo Sguera", "authors": "Carlo Sguera and Pedro Galeano and Rosa Lillo", "title": "Functional outlier detection by a local depth with application to NOx\n  levels", "comments": "in Stochastic Environmental Research and Risk Assessment, 2015", "journal-ref": null, "doi": "10.1007/s00477-015-1096-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes methods to detect outliers in functional data sets and\nthe task of identifying atypical curves is carried out using the recently\nproposed kernelized functional spatial depth (KFSD). KFSD is a local depth that\ncan be used to order the curves of a sample from the most to the least central,\nand since outliers are usually among the least central curves, we present a\nprobabilistic result which allows to select a threshold value for KFSD such\nthat curves with depth values lower than the threshold are detected as\noutliers. Based on this result, we propose three new outlier detection\nprocedures. The results of a simulation study show that our proposals generally\noutperform a battery of competitors. We apply our procedures to a real data set\nconsisting in daily curves of emission levels of nitrogen oxides (NOx) since it\nis of interest to identify abnormal NOx levels to take necessary environmental\npolitical actions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 14:15:39 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 15:23:23 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Sguera", "Carlo", ""], ["Galeano", "Pedro", ""], ["Lillo", "Rosa", ""]]}, {"id": "1501.01898", "submitter": "Jia  Liu", "authors": "Jia Liu, Dario Gasbarra, Juha Railavo", "title": "Fast Estimation of Diffusion Tensors under Rician noise by the EM\n  algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a fast computational method, the Expectation Maximization\nalgorithm, for Maximum Likelihood (ML) estimation in diffusion tensor imaging\nunder the Rice noise model. We further extend the ML framework to the maximum a\nposterior (MAP) estimation and describe the numerical similarities of both ML\nand MAP estimators. This novel method is implemented and applied using both\nsynthetic and real data in a wide range of b amplitudes. The comparison with\nother popular methods are made in accuracy, methodology and computation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 16:29:16 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Liu", "Jia", ""], ["Gasbarra", "Dario", ""], ["Railavo", "Juha", ""]]}, {"id": "1501.01937", "submitter": "Won Chang", "authors": "Won Chang, Murali Haran, Patrick Applegate, David Pollard", "title": "Calibrating an ice sheet model using high-dimensional binary spatial\n  data", "comments": null, "journal-ref": "Journal of the American Statistical Association (2016), Volume\n  111, Issue 513, 57-72", "doi": "10.1080/01621459.2015.1108199", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid retreat of ice in the Amundsen Sea sector of West Antarctica may cause\ndrastic sea level rise, posing significant risks to populations in low-lying\ncoastal regions. Calibration of computer models representing the behavior of\nthe West Antarctic Ice Sheet is key for informative projections of future sea\nlevel rise. However, both the relevant observations and the model output are\nhigh-dimensional binary spatial data; existing computer model calibration\nmethods are unable to handle such data. Here we present a novel calibration\nmethod for computer models whose output is in the form of binary spatial data.\nTo mitigate the computational and inferential challenges posed by our approach,\nwe apply a generalized principal component based dimension reduction method. To\ndemonstrate the utility of our method, we calibrate the PSU3D-ICE model by\ncomparing the output from a 499-member perturbed-parameter ensemble with\nobservations from the Amundsen Sea sector of the ice sheet. Our methods help\nrigorously characterize the parameter uncertainty even in the presence of\nsystematic data-model discrepancies and dependence in the errors. Our method\nalso helps inform environmental risk analyses by contributing to improved\nprojections of sea level rise from the ice sheets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 19:45:00 GMT"}, {"version": "v2", "created": "Fri, 9 Jan 2015 03:49:32 GMT"}, {"version": "v3", "created": "Tue, 5 May 2015 21:43:12 GMT"}, {"version": "v4", "created": "Thu, 2 Jul 2015 22:50:38 GMT"}, {"version": "v5", "created": "Thu, 3 Sep 2015 03:25:39 GMT"}, {"version": "v6", "created": "Fri, 20 May 2016 04:38:32 GMT"}], "update_date": "2016-05-23", "authors_parsed": [["Chang", "Won", ""], ["Haran", "Murali", ""], ["Applegate", "Patrick", ""], ["Pollard", "David", ""]]}, {"id": "1501.01946", "submitter": "Renato J Cintra", "authors": "D. Suarez, R. J. Cintra, F. M. Bayer, A. Sengupta, S. Kulasekera, A.\n  Madanayake", "title": "Multi-Beam RF Aperture Using Multiplierless FFT Approximation", "comments": "8 pages, 3 figures, 2 tables, sfg corrected", "journal-ref": "Electronics Letters, volume 50, issue 24, pages 1788-1790, 2014", "doi": "10.1049/el.2014.3561", "report-no": null, "categories": "stat.ME cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple independent radio frequency (RF) beams find applications in\ncommunications, radio astronomy, radar, and microwave imaging. An $N$-point FFT\napplied spatially across an array of receiver antennas provides $N$-independent\nRF beams at $\\frac{N}{2}\\log_2N$ multiplier complexity. Here, a low-complexity\nmultiplierless approximation for the 8-point FFT is presented for RF\nbeamforming, using only 26 additions. The algorithm provides eight beams that\nclosely resemble the antenna array patterns of the traditional FFT-based\nbeamformer albeit without using multipliers. The proposed FFT-like algorithm is\nuseful for low-power RF multi-beam receivers; being synthesized in 45 nm CMOS\ntechnology at 1.1 V supply, and verified on-chip using a Xilinx Virtex-6 Lx240T\nFPGA device. The CMOS simulation and FPGA implementation indicate bandwidths of\n588 MHz and 369 MHz, respectively, for each of the independent receive-mode RF\nbeams.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 20:21:05 GMT"}], "update_date": "2015-01-09", "authors_parsed": [["Suarez", "D.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Sengupta", "A.", ""], ["Kulasekera", "S.", ""], ["Madanayake", "A.", ""]]}, {"id": "1501.01950", "submitter": "Garth Tarr", "authors": "Garth Tarr, Samuel M\\\"uller, Neville C. Weber", "title": "Robust estimation of precision matrices under cellwise contamination", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2015.02.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a great need for robust techniques in data mining and machine\nlearning contexts where many standard techniques such as principal component\nanalysis and linear discriminant analysis are inherently susceptible to\noutliers. Furthermore, standard robust procedures assume that less than half\nthe observation rows of a data matrix are contaminated, which may not be a\nrealistic assumption when the number of observed features is large. This work\nlooks at the problem of estimating covariance and precision matrices under\ncellwise contamination. We consider using a robust pairwise covariance matrix\nas an input to various regularisation routines, such as the graphical lasso,\nQUIC and CLIME. To ensure the input covariance matrix is positive semidefinite,\nwe use a method that transforms a symmetric matrix of pairwise covariances to\nthe nearest covariance matrix. The result is a potentially sparse precision\nmatrix that is resilient to moderate levels of cellwise contamination. Since\nthis procedure is not based on subsampling it scales well as the number of\nvariables increases.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jan 2015 20:36:18 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Tarr", "Garth", ""], ["M\u00fcller", "Samuel", ""], ["Weber", "Neville C.", ""]]}, {"id": "1501.02157", "submitter": "Maria Francesca  Marino", "authors": "Maria Francesca Marino and Nikos Tzavidis and Marco Alfo'", "title": "Quantile regression for longitudinal data: unobserved heterogeneity and\n  informative missingness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear quantile regression models aim at providing a detailed and robust\npicture of the (conditional) response distribution as function of a set of\nobserved covariates. Longitudinal data represent an interesting field of\napplication of such models; due to their peculiar features, they represent a\nsubstantial challenge, in that the standard, cross-sectional, model\nrepresentation needs to be extended for dealing with such kind of data. In\nfact, repeated observations from the same statistical unit poses a problem of\ndependence; in a conditional perspective, this dependence could be ascribed to\nsources of unobserved, individual-specific, heterogeneity. Along these lines,\nquantile regression models have recently been extended to the analysis of\nlongitudinal, continuous, responses, by modelling dependence via time-constant\nor time-varying random effects. In this manuscript, we introduce a general\nquantile regression model for longitudinal, continuous, responses where\ntime-varying and time-constant random parameters are jointly taken into\naccount. A further feature of longitudinal designs is the presence of partially\nincomplete sequences, due to some individuals leaving the study before its\ndesigned end. The missing data process may produce a selection of units which\ncan be informative with respect to the parameters of the longitudinal data\nmodel. To deal with the case of irretrievable drop-out, we introduce a pattern\nmixture version of the linear quantile hidden Markov model, where we account\nfor time-varying heterogeneity and for changes in the fixed effect vector due\nto differential propensities to stay in the study. The proposed models are\nillustrated using a well known benchmark dataset on longitudinal dynamics of\nCD4 cells and by means of a large scale simulation study, entailing different\nquantiles and both complete and partially complete (ie subject to drop-out)\nindividual sequences.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 14:38:31 GMT"}, {"version": "v2", "created": "Wed, 29 Jul 2015 08:30:27 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Marino", "Maria Francesca", ""], ["Tzavidis", "Nikos", ""], ["Alfo'", "Marco", ""]]}, {"id": "1501.02248", "submitter": "Francesco Papi", "authors": "Francesco Papi and Du Yong Kim", "title": "A Particle Multi-Target Tracker for Superpositional Measurements using\n  Labeled Random Finite Sets", "comments": "arXiv admin note: text overlap with arXiv:1312.2372 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a general solution for multi-target tracking with\nsuperpositional measurements. Measurements that are functions of the sum of the\ncontributions of the targets present in the surveillance area are called\nsuperpositional measurements. We base our modelling on Labeled Random Finite\nSet (RFS) in order to jointly estimate the number of targets and their\ntrajectories. This modelling leads to a labeled version of Mahler's\nmulti-target Bayes filter. However, a straightforward implementation of this\ntracker using Sequential Monte Carlo (SMC) methods is not feasible due to the\ndifficulties of sampling in high dimensional spaces. We propose an efficient\nmulti-target sampling strategy based on Superpositional Approximate CPHD\n(SA-CPHD) filter and the recently introduced Labeled Multi-Bernoulli (LMB) and\nVo-Vo densities. The applicability of the proposed approach is verified through\nsimulation in a challenging radar application with closely spaced targets and\nlow signal-to-noise ratio.\n", "versions": [{"version": "v1", "created": "Fri, 19 Dec 2014 07:32:13 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2015 02:42:01 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Papi", "Francesco", ""], ["Kim", "Du Yong", ""]]}, {"id": "1501.02252", "submitter": "Junxiao Song", "authors": "Junxiao Song, Prabhu Babu, and Daniel P. Palomar", "title": "Optimization Methods for Designing Sequences with Low Autocorrelation\n  Sidelobes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unimodular sequences with low autocorrelations are desired in many\napplications, especially in the area of radar and code-division multiple access\n(CDMA). In this paper, we propose a new algorithm to design unimodular\nsequences with low integrated sidelobe level (ISL), which is a widely used\nmeasure of the goodness of a sequence's correlation property. The algorithm\nfalls into the general framework of majorization-minimization (MM) algorithms\nand thus shares the monotonic property of such algorithms. In addition, the\nalgorithm can be implemented via fast Fourier transform (FFT) operations and\nthus is computationally efficient. Furthermore, after some modifications the\nalgorithm can be adapted to incorporate spectral constraints, which makes the\ndesign more flexible. Numerical experiments show that the proposed algorithms\noutperform existing algorithms in terms of both the quality of designed\nsequences and the computational complexity.\n", "versions": [{"version": "v1", "created": "Fri, 26 Dec 2014 13:50:16 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Song", "Junxiao", ""], ["Babu", "Prabhu", ""], ["Palomar", "Daniel P.", ""]]}, {"id": "1501.02315", "submitter": "Panos Toulis", "authors": "Panagiotis (Panos) Toulis, David C. Parkes", "title": "Long-term causal effects via behavioral game theory", "comments": "30th Conference on Neural Information Processing Systems (NIPS'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planned experiments are the gold standard in reliably comparing the causal\neffect of switching from a baseline policy to a new policy. One critical\nshortcoming of classical experimental methods, however, is that they typically\ndo not take into account the dynamic nature of response to policy changes. For\ninstance, in an experiment where we seek to understand the effects of a new ad\npricing policy on auction revenue, agents may adapt their bidding in response\nto the experimental pricing changes. Thus, causal effects of the new pricing\npolicy after such adaptation period, the {\\em long-term causal effects}, are\nnot captured by the classical methodology even though they clearly are more\nindicative of the value of the new policy. Here, we formalize a framework to\ndefine and estimate long-term causal effects of policy changes in multiagent\neconomies. Central to our approach is behavioral game theory, which we leverage\nto formulate the ignorability assumptions that are necessary for causal\ninference. Under such assumptions we estimate long-term causal effects through\na latent space approach, where a behavioral model of how agents act conditional\non their latent behaviors is combined with a temporal model of how behaviors\nevolve over time.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 07:06:43 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 03:10:48 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2015 20:20:48 GMT"}, {"version": "v4", "created": "Fri, 16 Oct 2015 17:50:51 GMT"}, {"version": "v5", "created": "Tue, 20 Oct 2015 17:58:47 GMT"}, {"version": "v6", "created": "Tue, 27 Oct 2015 01:08:18 GMT"}, {"version": "v7", "created": "Wed, 2 Nov 2016 21:26:47 GMT"}, {"version": "v8", "created": "Fri, 4 Nov 2016 02:18:51 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Panagiotis", "", "", "Panos"], ["Toulis", "", ""], ["Parkes", "David C.", ""]]}, {"id": "1501.02458", "submitter": "Qingzhao Zhang PhD", "authors": "Qingzhao Zhang, Sanguo Zhang, Jin Liu, Jian Huang and Shuangge Ma", "title": "Penalized integrative analysis under the accelerated failure time model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For survival data with high-dimensional covariates, results generated in the\nanalysis of a single dataset are often unsatisfactory because of the small\nsample size. Integrative analysis pools raw data from multiple independent\nstudies with comparable designs, effectively increases sample size, and has\nbetter performance than meta-analysis and single-dataset analysis. In this\nstudy, we conduct integrative analysis of survival data under the accelerated\nfailure time (AFT) model. The sparsity structures of multiple datasets are\ndescribed using the homogeneity and heterogeneity models. For variable\nselection under the homogeneity model, we adopt group penalization approaches.\nFor variable selection under the heterogeneity model, we use composite\npenalization and sparse group penalization approaches. As a major advancement\nfrom the existing studies, the asymptotic selection and estimation properties\nare rigorously established. Simulation study is conducted to compare different\npenalization methods and against alternatives. We also analyze four lung cancer\nprognosis datasets with gene expression measurements.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 13:41:39 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Zhang", "Qingzhao", ""], ["Zhang", "Sanguo", ""], ["Liu", "Jin", ""], ["Huang", "Jian", ""], ["Ma", "Shuangge", ""]]}, {"id": "1501.02467", "submitter": "Justin Yang Mr.", "authors": "Justin J. Yang, Xufei Wang, Pavlos Protopapas, Luke Bornn", "title": "Fast and optimal nonparametric sequential design for astronomical\n  observations", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectral energy distribution (SED) is a relatively easy way for\nastronomers to distinguish between different astronomical objects such as\ngalaxies, black holes, and stellar objects. By comparing the observations from\na source at different frequencies with template models, astronomers are able to\ninfer the type of this observed object. In this paper, we take a Bayesian model\naveraging perspective to learn astronomical objects, employing a Bayesian\nnonparametric approach to accommodate the deviation from convex combinations of\nknown log-SEDs. To effectively use telescope time for observations, we then\nstudy Bayesian nonparametric sequential experimental design without conjugacy,\nin which we use sequential Monte Carlo as an efficient tool to maximize the\nvolume of information stored in the posterior distribution of the parameters of\ninterest. A new technique for performing inferences in log-Gaussian Cox\nprocesses called the Poisson log-normal approximation is also proposed.\nSimulations show the speed, accuracy, and usefulness of our method. While the\nstrategy we propose in this paper is brand new in the astronomy literature, the\ninferential techniques developed apply to more general nonparametric sequential\nexperimental design problems.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 15:50:09 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Yang", "Justin J.", ""], ["Wang", "Xufei", ""], ["Protopapas", "Pavlos", ""], ["Bornn", "Luke", ""]]}, {"id": "1501.02469", "submitter": "Hamed Haselimashhadi", "authors": "Hamed Haselimashhadi", "title": "Multipurpose Lasso", "comments": "Combining with other paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Nowadays, l1 penalized likelihood has absorbed a high amount of consideration\ndue to its simplicity and well developed theoretical properties. This method is\nknown as a reliable method in order to apply in a broad range of applications\nincluding high-dimensional cases. On the other hand, $L_1$ driven methods,\nprecisely lasso dependent regularizations, suffer the loss of sparsity when the\nnumber of observations is too low. In this paper we address a new\ndifferentiable approximation of lasso that can produce the same results as\nlasso and ridge and also can produce smooth results. We prove the theoretical\nproperties of the model as well as its computation complexity. Due to\ndifferentiability, proposed method can be implemented by means of the majority\nof convex optimization methods in literature. That means a higher accuracy in\nsituations where true coefficients are close to zero that is a major issue of\nLARS. Simulation study as well as flexibility of the method show that the\nproposed approach is as reliable as lass and ridge and can be used in both\nsituations.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jan 2015 16:38:15 GMT"}, {"version": "v2", "created": "Thu, 11 Jun 2015 15:58:26 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Haselimashhadi", "Hamed", ""]]}, {"id": "1501.02514", "submitter": "Martin L. Hazelton", "authors": "Martin L. Hazelton", "title": "Network tomography for integer-valued traffic", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS805 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 474-506", "doi": "10.1214/15-AOAS805", "report-no": "IMS-AOAS-AOAS805", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A classic network tomography problem is estimation of properties of the\ndistribution of route traffic volumes based on counts taken on the network\nlinks. We consider inference for a general class of models for integer-valued\ntraffic. Model identifiability is examined. We investigate both maximum\nlikelihood and Bayesian methods of estimation. In practice, these must be\nimplemented using stochastic EM and MCMC approaches. This requires a\nmethodology for sampling latent route flows conditional on the observed link\ncounts. We show that existing algorithms for doing so can fail entirely,\nbecause inflexibility in the choice of sampling directions can leave the\nsampler trapped at a vertex of the convex polytope that describes the feasible\nset of route flows. We prove that so long as the network's link-path incidence\nmatrix is totally unimodular, it is always possible to select a coordinate\nsystem representation of the polytope for which sampling parallel to the axes\nis adequate. This motivates a modified sampler in which the representation of\nthe polytope adapts to provide good mixing behavior. This methodology is\napplied to three road traffic data sets. We conclude with a discussion of the\nramifications of the unimodularity requirements for the routing matrix.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 01:03:07 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 07:16:29 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Hazelton", "Martin L.", ""]]}, {"id": "1501.02627", "submitter": "Oliver Serang", "authors": "Oliver Serang", "title": "A fast numerical method for max-convolution and the application to\n  efficient max-product inference in Bayesian networks", "comments": null, "journal-ref": "Journal of Computational Biology. August 2015, 22(8): 770-783", "doi": "10.1089/cmb.2015.0013", "report-no": null, "categories": "cs.NA math.NA stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observations depending on sums of random variables are common throughout many\nfields; however, no efficient solution is currently known for performing\nmax-product inference on these sums of general discrete distributions\n(max-product inference can be used to obtain maximum a posteriori estimates).\nThe limiting step to max-product inference is the max-convolution problem\n(sometimes presented in log-transformed form and denoted as \"infimal\nconvolution\", \"min-convolution\", or \"convolution on the tropical semiring\"),\nfor which no O(k log(k)) method is currently known. Here I present a O(k\nlog(k)) numerical method for estimating the max-convolution of two nonnegative\nvectors (e.g., two probability mass functions), where k is the length of the\nlarger vector. This numerical max-convolution method is then demonstrated by\nperforming fast max-product inference on a convolution tree, a data structure\nfor performing fast inference given information on the sum of n discrete random\nvariables in O(n k log(n k) log(n) ) steps (where each random variable has an\narbitrary prior distribution on k contiguous possible states). The numerical\nmax-convolution method can be applied to specialized classes of hidden Markov\nmodels to reduce the runtime of computing the Viterbi path from n k^2 to n k\nlog(k), and has potential application to the all-pairs shortest paths problem.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 12:57:01 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 01:49:25 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Serang", "Oliver", ""]]}, {"id": "1501.02663", "submitter": "Peiman Asadi", "authors": "Peiman Asadi, Anthony C. Davison, Sebastian Engelke", "title": "Extremes on river networks", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS863 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 4, 2023-2050", "doi": "10.1214/15-AOAS863", "report-no": "IMS-AOAS-AOAS863", "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable processes are the natural extension of the classical extreme-value\ndistributions to the functional setting, and they are increasingly widely used\nto estimate probabilities of complex extreme events. In this paper we broaden\nthem from the usual situation in which dependence varies according to functions\nof Euclidean distance to situations in which extreme river discharges at two\nlocations on a river network may be dependent because the locations are\nflow-connected or because of common meteorological events. In the former case\ndependence depends on river distance, and in the second it depends on the\nhydrological distance between the locations, either of which may be very\ndifferent from their Euclidean distance. Inference for the model parameters is\nperformed using a multivariate threshold likelihood, which is shown by\nsimulation to work well. The ideas are illustrated with data from the upper\nDanube basin.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 14:37:16 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 14:40:13 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Asadi", "Peiman", ""], ["Davison", "Anthony C.", ""], ["Engelke", "Sebastian", ""]]}, {"id": "1501.02995", "submitter": "Renato J Cintra", "authors": "U. S. Potluri, A. Madanayake, R. J. Cintra, F. M. Bayer, S.\n  Kulasekera, A. Edirisuriya", "title": "Improved 8-point Approximate DCT for Image and Video Compression\n  Requiring Only 14 Additions", "comments": "30 pages, 7 figures, 5 tables", "journal-ref": "Circuits and Systems I: Regular Papers, IEEE Transactions on,\n  Volume 61, Issue 6, June 2014, 1727--1740", "doi": "10.1109/TCSI.2013.2295022", "report-no": null, "categories": "cs.MM cs.CV cs.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video processing systems such as HEVC requiring low energy consumption needed\nfor the multimedia market has lead to extensive development in fast algorithms\nfor the efficient approximation of 2-D DCT transforms. The DCT is employed in a\nmultitude of compression standards due to its remarkable energy compaction\nproperties. Multiplier-free approximate DCT transforms have been proposed that\noffer superior compression performance at very low circuit complexity. Such\napproximations can be realized in digital VLSI hardware using additions and\nsubtractions only, leading to significant reductions in chip area and power\nconsumption compared to conventional DCTs and integer transforms. In this\npaper, we introduce a novel 8-point DCT approximation that requires only 14\naddition operations and no multiplications. The proposed transform possesses\nlow computational complexity and is compared to state-of-the-art DCT\napproximations in terms of both algorithm complexity and peak signal-to-noise\nratio. The proposed DCT approximation is a candidate for reconfigurable video\nstandards such as HEVC. The proposed transform and several other DCT\napproximations are mapped to systolic-array digital architectures and\nphysically realized as digital prototype circuits using FPGA technology and\nmapped to 45 nm CMOS technology.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 13:26:40 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Potluri", "U. S.", ""], ["Madanayake", "A.", ""], ["Cintra", "R. J.", ""], ["Bayer", "F. M.", ""], ["Kulasekera", "S.", ""], ["Edirisuriya", "A.", ""]]}, {"id": "1501.03221", "submitter": "Wen-Ting Wang", "authors": "Wen-Ting Wang and Hsin-Cheng Huang", "title": "Regularized Principal Component Analysis for Spatial Data", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics (2016)", "doi": "10.1080/10618600.2016.1157483", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many atmospheric and earth sciences, it is of interest to identify\ndominant spatial patterns of variation based on data observed at $p$ locations\nand $n$ time points with the possibility that $p>n$. While principal component\nanalysis (PCA) is commonly applied to find the dominant patterns, the\neigenimages produced from PCA may exhibit patterns that are too noisy to be\nphysically meaningful when $p$ is large relative to $n$. To obtain more precise\nestimates of eigenimages, we propose a regularization approach incorporating\nsmoothness and sparseness of eigenimages, while accounting for their\northogonality. Our method allows data taken at irregularly spaced or sparse\nlocations. In addition, the resulting optimization problem can be solved using\nthe alternating direction method of multipliers, which is easy to implement,\nand applicable to a large spatial dataset. Furthermore, the estimated\neigenfunctions provide a natural basis for representing the underlying spatial\nprocess in a spatial random-effects model, from which spatial covariance\nfunction estimation and spatial prediction can be efficiently performed using a\nregularized fixed-rank kriging method. Finally, the effectiveness of the\nproposed method is demonstrated by several numerical examples\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 00:41:13 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 01:52:24 GMT"}, {"version": "v3", "created": "Wed, 3 Feb 2016 02:34:00 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Wang", "Wen-Ting", ""], ["Huang", "Hsin-Cheng", ""]]}, {"id": "1501.03284", "submitter": "Roberto Fontana", "authors": "Roberto Fontana", "title": "Generalized Minimum Aberration mixed-level orthogonal arrays A general\n  approach based on sequential integer quadratically constrained quadratic\n  programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orthogonal Fractional Factorial Designs and in particular Orthogonal Arrays\nare frequently used in many fields of application, including medicine,\nengineering and agriculture. In this paper we present a methodology and an\nalgorithm to find an orthogonal array, of given size and strength, that\nsatisfies the generalized minimum aberration criterion. The methodology is\nbased on the joint use of polynomial counting functions, complex coding of\nlevels and algorithms for quadratic optimization and puts no restriction on the\nnumber of levels of each factor.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 08:53:20 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Fontana", "Roberto", ""]]}, {"id": "1501.03291", "submitter": "Michael Gutmann", "authors": "Michael U. Gutmann and Jukka Corander", "title": "Bayesian Optimization for Likelihood-Free Inference of Simulator-Based\n  Statistical Models", "comments": "In press with the Journal of Machine Learning Research (JMLR).\n  Accepted August 17, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our paper deals with inferring simulator-based statistical models given some\nobserved data. A simulator-based model is a parametrized mechanism which\nspecifies how data are generated. It is thus also referred to as generative\nmodel. We assume that only a finite number of parameters are of interest and\nallow the generative process to be very general; it may be a noisy nonlinear\ndynamical system with an unrestricted number of hidden variables. This weak\nassumption is useful for devising realistic models but it renders statistical\ninference very difficult. The main challenge is the intractability of the\nlikelihood function. Several likelihood-free inference methods have been\nproposed which share the basic idea of identifying the parameters by finding\nvalues for which the discrepancy between simulated and observed data is small.\nA major obstacle to using these methods is their computational cost. The cost\nis largely due to the need to repeatedly simulate data sets and the lack of\nknowledge about how the parameters affect the discrepancy. We propose a\nstrategy which combines probabilistic modeling of the discrepancy with\noptimization to facilitate likelihood-free inference. The strategy is\nimplemented using Bayesian optimization and is shown to accelerate the\ninference through a reduction in the number of required simulations by several\norders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 09:34:15 GMT"}, {"version": "v2", "created": "Thu, 6 Aug 2015 09:37:28 GMT"}, {"version": "v3", "created": "Thu, 31 Dec 2015 15:19:21 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Gutmann", "Michael U.", ""], ["Corander", "Jukka", ""]]}, {"id": "1501.03326", "submitter": "Heiko Strathmann", "authors": "Heiko Strathmann, Dino Sejdinovic, Mark Girolami", "title": "Unbiased Bayes for Big Data: Paths of Partial Posteriors", "comments": "18 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key quantity of interest in Bayesian inference are expectations of\nfunctions with respect to a posterior distribution. Markov Chain Monte Carlo is\na fundamental tool to consistently compute these expectations via averaging\nsamples drawn from an approximate posterior. However, its feasibility is being\nchallenged in the era of so called Big Data as all data needs to be processed\nin every iteration. Realising that such simulation is an unnecessarily hard\nproblem if the goal is estimation, we construct a computationally scalable\nmethodology that allows unbiased estimation of the required expectations --\nwithout explicit simulation from the full posterior. The scheme's variance is\nfinite by construction and straightforward to control, leading to algorithms\nthat are provably unbiased and naturally arrive at a desired error tolerance.\nThis is achieved at an average computational complexity that is sub-linear in\nthe size of the dataset and its free parameters are easy to tune. We\ndemonstrate the utility and generality of the methodology on a range of common\nstatistical models applied to large-scale benchmark and real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 12:15:14 GMT"}, {"version": "v2", "created": "Mon, 9 Feb 2015 16:21:20 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Strathmann", "Heiko", ""], ["Sejdinovic", "Dino", ""], ["Girolami", "Mark", ""]]}, {"id": "1501.03347", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi, Marius Bartcus, Herv\\'e Glotin", "title": "Dirichlet Process Parsimonious Mixtures for clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parsimonious Gaussian mixture models, which exploit an eigenvalue\ndecomposition of the group covariance matrices of the Gaussian mixture, have\nshown their success in particular in cluster analysis. Their estimation is in\ngeneral performed by maximum likelihood estimation and has also been considered\nfrom a parametric Bayesian prospective. We propose new Dirichlet Process\nParsimonious mixtures (DPPM) which represent a Bayesian nonparametric\nformulation of these parsimonious Gaussian mixture models. The proposed DPPM\nmodels are Bayesian nonparametric parsimonious mixture models that allow to\nsimultaneously infer the model parameters, the optimal number of mixture\ncomponents and the optimal parsimonious mixture structure from the data. We\ndevelop a Gibbs sampling technique for maximum a posteriori (MAP) estimation of\nthe developed DPMM models and provide a Bayesian model selection framework by\nusing Bayes factors. We apply them to cluster simulated data and real data\nsets, and compare them to the standard parsimonious mixture models. The\nobtained results highlight the effectiveness of the proposed nonparametric\nparsimonious mixture models as a good nonparametric alternative for the\nparametric parsimonious models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 13:56:35 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 11:54:17 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Chamroukhi", "Faicel", ""], ["Bartcus", "Marius", ""], ["Glotin", "Herv\u00e9", ""]]}, {"id": "1501.03465", "submitter": "Carlos Martinez Mr.", "authors": "Carlos Alberto Mart\\'inez (1 and 2), Kshitij Khare (2) and Mauricio A.\n  Elzo (1) ((1) Department of Animal Sciences, University of Florida,\n  Gainesville, FL, USA, (2) Department of Statistics, University of Florida,\n  Gainesville, FL, USA)", "title": "On the Bayesness, minimaxity, and admissibility of point estimators of\n  allelic frequencies", "comments": "36 pages, 3 figures", "journal-ref": "Journal of Theoretical Biology 383 (2015) 106-115", "doi": "10.1016/j.jtbi.2015.07.031", "report-no": null, "categories": "q-bio.QM q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, decision theory was used to derive Bayes and minimax decision\nrules to estimate allelic frequencies and to explore their admissibility.\nDecision rules with uniformly smallest risk usually do not exist and one\napproach to solve this problem is to use the Bayes principle and the minimax\nprinciple to find decision rules satisfying some general optimality criterion\nbased on their risk functions. Two cases were considered, the simpler case of\nbiallelic loci and the more complex case of multiallelic loci. For each locus,\nthe sampling model was a multinomial distribution and the prior was a Beta\n(biallelic case) or a Dirichlet (multiallelic case) distribution. Three loss\nfunctions were considered: squared error loss (SEL), Kulback-Leibler loss (KLL)\nand quadratic error loss (QEL). Bayes estimators were derived under these three\nloss functions and were subsequently used to find minimax estimators using\nresults from decision theory. The Bayes estimators obtained from SEL and KLL\nturned out to be the same. Under certain conditions, the Bayes estimator\nderived from QEL led to an admissible minimax estimator (which was also equal\nto the maximum likelihood estimator). The SEL also allowed finding admissible\nminimax estimators. Some estimators had uniformly smaller variance than the MLE\nand under suitable conditions the remaining estimators also satisfied this\nproperty. In addition to their statistical properties, the estimators derived\nhere allow variation in allelic frequencies, which is closer to the reality of\nfinite populations exposed to evolutionary forces.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 20:11:05 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 18:20:45 GMT"}, {"version": "v3", "created": "Wed, 21 Jan 2015 22:48:40 GMT"}, {"version": "v4", "created": "Fri, 23 Jan 2015 01:59:36 GMT"}, {"version": "v5", "created": "Wed, 19 Aug 2015 16:45:59 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Mart\u00ednez", "Carlos Alberto", "", "1 and 2"], ["Khare", "Kshitij", ""], ["Elzo", "Mauricio A.", ""]]}, {"id": "1501.03519", "submitter": "Cristina Mollica", "authors": "Cristina Mollica and Luca Tardella", "title": "Bayesian mixture of Plackett-Luce models for partially ranked data", "comments": "36 pages, 7 figures, 11 tables", "journal-ref": null, "doi": "10.1007/s11336-016-9530-0", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The elicitation of an ordinal judgment on multiple alternatives is often\nrequired in many psychological and behavioral experiments to investigate\npreference/choice orientation of a specific population. The Plackett-Luce model\nis one of the most popular and frequently applied parametric distributions to\nanalyze rankings of a finite set of items. The present work introduces a\nBayesian finite mixture of Plackett-Luce models to account for unobserved\nsample heterogeneity of partially ranked data. We describe an efficient way to\nincorporate the latent group structure in the data augmentation approach and\nthe derivation of existing maximum likelihood procedures as special instances\nof the proposed Bayesian method. Inference can be conducted with the\ncombination of the Expectation-Maximization algorithm for maximum \\textit{a\nposteriori} estimation and the Gibbs sampling iterative procedure. We\nadditionally investigate several Bayesian criteria for selecting the optimal\nmixture configuration and describe diagnostic tools for assessing the fitness\nof ranking distributions conditionally and unconditionally on the number of\nranked items. The utility of the novel Bayesian parametric Plackett-Luce\nmixture for characterizing sample heterogeneity is illustrated with several\napplications to simulated and real preference ranked data. We compare our\nmethod with the frequentist approach and a Bayesian nonparametric mixture model\nboth assuming the Plackett-Luce model as a mixture component. Our analysis on\nreal datasets reveals the importance of an accurate diagnostic check for an\nappropriate in-depth understanding of the heterogenous nature of the partial\nranking data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 21:31:10 GMT"}, {"version": "v2", "created": "Tue, 20 Jan 2015 21:13:18 GMT"}, {"version": "v3", "created": "Fri, 30 Jan 2015 19:32:08 GMT"}, {"version": "v4", "created": "Wed, 5 Oct 2016 08:02:42 GMT"}, {"version": "v5", "created": "Thu, 6 Oct 2016 21:44:22 GMT"}], "update_date": "2016-10-10", "authors_parsed": [["Mollica", "Cristina", ""], ["Tardella", "Luca", ""]]}, {"id": "1501.03571", "submitter": "Qingyuan Zhao", "authors": "Qingyuan Zhao and Daniel Percival", "title": "Entropy balancing is doubly robust", "comments": "23 pages, 6 figures, Journal of Causal Inference 2016", "journal-ref": null, "doi": "10.1515/jci-2016-0010", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariate balance is a conventional key diagnostic for methods used\nestimating causal effects from observational studies. Recently, there is an\nemerging interest in directly incorporating covariate balance in the\nestimation. We study a recently proposed entropy maximization method called\nEntropy Balancing (EB), which exactly matches the covariate moments for the\ndifferent experimental groups in its optimization problem. We show EB is doubly\nrobust with respect to linear outcome regression and logistic propensity score\nregression, and it reaches the asymptotic semiparametric variance bound when\nboth regressions are correctly specified. This is surprising to us because\nthere is no attempt to model the outcome or the treatment assignment in the\noriginal proposal of EB. Our theoretical results and simulations suggest that\nEB is a very appealing alternative to the conventional weighting estimators\nthat estimate the propensity score by maximum likelihood.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 04:49:54 GMT"}, {"version": "v2", "created": "Tue, 4 Aug 2015 04:46:32 GMT"}, {"version": "v3", "created": "Sat, 11 Feb 2017 17:27:37 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Zhao", "Qingyuan", ""], ["Percival", "Daniel", ""]]}, {"id": "1501.03599", "submitter": "Qingzhao Zhang PhD", "authors": "Yuan Huang, Qingzhao Zhang, Sanguo Zhang, Jian Huang and Shuangge Ma", "title": "Promoting Similarity of Sparsity Structures in Integrative Analysis with\n  Penalization", "comments": "This paper has been withdrawn by the author due to a bug in numerical\n  study", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For data with high-dimensional covariates but small to moderate sample sizes,\nthe analysis of single datasets often generates unsatisfactory results. The\nintegrative analysis of multiple independent datasets provides an effective way\nof pooling information and outperforms single-dataset analysis and some\nalternative multi-datasets approaches including meta-analysis. Under certain\nscenarios, multiple datasets are expected to share common important covariates,\nthat is, the multiple models have similarity in sparsity structures. However,\nthe existing methods do not have a mechanism to {\\it promote} the similarity of\nsparsity structures in integrative analysis. In this study, we consider\npenalized variable selection and estimation in integrative analysis. We develop\nan $L_0$-penalty based approach, which is the first to explicitly promote the\nsimilarity of sparsity structures. Computationally it is realized using a\ncoordinate descent algorithm. Theoretically it has the much desired consistency\nproperties. In simulation, it significantly outperforms the competing\nalternative when the models in multiple datasets share common important\ncovariates. It has better or similar performance as the alternative when the\nsparsity structures share no similarity. Thus it provides a \"safe\" choice for\ndata analysis. Applying the proposed method to three lung cancer datasets with\ngene expression measurements leads to models with significantly more similar\nsparsity structures and better prediction performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 08:43:15 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 01:53:11 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Huang", "Yuan", ""], ["Zhang", "Qingzhao", ""], ["Zhang", "Sanguo", ""], ["Huang", "Jian", ""], ["Ma", "Shuangge", ""]]}, {"id": "1501.03731", "submitter": "Yoann Altmann", "authors": "Yoann Altmann and Steve McLaughlin and Alfred Hero", "title": "Robust Linear Spectral Unmixing using Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a Bayesian algorithm for linear spectral unmixing of\nhyperspectral images that accounts for anomalies present in the data. The model\nproposed assumes that the pixel reflectances are linear mixtures of unknown\nendmembers, corrupted by an additional nonlinear term modelling anomalies and\nadditive Gaussian noise. A Markov random field is used for anomaly detection\nbased on the spatial and spectral structures of the anomalies. This allows\noutliers to be identified in particular regions and wavelengths of the data\ncube. A Bayesian algorithm is proposed to estimate the parameters involved in\nthe model yielding a joint linear unmixing and anomaly detection algorithm.\nSimulations conducted with synthetic and real hyperspectral images demonstrate\nthe accuracy of the proposed unmixing and outlier detection strategy for the\nanalysis of hyperspectral images.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 16:24:35 GMT"}, {"version": "v2", "created": "Sat, 3 Oct 2015 10:23:25 GMT"}], "update_date": "2015-10-06", "authors_parsed": [["Altmann", "Yoann", ""], ["McLaughlin", "Steve", ""], ["Hero", "Alfred", ""]]}, {"id": "1501.03856", "submitter": "Jean-Eudes Dazard PhD", "authors": "Jean-Eudes Dazard, Michael Choe, Michael LeBlanc, J. Sunil Rao", "title": "Cross-validation and Peeling Strategies for Survival Bump Hunting using\n  Recursive Peeling Methods", "comments": "Keywords: Exploratory Survival/Risk Analysis, Survival/Risk\n  Estimation & Prediction, Non-Parametric Method, Cross-Validation, Bump\n  Hunting, Rule-Induction Method", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework to build a survival/risk bump hunting model with a\ncensored time-to-event response. Our Survival Bump Hunting (SBH) method is\nbased on a recursive peeling procedure that uses a specific survival peeling\ncriterion derived from non/semi-parametric statistics such as the\nhazards-ratio, the log-rank test or the Nelson-Aalen estimator. To optimize the\ntuning parameter of the model and validate it, we introduce an objective\nfunction based on survival or prediction-error statistics, such as the log-rank\ntest and the concordance error rate. We also describe two alternative\ncross-validation techniques adapted to the joint task of decision-rule making\nby recursive peeling and survival estimation. Numerical analyses show the\nimportance of replicated cross-validation and the differences between criteria\nand techniques in both low and high-dimensional settings. Although several\nnon-parametric survival models exist, none addresses the problem of directly\nidentifying local extrema. We show how SBH efficiently estimates extreme\nsurvival/risk subgroups unlike other models. This provides an insight into the\nbehavior of commonly used models and suggests alternatives to be adopted in\npractice. Finally, our SBH framework was applied to a clinical dataset. In it,\nwe identified subsets of patients characterized by clinical and demographic\ncovariates with a distinct extreme survival outcome, for which tailored medical\ninterventions could be made. An R package `PRIMsrc` is available on CRAN and\nGitHub.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 00:21:49 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 20:40:43 GMT"}, {"version": "v3", "created": "Wed, 28 Jan 2015 02:22:21 GMT"}, {"version": "v4", "created": "Thu, 29 Jan 2015 02:43:37 GMT"}, {"version": "v5", "created": "Mon, 2 Mar 2015 03:00:33 GMT"}, {"version": "v6", "created": "Thu, 2 Apr 2015 02:06:43 GMT"}, {"version": "v7", "created": "Thu, 17 Sep 2015 04:05:15 GMT"}, {"version": "v8", "created": "Fri, 20 Nov 2015 23:34:51 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Dazard", "Jean-Eudes", ""], ["Choe", "Michael", ""], ["LeBlanc", "Michael", ""], ["Rao", "J. Sunil", ""]]}, {"id": "1501.03889", "submitter": "Yuki Kawakubo", "authors": "Yuki Kawakubo, Shonosuke Sugasawa, Tatsuya Kubokawa", "title": "Conditional Akaike information under covariate shift with application to\n  small area estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we consider the problem of selecting explanatory variables of\nfixed effects in linear mixed models under covariate shift, which is when the\nvalues of covariates in the model for prediction differ from those in the model\nfor observed data. We construct a variable selection criterion based on the\nconditional Akaike information introduced by Vaida and Blanchard (2005). We\nfocus especially on covariate shift in small area estimation and demonstrate\nthe usefulness of the proposed criterion. In addition, numerical performance is\ninvestigated through simulations, one of which is a design-based simulation\nusing a real dataset of land prices.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 06:30:39 GMT"}, {"version": "v2", "created": "Fri, 18 Mar 2016 03:08:38 GMT"}, {"version": "v3", "created": "Sat, 5 Nov 2016 04:42:05 GMT"}, {"version": "v4", "created": "Mon, 25 Sep 2017 13:47:27 GMT"}, {"version": "v5", "created": "Sun, 10 Dec 2017 15:15:29 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Kawakubo", "Yuki", ""], ["Sugasawa", "Shonosuke", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1501.04050", "submitter": "Joaquin Ortega", "authors": "Pedro C. Alvarez-Esteban, C. Eu\\'an, J. Ortega", "title": "Time Series Clustering using the Total Variation Distance with\n  Applications in Oceanography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for determining stationary periods for time series of random sea\nwaves is proposed in this work. This is a problem in which changes between\nstationary sea states are usually slow and segmentation procedures based on\nchange-point detection frequently give poor results. The method is based on a\nnew procedure for time series clustering, built on the use of the total\nvariation distance between normalized spectra as a measure of dissimilarity.\nThe oscillatory behavior of the series is thus considered the central\ncharacteristic for classification purposes. The proposed algorithm is compared\nto several other methods which are also based on features extracted from the\noriginal series and the results show that its performance is comparable to the\nbest methods available and in some tests it performs better. This clustering\nmethod may be of independent interest.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 17:01:30 GMT"}, {"version": "v2", "created": "Wed, 21 Jan 2015 11:31:08 GMT"}, {"version": "v3", "created": "Fri, 19 Jun 2015 15:45:33 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Alvarez-Esteban", "Pedro C.", ""], ["Eu\u00e1n", "C.", ""], ["Ortega", "J.", ""]]}, {"id": "1501.04070", "submitter": "Ernest Fokoue", "authors": "Ernest Fokoue and Necla Gunduz", "title": "An Information-Theoretic Alternative to the Cronbach's Alpha Coefficient\n  of Item Reliability", "comments": "8 pages, 2 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an information-theoretic alternative to the popular Cronbach alpha\ncoefficient of reliability. Particularly suitable for contexts in which\ninstruments are scored on a strictly nonnumeric scale, our proposed index is\nbased on functions of the entropy of the distributions of defined on the sample\nspace of responses. Our reliability index tracks the Cronbach alpha coefficient\nuniformly while offering several other advantages discussed in great details in\nthis paper.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 18:01:31 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Fokoue", "Ernest", ""], ["Gunduz", "Necla", ""]]}, {"id": "1501.04308", "submitter": "Enea Giuseppe Bongiorno", "authors": "Enea Bongiorno and Aldo Goia", "title": "Some Insights About the Small Ball Probability Factorization for Hilbert\n  Random Elements", "comments": "27 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Asymptotic factorizations for the small-ball probability (SmBP) of a Hilbert\nvalued random element $X$ are rigorously established and discussed. In\nparticular, given the first $d$ principal components (PCs) and as the radius\n$\\varepsilon$ of the ball tends to zero, the SmBP is asymptotically\nproportional to (a) the joint density of the first $d$ PCs, (b) the volume of\nthe $d$-dimensional ball with radius $\\varepsilon$, and (c) a correction factor\nweighting the use of a truncated version of the process expansion. Moreover,\nunder suitable assumptions on the spectrum of the covariance operator of $X$\nand as $d$ diverges to infinity when $\\varepsilon$ vanishes, some\nsimplifications occur. In particular, the SmBP factorizes asymptotically as the\nproduct of the joint density of the first $d$ PCs and a pure volume parameter.\nAll the provided factorizations allow to define a surrogate intensity of the\nSmBP that, in some cases, leads to a genuine intensity. To operationalize the\nstated results, a non-parametric estimator for the surrogate intensity is\nintroduced and it is proved that the use of estimated PCs, instead of the true\nones, does not affect the rate of convergence. Finally, as an illustration,\nsimulations in controlled frameworks are provided.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 14:48:30 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 08:31:19 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Bongiorno", "Enea", ""], ["Goia", "Aldo", ""]]}, {"id": "1501.04368", "submitter": "Joe Whittaker", "authors": "Joe Whittaker, Florian Martin, Yang Xiang", "title": "Synergy, suppression and immorality: forward differences of the entropy\n  function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional mutual information is important in the selection and\ninterpretation of graphical models. Its empirical version is well known as a\ngeneralised likelihood ratio test and that it may be represented as a\ndifference in entropy. We consider the forward difference expansion of the\nentropy function defined on all subsets of the variables under study. The\nelements of this expansion are invariant to permutation of their suffices and\nrelate higher order mutual informations to lower order ones. The third order\ndifference is expressible as an, apparently assymmetric, difference between a\nmarginal and a conditional mutual information. Its role in the decomposition\nfor explained information provides a technical definition for synergy between\nthree random variables. Positive values occur when two variables provide\nalternative explanations for a third; negative values, termed synergies, occur\nwhen the sum of explained information is greater than the sum of its parts.\nSynergies tend to be infrequent; they connect the seemingly unrelated concepts\nof suppressor variables in regression, on the one hand, and unshielded\ncolliders in Bayes networks (immoralities), on the other. We give novel\ncharacterizations of these phenomena that generalise to categorical variables\nand to higher dimensions. We propose an algorithm for systematically computing\nlow order differences from a given graph. Examples from small scale real-life\nstudies indicate the potential of these techniques for empirical statistical\nanalysis.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 23:53:52 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Whittaker", "Joe", ""], ["Martin", "Florian", ""], ["Xiang", "Yang", ""]]}, {"id": "1501.04419", "submitter": "Petter Arnesen", "authors": "Petter Arnesen and H{\\aa}kon Tjelmeland", "title": "Fully Bayesian binary Markov random field models: Prior specification\n  and posterior simulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible prior model for the parameters of binary Markov random\nfields (MRF) defined on rectangular lattices and with maximal cliques defined\nfrom a template maximal clique. The prior model allows higher-order\ninteractions to be included. We also define a reversible jump Markov chain\nMonte Carlo (RJMCMC) algorithm to sample from the associated posterior\ndistribution. The number of possible parameters for an MRF with for instance k\nx l maximal cliques becomes high even for small values of k and l. To get a\nflexible model which may adapt to the structure of a particular observed image\nwe do not put any absolute restrictions on the parametrisation. Instead we\ndefine a parametric form for the MRF where the parameters have interpretation\nas potentials for the various clique configurations, and limit the effective\nnumber of parameters by assigning apriori discrete probabilities for events\nwhere groups of parameter values are equal. To run our RJMCMC algorithm we have\nto cope with the computationally intractable normalising constant of MRFs. For\nthis we adopt a previously defined approximation for binary MRFs, but we also\nbriefly discuss other alternatives. We demonstrate the flexibility of our prior\nformulation with simulated and real data examples.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 08:44:44 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Arnesen", "Petter", ""], ["Tjelmeland", "H\u00e5kon", ""]]}, {"id": "1501.04739", "submitter": "Zaid Sawlan", "authors": "Fabrizio Ruggeri, Zaid Sawlan, Marco Scavino and Raul Tempone", "title": "A hierarchical Bayesian setting for an inverse problem in linear\n  parabolic PDEs with noisy boundary conditions", "comments": "30 pages, submitted, January 2015", "journal-ref": "F. Ruggeri, Z. Sawlan, M. Scavino, R. Tempone, A hierarchical\n  Bayesian setting for an inverse problem in linear parabolic PDEs with noisy\n  boundary conditions, Bayesian Analysis 12 (2) (2017) 407--433", "doi": "10.1214/16-BA1007", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we develop a Bayesian setting to infer unknown parameters in\ninitial-boundary value problems related to linear parabolic partial\ndifferential equations. We realistically assume that the boundary data are\nnoisy, for a given prescribed initial condition. We show how to derive the\njoint likelihood function for the forward problem, given some measurements of\nthe solution field subject to Gaussian noise. Given Gaussian priors for the\ntime-dependent Dirichlet boundary values, we analytically marginalize the joint\nlikelihood using the linearity of the equation. Our hierarchical Bayesian\napproach is fully implemented in an example that involves the heat equation. In\nthis example, the thermal diffusivity is the unknown parameter. We assume that\nthe thermal diffusivity parameter can be modeled a priori through a lognormal\nrandom variable or by means of a space-dependent stationary lognormal random\nfield. Synthetic data are used to test the inference. We exploit the behavior\nof the non-normalized log posterior distribution of the thermal diffusivity.\nThen, we use the Laplace method to obtain an approximated Gaussian posterior\nand therefore avoid costly Markov Chain Monte Carlo computations. Expected\ninformation gains and predictive posterior densities for observable quantities\nare numerically estimated using Laplace approximation for different\nexperimental setups.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 08:56:50 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 08:19:07 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Ruggeri", "Fabrizio", ""], ["Sawlan", "Zaid", ""], ["Scavino", "Marco", ""], ["Tempone", "Raul", ""]]}, {"id": "1501.04755", "submitter": "Valeria Vitelli", "authors": "Davide Floriello and Valeria Vitelli", "title": "Sparse Clustering of Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of clustering functional data while jointly selecting\nthe most relevant features for classification. This problem has never been\ntackled before in the functional data context, and it requires a proper\ndefinition of the concept of sparsity for functional data. Functional sparse\nclustering is here analytically defined as a variational problem with a hard\nthresholding constraint ensuring the sparsity of the solution. First, a unique\nsolution to sparse clustering with hard thresholding in finite dimensions is\nproved to exist. Then, the infinite dimensional generalization is given and\nproved to have a unique solution. Both the multivariate and the functional\nversion of sparse clustering with hard thresholding exhibits improvements on\nother standard and sparse clustering strategies on simulated data. A real\nfunctional data application is also shown.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 10:25:31 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Floriello", "Davide", ""], ["Vitelli", "Valeria", ""]]}, {"id": "1501.04933", "submitter": "Jing Lei", "authors": "Kehui Chen and Jing Lei", "title": "Localized Functional Principal Component Analysis", "comments": "30 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose localized functional principal component analysis (LFPCA), looking\nfor orthogonal basis functions with localized support regions that explain most\nof the variability of a random process. The LFPCA is formulated as a convex\noptimization problem through a novel Deflated Fantope Localization method and\nis implemented through an efficient algorithm to obtain the global optimum. We\nprove that the proposed LFPCA converges to the original FPCA when the tuning\nparameters are chosen appropriately. Simulation shows that the proposed LFPCA\nwith tuning parameters chosen by cross validation can almost perfectly recover\nthe true eigenfunctions and significantly improve the estimation accuracy when\nthe eigenfunctions are truly supported on some subdomains. In the scenario that\nthe original eigenfunctions are not localized, the proposed LFPCA also serves\nas a nice tool in finding orthogonal basis functions that balance between\ninterpretability and the capability of explaining variability of the data. The\nanalyses of a country mortality data and a growth curve data reveal interesting\nfeatures that cannot be found by standard FPCA methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 19:55:34 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Chen", "Kehui", ""], ["Lei", "Jing", ""]]}, {"id": "1501.05121", "submitter": "Li Yin", "authors": "Xiaoqin Wang, Weimin Ye and Li Yin", "title": "Point and interval estimation of exposure effects and interaction\n  between the exposures based on logistic model for observational studies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies with dichotomous outcome of a population,\nresearchers need to present the effects of exposures and interaction between\nthe exposures jointly in order to learn the relationship between the exposure\neffects and the interaction. In this article we study point and interval\nestimation of exposure effects and the interaction based on logistic model,\nwhere the exposure effects are measured by risk differences while the\ninteraction is measured by difference between risk differences. Using\napproximate normal distribution of the maximum-likelihood (ML) estimate of the\nmodel parameters, we obtain approximate non-normal distribution of the ML\nestimate of the exposure effects and the interaction. Using the obtained\ndistribution, we obtain point estimate and confidence region of (exposure\neffect, interaction) as well as point estimate and confidence interval of the\ninteraction when the ML estimate of an exposure effect falls into specified\nrange. Our maximum-likelihood-based approach provides a simple but reliable\nmethod of interval estimation of exposure effects and the interaction.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 10:53:35 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Wang", "Xiaoqin", ""], ["Ye", "Weimin", ""], ["Yin", "Li", ""]]}, {"id": "1501.05215", "submitter": "Peter Thwaites", "authors": "Peter A. Thwaites and Jim Q. Smith", "title": "A Separation Theorem for Chain Event Graphs", "comments": "39 pages, 10 figures. Submitted to Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Networks (BNs) are popular graphical models for the representation\nof statistical problems embodying dependence relationships between a number of\nvariables. Much of this popularity is due to the d-separation theorem of Pearl\nand Lauritzen, which allows an analyst to identify the conditional independence\nstatements that a model of the problem embodies using only the topology of the\ngraph. However for many problems the complete model dependence structure cannot\nbe depicted by a BN. The Chain Event Graph (CEG) was introduced for these types\nof problem. In this paper we introduce a separation theorem for CEGs, analogous\nto the d-separation theorem for BNs, which likewise allows an analyst to\nidentify the conditional independence structure of their model from the\ntopology of the graph.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 16:14:27 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Thwaites", "Peter A.", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1501.05225", "submitter": "Vered Madar", "authors": "Vered Madar and Sandra Batista", "title": "A more practical approach for the Benjamini-Hochberg FDR controlling\n  procedure for huge-scale testing problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a common problem in large-scale data analysis, and especially the\nfield of genetics, the huge-scale testing problem, where millions to billions\nof hypotheses are tested together creating a computational challenge to perform\nmultiple hypotheses testing procedures. As a solution we propose an alternative\nalgorithm to the well used Linear Step Up procedure of Benjamini and Hochberg\n(1995). Our algorithm requires linear time and does not require any p-value\nordering. It permits separating huge-scale testing problems arbitrarily into\ncomputationally feasible sets or chunks. Results from the chunks are combined\nby our algorithm to produce the same results as the controlling procedure on\nthe entire set of tests, thus controlling the global false discovery rate even\nwhen p-values are arbitrarily divided. The practical memory usage may also be\ndetermined arbitrarily by the size of available memory.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 16:48:10 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Madar", "Vered", ""], ["Batista", "Sandra", ""]]}, {"id": "1501.05356", "submitter": "Dina Ramadan", "authors": "M. A. El-Damcese, Dina. A. Ramadan", "title": "On the construction of bivariate linear exponential distribution with\n  FGM family", "comments": "25,3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we propose a Farlie-Gumbel-Morgenstern (FGM) family of\nbivariate linear exponential distributions generated from given marginal's.\nTherefore, properties of FGM are analogous to properties of bivariate\ndistributions. We study some important statistical properties and results for\nthe new distribution.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 00:11:57 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["El-Damcese", "M. A.", ""], ["Ramadan", "Dina. A.", ""]]}, {"id": "1501.05427", "submitter": "Maurizio Filippone", "authors": "Maurizio Filippone and Raphael Engler", "title": "Enabling scalable stochastic gradient-based inference for Gaussian\n  processes by employing the Unbiased LInear System SolvEr (ULISSE)", "comments": "10 pages - paper accepted at ICML 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications of Gaussian processes where quantification of uncertainty is\nof primary interest, it is necessary to accurately characterize the posterior\ndistribution over covariance parameters. This paper proposes an adaptation of\nthe Stochastic Gradient Langevin Dynamics algorithm to draw samples from the\nposterior distribution over covariance parameters with negligible bias and\nwithout the need to compute the marginal likelihood. In Gaussian process\nregression, this has the enormous advantage that stochastic gradients can be\ncomputed by solving linear systems only. A novel unbiased linear systems solver\nbased on parallelizable covariance matrix-vector products is developed to\naccelerate the unbiased estimation of gradients. The results demonstrate the\npossibility to enable scalable and exact (in a Monte Carlo sense)\nquantification of uncertainty in Gaussian processes without imposing any\nspecial structure on the covariance or reducing the number of input vectors.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 08:48:42 GMT"}, {"version": "v2", "created": "Fri, 6 Feb 2015 07:59:49 GMT"}, {"version": "v3", "created": "Mon, 18 May 2015 09:21:57 GMT"}, {"version": "v4", "created": "Thu, 3 Sep 2015 06:43:32 GMT"}], "update_date": "2015-09-04", "authors_parsed": [["Filippone", "Maurizio", ""], ["Engler", "Raphael", ""]]}, {"id": "1501.05447", "submitter": "Nial Friel", "authors": "N. Friel, J.P. McKeone, C.J. Oates and A.N. Pettitt", "title": "Investigation of the widely applicable Bayesian information criterion", "comments": "To appear in Statistics and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely applicable Bayesian information criterion (WBIC) is a simple and\nfast approximation to the model evidence that has received little practical\nconsideration. WBIC uses the fact that the log evidence can be written as an\nexpectation, with respect to a powered posterior proportional to the likelihood\nraised to a power $t^*\\in{(0,1)}$, of the log deviance. Finding this\ntemperature value $t^*$ is generally an intractable problem. We find that for a\nparticular tractable statistical model that the mean squared error of an\noptimally-tuned version of WBIC with correct temperature $t^*$ is lower than an\noptimally-tuned version of thermodynamic integration (power posteriors).\nHowever in practice WBIC uses the a canonical choice of $t=1/\\log(n)$. Here we\ninvestigate the performance of WBIC in practice, for a range of statistical\nmodels, both regular models and singular models such as latent variable models\nor those with a hierarchical structure for which BIC cannot provide an adequate\nsolution. Our findings are that, generally WBIC performs adequately when one\nuses informative priors, but it can systematically overestimate the evidence,\nparticularly for small sample sizes.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 10:30:23 GMT"}, {"version": "v2", "created": "Thu, 28 Apr 2016 09:13:24 GMT"}], "update_date": "2016-04-29", "authors_parsed": [["Friel", "N.", ""], ["McKeone", "J. P.", ""], ["Oates", "C. J.", ""], ["Pettitt", "A. N.", ""]]}, {"id": "1501.05788", "submitter": "Peng Yin", "authors": "Peng Yin and Jian Qing Shi", "title": "Simulation-based Sensitivity Analysis for Non-ignorable Missing Data", "comments": "18 pages, two additional examples at Appendix. Novel approach for\n  sensitivity analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Sensitivity analysis is popular in dealing with missing data problems\nparticularly for non-ignorable missingness. It analyses how sensitively the\nconclusions may depend on assumptions about missing data e.g. missing data\nmechanism (MDM). We called models under certain assumptions sensitivity models.\nTo make sensitivity analysis useful in practice we need to define some simple\nand interpretable statistical quantities to assess the sensitivity models.\nHowever, the assessment is difficult when the missing data mechanism is missing\nnot at random (MNAR). We propose a novel approach in this paper on attempting\nto investigate those assumptions based on the nearest-neighbour (KNN) distances\nof simulated datasets from various MNAR models. The method is generic and it\nhas been applied successfully to several specific models in this paper\nincluding meta-analysis model with publication bias, analysis of incomplete\nlongitudinal data and regression analysis with non-ignorable missing\ncovariates.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 12:56:21 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Yin", "Peng", ""], ["Shi", "Jian Qing", ""]]}, {"id": "1501.05961", "submitter": "Nicholas Henderson", "authors": "Nicholas C. Henderson and Paul J. Rathouz", "title": "AR(1) Latent Class Models for Longitudinal Count Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a variety of applications involving longitudinal or repeated-measurements\ndata, it is desired to uncover natural groupings or clusters which exist among\nstudy subjects. Motivated by the need to recover longitudinal trajectories of\nconduct problems in the field of developmental psychopathology, we propose a\nmethod to address this goal when the data in question are counts. We assume\nthat the subject-specific observations are generated from a first-order\nautoregressive process which is appropriate for counts. A key advantage of our\napproach is that the marginal distribution of the response can be expressed in\nclosed form, circumventing common computational issues associated with random\neffects models. To further improve computational efficiency, we propose a\nquasi-EM procedure for estimating the model parameters where, within each EM\niteration, the maximization step is approximated by solving an appropriately\nchosen set of estimating equations. Additionally, we introduce a novel method\nto express the degree to which both the underlying data and the fitted model\nare able to correctly assign subjects to their (unknown) latent classes. We\nexplore the effectiveness of our procedures through simulations based on a\nfour-class model, placing a special emphasis on posterior classification.\nFinally, we analyze data and recover trajectories of conduct problems in an\nimportant nationally representative sample.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 21:23:31 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Henderson", "Nicholas C.", ""], ["Rathouz", "Paul J.", ""]]}, {"id": "1501.05975", "submitter": "Rohmatul Fajriyah", "authors": "Rohmatul Fajriyah", "title": "An alternatif test to the two independent samples t test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we proposed the alternative test to the two independent and\nnormally distributed samples t test based on the cross variance concept. We\npresent the simulation results of the power and the error rate of the special\ncase of the cross variance which is when the variances of the two samples are\nhomogeneous and the t tests.\n  The simulation results show that the special case of the cross variance test\nhas the power and the error type I rate equal to the $t$ test. This result\nsuggests that the proposed test could be used as an alternative to detect\nwhether there is difference between means of the two independent normally\ndistributed samples. We give some example comparative case studies of the\nspecial case of the cross variance and the t tests.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 22:53:10 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Fajriyah", "Rohmatul", ""]]}, {"id": "1501.06155", "submitter": "Laszlo Martinek", "authors": "Laszlo Martinek and Miklos Arato and Miklos Malyusz", "title": "Comparison of Stochastic Claims Reserving Models in Insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The appropriate estimation of incurred but not reported (IBNR) reserves is\ntraditionally one of the most important task of actuaries working in casualty\nand property insurance. As certain claims are reported many years after their\noccurrence, the amount and appropriateness of the reserves has a strong effect\non the results of the institution. In recent years, stochastic reserving\nmethods had become increasingly widespread. The topic has a wide actuarial\nliterature, describing development models and evaluation techniques. We only\nmention the summarizing article \\cite{EV2002} and book \\cite{MV2008}.\n  The cardinal aim of our present work is the comparison of appropriateness of\nseveral stochastic estimation methods, supposing different distributional\ndevelopment models. We view stochastic reserving as a stochastic forecast, so\nusing the comparison techniques developed for stochastic forecasts, namely\nscores, is natural, see \\cite{GT2007}, for instance. We expect that in some\ncases this attitude will be more informative than the classical mean square\nerror of prediction measure.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 13:39:42 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Martinek", "Laszlo", ""], ["Arato", "Miklos", ""], ["Malyusz", "Miklos", ""]]}, {"id": "1501.06314", "submitter": "Matthieu Marbac", "authors": "Marbac Matthieu and Sedki Mohammed", "title": "Variable selection for model-based clustering using the integrated\n  complete-data likelihood", "comments": "submitted to Statistics and Computing", "journal-ref": null, "doi": "10.1007/s11222-016-9670-1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection in cluster analysis is important yet challenging. It can\nbe achieved by regularization methods, which realize a trade-off between the\nclustering accuracy and the number of selected variables by using a lasso-type\npenalty. However, the calibration of the penalty term can suffer from\ncriticisms. Model selection methods are an efficient alternative, yet they\nrequire a difficult optimization of an information criterion which involves\ncombinatorial problems. First, most of these optimization algorithms are based\non a suboptimal procedure (e.g. stepwise method). Second, the algorithms are\noften greedy because they need multiple calls of EM algorithms. Here we propose\nto use a new information criterion based on the integrated complete-data\nlikelihood. It does not require any estimate and its maximization is simple and\ncomputationally efficient. The original contribution of our approach is to\nperform the model selection without requiring any parameter estimation. Then,\nparameter inference is needed only for the unique selected model. This approach\nis used for the variable selection of a Gaussian mixture model with conditional\nindependence assumption. The numerical experiments on simulated and benchmark\ndatasets show that the proposed method often outperforms two classical\napproaches for variable selection.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 10:26:56 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 08:42:45 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Matthieu", "Marbac", ""], ["Mohammed", "Sedki", ""]]}, {"id": "1501.06332", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa, Kunio Shimizu and Shogo Kato", "title": "A flexible family of distributions on the cylinder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible family of distributions, generalized $t$-distributions,\non the cylinder which is obtained as a conditional distribution of a trivariate\n$t$ distribution. The new distribution has unimodality or bimodality, symmetry\nor asymmetry, depending on the values of parameters and flexibly fits the\ncylindrical data. The circular marginal of this distribution is distributed as\na generalized $t$-distribution on the circle. Some other properties are also\ninvestigated. The proposed distribution is applied to the real cylindrical\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 11:07:38 GMT"}, {"version": "v2", "created": "Fri, 17 Jul 2015 04:45:34 GMT"}], "update_date": "2015-07-20", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Shimizu", "Kunio", ""], ["Kato", "Shogo", ""]]}, {"id": "1501.06344", "submitter": "Petter Arnesen", "authors": "Petter Arnesen and H{\\aa}kon Tjelmeland", "title": "Prior specification of neighbourhood and interaction structure in binary\n  Markov random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a prior distribution for the clique set and\ndependence structure of binary Markov random fields (MRFs). In the formulation\nwe allow both pairwise and higher order interactions. We construct the prior by\nfirst defining a prior for the neighbourhood system of the MRF, and conditioned\non this we define a prior for the appearance of higher order interactions.\nFinally, for the parameter values we adopt a prior that allows for parameter\nvalues to equal, and in this way we reduce the effective number of free\nparameters. To sample from a resulting posterior distribution conditioned on an\nobserved scene we construct a reversible jump Markov chain Monte Carlo (RJMCMC)\nalgorithm. We circumvent evaluations of the intractable normalising constant of\nthe MRF when running this algorithm by adopting a previously defined\napproximate auxiliary variable algorithm. We demonstrate the usefulness of our\nprior in two simulation examples and one real data example.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 11:40:38 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Arnesen", "Petter", ""], ["Tjelmeland", "H\u00e5kon", ""]]}, {"id": "1501.06396", "submitter": "Ben Teng", "authors": "Ben Teng, Can Yang, Jiming Liu, Zhipeng Cai and Xiang Wan", "title": "Exploring the genetic patterns of complex diseases via the integrative\n  genome-wide approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivation: Genome-wide association studies (GWASs), which assay more than a\nmillion single nucleotide polymorphisms (SNPs) in thousands of individuals,\nhave been widely used to identify genetic risk variants for complex diseases.\nHowever, most of the variants that have been identified contribute relatively\nsmall increments of risk and only explain a small portion of the genetic\nvariation in complex diseases. This is the so-called missing heritability\nproblem. Evidence has indicated that many complex diseases are genetically\nrelated, meaning these diseases share common genetic risk variants. Therefore,\nexploring the genetic correlations across multiple related studies could be a\npromising strategy for removing spurious associations and identifying\nunderlying genetic risk variants, and thereby uncovering the mystery of missing\nheritability in complex diseases. Results: We present a general and robust\nmethod to identify genetic patterns from multiple large-scale genomic datasets.\nWe treat the summary statistics as a matrix and demonstrate that genetic\npatterns will form a low-rank matrix plus a sparse component. Hence, we\nformulate the problem as a matrix recovering problem, where we aim to discover\nrisk variants shared by multiple diseases/traits and those for each individual\ndisease/trait. We propose a convex formulation for matrix recovery and an\nefficient algorithm to solve the problem. We demonstrate the advantages of our\nmethod using both synthesized datasets and real datasets. The experimental\nresults show that our method can successfully reconstruct both the shared and\nthe individual genetic patterns from summary statistics and achieve better\nperformance compared with alternative methods under a wide range of scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 13:59:23 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Teng", "Ben", ""], ["Yang", "Can", ""], ["Liu", "Jiming", ""], ["Cai", "Zhipeng", ""], ["Wan", "Xiang", ""]]}, {"id": "1501.06444", "submitter": "Sophie Donnet", "authors": "Pierre Barbillon, Sophie Donnet, Emmanuel Lazega and Avner Bar-Hen", "title": "Stochastic Block Models for Multiplex networks: an application to\n  networks of researchers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling relations between individuals is a classical question in social\nsciences and clustering individuals according to the observed patterns of\ninteractions allows to uncover a latent structure in the data. Stochastic block\nmodel (SBM) is a popular approach for grouping the individuals with respect to\ntheir social comportment. When several relationships of various types can occur\njointly between the individuals, the data are represented by multiplex networks\nwhere more than one edge can exist between the nodes. In this paper, we extend\nthe SBM to multiplex networks in order to obtain a clustering based on more\nthan one kind of relationship. We propose to estimate the parameters --such as\nthe marginal probabilities of assignment to groups (blocks) and the matrix of\nprobabilities of connections between groups-- through a variational\nExpectation-Maximization procedure. Consistency of the estimates as well as\nstatistical properties of the model are obtained. The number of groups is\nchosen thanks to the Integrated Completed Likelihood criteria, a penalized\nlikelihood criterion. Multiplex Stochastic Block Model arises in many\nsituations but our applied example is motivated by a network of French cancer\nresearchers. The two possible links (edges) between researchers are a direct\nconnection or a connection through their labs. Our results show strong\ninteractions between these two kinds of connections and the groups that are\nobtained are discussed to emphasize the common features of researchers grouped\ntogether.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 15:27:56 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Barbillon", "Pierre", ""], ["Donnet", "Sophie", ""], ["Lazega", "Emmanuel", ""], ["Bar-Hen", "Avner", ""]]}, {"id": "1501.06530", "submitter": "David Bardos", "authors": "David C. Bardos, Gurutzeta Guillera-Arroita and Brendan A. Wintle", "title": "Covariate influence in spatially autocorrelated occupancy and abundance\n  data", "comments": "References updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The autologistic model and related auto-models, commonly applied as\nautocovariate regression, offer distinct advantages for analysing spatially\nautocorrelated ecological data. However, comparative studies by Carl and K\\\"uhn\n(Ecol. Model., 2007, 207, 159), Dormann (Ecol. Model., 2007, 207, 234), Dormann\net al. (Ecography, 2007, 30, 609) and Beale et al. (Ecol. Lett., 2010, 13, 246)\nconcluded that autocovariate regression yields anomalous covariate parameter\nestimates. The last three studies were based on erroneous numerical evidence,\ndue to violation of conditions (Besag, J. R. Stat. Soc., Ser. B, 1974, 36, 192)\nfor auto-model validity. Here we show that after correcting these technical\nerrors, a more fundamental conceptual error remains: the comparative studies\nare founded on a mathematically incorrect notion of bias, involving direct\ncomparison of parameter estimates across models differing in mathematical\nstructure. We develop a set of simulation-based measures of covariate influence\nthat are directly comparable across models and apply them to examples from the\nabovementioned studies. We find that in these cases, the effect of auto-model\nparameters is similar to (and consistent with) corresponding linear model\neffects, due to a phenomenon within auto-models that we refer to as \"covariate\namplification\". Thus, simple comparison of parameter magnitudes between\nstructurally different models can be highly misleading. We demonstrate that the\nrecent critique of auto-models is entirely unfounded. Correctly applied and\ninterpreted, autocovariate regression provides a practical approach to\ninference for spatially autocorrelated species distribution or abundance data,\nwhile overcoming well-known limitations of generalized linear models.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 19:21:43 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 17:00:12 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Bardos", "David C.", ""], ["Guillera-Arroita", "Gurutzeta", ""], ["Wintle", "Brendan A.", ""]]}, {"id": "1501.06943", "submitter": "Robin Pemantle", "authors": "Ville Satop\\\"a\\\"a and Robin Pemantle and Lyle Ungar", "title": "Combining Probability Forecasts and Understanding Probability\n  Extremizing through Information Diversity", "comments": "This paper has been withdrawn because it was meant to be a revision\n  to arXiv:1406.2148, not an independent submission. This was discovered on 27\n  May, 2015, when preparing a replacement version to arXiv:1406.2148. This\n  replacement will supersede both that version and the one withdrawn here", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomness in scientific estimation is generally assumed to arise from\nunmeasured or uncontrolled factors. However, when combining subjective\nprobability estimates, heterogeneity stemming from people's cognitive or\ninformation diversity is often more important than measurement noise. This\npaper presents a novel framework that uses partially overlapping information\nsources. A specific model is proposed within that framework and applied to the\ntask of aggregating the probabilities given by a group of forecasters who\npredict whether an event will occur or not. Our model describes the\ndistribution of information across forecasters in terms of easily interpretable\nparameters and shows how the optimal amount of extremizing of the average\nprobability forecast (shifting it closer to its nearest extreme) varies as a\nfunction of the forecasters' information overlap. Our model thus gives a more\nprincipled understanding of the historically ad hoc practice of extremizing\naverage forecasts.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 21:55:03 GMT"}, {"version": "v2", "created": "Wed, 27 May 2015 18:19:21 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Satop\u00e4\u00e4", "Ville", ""], ["Pemantle", "Robin", ""], ["Ungar", "Lyle", ""]]}, {"id": "1501.07000", "submitter": "Max Sommerfeld", "authors": "Max Sommerfeld, Stephen Sain, Armin Schwartzman", "title": "Confidence regions for excursion sets in asymptotically Gaussian random\n  fields, with an application to climate", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to give confidence regions for the excursion set of\na spatial function above a given threshold from repeated noisy observations on\na fine grid of fixed locations. Given an asymptotically Gaussian estimator of\nthe target function, a pair of data-dependent nested excursion sets are\nconstructed that are sub- and super-sets of the true excursion set,\nrespectively, with a desired confidence. Asymptotic coverage probabilities are\ndetermined via a multiplier bootstrap method, not requiring Gaussianity of the\noriginal data nor stationarity or smoothness of the limiting Gaussian field.\nThe method is used to determine regions in North America where the mean summer\nand winter temperatures are expected to increase by mid 21st century by more\nthan 2 degrees Celsius.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 06:31:00 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Sommerfeld", "Max", ""], ["Sain", "Stephen", ""], ["Schwartzman", "Armin", ""]]}, {"id": "1501.07047", "submitter": "Karel Hron", "authors": "Jitka Machalova, Karel Hron, Gianna Serafina Monti", "title": "Preprocessing of centred logratio transformed density functions using\n  smoothing splines", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With large-scale database systems, statistical analysis of data, formed by\nprobability distributions, become an important task in explorative data\nanalysis. Nevertheless, due to specific properties of density functions, their\nproper statistical treatment still represents a challenging task in functional\ndata analysis. Namely, the usual L2 metric does not fully accounts for the\nrelative character of information, carried by density functions; instead, their\ngeometrical features are followed by Bayes spaces of measures. The easiest\npossibility of expressing density functions in L2 space is to use centred\nlogratio transformation, nevertheless, it results in functional data with a\nconstant integral constraint that needs to be taken into account for further\nanalysis. While theoretical background for reasonable analysis of density\nfunctions is already provided comprehensively by Bayes spaces themselves,\npreprocessing issues still need to be developed. The aim of this paper is to\nintroduce optimal smoothing splines for centred logratio transformed density\nfunctions that take all their specific features into account and provide a\nconcise methodology for reasonable preprocessing of raw (discretized)\ndistributional observations. Theoretical developments are illustrated with a\nreal-world data set from official statistics.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 09:54:16 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Machalova", "Jitka", ""], ["Hron", "Karel", ""], ["Monti", "Gianna Serafina", ""]]}, {"id": "1501.07198", "submitter": "Brian Reich", "authors": "Yimin Kao, Brian J Reich, Howard D Bondell", "title": "A nonparametric Bayesian test of dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a new method for the fundamental task of testing\nfor dependence between two groups of variables. The response densities under\nthe null hypothesis of independence and the alternative hypothesis of\ndependence are specified by nonparametric Bayesian models. Under the null\nhypothesis, the joint distribution is modeled by the product of two independent\nDirichlet Process Mixture (DPM) priors; under the alternative, the full joint\ndensity is modeled by a multivariate DPM prior. The test is then based on the\nposterior probability of favoring the alternative hypothesis. The proposed test\nnot only has good performance for testing linear dependence among other popular\nnonparametric tests, but is also preferred to other methods in testing many of\nthe nonlinear dependencies we explored. In the analysis of gene expression\ndata, we compare different methods for testing pairwise dependence between\ngenes. The results show that the proposed test identifies some dependence\nstructures that are not detected by other tests.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 17:10:27 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Kao", "Yimin", ""], ["Reich", "Brian J", ""], ["Bondell", "Howard D", ""]]}, {"id": "1501.07240", "submitter": "Fatimah Alashwali", "authors": "Fatimah Alashwali and John Kent", "title": "The use of a common location measure in the invariant coordinate\n  selection and projection pursuit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Invariant coordinate selection (ICS) and projection pursuit (PP) are two\nmethods that can be used to detect clustering directions in multivariate data\nby optimizing criteria sensitive to non-normality. In particular, ICS finds\nclustering directions using a relative eigen-decomposition of two scatter\nmatrices with different levels of robustness; PP is a one-dimensional variant\nof ICS. Each of the two scatter matrices includes an implicit or explicit\nchoice of location. However, when different measures of location are used, ICS\nand PP can behave counter-intuitively. In this paper we explore this behavior\nin a variety of examples and propose a simple and natural solution: use the\nsame measure of location for both scatter matrices.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 19:11:13 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 13:23:17 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Alashwali", "Fatimah", ""], ["Kent", "John", ""]]}, {"id": "1501.07255", "submitter": "Renato J Cintra", "authors": "M. M. S. Lira, H. M. de Oliveira, R. J. Cintra", "title": "Elliptic-cylindrical Wavelets: The Mathieu Wavelets", "comments": "10 pages, 2 figures", "journal-ref": "IEEE Signal Processing Letters, vol. 11, issue 1, pp. 52-55, 2004", "doi": "10.1109/LSP.2003.819341", "report-no": null, "categories": "stat.ME cs.NA math.NA stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note introduces a new family of wavelets and a multiresolution analysis,\nwhich exploits the relationship between analysing filters and Floquet's\nsolution of Mathieu differential equations. The transfer function of both the\ndetail and the smoothing filter is related to the solution of a Mathieu\nequation of odd characteristic exponent. The number of notches of these filters\ncan be easily designed. Wavelets derived by this method have potential\napplication in the fields of Optics and Electromagnetism.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 20:00:17 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Lira", "M. M. S.", ""], ["de Oliveira", "H. M.", ""], ["Cintra", "R. J.", ""]]}, {"id": "1501.07329", "submitter": "Xing He", "authors": "X. He, Q. Ai, C. Qiu, W. Huang, L. Piao, H. Liu", "title": "A Big Data Architecture Design for Smart Grids Based on Random Matrix\n  Theory", "comments": "13 pages, 14 figures, Accepted by IEEE Trans on Smart Grid", "journal-ref": "IEEE Transactions on Smart Grid (Volume: 8, Issue: 2, March 2017)", "doi": "10.1109/TSG.2015.2445828.", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based analysis tools, built on assumptions and simplifications, are\ndifficult to handle smart grids with data characterized by 4Vs data. This\npaper, using random matrix theory (RMT), motivates data-driven tools to\nperceive the complex grids in highdimension; meanwhile, an architecture with\ndetailed procedures is proposed. In algorithm perspective, the architecture\nperforms a high-dimensional analysis, and compares the findings with RMT\npredictions to conduct anomaly detections. Mean Spectral Radius (MSR), as a\nstatistical indicator, is defined to reflect the correlations of system data in\ndifferent dimensions. In management mode perspective, a group-work mode is\ndiscussed for smart grids operation. This mode breaks through regional\nlimitations for energy flows and data flows, and makes advanced big data\nanalyses possible. For a specific large-scale zone-dividing system with\nmultiple connected utilities, each site, operating under the group-work mode,\nis able to work out the regional MSR only with its own measured/simulated data.\nThe large-scale interconnected system, in this way, is naturally decoupled from\nstatistical parameters perspective, rather than from engineering models\nperspective. Furthermore, a comparative analysis of these distributed MSRs,\neven with imperceptible different raw data, will produce a contour line to\ndetect the event and locate the source. It demonstrates that the architecture\nis compatible with the block calculation only using the regional small\ndatabase; beyond that, this architecture, as a data-driven solution, is\nsensitive to system situation awareness, and practical for real large-scale\ninterconnected systems. Five case studies and their visualizations validate the\ndesigned architecture in various fields of power systems. To our best\nknowledge, this study is the first attempt to apply big data technology into\nsmart grids.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 01:53:34 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 08:20:34 GMT"}, {"version": "v3", "created": "Mon, 15 Jun 2015 02:29:45 GMT"}, {"version": "v4", "created": "Tue, 16 Jun 2015 07:25:59 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["He", "X.", ""], ["Ai", "Q.", ""], ["Qiu", "C.", ""], ["Huang", "W.", ""], ["Piao", "L.", ""], ["Liu", "H.", ""]]}, {"id": "1501.07454", "submitter": "Tore Selland Kleppe", "authors": "Tore Selland Kleppe", "title": "Adaptive step size selection for Hessian-based manifold Langevin\n  samplers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of positive definite metric tensors derived from second derivative\ninformation in the context of the simplified manifold Metropolis adjusted\nLangevin algorithm (MALA) is explored. A new adaptive step length procedure\nthat resolves the shortcomings of such metric tensors in regions where the\nlog-target has near zero curvature in some direction is proposed. The adaptive\nstep length selection also appears to alleviate the need for different tuning\nparameters in transient and stationary regimes that is typical of MALA. The\ncombination of metric tensors derived from second derivative information and\nadaptive step length selection constitute a large step towards developing\nreliable manifold MCMC methods that can be implemented automatically for models\nwith unknown or intractable Fisher information, and even for target\ndistributions that do not admit factorization into prior and likelihood.\nThrough examples of low to moderate dimension, it is shown that proposed\nmethodology performs very well relative to alternative MCMC methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 13:49:54 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 19:03:15 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2015 09:38:01 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Kleppe", "Tore Selland", ""]]}, {"id": "1501.07481", "submitter": "Kristjan Greenewald", "authors": "Kristjan Greenewald and Edmund Zelnio and Alfred O. Hero III", "title": "Kronecker PCA Based Robust SAR STAP", "comments": "Tech report. Shorter version submitted to IEEE AES", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work the detection of moving targets in multiantenna SAR is\nconsidered. As a high resolution radar imaging modality, SAR detects and\nidentifies stationary targets very well, giving it an advantage over classical\nGMTI radars. Moving target detection is more challenging due to the \"burying\"\nof moving targets in the clutter and is often achieved using space-time\nadaptive processing (STAP) (based on learning filters from the spatio-temporal\nclutter covariance) to remove the stationary clutter and enhance the moving\ntargets. In this work, it is noted that in addition to the oft noted low rank\nstructure, the clutter covariance is also naturally in the form of a space vs\ntime Kronecker product with low rank factors. A low-rank KronPCA covariance\nestimation algorithm is proposed to exploit this structure, and a separable\nclutter cancelation filter based on the Kronecker covariance estimate is\nproposed. Together, these provide orders of magnitude reduction in the number\nof training samples required, as well as improved robustness to corruption of\nthe training data, e.g. due to outliers and moving targets. Theoretical\nproperties of the proposed estimation algorithm are derived and the significant\nreductions in training complexity are established under the spherically\ninvariant random vector model (SIRV). Finally, an extension of this approach\nincorporating multipass data (change detection) is presented. Simulation\nresults and experiments using the real Gotcha SAR GMTI challenge dataset are\npresented that confirm the advantages of our approach relative to existing\ntechniques.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 15:33:32 GMT"}, {"version": "v2", "created": "Fri, 30 Jan 2015 18:25:44 GMT"}, {"version": "v3", "created": "Thu, 1 Oct 2015 18:46:09 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Greenewald", "Kristjan", ""], ["Zelnio", "Edmund", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1501.07506", "submitter": "Van Huyen Do", "authors": "Van Huyen Do, Christine Thomas-Agnan and Anne Vanhems", "title": "Accuracy of areal interpolation methods for count data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of several socio-economic data bases originating from\ndifferent administrative sources collected on several different partitions of a\ngeographic zone of interest into administrative units induces the so called\nareal interpolation problem. This problem is that of allocating the data from a\nset of source spatial units to a set of target spatial units. A particular case\nof that problem is the re-allocation to a single target partition which is a\nregular grid. At the European level for example, the EU directive 'INSPIRE', or\nINfrastructure for SPatial InfoRmation, encourages the states to provide\nsocio-economic data on a common grid to facilitate economic studies across\nstates. In the literature, there are three main types of such techniques:\nproportional weighting schemes, smoothing techniques and regression based\ninterpolation. We propose a stochastic model based on Poisson point patterns to\nstudy the statistical accuracy of these techniques for regular grid targets in\nthe case of count data. The error depends on the nature of the target variable\nand its correlation with the auxiliary variable. For simplicity, we restrict\nattention to proportional weighting schemes and Poisson regression based\nmethods. Our conclusion is that there is no technique which always dominates.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 16:47:10 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Do", "Van Huyen", ""], ["Thomas-Agnan", "Christine", ""], ["Vanhems", "Anne", ""]]}, {"id": "1501.07551", "submitter": "Fabio M. Bayer Ph.D", "authors": "F\\'abio M. Bayer and Francisco Cribari-Neto", "title": "Bartlett corrections in beta regression models", "comments": "27 pages, 5 tables, 3 figures", "journal-ref": "Journal of Statistical Planning and Inference, vol 143, 2013", "doi": "10.1016/j.jspi.2012.08.018", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the issue of performing accurate small-sample testing inference\nin beta regression models, which are useful for modeling continuous variates\nthat assume values in $(0,1)$, such as rates and proportions. We derive the\nBartlett correction to the likelihood ratio test statistic and also consider a\nbootstrap Bartlett correction. Using Monte Carlo simulations we compare the\nfinite sample performances of the two corrected tests to that of the standard\nlikelihood ratio test and also to its variant that employs Skovgaard's\nadjustment; the latter is already available in the literature. The numerical\nevidence favors the corrected tests we propose. We also present an empirical\napplication.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 19:15:28 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Bayer", "F\u00e1bio M.", ""], ["Cribari-Neto", "Francisco", ""]]}, {"id": "1501.07622", "submitter": "Caren Hasler", "authors": "Caren Hasler and Yves Till\\'e", "title": "Balanced $k$-nearest neighbor imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to overcome the problem of item nonresponse, random imputation\nmethods are often used because they tend to preserve the distribution of the\nimputed variable. Among the random imputation methods, the random hot-deck has\nthe interesting property of imputing observed values. A new random hot-deck\nimputation method is proposed. The key innovation of this method is that the\nselection of donors is viewed as a sampling problem and uses calibration and\nbalanced sampling. This approach makes it possible to select donors such that\nif the auxiliary variables were imputed, their estimated totals would not\nchange. As a consequence, very accurate and stable totals estimations can be\nobtained. Moreover, the method is based on a nonparametric procedure. Donors\nare selected in neighborhoods of recipients. In this way, the missing value of\na recipient is replaced with an observed value of a similar unit. This new\napproach is very flexible and can greatly improve the quality of estimations.\nAlso, this method is unbiased under very different models and is thus resistant\nto model misspecification. Finally, the new method makes it possible to\nintroduce edit rules while imputing.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 21:45:46 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Hasler", "Caren", ""], ["Till\u00e9", "Yves", ""]]}, {"id": "1501.07815", "submitter": "Xin Zhang", "authors": "Lexin Li and Xin Zhang", "title": "Parsimonious Tensor Response Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at abundant scientific and engineering data with not only high\ndimensionality but also complex structure, we study the regression problem with\na multidimensional array (tensor) response and a vector predictor. Applications\ninclude, among others, comparing tensor images across groups after adjusting\nfor additional covariates, which is of central interest in neuroimaging\nanalysis. We propose parsimonious tensor response regression adopting a\ngeneralized sparsity principle. It models all voxels of the tensor response\njointly, while accounting for the inherent structural information among the\nvoxels. It effectively reduces the number of free parameters, leading to\nfeasible computation and improved interpretation. We achieve model estimation\nthrough a nascent technique called the envelope method, which identifies the\nimmaterial information and focuses the estimation based upon the material\ninformation in the tensor response. We demonstrate that the resulting estimator\nis asymptotically efficient, and it enjoys a competitive finite sample\nperformance. We also illustrate the new method on two real neuroimaging\nstudies.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jan 2015 15:48:05 GMT"}], "update_date": "2015-02-02", "authors_parsed": [["Li", "Lexin", ""], ["Zhang", "Xin", ""]]}]