[{"id": "1701.00029", "submitter": "Jean-Marie Dufour", "authors": "Jean-Marie Dufour and Richard Luger", "title": "Identification-robust moment-based tests for Markov-switching in\n  autoregressive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops tests of the null hypothesis of linearity in the context\nof autoregressive models with Markov-switching means and variances. These tests\nare robust to the identification failures that plague conventional\nlikelihood-based inference methods. The approach exploits the moments of normal\nmixtures implied by the regime-switching process and uses Monte Carlo test\ntechniques to deal with the presence of an autoregressive component in the\nmodel specification. The proposed tests have very respectable power in\ncomparison to the optimal tests for Markov-switching parameters of Carrasco, Hu\nand Ploberger (2014} and they are also quite attractive owing to their\ncomputational simplicity. The new tests are illustrated with an empirical\napplication to an autoregressive model of U.S. output growth.\n", "versions": [{"version": "v1", "created": "Fri, 30 Dec 2016 22:40:21 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Dufour", "Jean-Marie", ""], ["Luger", "Richard", ""]]}, {"id": "1701.00311", "submitter": "Yun Yang", "authors": "Yun Yang and Debdeep Pati", "title": "Bayesian model selection consistency and oracle inequality with\n  intractable marginal likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we investigate large sample properties of model selection\nprocedures in a general Bayesian framework when a closed form expression of the\nmarginal likelihood function is not available or a local asymptotic quadratic\napproximation of the log-likelihood function does not exist. Under appropriate\nidentifiability assumptions on the true model, we provide sufficient conditions\nfor a Bayesian model selection procedure to be consistent and obey the Occam's\nrazor phenomenon, i.e., the probability of selecting the \"smallest\" model that\ncontains the truth tends to one as the sample size goes to infinity. In order\nto show that a Bayesian model selection procedure selects the smallest model\ncontaining the truth, we impose a prior anti-concentration condition, requiring\nthe prior mass assigned by large models to a neighborhood of the truth to be\nsufficiently small. In a more general setting where the strong model\nidentifiability assumption may not hold, we introduce the notion of local\nBayesian complexity and develop oracle inequalities for Bayesian model\nselection procedures. Our Bayesian oracle inequality characterizes a trade-off\nbetween the approximation error and a Bayesian characterization of the local\ncomplexity of the model, illustrating the adaptive nature of averaging-based\nBayesian procedures towards achieving an optimal rate of posterior convergence.\nSpecific applications of the model selection theory are discussed in the\ncontext of high-dimensional nonparametric regression and density regression\nwhere the regression function or the conditional density is assumed to depend\non a fixed subset of predictors. As a result of independent interest, we\npropose a general technique for obtaining upper bounds of certain small ball\nprobability of stationary Gaussian processes.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 03:55:55 GMT"}, {"version": "v2", "created": "Mon, 9 Jan 2017 14:47:23 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Yang", "Yun", ""], ["Pati", "Debdeep", ""]]}, {"id": "1701.00338", "submitter": "Jing Liu", "authors": "Stefan Engblom, Carl Nettelblad, Jing Liu", "title": "Assessing Uncertainties in X-ray Single-particle Three-dimensional\n  reconstructions", "comments": "21 pages", "journal-ref": "Phys. Rev. E 98, 013303 (2018)", "doi": "10.1103/PhysRevE.98.013303", "report-no": null, "categories": "stat.ME cs.CV physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern technology for producing extremely bright and coherent X-ray laser\npulses provides the possibility to acquire a large number of diffraction\npatterns from individual biological nanoparticles, including proteins, viruses,\nand DNA. These two-dimensional diffraction patterns can be practically\nreconstructed and retrieved down to a resolution of a few \\angstrom. In\nprinciple, a sufficiently large collection of diffraction patterns will contain\nthe required information for a full three-dimensional reconstruction of the\nbiomolecule. The computational methodology for this reconstruction task is\nstill under development and highly resolved reconstructions have not yet been\nproduced.\n  We analyze the Expansion-Maximization-Compression scheme, the current state\nof the art approach for this very challenging application, by isolating\ndifferent sources of uncertainty. Through numerical experiments on synthetic\ndata we evaluate their respective impact. We reach conclusions of relevance for\nhandling actual experimental data, as well as pointing out certain improvements\nto the underlying estimation algorithm.\n  We also introduce a practically applicable computational methodology in the\nform of bootstrap procedures for assessing reconstruction uncertainty in the\nreal data case. We evaluate the sharpness of this approach and argue that this\ntype of procedure will be critical in the near future when handling the\nincreasing amount of data.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 08:58:19 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Engblom", "Stefan", ""], ["Nettelblad", "Carl", ""], ["Liu", "Jing", ""]]}, {"id": "1701.00403", "submitter": "Kehinde Olobatuyi Mr.", "authors": "Kehinde Olobatuyi", "title": "A New Class of Generalized Burr III Distribution for Lifetime Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the first time, the Generalized Gamma Burr III (GGBIII) is introduced as\nan important model for problems in several areas such as actuarial sciences,\nmeteorology, economics, finance, environmental studies, reliability, and\ncensored data in survival analysis. A review of some existing gamma families\nhave been presented. It was found that the distributions cannot exhibit\ncomplicated shapes such as unimodal and modified unimodal shapes which are very\ncommon in medical field. The Generalized Gamma Burr III (GGBIII) distribution\nwhich includes the family of Zografos and Balakrishnan as special cases is\nproposed and studied. It is expressed as the linear combination of Burr III\ndistribution and it has a tractable properties. Some mathematical properties of\nthe new distribution including hazard, survival, reverse hazard rate function,\nmoments, moments generating function, mean and median deviations, distribution\nof the order statistics are presented. Maximum likelihood estimation technique\nis used to estimate the model parameters and applications to real datasets in\norder to illustrate the usefulness of the model are presented. Examples and\napplications as well as comparisons of the GGBIII to the existing Gamma-G\nfamilies are given.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 14:27:01 GMT"}], "update_date": "2017-01-03", "authors_parsed": [["Olobatuyi", "Kehinde", ""]]}, {"id": "1701.00505", "submitter": "Pierre-Andr\\'e Maugis", "authors": "P-A. G. Maugis and Carey E. Priebe and S. C. Olhede and P. J. Wolfe", "title": "Statistical inference for network samples using subgraph counts", "comments": "42 pages, 6 figures, 2 tables", "journal-ref": null, "doi": "10.1080/10618600.2020.1736085", "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider that a network is an observation, and a collection of observed\nnetworks forms a sample. In this setting, we provide methods to test whether\nall observations in a network sample are drawn from a specified model. We\nachieve this by deriving, under the null of the graphon model, the joint\nasymptotic properties of average subgraph counts as the number of observed\nnetworks increases but the number of nodes in each network remains finite. In\ndoing so, we do not require that each observed network contains the same number\nof nodes, or is drawn from the same distribution. Our results yield joint\nconfidence regions for subgraph counts, and therefore methods for testing\nwhether the observations in a network sample are drawn from: a specified\ndistribution, a specified model, or from the same model as another network\nsample. We present simulation experiments and an illustrative example on a\nsample of brain networks where we find that highly creative individuals' brains\npresent significantly more short cycles.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jan 2017 19:24:21 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 01:44:11 GMT"}, {"version": "v3", "created": "Tue, 28 May 2019 21:10:24 GMT"}], "update_date": "2020-04-17", "authors_parsed": [["Maugis", "P-A. G.", ""], ["Priebe", "Carey E.", ""], ["Olhede", "S. C.", ""], ["Wolfe", "P. J.", ""]]}, {"id": "1701.00770", "submitter": "Johannes Klepsch", "authors": "Alexander Aue and Johannes Klepsch", "title": "Estimating functional time series by moving average model fitting", "comments": "31 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional time series have become an integral part of both functional data\nand time series analysis. Important contributions to methodology, theory and\napplication for the prediction of future trajectories and the estimation of\nfunctional time series parameters have been made in the recent past. This paper\ncontinues this line of research by proposing a first principled approach to\nestimate invertible functional time series by fitting functional moving average\nprocesses. The idea is to estimate the coefficient operators in a functional\nlinear filter. To do this a functional Innovations Algorithm is utilized as a\nstarting point to estimate the corresponding moving average operators via\nsuitable projections into principal directions. In order to establish\nconsistency of the proposed estimators, asymptotic theory is developed for\nincreasing subspaces of these principal directions. For practical purposes,\nseveral strategies to select the number of principal directions to include in\nthe estimation procedure as well as the choice of order of the functional\nmoving average process are discussed. Their empirical performance is evaluated\nthrough simulations and an application to vehicle traffic data.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 18:29:16 GMT"}], "update_date": "2017-01-04", "authors_parsed": [["Aue", "Alexander", ""], ["Klepsch", "Johannes", ""]]}, {"id": "1701.00845", "submitter": "Thomas Nagler", "authors": "Thomas Nagler, Christian Schellhase, Claudia Czado", "title": "Nonparametric estimation of simplified vine copula models: comparison of\n  methods", "comments": null, "journal-ref": null, "doi": "10.1515/demo-2017-0007", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last decade, simplified vine copula models have been an active area of\nresearch. They build a high dimensional probability density from the product of\nmarginals densities and bivariate copula densities. Besides parametric models,\nseveral approaches to nonparametric estimation of vine copulas have been\nproposed. In this article, we extend these approaches and compare them in an\nextensive simulation study and a real data application. We identify several\nfactors driving the relative performance of the estimators. The most important\none is the strength of dependence. No method was found to be uniformly better\nthan all others. Overall, the kernel estimators performed best, but do worse\nthan penalized B-spline estimators when there is weak dependence and no tail\ndependence.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 21:58:51 GMT"}, {"version": "v2", "created": "Wed, 5 Apr 2017 12:11:39 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Nagler", "Thomas", ""], ["Schellhase", "Christian", ""], ["Czado", "Claudia", ""]]}, {"id": "1701.00856", "submitter": "Daniel Eck", "authors": "Daniel J. Eck and R. Dennis Cook", "title": "Weighted envelope estimation to handle variability in model selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Envelope methodology can provide substantial efficiency gains in multivariate\nstatistical problems, but in some applications the estimation of the envelope\ndimension can induce selection volatility that may mitigate those gains.\nCurrent envelope methodology does not account for the added variance that can\nresult from this selection. In this article, we circumvent dimension selection\nvolatility through the development of a weighted envelope estimator.\nTheoretical justification is given for our estimator and validity of the\nresidual bootstrap for estimating its asymptotic variance is established. A\nsimulation study and an analysis on a real data set illustrate the utility of\nour weighted envelope estimator.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jan 2017 22:43:06 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 13:15:53 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Eck", "Daniel J.", ""], ["Cook", "R. Dennis", ""]]}, {"id": "1701.00902", "submitter": "Wen Yu", "authors": "Zhiliang Ying, Wen Yu, Ziqiang Zhao, Ming Zheng", "title": "Regression analysis of doubly truncated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doubly truncated data are found in astronomy, econometrics and survival\nanalysis literature. They arise when each observation is confined to an\ninterval, i.e., only those which fall within their respective intervals are\nobserved along with the intervals. Unlike the more widely studied one-sided\ntruncation that can be handled effectively by the counting process-based\napproach, doubly truncated data are much more difficult to handle. In their\nanalysis of an astronomical data set, Efron and Petrosian (1999) proposed some\nnonparametric methods, including a generalization of Kendall's tau test, for\ndoubly truncated data. Motivated by their approach, as well as by the work of\nBhattacharya et al. (1983) for right truncated data, we proposed a general\nmethod for estimating the regression parameter when the dependent variable is\nsubject to the double truncation. It extends the Mann-Whitney-type rank\nestimator and can be computed easily by existing software packages. We show\nthat the resulting estimator is consistent and asymptotically normal. A\nresampling scheme is proposed with large sample justification for approximating\nthe limiting distribution. The quasar data in Efron and Petrosian (1999) are\nre-analyzed by the new method. Simulation results show that the proposed method\nworks well. Extension to weighted rank estimation are also given.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 05:47:25 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Ying", "Zhiliang", ""], ["Yu", "Wen", ""], ["Zhao", "Ziqiang", ""], ["Zheng", "Ming", ""]]}, {"id": "1701.01001", "submitter": "Jimmy Olsson Dr", "authors": "Jimmy Olsson and Randal Douc", "title": "Numerically stable online estimation of variance in particle filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses variance estimation in sequential Monte Carlo methods,\nalternatively termed particle filters. The variance estimator that we propose\nis a natural modification of that suggested by H. P. Chan and T. L. Lai [A\ngeneral theory of particle filters in hidden Markov models and some\napplications. Ann. Statist., 41(6):2877-2904, 2013], which allows the variance\nto be estimated in a single run of the particle filter by tracing the\ngenealogical history of the particles. However, due particle lineage\ndegeneracy, the estimator of the mentioned work becomes numerically unstable as\nthe number of sequential particle updates increases. Thus, by tracing only a\npart of the particles' genealogy rather than the full one, our estimator gains\nlong-term numerical stability at the cost of a bias. The scope of the\ngenealogical tracing is regulated by a lag, and under mild, easily checked\nmodel assumptions, we prove that the bias tends to zero geometrically fast as\nthe lag increases. As confirmed by our numerical results, this allows the bias\nto be tightly controlled also for moderate particle sample sizes.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 13:31:03 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Olsson", "Jimmy", ""], ["Douc", "Randal", ""]]}, {"id": "1701.01037", "submitter": "Eric Lock", "authors": "Eric F. Lock", "title": "Tensor-on-tensor regression", "comments": "33 pages, 3 figures", "journal-ref": "Journal of Computational and Graphical Statistics 27 (3), 638-647,\n  2018", "doi": "10.1080/10618600.2017.1401544", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for the linear prediction of a multi-way array (i.e.,\na tensor) from another multi-way array of arbitrary dimension, using the\ncontracted tensor product. This framework generalizes several existing\napproaches, including methods to predict a scalar outcome from a tensor, a\nmatrix from a matrix, or a tensor from a scalar. We describe an approach that\nexploits the multiway structure of both the predictors and the outcomes by\nrestricting the coefficients to have reduced CP-rank. We propose a general and\nefficient algorithm for penalized least-squares estimation, which allows for a\nridge (L_2) penalty on the coefficients. The objective is shown to give the\nmode of a Bayesian posterior, which motivates a Gibbs sampling algorithm for\ninference. We illustrate the approach with an application to facial image data.\nAn R package is available at https://github.com/lockEF/MultiwayRegression .\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 14:59:35 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 21:09:45 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Lock", "Eric F.", ""]]}, {"id": "1701.01097", "submitter": "Young-Geun Choi", "authors": "Yuneung Kim, Johan Lim, Young-Geun Choi, Sujung Choi, and Do Hwan Park", "title": "Regression with Partially Observed Ranks on a Covariate:\n  Distribution-Guided Scores for Ranks", "comments": "28 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by a hand-collected data set from one of the largest\nInternet portals in Korea. This data set records the top 30 most frequently\ndiscussed stocks on its on-line message board. The frequencies are considered\nto measure the attention paid by investors to individual stocks. The empirical\ngoal of the data analysis is to investigate the effect of this attention on\ntrading behavior. For this purpose, we regress the (next day) returns and the\n(partially) observed ranks of frequencies. In the regression, the ranks are\ntransformed into scores, for which purpose the identity or linear scores are\ncommonly used. In this paper, we propose a new class of scores (a score\nfunction) that is based on the moments of order statistics of a pre-decided\nrandom variable. The new score function, denoted by D-rank, is shown to be\nasymptotically optimal to maximize the correlation between the response and\nscore, when the pre-decided random variable and true covariate are in the same\nlocation-scale family. In addition, the least-squares estimator using the\nD-rank consistently estimates the true correlation between the response and the\ncovariate, and asymptotically approaches the normal distribution. We\nadditionally propose a procedure for diagnosing a given score function\n(equivalently, the pre-decided random variable Z) and selecting one that is\nbetter suited to the data. We numerically demonstrate the advantage of using a\ncorrectly specified score function over that of the identity scores (or other\nmisspecified scores) in estimating the correlation coefficient. Finally, we\napply our proposal to test the effects of investors' attention on their returns\nusing the motivating data set.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jan 2017 18:26:31 GMT"}], "update_date": "2017-01-05", "authors_parsed": [["Kim", "Yuneung", ""], ["Lim", "Johan", ""], ["Choi", "Young-Geun", ""], ["Choi", "Sujung", ""], ["Park", "Do Hwan", ""]]}, {"id": "1701.01356", "submitter": "Jakub Pr\\\"uher", "authors": "Jakub Pr\\\"uher, Ond\\v{r}ej Straka", "title": "Gaussian Process Quadrature Moment Transform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computation of moments of transformed random variables is a problem appearing\nin many engineering applications. The current methods for moment transformation\nare mostly based on the classical quadrature rules which cannot account for the\napproximation errors. Our aim is to design a method for moment transformation\nfor Gaussian random variables which accounts for the error in the numerically\ncomputed mean. We employ an instance of Bayesian quadrature, called Gaussian\nprocess quadrature (GPQ), which allows us to treat the integral itself as a\nrandom variable, where the integral variance informs about the incurred\nintegration error. Experiments on the coordinate transformation and nonlinear\nfiltering examples show that the proposed GPQ moment transform performs better\nthan the classical transforms.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 15:39:17 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Pr\u00fcher", "Jakub", ""], ["Straka", "Ond\u0159ej", ""]]}, {"id": "1701.01376", "submitter": "Niklas Linde", "authors": "T. Lochb\\\"uhler, J. A. Vrugt, M. Sadegh, N. Linde", "title": "Summary statistics from training images as prior information in\n  probabilistic inversion", "comments": null, "journal-ref": "Geophysical Journal International, 201, 155-171 (2015)", "doi": "10.1093/gji/ggv008", "report-no": null, "categories": "physics.geo-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A strategy is presented to incorporate prior information from conceptual\ngeological models in probabilistic inversion of geophysical data. The\nconceptual geological models are represented by multiple-point statistics\ntraining images (TIs) featuring the expected lithological units and structural\npatterns. Information from an ensemble of TI realizations is used in two\ndifferent ways. First, dominant modes are identified by analysis of the\nfrequency content in the realizations, which drastically reduces the model\nparameter space in the frequency-amplitude domain. Second, the distributions of\nglobal, summary metrics (e.g. model roughness) are used to formulate a prior\nprobability density function. The inverse problem is formulated in a Bayesian\nframework and the posterior pdf is sampled using Markov chain Monte Carlo\nsimulation. The usefulness and applicability of this method is demonstrated on\ntwo case studies in which synthetic crosshole ground-penetrating radar\ntraveltime data are inverted to recover 2-D porosity fields. The use of prior\ninformation from TIs significantly enhances the reliability of the posterior\nmodels by removing inversion artefacts and improving individual parameter\nestimates. The proposed methodology reduces the ambiguity inherent in the\ninversion of high-dimensional parameter spaces, accommodates a wide range of\nsummary statistics and geophysical forward problems.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 16:46:23 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Lochb\u00fchler", "T.", ""], ["Vrugt", "J. A.", ""], ["Sadegh", "M.", ""], ["Linde", "N.", ""]]}, {"id": "1701.01395", "submitter": "Mauricio Sadinle", "authors": "Mauricio Sadinle, Jerome P. Reiter", "title": "Sequential identification of nonignorable missing data mechanisms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With nonignorable missing data, likelihood-based inference should be based on\nthe joint distribution of the study variables and their missingness indicators.\nThese joint models cannot be estimated from the data alone, thus requiring the\nanalyst to impose restrictions that make the models uniquely obtainable from\nthe distribution of the observed data. We present an approach for constructing\nclasses of identifiable nonignorable missing data models. The main idea is to\nuse a sequence of carefully set up identifying assumptions, whereby we specify\npotentially different missingness mechanisms for different blocks of variables.\nWe show that the procedure results in models with the desirable property of\nbeing non-parametric saturated.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 17:34:25 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Sadinle", "Mauricio", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1701.01467", "submitter": "Roberto Trotta", "authors": "Roberto Trotta (Imperial)", "title": "Bayesian Methods in Cosmology", "comments": "86 pages, 16 figures. Lecture notes for the 44th Saas Fee Advanced\n  Course on Astronomy and Astrophysics, \"Cosmology with wide-field surveys\"\n  (March 2014), to be published by Springer. Comments welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These notes aim at presenting an overview of Bayesian statistics, the\nunderlying concepts and application methodology that will be useful to\nastronomers seeking to analyse and interpret a wide variety of data about the\nUniverse. The level starts from elementary notions, without assuming any\nprevious knowledge of statistical methods, and then progresses to more\nadvanced, research-level topics. After an introduction to the importance of\nstatistical inference for the physical sciences, elementary notions of\nprobability theory and inference are introduced and explained. Bayesian methods\nare then presented, starting from the meaning of Bayes Theorem and its use as\ninferential engine, including a discussion on priors and posterior\ndistributions. Numerical methods for generating samples from arbitrary\nposteriors (including Markov Chain Monte Carlo and Nested Sampling) are then\ncovered. The last section deals with the topic of Bayesian model selection and\nhow it is used to assess the performance of models, and contrasts it with the\nclassical p-value approach. A series of exercises of various levels of\ndifficulty are designed to further the understanding of the theoretical\nmaterial, including fully worked out solutions for most of them.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 20:23:53 GMT"}], "update_date": "2017-01-09", "authors_parsed": [["Trotta", "Roberto", "", "Imperial"]]}, {"id": "1701.01503", "submitter": "Jared Murray", "authors": "Jared S. Murray", "title": "Log-Linear Bayesian Additive Regression Trees for Multinomial Logistic\n  and Count Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Bayesian additive regression trees (BART) for log-linear models\nincluding multinomial logistic regression and count regression with\nzero-inflation and overdispersion. BART has been applied to nonparametric mean\nregression and binary classification problems in a range of settings. However,\nexisting applications of BART have been limited to models for Gaussian \"data\",\neither observed or latent. This is primarily because efficient MCMC algorithms\nare available for Gaussian likelihoods. But while many useful models are\nnaturally cast in terms of latent Gaussian variables, many others are not --\nincluding models considered in this paper.\n  We develop new data augmentation strategies and carefully specified prior\ndistributions for these new models. Like the original BART prior, the new prior\ndistributions are carefully constructed and calibrated to be flexible while\nguarding against overfitting. Together the new priors and data augmentation\nschemes allow us to implement an efficient MCMC sampler outside the context of\nGaussian models. The utility of these new methods is illustrated with examples\nand an application to a previously published dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 23:24:04 GMT"}, {"version": "v2", "created": "Mon, 26 Aug 2019 21:56:41 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Murray", "Jared S.", ""]]}, {"id": "1701.01560", "submitter": "Seung Jun Shin", "authors": "Arlene K. H. Kim and Seung Jun Shin", "title": "The cumulative Kolmogorov filter for model-free screening in ultrahigh\n  dimensional data", "comments": null, "journal-ref": "Statistics & Probability Letters, Volume 126 (July), p. 238-243\n  (2017)", "doi": "10.1016/j.spl.2017.03.012", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a cumulative Kolmogorov filter to improve the fused Kolmogorov\nfilter proposed by Zou (2015) via cumulative slicing. We establish an improved\nasymptotic result under relaxed assumptions and numerically demonstrate its\nenhanced finite sample performance.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 07:43:14 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 00:12:23 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Kim", "Arlene K. H.", ""], ["Shin", "Seung Jun", ""]]}, {"id": "1701.01672", "submitter": "Paul Fearnhead", "authors": "Robert Maidstone, Paul Fearnhead and Adam Letchford", "title": "Detecting changes in slope with an $L_0$ penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Whilst there are many approaches to detecting changes in mean for a\nunivariate time-series, the problem of detecting multiple changes in slope has\ncomparatively been ignored. Part of the reason for this is that detecting\nchanges in slope is much more challenging. For example, simple binary\nsegmentation procedures do not work for this problem, whilst efficient dynamic\nprogramming methods that work well for the change in mean problem cannot be\ndirectly used for detecting changes in slope. We present a novel dynamic\nprogramming approach, CPOP, for finding the \"best\" continuous piecewise-linear\nfit to data. We define best based on a criterion that measures fit to data\nusing the residual sum of squares, but penalises complexity based on an $L_0$\npenalty on changes in slope. We show that using such a criterion is more\nreliable at estimating changepoint locations than approaches that penalise\ncomplexity using an $L_1$ penalty. Empirically CPOP has good computational\nproperties, and can analyse a time-series with over 10,000 observations and\nover 100 changes in a few minutes. Our method is used to analyse data on the\nmotion of bacteria, and provides fits to the data that both have substantially\nsmaller residual sum of squares and are more parsimonious than two competing\napproaches.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 15:52:45 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2017 10:26:34 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Maidstone", "Robert", ""], ["Fearnhead", "Paul", ""], ["Letchford", "Adam", ""]]}, {"id": "1701.01741", "submitter": "Anne van Delft Dr.", "authors": "Alexander Aue and Anne van Delft", "title": "Testing for stationarity of functional time series in the frequency\n  domain", "comments": "63 pages, includes Online Supplement", "journal-ref": null, "doi": "10.1214/19-AOS1895", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in functional time series has spiked in the recent past with papers\ncovering both methodology and applications being published at a much increased\npace. This article contributes to the research in this area by proposing a new\nstationarity test for functional time series based on frequency domain methods.\nThe proposed test statistics is based on joint dimension reduction via\nfunctional principal components analysis across the spectral density operators\nat all Fourier frequencies, explicitly allowing for frequency-dependent levels\nof truncation to adapt to the dynamics of the underlying functional time\nseries. The properties of the test are derived both under the null hypothesis\nof stationary functional time series and under the smooth alternative of\nlocally stationary functional time series. The methodology is theoretically\njustified through asymptotic results. Evidence from simulation studies and an\napplication to annual temperature curves suggests that the test works well in\nfinite samples.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jan 2017 19:40:45 GMT"}, {"version": "v2", "created": "Sun, 8 Oct 2017 09:51:32 GMT"}, {"version": "v3", "created": "Mon, 29 Apr 2019 21:02:16 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Aue", "Alexander", ""], ["van Delft", "Anne", ""]]}, {"id": "1701.01960", "submitter": "Hiroki Okada", "authors": "Hiroki Okada and Ken Umeno", "title": "Randomness Evaluation with the Discrete Fourier Transform Test Based on\n  Exact Analysis of the Reference Distribution", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": "10.1109/TIFS.2017.2656473", "report-no": "T-IFS-06137-2016.R2", "categories": "cs.CR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problems in the discrete Fourier transform (DFT)\ntest included in NIST SP 800-22 released by the National Institute of Standards\nand Technology (NIST), which is a collection of tests for evaluating both\nphysical and pseudo-random number generators for cryptographic applications.\nThe most crucial problem in the DFT test is that its reference distribution of\nthe test statistic is not derived mathematically but rather numerically\nestimated, the DFT test for randomness is based on a pseudo-random number\ngenerator (PRNG). Therefore, the present DFT test should not be used unless the\nreference distribution is mathematically derived. Here, we prove that a power\nspectrum, which is a component of the test statistic, follows a chi-squared\ndistribution with 2 degrees of freedom. Based on this fact, we propose a test\nwhose reference distribution of the test statistic is mathematically derived.\nFurthermore, the results of testing non-random sequences and several PRNGs\nshowed that the proposed test is more reliable and definitely more sensitive\nthan the present DFT test.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 14:02:52 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Okada", "Hiroki", ""], ["Umeno", "Ken", ""]]}, {"id": "1701.02002", "submitter": "Pierre E. Jacob", "authors": "Pierre E. Jacob, Fredrik Lindsten, Thomas B. Sch\\\"on", "title": "Smoothing with Couplings of Conditional Particle Filters", "comments": "This document is a self-contained and direct description of the\n  smoothing method introduced in Coupling of Particle Filters\n  (arXiv:1606.01156). Code is available at github.com/pierrejacob/CoupledCPF.\n  Compared to the previous version, a bug was fixed in the code, and the\n  numerical results were updated", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In state space models, smoothing refers to the task of estimating a latent\nstochastic process given noisy measurements related to the process. We propose\nan unbiased estimator of smoothing expectations. The lack-of-bias property has\nmethodological benefits: independent estimators can be generated in parallel,\nand confidence intervals can be constructed from the central limit theorem to\nquantify the approximation error. To design unbiased estimators, we combine a\ngeneric debiasing technique for Markov chains with a Markov chain Monte Carlo\nalgorithm for smoothing. The resulting procedure is widely applicable and we\nshow in numerical experiments that the removal of the bias comes at a\nmanageable increase in variance. We establish the validity of the proposed\nestimators under mild assumptions. Numerical experiments are provided on toy\nmodels, including a setting of highly-informative observations, and a realistic\nLotka-Volterra model with an intractable transition density.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jan 2017 19:24:59 GMT"}, {"version": "v2", "created": "Mon, 13 Aug 2018 20:07:43 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2018 19:52:16 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Jacob", "Pierre E.", ""], ["Lindsten", "Fredrik", ""], ["Sch\u00f6n", "Thomas B.", ""]]}, {"id": "1701.02110", "submitter": "Torsten Hothorn", "authors": "Torsten Hothorn and Achim Zeileis", "title": "Transformation Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regression models for supervised learning problems with a continuous target\nare commonly understood as models for the conditional mean of the target given\npredictors. This notion is simple and therefore appealing for interpretation\nand visualisation. Information about the whole underlying conditional\ndistribution is, however, not available from these models. A more general\nunderstanding of regression models as models for conditional distributions\nallows much broader inference from such models, for example the computation of\nprediction intervals. Several random forest-type algorithms aim at estimating\nconditional distributions, most prominently quantile regression forests\n(Meinshausen, 2006, JMLR). We propose a novel approach based on a parametric\nfamily of distributions characterised by their transformation function. A\ndedicated novel \"transformation tree\" algorithm able to detect distributional\nchanges is developed. Based on these transformation trees, we introduce\n\"transformation forests\" as an adaptive local likelihood estimator of\nconditional distribution functions. The resulting models are fully parametric\nyet very general and allow broad inference procedures, such as the model-based\nbootstrap, to be applied in a straightforward way.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 09:52:03 GMT"}, {"version": "v2", "created": "Mon, 8 Jan 2018 10:08:16 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Hothorn", "Torsten", ""], ["Zeileis", "Achim", ""]]}, {"id": "1701.02156", "submitter": "Tore Selland Kleppe", "authors": "Tore Selland Kleppe and Atle Oglend", "title": "Estimating the Competitive Storage Model: A Simulated Likelihood\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a particle filter maximum likelihood estimator for the\ncompetitive storage model. The estimator is suitable for inference problems in\ncommodity markets where only reliable price data is available for estimation,\nand shocks are temporally dependent. The estimator efficiently utilizes the\ninformation present in the conditional distribution of prices when shocks are\nnot iid. Compared to Deaton and Laroque's composite quasi-maximum likelihood\nestimator, simulation experiments and real-data estimation show substantial\nimprovements in both bias and precision. Simulation experiments also show that\nthe precision of the particle filter estimator improves faster than for\ncomposite quasi-maximum likelihood with more price data. To demonstrate the\nestimator and its relevance to actual data, we fit the storage model to data\nset of monthly natural gas prices. It is shown that the storage model estimated\nwith the particle filter estimator beats, in terms of log-likelihood, commonly\nused reduced form time-series models such as the linear AR(1), AR(1)-GARCH(1,1)\nand Markov Switching AR(1) models for this data set.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jan 2017 12:17:54 GMT"}], "update_date": "2017-01-10", "authors_parsed": [["Kleppe", "Tore Selland", ""], ["Oglend", "Atle", ""]]}, {"id": "1701.02424", "submitter": "Joshua Chang", "authors": "Aaron Heuser, Minh Huynh and Joshua C. Chang", "title": "Asymptotic convergence in distribution of the area bounded by\n  prevalence-weighted Kaplan-Meier curves using empirical process modeling", "comments": "Major revision in review by RSOS. Includes fixes to Theorem 1 as well\n  as numerical results", "journal-ref": "R. Soc. open sci, 2018, 5(180496)", "doi": "10.1098/rsos.180496", "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Kaplan-Meier product-limit estimator is a simple and powerful tool in\ntime to event analysis. An extension exists for populations stratified into\ncohorts where a population survival curve is generated by weighted averaging of\ncohort-level survival curves. For making population-level comparisons using\nthis statistic, we analyze the statistics of the area between two such weighted\nsurvival curves. We derive the large sample behavior of this statistic based on\nan empirical process of product-limit estimators. This estimator was used by an\ninterdisciplinary NIH-SSA team in the identification of medical conditions to\nprioritize for adjudication in disability benefits processing.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 03:06:10 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 20:20:42 GMT"}, {"version": "v3", "created": "Tue, 21 Aug 2018 16:20:28 GMT"}, {"version": "v4", "created": "Tue, 2 Oct 2018 05:23:30 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Heuser", "Aaron", ""], ["Huynh", "Minh", ""], ["Chang", "Joshua C.", ""]]}, {"id": "1701.02434", "submitter": "Michael Betancourt", "authors": "Michael Betancourt", "title": "A Conceptual Introduction to Hamiltonian Monte Carlo", "comments": "60 pages, 42 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo has proven a remarkable empirical success, but only\nrecently have we begun to develop a rigorous understanding of why it performs\nso well on difficult problems and how it is best applied in practice.\nUnfortunately, that understanding is confined within the mathematics of\ndifferential geometry which has limited its dissemination, especially to the\napplied communities for which it is particularly important. In this review I\nprovide a comprehensive conceptual account of these theoretical foundations,\nfocusing on developing a principled intuition behind the method and its optimal\nimplementations rather of any exhaustive rigor. Whether a practitioner or a\nstatistician, the dedicated reader will acquire a solid grasp of how\nHamiltonian Monte Carlo works, when it succeeds, and, perhaps most importantly,\nwhen it fails.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 04:26:06 GMT"}, {"version": "v2", "created": "Mon, 16 Jul 2018 02:46:10 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Betancourt", "Michael", ""]]}, {"id": "1701.02483", "submitter": "Matthieu Wilhelm", "authors": "Yves Till\\'e and Lionel Qualit\\'e and Matthieu Wilhelm", "title": "Sampling Designs on Finite Populations with Spreading Control Parameters", "comments": "Accepted in Statistica Sinica, typos and minor modifications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present new sampling methods in finite population that allow to control\nthe joint inclusion probabilities of units and especially the spreading of\nsampled units in the population. They are based on the use of renewal chains\nand multivariate discrete distributions to generate the difference of\npopulation ranks between two successive selected units. With a Bernoulli\nsampling design, these differences follow a geometric distribution, and with a\nsimple random sampling design they follow a negative hypergeometric\ndistribution. We propose to use other distributions and introduce a large class\nof sampling designs with and without fixed sample size. The choice of the\nrank-difference distribution allows us to control units joint inclusion\nprobabilities with a relatively simple method and closed form formula. Joint\ninclusion probabilities of neighboring units can be chosen to be larger, or\nsmaller, compared to those of Bernoulli or simple random sampling, thus\nallowing to more or less spread the sample on the population. This can be\nuseful when neighboring units have similar characteristics or, on the contrary,\nare very different. A set of simulations illustrates the qualities of this\nmethod.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 09:04:23 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 12:22:28 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Till\u00e9", "Yves", ""], ["Qualit\u00e9", "Lionel", ""], ["Wilhelm", "Matthieu", ""]]}, {"id": "1701.02512", "submitter": "Beatriz Bueno-Larraz Beatriz Bueno-Larraz", "authors": "Jos\\'e R. Berrendero, Beatriz Bueno-Larraz and Antonio Cuevas", "title": "An RKHS model for variable selection in functional regression", "comments": "31 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mathematical model for variable selection in functional regression models\nwith scalar response is proposed. By \"variable selection\" we mean a procedure\nto replace the whole trajectories of the functional explanatory variables with\ntheir values at a finite number of carefully selected instants (or \"impact\npoints\"). The basic idea of our approach is to use the Reproducing Kernel\nHilbert Space (RKHS) associated with the underlying process, instead of the\nmore usual L2[0,1] space, in the definition of the linear model. This turns out\nto be especially suitable for variable selection purposes, since the\nfinite-dimensional linear model based on the selected \"impact points\" can be\nseen as a particular case of the RKHS-based linear functional model. In this\nframework, we address the consistent estimation of the optimal design of impact\npoints and we check, via simulations and real data examples, the performance of\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 10:44:11 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 13:30:38 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Berrendero", "Jos\u00e9 R.", ""], ["Bueno-Larraz", "Beatriz", ""], ["Cuevas", "Antonio", ""]]}, {"id": "1701.02666", "submitter": "Andreas F. Haselsteiner", "authors": "Andreas F. Haselsteiner, Jan-Hendrik Ohlendorf, Werner Wosniok,\n  Klaus-Dieter Thoben", "title": "Deriving environmental contours from highest density regions", "comments": "preprint of the accepted version, 14 pages, 10 figures", "journal-ref": "Coast.Eng. 123 (2017) 42-51", "doi": "10.1016/j.coastaleng.2017.03.002", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Environmental contours are an established method in probabilistic engineering\ndesign, especially in ocean engineering. The contours help engineers to select\nthe environmental states which are appropriate for structural design\ncalculations. Defining an environmental contour means enclosing a region in the\nvariable space which corresponds to a certain return period. However, there are\nmultiple definitions and methods to calculate an environmental contour for a\ngiven return period. Here, we analyze the established approaches and present a\nnew concept which we call highest density contour (HDC). We define this\nenvironmental contour to enclose the highest density region (HDR) of a given\nprobability density. This region occupies the smallest possible volume in the\nvariable space among all regions with the same included probability, which is\nadvantageous for engineering design. We perform the calculations using a\nnumerical grid to discretize the original variable space into a finite number\nof grid cells. Each cell's probability is estimated and used for numerical\nintegration. The proposed method can be applied to any number of dimensions,\ni.e. number of different variables in the joint probability model. To put the\nhighest density contour method in context, we compare it to the established\ninverse first-order reliability method (IFORM) and show that for common\nprobability distributions the two methods yield similarly shaped contours. In\nmultimodal probability distributions, however, where IFORM leads to contours\nwhich are dificult to interpret, the presented method still generates clearly\ndefined contours.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 16:22:51 GMT"}, {"version": "v2", "created": "Fri, 31 Mar 2017 13:25:53 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Haselsteiner", "Andreas F.", ""], ["Ohlendorf", "Jan-Hendrik", ""], ["Wosniok", "Werner", ""], ["Thoben", "Klaus-Dieter", ""]]}, {"id": "1701.02786", "submitter": "Joseph Voelkel", "authors": "Joseph G. Voelkel", "title": "The Design of Order-of-Addition Experiments", "comments": "Updates: * Moved the location of some tables * Expanded\n  table-isomorphic ideas * Showed by an example that a D-efficiency of 1 and OA\n  are not equivalent * Illustrated some properties of OofA OA designs", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce systematic methods to create optimal designs for\norder-of-addition (OofA) experiments, those that study the order in which $m$\ncomponents are applied---for example, the order in which chemicals are added to\na reaction or layers are added to a film. Full designs require $m!$ runs, so we\ninvestigate design fractions. Balance criteria for creating such designs employ\nan extension of orthogonal arrays (OA's) to OofA-OA's. A connection is made\nbetween $D$-efficient and OofA-OA designs. Necessary conditions are found for\nthe number of runs needed to create OofA-OA's of strengths 2 and 3. We create a\nnumber of new, optimal, designs: 12-run OofA-OA's in 4 and 5 components, 24-run\nOofA-OA's in 5 and 6 components, and near OofA-OA's in 7 components. We extend\nthese designs to include (a) process factors, and (b) the common case in which\ncomponent orderings are restricted. We also suggest how such designs may be\nanalyzed.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jan 2017 21:08:40 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 19:08:29 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Voelkel", "Joseph G.", ""]]}, {"id": "1701.02857", "submitter": "Gourab Mukherjee", "authors": "Trambak Banerjee, Gourab Mukherjee and Peter Radchenko", "title": "Feature Screening in Large Scale Cluster Analysis", "comments": "final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel methodology for feature screening in clustering massive\ndatasets, in which both the number of features and the number of observations\ncan potentially be very large. Taking advantage of a fusion penalization based\nconvex clustering criterion, we propose a very fast screening procedure that\nefficiently discards non-informative features by first computing a clustering\nscore corresponding to the clustering tree constructed for each feature, and\nthen thresholding the resulting values. We provide theoretical support for our\napproach by establishing uniform non-asymptotic bounds on the clustering scores\nof the \"noise\" features. These bounds imply perfect screening of\nnon-informative features with high probability and are derived via careful\nanalysis of the empirical processes corresponding to the clustering trees that\nare constructed for each of the features by the associated clustering\nprocedure. Through extensive simulation experiments we compare the performance\nof our proposed method with other screening approaches, popularly used in\ncluster analysis, and obtain encouraging results. We demonstrate empirically\nthat our method is applicable to cluster analysis of big datasets arising in\nsingle-cell gene expression studies.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 06:08:44 GMT"}, {"version": "v2", "created": "Wed, 4 Oct 2017 16:28:24 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Banerjee", "Trambak", ""], ["Mukherjee", "Gourab", ""], ["Radchenko", "Peter", ""]]}, {"id": "1701.02887", "submitter": "Marie-Colette van Lieshout", "authors": "A. Iftimi, M.N.M. van Lieshout, F. Montes", "title": "A multi-scale area-interaction model for spatio-temporal point patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models for fitting spatio-temporal point processes should incorporate\nspatio-temporal inhomogeneity and allow for different types of interaction\nbetween points (clustering or regularity). This paper proposes an extension of\nthe spatial multi-scale area-interaction model to a spatio-temporal framework.\nThis model allows for interaction between points at different spatio-temporal\nscales and the inclusion of covariates. We fit the proposed model to varicella\ncases registered during 2013 in Valencia, Spain. The fitted model indicates\nsmall scale clustering and regularity for higher spatio-temporal scales.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 08:39:28 GMT"}], "update_date": "2017-01-12", "authors_parsed": [["Iftimi", "A.", ""], ["van Lieshout", "M. N. M.", ""], ["Montes", "F.", ""]]}, {"id": "1701.02950", "submitter": "Antonio Canale", "authors": "Antonio Canale, Daniele Durante, David Dunson", "title": "Convex Mixture Regression for Quantitative Risk Assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is wide interest in studying how the distribution of a continuous\nresponse changes with a predictor. We are motivated by environmental\napplications in which the predictor is the dose of an exposure and the response\nis a health outcome. A main focus in these studies is inference on dose levels\nassociated with a given increase in risk relative to a baseline. Popular\nmethods either dichotomize the continuous response or focus on modeling changes\nwith the dose in the expectation of the outcome. Such choices may lead to\ninformation loss and provide inaccurate inference on dose-response\nrelationships. We instead propose a Bayesian convex mixture regression model\nthat allows the entire distribution of the health outcome to be unknown and\nchanging with the dose. To balance flexibility and parsimony, we rely on a\nmixture model for the density at the extreme doses, and express the conditional\ndensity at each intermediate dose via a convex combination of these extremal\ndensities. This representation generalizes classical dose-response models for\nquantitative outcomes, and provides a more parsimonious, but still powerful,\nformulation compared to nonparametric methods, thereby improving\ninterpretability and efficiency in inference on risk functions. A Markov chain\nMonte Carlo algorithm for posterior inference is developed, and the benefits of\nour methods are outlined in simulations, along with a study on the impact of\nDDT exposure on gestational age.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 12:30:53 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 10:26:15 GMT"}, {"version": "v3", "created": "Sat, 12 Aug 2017 21:11:20 GMT"}, {"version": "v4", "created": "Thu, 30 Nov 2017 08:56:10 GMT"}, {"version": "v5", "created": "Tue, 24 Apr 2018 19:53:53 GMT"}, {"version": "v6", "created": "Wed, 9 May 2018 14:54:28 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Canale", "Antonio", ""], ["Durante", "Daniele", ""], ["Dunson", "David", ""]]}, {"id": "1701.03139", "submitter": "Luke Miratrix", "authors": "Luke Miratrix, Jane Furey, Avi Feller, Todd Grindal, Lindsay C. Page", "title": "Bounding, an accessible method for estimating principal causal effects,\n  examined and explained", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating treatment effects for subgroups defined by post-treatment behavior\n(i.e., estimating causal effects in a principal stratification framework) can\nbe technically challenging and heavily reliant on strong assumptions. We\ninvestigate an alternative path: using bounds to identify ranges of possible\neffects that are consistent with the data. This simple approach relies on fewer\nassumptions and yet can result in policy-relevant findings. As we show,\ncovariates can be used to substantially tighten bounds in a straightforward\nmanner. Via simulation, we demonstrate which types of covariates are maximally\nbeneficial. We conclude with an analysis of a multi-site experimental study of\nEarly College High Schools. When examining the program's impact on students\ncompleting the ninth grade \"on-track\" for college, we find little impact for\nECHS students who would otherwise attend a high quality high school, but\nsubstantial effects for those who would not. This suggests potential benefit in\nexpanding these programs in areas primarily served by lower quality schools.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jan 2017 19:42:33 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 11:49:15 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Miratrix", "Luke", ""], ["Furey", "Jane", ""], ["Feller", "Avi", ""], ["Grindal", "Todd", ""], ["Page", "Lindsay C.", ""]]}, {"id": "1701.03208", "submitter": "David  Nott", "authors": "Victor M.-H. Ong and David J. Nott and Michael S. Smith", "title": "Gaussian variational approximation with a factor covariance structure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational approximation methods have proven to be useful for scaling\nBayesian computations to large data sets and highly parametrized models.\nApplying variational methods involves solving an optimization problem, and\nrecent research in this area has focused on stochastic gradient ascent methods\nas a general approach to implementation. Here variational approximation is\nconsidered for a posterior distribution in high dimensions using a Gaussian\napproximating family. Gaussian variational approximation with an unrestricted\ncovariance matrix can be computationally burdensome in many problems because\nthe number of elements in the covariance matrix increases quadratically with\nthe dimension of the model parameter. To circumvent this problem,\nlow-dimensional factor covariance structures are considered. General stochastic\ngradient approaches to efficiently perform the optimization are described, with\ngradient estimates obtained using the so-called \"reparametrization trick\". The\nend result is a flexible and efficient approach to high-dimensional Gaussian\nvariational approximation, which we illustrate using eight real datasets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 01:57:24 GMT"}], "update_date": "2017-01-13", "authors_parsed": [["Ong", "Victor M. -H.", ""], ["Nott", "David J.", ""], ["Smith", "Michael S.", ""]]}, {"id": "1701.03314", "submitter": "Joris Chau", "authors": "Joris Chau, Rainer von Sachs", "title": "Intrinsic wavelet regression for curves of Hermitian positive definite\n  matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic wavelet transforms and wavelet estimation methods are introduced\nfor curves in the non-Euclidean space of Hermitian positive definite matrices,\nwith in mind the application to Fourier spectral estimation of multivariate\nstationary time series. The main focus is on intrinsic average-interpolation\nwavelet transforms in the space of positive definite matrices equipped with an\naffine-invariant Riemannian metric, and convergence rates of linear wavelet\nthresholding are derived for intrinsically smooth curves of Hermitian positive\ndefinite matrices. In the context of multivariate Fourier spectral estimation,\nintrinsic wavelet thresholding is equivariant under a change of basis of the\ntime series, and nonlinear wavelet thresholding is able to capture localized\nfeatures in the spectral density matrix across frequency, always guaranteeing\npositive definite estimates. The finite-sample performance of intrinsic wavelet\nthresholding is assessed by means of simulated data and compared to several\nbenchmark estimators in the Riemannian manifold. Further illustrations are\nprovided by examining the multivariate spectra of trial-replicated brain signal\ntime series recorded during a learning experiment.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 11:30:04 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 12:13:33 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 16:25:38 GMT"}, {"version": "v4", "created": "Sat, 30 Dec 2017 12:59:08 GMT"}, {"version": "v5", "created": "Tue, 21 May 2019 07:08:33 GMT"}, {"version": "v6", "created": "Sun, 10 Nov 2019 12:07:36 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Chau", "Joris", ""], ["von Sachs", "Rainer", ""]]}, {"id": "1701.03462", "submitter": "Hon Keung Tony Ng", "authors": "Nozer D. Singpurwalla, Barry C. Arnold, Joseph L. Gastwirth, Anna S.\n  Gordon, Hon Keung Tony Ng", "title": "Adversarial and Amiable Inference in Medical Diagnosis, Reliability, and\n  Survival Analysis", "comments": null, "journal-ref": "International Statistical Review (2016), Vol. 84, pp. 390-412", "doi": "10.1111/insr.12104", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a family of bivariate beta distributions that\nencapsulate both positive and negative correlations, and which can be of\ngeneral interest for Bayesian inference. We then invoke a use of these\nbivariate distributions in two contexts. The first is diagnostic testing in\nmedicine, threat detection, and signal processing. The second is system\nsurvivability assessment, relevant to engineering reliability, and to survival\nanalysis in biomedicine. In diagnostic testing one encounters two parameters\nthat characterize the efficacy of the testing mechanism, {\\it test\nsensitivity}, and {\\it test specificity}. These tend to be adversarial when\ntheir values are interpreted as utilities. In system survivability, the\nparameters of interest are the component reliabilities, whose values when\ninterpreted as utilities tend to exhibit co-operative (amiable) behavior.\nBesides probability modeling and Bayesian inference, this paper has a\nfoundational import. Specifically, it advocates a conceptual change in how one\nmay think about reliability and survival analysis. The philosophical writings\nof de Finetti, Kolmogorov, Popper, and Savage, when brought to bear on these\ntopics constitute the essence of this change. Its consequence is that we have\nat hand a defensible framework for invoking Bayesian inferential methods in\ndiagnostics, reliability, and survival analysis. Another consequence is a\ndeeper appreciation of the judgment of independent lifetimes. Specifically, we\nmake the important point that independent lifetimes entail at a minimum, a\ntwo-stage hierarchical construction.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 16:16:41 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Singpurwalla", "Nozer D.", ""], ["Arnold", "Barry C.", ""], ["Gastwirth", "Joseph L.", ""], ["Gordon", "Anna S.", ""], ["Ng", "Hon Keung Tony", ""]]}, {"id": "1701.03504", "submitter": "Gabriel Loaiza-Ganem", "authors": "Gabriel Loaiza-Ganem, Yuanjun Gao, John P. Cunningham", "title": "Maximum Entropy Flow Networks", "comments": "Accepted at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum entropy modeling is a flexible and popular framework for formulating\nstatistical models given partial knowledge. In this paper, rather than the\ntraditional method of optimizing over the continuous density directly, we learn\na smooth and invertible transformation that maps a simple distribution to the\ndesired maximum entropy distribution. Doing so is nontrivial in that the\nobjective being maximized (entropy) is a function of the density itself. By\nexploiting recent developments in normalizing flow networks, we cast the\nmaximum entropy problem into a finite-dimensional constrained optimization, and\nsolve the problem by combining stochastic optimization with the augmented\nLagrangian method. Simulation results demonstrate the effectiveness of our\nmethod, and applications to finance and computer vision show the flexibility\nand accuracy of using maximum entropy flow networks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 21:00:30 GMT"}, {"version": "v2", "created": "Fri, 28 Apr 2017 17:13:18 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Loaiza-Ganem", "Gabriel", ""], ["Gao", "Yuanjun", ""], ["Cunningham", "John P.", ""]]}, {"id": "1701.03513", "submitter": "Pavlo Mozharovskyi", "authors": "Pavlo Mozharovskyi, Julie Josse, Francois Husson", "title": "Nonparametric imputation by data depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present single imputation method for missing values which borrows the idea\nof data depth---a measure of centrality defined for an arbitrary point of a\nspace with respect to a probability distribution or data cloud. This consists\nin iterative maximization of the depth of each observation with missing values,\nand can be employed with any properly defined statistical depth function. For\neach single iteration, imputation reverts to optimization of quadratic, linear,\nor quasiconcave functions that are solved analytically by linear programming or\nthe Nelder-Mead method. As it accounts for the underlying data topology, the\nprocedure is distribution free, allows imputation close to the data geometry,\ncan make prediction in situations where local imputation (k-nearest neighbors,\nrandom forest) cannot, and has attractive robustness and asymptotic properties\nunder elliptical symmetry. It is shown that a special case---when using the\nMahalanobis depth---has direct connection to well-known methods for the\nmultivariate normal model, such as iterated regression and regularized PCA. The\nmethodology is extended to multiple imputation for data stemming from an\nelliptically symmetric distribution. Simulation and real data studies show good\nresults compared with existing popular alternatives. The method has been\nimplemented as an R-package. Supplementary materials for the article are\navailable online.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jan 2017 21:43:10 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 20:33:25 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Mozharovskyi", "Pavlo", ""], ["Josse", "Julie", ""], ["Husson", "Francois", ""]]}, {"id": "1701.03535", "submitter": "Christian Walder Dr", "authors": "Christian J. Walder and Adrian N. Bishop", "title": "Fast Bayesian Intensity Estimation for the Permanental Process", "comments": null, "journal-ref": "In Proceedings of the 34th International Conference on Machine\n  Learning, pages: 3579-3588, 6-11 August 2017", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Cox process is a stochastic process which generalises the Poisson process\nby letting the underlying intensity function itself be a stochastic process. In\nthis paper we present a fast Bayesian inference scheme for the permanental\nprocess, a Cox process under which the square root of the intensity is a\nGaussian process. In particular we exploit connections with reproducing kernel\nHilbert spaces, to derive efficient approximate Bayesian inference algorithms\nbased on the Laplace approximation to the predictive distribution and marginal\nlikelihood. We obtain a simple algorithm which we apply to toy and real-world\nproblems, obtaining orders of magnitude speed improvements over previous work.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 00:40:44 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 07:03:04 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 11:07:04 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Walder", "Christian J.", ""], ["Bishop", "Adrian N.", ""]]}, {"id": "1701.03550", "submitter": "Yong Huang", "authors": "Yong Huang, James L. Beck and Hui Li", "title": "Bayesian System Identification based on Hierarchical Sparse Bayesian\n  Learning and Gibbs Sampling with Application to Structural Damage Assessment", "comments": "12 figures", "journal-ref": null, "doi": "10.1016/j.cma.2017.01.030", "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus in this paper is Bayesian system identification based on noisy\nincomplete modal data where we can impose spatially-sparse stiffness changes\nwhen updating a structural model. To this end, based on a similar hierarchical\nsparse Bayesian learning model from our previous work, we propose two Gibbs\nsampling algorithms. The algorithms differ in their strategies to deal with the\nposterior uncertainty of the equation-error precision parameter, but both\nsample from the conditional posterior probability density functions (PDFs) for\nthe structural stiffness parameters and system modal parameters. The effective\ndimension for the Gibbs sampling is low because iterative sampling is done from\nonly three conditional posterior PDFs that correspond to three parameter\ngroups, along with sampling of the equation-error precision parameter from\nanother conditional posterior PDF in one of the algorithms where it is not\nintegrated out as a \"nuisance\" parameter. A nice feature from a computational\nperspective is that it is not necessary to solve a nonlinear eigenvalue problem\nof a structural model. The effectiveness and robustness of the proposed\nalgorithms are illustrated by applying them to the IASE-ASCE Phase II simulated\nand experimental benchmark studies. The goal is to use incomplete modal data\nidentified before and after possible damage to detect and assess\nspatially-sparse stiffness reductions induced by any damage. Our past and\ncurrent focus on meeting challenges arising from Bayesian inference of\nstructural stiffness serve to strengthen the capability of vibration-based\nstructural system identification but our methods also have much broader\napplicability for inverse problems in science and technology where system\nmatrices are to be inferred from noisy partial information about their\neigenquantities.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 02:51:37 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Huang", "Yong", ""], ["Beck", "James L.", ""], ["Li", "Hui", ""]]}, {"id": "1701.03569", "submitter": "Debasis Kundu Professor", "authors": "Vahid Nekoukhou and Debasis Kundu", "title": "Bivariate Discrete Generalized Exponential Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a bivariate discrete generalized exponential\ndistribution, whose marginals are discrete generalized exponential distribution\nas proposed by Nekoukhou, Alamatsaz and Bidram (\"Discrete generalized\nexponential distribution of a second type\", Statistics, 47, 876 - 887, 2013).\nIt is observed that the proposed bivariate distribution is a very flexible\ndistribution and the bivariate geometric distribution can be obtained as a\nspecial case of this distribution. The proposed distribution can be seen as a\nnatural discrete analogue of the bivariate generalized exponential distribution\nproposed by Kundu and Gupta (\"Bivariate generalized exponential distribution\",\nJournal of Multivariate Analysis, 100, 581 - 593, 2009). We study different\nproperties of this distribution and explore its dependence structures. We\npropose a new EM algorithm to compute the maximum likelihood estimators of the\nunknown parameters which can be implemented very efficiently, and discuss some\ninferential issues also. The analysis of one data set has been performed to\nshow the effectiveness of the proposed model. Finally we propose some open\nproblems and conclude the paper.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 06:03:45 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Nekoukhou", "Vahid", ""], ["Kundu", "Debasis", ""]]}, {"id": "1701.03772", "submitter": "Binhuan Wang", "authors": "Binhuan Wang, Yixin Fang, Heng Lian, Hua Liang", "title": "Additive Partially Linear Models for Massive Heterogeneous Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an additive partially linear framework for modelling massive\nheterogeneous data. The major goal is to extract multiple common features\nsimultaneously across all sub-populations while exploring heterogeneity of each\nsub-population. We propose an aggregation type of estimators for the\ncommonality parameters that possess the asymptotic optimal bounds and the\nasymptotic distributions as if there were no heterogeneity. This oracle result\nholds when the number of sub-populations does not grow too fast and the tuning\nparameters are selected carefully. A plug-in estimator for the heterogeneity\nparameter is further constructed, and shown to possess the asymptotic\ndistribution as if the commonality information were available. Furthermore, we\ndevelop a heterogeneity test for the linear components and a homogeneity test\nfor the non-linear components accordingly. The performance of the proposed\nmethods is evaluated via simulation studies and an application to the Medicare\nProvider Utilization and Payment data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 18:42:42 GMT"}, {"version": "v2", "created": "Fri, 28 Dec 2018 19:46:21 GMT"}], "update_date": "2019-01-01", "authors_parsed": [["Wang", "Binhuan", ""], ["Fang", "Yixin", ""], ["Lian", "Heng", ""], ["Liang", "Hua", ""]]}, {"id": "1701.03822", "submitter": "Lazhar Benkhelifa", "authors": "Lazhar Benkhelifa", "title": "Efficient estimation in the Topp-Leone distribution", "comments": "12pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current paper, the estimation of the probability density function and\nthe cumulative distribution function of the Topp-Leone distribution is\nconsidered. We derive the following estimators: maximum likelihood estimator,\nuniformly minimum variance unbiased estimator, percentile estimator, least\nsquares estimator and weighted least squares estimator. A simulation study\nshows that the maximum likelihood estimator is more efficient than the others\nestimators.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jan 2017 20:15:38 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Benkhelifa", "Lazhar", ""]]}, {"id": "1701.03934", "submitter": "Pieter Segaert", "authors": "Kris Peremans, Pieter Segaert, Stefan Van Aelst, Tim Verdonck", "title": "Robust bootstrap procedures for the chain-ladder method", "comments": null, "journal-ref": null, "doi": "10.1080/03461238.2016.1263236", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Insurers are faced with the challenge of estimating the future reserves\nneeded to handle historic and outstanding claims that are not fully settled. A\nwell-known and widely used technique is the chain-ladder method, which is a\ndeterministic algorithm. To include a stochastic component one may apply\ngeneralized linear models to the run-off triangles based on past claims data.\nAnalytical expressions for the standard deviation of the resulting reserve\nestimates are typically difficult to derive. A popular alternative approach to\nobtain inference is to use the bootstrap technique. However, the standard\nprocedures are very sensitive to the possible presence of outliers. These\natypical observations, deviating from the pattern of the majority of the data,\nmay both inflate or deflate traditional reserve estimates and corresponding\ninference such as their standard errors. Even when paired with a robust\nchain-ladder method, classical bootstrap inference may break down. Therefore,\nwe discuss and implement several robust bootstrap procedures in the claims\nreserving framework and we investigate and compare their performance on both\nsimulated and real data. We also illustrate their use for obtaining the\ndistribution of one year risk measures.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jan 2017 15:33:42 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Peremans", "Kris", ""], ["Segaert", "Pieter", ""], ["Van Aelst", "Stefan", ""], ["Verdonck", "Tim", ""]]}, {"id": "1701.03992", "submitter": "Zhidong Bai", "authors": "Zhidong Bai, Yongchang Hui, Zhihui Lv, Wing-Keung Wong, Zhen-Zhen Zhu", "title": "The Hiemstra-Jones Test Revisited", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The famous Hiemstra-Jones (HJ) test developed by Hiemstra and Jones (1994)\nplays a significant role in studying nonlinear causality. Over the last two\ndecades, there have been numerous applications and theoretical extensions based\non this pioneering work. However, several works note that counterintuitive\nresults are obtained from the HJ test, and some researchers find that the HJ\ntest is seriously over-rejecting in simulation studies. In this paper, we\nreinvestigate HJ's creative 1994 work and find that their proposed estimators\nof the probabilities over different time intervals were not consistent with the\ntarget ones proposed in their criterion. To test HJ's novel hypothesis on\nGranger causality, we propose new estimators of the probabilities defined in\ntheir paper and reestablish the asymptotic properties to induce new tests\nsimilar to those of HJ. Some simulations will also be presented to support our\nfindings.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 04:56:35 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Bai", "Zhidong", ""], ["Hui", "Yongchang", ""], ["Lv", "Zhihui", ""], ["Wong", "Wing-Keung", ""], ["Zhu", "Zhen-Zhen", ""]]}, {"id": "1701.04006", "submitter": "Jon Cockayne", "authors": "Jon Cockayne, Chris Oates, Tim Sullivan, Mark Girolami", "title": "Probabilistic Numerical Methods for PDE-constrained Bayesian Inverse\n  Problems", "comments": null, "journal-ref": null, "doi": "10.1063/1.4985359", "report-no": null, "categories": "stat.ME cs.NA math.NA math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops meshless methods for probabilistically describing\ndiscretisation error in the numerical solution of partial differential\nequations. This construction enables the solution of Bayesian inverse problems\nwhile accounting for the impact of the discretisation of the forward problem.\nIn particular, this drives statistical inferences to be more conservative in\nthe presence of significant solver error. Theoretical results are presented\ndescribing rates of convergence for the posteriors in both the forward and\ninverse problems. This method is tested on a challenging inverse problem with a\nnonlinear forward model.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 08:50:06 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Cockayne", "Jon", ""], ["Oates", "Chris", ""], ["Sullivan", "Tim", ""], ["Girolami", "Mark", ""]]}, {"id": "1701.04093", "submitter": "L\\'eo Belzile", "authors": "Olli Saarela, L\\'eo R. Belzile, David A. Stephens", "title": "A Bayesian view of doubly robust causal inference", "comments": "Author's original version. 21 pages, including supplementary material", "journal-ref": "Biometrika (2016), 103 (3): 667-681", "doi": "10.1093/biomet/asw025", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In causal inference confounding may be controlled either through regression\nadjustment in an outcome model, or through propensity score adjustment or\ninverse probability of treatment weighting, or both. The latter approaches,\nwhich are based on modelling of the treatment assignment mechanism and their\ndoubly robust extensions have been difficult to motivate using formal Bayesian\narguments, in principle, for likelihood-based inferences, the treatment\nassignment model can play no part in inferences concerning the expected\noutcomes if the models are assumed to be correctly specified. On the other\nhand, forcing dependency between the outcome and treatment assignment models by\nallowing the former to be misspecified results in loss of the balancing\nproperty of the propensity scores and the loss of any double robustness. In\nthis paper, we explain in the framework of misspecified models why doubly\nrobust inferences cannot arise from purely likelihood-based arguments, and\ndemonstrate this through simulations. As an alternative to Bayesian propensity\nscore analysis, we propose a Bayesian posterior predictive approach for\nconstructing doubly robust estimation procedures. Our approach appropriately\ndecouples the outcome and treatment assignment models by incorporating the\ninverse treatment assignment probabilities in Bayesian causal inferences as\nimportance sampling weights in Monte Carlo integration.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jan 2017 18:29:25 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Saarela", "Olli", ""], ["Belzile", "L\u00e9o R.", ""], ["Stephens", "David A.", ""]]}, {"id": "1701.04167", "submitter": "Bikramjit Das", "authors": "Anulekha Dhara, Bikramjit Das, and Karthik Natarajan", "title": "Worst-Case Expected Shortfall with Univariate and Bivariate Marginals", "comments": "23 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Worst-case bounds on the expected shortfall risk given only limited\ninformation on the distribution of the random variables has been studied\nextensively in the literature. In this paper, we develop a new worst-case bound\non the expected shortfall when the univariate marginals are known exactly and\nadditional expert information is available in terms of bivariate marginals.\nSuch expert information allows for one to choose from among the many possible\nparametric families of bivariate copulas. By considering a neighborhood of\ndistance $\\rho$ around the bivariate marginals with the Kullback-Leibler\ndivergence measure, we model the trade-off between conservatism in the\nworst-case risk measure and confidence in the expert information. Our bound is\ndeveloped when the only information available on the bivariate marginals forms\na tree structure in which case it is efficiently computable using convex\noptimization. For consistent marginals, as $\\rho$ approaches $\\infty$, the\nbound reduces to the comonotonic upper bound and as $\\rho$ approaches $0$, the\nbound reduces to the worst-case bound with bivariates known exactly. We also\ndiscuss extensions to inconsistent marginals and instances where the expert\ninformation which might be captured using other parameters such as\ncorrelations.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 04:51:52 GMT"}], "update_date": "2017-02-12", "authors_parsed": [["Dhara", "Anulekha", ""], ["Das", "Bikramjit", ""], ["Natarajan", "Karthik", ""]]}, {"id": "1701.04172", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen", "title": "Near Universal Consistency of the Maximum Pseudolikelihood Estimator for\n  Discrete Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum pseudolikelihood (MPL) estimators are useful alternatives to maximum\nlikelihood (ML) estimators when likelihood functions are more difficult to\nmanipulate than their marginal and conditional components. Furthermore, MPL\nestimators subsume a large number of estimation techniques including ML\nestimators, maximum composite marginal likelihood estimators, and maximum\npairwise likelihood estimators. When considering only the estimation of\ndiscrete models (on a possibly countably infinite support), we show that a\nsimple finiteness assumption on an entropy-based measure is sufficient for\nassessing the consistency of the MPL estimator. As a consequence, we\ndemonstrate that the MPL estimator of any discrete model on a bounded support\nwill be consistent. Our result is valid in parametric, semiparametric, and\nnonparametric settings.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 05:21:05 GMT"}, {"version": "v2", "created": "Tue, 17 Jan 2017 04:28:30 GMT"}, {"version": "v3", "created": "Sat, 6 May 2017 11:13:56 GMT"}, {"version": "v4", "created": "Tue, 29 Aug 2017 14:15:45 GMT"}], "update_date": "2017-08-30", "authors_parsed": [["Nguyen", "Hien D.", ""]]}, {"id": "1701.04176", "submitter": "Masayo Hirose", "authors": "Masayo Yoshimori Hirose and Partha Lahiri", "title": "A New Model Variance Estimator for an Area Level Small Area Model to\n  Solve Multiple Problems Simultaneously", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-level normal hierarchical model (NHM) has played a critical role in\nthe theory of small area estimation (SAE), one of the growing areas in\nstatistics with numerous applications in different disciplines. In this paper,\nwe address major well-known shortcomings associated with the empirical best\nlinear unbiased prediction (EBLUP) of a small area mean and its mean squared\nerror (MSE) estimation by considering an appropriate model variance estimator\nthat satisfies multiple properties. The proposed model variance estimator\nsimultaneously (i) improves on the estimation of the related shrinkage factors,\n(ii) protects EBLUP from the common overshrinkage problem, (iii) avoids complex\nbias correction in generating strictly positive second-order unbiased mean\nsquare error (MSE) estimator either by the Taylor series or single parametric\nbootstrap method. The idea of achieving multiple desirable properties in an\nEBLUP method through a suitably devised model variance estimator is the first\nof its kind and holds promise in providing good inferences for small area means\nunder the classical linear mixed model prediction framework. The proposed\nmethodology is also evaluated using a Monte Carlo simulation study and real\ndata analysis.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 05:57:39 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Hirose", "Masayo Yoshimori", ""], ["Lahiri", "Partha", ""]]}, {"id": "1701.04244", "submitter": "Andrew Bruce Duncan", "authors": "Joris Bierkens, Alexandre Bouchard-C\\^ot\\'e, Arnaud Doucet, Andrew B.\n  Duncan, Paul Fearnhead, Thibaut Lienart, Gareth Roberts, Sebastian J. Vollmer", "title": "Piecewise Deterministic Markov Processes for Scalable Monte Carlo on\n  Restricted Domains", "comments": null, "journal-ref": "Statistics & Probability Letters Volume 136, May 2018, Pages\n  148-154", "doi": "10.1016/j.spl.2018.02.021", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Piecewise Deterministic Monte Carlo algorithms enable simulation from a\nposterior distribution, whilst only needing to access a sub-sample of data at\neach iteration. We show how they can be implemented in settings where the\nparameters live on a restricted domain.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 11:18:51 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 11:59:26 GMT"}, {"version": "v3", "created": "Sat, 17 Feb 2018 10:31:07 GMT"}], "update_date": "2020-09-29", "authors_parsed": [["Bierkens", "Joris", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Doucet", "Arnaud", ""], ["Duncan", "Andrew B.", ""], ["Fearnhead", "Paul", ""], ["Lienart", "Thibaut", ""], ["Roberts", "Gareth", ""], ["Vollmer", "Sebastian J.", ""]]}, {"id": "1701.04247", "submitter": "Andrew Bruce Duncan", "authors": "A. B. Duncan, G. A. Pavliotis, K. C. Zygalakis", "title": "Nonreversible Langevin Samplers: Splitting Schemes, Analysis and\n  Implementation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given target density, there exist an infinite number of diffusion\nprocesses which are ergodic with respect to this density. As observed in a\nnumber of papers, samplers based on nonreversible diffusion processes can\nsignificantly outperform their reversible counterparts both in terms of\nasymptotic variance and rate of convergence to equilibrium. In this paper, we\ntake advantage of this in order to construct efficient sampling algorithms\nbased on the Lie-Trotter decomposition of a nonreversible diffusion process\ninto reversible and nonreversible components. We show that samplers based on\nthis scheme can significantly outperform standard MCMC methods, at the cost of\nintroducing some controlled bias. In particular, we prove that numerical\nintegrators constructed according to this decomposition are geometrically\nergodic and characterise fully their asymptotic bias and variance, showing that\nthe sampler inherits the good mixing properties of the underlying nonreversible\ndiffusion. This is illustrated further with a number of numerical examples\nranging from highly correlated low dimensional distributions, to logistic\nregression problems in high dimensions as well as inference for spatial models\nwith many latent variables.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 11:28:50 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Duncan", "A. B.", ""], ["Pavliotis", "G. A.", ""], ["Zygalakis", "K. C.", ""]]}, {"id": "1701.04387", "submitter": "Murilo Soares Pinheiro", "authors": "Murilo S. Pinheiro, Alu\\'isio S. Pinheiro, Denilon S. Carvalho", "title": "A CUSUM approach to the detection of copy-number neutral loss of\n  heterozygosity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several genetic alterations are involved in the genesis and development of\ncancers. The determination of whether and how each genetic alterations\ncontributes to cancer development is fundamental for a complete understanding\nof the human cancer etiology. Loss of heterozygosity (LOH) is one of such\ngenetic phenomenon linked to a variate of diseases and characterized by the\nchange from heterozygosity (the presence of both alleles of a gene) to to\nhomozygosity (presence of only one type of allele) in a particular DNA locus.\nThus identification of DNA regions where LOH has taken place is a important\nissue in the health sciences. In this article we formulate the LOH detection as\nthe identification of change-points in the parameters of a mixture model and\npresent a detection algorithm based on the cumulative sums (CUSUM) method. We\nfound that even under mild contamination our proposal is a fast and reliable\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 18:30:16 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Pinheiro", "Murilo S.", ""], ["Pinheiro", "Alu\u00edsio S.", ""], ["Carvalho", "Denilon S.", ""]]}, {"id": "1701.04405", "submitter": "Murilo Soares Pinheiro", "authors": "Murilo S. Pinheiro, Benilton S. Carvalho, Alu\\'isio S. Pinheiro", "title": "Screening and merging algorithm for the detection of copy-number\n  alterations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We call change-point problem (CPP) the identification of changes in the\nprobabilistic behavior of a sequence of observations. Solving the CPP involves\ndetecting the number and position of such changes. In genetics the study of how\nand what characteristics of a individual's genetic content might contribute to\nthe occurrence and evolution of cancer has fundamental importance in the\ndiagnosis and treatment of such diseases and can be formulated in the framework\nof chage-point analysis. In this article we propose a modification to a\nexisting method of segmentation with the objective of producing a algorithm\nthat is robust to a variety of sampling distributions and that is adequate for\nmore recent method of accessing DNA copy-number which might require a\nrestriction on the minimum length of a altered segment.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 18:31:40 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Pinheiro", "Murilo S.", ""], ["Carvalho", "Benilton S.", ""], ["Pinheiro", "Alu\u00edsio S.", ""]]}, {"id": "1701.04457", "submitter": "Jos\\'e Javier Quinlan", "authors": "J. J. Quinlan, F. A. Quintana and G. L. Page", "title": "Parsimonious Hierarchical Modeling Using Repulsive Distributions", "comments": "36 pages, 9 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Employing nonparametric methods for density estimation has become routine in\nBayesian statistical practice. Models based on discrete nonparametric priors\nsuch as Dirichlet Process Mixture (DPM) models are very attractive choices due\nto their flexibility and tractability. However, a common problem in fitting\nDPMs or other discrete models to data is that they tend to produce a large\nnumber of (sometimes) redundant clusters. In this work we propose a method that\nproduces parsimonious mixture models (i.e. mixtures that discourage the\ncreation of redundant clusters), without sacrificing flexibility or model fit.\nThis method is based on the idea of repulsion, that is, that any two mixture\ncomponents are encouraged to be well separated. We propose a family of\nd-dimensional probability densities whose coordinates tend to repel each other\nin a smooth way. The induced probability measure has a close relation with\nGibbs measures, graph theory and point processes. We investigate its global\nproperties and explore its use in the context of mixture models for density\nestimation. Computational techniques are detailed and we illustrate its\nusefulness with some well-known data sets and a small simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 21:11:46 GMT"}, {"version": "v2", "created": "Thu, 29 Jun 2017 20:26:31 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Quinlan", "J. J.", ""], ["Quintana", "F. A.", ""], ["Page", "G. L.", ""]]}, {"id": "1701.04485", "submitter": "Patrick McDermott", "authors": "Patrick L. McDermott, Christopher K. Wikle, Joshua Millspaugh", "title": "A Hierarchical Spatio-Temporal Analog Forecasting Model for Count Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  1. Analog forecasting has been successful at producing robust forecasts for a\nvariety of ecological and physical processes. Analog forecasting is a\nmechanism-free nonlinear method that forecasts a system forward in time by\nexamining how past states deemed similar to the current state moved forward.\nPrevious work on analog forecasting has typically been presented in an\nempirical or heuristic context, as opposed to a formal statistical context. 2.\nThe model presented here extends the model-based analog method of McDermott and\nWikle (2016) by placing analog forecasting within a fully hierarchical\nstatistical frame- work. In particular, a Bayesian hierarchical\nspatial-temporal Poisson analog forecasting model is formulated. 3. In\ncomparison to a Poisson Bayesian hierarchical model with a latent dynamical\nspatio- temporal process, the hierarchical analog model consistently produced\nmore accurate forecasts. By using a Bayesian approach, the hierarchical analog\nmodel is able to quantify rigorously the uncertainty associated with forecasts.\n4. Forecasting waterfowl settling patterns in the northwestern United States\nand Canada is conducted by applying the hierarchical analog model to a breeding\npopulation survey dataset. Sea Surface Temperature (SST) in the Pacific ocean\nis used to help identify potential analogs for the waterfowl settling patterns.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jan 2017 23:17:52 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["McDermott", "Patrick L.", ""], ["Wikle", "Christopher K.", ""], ["Millspaugh", "Joshua", ""]]}, {"id": "1701.04512", "submitter": "Hien Nguyen", "authors": "Hien D Nguyen, Geoffrey J McLachlan", "title": "Some Theoretical Results Regarding the Polygonal Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The polygonal distributions are a class of distributions that can be defined\nvia the mixture of triangular distributions over the unit interval. The class\nincludes the uniform and trapezoidal distributions, and is an alternative to\nthe beta distribution. We demonstrate that the polygonal densities are dense in\nthe class of continuous and concave densities with bounded second derivatives.\nPointwise consistency and Hellinger consistency results for the maximum\nlikelihood (ML) estimator are obtained. A useful model selection theorem is\nstated as well as results for a related distribution that is obtained via the\npointwise square of polygonal density functions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 02:24:19 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Nguyen", "Hien D", ""], ["McLachlan", "Geoffrey J", ""]]}, {"id": "1701.04605", "submitter": "Panagiotis Papastamoulis", "authors": "Panagiotis Papastamoulis", "title": "Overfitting Bayesian Mixtures of Factor Analyzers with an Unknown Number\n  of Components", "comments": "Computational Statistics and Data Analysis (to appear)", "journal-ref": "Computational Statistics & Data Analysis, 2018", "doi": "10.1016/j.csda.2018.03.007", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances on overfitting Bayesian mixture models provide a solid and\nstraightforward approach for inferring the underlying number of clusters and\nmodel parameters in heterogeneous datasets. The applicability of such a\nframework in clustering correlated high dimensional data is demonstrated. For\nthis purpose an overfitting mixture of factor analyzers is introduced, assuming\nthat the number of factors is fixed. A Markov chain Monte Carlo (MCMC) sampler\ncombined with a prior parallel tempering scheme is used to estimate the\nposterior distribution of model parameters. The optimal number of factors is\nestimated using information criteria. Identifiability issues related to the\nlabel switching problem are dealt by post-processing the simulated MCMC sample\nby relabelling algorithms. The method is benchmarked against state-of-the-art\nsoftware for maximum likelihood estimation of mixtures of factor analyzers\nusing an extensive simulation study. Finally, the applicability of the method\nis illustrated in publicly available data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 10:13:49 GMT"}, {"version": "v2", "created": "Sun, 2 Apr 2017 19:56:35 GMT"}, {"version": "v3", "created": "Mon, 5 Jun 2017 08:58:04 GMT"}, {"version": "v4", "created": "Mon, 26 Feb 2018 08:21:43 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Papastamoulis", "Panagiotis", ""]]}, {"id": "1701.04735", "submitter": "Gane Samb Lo", "authors": "Pape Djiby Mergane, Cheikh Mohamed Haidara, Cheikh Tidiane Seck, Gane\n  Samb Lo", "title": "Asymptotic Theory and Statistical Decomposability gap Estimation for\n  Takayama's Index", "comments": "2", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the spirit of recent asymptotic works on the General Poverty Index (GPI)\nin the field of Welfare Analysis, the asymptotic representation of the\nnon-decomposable Takayama's index, which has failed to be incorporated in the\nunified GPI approach, is addressed and established here. This representation\nallows also to extend to it, recent results of statistical decomposability gaps\nestimations. The theoretical results are applied to real databases. The\nconclusions of the undertaken applications recommend to use Takayama's index as\na practically decomposable one, in virtue of the low decomposability gaps with\nrespect to the large values of the index.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 15:46:21 GMT"}], "update_date": "2017-01-18", "authors_parsed": [["Mergane", "Pape Djiby", ""], ["Haidara", "Cheikh Mohamed", ""], ["Seck", "Cheikh Tidiane", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1701.04846", "submitter": "Claudia Kirch", "authors": "Claudia Kirch, Matthew C. Edwards, Alexander Meier, Renate Meyer", "title": "Beyond Whittle: Nonparametric correction of a parametric likelihood with\n  a focus on Bayesian time series analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Whittle likelihood is widely used for Bayesian nonparametric estimation\nof the spectral density of stationary time series. However, the loss of\nefficiency for non-Gaussian time series can be substantial. On the other hand,\nparametric methods are more powerful if the model is well-specified, but may\nfail entirely otherwise. Therefore, we suggest a nonparametric correction of a\nparametric likelihood taking advantage of the efficiency of parametric models\nwhile mitigating sensitivities through a nonparametric amendment. Using a\nBernstein-Dirichlet prior for the nonparametric spectral correction, we show\nposterior consistency and illustrate the performance of our procedure in a\nsimulation study and with LIGO gravitational wave data.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 19:37:47 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Kirch", "Claudia", ""], ["Edwards", "Matthew C.", ""], ["Meier", "Alexander", ""], ["Meyer", "Renate", ""]]}, {"id": "1701.04889", "submitter": "Abhishek Chakrabortty", "authors": "Abhishek Chakrabortty and Tianxi Cai", "title": "Efficient and Adaptive Linear Regression in Semi-Supervised Settings", "comments": "51 pages; Revised version - to appear in The Annals of Statistics", "journal-ref": "The Annals of Statistics 2018, Vol. 46, No. 4, 1541-1572", "doi": "10.1214/17-AOS1594", "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the linear regression problem under semi-supervised settings\nwherein the available data typically consists of: (i) a small or moderate sized\n'labeled' data, and (ii) a much larger sized 'unlabeled' data. Such data arises\nnaturally from settings where the outcome, unlike the covariates, is expensive\nto obtain, a frequent scenario in modern studies involving large databases like\nelectronic medical records (EMR). Supervised estimators like the ordinary least\nsquares (OLS) estimator utilize only the labeled data. It is often of interest\nto investigate if and when the unlabeled data can be exploited to improve\nestimation of the regression parameter in the adopted linear model.\n  In this paper, we propose a class of 'Efficient and Adaptive Semi-Supervised\nEstimators' (EASE) to improve estimation efficiency. The EASE are two-step\nestimators adaptive to model mis-specification, leading to improved (optimal in\nsome cases) efficiency under model mis-specification, and equal (optimal)\nefficiency under a linear model. This adaptive property, often unaddressed in\nthe existing literature, is crucial for advocating 'safe' use of the unlabeled\ndata. The construction of EASE primarily involves a flexible\n'semi-non-parametric' imputation, including a smoothing step that works well\neven when the number of covariates is not small; and a follow up 'refitting'\nstep along with a cross-validation (CV) strategy both of which have useful\npractical as well as theoretical implications towards addressing two important\nissues: under-smoothing and over-fitting. We establish asymptotic results\nincluding consistency, asymptotic normality and the adaptive properties of\nEASE. We also provide influence function expansions and a 'double' CV strategy\nfor inference. The results are further validated through extensive simulations,\nfollowed by application to an EMR study on auto-immunity.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jan 2017 22:29:22 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 18:15:39 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Chakrabortty", "Abhishek", ""], ["Cai", "Tianxi", ""]]}, {"id": "1701.05128", "submitter": "Xiliang Lu", "authors": "Jian Huang, Yuling Jiao, Yanyan Liu and Xiliang Lu", "title": "A Constructive Approach to High-dimensional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a constructive approach to estimating sparse, high-dimensional\nlinear regression models. The approach is a computational algorithm motivated\nfrom the KKT conditions for the $\\ell_0$-penalized least squares solutions. It\ngenerates a sequence of solutions iteratively, based on support detection using\nprimal and dual information and root finding. We refer to the algorithm as SDAR\nfor brevity. Under a sparse Rieze condition on the design matrix and certain\nother conditions, we show that with high probability, the $\\ell_2$ estimation\nerror of the solution sequence decays exponentially to the minimax error bound\nin $O(\\sqrt{J}\\log(R))$ steps; and under a mutual coherence condition and\ncertain other conditions, the $\\ell_{\\infty}$ estimation error decays to the\noptimal error bound in $O(\\log(R))$ steps, where $J$ is the number of important\npredictors, $R$ is the relative magnitude of the nonzero target coefficients.\nComputational complexity analysis shows that the cost of SDAR is $O(np)$ per\niteration. Moreover the oracle least squares estimator can be exactly recovered\nwith high probability at the same cost if we know the sparsity level. We also\nconsider an adaptive version of SDAR to make it more practical in applications.\nNumerical comparisons with Lasso, MCP and greedy methods demonstrate that SDAR\nis competitive with or outperforms them in accuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 16:13:24 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Huang", "Jian", ""], ["Jiao", "Yuling", ""], ["Liu", "Yanyan", ""], ["Lu", "Xiliang", ""]]}, {"id": "1701.05132", "submitter": "Michael Lopez", "authors": "Michael J Lopez, Roee Gutman", "title": "Estimation of causal effects with multiple treatments: a review and new\n  ideas", "comments": null, "journal-ref": "Statistical Science, 2017", "doi": "10.1214/17-STS612", "report-no": "Volume 32, Number 3", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The propensity score is a common tool for estimating the causal effect of a\nbinary treatment in observational data. In this setting, matching,\nsubclassification, imputation, or inverse probability weighting on the\npropensity score can reduce the initial covariate bias between the treatment\nand control groups. With more than two treatment options, however, estimation\nof causal effects requires additional assumptions and techniques, the\nimplementations of which have varied across disciplines. This paper reviews\ncurrent methods, and it identifies and contrasts the treatment effects that\neach one estimates. Additionally, we propose possible matching techniques for\nuse with multiple, nominal categorical treatments, and use simulations to show\nhow such algorithms can yield improved covariate similarity between those in\nthe matched sets, relative the pre-matched cohort. To sum, this manuscript\nprovides a synopsis of how to notate and use causal methods for categorical\ntreatments.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 16:21:26 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 18:33:32 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Lopez", "Michael J", ""], ["Gutman", "Roee", ""]]}, {"id": "1701.05146", "submitter": "Espen Bernton", "authors": "Espen Bernton (Harvard University), Pierre E. Jacob (Harvard\n  University), Mathieu Gerber (University of Bristol), Christian P. Robert\n  (Universit\\'e Paris-Dauphine, PSL and University of Warwick)", "title": "On parameter estimation with the Wasserstein distance", "comments": "29 pages (+18 pages of appendices), 6 figures. To appear in\n  Information and Inference: A Journal of the IMA. A previous version of this\n  paper contained work on approximate Bayesian computation with the Wasserstein\n  distance, which can now be found at arxiv:1905.03747", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference can be performed by minimizing, over the parameter\nspace, the Wasserstein distance between model distributions and the empirical\ndistribution of the data. We study asymptotic properties of such minimum\nWasserstein distance estimators, complementing results derived by Bassetti,\nBodini and Regazzini in 2006. In particular, our results cover the misspecified\nsetting, in which the data-generating process is not assumed to be part of the\nfamily of distributions described by the model. Our results are motivated by\nrecent applications of minimum Wasserstein estimators to complex generative\nmodels. We discuss some difficulties arising in the approximation of these\nestimators and illustrate their behavior in several numerical experiments. Two\nof our examples are taken from the literature on approximate Bayesian\ncomputation and have likelihood functions that are not analytically tractable.\nTwo other examples involve misspecified models.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 16:59:55 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 22:47:12 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 02:03:12 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Bernton", "Espen", "", "Harvard University"], ["Jacob", "Pierre E.", "", "Harvard\n  University"], ["Gerber", "Mathieu", "", "University of Bristol"], ["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine, PSL and University of Warwick"]]}, {"id": "1701.05179", "submitter": "Nikolaos Ignatiadis", "authors": "Nikolaos Ignatiadis and Wolfgang Huber", "title": "Covariate powered cross-weighted multiple testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental task in the analysis of datasets with many variables is\nscreening for associations. This can be cast as a multiple testing task, where\nthe objective is achieving high detection power while controlling type I error.\nWe consider $m$ hypothesis tests represented by pairs $((P_i, X_i))_{1\\leq i\n\\leq m}$ of p-values $P_i$ and covariates $X_i$, such that $P_i \\perp X_i$ if\n$H_i$ is null. Here, we show how to use information potentially available in\nthe covariates about heterogeneities among hypotheses to increase power\ncompared to conventional procedures that only use the $P_i$. To this end, we\nupgrade existing weighted multiple testing procedures through the Independent\nHypothesis Weighting (IHW) framework to use data-driven weights that are\ncalculated as a function of the covariates. Finite sample guarantees, e.g.,\nfalse discovery rate (FDR) control, are derived from cross-weighting, a\ndata-splitting approach that enables learning the weight-covariate function\nwithout overfitting as long as the hypotheses can be partitioned into\nindependent folds, with arbitrary within-fold dependence. IHW has increased\npower compared to methods that do not use covariate information. A key\nimplication of IHW is that hypothesis rejection in common multiple testing\nsetups should not proceed according to the ranking of the p-values, but by an\nalternative ranking implied by the covariate-weighted p-values.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 18:42:26 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2018 08:42:05 GMT"}, {"version": "v3", "created": "Wed, 11 Jul 2018 17:52:28 GMT"}, {"version": "v4", "created": "Sun, 2 Feb 2020 23:29:17 GMT"}, {"version": "v5", "created": "Sat, 17 Oct 2020 17:34:52 GMT"}, {"version": "v6", "created": "Mon, 21 Jun 2021 15:14:39 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ignatiadis", "Nikolaos", ""], ["Huber", "Wolfgang", ""]]}, {"id": "1701.05223", "submitter": "Santosh Yadav", "authors": "Santosh Kumar Yadav, Rohit Sinha, Prabin Kumar Bora", "title": "A Fast Data Driven Shrinkage of Singular Values for Arbitrary Rank\n  Signal Matrix Denoising", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recovering a low-rank signal matrix from its noisy observation, commonly\nknown as matrix denoising, is a fundamental inverse problem in statistical\nsignal processing. Matrix denoising methods are generally based on shrinkage or\nthresholding of singular values with a predetermined shrinkage parameter or\nthreshold. However, most of the existing adaptive shrinkage methods use\nmultiple parameters to obtain a better flexibility in shrinkage. The optimal\nvalue of these parameters using either cross-validation or Stein's principle.\nIn both the cases, the iterative estimation of various parameters render the\nexisting shrinkage methods computationally latent for most of the real-time\napplications. This paper presents an efficient data dependent shrinkage\nfunction whose parameters are estimated using Stein's principle but in a\nnon-iterative manner, thereby providing a comparatively faster shrinkage\nmethod. In addition, the proposed estimator is found to be consistent with the\nrecently proposed asymptotically optimal estimators using the results from\nrandom matrix theory. The experimental studies on artificially generated\nlow-rank matrices and on the magnetic resonant imagery data, show the efficacy\nof the proposed denoising method.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 02:45:01 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 04:49:57 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Yadav", "Santosh Kumar", ""], ["Sinha", "Rohit", ""], ["Bora", "Prabin Kumar", ""]]}, {"id": "1701.05230", "submitter": "Abhishek Chakrabortty", "authors": "Abhishek Chakrabortty, Matey Neykov, Raymond Carroll and Tianxi Cai", "title": "Surrogate Aided Unsupervised Recovery of Sparse Signals in Single Index\n  Models for Binary Outcomes", "comments": "50 pages, 3 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the recovery of regression coefficients, denoted by\n$\\boldsymbol{\\beta}_0$, for a single index model (SIM) relating a binary\noutcome $Y$ to a set of possibly high dimensional covariates $\\boldsymbol{X}$,\nbased on a large but 'unlabeled' dataset $\\mathcal{U}$, with $Y$ never\nobserved. On $\\mathcal{U}$, we fully observe $\\boldsymbol{X}$ and additionally,\na surrogate $S$ which, while not being strongly predictive of $Y$ throughout\nthe entirety of its support, can forecast it with high accuracy when it assumes\nextreme values. Such datasets arise naturally in modern studies involving large\ndatabases such as electronic medical records (EMR) where $Y$, unlike\n$(\\boldsymbol{X}, S)$, is difficult and/or expensive to obtain. In EMR studies,\nan example of $Y$ and $S$ would be the true disease phenotype and the count of\nthe associated diagnostic codes respectively. Assuming another SIM for $S$\ngiven $\\boldsymbol{X}$, we show that under sparsity assumptions, we can recover\n$\\boldsymbol{\\beta}_0$ proportionally by simply fitting a least squares LASSO\nestimator to the subset of the observed data on $(\\boldsymbol{X}, S)$\nrestricted to the extreme sets of $S$, with $Y$ imputed using the surrogacy of\n$S$. We obtain sharp finite sample performance bounds for our estimator,\nincluding deterministic deviation bounds and probabilistic guarantees. We\ndemonstrate the effectiveness of our approach through multiple simulation\nstudies, as well as by application to real data from an EMR study conducted at\nthe Partners HealthCare Systems.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jan 2017 20:51:27 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 17:59:30 GMT"}, {"version": "v3", "created": "Sun, 1 Jul 2018 01:09:51 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Chakrabortty", "Abhishek", ""], ["Neykov", "Matey", ""], ["Carroll", "Raymond", ""], ["Cai", "Tianxi", ""]]}, {"id": "1701.05358", "submitter": "Saeid Rezakhah", "authors": "Ferdous Mohammadi and Saeid Rezakhah", "title": "Smooth Transition HYGARCH Model: Stability and Forecasting", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  HYGARCH process is the commonly used long memory process in modeling the\nlong-rang dependence in volatility.\n  Financial time series are characterized by transition between phases of\ndifferent volatility levels. The smooth transition HYGARCH (ST-HYGARCH) model\nis proposed to model time-varying structure with long memory property. The\nasymptotic behavior of the second moment is studied and an upper bound for it\nis derived. A score test is developed to check the smooth transition property.\nThe asymptotic behavior of the proposed model and the score test is examined by\nsimulation. The proposed model is applied to the \\textit{S}\\&\\textit{P}500\nindices for some period which show evidence of smooth transition property and\ndemonstrates out-performance of the ST-HYGARCH than HYGARCH in forecasting.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 10:26:48 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Mohammadi", "Ferdous", ""], ["Rezakhah", "Saeid", ""]]}, {"id": "1701.05441", "submitter": "Amir Payandeh Dr", "authors": "Amir T. Payandeh Najafabadi and Mansoureh Sakizadeh", "title": "Designing an Optimal Bonus--Malus System Using the Number of Reported\n  Claims, Steady-State Distribution, and Mixture Claim Size Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article, in a first step, considers two Bayes estimators for the\nrelativity premium of a given Bonus--Malus system. It then develops a linear\nrelativity premium that closes, in the sense of weighted mean square error\nloss, to such Bayes estimators. In a second step, it supposes that the claim\nsize distribution for a given Bonus--Malus system can be formulated as a finite\nmixture distribution. It then evaluates the base premium under a Bayesian\nframework for such a finite mixture distribution. The Loimaranta efficiency of\nsuch a linear relativity premium, for several Bonus--Malus systems, has been\ncompared with two Bayes and ordinary linear relativity premiums.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 14:41:20 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Najafabadi", "Amir T. Payandeh", ""], ["Sakizadeh", "Mansoureh", ""]]}, {"id": "1701.05447", "submitter": "Amir Payandeh Dr", "authors": "Amir T. Payandeh Najafabadi and Ali Panahi Bazaz", "title": "An Optimal Multi-layer Reinsurance Policy under Conditional Tail\n  Expectation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A usual reinsurance policy for insurance companies admits one or two layers\nof the payment deductions. Under optimal criterion of minimizing the\nconditional tail expectation (CTE) risk measure of the insurer's total risk,\nthis article generalized an optimal stop-loss reinsurance policy to an optimal\nmulti-layer reinsurance policy. To achieve such optimal multi-layer reinsurance\npolicy, this article starts from a given optimal stop-loss reinsurance policy\n$f(\\cdot).$ In the first step, it cuts down an interval $[0,\\infty)$ into two\nintervals $[0,M_1)$ and $[M_1,\\infty).$ By shifting the origin of Cartesian\ncoordinate system to $(M_{1},f(M_{1})),$ and showing that under the $CTE$\ncriteria $f(x)I_{[0, M_1)}(x)+(f(M_1)+f(x-M_1))I_{[M_1,\\infty)}(x)$ is, again,\nan optimal policy. This extension procedure can be repeated to obtain an\noptimal k-layer reinsurance policy. Finally, unknown parameters of the optimal\nmulti-layer reinsurance policy are estimated using some additional appropriate\ncriteria. Three simulation-based studies have been conducted to demonstrate:\n({\\bf 1}) The practical applications of our findings and ({\\bf 2}) How one may\nemploy other appropriate criteria to estimate unknown parameters of an optimal\nmulti-layer contract. The multi-layer reinsurance policy, similar to the\noriginal stop-loss reinsurance policy is optimal, in a same sense. Moreover it\nhas some other optimal criteria which the original policy does not have. Under\noptimal criterion of minimizing general translative and monotone risk measure\n$\\rho(\\cdot)$ of {\\it either} the insurer's total risk {\\it or} both the\ninsurer's and the reinsurer's total risks, this article (in its discussion)\nalso extends a given optimal reinsurance contract $f(\\cdot)$ to a multi-layer\nand continuous reinsurance policy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 14:51:11 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Najafabadi", "Amir T. Payandeh", ""], ["Bazaz", "Ali Panahi", ""]]}, {"id": "1701.05450", "submitter": "Amir Payandeh Dr", "authors": "Amir T. Payandeh-Najafabadi and Ali Panahi-Bazaz", "title": "An Optimal Combination of Proportional and Stop-Loss Reinsurance\n  Contracts From Insurer's and Reinsurer's Viewpoints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A reinsurance contract should address the conflicting interests of the\ninsurer and reinsurer. Most of existing optimal reinsurance contracts only\nconsiders the interests of one party. This article combines the proportional\nand stop-loss reinsurance contracts and introduces a new reinsurance contract\ncalled proportional-stop-loss reinsurance. Using the balanced loss function,\nunknown parameters of the proportional-stop-loss reinsurance have been\nestimated such that the expected surplus for both the insurer and reinsurer are\nmaximized. Several characteristics for the new reinsurance are provided.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 14:57:39 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Payandeh-Najafabadi", "Amir T.", ""], ["Panahi-Bazaz", "Ali", ""]]}, {"id": "1701.05452", "submitter": "Amir Payandeh Dr", "authors": "Amir T. Payandeh Najafabadi and Saeed MohammadPour", "title": "A k-Inflated Negative Binomial Mixture Regression Model: Application to\n  Rate--Making Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article introduces a k-Inflated Negative Binomial mixture\ndistribution/regression model as a more flexible alternative to zero-inflated\nPoisson distribution/regression model. An EM algorithm has been employed to\nestimate the model's parameters. Then, such new model along with a Pareto\nmixture model have been employed to design an optimal rate--making system.\nNamely, this article employs number/size of reported claims of Iranian third\nparty insurance dataset. Then, it employs the k-Inflated Negative Binomial\nmixture distribution/regression model as well as other well developed counting\nmodels along with a Pareto mixture model to model frequency/severity of\nreported claims in Iranian third party insurance dataset. Such numerical\nillustration shows that: ({\\bf 1}) the k-Inflated Negative Binomial mixture\nmodels provide more fair rate/pure premiums for policyholders under a\nrate--making system; and ({\\bf 2}) in the situation that number of reported\nclaims uniformly distributed in past experience of a policyholder (for instance\n$k_1=1$ and $k_2=1$ instead of $k_1=0$ and $k_2=2$). The rate/pure premium\nunder the k-Inflated Negative Binomial mixture models are more appealing and\nacceptable.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 15:00:55 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Najafabadi", "Amir T. Payandeh", ""], ["MohammadPour", "Saeed", ""]]}, {"id": "1701.05530", "submitter": "Frank Marrs", "authors": "Frank W. Marrs and Bailey K. Fosdick and Tyler H. McCormick", "title": "Regression of exchangeable relational arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational arrays represent measures of association between pairs of actors,\noften in varied contexts or over time. Such data appear as trade flows between\ncountries, financial transactions between individuals, contact frequencies\nbetween school children in classrooms, and dynamic protein-protein\ninteractions. Elements of a relational array are often modeled as a linear\nfunction of observable covariates, where the regression coefficients are the\nsubjects of inference. The structure of the relational array engenders\ndependence among relations that involve the same actor. Uncertainty estimates\nfor regression coefficient estimators -- and ideally the coefficient estimators\nthemselves -- must account for this relational dependence. Existing estimators\nof standard errors that recognize such relational dependence rely on estimating\nextremely complex, heterogeneous structure across actors. This paper proposes a\nnew class of parsimonious coefficient and standard error estimators for\nregressions of relational arrays. We leverage an exchangeability assumption to\nderive standard error estimators that pool information across actors and are\nsubstantially more accurate than existing estimators in a variety of settings.\nThis exchangeability assumption is pervasive in network and array models in the\nstatistics literature, but not previously considered when adjusting for\ndependence in a regression setting with relational data. We demonstrate\nimprovements in inference theoretically, via a simulation study, and by\nanalysis of a data set involving international trade.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 17:56:10 GMT"}, {"version": "v2", "created": "Mon, 8 May 2017 18:23:46 GMT"}, {"version": "v3", "created": "Fri, 13 Apr 2018 15:48:43 GMT"}, {"version": "v4", "created": "Tue, 16 Feb 2021 14:33:20 GMT"}], "update_date": "2021-02-17", "authors_parsed": [["Marrs", "Frank W.", ""], ["Fosdick", "Bailey K.", ""], ["McCormick", "Tyler H.", ""]]}, {"id": "1701.05547", "submitter": "Simon Mak", "authors": "Simon Mak, C. F. Jeff Wu", "title": "cmenet: a new method for bi-level variable selection of conditional main\n  effects", "comments": "JASA T&M, under revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel method for selecting main effects and a set of\nreparametrized effects called conditional main effects (CMEs), which capture\nthe conditional effect of a factor at a fixed level of another factor. CMEs\nrepresent interpretable, domain-specific phenomena for a wide range of\napplications in engineering, social sciences and genomics. The key challenge is\nin incorporating the implicit grouped structure of CMEs within the variable\nselection procedure itself. We propose a new method, cmenet, which employs two\nprinciples called CME coupling and CME reduction to effectively navigate the\nselection algorithm. Simulation studies demonstrate the improved CME selection\nperformance of cmenet over more generic selection methods. Applied to a gene\nassociation study on fly wing shape, cmenet not only yields more parsimonious\nmodels and improved predictive performance over standard two-factor interaction\nanalysis methods, but also reveals important insights on gene activation\nbehavior, which can be used to guide further experiments. Efficient\nimplementations of our algorithms are available in the R package cmenet in\nCRAN.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 18:36:45 GMT"}, {"version": "v2", "created": "Sat, 18 Nov 2017 21:07:53 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Mak", "Simon", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "1701.05553", "submitter": "Casey Kneale", "authors": "Casey Kneale, Dominic Poerio, Karl S. Booksh", "title": "Optimized Spatial Partitioning via Minimal Swarm Intelligence", "comments": "To be submitted to the Journal of Technometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimized spatial partitioning algorithms are the corner stone of many\nsuccessful experimental designs and statistical methods. Of these algorithms,\nthe Centroidal Voronoi Tessellation (CVT) is the most widely utilized. CVT\nbased methods require global knowledge of spatial boundaries, do not readily\nallow for weighted regions, have challenging implementations, and are\ninefficiently extended to high dimensional spaces. We describe two simple\npartitioning schemes based on nearest and next nearest neighbor locations which\neasily incorporate these features at the slight expense of optimal placement.\nSeveral novel qualitative techniques which assess these partitioning schemes\nare also included. The feasibility of autonomous uninformed sensor networks\nutilizing these algorithms are considered. Some improvements in particle swarm\noptimizer results on multimodal test functions from partitioned initial\npositions in two space are also illustrated. Pseudo code for all of the novel\nalgorithms depicted here-in is available in the supplementary information of\nthis manuscript.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 18:59:04 GMT"}], "update_date": "2017-01-20", "authors_parsed": [["Kneale", "Casey", ""], ["Poerio", "Dominic", ""], ["Booksh", "Karl S.", ""]]}, {"id": "1701.05593", "submitter": "Peyman Tavallali", "authors": "Peyman Tavallali, Marianne Razavi, Sean Brady", "title": "Parameter Selection Algorithm For Continuous Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a new algorithm for supervised learning methods,\nby which one can both capture the non-linearity in data and also find the best\nsubset model. To produce an enhanced subset of the original variables, an ideal\nselection method should have the potential of adding a supplementary level of\nregression analysis that would capture complex relationships in the data via\nmathematical transformation of the predictors and exploration of synergistic\neffects of combined variables. The method that we present here has the\npotential to produce an optimal subset of variables, rendering the overall\nprocess of model selection to be more efficient. The core objective of this\npaper is to introduce a new estimation technique for the classical least square\nregression framework. This new automatic variable transformation and model\nselection method could offer an optimal and stable model that minimizes the\nmean square error and variability, while combining all possible subset\nselection methodology and including variable transformations and interaction.\nMoreover, this novel method controls multicollinearity, leading to an optimal\nset of explanatory variables.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 20:35:31 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Tavallali", "Peyman", ""], ["Razavi", "Marianne", ""], ["Brady", "Sean", ""]]}, {"id": "1701.05638", "submitter": "Francisco Javier Rubio", "authors": "Cristiano Villa and Francisco J. Rubio", "title": "Objective priors for the number of degrees of freedom of a multivariate\n  t distribution and the t-copula", "comments": "To appear in Computational Statistics and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An objective Bayesian approach to estimate the number of degrees of freedom\n$(\\nu)$ for the multivariate $t$ distribution and for the $t$-copula, when the\nparameter is considered discrete, is proposed. Inference on this parameter has\nbeen problematic for the multivariate $t$ and, for the absence of any method,\nfor the $t$-copula. An objective criterion based on loss functions which allows\nto overcome the issue of defining objective probabilities directly is employed.\nThe support of the prior for $\\nu$ is truncated, which derives from the\nproperty of both the multivariate $t$ and the $t$-copula of convergence to\nnormality for a sufficiently large number of degrees of freedom. The\nperformance of the priors is tested on simulated scenarios. The R codes and the\nreplication material are available as a supplementary material of the\nelectronic version of the paper and on real data: daily logarithmic returns of\nIBM and of the Center for Research in Security Prices Database.\n", "versions": [{"version": "v1", "created": "Thu, 19 Jan 2017 23:09:08 GMT"}, {"version": "v2", "created": "Sat, 15 Jul 2017 12:38:04 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 08:59:19 GMT"}, {"version": "v4", "created": "Tue, 13 Mar 2018 08:17:31 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Villa", "Cristiano", ""], ["Rubio", "Francisco J.", ""]]}, {"id": "1701.05647", "submitter": "Gaorong Li", "authors": "Xiujuan Yang, Suigen Yang and Gaorong Li", "title": "Simultaneous Confidence Band for Partially Linear Panel Data Models with\n  Fixed Effects", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we construct the simultaneous confidence band (SCB) for the\nnonparametric component in partially linear panel data models with fixed\neffects. We remove the fixed effects, and further obtain the estimators of\nparametric and nonparametric components, which do not depend on the fixed\neffects. We establish the asymptotic distribution of their maximum absolute\ndeviation between the estimated nonparametric component and the true\nnonparametric component under some suitable conditions, and hence the result\ncan be used to construct the simultaneous confidence band of the nonparametric\ncomponent. Based on the asymptotic distribution, it becomes difficult for the\nconstruction of the simultaneous confidence band. The reason is that the\nasymptotic distribution involves the estimators of the asymptotic bias and\nconditional variance, and the choice of the bandwidth for estimating the second\nderivative of nonparametric function. Clearly, these will cause computational\nburden and accumulative errors. To overcome these problems, we propose a\nBootstrap method to construct simultaneous confidence band. Simulation studies\nindicate that the proposed Bootstrap method exhibits better performance under\nthe limited samples.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 00:26:37 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["Yang", "Xiujuan", ""], ["Yang", "Suigen", ""], ["Li", "Gaorong", ""]]}, {"id": "1701.05656", "submitter": "Sutanoy Dasgupta", "authors": "Sutanoy Dasgupta, Debdeep Pati, Anuj Srivastava", "title": "A Two-Step Geometric Framework For Density Modeling", "comments": "Submitted to a journal currently. Sections rewritten and arranged\n  differently from previous version. Some errors in proofs and notations\n  corrected. Section on Bivariate density estimation and Irrelevant predictors\n  removed. Performance of conditional density estimation compared to a\n  different package with different simulation examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel two-step approach for estimating a probability density\nfunction (pdf) given its samples, with the second and important step coming\nfrom a geometric formulation. The procedure involves obtaining an initial\nestimate of the pdf and then transforming it via a warping function to reach\nthe final estimate. The initial estimate is intended to be computationally\nfast, albeit suboptimal, but its warping creates a larger, flexible class of\ndensity functions, resulting in substantially improved estimation. The search\nfor optimal warping is accomplished by mapping diffeomorphic functions to the\ntangent space of a Hilbert sphere, a vector space whose elements can be\nexpressed using an orthogonal basis. Using a truncated basis expansion, we\nestimate the optimal warping under a (penalized) likelihood criterion and,\nthus, the optimal density estimate. This framework is introduced for\nunivariate, unconditional pdf estimation and then extended to conditional pdf\nestimation. The approach avoids many of the computational pitfalls associated\nwith classical conditional-density estimation methods, without losing on\nestimation performance. We derive asymptotic convergence rates of the density\nestimator and demonstrate this approach using both synthetic datasets and real\ndata, the latter relating to the association of a toxic metabolite on preterm\nbirth.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 02:14:08 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 20:41:22 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Dasgupta", "Sutanoy", ""], ["Pati", "Debdeep", ""], ["Srivastava", "Anuj", ""]]}, {"id": "1701.05666", "submitter": "Yifei Yan", "authors": "Yifei Yan and Athanasios Kottas", "title": "A New Family of Error Distributions for Bayesian Quantile Regression", "comments": "Presented at the 10th ICSA international conference in Shanghai,\n  China (December 19-22, 2016); submitted to Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new family of error distributions for model-based quantile\nregression, which is constructed through a structured mixture of normal\ndistributions. The construction enables fixing specific percentiles of the\ndistribution while, at the same time, allowing for varying mode, skewness and\ntail behavior. It thus overcomes the severe limitation of the asymmetric\nLaplace distribution -- the most commonly used error model for parametric\nquantile regression -- for which the skewness of the error density is fully\nspecified when a particular percentile is fixed. We develop a Bayesian\nformulation for the proposed quantile regression model, including conditional\nlasso regularized quantile regression based on a hierarchical Laplace prior for\nthe regression coefficients, and a Tobit quantile regression model. Posterior\ninference is implemented via Markov Chain Monte Carlo methods. The flexibility\nof the new model relative to the asymmetric Laplace distribution is studied\nthrough relevant model properties, and through a simulation experiment to\ncompare the two error distributions in regularized quantile regression.\nMoreover, model performance in linear quantile regression, regularized quantile\nregression, and Tobit quantile regression is illustrated with data examples\nthat have been previously considered in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 03:14:19 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 03:43:52 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Yan", "Yifei", ""], ["Kottas", "Athanasios", ""]]}, {"id": "1701.05870", "submitter": "Davide Pigoli", "authors": "Alessandra Cabassi, Davide Pigoli, Piercesare Secchi, Patrick A.\n  Carter", "title": "Permutation tests for the equality of covariance operators of functional\n  data with applications to evolutionary biology", "comments": null, "journal-ref": null, "doi": "10.1214/17-EJS1347", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we generalize the metric-based permutation test for the\nequality of covariance operators proposed by Pigoli et al. (2014) to the case\nof multiple samples of functional data. To this end, the non-parametric\ncombination methodology of Pesarin and Salmaso (2010) is used to combine all\nthe pairwise comparisons between samples into a global test. Different\ncombining functions and permutation strategies are reviewed and analyzed in\ndetail. The resulting test allows to make inference on the equality of the\ncovariance operators of multiple groups and, if there is evidence to reject the\nnull hypothesis, to identify the pairs of groups having different covariances.\nIt is shown that, for some combining functions, step-down adjusting procedures\nare available to control for the multiple testing problem in this setting. The\nempirical power of this new test is then explored via simulations and compared\nwith those of existing alternative approaches in different scenarios. Finally,\nthe proposed methodology is applied to data from wheel running activity\nexperiments, that used selective breeding to study the evolution of locomotor\nbehavior in mice.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jan 2017 17:47:04 GMT"}], "update_date": "2020-08-05", "authors_parsed": [["Cabassi", "Alessandra", ""], ["Pigoli", "Davide", ""], ["Secchi", "Piercesare", ""], ["Carter", "Patrick A.", ""]]}, {"id": "1701.05964", "submitter": "Assaf Oron", "authors": "Assaf P. Oron and Nancy Flournoy", "title": "Centered Isotonic Regression: Point and Interval Estimation for\n  Dose-Response Studies", "comments": "30 pages, 3 figures. Associated R package: GitHub assaforon/cir.\n  Accepted for publication 1/2017. Exact reference to be added upon final\n  publishing, Statistics in Biopharmaceutical Research, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Univariate isotonic regression (IR) has been used for nonparametric\nestimation in dose-response and dose-finding studies. One undesirable property\nof IR is the prevalence of piecewise-constant stretches in its estimates,\nwhereas the dose-response function is usually assumed to be strictly\nincreasing. We propose a simple modification to IR, called centered isotonic\nregression (CIR). CIR's estimates are strictly increasing in the interior of\nthe dose range. In the absence of monotonicity violations, CIR and IR both\nreturn the original observations. Numerical examination indicates that for\nsample sizes typical of dose-response studies and with realistic dose-response\ncurves, CIR provides a substantial reduction in estimation error compared with\nIR when monotonicity violations occur. We also develop analytical interval\nestimates for IR and CIR, with good coverage behavior. An R package implements\nthese point and interval estimates.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 01:40:09 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Oron", "Assaf P.", ""], ["Flournoy", "Nancy", ""]]}, {"id": "1701.06014", "submitter": "Mats Julius Stensrud", "authors": "Mats Julius Stensrud", "title": "Interpreting Hazard Ratios: Insights from Frailty Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hazard ratios are often used to evaluate time to event outcomes, but they may\nbe hard to interpret. A particular issue arise because hazards are typically\nestimated conditional on survival, i.e.\\ on left truncated samples. Then,\nhazard ratios from conventional models cannot be interpreted as counterfactual\nhazard ratios that are immediately relevant to individual patients. This\narticle explores how the hazard ratios from Cox models may differ from hazard\nratios with a causal interpretation. Using summary data from twin studies, I\nsuggest an approach to learn about the unmeasured heterogeneity in risk of an\noutcome, and this information allows us to explore the interpretation and\nmagnitude of hazard ratios. Under explicit parametric assumptions, I present a\ntwo-step method to obtain hazard ratios that are more relevant to individual\nsubjects. The strategy relies on untestable assumptions, but may nevertheless\nbe useful for sensitivity analyses that are relatively easy to interpret.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 12:07:54 GMT"}, {"version": "v2", "created": "Mon, 30 Jan 2017 17:11:13 GMT"}, {"version": "v3", "created": "Sun, 26 Mar 2017 09:47:14 GMT"}, {"version": "v4", "created": "Tue, 1 Aug 2017 20:09:46 GMT"}, {"version": "v5", "created": "Wed, 21 Mar 2018 19:44:55 GMT"}], "update_date": "2018-03-23", "authors_parsed": [["Stensrud", "Mats Julius", ""]]}, {"id": "1701.06054", "submitter": "Xiaoming Huo", "authors": "Cheng Huang and Xiaoming Huo", "title": "A Statistically and Numerically Efficient Independence Test based on\n  Random Projections and Distance Covariance", "comments": "52 pages, 8 figures, technical paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Test of independence plays a fundamental role in many statistical techniques.\nAmong the nonparametric approaches, the distance-based methods (such as the\ndistance correlation based hypotheses testing for independence) have numerous\nadvantages, comparing with many other alternatives. A known limitation of the\ndistance-based method is that its computational complexity can be high. In\ngeneral, when the sample size is $n$, the order of computational complexity of\na distance-based method, which typically requires computing of all pairwise\ndistances, can be $O(n^2)$. Recent advances have discovered that in the {\\it\nunivariate} cases, a fast method with $O(n \\log n)$ computational complexity\nand $O(n)$ memory requirement exists. In this paper, we introduces a test of\nindependence method based on random projection and distance correlation, which\nachieves nearly the same power as the state-of-the-art distance-based approach,\nworks in the {\\it multivariate} cases, and enjoys the $O(n K \\log n)$\ncomputational complexity and $O(\\max\\{n,K\\})$ memory requirement, where $K$ is\nthe number of random projections. Note that saving is achieved when $K < n/\\log\nn$. We name our method a Randomly Projected Distance Covariance (RPDC). The\nstatistical theoretical analysis takes advantage of some techniques on random\nprojection which are rooted in contemporary machine learning. Numerical\nexperiments demonstrate the efficiency of the proposed method, in relative to\nseveral competitors.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 17:22:01 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Huang", "Cheng", ""], ["Huo", "Xiaoming", ""]]}, {"id": "1701.06088", "submitter": "Shih-Kang Chao", "authors": "Stanislav Volgushev and Shih-Kang Chao and Guang Cheng", "title": "Distributed inference for quantile regression processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased availability of massive data sets provides a unique opportunity\nto discover subtle patterns in their distributions, but also imposes\noverwhelming computational challenges. To fully utilize the information\ncontained in big data, we propose a two-step procedure: (i) estimate\nconditional quantile functions at different levels in a parallel computing\nenvironment; (ii) construct a conditional quantile regression process through\nprojection based on these estimated quantile curves. Our general quantile\nregression framework covers both linear models with fixed or growing dimension\nand series approximation models. We prove that the proposed procedure does not\nsacrifice any statistical inferential accuracy provided that the number of\ndistributed computing units and quantile levels are chosen properly. In\nparticular, a sharp upper bound for the former and a sharp lower bound for the\nlatter are derived to capture the minimal computational cost from a statistical\nperspective. As an important application, the statistical inference on\nconditional distribution functions is considered. Moreover, we propose\ncomputationally efficient approaches to conducting inference in the distributed\nestimation setting described above. Those approaches directly utilize the\navailability of estimators from sub-samples and can be carried out at almost no\nadditional computational cost. Simulations confirm our statistical inferential\ntheory.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jan 2017 21:32:34 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 16:38:35 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2018 18:13:27 GMT"}], "update_date": "2018-04-12", "authors_parsed": [["Volgushev", "Stanislav", ""], ["Chao", "Shih-Kang", ""], ["Cheng", "Guang", ""]]}, {"id": "1701.06105", "submitter": "Xianzheng Huang", "authors": "Xianzheng Huang and Haiming Zhou", "title": "An alternative local polynomial estimator for the error-in-variables\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a regression function when a covariate\nis measured with error. Using the local polynomial estimator of Delaigle, Fan,\nand Carroll (2009) as a benchmark, we propose an alternative way of solving the\nproblem without transforming the kernel function. The asymptotic properties of\nthe alternative estimator are rigorously studied. A detailed implementing\nalgorithm and a computationally efficient bandwidth selection procedure are\nalso provided. The proposed estimator is compared with the existing local\npolynomial estimator via extensive simulations and an application to the\nmotorcycle crash data. The results show that the new estimator can be less\nbiased than the existing estimator and is numerically more stable.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jan 2017 00:34:24 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Huang", "Xianzheng", ""], ["Zhou", "Haiming", ""]]}, {"id": "1701.06263", "submitter": "Raymond K. W. Wong", "authors": "Raymond K. W. Wong, Xiaoke Zhang", "title": "Nonparametric Operator-Regularized Covariance Function Estimation for\n  Functional Data", "comments": "26 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In functional data analysis (FDA), covariance function is fundamental not\nonly as a critical quantity for understanding elementary aspects of functional\ndata but also as an indispensable ingredient for many advanced FDA methods.\nThis paper develops a new class of nonparametric covariance function estimators\nin terms of various spectral regularizations of an operator associated with a\nreproducing kernel Hilbert space. Despite their nonparametric nature, the\ncovariance estimators are automatically positive semi-definite without any\nadditional modification steps. An unconventional representer theorem is\nestablished to provide a finite dimensional representation for this class of\ncovariance estimators, which leads to a closed-form expression of the\ncorresponding $L^2$ eigen-decomposition. Trace-norm regularization is\nparticularly studied to further achieve a low-rank representation, another\ndesirable property which leads to dimension reduction and is often needed in\nadvanced FDA approaches. An efficient algorithm is developed based on the\naccelerated proximal gradient method. This resulted estimator is shown to enjoy\nan excellent rate of convergence under both fixed and random designs. The\noutstanding practical performance of the trace-norm-regularized covariance\nestimator is demonstrated by a simulation study and the analysis of a traffic\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 04:14:59 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Wong", "Raymond K. W.", ""], ["Zhang", "Xiaoke", ""]]}, {"id": "1701.06455", "submitter": "Paul Kinsvater", "authors": "Paul Kinsvater and Friederike Deiters and Roland Fried", "title": "Dealing with seasonal variability and inter-site dependence in regional\n  flood frequency analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the regional estimation of high quantiles of annual\nmaximal river flow distributions $F$, an important problem from flood frequency\nanalysis. Even though this particular problem has been addressed by many\npapers, less attention has been payed to incorporating seasonal variability and\nspatial dependence into the methods. We are going to discuss two regional\nestimators of high quantiles of local distributions $F$ that are able to deal\nwith these important features, namely, a parametric approach based on so-called\ntwo-component extreme value distributions and a semi-parametric approach based\non regional estimation of a tail index. The asymptotic normality of the\nestimators is derived for both procedures, which for instance enables us to\naccount for estimation uncertainty without the need of parametric dependence\nmodels or bootstrap procedures. A comprehensive simulation study is conducted\nand our main findings are illustrated on river flow series from the Mulde basin\nin Germany, where people have suffered several times from severe floods over\nthe last 100 years.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 15:33:13 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 15:32:40 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Kinsvater", "Paul", ""], ["Deiters", "Friederike", ""], ["Fried", "Roland", ""]]}, {"id": "1701.06605", "submitter": "Jalal Etesami", "authors": "Saber Salehkaleybar and Jalal Etesami and Negar Kiyavash", "title": "Identifying Nonlinear 1-Step Causal Influences in Presence of Latent\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.LG math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach for learning the causal structure in stochastic\ndynamical systems with a $1$-step functional dependency in the presence of\nlatent variables. We propose an information-theoretic approach that allows us\nto recover the causal relations among the observed variables as long as the\nlatent variables evolve without exogenous noise. We further propose an\nefficient learning method based on linear regression for the special sub-case\nwhen the dynamics are restricted to be linear. We validate the performance of\nour approach via numerical simulations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 19:48:11 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Salehkaleybar", "Saber", ""], ["Etesami", "Jalal", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1701.06686", "submitter": "Ilya Shpitser", "authors": "Thomas S. Richardson, Robin J. Evans, James M. Robins, and Ilya\n  Shpitser", "title": "Nested Markov Properties for Acyclic Directed Mixed Graphs", "comments": "67 pages (not including appendix and references), 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Directed acyclic graph (DAG) models may be characterized in at least four\ndifferent ways: via a factorization, the d-separation criterion, the\nmoralization criterion, and the local Markov property. As pointed out by Robins\n(1986, 1999), Verma and Pearl (1990), and Tian and Pearl (2002b), marginals of\nDAG models also imply equality constraints that are not conditional\nindependences. The well-known `Verma constraint' is an example. Constraints of\nthis type were used for testing edges (Shpitser et al., 2009), and an efficient\nmarginalization scheme via variable elimination (Shpitser et al., 2011).\n  We show that equality constraints like the `Verma constraint' can be viewed\nas conditional independences in kernel objects obtained from joint\ndistributions via a fixing operation that generalizes conditioning and\nmarginalization. We use these constraints to define, via Markov properties and\na factorization, a graphical model associated with acyclic directed mixed\ngraphs (ADMGs). We show that marginal distributions of DAG models lie in this\nmodel, prove that a characterization of these constraints given in (Tian and\nPearl, 2002b) gives an alternative definition of the model, and finally show\nthat the fixing operation we used to define the model can be used to give a\nparticularly simple characterization of identifiable causal effects in hidden\nvariable graphical causal models.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jan 2017 23:56:09 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 01:46:38 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Richardson", "Thomas S.", ""], ["Evans", "Robin J.", ""], ["Robins", "James M.", ""], ["Shpitser", "Ilya", ""]]}, {"id": "1701.06739", "submitter": "Alexander Luedtke", "authors": "Alexander R. Luedtke and Peter B. Gilbert", "title": "Partial Bridging of Vaccine Efficacy to New Populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose one has data from one or more completed vaccine efficacy trials and\nwishes to estimate the efficacy in a new setting. Often logistical or ethical\nconsiderations make running another efficacy trial impossible. Fortunately, if\nthere is a biomarker that is the primary modifier of efficacy, then the\nbiomarker-conditional efficacy may be identical in the completed trials and the\nnew setting, or at least informative enough to meaningfully bound this\nquantity. Given a sample of this biomarker from the new population, we might\nhope we can bridge the results of the completed trials to estimate the vaccine\nefficacy in this new population. Unfortunately, even knowing the true\nconditional efficacy in the new population fails to identify the marginal\nefficacy due to the unknown conditional unvaccinated risk. We define a curve\nthat partially identifies (lower bounds) the marginal efficacy in the new\npopulation as a function of the population's marginal unvaccinated risk, under\nthe assumption that one can identify bounds on the conditional unvaccinated\nrisk in the new population. Interpreting the curve only requires identifying\nplausible regions of the marginal unvaccinated risk in the new population. We\npresent a nonparametric estimator of this curve and develop valid lower\nconfidence bounds that concentrate at a parametric rate. We use vaccine\nterminology throughout, but the results apply to general binary interventions\nand bounded outcomes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 06:03:00 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Luedtke", "Alexander R.", ""], ["Gilbert", "Peter B.", ""]]}, {"id": "1701.06820", "submitter": "Sara Algeri", "authors": "Sara Algeri and David A. van Dyk", "title": "Testing One Hypothesis Multiple times", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applied settings, tests of hypothesis where a nuisance parameter is only\nidentifiable under the alternative often reduces into one of Testing One\nHypothesis Multiple times (TOHM). Specifically, a fine discretization of the\nspace of the non-identifiable parameter is specified, and the null hypothesis\nis tested against a set of sub-alternative hypothesis, one for each point of\nthe discretization. The resulting sub-test statistics are then combined to\nobtain a global p-value. In this paper, we discuss a computationally efficient\ninferential tool to perform TOHM under stringent significance requirements,\nsuch as those typically required in the physical sciences, (e.g., p-value\n$<10^{-7}$). The resulting procedure leads to a generalized approach to perform\ninference under non-standard conditions, including non-nested models\ncomparisons.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 11:37:11 GMT"}, {"version": "v2", "created": "Wed, 19 Jul 2017 17:12:45 GMT"}, {"version": "v3", "created": "Wed, 15 Nov 2017 13:27:06 GMT"}, {"version": "v4", "created": "Sat, 18 May 2019 17:52:33 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Algeri", "Sara", ""], ["van Dyk", "David A.", ""]]}, {"id": "1701.06876", "submitter": "Yoav Zemel", "authors": "Yoav Zemel and Victor M. Panaretos", "title": "Fr\\'echet Means and Procrustes Analysis in Wasserstein Space", "comments": "45 pages, 10 figures; to appear in Bernoulli Journal. Added\n  references, mainly from computer science literature", "journal-ref": "Bernoulli 25(2):932-976, 2019", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two statistical problems at the intersection of functional and\nnon-Euclidean data analysis: the determination of a Fr\\'echet mean in the\nWasserstein space of multivariate distributions; and the optimal registration\nof deformed random measures and point processes. We elucidate how the two\nproblems are linked, each being in a sense dual to the other. We first study\nthe finite sample version of the problem in the continuum. Exploiting the\ntangent bundle structure of Wasserstein space, we deduce the Fr\\'echet mean via\ngradient descent. We show that this is equivalent to a Procrustes analysis for\nthe registration maps, thus only requiring successive solutions to pairwise\noptimal coupling problems. We then study the population version of the problem,\nfocussing on inference and stability: in practice, the data are i.i.d.\nrealisations from a law on Wasserstein space, and indeed their observation is\ndiscrete, where one observes a proxy finite sample or point process. We\nconstruct regularised nonparametric estimators, and prove their consistency for\nthe population mean, and uniform consistency for the population Procrustes\nregistration maps.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 14:00:19 GMT"}, {"version": "v2", "created": "Wed, 17 Jan 2018 15:45:31 GMT"}], "update_date": "2020-12-17", "authors_parsed": [["Zemel", "Yoav", ""], ["Panaretos", "Victor M.", ""]]}, {"id": "1701.06952", "submitter": "Yao Xie", "authors": "Yang Cao and Yao Xie", "title": "Robust Sequential Change-Point Detection by Convex Optimization", "comments": "Accepted by ISIT 2017", "journal-ref": null, "doi": "10.3390/e20020108", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the computational challenge of finding the robust sequential\nchange-point detection procedures when the pre- and post-change distributions\nare not completely specified. Earlier works [veeravalli 1994] and [Unnikrishnan\n2011] establish the general conditions for robust procedures which include\nfinding a pair of least favorable distributions (LFDs). However, in the\nmulti-dimensional setting, it is hard to find such LFDs computationally. We\npresent a method based on convex optimization that addresses this issue when\nthe distributions are Gaussian with unknown parameters from pre-specified\nuncertainty sets. We also establish theoretical properties of our robust\nprocedures, and numerical examples demonstrate their good performance.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 16:06:48 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 13:57:41 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Cao", "Yang", ""], ["Xie", "Yao", ""]]}, {"id": "1701.06967", "submitter": "Gerrit Van Den Burg", "authors": "Gerrit J. J. van den Burg, Patrick J. F. Groenen, Andreas Alfons", "title": "SparseStep: Approximating the Counting Norm for Sparse Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SparseStep algorithm is presented for the estimation of a sparse\nparameter vector in the linear regression problem. The algorithm works by\nadding an approximation of the exact counting norm as a constraint on the model\nparameters and iteratively strengthening this approximation to arrive at a\nsparse solution. Theoretical analysis of the penalty function shows that the\nestimator yields unbiased estimates of the parameter vector. An iterative\nmajorization algorithm is derived which has a straightforward implementation\nreminiscent of ridge regression. In addition, the SparseStep algorithm is\ncompared with similar methods through a rigorous simulation study which shows\nit often outperforms existing methods in both model fit and prediction\naccuracy.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 16:31:29 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Burg", "Gerrit J. J. van den", ""], ["Groenen", "Patrick J. F.", ""], ["Alfons", "Andreas", ""]]}, {"id": "1701.06976", "submitter": "Haiming Zhou", "authors": "Haiming Zhou and Timothy Hanson", "title": "A unified framework for fitting Bayesian semiparametric models to\n  arbitrarily censored survival data, including spatially-referenced data", "comments": "To appear in Journal of the American Statistical Association", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A comprehensive, unified approach to modeling arbitrarily censored spatial\nsurvival data is presented for the three most commonly-used semiparametric\nmodels: proportional hazards, proportional odds, and accelerated failure time.\nUnlike many other approaches, all manner of censored survival times are\nsimultaneously accommodated including uncensored, interval censored,\ncurrent-status, left and right censored, and mixtures of these. Left-truncated\ndata are also accommodated leading to models for time-dependent covariates.\nBoth georeferenced (location exactly observed) and areally observed (location\nknown up to a geographic unit such as a county) spatial locations are handled;\nformal variable selection makes model selection especially easy. Model fit is\nassessed with conditional Cox-Snell residual plots, and model choice is carried\nout via LPML and DIC. Baseline survival is modeled with a novel transformed\nBernstein polynomial prior. All models are fit via a new function which calls\nefficient compiled C++ in the R package spBayesSurv. The methodology is broadly\nillustrated with simulations and real data applications. An important finding\nis that proportional odds and accelerated failure time models often fit\nsignificantly better than the commonly-used proportional hazards model.\nSupplementary materials are available online.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 16:47:47 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 02:47:45 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Zhou", "Haiming", ""], ["Hanson", "Timothy", ""]]}, {"id": "1701.07010", "submitter": "Keefe Murphy", "authors": "Keefe Murphy, Cinzia Viroli, and Isobel Claire Gormley", "title": "Infinite Mixtures of Infinite Factor Analysers", "comments": "Published in Bayesian Analysis", "journal-ref": "Bayesian Analysis, 15(3): 937-963 (2020)", "doi": "10.1214/19-BA1179", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Factor-analytic Gaussian mixture models are often employed as a model-based\napproach to clustering high-dimensional data. Typically, the numbers of\nclusters and latent factors must be specified in advance of model fitting, and\nremain fixed. The pair which optimises some model selection criterion is then\nchosen. For computational reasons, models in which the number of latent factors\ndiffer across clusters are rarely considered. Here the infinite mixture of\ninfinite factor analysers (IMIFA) model is introduced. IMIFA employs a\nPitman-Yor process prior to facilitate automatic inference of the number of\nclusters using the stick-breaking construction and a slice sampler.\nFurthermore, IMIFA employs multiplicative gamma process shrinkage priors to\nallow cluster-specific numbers of factors, automatically inferred via an\nadaptive Gibbs sampler. IMIFA is presented as the flagship of a family of\nfactor-analytic mixture models, providing flexible approaches to clustering\nhigh-dimensional data. Applications to a benchmark data set, metabolomic\nspectral data, and a manifold learning handwritten digit example illustrate the\nIMIFA model and its advantageous features. These include obviating the need for\nmodel selection criteria, reducing the computational burden associated with the\nsearch of the model space, improving clustering performance by allowing\ncluster-specific numbers of factors, and quantifying uncertainty in the numbers\nof clusters and cluster-specific factors.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 18:53:44 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 16:51:16 GMT"}, {"version": "v3", "created": "Tue, 9 May 2017 13:45:56 GMT"}, {"version": "v4", "created": "Thu, 26 Apr 2018 15:23:52 GMT"}, {"version": "v5", "created": "Wed, 8 May 2019 13:09:39 GMT"}, {"version": "v6", "created": "Tue, 13 Jul 2021 21:29:12 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Murphy", "Keefe", ""], ["Viroli", "Cinzia", ""], ["Gormley", "Isobel Claire", ""]]}, {"id": "1701.07011", "submitter": "Lingfei Wang", "authors": "Lingfei Wang and Tom Michoel", "title": "Controlling false discoveries in Bayesian gene networks with lasso\n  regression p-values", "comments": "9 pages, 6 figures, 3 tables. Supplementary info: 2 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.MN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian networks can represent directed gene regulations and therefore are\nfavored over co-expression networks. However, hardly any Bayesian network study\nconcerns the false discovery control (FDC) of network edges, leading to low\naccuracies due to systematic biases from inconsistent false discovery levels in\nthe same study. We design four empirical tests to examine the FDC of Bayesian\nnetworks from three p-value based lasso regression variable selections --- two\nexisting and one we originate. Our method, lassopv, computes p-values for the\ncritical regularization strength at which a predictor starts to contribute to\nlasso regression. Using null and Geuvadis datasets, we find that lassopv\nobtains optimal FDC in Bayesian gene networks, whilst existing methods have\ndefective p-values. The FDC concept and tests extend to most network inference\nscenarios and will guide the design and improvement of new and existing\nmethods. Our novel variable selection method with lasso regression also allows\nFDC on other datasets and questions, even beyond network inference and\ncomputational biology. Lassopv is implemented in R and freely available at\nhttps://github.com/lingfeiwang/lassopv and\nhttps://cran.r-project.org/package=lassopv\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 18:56:45 GMT"}, {"version": "v2", "created": "Tue, 27 Mar 2018 19:45:20 GMT"}], "update_date": "2018-03-29", "authors_parsed": [["Wang", "Lingfei", ""], ["Michoel", "Tom", ""]]}, {"id": "1701.07050", "submitter": "Jean-Marie Dufour", "authors": "Firmin Doko Tchatoka and Jean-Marie Dufour", "title": "Exogeneity tests, incomplete models, weak identification and\n  non-Gaussian distributions: invariance and finite-sample distributional\n  theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distribution of Durbin-Wu-Hausman (DWH) and Revankar-Hartley\n(RH) tests for exogeneity from a finite-sample viewpoint, under the null and\nalternative hypotheses. We consider linear structural models with possibly\nnon-Gaussian errors, where structural parameters may not be identified and\nwhere reduced forms can be incompletely specified (or nonparametric). On level\ncontrol, we characterize the null distributions of all the test statistics.\nThrough conditioning and invariance arguments, we show that these distributions\ndo not involve nuisance parameters. In particular, this applies to several test\nstatistics for which no finite-sample distributional theory is yet available,\nsuch as the standard statistic proposed by Hausman (1978). The distributions of\nthe test statistics may be non-standard -- so corrections to usual asymptotic\ncritical values are needed -- but the characterizations are sufficiently\nexplicit to yield finite-sample (Monte-Carlo) tests of the exogeneity\nhypothesis. The procedures so obtained are robust to weak identification,\nmissing instruments or misspecified reduced forms, and can easily be adapted to\nallow for parametric non-Gaussian error distributions. We give a general\ninvariance result (block triangular invariance) for exogeneity test statistics.\nThis property yields a convenient exogeneity canonical form and a parsimonious\nreduction of the parameters on which power depends. In the extreme case where\nno structural parameter is identified, the distributions under the alternative\nhypothesis and the null hypothesis are identical, so the power function is\nflat, for all the exogeneity statistics. However, as soon as identification\ndoes not fail completely, this phenomenon typically disappears.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 19:39:01 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Tchatoka", "Firmin Doko", ""], ["Dufour", "Jean-Marie", ""]]}, {"id": "1701.07078", "submitter": "Ronald Mahler", "authors": "Ronald Mahler", "title": "Measurement-to-Track Association and Finite-Set Statistics", "comments": "9 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-hypothesis trackers (MHT's), which are based on the\nmeasurement-to-track association (MTA) concept, have long been asserted to be\n\"Bayes-optimal.\" Recently, rather bolder claims have come to the fore: \"The\nright model of the multitarget state is that used in the multi-hypothesis\ntracker (MHT) paradigm, not the RSF [random finite set] paradigm.\" Or, the RFS\napproach is essentially a mathematically obfuscated reinvention of MHT. In this\npaper it is shown that: (a) although MTA's can be given a Bayesian formulation,\nthis formulation is not fully consistent with Bayesian statistics; (b)\nphenomenologically, an MTA is a heuristic extrapolation of an intuitive special\ncase to general multitarget scenarios; (c) MTA's are, therefore, not physically\nreal entities and thus cannot (as with MHT's) be employed as state\nrepresentations of a multitarget system; (d) MHT's are, consequently, heuristic\napproximations of the actual Bayes-optimal approach, the multitarget Bayes\nfilter; (d)the theoretically correct measurement modeling approach is the RSF\nmultitarget likelihood function L_Z(X) = f(Z|X); (f) although MTA's do occur in\nf(Z|X), they are the consequence of a mere change of notation during the RFS\nderivation of f(Z|X); and (g) the generalized labeled multi-Bernoulli (GLMB)\nfilter of Vo and Vo is currently the only provably Bayes-optimal and\ncomputationally tractable approach for true multitarget tracking using MTA's.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jan 2017 22:50:32 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Mahler", "Ronald", ""]]}, {"id": "1701.07086", "submitter": "Peter Rousseeuw", "authors": "Kris Boudt, Peter J. Rousseeuw, Steven Vanduffel, Tim Verdonck", "title": "The Minimum Regularized Covariance Determinant estimator", "comments": null, "journal-ref": "Statistics and Computing, 2020, Vol. 30, 113-128", "doi": "10.1007/s11222-019-09869-x", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Covariance Determinant (MCD) approach robustly estimates the\nlocation and scatter matrix using the subset of given size with lowest sample\ncovariance determinant. Its main drawback is that it cannot be applied when the\ndimension exceeds the subset size. We propose the Minimum Regularized\nCovariance Determinant (MRCD) approach, which differs from the MCD in that the\nscatter matrix is a convex combination of a target matrix and the sample\ncovariance matrix of the subset. A data-driven procedure sets the weight of the\ntarget matrix, so that the regularization is only used when needed. The MRCD\nestimator is defined in any dimension, is well-conditioned by construction and\npreserves the good robustness properties of the MCD. We prove that so-called\nconcentration steps can be performed to reduce the MRCD objective function, and\nwe exploit this fact to construct a fast algorithm. We verify the accuracy and\nrobustness of the MRCD estimator in a simulation study and illustrate its\npractical use for outlier detection and regression analysis on real-life\nhigh-dimensional data sets in chemistry and criminology.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 21:20:21 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 21:14:33 GMT"}, {"version": "v3", "created": "Sat, 1 Dec 2018 13:51:52 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Boudt", "Kris", ""], ["Rousseeuw", "Peter J.", ""], ["Vanduffel", "Steven", ""], ["Verdonck", "Tim", ""]]}, {"id": "1701.07263", "submitter": "Piotr Fryzlewicz", "authors": "Piotr Fryzlewicz", "title": "Likelihood ratio Haar variance stabilization and normalization for\n  Poisson and other non-Gaussian noise removal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new methodology for denoising, variance-stabilizing and\nnormalizing signals whose both mean and variance are parameterized by a single\nunknown varying parameter, such as Poisson or scaled chi-squared. Key to our\nmethodology is the observation that the signed and square-rooted generalized\nlog-likelihood ratio test for the equality of the local means is approximately\nand asymptotically distributed as standard normal under the null. We use these\ntest statistics within the Haar wavelet transform at each scale and location,\nreferring to them as the likelihood ratio Haar (LRH) coefficients of the data.\nIn the denoising algorithm, the LRH coefficients are used as thresholding\ndecision statistics, which enables the use of thresholds suitable for i.i.d.\nGaussian noise, despite the standard Haar coefficients of the signal being\nheteroscedastic. In the variance-stabilizing and normalizing algorithm, the LRH\ncoefficients replace the standard Haar coefficients in the Haar basis\nexpansion. To the best of our knowledge, the variance-stabilizing and\nnormalizing properties of the generalized likelihood ratio test have not been\ninterpreted or exploited in this manner before. We prove the consistency of our\nLRH smoother for Poisson counts with a near-parametric rate, and various\nnumerical experiments demonstrate the good practical performance of our\nmethodology.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 11:05:20 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Fryzlewicz", "Piotr", ""]]}, {"id": "1701.07328", "submitter": "Adrian Bowman Prof.", "authors": "Liberty Vittert and Adrian Bowman and Stanislav Katina", "title": "Statistical models for manifold data with applications to the human face", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the data structures generated by medical imaging technology is high\nresolution point clouds representing anatomical surfaces. Raw images are in the\nform of triangulated surfaces and the first step is to create a standardised\nrepresentation of surface shape which provides a meaningful correspondence\nacross different images, to provide a basis for statistical analysis. Point\nlocations with anatomical definitions, referred to as landmarks, have been the\ntraditional approach, with analysis and interpretation which is widely used and\nwell understood. Landmarks can also be taken as the starting point for more\ngeneral surface representations, often using templates which are warped on to\nan observed surface by matching landmark positions and subsequent local\nadjustment of the surface. The aim of the present paper is to use the\nintermediate structures of ridge and valley curves to capture the principal\nfeatures of the manifold (surface) of interest. Landmarks are used as anchoring\npoints, as usual, but curvature information across the surface is used to guide\nthe curve locations. Once the ridges and valleys have been located, the\nintervening surface patches are relatively flat and can be represented in a\nstandardised manner by appropriate surface transects, to give a complete\nsurface model. However, the intermediate curve representation is of\nconsiderable interest in its own right, as it captures the principal features\nof the surface and therefore embodies much of the shape information on the\nobject of interest.\n  Methods of identifying these curves are described and evaluated. While the\nmethods are generic, the human face is used as an important application. In\nparticular, the models are used to investigate sexual dimorphism (the\ndifferences in shape between male and female faces). The curve representation\nis shown to capture a major component of the relevant information.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 14:17:59 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Vittert", "Liberty", ""], ["Bowman", "Adrian", ""], ["Katina", "Stanislav", ""]]}, {"id": "1701.07351", "submitter": "Claudia Kluppelberg", "authors": "Nadine Gissibl, Claudia Kl\\\"uppelberg and Moritz Otto", "title": "Tail dependence of recursive max-linear models with regularly varying\n  noise variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive max-linear structural equation models with regularly varying noise\nvariables are considered. Their causal structure is represented by a directed\nacyclic graph (DAG). The problem of identifying a recursive max-linear model\nand its associated DAG from its matrix of pairwise tail dependence coefficients\nis discussed. For example, it is shown that if a causal ordering of the\nassociated DAG is additionally known, then the minimum DAG representing the\nrecursive structural equations can be recovered from the tail dependence\nmatrix. For the relevant subclass of recursive max-linear models,\nidentifiability of the associated minimum DAG from the tail dependence matrix\nand the initial nodes is shown. Algorithms find the associated minimum DAG for\nthe different situations. Furthermore, given a tail dependence matrix, an\nalgorithm outputs all compatible recursive max-linear models and their\nassociated minimum DAGs.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 15:27:30 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 08:29:30 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Gissibl", "Nadine", ""], ["Kl\u00fcppelberg", "Claudia", ""], ["Otto", "Moritz", ""]]}, {"id": "1701.07359", "submitter": "Kim Hendrickx", "authors": "Piet Groeneboom and Kim Hendrickx", "title": "The nonparametric bootstrap for the current status model", "comments": "39 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been proved that direct bootstrapping of the nonparametric maximum\nlikelihood estimator (MLE) of the distribution function in the current status\nmodel leads to inconsistent confidence intervals. We show that bootstrapping of\nfunctionals of the MLE can however be used to produce valid intervals. To this\nend, we prove that the bootstrapped MLE converges at the right rate in the\n$L_p$-distance. We also discuss applications of this result to the current\nstatus regression model.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 15:48:14 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 06:56:11 GMT"}, {"version": "v3", "created": "Wed, 20 Sep 2017 13:41:55 GMT"}], "update_date": "2017-09-21", "authors_parsed": [["Groeneboom", "Piet", ""], ["Hendrickx", "Kim", ""]]}, {"id": "1701.07429", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Robust mixture of experts modeling using the $t$ distribution", "comments": "arXiv admin note: substantial text overlap with arXiv:1506.06707,\n  arXiv:1612.06879", "journal-ref": "Neural Networks 79: 20-36 (2016)", "doi": "10.1016/j.neunet.2016.03.002", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of Experts (MoE) is a popular framework for modeling heterogeneity in\ndata for regression, classification, and clustering. For regression and cluster\nanalyses of continuous data, MoE usually use normal experts following the\nGaussian distribution. However, for a set of data containing a group or groups\nof observations with heavy tails or atypical observations, the use of normal\nexperts is unsuitable and can unduly affect the fit of the MoE model. We\nintroduce a robust MoE modeling using the $t$ distribution. The proposed $t$\nMoE (TMoE) deals with these issues regarding heavy-tailed and noisy data. We\ndevelop a dedicated expectation-maximization (EM) algorithm to estimate the\nparameters of the proposed model by monotonically maximizing the observed data\nlog-likelihood. We describe how the presented model can be used in prediction\nand in model-based clustering of regression data. The proposed model is\nvalidated on numerical experiments carried out on simulated data, which show\nthe effectiveness and the robustness of the proposed model in terms of modeling\nnon-linear regression functions as well as in model-based clustering. Then, it\nis applied to the real-world data of tone perception for musical data analysis,\nand the one of temperature anomalies for the analysis of climate change data.\nThe obtained results show the usefulness of the TMoE model for practical\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 9 Dec 2016 14:42:40 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1701.07483", "submitter": "Ashwin Venkataraman", "authors": "Srikanth Jagabathula, Lakshminarayanan Subramanian, Ashwin\n  Venkataraman", "title": "A Model-based Projection Technique for Segmenting Customers", "comments": "51 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of segmenting a large population of customers into\nnon-overlapping groups with similar preferences, using diverse preference\nobservations such as purchases, ratings, clicks, etc. over subsets of items. We\nfocus on the setting where the universe of items is large (ranging from\nthousands to millions) and unstructured (lacking well-defined attributes) and\neach customer provides observations for only a few items. These data\ncharacteristics limit the applicability of existing techniques in marketing and\nmachine learning. To overcome these limitations, we propose a model-based\nprojection technique, which transforms the diverse set of observations into a\nmore comparable scale and deals with missing data by projecting the transformed\ndata onto a low-dimensional space. We then cluster the projected data to obtain\nthe customer segments. Theoretically, we derive precise necessary and\nsufficient conditions that guarantee asymptotic recovery of the true customer\nsegments. Empirically, we demonstrate the speed and performance of our method\nin two real-world case studies: (a) 84% improvement in the accuracy of new\nmovie recommendations on the MovieLens data set and (b) 6% improvement in the\nperformance of similar item recommendations algorithm on an offline dataset at\neBay. We show that our method outperforms standard latent-class and\ndemographic-based techniques.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 20:47:40 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Jagabathula", "Srikanth", ""], ["Subramanian", "Lakshminarayanan", ""], ["Venkataraman", "Ashwin", ""]]}, {"id": "1701.07496", "submitter": "Max Tolkoff", "authors": "Max R. Tolkoff, Michael L. Alfaro, Guy Baele, Philippe Lemey, and Marc\n  A. Suchard", "title": "Phylogenetic Factor Analysis", "comments": "51 pages (42 main, 9 supplemental), 9 figures (5 main, 4\n  supplemental), 4 tables (2 main, 2 supplemental), submitted to Systematic\n  Biology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phylogenetic comparative methods explore the relationships between\nquantitative traits adjusting for shared evolutionary history. This adjustment\noften occurs through a Brownian diffusion process along the branches of the\nphylogeny that generates model residuals or the traits themselves. For\nhigh-dimensional traits, inferring all pair-wise correlations within the\nmultivariate diffusion is limiting. To circumvent this problem, we propose\nphylogenetic factor analysis (PFA) that assumes a small unknown number of\nindependent evolutionary factors arise along the phylogeny and these factors\ngenerate clusters of dependent traits. Set in a Bayesian framework, PFA\nprovides measures of uncertainty on the factor number and groupings, combines\nboth continuous and discrete traits, integrates over missing measurements and\nincorporates phylogenetic uncertainty with the help of molecular sequences. We\ndevelop Gibbs samplers based on dynamic programming to estimate the PFA\nposterior distribution, over three-fold faster than for multivariate diffusion\nand a further order-of-magnitude more efficiently in the presence of latent\ntraits. We further propose a novel marginal likelihood estimator for previously\nimpractical models with discrete data and find that PFA also provides a better\nfit than multivariate diffusion in evolutionary questions in columbine flower\ndevelopment, placental reproduction transitions and triggerfish fin\nmorphometry.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 21:43:03 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Tolkoff", "Max R.", ""], ["Alfaro", "Michael L.", ""], ["Baele", "Guy", ""], ["Lemey", "Philippe", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1701.07506", "submitter": "Jonathan Bradley", "authors": "Jonathan R. Bradley, Scott H. Holan, Christopher K. Wikle", "title": "Bayesian Hierarchical Models with Conjugate Full-Conditional\n  Distributions for Dependent Data from the Natural Exponential Family", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a Bayesian approach for analyzing (possibly) high-dimensional\ndependent data that are distributed according to a member from the natural\nexponential family of distributions. This problem requires extensive\nmethodological advancements, as jointly modeling high-dimensional dependent\ndata leads to the so-called \"big n problem.\" The computational complexity of\nthe \"big n problem\" is further exacerbated when allowing for non-Gaussian data\nmodels, as is the case here. Thus, we develop new computationally efficient\ndistribution theory for this setting. In particular, we introduce the\n\"conjugate multivariate distribution,\" which is motivated by the univariate\ndistribution introduced in Diaconis and Ylvisaker (1979). Furthermore, we\nprovide substantial theoretical and methodological development including:\nresults regarding conditional distributions, an asymptotic relationship with\nthe multivariate normal distribution, conjugate prior distributions, and\nfull-conditional distributions for a Gibbs sampler. To demonstrate the\nwide-applicability of the proposed methodology, we provide two simulation\nstudies and three applications based on an epidemiology dataset, a federal\nstatistics dataset, and an environmental dataset, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 25 Jan 2017 22:20:07 GMT"}, {"version": "v2", "created": "Thu, 15 Feb 2018 19:08:51 GMT"}, {"version": "v3", "created": "Wed, 17 Apr 2019 20:15:55 GMT"}], "update_date": "2019-04-19", "authors_parsed": [["Bradley", "Jonathan R.", ""], ["Holan", "Scott H.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1701.07555", "submitter": "Gery Geenens", "authors": "Gery Geenens, Thomas Cuddihy", "title": "Robust analysis of second-leg home advantage in UEFA football through\n  better nonparametric confidence intervals for binary regression functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In international football (soccer), two-legged knockout ties, with each team\nplaying at home in one leg and the final outcome decided on aggregate, are\ncommon. Many players, managers and followers seem to believe in the `second-leg\nhome advantage', i.e. that it is beneficial to play at home on the second leg.\nA more complex effect than the usual and well-established home advantage, it is\nharder to identify, and previous statistical studies did not prove conclusive\nabout its actuality. Yet, given the amount of money handled in international\nfootball competitions nowadays, the question of existence or otherwise of this\neffect is of real import. As opposed to previous research, this paper addresses\nit from a purely nonparametric perspective and brings a very objective answer,\nnot based on any particular model specification which could orientate the\nanalysis in one or the other direction. Along the way, the paper reviews the\nwell-known shortcomings of the Wald confidence interval for a proportion,\nsuggests new nonparametric confidence intervals for conditional probability\nfunctions, revisits the problem of the bias when building confidence intervals\nin nonparametric regression, and provides a novel bootstrap-based solution to\nit. Finally, the new intervals are used in a careful analysis of game outcome\ndata for the UEFA Champions and Europa leagues from 2009/10 to 2014/15. A\nslight `second-leg home advantage' is evidenced.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 02:49:06 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Geenens", "Gery", ""], ["Cuddihy", "Thomas", ""]]}, {"id": "1701.07577", "submitter": "Mahbub Latif A", "authors": "Md. Shaddam Hossain Bagmar, Wasimul Bari, and A. H. M. Mahbub Latif", "title": "Comparing robustness properties of optimal designs under standard and\n  compound criteria", "comments": "16 pages, 4 tables in main text and 4 tables in Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard optimality criteria (e.g. A-, D-optimality criterion, etc.) have\nbeen commonly used for obtaining optimal designs. For a given statistical\nmodel, standard criteria assume the error variance is known at the design\nstage. However, in practice the error variance is estimated to make inference\nabout the model parameters. Modified criteria are defined as a function of the\nstandard criteria and the corresponding error degrees of freedom, which may\nlead to extreme optimal design. Compound criteria are defined as the function\nof different modified criteria and corresponding user specified weights.\nStandard, modified, and compound criteria based optimal designs are obtained\nfor $3^3$ factorial design. Robustness properties of the optimal designs are\nalso compared.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 04:46:43 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Bagmar", "Md. Shaddam Hossain", ""], ["Bari", "Wasimul", ""], ["Latif", "A. H. M. Mahbub", ""]]}, {"id": "1701.07745", "submitter": "Thomas Lumley", "authors": "Thomas Lumley", "title": "Pseudo-$R^2$ statistics under complex sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model summaries based on the ratio of fitted and null likelihoods have been\nproposed for generalised linear models, reducing to the familiar $R^2$\ncoefficient of determination in the Gaussian model with identity link. In this\nnote I show how to define the Cox--Snell and Nagelkerke summaries under\narbitrary probability sampling designs, giving a design-consistent estimator of\nthe population model summary. I also show that for logistic regression models\nunder case--control sampling the usual Cox--Snell and Nagelkerke $R^2$ are not\ndesign-consistent, but are systematically larger than would be obtained with a\ncross-sectional or cohort sample, even in settings where the weighted and\nunweighted logistic regression estimators are similar or identical.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 15:44:22 GMT"}], "update_date": "2017-01-27", "authors_parsed": [["Lumley", "Thomas", ""]]}, {"id": "1701.07776", "submitter": "Christos Merkatas", "authors": "Spyridon J. Hatjispyros, Christos Merkatas, Theodoros Nicoleris,\n  Stephen G. Walker", "title": "Dependent Mixtures of Geometric Weights Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach on the joint estimation of partially exchangeable observations\nis presented by constructing pairwise dependence between $m$ random density\nfunctions, each of which is modeled as a mixture of geometric stick breaking\nprocesses. This approach is based on a new random central masses version of the\nPairwise Dependent Dirichlet Process prior mixture model (PDDP) first\nintroduced in Hatjispyros et al. (2011). The idea is to create pairwise\ndependence through random measures that are location-preserving-expectations of\nDirichlet random measures. Our contention is that mixture modeling with\nPairwise Dependent Geometric Stick Breaking Process (PDGSBP) priors is\nsufficient for prediction and estimation purposes; moreover the associated\nGibbs sampler is much faster and easier to implement than its Dirichlet Process\nbased counterpart. To this respect, we provide a-priori-synchronized comparison\nstudies under sparse $m$-scalable synthetic and real data examples.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 17:06:30 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 15:47:12 GMT"}], "update_date": "2017-09-29", "authors_parsed": [["Hatjispyros", "Spyridon J.", ""], ["Merkatas", "Christos", ""], ["Nicoleris", "Theodoros", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1701.07787", "submitter": "Jere Koskela", "authors": "Jere Koskela", "title": "Multi-locus data distinguishes between population growth and multiple\n  merger coalescents", "comments": "24 pages, 13 figures", "journal-ref": "Statistical Applications in Genetics and Molecular Biology\n  17(3):20170011, 2018", "doi": "10.1515/sagmb-2017-0011", "report-no": null, "categories": "q-bio.PE q-bio.QM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a low dimensional function of the site frequency spectrum that\nis tailor-made for distinguishing coalescent models with multiple mergers from\nKingman coalescent models with population growth, and use this function to\nconstruct a hypothesis test between these model classes. The null and\nalternative sampling distributions of the statistic are intractable, but its\nlow dimensionality renders them amenable to Monte Carlo estimation. We\nconstruct kernel density estimates of the sampling distributions based on\nsimulated data, and show that the resulting hypothesis test dramatically\nimproves on the statistical power of a current state-of-the-art method. A key\nreason for this improvement is the use of multi-locus data, in particular\naveraging observed site frequency spectra across unlinked loci to reduce\nsampling variance. We also demonstrate the robustness of our method to nuisance\nand tuning parameters. Finally we show that the same kernel density estimates\ncan be used to conduct parameter estimation, and argue that our method is\nreadily generalisable for applications in model selection, parameter inference\nand experimental design.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 17:40:31 GMT"}, {"version": "v2", "created": "Wed, 8 Feb 2017 17:18:01 GMT"}, {"version": "v3", "created": "Thu, 9 Feb 2017 12:18:42 GMT"}, {"version": "v4", "created": "Tue, 5 Sep 2017 14:18:29 GMT"}, {"version": "v5", "created": "Fri, 23 Mar 2018 08:48:37 GMT"}, {"version": "v6", "created": "Thu, 19 Apr 2018 15:17:51 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Koskela", "Jere", ""]]}, {"id": "1701.07899", "submitter": "Emilie Devijver", "authors": "Emilie Devijver, M\\'elina Gallopin and Emeline Perthame", "title": "Nonlinear network-based quantitative trait prediction from\n  transcriptomic data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantitatively predicting phenotype variables by the expression changes in a\nset of candidate genes is of great interest in molecular biology but it is also\na challenging task for several reasons. First, the collected biological\nobservations might be heterogeneous and correspond to different biological\nmechanisms. Secondly, the gene expression variables used to predict the\nphenotype are potentially highly correlated since genes interact though unknown\nregulatory networks. In this paper, we present a novel approach designed to\npredict quantitative trait from transcriptomic data, taking into account the\nheterogeneity in biological samples and the hidden gene regulatory networks\nunderlying different biological mechanisms. The proposed model performs well on\nprediction but it is also fully parametric, which facilitates the downstream\nbiological interpretation. The model provides clusters of individuals based on\nthe relation between gene expression data and the phenotype, and also leads to\ninfer a gene regulatory network specific for each cluster of individuals. We\nperform numerical simulations to demonstrate that our model is competitive with\nother prediction models, and we demonstrate the predictive performance and the\ninterpretability of our model to predict alcohol sensitivity from\ntranscriptomic data on real data from Drosophila Melanogaster Genetic Reference\nPanel (DGRP).\n", "versions": [{"version": "v1", "created": "Thu, 26 Jan 2017 23:05:40 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 18:53:25 GMT"}, {"version": "v3", "created": "Fri, 12 May 2017 13:14:53 GMT"}, {"version": "v4", "created": "Wed, 19 Jul 2017 11:23:30 GMT"}, {"version": "v5", "created": "Thu, 20 Jul 2017 16:45:07 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Devijver", "Emilie", ""], ["Gallopin", "M\u00e9lina", ""], ["Perthame", "Emeline", ""]]}, {"id": "1701.07910", "submitter": "Daniel Eck", "authors": "Daniel J. Eck, Charles J. Geyer, and R. Dennis Cook", "title": "Combining Envelope Methodology and Aster Models for Variance Reduction\n  in Life History Analyses", "comments": "Title changed from \"An Application of Envelope Methodology and Aster\n  Models\" to \"Combining Envelope Methodology and Aster Models for Variance\n  Reduction in Life History Analyses\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Precise estimation of expected Darwinian fitness, the expected lifetime\nnumber of offspring of organism, is a central component of life history\nanalysis. The aster model serves as a defensible statistical model for\ndistributions of Darwinian fitness. The aster model is equipped to incorporate\nthe major life stages an organism travels through which separately may effect\nDarwinian fitness. Envelope methodology reduces asymptotic variability by\nestablishing a link between unknown parameters of interest and the asymptotic\ncovariance matrices of their estimators. It is known both theoretically and in\napplications that incorporation of envelope methodology reduces asymptotic\nvariability. We develop an envelope framework, including a new envelope\nestimator, that is appropriate for aster analyses. The level of precision\nprovided from our methods allows researchers to draw stronger conclusions about\nthe driving forces of Darwinian fitness from their life history analyses than\nthey could with the aster model alone. Our methods are illustrated on a\nsimulated dataset and a life history analysis of \\emph{Mimulus guttatus}\nflowers is provided. Useful variance reduction is obtained in both analyses.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 00:22:42 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 16:08:36 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Eck", "Daniel J.", ""], ["Geyer", "Charles J.", ""], ["Cook", "R. Dennis", ""]]}, {"id": "1701.08055", "submitter": "Franz J. Kir\\'aly", "authors": "Franz J. Kir\\'aly and Zhaozhi Qian", "title": "Modelling Competitive Sports: Bradley-Terry-\\'{E}l\\H{o} Models for\n  Supervised and On-Line Learning of Paired Competition Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction and modelling of competitive sports outcomes has received much\nrecent attention, especially from the Bayesian statistics and machine learning\ncommunities. In the real world setting of outcome prediction, the seminal\n\\'{E}l\\H{o} update still remains, after more than 50 years, a valuable baseline\nwhich is difficult to improve upon, though in its original form it is a\nheuristic and not a proper statistical \"model\". Mathematically, the \\'{E}l\\H{o}\nrating system is very closely related to the Bradley-Terry models, which are\nusually used in an explanatory fashion rather than in a predictive supervised\nor on-line learning setting.\n  Exploiting this close link between these two model classes and some newly\nobserved similarities, we propose a new supervised learning framework with\nclose similarities to logistic regression, low-rank matrix completion and\nneural networks. Building on it, we formulate a class of structured log-odds\nmodels, unifying the desirable properties found in the above: supervised\nprobabilistic prediction of scores and wins/draws/losses, batch/epoch and\non-line learning, as well as the possibility to incorporate features in the\nprediction, without having to sacrifice simplicity, parsimony of the\nBradley-Terry models, or computational efficiency of \\'{E}l\\H{o}'s original\napproach.\n  We validate the structured log-odds modelling approach in synthetic\nexperiments and English Premier League outcomes, where the added expressivity\nyields the best predictions reported in the state-of-art, close to the quality\nof contemporary betting odds.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 14:01:53 GMT"}], "update_date": "2017-01-30", "authors_parsed": [["Kir\u00e1ly", "Franz J.", ""], ["Qian", "Zhaozhi", ""]]}, {"id": "1701.08140", "submitter": "Jesus Daniel Arroyo Relion", "authors": "Jes\\'us D. Arroyo-Reli\\'on, Daniel Kessler, Elizaveta Levina, Stephan\n  F. Taylor", "title": "Network classification with applications to brain connectomics", "comments": null, "journal-ref": null, "doi": "10.1214/19-AOAS1252", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While statistical analysis of a single network has received a lot of\nattention in recent years, with a focus on social networks, analysis of a\nsample of networks presents its own challenges which require a different set of\nanalytic tools. Here we study the problem of classification of networks with\nlabeled nodes, motivated by applications in neuroimaging. Brain networks are\nconstructed from imaging data to represent functional connectivity between\nregions of the brain, and previous work has shown the potential of such\nnetworks to distinguish between various brain disorders, giving rise to a\nnetwork classification problem. Existing approaches tend to either treat all\nedge weights as a long vector, ignoring the network structure, or focus on\ngraph topology as represented by summary measures while ignoring the edge\nweights. Our goal is to design a classification method that uses both the\nindividual edge information and the network structure of the data in a\ncomputationally efficient way, and that can produce a parsimonious and\ninterpretable representation of differences in brain connectivity patterns\nbetween classes. We propose a graph classification method that uses edge\nweights as predictors but incorporates the network nature of the data via\npenalties that promote sparsity in the number of nodes, in addition to the\nusual sparsity penalties that encourage selection of edges. We implement the\nmethod via efficient convex optimization and provide a detailed analysis of\ndata from two fMRI studies of schizophrenia.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 18:26:38 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 06:39:26 GMT"}, {"version": "v3", "created": "Fri, 1 Feb 2019 20:13:32 GMT"}], "update_date": "2019-10-23", "authors_parsed": [["Arroyo-Reli\u00f3n", "Jes\u00fas D.", ""], ["Kessler", "Daniel", ""], ["Levina", "Elizaveta", ""], ["Taylor", "Stephan F.", ""]]}, {"id": "1701.08142", "submitter": "Clara Grazian", "authors": "Clara Grazian, Fabrizio Leisen, Brunero Liseo", "title": "Modelling Preference Data with the Wallenius Distribution", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wallenius distribution is a generalisation of the Hypergeometric\ndistribution where weights are assigned to balls of different colours. This\nnaturally defines a model for ranking categories which can be used for\nclassification purposes. Since, in general, the resulting likelihood is not\nanalytically available, we adopt an approximate Bayesian computational (ABC)\napproach for estimating the importance of the categories. We illustrate the\nperformance of the estimation procedure on simulated datasets. Finally, we use\nthe new model for analysing two datasets about movies ratings and Italian\nacademic statisticians' journal preferences. The latter is a novel dataset\ncollected by the authors.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jan 2017 18:30:09 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2017 14:45:06 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 15:54:59 GMT"}, {"version": "v4", "created": "Wed, 7 Feb 2018 12:03:47 GMT"}, {"version": "v5", "created": "Thu, 28 Jun 2018 14:46:09 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Grazian", "Clara", ""], ["Leisen", "Fabrizio", ""], ["Liseo", "Brunero", ""]]}, {"id": "1701.08284", "submitter": "Mareile Gro{\\ss}e Ruse", "authors": "Mareile Gro{\\ss}e Ruse, Adeline Samson and Susanne Ditlevsen", "title": "Multivariate inhomogeneous diffusion models with covariates and mixed\n  effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling of longitudinal data often requires diffusion models that\nincorporate overall time-dependent, nonlinear dynamics of multiple components\nand provide sufficient flexibility for subject-specific modeling. This\ncomplexity challenges parameter inference and approximations are inevitable. We\npropose a method for approximate maximum-likelihood parameter estimation in\nmultivariate time-inhomogeneous diffusions, where subject-specific flexibility\nis accounted for by incorporation of multidimensional mixed effects and\ncovariates. We consider $N$ multidimensional independent diffusions $X^i =\n(X^i_t)_{0\\leq t\\leq T^i}, 1\\leq i\\leq N$, with common overall model structure\nand unknown fixed-effects parameter $\\mu$. Their dynamics differ by the\nsubject-specific random effect $\\phi^i$ in the drift and possibly by (known)\ncovariate information, different initial conditions and observation times and\nduration. The distribution of $\\phi^i$ is parametrized by an unknown\n$\\vartheta$ and $\\theta = (\\mu, \\vartheta)$ is the target of statistical\ninference. Its maximum likelihood estimator is derived from the continuous-time\nlikelihood. We prove consistency and asymptotic normality of $\\hat{\\theta}_N$\nwhen the number $N$ of subjects goes to infinity using standard techniques and\nconsider the more general concept of local asymptotic normality for less\nregular models. The bias induced by time-discretization of sufficient\nstatistics is investigated. We discuss verification of conditions and\ninvestigate parameter estimation and hypothesis testing in simulations.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jan 2017 11:37:37 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Ruse", "Mareile Gro\u00dfe", ""], ["Samson", "Adeline", ""], ["Ditlevsen", "Susanne", ""]]}, {"id": "1701.08363", "submitter": "Eduardo Garc\\'ia-Portugu\\'es", "authors": "Juan A. Cuesta-Albertos, Eduardo Garc\\'ia-Portugu\\'es, Manuel\n  Febrero-Bande, Wenceslao Gonz\\'alez-Manteiga", "title": "Goodness-of-fit tests for the functional linear model based on randomly\n  projected empirical processes", "comments": "Paper: 23 pages, 4 figures, 1 table. Supplementary material: 17\n  pages, 4 figures, 3 tables", "journal-ref": "The Annals of Statistics, 47(1):439-467, 2019", "doi": "10.1214/18-AOS1693", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider marked empirical processes indexed by a randomly projected\nfunctional covariate to construct goodness-of-fit tests for the functional\nlinear model with scalar response. The test statistics are built from\ncontinuous functionals over the projected process, resulting in computationally\nefficient tests that exhibit root-n convergence rates and circumvent the curse\nof dimensionality. The weak convergence of the empirical process is obtained\nconditionally on a random direction, whilst the almost surely equivalence\nbetween the testing for significance expressed on the original and on the\nprojected functional covariate is proved. The computation of the test in\npractice involves calibration by wild bootstrap resampling and the combination\nof several p-values, arising from different projections, by means of the false\ndiscovery rate method. The finite sample properties of the tests are\nillustrated in a simulation study for a variety of linear models, underlying\nprocesses, and alternatives. The software provided implements the tests and\nallows the replication of simulations and data applications.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 10:48:39 GMT"}, {"version": "v2", "created": "Fri, 24 Mar 2017 10:11:52 GMT"}, {"version": "v3", "created": "Mon, 9 Apr 2018 11:09:12 GMT"}, {"version": "v4", "created": "Mon, 21 Sep 2020 10:17:25 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Cuesta-Albertos", "Juan A.", ""], ["Garc\u00eda-Portugu\u00e9s", "Eduardo", ""], ["Febrero-Bande", "Manuel", ""], ["Gonz\u00e1lez-Manteiga", "Wenceslao", ""]]}, {"id": "1701.08381", "submitter": "Dimosthenis Tsagkrasoulis", "authors": "Dimosthenis Tsagkrasoulis and Giovanni Montana", "title": "Random Forest regression for manifold-valued responses", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An increasing array of biomedical and computer vision applications requires\nthe predictive modeling of complex data, for example images and shapes. The\nmain challenge when predicting such objects lies in the fact that they do not\ncomply to the assumptions of Euclidean geometry. Rather, they occupy non-linear\nspaces, a.k.a. manifolds, where it is difficult to define concepts such as\ncoordinates, vectors and expected values. In this work, we construct a\nnon-parametric predictive methodology for manifold-valued objects, based on a\ndistance modification of the Random Forest algorithm. Our method is versatile\nand can be applied both in cases where the response space is a well-defined\nmanifold, but also when such knowledge is not available. Model fitting and\nprediction phases only require the definition of a suitable distance function\nfor the observed responses. We validate our methodology using simulations and\napply it on a series of illustrative image completion applications, showcasing\nsuperior predictive performance, compared to various established regression\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jan 2017 13:47:38 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 22:07:32 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Tsagkrasoulis", "Dimosthenis", ""], ["Montana", "Giovanni", ""]]}, {"id": "1701.08515", "submitter": "Chris Holmes", "authors": "Chris Holmes and Stephen Walker", "title": "Assigning a value to a power likelihood in a general Bayesian model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian approaches to data analysis and machine learning are widespread and\npopular as they provide intuitive yet rigorous axioms for learning from data;\nsee Bernardo and Smith (2004) and Bishop (2006). However, this rigour comes\nwith a caveat that the Bayesian model is a precise reflection of Nature. There\nhas been a recent trend to address potential model misspecification by raising\nthe likelihood function to a power, primarily for robustness reasons, though\nnot exclusively. In this paper we provide a coherent specification of the power\nparameter once the Bayesian model has been specified in the absence of a\nperfect model.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 09:10:14 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Holmes", "Chris", ""], ["Walker", "Stephen", ""]]}, {"id": "1701.08671", "submitter": "David Fisher David N Fisher", "authors": "David N Fisher, Matthew J Silk, Daniel W Franks", "title": "The perceived assortativity of social networks: Methodological problems\n  and solutions", "comments": "27 pages, including two figures, a table and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph q-bio.QM stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Networks describe a range of social, biological and technical phenomena. An\nimportant property of a network is its degree correlation or assortativity,\ndescribing how nodes in the network associate based on their number of\nconnections. Social networks are typically thought to be distinct from other\nnetworks in being assortative (possessing positive degree correlations);\nwell-connected individuals associate with other well-connected individuals, and\npoorly-connected individuals associate with each other. We review the evidence\nfor this in the literature and find that, while social networks are more\nassortative than non-social networks, only when they are built using\ngroup-based methods do they tend to be positively assortative. Non-social\nnetworks tend to be disassortative. We go on to show that connecting\nindividuals due to shared membership of a group, a commonly used method, biases\ntowards assortativity unless a large enough number of censuses of the network\nare taken. We present a number of solutions to overcoming this bias by drawing\non advances in sociological and biological fields. Adoption of these methods\nacross all fields can greatly enhance our understanding of social networks and\nnetworks in general.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 16:08:42 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Fisher", "David N", ""], ["Silk", "Matthew J", ""], ["Franks", "Daniel W", ""]]}, {"id": "1701.08673", "submitter": "Roland Langrock", "authors": "Jennifer Pohle and Roland Langrock and Floris van Beest and Niels\n  Martin Schmidt", "title": "Selecting the Number of States in Hidden Markov Models - Pitfalls,\n  Practical Challenges and Pragmatic Solutions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the notorious problem of order selection in hidden Markov models,\ni.e. of selecting an adequate number of states, highlighting typical pitfalls\nand practical challenges arising when analyzing real data. Extensive\nsimulations are used to demonstrate the reasons that render order selection\nparticularly challenging in practice despite the conceptual simplicity of the\ntask. In particular, we demonstrate why well-established formal procedures for\nmodel selection, such as those based on standard information criteria, tend to\nfavor models with numbers of states that are undesirably large in situations\nwhere states shall be meaningful entities. We also offer a pragmatic\nstep-by-step approach together with comprehensive advice for how practitioners\ncan implement order selection. Our proposed strategy is illustrated with a\nreal-data case study on muskox movement.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 16:11:52 GMT"}, {"version": "v2", "created": "Fri, 14 Apr 2017 19:51:55 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Pohle", "Jennifer", ""], ["Langrock", "Roland", ""], ["van Beest", "Floris", ""], ["Schmidt", "Niels Martin", ""]]}, {"id": "1701.08687", "submitter": "Christian Hansen", "authors": "Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo,\n  Christian Hansen, and Whitney Newey", "title": "Double/Debiased/Neyman Machine Learning of Treatment Effects", "comments": "Conference paper, forthcoming in American Economic Review, Papers and\n  Proceedings, 2017. arXiv admin note: text overlap with arXiv:1608.00060", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey (2016) provide a\ngeneric double/de-biased machine learning (DML) approach for obtaining valid\ninferential statements about focal parameters, using Neyman-orthogonal scores\nand cross-fitting, in settings where nuisance parameters are estimated using a\nnew generation of nonparametric fitting methods for high-dimensional data,\ncalled machine learning methods. In this note, we illustrate the application of\nthis method in the context of estimating average treatment effects (ATE) and\naverage treatment effects on the treated (ATTE) using observational data. A\nmore general discussion and references to the existing literature are available\nin Chernozhukov, Chetverikov, Demirer, Duflo, Hansen, and Newey (2016).\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 16:31:44 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Chetverikov", "Denis", ""], ["Demirer", "Mert", ""], ["Duflo", "Esther", ""], ["Hansen", "Christian", ""], ["Newey", "Whitney", ""]]}, {"id": "1701.08862", "submitter": "Geert H. van Kollenburg", "authors": "Geert H. van Kollenburg, Marcel A. Croon", "title": "Indirect (mediated) Moderation: The missing link in integrating\n  mediation and moderation in regression analysis", "comments": "This version has been submitted to Journal of Educational and\n  Behavioral Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two of the most important extensions of the basic regression model are\nmoderated effects (due to interactions) and mediated effects (i.e. indirect\neffects). Combinations of these effects may also be present. In this work an\nimportant, yet missing, combination is presented that can determine whether a\nmoderating effect itself is mediated by another variable. This indirect\nmoderation model can be assessed by a four-step decision tree which guides the\nuser through the necessary regression analyses to infer or refute indirect\nmoderation. A simulation experiment shows how the method works under some basic\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jan 2017 22:36:20 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 06:45:36 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["van Kollenburg", "Geert H.", ""], ["Croon", "Marcel A.", ""]]}, {"id": "1701.08994", "submitter": "Miguel de Carvalho", "authors": "Miguel de Carvalho, Garritt L. Page, Bradley J. Barney", "title": "On the geometry of Bayesian inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a geometric interpretation to Bayesian inference that allows us to\nintroduce a natural measure of the level of agreement between priors,\nlikelihoods, and posteriors. The starting point for the construction of our\ngeometry is the simple observation that the marginal likelihood can be regarded\nas an inner product between the prior and the likelihood. A key concept in our\ngeometry is that of compatibility, a measure which is based on the same\nconstruction principles as Pearson correlation, but which can be used to assess\nhow much the prior agrees with the likelihood, to gauge the sensitivity of the\nposterior to the prior, and to quantify the coherency of the opinions of two\nexperts. Estimators for all the quantities involved in our geometric setup are\ndiscussed, which can be directly computed from the posterior simulation output.\nSome examples are used to illustrate our methods, including data related to\non-the-job drug usage, midge wing length, and prostate cancer.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 11:16:12 GMT"}, {"version": "v2", "created": "Wed, 23 May 2018 17:44:07 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["de Carvalho", "Miguel", ""], ["Page", "Garritt L.", ""], ["Barney", "Bradley J.", ""]]}, {"id": "1701.09025", "submitter": "Steve Huntsman", "authors": "Steve Huntsman", "title": "Topological density estimation", "comments": "29 pages; mostly figures and MATLAB code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.AT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce \\emph{topological density estimation} (TDE), in which the\nmultimodal structure of a probability density function is topologically\ninferred and subsequently used to perform bandwidth selection for kernel\ndensity estimation. We show that TDE has performance and runtime advantages\nover competing methods of kernel density estimation for highly multimodal\nprobability density functions. We also show that TDE yields useful auxiliary\ninformation, that it can determine its own suitability for use, and we explain\nits performance.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 13:16:37 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Huntsman", "Steve", ""]]}, {"id": "1701.09143", "submitter": "Casey Kneale", "authors": "Casey Kneale, Karl S. Booksh", "title": "Effective Calibration Transfer via M\\\"obius and Affine Transformations", "comments": "To be revised and submitted to the Journal of Chemometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel technique for calibration transfer called the Modified Four Point\nInterpolant (MFPI) method is introduced for near infrared spectra. The method\nis founded on physical intuition and utilizes a series of quasiconformal maps\nin the frequency domain to transfer spectra from a slave instrument to a master\ninstrument's approximated space. Comparisons between direct standardization\n(DS), piecewise direct standardization (PDS), and MFPI for two publicly\navailable datasets are detailed herein. The results suggest that MFPI can\noutperform DS and PDS with respect to root mean squared errors of transfer and\nprediction. Combinations of MFPI with DS/PDS are also shown to reduce\npredictive errors after transfer.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 17:27:21 GMT"}], "update_date": "2017-02-01", "authors_parsed": [["Kneale", "Casey", ""], ["Booksh", "Karl S.", ""]]}]