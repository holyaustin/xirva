[{"id": "1806.00048", "submitter": "Zachary del Rosario", "authors": "Zachary del Rosario, Richard W. Fenrich, and Gianluca Iaccarino", "title": "Cutting the Double Loop: Theory and Algorithms for Reliability-Based\n  Design Optimization with Statistical Uncertainty", "comments": "25 pages, 16 figures, linked GitHub repo", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical uncertainties complicate engineering design -- confounding\nregulated design approaches, and degrading the performance of reliability\nefforts. The simplest means to tackle this uncertainty is double loop\nsimulation; a nested Monte Carlo method that, for practical problems, is\nintractable. In this work, we introduce a flexible, general approximation\ntechnique that obviates the double loop. This approximation is constructed in\nthe context of a novel theory of reliability design under statistical\nuncertainty: We introduce metrics for measuring the efficacy of RBDO strategies\n(effective margin and effective reliability), minimal conditions for\ncontrolling uncertain reliability (precision margin), and stricter conditions\nthat guarantee the desired reliability at a designed confidence level. We\nprovide a number of examples with open-source code to demonstrate our\napproaches in a reproducible fashion.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 18:50:58 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 18:53:10 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["del Rosario", "Zachary", ""], ["Fenrich", "Richard W.", ""], ["Iaccarino", "Gianluca", ""]]}, {"id": "1806.00221", "submitter": "Jakob Rasmussen", "authors": "Jakob Gulddahl Rasmussen", "title": "Lecture Notes: Temporal Point Processes and the Conditional Intensity\n  Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These short lecture notes contain a not too technical introduction to point\nprocesses on the time line. The focus lies on defining these processes using\nthe conditional intensity function. Furthermore, likelihood inference, methods\nof simulation and residual analysis for temporal point processes specified by a\nconditional intensity function are considered.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 07:35:20 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Rasmussen", "Jakob Gulddahl", ""]]}, {"id": "1806.00225", "submitter": "Mirko Signorelli", "authors": "Mirko Signorelli, Ernst Wit", "title": "Model-based clustering for populations of networks", "comments": "The final (published) version of the article can be downloaded for\n  free (Open Access) from the editor's website (click on the DOI link below)", "journal-ref": "Statistical Modelling, 2020, 20 (1), 9-29", "doi": "10.1177/1471082X19871128", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Until recently obtaining data on populations of networks was typically rare.\nHowever, with the advancement of automatic monitoring devices and the growing\nsocial and scientific interest in networks, such data has become more widely\navailable. From sociological experiments involving cognitive social structures\nto fMRI scans revealing large-scale brain networks of groups of patients, there\nis a growing awareness that we urgently need tools to analyse populations of\nnetworks and particularly to model the variation between networks due to\ncovariates. We propose a model-based clustering method based on mixtures of\ngeneralized linear (mixed) models that can be employed to describe the joint\ndistribution of a populations of networks in a parsimonious manner and to\nidentify subpopulations of networks that share certain topological properties\nof interest (degree distribution, community structure, effect of covariates on\nthe presence of an edge, etc.). Maximum likelihood estimation for the proposed\nmodel can be efficiently carried out with an implementation of the EM\nalgorithm. We assess the performance of this method on simulated data and\nconclude with an example application on advice networks in a small business.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 07:48:28 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 13:23:55 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 10:13:21 GMT"}, {"version": "v4", "created": "Mon, 20 Jan 2020 12:00:43 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Signorelli", "Mirko", ""], ["Wit", "Ernst", ""]]}, {"id": "1806.00275", "submitter": "Martin Radloff", "authors": "Martin Radloff and Rainer Schwabe", "title": "Locally $D$-optimal Designs for Non-linear Models on the $k$-dimensional\n  Ball", "comments": null, "journal-ref": "Journal of Statistical Planning and Inference 203 (2019) 106-116", "doi": "10.1016/j.jspi.2019.03.004", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we construct (locally) $D$-optimal designs for a wide class of\nnon-linear multiple regression models, when the design region is a\n$k$-dimensional ball. For this construction we make use of the concept of\ninvariance and equivariance in the context of optimal designs. As examples we\nconsider Poisson and negative binomial regression as well as proportional\nhazard models with censoring. By generalisation we can extend these results to\narbitrary ellipsoids.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 10:31:36 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Radloff", "Martin", ""], ["Schwabe", "Rainer", ""]]}, {"id": "1806.00446", "submitter": "Lu Chen", "authors": "Balgobin Nandram, Lu Chen, Shuting Fu and Binod Manandhar", "title": "Bayesian Logistic Regression for Small Areas with Numerous Households", "comments": "36 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze binary data, available for a relatively large number (big data) of\nfamilies (or households), which are within small areas, from a population-based\nsurvey. Inference is required for the finite population proportion of\nindividuals with a specific character for each area. To accommodate the binary\ndata and important features of all sampled individuals, we use a hierarchical\nBayesian logistic regression model with each family (not area) having its own\nrandom effect. This modeling helps to correct for overshrinkage so common in\nsmall area estimation. Because there are numerous families, the computational\ntime on the joint posterior density using standard Markov chain Monte Carlo\n(MCMC) methods is prohibitive. Therefore, the joint posterior density of the\nhyper-parameters is approximated using an integrated nested normal\napproximation (INNA) via the multiplication rule. This approach provides a\nsampling-based method that permits fast computation, thereby avoiding very\ntime-consuming MCMC methods. Then, the random effects are obtained from the\nexact conditional posterior density using parallel computing. The unknown\nnonsample features and household sizes are obtained using a nested Bayesian\nbootstrap that can be done using parallel computing as well. For relatively\nsmall data sets (e.g., 5000 families), we compare our method with a MCMC method\nto show that our approach is reasonable. We discuss an example on health\nseverity using the Nepal Living Standards Survey (NLSS).\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 17:04:27 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Nandram", "Balgobin", ""], ["Chen", "Lu", ""], ["Fu", "Shuting", ""], ["Manandhar", "Binod", ""]]}, {"id": "1806.00473", "submitter": "Vanda In\\'acio de Carvalho", "authors": "Vanda Inacio de Carvalho and Maria Xose Rodriguez-Alvarez", "title": "Bayesian nonparametric inference for the covariate-adjusted ROC curve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate diagnosis of disease is of fundamental importance in clinical\npractice and medical research. Before a medical diagnostic test is routinely\nused in practice, its ability to distinguish between diseased and nondiseased\nstates must be rigorously assessed through statistical analysis. The receiver\noperating characteristic (ROC) curve is the most popular used tool for\nevaluating the discriminatory ability of continuous-outcome diagnostic tests.\nIt has been acknowledged that several factors (e.g., subject-specific\ncharacteristics, such as age and/or gender) can affect the test's accuracy\nbeyond disease status. Recently, the covariate-adjusted ROC curve has been\nproposed and successfully applied as a global summary measure of diagnostic\naccuracy that takes covariate information into account. We motivate the use of\nthe covariate-adjusted ROC curve and develop a highly robust model based on a\ncombination of B-splines dependent Dirichlet process mixture models and the\nBayesian bootstrap. Multiple simulation studies demonstrate the ability of our\nmodel to successfully recover the true covariate-adjusted ROC curve and to\nproduce valid inferences in a variety of complex scenarios. Our methods are\nmotivated by and applied to an endocrine study where the main goal is to assess\nthe accuracy of the body mass index, adjusted for age and gender, for\npredicting clusters of cardiovascular disease risk factors. The R-package AROC,\nimplementing our proposed methods, is provided.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 17:36:25 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["de Carvalho", "Vanda Inacio", ""], ["Rodriguez-Alvarez", "Maria Xose", ""]]}, {"id": "1806.00550", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Will Stephenson, Runjing Liu, Michael I. Jordan, Tamara\n  Broderick", "title": "A Swiss Army Infinitesimal Jackknife", "comments": "Accepted at AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The error or variability of machine learning algorithms is often assessed by\nrepeatedly re-fitting a model with different weighted versions of the observed\ndata. The ubiquitous tools of cross-validation (CV) and the bootstrap are\nexamples of this technique. These methods are powerful in large part due to\ntheir model agnosticism but can be slow to run on modern, large data sets due\nto the need to repeatedly re-fit the model. In this work, we use a linear\napproximation to the dependence of the fitting procedure on the weights,\nproducing results that can be faster than repeated re-fitting by an order of\nmagnitude. This linear approximation is sometimes known as the \"infinitesimal\njackknife\" in the statistics literature, where it is mostly used as a\ntheoretical tool to prove asymptotic results. We provide explicit finite-sample\nerror bounds for the infinitesimal jackknife in terms of a small number of\nsimple, verifiable assumptions. Our results apply whether the weights and data\nare stochastic or deterministic, and so can be used as a tool for proving the\naccuracy of the infinitesimal jackknife on a wide variety of problems. As a\ncorollary, we state mild regularity conditions under which our approximation\nconsistently estimates true leave-$k$-out cross-validation for any fixed $k$.\nThese theoretical results, together with modern automatic differentiation\nsoftware, support the application of the infinitesimal jackknife to a wide\nvariety of practical problems in machine learning, providing a \"Swiss Army\ninfinitesimal jackknife\". We demonstrate the accuracy of our methods on a range\nof simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 21:48:44 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 04:18:49 GMT"}, {"version": "v3", "created": "Fri, 19 Oct 2018 20:52:13 GMT"}, {"version": "v4", "created": "Sun, 24 Mar 2019 21:55:48 GMT"}, {"version": "v5", "created": "Fri, 7 Feb 2020 17:18:46 GMT"}], "update_date": "2020-02-10", "authors_parsed": [["Giordano", "Ryan", ""], ["Stephenson", "Will", ""], ["Liu", "Runjing", ""], ["Jordan", "Michael I.", ""], ["Broderick", "Tamara", ""]]}, {"id": "1806.00731", "submitter": "Guangwei Weng", "authors": "Charles R. Doss and Guangwei Weng", "title": "Bandwidth selection for kernel density estimators of multivariate level\n  sets and highest density regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider bandwidth matrix selection for kernel density estimators (KDEs)\nof density level sets in $\\mathbb{R}^d$, $d \\ge 2$. We also consider estimation\nof highest density regions, which differs from estimating level sets in that\none specifies the probability content of the set rather than specifying the\nlevel directly. This complicates the problem. Bandwidth selection for KDEs is\nwell studied, but the goal of most methods is to minimize a global loss\nfunction for the density or its derivatives. The loss we consider here is\ninstead the measure of the symmetric difference of the true set and estimated\nset. We derive an asymptotic approximation to the corresponding risk. The\napproximation depends on unknown quantities which can be estimated, and the\napproximation can then be minimized to yield a choice of bandwidth, which we\nshow in simulations performs well. We provide an R package lsbs for\nimplementing our procedure.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 02:50:59 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 01:42:22 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Doss", "Charles R.", ""], ["Weng", "Guangwei", ""]]}, {"id": "1806.00792", "submitter": "Yongli Sang", "authors": "Yongli Sang, Xin Dang and Yichuan Zhao", "title": "Jackknife Empirical Likelihood Methods for Gini Correlations and their\n  Equality Testing", "comments": "20 pages, 6 tables, two figures", "journal-ref": null, "doi": "10.1016/j.jspi.2018.05.004", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gini correlation plays an important role in measuring dependence of\nrandom variables with heavy tailed distributions, whose properties are a\nmixture of Pearson's and Spearman's correlations. Due to the structure of this\ndependence measure, there are two Gini correlations between each pair of random\nvariables, which are not equal in general. Both the Gini correlation and the\nequality of the two Gini correlations play important roles in Economics. In the\nliterature, there are limited papers focusing on the inference of the Gini\ncorrelations and their equality testing. In this paper, we develop the\njackknife empirical likelihood (JEL) approach for the single Gini correlation,\nfor testing the equality of the two Gini correlations, and for the Gini\ncorrelations' differences of two independent samples. The standard limiting\nchi-square distributions of those jackknife empirical likelihood ratio\nstatistics are established and used to construct confidence intervals,\nrejection regions, and to calculate $p$-values of the tests. Simulation studies\nshow that our methods are competitive to existing methods in terms of coverage\naccuracy and shortness of confidence intervals, as well as in terms of power of\nthe tests. The proposed methods are illustrated in an application on a real\ndata set from UCI Machine Learning Repository.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 13:22:07 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Sang", "Yongli", ""], ["Dang", "Xin", ""], ["Zhao", "Yichuan", ""]]}, {"id": "1806.00849", "submitter": "Chaoran Hu", "authors": "Vladimir Pozdnyakov, L. Mark Elbroch, Chaoran Hu, Thomas Meyer, Jun\n  Yan", "title": "On estimation for Brownian motion governed by telegraph process with\n  multiple off states", "comments": null, "journal-ref": "Methodology and Computing in Applied Probability 22 (2020)\n  1275-1291", "doi": "10.1007/s11009-020-09774-1", "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Brownian motion whose infinitesimal variance changes according to a\nthree-state continuous time Markov Chain is studied. This Markov Chain can be\nviewed as a telegraph process with one on state and two off states. We first\nderive the distribution of occupation time of the on state. Then the result is\nused to develop a likelihood estimation procedure when the stochastic process\nat hand is observed at discrete, possibly irregularly spaced time points. The\nlikelihood function is evaluated with the forward algorithm in the general\nframework of hidden Markov models. The analytic results are confirmed with\nsimulation studies. The estimation procedure is applied to analyze the position\ndata from a mountain lion.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 18:49:50 GMT"}], "update_date": "2020-08-25", "authors_parsed": [["Pozdnyakov", "Vladimir", ""], ["Elbroch", "L. Mark", ""], ["Hu", "Chaoran", ""], ["Meyer", "Thomas", ""], ["Yan", "Jun", ""]]}, {"id": "1806.00928", "submitter": "Georgia Papadogeorgou", "authors": "Georgia Papadogeorgou, Francesca Dominici", "title": "A causal exposure response function with local adjustment for\n  confounding: Estimating health effects of exposure to low levels of ambient\n  fine particulate matter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Clean Air Act mandates that the National Ambient Air Quality Standards\n(NAAQS) must be routinely assessed to protect populations based on the latest\nscience. Therefore, researchers should continue to address whether exposure to\nlevels of air pollution below the NAAQS is harmful to human health. The\ncontentious nature surrounding environmental regulations urges us to cast this\nquestion within a causal inference framework. Parametric and semi-parametric\nregression approaches have been used to estimate the exposure-response (ER)\ncurve between ambient air pollution and health outcomes. Most of these\napproaches are not formulated within a causal framework, adjust for the same\ncovariates across all levels of exposure, and do not account for model\nuncertainty. We introduce a Bayesian framework for the estimation of a causal\nER curve called LERCA (Local Exposure Response Confounding Adjustment), which\nallows for different confounders and different strength of confounding at the\ndifferent exposure levels; and propagates uncertainty regarding confounders'\nselection and the shape of the ER. LERCA provides a principled way of assessing\nthe covariates' confounding importance at different exposure levels, providing\nresearchers with information regarding the variables to adjust for in\nregression models. Using simulations, we show that state of the art approaches\nperform poorly in estimating the ER curve in the presence of local confounding.\nLERCA is used to evaluate the relationship between exposure to ambient PM2.5\nand cardiovascular hospitalizations for 5,362 zip codes in the US, while\nadjusting for a potentially varying set of confounders across the exposure\nrange. Ambient PM2.5 leads to an increase in cardiovascular hospitalization\nrates when focusing at the low exposure range. Our results indicate that there\nis no threshold for the effect of PM2.5 on cardiovascular hospitalizations.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 02:21:24 GMT"}, {"version": "v2", "created": "Fri, 15 Mar 2019 20:47:02 GMT"}, {"version": "v3", "created": "Wed, 8 Jan 2020 16:26:34 GMT"}], "update_date": "2020-01-09", "authors_parsed": [["Papadogeorgou", "Georgia", ""], ["Dominici", "Francesca", ""]]}, {"id": "1806.00954", "submitter": "Peter Rousseeuw", "authors": "Mia Hubert, Peter J. Rousseeuw, Wannes Van den Bossche", "title": "MacroPCA: An all-in-one PCA method allowing for missing values as well\n  as cellwise and rowwise outliers", "comments": null, "journal-ref": "Technometrics, 2019, Vol. 61, 459-473", "doi": "10.1080/00401706.2018.1562989", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate data are typically represented by a rectangular matrix (table)\nin which the rows are the objects (cases) and the columns are the variables\n(measurements). When there are many variables one often reduces the dimension\nby principal component analysis (PCA), which in its basic form is not robust to\noutliers. Much research has focused on handling rowwise outliers, i.e. rows\nthat deviate from the majority of the rows in the data (for instance, they\nmight belong to a different population). In recent years also cellwise outliers\nare receiving attention. These are suspicious cells (entries) that can occur\nanywhere in the table. Even a relatively small proportion of outlying cells can\ncontaminate over half the rows, which causes rowwise robust methods to break\ndown. In this paper a new PCA method is constructed which combines the\nstrengths of two existing robust methods in order to be robust against both\ncellwise and rowwise outliers. At the same time, the algorithm can cope with\nmissing values. As of yet it is the only PCA method that can deal with all\nthree problems simultaneously. Its name MacroPCA stands for PCA allowing for\nMissingness And Cellwise & Rowwise Outliers. Several simulations and real data\nsets illustrate its robustness. New residual maps are introduced, which help to\ndetermine which variables are responsible for the outlying behavior. The method\nis well-suited for online process control.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 05:08:27 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 06:20:38 GMT"}, {"version": "v3", "created": "Sun, 9 Dec 2018 15:16:07 GMT"}], "update_date": "2021-01-13", "authors_parsed": [["Hubert", "Mia", ""], ["Rousseeuw", "Peter J.", ""], ["Bossche", "Wannes Van den", ""]]}, {"id": "1806.01015", "submitter": "Christian R\\\"over", "authors": "Christian R\\\"over and Tim Friede", "title": "Dynamically borrowing strength from another study through shrinkage\n  estimation", "comments": "23 pages, 3 figures, 4 tables", "journal-ref": "Statistical Methods in Medical Research, 29(1):293-308, 2020", "doi": "10.1177/0962280219833079", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-analytic methods may be used to combine evidence from different sources\nof information. Quite commonly, the normal-normal hierarchical model (NNHM)\nincluding a random-effect to account for between-study heterogeneity is\nutilized for such analyses. The same modeling framework may also be used to not\nonly derive a combined estimate, but also to borrow strength for a particular\nstudy from another by deriving a shrinkage estimate. For instance, a\nsmall-scale randomized controlled trial could be supported by a non-randomized\nstudy, e.g. a clinical registry. This would be particularly attractive in the\ncontext of rare diseases. We demonstrate that a meta-analysis still makes sense\nin this extreme two-study setup, as illustrated using a recent trial and a\nclinical registry in Creutzfeld-Jakob disease. Derivation of a shrinkage\nestimate within a Bayesian random-effects meta-analysis may substantially\nimprove a given estimate even based on only a single additional estimate while\naccounting for potential effect heterogeneity between the studies.\nAlternatively, inference may equivalently be motivated via a model\nspecification that does not require a common overall mean parameter but\nconsiders the treatment effect in one study, and the difference in effects\nbetween the studies. The proposed approach is quite generally applicable to\ncombine different types of evidence originating e.g. from meta-analyses or\nindividual studies. An application of this more general setup is provided in\nimmunosuppression following liver transplantation in children.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 08:56:54 GMT"}, {"version": "v2", "created": "Tue, 4 Dec 2018 14:36:16 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["R\u00f6ver", "Christian", ""], ["Friede", "Tim", ""]]}, {"id": "1806.01082", "submitter": "Fran\\c{c}ois Portier", "authors": "Fran\\c{c}ois Portier, Ingrid Van Keilegom and Anouar El Ghouch", "title": "On an extension of the promotion time cure model", "comments": "41 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the distribution of time-to-event data\nthat are subject to censoring and for which the event of interest might never\noccur, i.e., some subjects are cured. To model this kind of data in the\npresence of covariates, one of the leading semiparametric models is the\npromotion time cure model \\citep{yakovlev1996}, which adapts the Cox model to\nthe presence of cured subjects. Estimating the conditional distribution results\nin a complicated constrained optimization problem, and inference is difficult\nas no closed-formula for the variance is available. We propose a new model,\ninspired by the Cox model, that leads to a simple estimation procedure and that\npresents a closed formula for the variance. We derive some asymptotic\nproperties of the estimators and we show the practical behaviour of our\nprocedure by means of simulations. We also apply our model and estimation\nmethod to a breast cancer data set.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 12:51:03 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Portier", "Fran\u00e7ois", ""], ["Van Keilegom", "Ingrid", ""], ["Ghouch", "Anouar El", ""]]}, {"id": "1806.01083", "submitter": "Michele Santacatterina", "authors": "Nathan Kallus, Michele Santacatterina", "title": "Optimal Balancing of Time-Dependent Confounders for Marginal Structural\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal structural models (MSMs) estimate the causal effect of a\ntime-varying treatment in the presence of time-dependent confounding via\nweighted regression. The standard approach of using inverse probability of\ntreatment weighting (IPTW) can lead to high-variance estimates due to extreme\nweights and be sensitive to model misspecification. Various methods have been\nproposed to partially address this, including truncation and stabilized-IPTW to\ntemper extreme weights and covariate balancing propensity score (CBPS) to\naddress treatment model misspecification. In this paper, we present Kernel\nOptimal Weighting (KOW), a convex-optimization-based approach that finds\nweights for fitting the MSM that optimally balance time-dependent confounders\nwhile simultaneously controlling for precision, directly addressing the above\nlimitations. KOW directly minimizes the error in estimation due to\ntime-dependent confounding via a new decomposition as a functional. We further\nextend KOW to control for informative censoring. We evaluate the performance of\nKOW in a simulation study, comparing it with IPTW, stabilized-IPTW, and CBPS.\nWe demonstrate the use of KOW in studying the effect of treatment initiation on\ntime-to-death among people living with HIV and the effect of negative\nadvertising on elections in the United States.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 12:51:05 GMT"}, {"version": "v2", "created": "Mon, 12 Aug 2019 14:02:43 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kallus", "Nathan", ""], ["Santacatterina", "Michele", ""]]}, {"id": "1806.01094", "submitter": "Sebastian Weichwald", "authors": "Niklas Pfister, Sebastian Weichwald, Peter B\\\"uhlmann, Bernhard\n  Sch\\\"olkopf", "title": "Robustifying Independent Component Analysis by Adjusting for Group-Wise\n  Stationary Noise", "comments": "equal contribution between Pfister and Weichwald", "journal-ref": "Journal of Machine Learning Research, 20(147):1-50, 2019. (\n  http://www.jmlr.org/papers/v20/18-399.html )", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.QM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce coroICA, confounding-robust independent component analysis, a\nnovel ICA algorithm which decomposes linearly mixed multivariate observations\ninto independent components that are corrupted (and rendered dependent) by\nhidden group-wise stationary confounding. It extends the ordinary ICA model in\na theoretically sound and explicit way to incorporate group-wise (or\nenvironment-wise) confounding. We show that our proposed general noise model\nallows to perform ICA in settings where other noisy ICA procedures fail.\nAdditionally, it can be used for applications with grouped data by adjusting\nfor different stationary noise within each group. Our proposed noise model has\na natural relation to causality and we explain how it can be applied in the\ncontext of causal inference. In addition to our theoretical framework, we\nprovide an efficient estimation procedure and prove identifiability of the\nunmixing matrix under mild assumptions. Finally, we illustrate the performance\nand robustness of our method on simulated data, provide audible and visual\nexamples, and demonstrate the applicability to real-world scenarios by\nexperiments on publicly available Antarctic ice core data as well as two EEG\ndata sets. We provide a scikit-learn compatible pip-installable Python package\ncoroICA as well as R and Matlab implementations accompanied by a documentation\nat https://sweichwald.de/coroICA/\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 13:17:14 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 17:02:44 GMT"}, {"version": "v3", "created": "Wed, 30 Oct 2019 14:38:49 GMT"}], "update_date": "2019-10-31", "authors_parsed": [["Pfister", "Niklas", ""], ["Weichwald", "Sebastian", ""], ["B\u00fchlmann", "Peter", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1806.01126", "submitter": "Tobias Hossfeld", "authors": "Tobias Hossfeld, Poul E. Heegaard, Martin Varela, Lea Skorin-Kapov", "title": "Confidence Interval Estimators for MOS Values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.HC cs.MM cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the quantification of QoE, subjects often provide individual rating\nscores on certain rating scales which are then aggregated into Mean Opinion\nScores (MOS). From the observed sample data, the expected value is to be\nestimated. While the sample average only provides a point estimator, confidence\nintervals (CI) are an interval estimate which contains the desired expected\nvalue with a given confidence level. In subjective studies, the number of\nsubjects performing the test is typically small, especially in lab\nenvironments. The used rating scales are bounded and often discrete like the\n5-point ACR rating scale. Therefore, we review statistical approaches in the\nliterature for their applicability in the QoE domain for MOS interval\nestimation (instead of having only a point estimator, which is the MOS). We\nprovide a conservative estimator based on the SOS hypothesis and binomial\ndistributions and compare its performance (CI width, outlier ratio of CI\nviolating the rating scale bounds) and coverage probability with well known CI\nestimators. We show that the provided CI estimator works very well in practice\nfor MOS interval estimators, while the commonly used studentized CIs suffer\nfrom a positive outlier ratio, i.e., CIs beyond the bounds of the rating scale.\nAs an alternative, bootstrapping, i.e., random sampling of the subjective\nratings with replacement, is an efficient CI estimator leading to typically\nsmaller CIs, but lower coverage than the proposed estimator.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 13:59:55 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Hossfeld", "Tobias", ""], ["Heegaard", "Poul E.", ""], ["Varela", "Martin", ""], ["Skorin-Kapov", "Lea", ""]]}, {"id": "1806.01139", "submitter": "Jerome Dockes", "authors": "J\\'er\\^ome Dock\\`es (PARIETAL), Demian Wassermann (PARIETAL), Russell\n  Poldrack, Fabian Suchanek, Bertrand Thirion (PARIETAL), Ga\\\"el Varoquaux\n  (PARIETAL)", "title": "Text to brain: predicting the spatial distribution of neuroimaging\n  observations from text reports", "comments": null, "journal-ref": "MICCAI 2018 - 21st International Conference on Medical Image\n  Computing and Computer Assisted Intervention, Sep 2018, Granada, Spain.\n  pp.1-18, 2018", "doi": null, "report-no": null, "categories": "stat.ME cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the digital nature of magnetic resonance imaging, the resulting\nobservations are most frequently reported and stored in text documents. There\nis a trove of information untapped in medical health records, case reports, and\nmedical publications. In this paper, we propose to mine brain medical\npublications to learn the spatial distribution associated with anatomical\nterms. The problem is formulated in terms of minimization of a risk on\ndistributions which leads to a least-deviation cost function. An efficient\nalgorithm in the dual then learns the mapping from documents to brain\nstructures. Empirical results using coordinates extracted from the\nbrain-imaging literature show that i) models must adapt to semantic variation\nin the terms used to describe a given anatomical structure, ii) voxel-wise\nparameterization leads to higher likelihood of locations reported in unseen\ndocuments, iii) least-deviation cost outperforms least-square. As a proof of\nconcept for our method, we use our model of spatial distributions to predict\nthe distribution of specific neurological conditions from text-only reports.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 14:16:43 GMT"}, {"version": "v2", "created": "Tue, 5 Jun 2018 06:37:41 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 10:48:51 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Dock\u00e8s", "J\u00e9r\u00f4me", "", "PARIETAL"], ["Wassermann", "Demian", "", "PARIETAL"], ["Poldrack", "Russell", "", "PARIETAL"], ["Suchanek", "Fabian", "", "PARIETAL"], ["Thirion", "Bertrand", "", "PARIETAL"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"]]}, {"id": "1806.01238", "submitter": "Juan A. Cuesta-Albertos", "authors": "Eustasio del Barrio, Juan A. Cuesta-Albertos, Marc Hallin and Carlos\n  Matr\\'an", "title": "Center-Outward Distribution Functions, Quantiles, Ranks, and Signs in\n  $\\mathbb{R}^d$", "comments": "66 pages 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Univariate concepts as quantile and distribution functions involving ranks\nand signs, do not canonically extend to $\\mathbb{R}^d, d\\geq 2$. Palliating\nthat has generated an abundant literature. Chapter 1 shows that, unlike the\nmany definitions that have been proposed so far, the measure\ntransportation-based ones introduced in Chernozhukov et al. (2017) enjoy all\nthe properties that make univariate quantiles and ranks successful tools for\nsemiparametric statistical inference.\n  We therefore propose a new center-outward definition of multivariate\ndistribution and quantile functions, along with their empirical counterparts,\nfor which we obtain a Glivenko-Cantelli result. Our approach is geometric and,\ncontrary to the Monge-Kantorovich one in Chernozhukov et al. (2017), does not\nrequire any moment assumptions. The resulting ranks and signs are strictly\ndistribution-free, and maximal invariant under the action of a data-driven\nclass of (order-preserving) transformations generating the family of absolutely\ncontinuous distributions; that property is the theoretical foundation of the\nsemiparametric efficiency preservation property of ranks. The corresponding\nquantiles are equivariant under the same transformations.\n  The empirical proposed distribution functions are defined at observed values\nonly. A continuous extension to the entire $\\mathbb{R}^d$, yielding continuous\nempirical quantile contours while preserving the monotonicity and\nGlivenko-Cantelli features is desirable. Such extension requires solving a\nnontrivial problem of smooth interpolation under cyclical monotonicity\nconstraints. A complete solution of that problem is given in Chapter 2; we show\nthat the resulting distribution and quantile functions are Lipschitz, and\nprovide a sharp lower bound for the Lipschitz constants. A numerical study of\nempirical center-outward quantile contours and their consistency is conducted.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:25:38 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 12:46:54 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 08:26:22 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["del Barrio", "Eustasio", ""], ["Cuesta-Albertos", "Juan A.", ""], ["Hallin", "Marc", ""], ["Matr\u00e1n", "Carlos", ""]]}, {"id": "1806.01325", "submitter": "Diaa Al Mohamad", "authors": "Diaa Al Mohamad and Jelle J. Goeman and Erik W. van Zwet and Eric A.\n  Cator", "title": "Adaptive Critical Value for Constrained Likelihood Ratio Testing", "comments": "We proved the conjecture from last version. We found out that some\n  part of this works was already published in the literature and was made clear\n  in the current version. The main text is the first 16 pages. The appendix\n  includes other ideas and a part that was already discussed in the literature", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new way of testing ordered hypotheses against all alternatives\nwhich overpowers the classical approach both in simplicity and statistical\npower. Our new method tests the constrained likelihood ratio statistic against\nthe quantile of one and only one chi-squared random variable with a\ndata-dependent degrees of freedom instead of a mixture of chi-squares. Our new\ntest is proved to have a valid finite-sample significance level $\\alpha$ and\nprovides more power especially for sparse alternatives (those with a few or\nmoderate number of null constraints violations) in comparison to the classical\napproach. Our method is also easier to use than the classical approach which\nrequires to calculate or simulate a set of complicated weights. Two special\ncases are considered with more details, namely the case of testing orthants\n$\\mu_1<0, \\cdots, \\mu_n<0$ and the isotonic case of testing $\\mu_1<\\mu_2<\\mu_3$\nagainst all alternatives. Contours of the difference in power are shown for\nthese examples showing the interest of our new approach.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 18:57:29 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 07:41:54 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Mohamad", "Diaa Al", ""], ["Goeman", "Jelle J.", ""], ["van Zwet", "Erik W.", ""], ["Cator", "Eric A.", ""]]}, {"id": "1806.01326", "submitter": "Leying Guan", "authors": "Leying Guan and Robert Tibshirani", "title": "Post model-fitting exploration via a \"Next-Door\" analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple method for evaluating the model that has been chosen by\nan adaptive regression procedure, our main focus being the lasso. This\nprocedure deletes each chosen predictor and refits the lasso to get a set of\nmodels that are \"close\" to the one chosen, referred to as \"base model\". If the\ndeletion of a predictor leads to significant deterioration in the model's\npredictive power, the predictor is called indispensable; otherwise, the nearby\nmodel is called acceptable and can serve as a good alternative to the base\nmodel. This provides both an assessment of the predictive contribution of each\nvariable and a set of alternative models that may be used in place of the\nchosen model.\n  In this paper, we will focus on the cross-validation (CV) setting and a\nmodel's predictive power is measured by its CV error, with base model tuned by\ncross-validation. We propose a method for comparing the error rates of the base\nmodel with that of nearby models, and a p-value for testing whether a predictor\nis dispensable. We also propose a new quantity called model score which works\nsimilarly as the p-value for the control of type I error. Our proposal is\nclosely related to the LOCO (leave-one-covarate-out) methods of ([Rinaldo 2016\nBootstrapping]) and less so, to Stability Selection ([Meinshausen 2010\nstability]).\n  We call this procedure \"Next-Door analysis\" since it examines models close to\nthe base model. It can be applied to Gaussian regression data, generalized\nlinear models, and other supervised learning problems with $\\ell_1$\npenalization. It could also be applied to best subset and stepwise regression\nprocedures. We have implemented it in the R language as a library to accompany\nthe well-known {\\tt glmnet} library.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 19:00:14 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Guan", "Leying", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1806.01401", "submitter": "Avanti Athreya", "authors": "Avanti Athreya, Minh Tang, Youngser Park, Carey E. Priebe", "title": "On estimation and inference in latent structure random graphs", "comments": "6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define a latent structure model (LSM) random graph as a random dot product\ngraph (RDPG) in which the latent position distribution incorporates both\nprobabilistic and geometric constraints, delineated by a family of underlying\ndistributions on some fixed Euclidean space, and a structural support\nsubmanifold from which the latent positions for the graph are drawn. For a\none-dimensional latent structure model with known structural support, we show\nhow spectral estimates of the latent positions of an RDPG can be used for\nefficient estimation of the paramaters of the LSM. We describe how to estimate\nor learn the structural support in cases where it is unknown, with an\nillustrative focus on graphs with latent positions along the Hardy-Weinberg\ncurve. Finally, we use the latent structure model formulation to test bilateral\nhomology in the Drosophila connectome.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 21:31:48 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 22:12:45 GMT"}, {"version": "v3", "created": "Fri, 17 Apr 2020 02:56:44 GMT"}], "update_date": "2020-04-20", "authors_parsed": [["Athreya", "Avanti", ""], ["Tang", "Minh", ""], ["Park", "Youngser", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1806.01412", "submitter": "Peter Carbonetto", "authors": "Youngseok Kim, Peter Carbonetto, Matthew Stephens, Mihai Anitescu", "title": "A fast algorithm for maximum likelihood estimation of mixture\n  proportions using sequential quadratic programming", "comments": "28 pages, 6 figures", "journal-ref": "Journal of Computational and Graphical Statistics 29 (2020),\n  261-273", "doi": "10.1080/10618600.2019.1689985", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Maximum likelihood estimation of mixture proportions has a long history, and\ncontinues to play an important role in modern statistics, including in\ndevelopment of nonparametric empirical Bayes methods. Maximum likelihood of\nmixture proportions has traditionally been solved using the expectation\nmaximization (EM) algorithm, but recent work by Koenker & Mizera shows that\nmodern convex optimization techniques -- in particular, interior point methods\n-- are substantially faster and more accurate than EM. Here, we develop a new\nsolution based on sequential quadratic programming (SQP). It is substantially\nfaster than the interior point method, and just as accurate. Our approach\ncombines several ideas: first, it solves a reformulation of the original\nproblem; second, it uses an SQP approach to make the best use of the expensive\ngradient and Hessian computations; third, the SQP iterations are implemented\nusing an active set method to exploit the sparse nature of the quadratic\nsubproblems; fourth, it uses accurate low-rank approximations for more\nefficient gradient and Hessian computations. We illustrate the benefits of our\napproach in experiments on synthetic data sets as well as a large genetic\nassociation data set. In large data sets (n = 1,000,000 observations, m = 1,000\nmixture components), our implementation achieves at least 100-fold reduction in\nruntime compared with a state-of-the-art interior point solver. Our methods are\nimplemented in Julia, and in an R package available on CRAN (see\nhttps://CRAN.R-project.org/package=mixsqp).\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 22:11:11 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 20:51:05 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 18:30:21 GMT"}, {"version": "v4", "created": "Fri, 8 Nov 2019 16:34:27 GMT"}, {"version": "v5", "created": "Wed, 9 Dec 2020 18:49:37 GMT"}], "update_date": "2020-12-10", "authors_parsed": [["Kim", "Youngseok", ""], ["Carbonetto", "Peter", ""], ["Stephens", "Matthew", ""], ["Anitescu", "Mihai", ""]]}, {"id": "1806.01453", "submitter": "Chih-Li Sung", "authors": "Chih-Li Sung, Ying Hung, William Rittase, Cheng Zhu, C. F. Jeff Wu", "title": "Calibration for computer experiments with binary responses and\n  application to cell adhesion study", "comments": "39 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calibration refers to the estimation of unknown parameters which are present\nin computer experiments but not available in physical experiments. An accurate\nestimation of these parameters is important because it provides a scientific\nunderstanding of the underlying system which is not available in physical\nexperiments. Most of the work in the literature is limited to the analysis of\ncontinuous responses. Motivated by a study of cell adhesion experiments, we\npropose a new calibration framework for binary responses. Its application to\nthe T cell adhesion data provides insight into the unknown values of the\nkinetic parameters which are difficult to determine by physical experiments due\nto the limitation of the existing experimental techniques.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 01:21:31 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 20:51:24 GMT"}, {"version": "v3", "created": "Wed, 20 Mar 2019 14:53:53 GMT"}], "update_date": "2019-03-21", "authors_parsed": [["Sung", "Chih-Li", ""], ["Hung", "Ying", ""], ["Rittase", "William", ""], ["Zhu", "Cheng", ""], ["Wu", "C. F. Jeff", ""]]}, {"id": "1806.01458", "submitter": "Jacob Parsons", "authors": "Jacob Parsons and Le Bao", "title": "The Value of Information in Retrospect", "comments": "23 pages, 3 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the course of any statistical analysis, it is necessary to consider issues\nof data quality and model appropriateness. Value of information methods were\ninitially put forward in the middle of the twentieth century in order to\nprovide a framework for choosing between potential sources of information.\nHowever, since their genesis, value of information methods have been largely\nneglected by statisticians. In this paper we review and extend existing value\nof information methods and recommend the use of three quantities for\nidentifying influential and outlying data: an influence measure previously\nsuggested by \\cite{kempthorne1986}, a related quantity known as the expected\nvalue of sample information that is used to gauge how much influence we would\nexpect a portion of the data to have, and the ratio of these two quantities\nwhich serves as a comparison between observed influence and expected influence.\n  We study the basic theoretical properties of those quantities and illustrate\nour proposed approach using two datasets. A data set containing employment\nrates and other economic factors in U.S. first presented by \\cite{longley} is\nused to provide an example in the case of linear regression. HIV surveillance\ndata collected from prenatal clinics have been the main source of information\nfor monitoring the HIV epidemic in low and middle income countries. A data set\nproviding information about HIV prevalence in Swaziland is used as an example\nin the case of generalized linear mixed models.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 01:37:10 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 17:16:54 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Parsons", "Jacob", ""], ["Bao", "Le", ""]]}, {"id": "1806.01460", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal", "title": "Dynamic Function-on-Scalars Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a modeling framework for dynamic function-on-scalars regression,\nin which a time series of functional data is regressed on a time series of\nscalar predictors. The regression coefficient function for each predictor is\nallowed to be dynamic, which is essential for applications where the\nassociation between predictors and a (functional) response is time-varying. For\ngreater modeling flexibility, we design a nonparametric reduced-rank functional\ndata model with an unknown functional basis expansion, which is data-adaptive\nand, unlike most existing methods, modeled as unknown for appropriate\nuncertainty quantification. Within a Bayesian framework, we introduce shrinkage\npriors that simultaneously (i) regularize time-varying regression coefficient\nfunctions to be locally static, (ii) effectively remove unimportant predictor\nvariables from the model, and (iii) reduce sensitivity to the dimension of the\nfunctional basis. A simulation analysis confirms the importance of these\nshrinkage priors, with notable improvements over existing alternatives. We\ndevelop a novel projection-based Gibbs sampling algorithm, which offers\nunrivaled computational scalability for fully Bayesian functional regression.\nWe apply the proposed methodology (i) to analyze the time-varying impact of\nmacroeconomic variables on the U.S. yield curve and (ii) to characterize the\neffects of socioeconomic and demographic predictors on age-specific fertility\nrates in South and Southeast Asia.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 01:49:02 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 03:31:47 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Kowal", "Daniel R.", ""]]}, {"id": "1806.01558", "submitter": "Thomas Romary", "authors": "Thomas Romary (GEOSCIENCES), Nicolas Desassis", "title": "Combining covariance tapering and lasso driven low rank decomposition\n  for the kriging of large spatial datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large spatial datasets are becoming ubiquitous in environmental sciences with\nthe explosion in the amount of data produced by sensors that monitor and\nmeasure the Earth system. Consequently, the geostatistical analysis of these\ndata requires adequate methods. Richer datasets lead to more complex modeling\nbut may also prevent from using classical techniques. Indeed, the kriging\npredictor is not straightforwarldly available as it requires the inversion of\nthe covariance matrix of the data. The challenge of handling such datasets is\ntherefore to extract the maximum of information they contain while ensuring the\nnumerical tractability of the associated inference and prediction algorithms.\nThe different approaches that have been developed in the literature to address\nthis problem can be classified into two families, both aiming at making the\ninversion of the covariance matrix computationally feasible. The covariance\ntapering approach circumvents the problem by enforcing the sparsity of the\ncovariance matrix, making it invertible in a reasonable computation time. The\nsecond available approach assumes a low rank representation of the covariance\nfunction. While both approaches have their drawbacks, we propose a way to\ncombine them and benefit from their advantages. The covariance model is assumed\nto have the form low rank plus sparse. The choice of the basis functions\nsustaining the low rank component is data driven and is achieved through a\nselection procedure, thus alleviating the computational burden of the low rank\npart. This model expresses as a spatial random effects model and the estimation\nof the parameters is conducted through a step by step approach treating each\nscale separately. The resulting model can account for second order non\nstationarity and handle large volumes of data.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 08:44:44 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Romary", "Thomas", "", "GEOSCIENCES"], ["Desassis", "Nicolas", ""]]}, {"id": "1806.01615", "submitter": "Michael Crowther", "authors": "Michael J. Crowther", "title": "merlin - a unified modelling framework for data analysis and methods\n  development in Stata", "comments": "Submitted to the Stata Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  merlin can do a lot of things. From simple stuff, like fitting a linear\nregression or a Weibull survival model, to a three-level logistic mixed effects\nmodel, or a multivariate joint model of multiple longitudinal outcomes (of\ndifferent types) and a recurrent event and survival with non-linear\neffects...the list is rather endless. merlin can do things I haven't even\nthought of yet. I'll take a single dataset, and attempt to show you the full\nrange of capabilities of merlin, and discuss some future directions for the\nimplementation in Stata.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 11:37:53 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Crowther", "Michael J.", ""]]}, {"id": "1806.01760", "submitter": "Yuan Wu", "authors": "Yuan Wu and Xiaofei Wang and Jiaxing Lin and Beilin Jia and Kouros\n  Owzar", "title": "Predictive Accuracy of Markers or Risk Scores for Interval Censored\n  Survival Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for the evaluation of the predictive accuracy of biomarkers with\nrespect to survival outcomes subject to right censoring have been discussed\nextensively in the literature. In cancer and other diseases, survival outcomes\nare commonly subject to interval censoring by design or due to the follow up\nschema. In this paper, we present an estimator for the area under the\ntime-dependent receiver operating characteristic ROC curve for interval\ncensored data based on a nonparametric sieve maximum likelihood approach. We\nestablish the asymptotic properties of the proposed estimator, and illustrate\nits finite-sample properties using a simulation study. The application of our\nmethod is illustrated using data from a cancer clinical study. An open-source R\npackage to implement the proposed method is available on CRAN.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 15:56:39 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Wu", "Yuan", ""], ["Wang", "Xiaofei", ""], ["Lin", "Jiaxing", ""], ["Jia", "Beilin", ""], ["Owzar", "Kouros", ""]]}, {"id": "1806.01936", "submitter": "Jared Huling", "authors": "Xiaowu Dai and Jared D. Huling", "title": "Selection and Estimation Optimality in High Dimensions with the TWIN\n  Penalty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel class of variable selection penalties called TWIN, which\nprovides sensible data-adaptive penalization. Under a linear sparsity regime\nand random Gaussian designs we show that penalties in the TWIN class have a\nhigh probability of selecting the correct model and furthermore result in\nminimax optimal estimators. The general shape of penalty functions in the TWIN\nclass is the key ingredient to its desirable properties and results in improved\ntheoretical and empirical performance over existing penalties. In this work we\nintroduce two examples of TWIN penalties that admit simple and efficient\ncoordinate descent algorithms, making TWIN practical in large data settings. We\ndemonstrate in challenging and realistic simulation settings with high\ncorrelations between active and inactive variables that TWIN has high power in\nvariable selection while controlling the number of false discoveries,\noutperforming standard penalties.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 21:11:59 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Dai", "Xiaowu", ""], ["Huling", "Jared D.", ""]]}, {"id": "1806.01947", "submitter": "Alexander Fisch", "authors": "Alexander T. M. Fisch, Idris A. Eckley, Paul Fearnhead", "title": "A linear time method for the detection of point and collective anomalies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of efficiently identifying anomalies in data sequences is an\nimportant statistical problem that now arises in many applications. Whilst\nthere has been substantial work aimed at making statistical analyses robust to\noutliers, or point anomalies, there has been much less work on detecting\nanomalous segments, or collective anomalies, particularly in those settings\nwhere point anomalies might also occur. In this article, we introduce\nCollective And Point Anomalies (CAPA), a computationally efficient approach\nthat is suitable when collective anomalies are characterised by either a change\nin mean, variance, or both, and distinguishes them from point anomalies.\nTheoretical results establish the consistency of CAPA at detecting collective\nanomalies and, as a by-product, the consistency of a popular penalised cost\nbased change in mean and variance detection method. Empirical results show that\nCAPA has close to linear computational cost as well as being more accurate at\ndetecting and locating collective anomalies than other approaches. We\ndemonstrate the utility of CAPA through its ability to detect exoplanets from\nlight curve data from the Kepler telescope.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 22:02:53 GMT"}, {"version": "v2", "created": "Thu, 11 Apr 2019 11:57:18 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Fisch", "Alexander T. M.", ""], ["Eckley", "Idris A.", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1806.01956", "submitter": "Thuong Nguyen", "authors": "Thuong Nguyen", "title": "Distribution free goodness of fit tests for regularly varying tail\n  distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss in this paper a possibility of constructing a whole class of\nasymptotic distribution-free tests for testing regularly varying tail\ndistributions. The idea is that we treat the tails of distributions as members\nof a parametric family and using MLE to estimate the exponent. No matter what\nthe exponent's estimator is, we are able to transform the whole class into a\nspecific distribution with a prefix exponent so that we are free from choosing\nany functional of the tail empirical process as a distribution-free test\nstatistic. The asymptotic behavior of some new tests, as examples from the\nwhole class of new tests, are demonstrated as well.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 22:53:21 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Nguyen", "Thuong", ""]]}, {"id": "1806.01979", "submitter": "Andrew Song", "authors": "Andrew H. Song, Francisco Flores, Demba Ba", "title": "Spike Sorting by Convolutional Dictionary Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spike sorting refers to the problem of assigning action potentials observed\nin extra-cellular recordings of neural activity to the neuron(s) from which\nthey originate. We cast this problem as one of learning a convolutional\ndictionary from raw multi-electrode waveform data, subject to sparsity\nconstraints. In this context, sparsity refers to the number of neurons that are\nallowed to spike simultaneously. The convolutional dictionary setting, along\nwith its assumptions (e.g. refractoriness) that are motivated by the\nspike-sorting problem, let us give theoretical bounds on the sample complexity\nof spike sorting as a function of the number of underlying neurons, the rate of\noccurrence of simultaneous spiking, and the firing rate of the neurons. We\nderive memory/computation-efficient convolutional versions of OMP (cOMP) and\nKSVD (cKSVD), popular algorithms for sparse coding and dictionary learning\nrespectively. We demonstrate via simulations that an algorithm that alternates\nbetween cOMP and cKSVD can recover the underlying spike waveforms successfully,\nassuming few neurons spike simultaneously, and is stable in the presence of\nnoise. We also apply the algorithm to extra-cellular recordings from a tetrode\nin the rat Hippocampus.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 02:12:20 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Song", "Andrew H.", ""], ["Flores", "Francisco", ""], ["Ba", "Demba", ""]]}, {"id": "1806.02068", "submitter": "Tore Selland Kleppe", "authors": "Tore Selland Kleppe", "title": "Dynamically rescaled Hamiltonian Monte Carlo for Bayesian Hierarchical\n  Models", "comments": "Includes supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamically rescaled Hamiltonian Monte Carlo (DRHMC) is introduced as a\ncomputationally fast and easily implemented method for performing full Bayesian\nanalysis in hierarchical statistical models. The method relies on introducing a\nmodified parameterisation so that the re-parameterised target distribution has\nclose to constant scaling properties, and thus is easily sampled using standard\n(Euclidian metric) Hamiltonian Monte Carlo. Provided that the parameterisations\nof the conditional distributions specifying the hierarchical model are\n\"constant information parameterisations\" (CIP), the relation between the\nmodified- and original parameterisation is bijective, explicitly computed and\nadmit exploitation of sparsity in the numerical linear algebra involved. CIPs\nfor a large catalogue of statistical models are presented, and from the\ncatalogue, it is clear that many CIPs are currently routinely used in\nstatistical computing. A relation between the proposed methodology and a class\nof explicitly integrated Riemann manifold Hamiltonian Monte Carlo methods is\ndiscussed. The methodology is illustrated on several example models, including\na model for inflation rates with multiple levels of non-linearly dependent\nlatent variables.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:52:57 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 12:35:56 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Kleppe", "Tore Selland", ""]]}, {"id": "1806.02084", "submitter": "Maria Franco-Villoria", "authors": "Maria Franco-Villoria, Massimo Ventrucci, H{\\aa}vard Rue", "title": "A unified view on Bayesian varying coefficient models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Varying coefficient models are useful in applications where the effect of the\ncovariate might depend on some other covariate such as time or location.\nVarious applications of these models often give rise to case-specific prior\ndistributions for the parameter(s) describing how much the coefficients vary.\nIn this work, we introduce a unified view of varying coefficients models,\narguing for a way of specifying these prior distributions that are coherent\nacross various applications, avoid overfitting and have a coherent\ninterpretation. We do this by considering varying coefficients models as a\nflexible extension of the natural simpler model and capitalising on the\nrecently proposed framework of penalized complexity (PC) priors. We illustrate\nour approach in two spatial examples where varying coefficient models are\nrelevant.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 09:30:11 GMT"}, {"version": "v2", "created": "Wed, 4 Dec 2019 15:22:46 GMT"}], "update_date": "2019-12-05", "authors_parsed": [["Franco-Villoria", "Maria", ""], ["Ventrucci", "Massimo", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1806.02138", "submitter": "Soham Sarkar", "authors": "Soham Sarkar, Rahul Biswas and Anil K. Ghosh", "title": "On high-dimensional modifications of some graph-based two-sample tests", "comments": null, "journal-ref": "Machine Learning, 2019", "doi": "10.1007/s10994-019-05857-4", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing for the equality of two high-dimensional distributions is a\nchallenging problem, and this becomes even more challenging when the sample\nsize is small. Over the last few decades, several graph-based two-sample tests\nhave been proposed in the literature, which can be used for data of arbitrary\ndimensions. Most of these test statistics are computed using pairwise Euclidean\ndistances among the observations. But, due to concentration of pairwise\nEuclidean distances, these tests have poor performance in many high-dimensional\nproblems. Some of them can have powers even below the nominal level when the\nscale-difference between two distributions dominates the location-difference.\nTo overcome these limitations, we introduce a new class of dissimilarity\nindices and use it to modify some popular graph-based tests. These modified\ntests use the distance concentration phenomenon to their advantage, and as a\nresult, they outperform the corresponding tests based on the Euclidean distance\nin a wide variety of examples. We establish the high-dimensional consistency of\nthese modified tests under fairly general conditions. Analyzing several\nsimulated as well as real data sets, we demonstrate their usefulness in high\ndimension, low sample size situations.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 12:07:57 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Sarkar", "Soham", ""], ["Biswas", "Rahul", ""], ["Ghosh", "Anil K.", ""]]}, {"id": "1806.02160", "submitter": "Aliaksandr Hubin", "authors": "Aliaksandr Hubin, Geir Storvik and Florian Frommlet", "title": "Deep Bayesian regression models", "comments": "50 pages, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models are used for inference and prediction in a wide range of\napplications providing a powerful scientific tool for researchers and analysts\nfrom different fields. In many research fields the amount of available data as\nwell as the number of potential explanatory variables is rapidly increasing.\nVariable selection and model averaging have become extremely important tools\nfor improving inference and prediction. However, often linear models are not\nsufficient and the complex relationship between input variables and a response\nis better described by introducing non-linearities and complex functional\ninteractions. Deep learning models have been extremely successful in terms of\nprediction although they are often difficult to specify and potentially suffer\nfrom overfitting. The aim of this paper is to bring the ideas of deep learning\ninto a statistical framework which yields more parsimonious models and allows\nto quantify model uncertainty. To this end we introduce the class of deep\nBayesian regression models (DBRM) consisting of a generalized linear model\ncombined with a comprehensive non-linear feature space, where non-linear\nfeatures are generated just like in deep learning but combined with variable\nselection in order to include only important features. DBRM can easily be\nextended to include latent Gaussian variables to model complex correlation\nstructures between observations, which seems to be not easily possible with\nexisting deep learning approaches. Two different algorithms based on MCMC are\nintroduced to fit DBRM and to perform Bayesian inference. The predictive\nperformance of these algorithms is compared with a large number of state of the\nart algorithms. Furthermore we illustrate how DBRM can be used for model\ninference in various applications.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 13:13:46 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 09:41:27 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Hubin", "Aliaksandr", ""], ["Storvik", "Geir", ""], ["Frommlet", "Florian", ""]]}, {"id": "1806.02304", "submitter": "Yuexi Wang", "authors": "Yi Liu, Veronika Ro\\v{c}kov\\'a, Yuexi Wang", "title": "Variable Selection with ABC Bayesian Forests", "comments": null, "journal-ref": null, "doi": "10.1111/rssb.12423", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few problems in statistics are as perplexing as variable selection in the\npresence of very many redundant covariates. The variable selection problem is\nmost familiar in parametric environments such as the linear model or additive\nvariants thereof. In this work, we abandon the linear model framework, which\ncan be quite detrimental when the covariates impact the outcome in a non-linear\nway, and turn to tree-based methods for variable selection. Such variable\nscreening is traditionally done by pruning down large trees or by ranking\nvariables based on some importance measure. Despite heavily used in practice,\nthese ad-hoc selection rules are not yet well understood from a theoretical\npoint of view. In this work, we devise a Bayesian tree-based probabilistic\nmethod and show that it is consistent for variable selection when the\nregression surface is a smooth mix of $p>n$ covariates. These results are the\nfirst model selection consistency results for Bayesian forest priors.\nProbabilistic assessment of variable importance is made feasible by a\nspike-and-slab wrapper around sum-of-trees priors. Sampling from posterior\ndistributions over trees is inherently very difficult. As an alternative to\nMCMC, we propose ABC Bayesian Forests, a new ABC sampling method based on\ndata-splitting that achieves higher ABC acceptance rate. We show that the\nmethod is robust and successful at finding variables with high marginal\ninclusion probabilities. Our ABC algorithm provides a new avenue towards\napproximating the median probability model in non-parametric setups where the\nmarginal likelihood is intractable.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:09:43 GMT"}, {"version": "v2", "created": "Thu, 31 Jan 2019 22:27:07 GMT"}, {"version": "v3", "created": "Sun, 28 Jun 2020 02:05:47 GMT"}, {"version": "v4", "created": "Wed, 24 Feb 2021 16:33:04 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Liu", "Yi", ""], ["Ro\u010dkov\u00e1", "Veronika", ""], ["Wang", "Yuexi", ""]]}, {"id": "1806.02321", "submitter": "Patrick Perry", "authors": "Ningshan Zhang, Kyle Schmaus, Patrick O. Perry", "title": "Fitting a deeply-nested hierarchical model to a large book review\n  dataset using a moment-based estimator", "comments": "32 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a particular instance of a common problem in recommender systems:\nusing a database of book reviews to inform user-targeted recommendations. In\nour dataset, books are categorized into genres and sub-genres. To exploit this\nnested taxonomy, we use a hierarchical model that enables information pooling\nacross across similar items at many levels within the genre hierarchy. The main\nchallenge in deploying this model is computational: the data sizes are large,\nand fitting the model at scale using off-the-shelf maximum likelihood\nprocedures is prohibitive. To get around this computational bottleneck, we\nextend a moment-based fitting procedure proposed for fitting single-level\nhierarchical models to the general case of arbitrarily deep hierarchies. This\nextension is an order of magnetite faster than standard maximum likelihood\nprocedures. The fitting method can be deployed beyond recommender systems to\ngeneral contexts with deeply-nested hierarchical generalized linear mixed\nmodels.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 01:34:04 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Zhang", "Ningshan", ""], ["Schmaus", "Kyle", ""], ["Perry", "Patrick O.", ""]]}, {"id": "1806.02411", "submitter": "Bret Zeldow", "authors": "Bret Zeldow, James Flory, Alisa Stephens-Shields, Marsha Raebel, Jason\n  Roy", "title": "Outcome identification in electronic health records using predictions\n  from an enriched Dirichlet process mixture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel semiparametric model for the joint distribution of a\ncontinuous longitudinal outcome and the baseline covariates using an enriched\nDirichlet process (EDP) prior. This joint model decomposes into a linear mixed\nmodel for the outcome given the covariates and marginals for the covariates.\nThe nonparametric EDP prior is placed on the regression and spline\ncoefficients, the error variance, and the parameters governing the predictor\nspace. We predict the outcome at unobserved time points for subjects with data\nat other time points as well as for new subjects with only baseline covariates.\nWe find improved prediction over mixed models with Dirichlet process (DP)\npriors when there are a large number of covariates. Our method is demonstrated\nwith electronic health records consisting of initiators of second generation\nantipsychotic medications, which are known to increase the risk of diabetes. We\nuse our model to predict laboratory values indicative of diabetes for each\nindividual and assess incidence of suspected diabetes from the predicted\ndataset. Our model also serves as a functional clustering algorithm in which\nsubjects are clustered into groups with similar longitudinal trajectories of\nthe outcome over time.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 20:26:03 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Zeldow", "Bret", ""], ["Flory", "James", ""], ["Stephens-Shields", "Alisa", ""], ["Raebel", "Marsha", ""], ["Roy", "Jason", ""]]}, {"id": "1806.02419", "submitter": "Nicholas Adams Dr", "authors": "Nicholas Adams and Gerard O'Reilly", "title": "A Likelihood-based Alternative to Null Hypothesis Significance Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The logical and practical difficulties associated with research\ninterpretation using P values and null hypothesis significance testing have\nbeen extensively documented. This paper describes an alternative,\nlikelihood-based approach to P-value interpretation. The P-value and sample\nsize of a research study are used to derive a likelihood function with a single\nparameter, the estimated population effect size, and the method of maximum\nlikelihood estimation is used to calculate the most likely effect size.\nComparison of the likelihood of the most likely effect size and the likelihood\nof the minimum clinically significant effect size using the likelihood ratio\ntest yields the clinical significance support level (or S-value), a logical and\neasily understood metric of research evidence. This clinical significance\nlikelihood approach has distinct advantages over null hypothesis significance\ntesting. As motivating examples we demonstrate the calculation and\ninterpretation of S-values applied to two recent widely publicised trials,\nWOMAN from the Lancet and RELIEF from the New England Journal of Medicine.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 20:48:35 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 01:47:44 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Adams", "Nicholas", ""], ["O'Reilly", "Gerard", ""]]}, {"id": "1806.02430", "submitter": "Yunfan Tang", "authors": "Yunfan Tang, Dan Nicolae", "title": "Identifying Heritable Communities of Microbiome by Root-Unifrac and\n  Wishart Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to identify heritable microbiome communities when the\ninput is a pairwise dissimilarity matrix among all samples. Current methods\ntarget each taxon individually and are unable to take advantage of their\nphylogenetic relationships. In contrast, our approach focuses on community\nheritability by using the root-Unifrac to summarize the microbiome samples\nthrough their pairwise dissimilarities while taking the phylogeny into account.\nThe resulting dissimilarity matrix is then transformed into an outer product\nmatrix and further modeled through a Wishart distribution with the same set of\nvariance components as in the univariate model. Directly modeling the entire\ndissimilarity matrix allows us to bypass any dimension reduction steps. An\nimportant contribution of our work is to prove the positive definiteness of\nsuch outer product matrix, hence the applicability of the Wishart distribution.\nSimulation shows that this community heritability approach has higher power\nthan existing methods to identify heritable groups of taxa. Empirical results\non the TwinsUK dataset are also provided.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 21:16:06 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Tang", "Yunfan", ""], ["Nicolae", "Dan", ""]]}, {"id": "1806.02458", "submitter": "Theodore  Kypraios", "authors": "Iker Perez, Lax Chan, Mercedes Torres Torres, James Goulding, Theodore\n  Kypraios", "title": "On Bayesian inferential tasks with infinite-state jump processes:\n  efficient data augmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in sampling schemes for Markov jump processes have recently enabled\nmultiple inferential tasks. However, in statistical and machine learning\napplications, we often require that these continuous-time models find support\non structured and infinite state spaces. In these cases, exact sampling may\nonly be achieved by often inefficient particle filtering procedures, and\nrapidly augmenting observed datasets remains a significant challenge. Here, we\nbuild on the principles of uniformization and present a tractable framework to\naddress this problem, which greatly improves the efficiency of existing\nstate-of-the-art methods commonly used in small finite-state systems, and\nfurther scales their use to infinite-state scenarios. We capitalize on the\nmarginal role of variable subsets in a model hierarchy during the process\njumps, and describe an algorithm that relies on measurable mappings between\npairs of states and carefully designed sets of synthetic jump observations. The\nproposed method enables the efficient integration of slice sampling techniques\nand it can overcome the existing computational bottleneck. We offer evidence by\nmeans of experiments addressing inference and clustering tasks on both\nsimulated and real data sets.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 23:27:39 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Perez", "Iker", ""], ["Chan", "Lax", ""], ["Torres", "Mercedes Torres", ""], ["Goulding", "James", ""], ["Kypraios", "Theodore", ""]]}, {"id": "1806.02550", "submitter": "Ting Yan", "authors": "Ting Yan", "title": "A Unified Framework for Inference in Network Models with Degree\n  Heterogeneity and Homophily", "comments": "33 pages. Major revision. Add simulations. Changed the original title\n  to \"A Unified Framework for Inference in Network Models with Degree\n  Heterogeneity and Homophily\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The degree heterogeneity and homophily are two typical features in network\ndata. In this paper, we formulate a general model for undirected networks with\nthese two features and present the moment estimation for inferring the degree\nand homophily parameters. The binary or nonbinary network edges are\nsimultaneously considered. We establish a unified theoretical framework under\nwhich the consistency of the moment estimator holds as the size of networks\ngoes to infinity. We also derive the asymptotic representation of the moment\nestimator that can be used to characterize its limiting distribution. The\nasymptotic representation of the moment estimator of the homophily parameter\ncontains a bias term. Two applications are provided to illustrate the\ntheoretical result. Numerical studies and a real data analysis demonstrate our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 07:53:44 GMT"}, {"version": "v2", "created": "Mon, 7 Oct 2019 02:25:29 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Yan", "Ting", ""]]}, {"id": "1806.02588", "submitter": "C. H. Bryan Liu", "authors": "C. H. Bryan Liu, Elaine M. Bettaney, Benjamin Paul Chamberlain", "title": "Designing Experiments to Measure Incrementality on Facebook", "comments": "Accepted into 2018 AdKDD & TargetAd Workshop in conjunction with KDD\n  2018; 6 pages, 4 figures, and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of Facebook advertising has risen dramatically in recent\nyears, with the platform accounting for almost 20% of the global online ad\nspend in 2017. An important consideration in advertising is incrementality: how\nmuch of the change in an experimental metric is an advertising campaign\nresponsible for. To measure incrementality, Facebook provide lift studies. As\nFacebook lift studies differ from standard A/B tests, the online\nexperimentation literature does not describe how to calculate parameters such\nas power and minimum sample size. Facebook also offer multi-cell lift tests,\nwhich can be used to compare campaigns that don't have statistically identical\naudiences. In this case, there is no literature describing how to measure the\nsignificance of the difference in incrementality between cells, or how to\nestimate the power or minimum sample size. We fill these gaps in the literature\nby providing the statistical power and required sample size calculation for\nFacebook lift studies. We then generalise the statistical significance, power,\nand required sample size calculation to multi-cell lift studies. We represent\nour results theoretically in terms of the distributions of test metrics and in\npractical terms relating to the metrics used by practitioners, making all of\nour code publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 09:51:23 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 12:09:30 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Liu", "C. H. Bryan", ""], ["Bettaney", "Elaine M.", ""], ["Chamberlain", "Benjamin Paul", ""]]}, {"id": "1806.02670", "submitter": "Yang Ni", "authors": "Yang Ni and Peter M\\\"uller and Maurice Diesendruck and Sinead\n  Williamson and Yitan Zhu and Yuan Ji", "title": "Scalable Bayesian Nonparametric Clustering and Classification", "comments": "29 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a scalable multi-step Monte Carlo algorithm for inference under a\nlarge class of nonparametric Bayesian models for clustering and classification.\nEach step is \"embarrassingly parallel\" and can be implemented using the same\nMarkov chain Monte Carlo sampler. The simplicity and generality of our approach\nmakes inference for a wide range of Bayesian nonparametric mixture models\napplicable to large datasets. Specifically, we apply the approach to inference\nunder a product partition model with regression on covariates. We show results\nfor inference with two motivating data sets: a large set of electronic health\nrecords (EHR) and a bank telemarketing dataset. We find interesting clusters\nand favorable classification performance relative to other widely used\ncompeting classifiers.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 13:30:36 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Ni", "Yang", ""], ["M\u00fcller", "Peter", ""], ["Diesendruck", "Maurice", ""], ["Williamson", "Sinead", ""], ["Zhu", "Yitan", ""], ["Ji", "Yuan", ""]]}, {"id": "1806.02730", "submitter": "Dexter Cahoy", "authors": "Dexter Cahoy", "title": "A bootstrap test for equality of variances", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, Volume 54, Issue 10, 1\n  October 2010, Pages 2306-2316", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a bootstrap procedure to test the hypothesis $H_o$ that $K+1$\nvariances are homogeneous. The procedure uses a variance-based statistic, and\nis derived from a normal-theory test for equality of variances. The test\nequivalently expressed the hypothesis as $H_o: \\mathbf{\\eta}=(\n\\eta_1,\\ldots,\\eta_{K+1})^T=\\mathbf{0}$, where $\\eta_i$'s are log contrasts of\nthe population variances. A box-type acceptance region is constructed to test\nthe hypothesis $H_o$. Simulation results indicated that our method is generally\nsuperior to the Shoemaker and Levene tests, and the bootstrapped version of\nLevene test in controlling the Type I and Type II errors.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 15:25:11 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Cahoy", "Dexter", ""]]}, {"id": "1806.02748", "submitter": "Theresa Smith", "authors": "Theresa Smith", "title": "A stratified age-period-cohort model for spatial heterogeneity in\n  all-cause mortality", "comments": "23 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common goal in modeling demographic rates is to compare two or more groups.\nFor ex- ample comparing mortality rates between men and women or between\ngeographic regions may reveal health inequalities. A popular class of models\nfor all-cause mortality as well as incidence of specific diseases like cancer\nis the age-period-cohort (APC) model. Extending this model to the multivariate\nsetting is not straightforward because the univariate APC model suffers from\nwell-known identifiability problems. Often APC models are fit separately for\neach strata, and then comparisons are made post hoc. This paper introduces a\nstratified APC model to directly assess the sources of heterogeneity in\nmortality rates using a Bayesian hierarchical model with matrix-normal priors\nthat share information on linear and nonlinear aspects of the APC effects\nacross strata. Computing, model selection, and prior specification are\naddressed and the model is then applied to all-cause mortality data from the\nEuropean Union.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 16:02:39 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Smith", "Theresa", ""]]}, {"id": "1806.02774", "submitter": "Dexter Cahoy", "authors": "Dexter Cahoy, Vladimir V.Uchaikin, Wojbor A.Woyczynski", "title": "Parameter estimation for fractional Poisson processes", "comments": null, "journal-ref": "Journal of Statistical Planning and Inference, Volume 140, Issue\n  11, November 2010, Pages 3106-3120", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a formal estimation procedure for parameters of the\nfractional Poisson process (fPp). Such procedures are needed to make the fPp\nmodel usable in applied situations. The basic idea of fPp, motivated by\nexperimental data with long memory is to make the standard Poisson model more\nflexible by permitting non-exponential, heavy-tailed distributions of\ninterarrival times and different scaling properties. We establish the\nasymptotic normality of our estimators for the two parameters appearing in our\nfPp model. This fact permits construction of the corresponding confidence\nintervals. The properties of the estimators are then tested using simulated\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 16:43:40 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Cahoy", "Dexter", ""], ["Uchaikin", "Vladimir V.", ""], ["Woyczynski", "Wojbor A.", ""]]}, {"id": "1806.02792", "submitter": "Dexter Cahoy", "authors": "Dexter Cahoy", "title": "Estimation of Mittag-Leffler Parameters", "comments": null, "journal-ref": "Communications in Statistics - Simulation and Computation, Volume\n  42, Issue 2, 303-315 (2013)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a procedure for estimating the parameters of the Mittag-Leffler\n(ML) and the generalized Mittag-Leffler (GML) distributions. The algorithm is\nless restrictive, computationally simple, and necessary to make these models\nusable in practice. A comparison with the fractional moment estimator indicated\nfavorable results for the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 17:09:56 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Cahoy", "Dexter", ""]]}, {"id": "1806.02935", "submitter": "Kwangho Kim", "authors": "Kwangho Kim, Jisu Kim, Edward H. Kennedy", "title": "Causal effects based on distributional distances", "comments": "46 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a framework for characterizing causal effects via\ndistributional distances. In particular we define a causal effect in terms of\nthe $L_1$ distance between different counterfactual outcome distributions,\nrather than the typical mean difference in outcome values. Comparing entire\ncounterfactual outcome distributions can provide more nuanced and valuable\nmeasures for exploring causal effects beyond the average treatment effect.\nFirst, we propose a novel way to estimate counterfactual outcome densities,\nwhich is of independent interest. Then we develop an efficient estimator of our\ntarget causal effect. We go on to provide error bounds and asymptotic\nproperties of the proposed estimator, along with bootstrap-based confidence\nintervals. Finally, we illustrate the methods via simulations and real data.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 01:26:46 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2021 21:07:16 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kim", "Kwangho", ""], ["Kim", "Jisu", ""], ["Kennedy", "Edward H.", ""]]}, {"id": "1806.03087", "submitter": "Jie He", "authors": "Jie He, Xiaogang Duan, Shumei Zhang and Hui Li", "title": "Estimation of marginal model with subgroup auxiliary information", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal model is a popular instrument for studying longitudinal data and\ncluster data. This paper investigates the estimator of marginal model with\nsubgroup auxiliary information. To marginal model, we propose a new type of\nauxiliary information, and combine them with the traditional estimating\nequations of the quadratic inference function (QIF) method based on the\ngeneralized method of moments (GMM). Thus obtaining a more efficient estimator.\nThe asymptotic normality and the test statistics of the proposed estimator are\nestablished. The theoretical result shows that the estimator with subgroup\ninformation is more efficient than the conventional QIF one. Simulation studies\nare carried out to examine the performance of the proposed method under finite\nsample. We apply the proposed method to a real data for illustration.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 11:07:56 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["He", "Jie", ""], ["Duan", "Xiaogang", ""], ["Zhang", "Shumei", ""], ["Li", "Hui", ""]]}, {"id": "1806.03120", "submitter": "Julien  Chiquet Dr.", "authors": "Julien Chiquet, Mahendra Mariadassou, St\\'ephane Robin", "title": "Variational inference for sparse network reconstruction from count data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multivariate statistics, the question of finding direct interactions can\nbe formulated as a problem of network inference - or network reconstruction -\nfor which the Gaussian graphical model (GGM) provides a canonical framework.\nUnfortunately, the Gaussian assumption does not apply to count data which are\nencountered in domains such as genomics, social sciences or ecology.\n  To circumvent this limitation, state-of-the-art approaches use two-step\nstrategies that first transform counts to pseudo Gaussian observations and then\napply a (partial) correlation-based approach from the abundant literature of\nGGM inference. We adopt a different stance by relying on a latent model where\nwe directly model counts by means of Poisson distributions that are conditional\nto latent (hidden) Gaussian correlated variables. In this multivariate Poisson\nlognormal-model, the dependency structure is completely captured by the latent\nlayer. This parametric model enables to account for the effects of covariates\non the counts.\n  To perform network inference, we add a sparsity inducing constraint on the\ninverse covariance matrix of the latent Gaussian vector. Unlike the usual\nGaussian setting, the penalized likelihood is generally not tractable, and we\nresort instead to a variational approach for approximate likelihood\nmaximization. The corresponding optimization problem is solved by alternating a\ngradient ascent on the variational parameters and a graphical-Lasso step on the\ncovariance matrix.\n  We show that our approach is highly competitive with the existing methods on\nsimulation inspired from microbiological data. We then illustrate on three\nvarious data sets how accounting for sampling efforts via offsets and\nintegrating external covariates (which is mostly never done in the existing\nliterature) drastically changes the topology of the inferred network.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 12:38:21 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Chiquet", "Julien", ""], ["Mariadassou", "Mahendra", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1806.03471", "submitter": "Rose Baker", "authors": "Rose Baker and Dan Jackson", "title": "A new measure of treatment effect for random-effects meta-analysis of\n  comparative binary outcome data", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparative binary outcome data are of fundamental interest in statistics and\nare often pooled in meta-analyses. Here we examine the simplest case where for\neach study there are two patient groups and a binary event of interest, giving\nrise to a series of $2 \\times 2$ tables. A variety of measures of treatment\neffect are then available and are conventionally used in meta-analyses, such as\nthe odds ratio, the risk ratio and the risk difference. Here we propose a new\ntype of measure of treatment effect for this type of data that is very easily\ninterpretable by lay audiences. We give the rationale for the new measure and\nwe present three contrasting methods for computing its within-study variance so\nthat it can be used in conventional meta-analyses. We then develop three\nalternative methods for random-effects meta-analysis that use our measure and\nwe apply our methodolgy to some real examples. We conclude that our new measure\nis a fully viable alternative to existing measures. It has the advantage that\nits interpretation is especially simple and direct, so that its meaning can be\nmore readily understood by those with little or no formal statistical training.\nThis may be especially valuable when presenting `plain language summaries',\nsuch as those used by Cochrane.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 12:55:46 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Baker", "Rose", ""], ["Jackson", "Dan", ""]]}, {"id": "1806.03647", "submitter": "Matteo Barigozzi", "authors": "Matteo Barigozzi, Lorenzo Trapani", "title": "Determining the dimension of factor structures in non-stationary large\n  datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a procedure to determine the dimension of the common factor space\nin a large, possibly non-stationary, dataset. Our procedure is designed to\ndetermine whether there are (and how many) common factors (i) with linear\ntrends, (ii) with stochastic trends, (iii) with no trends, i.e. stationary. Our\nanalysis is based on the fact that the largest eigenvalues of a suitably scaled\ncovariance matrix of the data (corresponding to the common factor part)\ndiverge, as the dimension $N$ of the dataset diverges, whilst the others stay\nbounded. Therefore, we propose a class of randomised test statistics for the\nnull that the $p$-th eigenvalue diverges, based directly on the estimated\neigenvalue. The tests only requires minimal assumptions on the data, and no\nrestrictions on the relative rates of divergence of $N$ and $T$ are imposed.\nMonte Carlo evidence shows that our procedure has very good finite sample\nproperties, clearly dominating competing approaches when no common factors are\npresent. We illustrate our methodology through an application to US bond yields\nwith different maturities observed over the last 30 years. A common linear\ntrend and two common stochastic trends are found and identified as the\nclassical level, slope and curvature factors.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 12:23:15 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Barigozzi", "Matteo", ""], ["Trapani", "Lorenzo", ""]]}, {"id": "1806.03668", "submitter": "Zheyang Wu", "authors": "Hong Zhang and Zheyang Wu", "title": "Generalized Goodness-Of-Fit Tests for Correlated Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the problem of applying the generalized goodness-of-fit\n(gGOF) type tests for analyzing correlated data. The gGOF family broadly covers\nthe maximum-based testing procedures by ordered input $p$-values, such as the\nfalse discovery rate procedure, the Kolmogorov-Smirnov type statistics, the\n$\\phi$-divergence family, etc. Data analysis framework and a novel $p$-value\ncalculation approach is developed under the Gaussian mean model and the\ngeneralized linear model (GLM). We reveal the influence of data transformations\nto the signal-to-noise ratio and the statistical power under both sparse and\ndense signal patterns and various correlation structures. In particular, the\ninnovated transformation (IT), which is shown equivalent to the marginal\nmodel-fitting under the GLM, is often preferred for detecting sparse signals in\ncorrelated data. We propose a testing strategy called the digGOF, which\ncombines a double-adaptation procedure (i.e., adapting to both the statistic's\nformula and the truncation scheme of the input $p$-values) and the IT within\nthe gGOF family. It features efficient computation and robust adaptation to the\nfamily-retained advantages for given data. Relevant approaches are assessed by\nextensive simulations and by genetic studies of Crohn's disease and amyotrophic\nlateral sclerosis. Computations have been included into the R package SetTest\navailable on CRAN.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 14:50:12 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Zhang", "Hong", ""], ["Wu", "Zheyang", ""]]}, {"id": "1806.03673", "submitter": "Georg Zimmermann", "authors": "Georg Zimmermann, Meinhard Kieser, Arne Bathke", "title": "Sample size calculation and blinded recalculation for analysis of\n  covariance models with multiple random covariates", "comments": null, "journal-ref": null, "doi": "10.1080/10543406.2019.1632871", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When testing for superiority in a parallel-group setting with a continuous\noutcome, adjusting for covariates (e.g., baseline measurements) is usually\nrecommended, in order to reduce bias and increase power. For this purpose, the\nanalysis of covariance (ANCOVA) is frequently used, and recently, several exact\nand approximate sample size calculation procedures have been proposed. However,\nin case of multiple covariates, the planning might pose some practical\nchallenges and surprising pitfalls, which have not been recognized so far.\nMoreover, since a considerable number of parameters have to be specified in\nadvance, the risk of making erroneous initial assumptions, leading to\nsubstantially over- or underpowered studies, is increased. Therefore, we\npropose a method, which allows for re-estimating the sample size at a\nprespecified time point during the course of the trial. Extensive simulations\nfor a broad range of settings, including unbalanced designs, confirm that the\nproposed method provides reliable results in many practically relevant\nsituations. An advantage of the reassessment procedure is that it does not\nrequire unblinding of the data. In order to facilitate the application of the\nproposed method, we provide some R code and discuss a real-life data example.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 15:28:23 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Zimmermann", "Georg", ""], ["Kieser", "Meinhard", ""], ["Bathke", "Arne", ""]]}, {"id": "1806.03729", "submitter": "Johannes Martini", "authors": "Johannes W R Martini and Francisco Rosales and Ngoc-Thuy Ha and Thomas\n  Kneib and Johannes Heise and Valentin Wimmer", "title": "Lost in translation: On the impact of data coding on penalized\n  regression with interactions", "comments": null, "journal-ref": "G3 (2019) https://doi.org/10.1534/g3.118.200961", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression approaches are standard tools in quantitative genetics.\nIt is known that the fit of an \\emph{ordinary least squares} (OLS) regression\nis independent of certain transformations of the coding of the predictor\nvariables, and that the standard mixed model \\emph{ridge regression best linear\nunbiased prediction} (RRBLUP) is neither affected by translations of the\nvariable coding, nor by global scaling. However, it has been reported that an\nextended version of this mixed model, which incorporates interactions by\nproducts of markers as additional predictor variables is affected by\ntranslations of the marker coding. In this work, we identify the cause of this\nloss of invariance in a general context of penalized regression on polynomials\nin the predictor variables. We show that in most cases, translating the coding\nof the predictor variables has an impact on effect estimates, with an exception\nwhen only the size of the coefficients of monomials of highest total degree are\npenalized. The invariance of RRBLUP can thus be considered as a special case of\nthis setting, with a polynomial of total degree 1, where the size of the fixed\neffect (total degree 0) is not penalized but all coefficients of monomials of\ntotal degree 1 are. The extended RRBLUP, which includes interactions, is not\ninvariant to translations because it does not only penalize interactions (total\ndegree 2), but also additive effects (total degree 1). Our observations are not\nrestricted to ridge regression, but generally valid for penalized regressions,\nfor instance also for the $\\ell_1$ penalty of LASSO.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 21:46:41 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Martini", "Johannes W R", ""], ["Rosales", "Francisco", ""], ["Ha", "Ngoc-Thuy", ""], ["Kneib", "Thomas", ""], ["Heise", "Johannes", ""], ["Wimmer", "Valentin", ""]]}, {"id": "1806.03745", "submitter": "Julie Bessac", "authors": "Julie Bessac and Philippe Naveau", "title": "Forecast score distributions with imperfect observations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classical paradigm of scoring rules is to discriminate between two\ndifferent forecasts by comparing them with observations. The probability\ndistribution of the observed record is assumed to be perfect as a verification\nbenchmark. In practice, however, observations are almost always tainted by\nerrors and uncertainties. If the yardstick used to compare forecasts is\nimprecise, one can wonder whether such types of errors may or may not have a\nstrong influence on decisions based on classical scoring rules. We propose a\nnew scoring rule scheme in the context of models that incorporate errors of the\nverification data. We rely on existing scoring rules and incorporate\nuncertainty and error of the verification data through a hidden variable and\nthe conditional expectation of scores when they are viewed as a random\nvariable. The proposed scoring framework is compared to scores used in\npractice, and is expressed in various setups, mainly an additive Gaussian noise\nmodel and a multiplicative Gamma noise model. By considering scores as random\nvariables one can access the entire range of their distribution. In particular\nwe illustrate that the commonly used mean score can be a misleading\nrepresentative of the distribution when this latter is highly skewed or have\nheavy tails. In a simulation study, through the power of a statistical test and\nthe computation of Wasserstein distances between scores distributions, we\ndemonstrate the ability of the newly proposed score to better discriminate\nbetween forecasts when verification data are subject to uncertainty compared\nwith the scores used in practice. Finally, we illustrate the benefit of\naccounting for the uncertainty of the verification data into the scoring\nprocedure on a dataset of surface wind speed from measurements and numerical\nmodel outputs.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 23:38:02 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2021 19:51:23 GMT"}], "update_date": "2021-02-08", "authors_parsed": [["Bessac", "Julie", ""], ["Naveau", "Philippe", ""]]}, {"id": "1806.03829", "submitter": "Emma Jingfei Zhang", "authors": "Jingfei Zhang, Will Wei Sun and Lexin Li", "title": "Mixed-Effect Time-Varying Network Model and Application in Brain\n  Connectivity Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-varying networks are fast emerging in a wide range of scientific and\nbusiness disciplines. Most existing dynamic network models are limited to a\nsingle-subject and discrete-time setting. In this article, we propose a\nmixed-effect multi-subject continuous-time stochastic blockmodel that\ncharacterizes the time-varying behavior of the network at the population level,\nmeanwhile taking into account individual subject variability. We develop a\nmulti-step optimization procedure for a constrained stochastic blockmodel\nestimation, and derive the asymptotic property of the estimator. We demonstrate\nthe effectiveness of our method through both simulations and an application to\na study of brain development in youth.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 06:49:50 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Zhang", "Jingfei", ""], ["Sun", "Will Wei", ""], ["Li", "Lexin", ""]]}, {"id": "1806.03850", "submitter": "Vitalii Miroshnichenko", "authors": "Vitalii Miroshnichenko, Rostyslav Maiboroda", "title": "Confidence ellipsoids for regression coefficients by observations from a\n  mixture", "comments": "Published at https://doi.org/10.15559/18-VMSTA105 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)", "journal-ref": "Modern Stochastics: Theory and Applications 2018, Vol. 5, No. 2,\n  225-245", "doi": "10.15559/18-VMSTA105", "report-no": "VTeX-VMSTA-VMSTA105", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Confidence ellipsoids for linear regression coefficients are constructed by\nobservations from a mixture with varying concentrations. Two approaches are\ndiscussed. The first one is the nonparametric approach based on the weighted\nleast squares technique. The second one is an approximate maximum likelihood\nestimation with application of the EM-algorithm for the estimates calculation.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 08:01:53 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Miroshnichenko", "Vitalii", ""], ["Maiboroda", "Rostyslav", ""]]}, {"id": "1806.03954", "submitter": "Eardi Lila", "authors": "Eardi Lila, Simon Arridge, John A. D. Aston", "title": "Representation and reconstruction of covariance operators in linear\n  inverse problems", "comments": "40 pages", "journal-ref": null, "doi": "10.1088/1361-6420/ab8713", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for the reconstruction and representation of\nfunctions in a setting where these objects cannot be directly observed, but\nonly indirect and noisy measurements are available, namely an inverse problem\nsetting. The proposed methodology can be applied either to the analysis of\nindirectly observed functional images or to the associated covariance\noperators, representing second-order information, and thus lying on a\nnon-Euclidean space. To deal with the ill-posedness of the inverse problem, we\nexploit the spatial structure of the sample data by introducing a flexible\nregularizing term embedded in the model. Thanks to its efficiency, the proposed\nmodel is applied to MEG data, leading to a novel approach to the investigation\nof functional connectivity.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 13:23:13 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 16:38:32 GMT"}, {"version": "v3", "created": "Sun, 13 Sep 2020 18:59:52 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Lila", "Eardi", ""], ["Arridge", "Simon", ""], ["Aston", "John A. D.", ""]]}, {"id": "1806.04119", "submitter": "Arun Kuchibhotla", "authors": "Arun Kumar Kuchibhotla, Lawrence D. Brown, Andreas Buja, Edward I.\n  George and Linda Zhao", "title": "Valid Post-selection Inference in Assumption-lean Linear Regression", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Construction of valid statistical inference for estimators based on\ndata-driven selection has received a lot of attention in the recent times. Berk\net al. (2013) is possibly the first work to provide valid inference for\nGaussian homoscedastic linear regression with fixed covariates under arbitrary\ncovariate/variable selection. The setting is unrealistic and is extended by\nBachoc et al. (2016) by relaxing the distributional assumptions. A major\ndrawback of the aforementioned works is that the construction of valid\nconfidence regions is computationally intensive. In this paper, we first prove\nthat post-selection inference is equivalent to simultaneous inference and then\nconstruct valid post-selection confidence regions which are computationally\nsimple. Our construction is based on deterministic inequalities and apply to\nindependent as well as dependent random variables without the requirement of\ncorrect distributional assumptions. Finally, we compare the volume of our\nconfidence regions with the existing ones and show that under non-stochastic\ncovariates, our regions are much smaller.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 17:39:37 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Kuchibhotla", "Arun Kumar", ""], ["Brown", "Lawrence D.", ""], ["Buja", "Andreas", ""], ["George", "Edward I.", ""], ["Zhao", "Linda", ""]]}, {"id": "1806.04206", "submitter": "Federico Bugni", "authors": "Federico A. Bugni and Ivan A. Canay and Azeem M. Shaikh", "title": "Inference under Covariate-Adaptive Randomization with Multiple\n  Treatments", "comments": "33 pages, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies inference in randomized controlled trials with\ncovariate-adaptive randomization when there are multiple treatments. More\nspecifically, we study inference about the average effect of one or more\ntreatments relative to other treatments or a control. As in Bugni et al.\n(2018), covariate-adaptive randomization refers to randomization schemes that\nfirst stratify according to baseline covariates and then assign treatment\nstatus so as to achieve balance within each stratum. In contrast to Bugni et\nal. (2018), we not only allow for multiple treatments, but further allow for\nthe proportion of units being assigned to each of the treatments to vary across\nstrata. We first study the properties of estimators derived from a fully\nsaturated linear regression, i.e., a linear regression of the outcome on all\ninteractions between indicators for each of the treatments and indicators for\neach of the strata. We show that tests based on these estimators using the\nusual heteroskedasticity-consistent estimator of the asymptotic variance are\ninvalid; on the other hand, tests based on these estimators and suitable\nestimators of the asymptotic variance that we provide are exact. For the\nspecial case in which the target proportion of units being assigned to each of\nthe treatments does not vary across strata, we additionally consider tests\nbased on estimators derived from a linear regression with strata fixed effects,\ni.e., a linear regression of the outcome on indicators for each of the\ntreatments and indicators for each of the strata. We show that tests based on\nthese estimators using the usual heteroskedasticity-consistent estimator of the\nasymptotic variance are conservative, but tests based on these estimators and\nsuitable estimators of the asymptotic variance that we provide are exact. A\nsimulation study illustrates the practical relevance of our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 19:22:10 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 09:12:48 GMT"}, {"version": "v3", "created": "Thu, 17 Jan 2019 21:07:34 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Bugni", "Federico A.", ""], ["Canay", "Ivan A.", ""], ["Shaikh", "Azeem M.", ""]]}, {"id": "1806.04251", "submitter": "Olga Vsevolozhskaya", "authors": "Olga A Vsevolozhskaya and Dmitri V Zaykin", "title": "Put the odds on your side: a new measure for epidemiological\n  associations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The odds ratio (OR) is a measure of effect size commonly used in\nobservational research. OR reflects statistical association between a binary\noutcome, such as the presence of a health condition, and a binary predictor,\nsuch as an exposure to a pollutant. Statistical inference and interval\nestimation for OR are often performed on the logarithmic scale, due to\nasymptotic convergence of log(OR) to a normal distribution. Here, we propose a\nnew normalized measure of effect size, $\\gamma'$, and derive its asymptotic\ndistribution. We show that the new statistic, based on the $\\gamma'$\ndistribution, is more powerful than the traditional one for testing the\nhypothesis $H_0$: log(OR)=0. The new normalized effect size is termed `gamma\nprime' in the spirit of $D'$, a normalized measure of genetic linkage\ndisequilibrium, which ranges from -1 to 1 for a pair of genetic loci. The\nnormalization constant for $\\gamma'$ is based on the maximum range of the\nstandardized effect size, for which we establish a peculiar connection to the\nLaplace Limit Constant. Furthermore, while standardized effects are of little\nvalue on their own, we propose a powerful application, in which standardized\neffects are employed as an intermediate step in an approximate, yet accurate\nposterior inference for raw effect size measures, such as log(OR) and\n$\\gamma'$.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 21:36:24 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 23:03:07 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Vsevolozhskaya", "Olga A", ""], ["Zaykin", "Dmitri V", ""]]}, {"id": "1806.04334", "submitter": "Jami Mulgrave", "authors": "Jami J. Mulgrave and Subhashis Ghosal", "title": "Bayesian Inference in Nonparanormal Graphical Models", "comments": null, "journal-ref": null, "doi": "10.1214/19-BA1159", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian graphical models have been used to study intrinsic dependence among\nseveral variables, but the Gaussianity assumption may be restrictive in many\napplications. A nonparanormal graphical model is a semiparametric\ngeneralization for continuous variables where it is assumed that the variables\nfollow a Gaussian graphical model only after some unknown smooth monotone\ntransformations on each of them. We consider a Bayesian approach in the\nnonparanormal graphical model by putting priors on the unknown transformations\nthrough a random series based on B-splines where the coefficients are ordered\nto induce monotonicity. A truncated normal prior leads to partial conjugacy in\nthe model and is useful for posterior simulation using Gibbs sampling. On the\nunderlying precision matrix of the transformed variables, we consider a\nspike-and-slab prior and use an efficient posterior Gibbs sampling scheme. We\nuse the Bayesian Information Criterion to choose the hyperparameters for the\nspike-and-slab prior. We present a posterior consistency result on the\nunderlying transformation and the precision matrix. We study the numerical\nperformance of the proposed method through an extensive simulation study and\nfinally apply the proposed method on a real data set.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 05:11:00 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 22:00:47 GMT"}, {"version": "v3", "created": "Thu, 11 Apr 2019 23:46:27 GMT"}], "update_date": "2020-05-20", "authors_parsed": [["Mulgrave", "Jami J.", ""], ["Ghosal", "Subhashis", ""]]}, {"id": "1806.04420", "submitter": "Herv\\'e Cardot", "authors": "Herv\\'e Cardot, Guillaume Lecuelle, Pascal Schlich, Michel Visalli", "title": "Estimating finite mixtures of semi-Markov chains: an application to the\n  segmentation of temporal sensory data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In food science, it is of great interest to get information about the\ntemporal perception of aliments to create new products, to modify existing ones\nor more generally to understand the perception mechanisms. Temporal Dominance\nof Sensations (TDS) is a technique to measure temporal perception which\nconsists in choosing sequentially attributes describing a food product over\ntasting. This work introduces new statistical models based on finite mixtures\nof semi-Markov chains in order to describe data collected with the TDS\nprotocol, allowing different temporal perceptions for a same product within a\npopulation. The identifiability of the parameters of such mixture models is\ndiscussed. Sojourn time distributions are fitted with gamma probability\ndistribution and a penalty is added to the log likelihood to ensure convergence\nof the EM algorithm to a non degenerate solution. Information criterions are\nemployed for determining the number of mixture components. Then, the individual\nqualitative trajectories are clustered with the help of the maximum a\nposteriori probability (MAP) approach. A simulation study confirms the good\nbehavior of the proposed estimation procedure. The methodology is illustrated\non an example of consumers perception of a Gouda cheese and assesses the\nexistence of several behaviors in terms of perception of this product.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 09:51:32 GMT"}, {"version": "v2", "created": "Tue, 12 Feb 2019 15:17:43 GMT"}], "update_date": "2019-02-13", "authors_parsed": [["Cardot", "Herv\u00e9", ""], ["Lecuelle", "Guillaume", ""], ["Schlich", "Pascal", ""], ["Visalli", "Michel", ""]]}, {"id": "1806.04503", "submitter": "Anthony Vella", "authors": "Anthony Vella", "title": "Tutorial: Maximum likelihood estimation in the context of an optical\n  measurement", "comments": "84 pages, 52 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an physics.optics stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The method of maximum likelihood estimation (MLE) is a widely used\nstatistical approach for estimating the values of one or more unknown\nparameters of a probabilistic model based on observed data. In this tutorial, I\nbriefly review the mathematical foundations of MLE, then reformulate the\nproblem for the measurement of a spatially-varying optical intensity\ndistribution. In this context, the detection of each individual photon is\ntreated as a random event, the outcome being the photon's location. A typical\nmeasurement consists of many detected photons, which accumulate to form a\nspatial intensity profile. Here, I show a straightforward derivation for the\nlikelihood function and Fisher information matrix (FIM) associated with a\nmeasurement of multiple photons incident on a detector comprised of a discrete\narray of pixels. An estimate for the parameter(s) of interest may then be\nobtained by maximizing the likelihood function, while the FIM determines the\nuncertainty of the estimate. To illustrate these concepts, several simple\nexamples are presented for the one- and two-parameter cases, revealing many\ninteresting properties of the MLE formalism, as well as some practical\nconsiderations for optical experiments. Throughout these examples, connections\nare also drawn to optical applications of quantum weak measurements, including\noff-null ellipsometry and scatterometry.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 06:36:34 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 19:44:25 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 22:13:11 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Vella", "Anthony", ""]]}, {"id": "1806.04530", "submitter": "Woundjiague Apollinaire", "authors": "Apollinaire Woundjiague, Martin Le Doux Mbele Bidima, Ronald Waweru\n  Mwangi", "title": "A Least Squares Estimation of a Hybrid log-Poisson Regression and its\n  Goodness of Fit for Optimal Loss Reserves in Insurance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, the parameters of a hybrid log-linear model (log-Poisson)\nare estimated using the fuzzy least-squares (FLS) procedures (Celmi\\c{n}\\v{s},\n987a,b, D'Urso and Gastaldi, 2000, DUrso and Gastaldi, 2001). A goodness of fit\nhave been derived in order to assess and compare this new model and the\nclassical log-Poisson regression in loss reserving framework (Mack, 1991). Both\nthe hybrid model and its goodness of fit are performed on a loss reserving\ndata.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 19:24:18 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Woundjiague", "Apollinaire", ""], ["Bidima", "Martin Le Doux Mbele", ""], ["Mwangi", "Ronald Waweru", ""]]}, {"id": "1806.04715", "submitter": "Jun Hee Kim", "authors": "Jun Hee Kim, Eun Kyung Kwon, Qian Sha, Brian Junker, Tracy Sweet", "title": "CID Models on Real-world Social Networks and Goodness of Fit\n  Measurements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the model fit quality of statistical models for network data is an\nongoing and under-examined topic in statistical network analysis. Traditional\nmetrics for evaluating model fit on tabular data such as the Bayesian\nInformation Criterion are not suitable for models specialized for network data.\nWe propose a novel self-developed goodness of fit (GOF) measure, the\n`stratified-sampling cross-validation' (SCV) metric, that uses a procedure\nsimilar to traditional cross-validation via stratified-sampling to select dyads\nin the network's adjacency matrix to be removed. SCV is capable of intuitively\nexpressing different models' ability to predict on missing dyads. Using SCV on\nreal-world social networks, we identify the appropriate statistical models for\ndifferent network structures and generalize such patterns. In particular, we\nfocus on conditionally independent dyad (CID) models such as the Erdos Renyi\nmodel, the stochastic block model, the sender-receiver model, and the latent\nspace model.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 18:55:19 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 23:52:12 GMT"}, {"version": "v3", "created": "Mon, 25 Jun 2018 17:07:46 GMT"}, {"version": "v4", "created": "Wed, 27 Jun 2018 18:21:09 GMT"}, {"version": "v5", "created": "Tue, 3 Jul 2018 20:18:56 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Kim", "Jun Hee", ""], ["Kwon", "Eun Kyung", ""], ["Sha", "Qian", ""], ["Junker", "Brian", ""], ["Sweet", "Tracy", ""]]}, {"id": "1806.04743", "submitter": "Pablo de Castro", "authors": "Pablo de Castro and Tommaso Dorigo", "title": "INFERNO: Inference-Aware Neural Optimisation", "comments": "Code available at https://github.com/pablodecm/paper-inferno .\n  Version updates: - v2: fixed typos, improve text, link to code and a better\n  synthetic experiment", "journal-ref": null, "doi": "10.1016/j.cpc.2019.06.007", "report-no": null, "categories": "stat.ML cs.LG hep-ex physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex computer simulations are commonly required for accurate data\nmodelling in many scientific disciplines, making statistical inference\nchallenging due to the intractability of the likelihood evaluation for the\nobserved data. Furthermore, sometimes one is interested on inference drawn over\na subset of the generative model parameters while taking into account model\nuncertainty or misspecification on the remaining nuisance parameters. In this\nwork, we show how non-linear summary statistics can be constructed by\nminimising inference-motivated losses via stochastic gradient descent such they\nprovided the smallest uncertainty for the parameters of interest. As a use\ncase, the problem of confidence interval estimation for the mixture coefficient\nin a multi-dimensional two-component mixture model (i.e. signal vs background)\nis considered, where the proposed technique clearly outperforms summary\nstatistics based on probabilistic classification, which are a commonly used\nalternative but do not account for the presence of nuisance parameters.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 20:08:53 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 12:41:56 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["de Castro", "Pablo", ""], ["Dorigo", "Tommaso", ""]]}, {"id": "1806.04892", "submitter": "Jamie Halliday", "authors": "Jamie Halliday and Georgi N. Boshnakov", "title": "PoARX Modelling for Multivariate Count Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces multivariate Poisson autoregressive models with\nexogenous covariates (PoARX) for modelling multivariate time series of counts.\nWe obtain conditions for the PoARX process to be stationary and ergodic before\nproposing a computationally efficient procedure for estimation of parameters by\nthe method of inference functions (IFM) and obtaining asymptotic normality of\nthese estimators. Lastly, we demonstrate an application to count data for the\nnumber of people entering and exiting a building, and show how the different\naspects of the model combine to produce a strong predictive model. We conclude\nby suggesting some further areas of application and by listing directions for\nfuture work.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 08:33:43 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Halliday", "Jamie", ""], ["Boshnakov", "Georgi N.", ""]]}, {"id": "1806.05081", "submitter": "Chen Huang", "authors": "Victor Chernozhukov, Wolfgang K. H\\\"ardle, Chen Huang, Weining Wang", "title": "LASSO-Driven Inference in Time and Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the estimation and inference in a system of high-dimensional\nregression equations allowing for temporal and cross-sectional dependency in\ncovariates and error processes, covering rather general forms of weak temporal\ndependence. A sequence of regressions with many regressors using LASSO (Least\nAbsolute Shrinkage and Selection Operator) is applied for variable selection\npurpose, and an overall penalty level is carefully chosen by a block multiplier\nbootstrap procedure to account for multiplicity of the equations and\ndependencies in the data. Correspondingly, oracle properties with a jointly\nselected tuning parameter are derived. We further provide high-quality\nde-biased simultaneous inference on the many target parameters of the system.\nWe provide bootstrap consistency results of the test procedure, which are based\non a general Bahadur representation for the $Z$-estimators with dependent data.\nSimulations demonstrate good performance of the proposed inference procedure.\nFinally, we apply the method to quantify spillover effects of textual sentiment\nindices in a financial market and to test the connectedness among sectors.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 14:24:06 GMT"}, {"version": "v2", "created": "Thu, 9 Aug 2018 11:46:52 GMT"}, {"version": "v3", "created": "Thu, 25 Apr 2019 14:17:30 GMT"}, {"version": "v4", "created": "Fri, 15 May 2020 11:00:42 GMT"}], "update_date": "2020-05-18", "authors_parsed": [["Chernozhukov", "Victor", ""], ["H\u00e4rdle", "Wolfgang K.", ""], ["Huang", "Chen", ""], ["Wang", "Weining", ""]]}, {"id": "1806.05127", "submitter": "Max Tabord-Meehan", "authors": "Max Tabord-Meehan", "title": "Stratification Trees for Adaptive Randomization in Randomized Controlled\n  Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an adaptive randomization procedure for two-stage\nrandomized controlled trials. The method uses data from a first-wave experiment\nin order to determine how to stratify in a second wave of the experiment, where\nthe objective is to minimize the variance of an estimator for the average\ntreatment effect (ATE). We consider selection from a class of stratified\nrandomization procedures which we call stratification trees: these are\nprocedures whose strata can be represented as decision trees, with differing\ntreatment assignment probabilities across strata. By using the first wave to\nestimate a stratification tree, we simultaneously select which covariates to\nuse for stratification, how to stratify over these covariates, as well as the\nassignment probabilities within these strata. Our main result shows that using\nthis randomization procedure with an appropriate estimator results in an\nasymptotic variance which is minimal in the class of stratification trees.\nMoreover, the results we present are able to accommodate a large class of\nassignment mechanisms within strata, including stratified block randomization.\nIn a simulation study, we find that our method, paired with an appropriate\ncross-validation procedure ,can improve on ad-hoc choices of stratification. We\nconclude by applying our method to the study in Karlan and Wood (2017), where\nwe estimate stratification trees using the first wave of their experiment.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 16:03:00 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 17:29:08 GMT"}, {"version": "v3", "created": "Thu, 9 Jan 2020 18:57:43 GMT"}, {"version": "v4", "created": "Sun, 12 Jan 2020 06:02:04 GMT"}, {"version": "v5", "created": "Thu, 11 Jun 2020 04:18:48 GMT"}], "update_date": "2020-06-12", "authors_parsed": [["Tabord-Meehan", "Max", ""]]}, {"id": "1806.05144", "submitter": "Sean Yiu", "authors": "Sean Yiu and Li Su", "title": "Joint calibrated estimation of inverse probability of treatment and\n  censoring weights for marginal structural models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal structural models (MSMs) with inverse probability weighting offer an\napproach to estimating causal effects of treatment sequences on repeated\noutcome measures in the presence of time-varying confounding and dependent\ncensoring. However, when weights are estimated by maximum likelihood, inverse\nprobability weighted estimators (IPWEs) can be inefficient and unstable in\npractice. We propose a joint calibration approach for inverse probability of\ntreatment and censoring weights to improve the efficiency and robustness of the\nIPWEs for MSMs with time-varying treatments of arbitrary (i.e., binary and\nnon-binary) distributions. Specifically, novel calibration restrictions are\nderived by explicitly eliminating covariate associations with both the\ntreatment assignment process and the censoring process after weighting the\ncurrent sample (i.e., to optimise covariate balance in finite samples). A\nconvex minimization procedure is developed to implement the calibration.\nSimulations show that IPWEs with calibrated weights perform better than IPWEs\nwith weights from maximum likelihood. We apply our method to a natural history\nstudy of HIV for estimating the cumulative effect of highly active\nantiretroviral therapy on CD4 cell counts over time.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 16:56:11 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2018 10:54:07 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Yiu", "Sean", ""], ["Su", "Li", ""]]}, {"id": "1806.05297", "submitter": "Mireille Boutin", "authors": "Tarun Yellamraju and Mireille Boutin", "title": "Pattern Dependence Detection using n-TARP Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider an experiment involving a potentially small number of subjects. Some\nrandom variables are observed on each subject: a high-dimensional one called\nthe \"observed\" random variable, and a one-dimensional one called the \"outcome\"\nrandom variable. We are interested in the dependencies between the observed\nrandom variable and the outcome random variable. We propose a method to\nquantify and validate the dependencies of the outcome random variable on the\nvarious patterns contained in the observed random variable. Different degrees\nof relationship are explored (linear, quadratic, cubic, ...). This work is\nmotivated by the need to analyze educational data, which often involves\nhigh-dimensional data representing a small number of students. Thus our\nimplementation is designed for a small number of subjects; however, it can be\neasily modified to handle a very large dataset. As an illustration, the\nproposed method is used to study the influence of certain skills on the course\ngrade of students in a signal processing class. A valid dependency of the grade\non the different skill patterns is observed in the data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 23:14:45 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Yellamraju", "Tarun", ""], ["Boutin", "Mireille", ""]]}, {"id": "1806.05429", "submitter": "Jasper Jonathan Velthoen", "authors": "Jasper Velthoen, Juan-Juan Cai, Geurt Jongbloed and Maurice Schmeits", "title": "Improving precipitation forecasts using extreme quantile regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming to estimate extreme precipitation forecast quantiles, we propose a\nnonparametric regression model that features a constant extreme value index.\nUsing local linear quantile regression and an extrapolation technique from\nextreme value theory, we develop an estimator for conditional quantiles\ncorresponding to extreme high probability levels. We establish uniform\nconsistency and asymptotic normality of the estimators. In a simulation study,\nwe examine the performance of our estimator on finite samples in comparison\nwith a method assuming linear quantiles. On a precipitation data set in the\nNetherlands, these estimators have greater predictive skill compared to the\nupper member of ensemble forecasts provided by a numerical weather prediction\nmodel.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 09:24:22 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 08:21:11 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Velthoen", "Jasper", ""], ["Cai", "Juan-Juan", ""], ["Jongbloed", "Geurt", ""], ["Schmeits", "Maurice", ""]]}, {"id": "1806.05471", "submitter": "Xinghao Qiao", "authors": "Cheng Chen, Shaojun Guo, Xinghao Qiao", "title": "Functional Linear Regression: Dependence and Error Contamination", "comments": "45 pages, 3 figures, 8 tables, accepted by JBES", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional linear regression is an important topic in functional data\nanalysis. It is commonly assumed that samples of the functional predictor are\nindependent realizations of an underlying stochastic process, and are observed\nover a grid of points contaminated by i.i.d. measurement errors. In practice,\nhowever, the dynamical dependence across different curves may exist and the\nparametric assumption on the error covariance structure could be unrealistic.\nIn this paper, we consider functional linear regression with serially dependent\nobservations of the functional predictor, when the contamination of the\npredictor by the white noise is genuinely functional with fully nonparametric\ncovariance structure. Inspired by the fact that the autocovariance function of\nobserved functional predictors automatically filters out the impact from the\nunobservable noise term, we propose a novel autocovariance-based generalized\nmethod-of-moments estimate of the slope function. We also develop a\nnonparametric smoothing approach to handle the scenario of partially observed\nfunctional predictors. The asymptotic properties of the resulting estimators\nunder different scenarios are established. Finally, we demonstrate that our\nproposed method significantly outperforms possible competing methods through an\nextensive set of simulations and an analysis of a public financial dataset.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 11:26:35 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 12:59:48 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2020 15:41:40 GMT"}, {"version": "v4", "created": "Sat, 12 Sep 2020 09:51:14 GMT"}], "update_date": "2020-09-15", "authors_parsed": [["Chen", "Cheng", ""], ["Guo", "Shaojun", ""], ["Qiao", "Xinghao", ""]]}, {"id": "1806.05500", "submitter": "Yoav Zemel", "authors": "Victor M. Panaretos and Yoav Zemel", "title": "Statistical Aspects of Wasserstein Distances", "comments": "Official version available at\n  https://www.annualreviews.org/doi/full/10.1146/annurev-statistics-030718-104938", "journal-ref": "Annual Review of Statistics and Its Application 2019 6:1, 405-431", "doi": "10.1146/annurev-statistics-030718-104938", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wasserstein distances are metrics on probability distributions inspired by\nthe problem of optimal mass transportation. Roughly speaking, they measure the\nminimal effort required to reconfigure the probability mass of one distribution\nin order to recover the other distribution. They are ubiquitous in mathematics,\nwith a long history that has seen them catalyse core developments in analysis,\noptimization, and probability. Beyond their intrinsic mathematical richness,\nthey possess attractive features that make them a versatile tool for the\nstatistician: they can be used to derive weak convergence and convergence of\nmoments, and can be easily bounded; they are well-adapted to quantify a natural\nnotion of perturbation of a probability distribution; and they seamlessly\nincorporate the geometry of the domain of the distributions in question, thus\nbeing useful for contrasting complex objects. Consequently, they frequently\nappear in the development of statistical theory and inferential methodology,\nand have recently become an object of inference in themselves. In this review,\nwe provide a snapshot of the main concepts involved in Wasserstein distances\nand optimal transportation, and a succinct overview of some of their many\nstatistical aspects.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:29:46 GMT"}, {"version": "v2", "created": "Sat, 11 Aug 2018 14:17:41 GMT"}, {"version": "v3", "created": "Tue, 9 Apr 2019 15:36:03 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Panaretos", "Victor M.", ""], ["Zemel", "Yoav", ""]]}, {"id": "1806.05830", "submitter": "Gildas Mazo", "authors": "Gildas Mazo (MaIAGE), Fran\\c{c}ois Portier", "title": "Parametric versus nonparametric: the fitness coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fitness coefficient, introduced in this paper, results from a competition\nbetween parametric and nonparametric density estimators within the likelihood\nof the data. As illustrated on several real datasets, the fitness coefficient\ngenerally agrees with p-values but is easier to compute and interpret. Namely,\nthe fitness coefficient can be interpreted as the proportion of data coming\nfrom the parametric model. Moreover, the fitness coefficient can be used to\nbuild a semiparamteric compromise which improves inference over the parametric\nand nonparametric approaches. From a theoretical perspective, the fitness\ncoefficient is shown to converge in probability to one if the model is true and\nto zero if the model is false. From a practical perspective, the utility of the\nfitness coefficient is illustrated on real and simulated datasets.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 07:16:00 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Mazo", "Gildas", "", "MaIAGE"], ["Portier", "Fran\u00e7ois", ""]]}, {"id": "1806.05928", "submitter": "Emanuele Taufer", "authors": "Emanuele Taufer, Flavio Santi, Giuseppe Espa, Maria Michela Dickson", "title": "On a property of the inequality curve $\\lambda(p)$", "comments": "6 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Zenga (1984) inequality curve is constant in p for Type I Pareto\ndistributions. We show that this property holds exactly only for the Pareto\ndistribution and, asymptotically, for distributions with power tail with index\n-a, with a greater than 1. Exploiting these properties one can develop powerful\ntools to analyze and estimate the tail of a distribution.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 12:27:25 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Taufer", "Emanuele", ""], ["Santi", "Flavio", ""], ["Espa", "Giuseppe", ""], ["Dickson", "Maria Michela", ""]]}, {"id": "1806.05939", "submitter": "Bent Nielsen", "authors": "D. Kuang and B. Nielsen", "title": "Generalized Log-Normal Chain-Ladder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an asymptotic theory for distribution forecasting from the log\nnormal chain-ladder model. The theory overcomes the difficulty of convoluting\nlog normal variables and takes estimation error into account. The results\ndiffer from that of the over-dispersed Poisson model and from the chain-ladder\nbased bootstrap. We embed the log normal chain-ladder model in a class of\ninfinitely divisible distributions called the generalized log normal\nchain-ladder model. The asymptotic theory uses small $\\sigma$ asymptotics where\nthe dimension of the reserving triangle is kept fixed while the standard\ndeviation is assumed to decrease. The resulting asymptotic forecast\ndistributions follow t distributions. The theory is supported by simulations\nand an empirical application.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 13:04:30 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Kuang", "D.", ""], ["Nielsen", "B.", ""]]}, {"id": "1806.05951", "submitter": "Emanuele Taufer", "authors": "Emanuele Taufer, Flavio Santi, Giuseppe Espa, Maria Michela Dickson", "title": "A goodness of fit test for the Pareto distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Zenga (1984) inequality curve is constant in p for Type I Pareto\ndistributions. This characterizing behavior will be exploited to obtain\ngraphical and analytical tools for tail analysis and goodness of fit tests. A\ntesting procedure for Pareto-type behavior based on a regression of technique\nwill be introduced.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 13:32:51 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Taufer", "Emanuele", ""], ["Santi", "Flavio", ""], ["Espa", "Giuseppe", ""], ["Dickson", "Maria Michela", ""]]}, {"id": "1806.06028", "submitter": "Bruno Ebner", "authors": "Steffen Betsch and Bruno Ebner", "title": "A new characterization of the Gamma distribution and associated goodness\n  of fit tests", "comments": null, "journal-ref": "Metrika, Volume 82, Issue 7, pages 779-806, (2019)", "doi": "10.1007/s00184-019-00708-7", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of weighted $L_2$-type tests of fit to the Gamma\ndistribution. Our novel procedure is based on a fixed point property of a new\ntransformation connected to a Steinian characterization of the family of Gamma\ndistributions. We derive the weak limits of the statistic under the null\nhypothesis and under contiguous alternatives. Further, we establish the global\nconsistency of the tests and apply a parametric bootstrap technique in a Monte\nCarlo simulation study to show the competitiveness to existing procedures.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 15:56:58 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 15:57:28 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Betsch", "Steffen", ""], ["Ebner", "Bruno", ""]]}, {"id": "1806.06136", "submitter": "Jessica Young PhD", "authors": "Jessica G. Young, Mats J. Stensrud, Eric J. Tchetgen Tchetgen, Miguel\n  A. Hern\\'an", "title": "A causal framework for classical statistical estimands in failure time\n  settings with competing events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In failure-time settings, a competing risk event is any event that makes it\nimpossible for the event of interest to occur. For example, cardiovascular\ndisease death is a competing event for prostate cancer death because an\nindividual cannot die of prostate cancer once he has died of cardiovascular\ndisease. Various statistical estimands have been defined as possible targets of\ninference in the classical competing risks literature. Many reviews have\ndescribed these statistical estimands and their estimating procedures with\nrecommendations about their use. However, this previous work has not used a\nformal framework for characterizing causal effects and their identifying\nconditions, which makes it difficult to interpret effect estimates and assess\nrecommendations regarding analytic choices. Here we use a counterfactual\nframework to explicitly define each of these classical estimands. We clarify\nthat, depending on whether competing events are defined as censoring events,\ncontrasts of risks can define a total effect of the treatment on the event of\ninterest, or a direct effect of the treatment on the event of interest not\nmediated through the competing event. In contrast, regardless of whether\ncompeting events are defined as censoring events, counterfactual hazard\ncontrasts cannot generally be interpreted as causal effects. We illustrate how\nidentifying assumptions for all of these counterfactual estimands can be\nrepresented in causal diagrams in which competing events are depicted as\ntime-varying covariates. We present an application of these ideas to data from\na randomized trial designed to estimate the effect of estrogen therapy on\nprostate cancer mortality.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 21:46:20 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 02:06:06 GMT"}, {"version": "v3", "created": "Thu, 7 Nov 2019 01:11:59 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Young", "Jessica G.", ""], ["Stensrud", "Mats J.", ""], ["Tchetgen", "Eric J. Tchetgen", ""], ["Hern\u00e1n", "Miguel A.", ""]]}, {"id": "1806.06179", "submitter": "Zijian Guo", "authors": "T. Tony Cai, Zijian Guo", "title": "Semi-supervised Inference for Explained Variance in High-dimensional\n  Linear Regression and Its Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers statistical inference for the explained variance\n$\\beta^{\\intercal}\\Sigma \\beta$ under the high-dimensional linear model\n$Y=X\\beta+\\epsilon$ in the semi-supervised setting, where $\\beta$ is the\nregression vector and $\\Sigma$ is the design covariance matrix. A calibrated\nestimator, which efficiently integrates both labelled and unlabelled data, is\nproposed. It is shown that the estimator achieves the minimax optimal rate of\nconvergence in the general semi-supervised framework. The optimality result\ncharacterizes how the unlabelled data contributes to the estimation accuracy.\nMoreover, the limiting distribution for the proposed estimator is established\nand the unlabelled data has also proven useful in reducing the length of the\nconfidence interval for the explained variance. The proposed method is extended\nto the semi-supervised inference for the unweighted quadratic functional,\n$\\|\\beta\\|_2^2$. The obtained inference results are then applied to a range of\nhigh-dimensional statistical problems, including signal detection and global\ntesting, prediction accuracy evaluation, and confidence ball construction. The\nnumerical improvement of incorporating the unlabelled data is demonstrated\nthrough simulation studies and an analysis of estimating heritability for a\nyeast segregant data set with multiple traits.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 04:39:14 GMT"}, {"version": "v2", "created": "Mon, 30 Nov 2020 13:46:16 GMT"}], "update_date": "2020-12-01", "authors_parsed": [["Cai", "T. Tony", ""], ["Guo", "Zijian", ""]]}, {"id": "1806.06209", "submitter": "Arjun Sondhi", "authors": "Arjun Sondhi, Ali Shojaie", "title": "The Reduced PC-Algorithm: Improved Causal Structure Learning in Large\n  Random Networks", "comments": null, "journal-ref": "Journal of Machine Learning Research, 20(164), 1-31 (2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG q-bio.GN stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of estimating a high-dimensional directed acyclic graph,\ngiven observations from a linear structural equation model with arbitrary noise\ndistribution. By exploiting properties of common random graphs, we develop a\nnew algorithm that requires conditioning only on small sets of variables. The\nproposed algorithm, which is essentially a modified version of the\nPC-Algorithm, offers significant gains in both computational complexity and\nestimation accuracy. In particular, it results in more efficient and accurate\nestimation in large networks containing hub nodes, which are common in\nbiological systems. We prove the consistency of the proposed algorithm, and\nshow that it also requires a less stringent faithfulness assumption than the\nPC-Algorithm. Simulations in low and high-dimensional settings are used to\nillustrate these findings. An application to gene expression data suggests that\nthe proposed algorithm can identify a greater number of clinically relevant\ngenes than current methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 08:40:14 GMT"}, {"version": "v2", "created": "Sun, 22 Sep 2019 21:00:15 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Sondhi", "Arjun", ""], ["Shojaie", "Ali", ""]]}, {"id": "1806.06231", "submitter": "Frederic Lavancier", "authors": "Fr\\'ed\\'eric Lavancier and Arnaud Poinas and Rasmus Waagepetersen", "title": "Adaptive estimating function inference for non-stationary determinantal\n  point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating function inference is indispensable for many common point process\nmodels where the joint intensities are tractable while the likelihood function\nis not. In this paper we establish asymptotic normality of estimating function\nestimators in a very general setting of non-stationary point processes. We then\nadapt this result to the case of non-stationary determinantal point processes\nwhich are an important class of models for repulsive point patterns. In\npractice often first and second order estimating functions are used. For the\nlatter it is common practice to omit contributions for pairs of points\nseparated by a distance larger than some truncation distance which is usually\nspecified in an ad hoc manner. We suggest instead a data-driven approach where\nthe truncation distance is adapted automatically to the point process being\nfitted and where the approach integrates seamlessly with our asymptotic\nframework. The good performance of the adaptive approach is illustrated via\nsimulation studies for non-stationary determinantal point processes and by an\napplication to a real dataset.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 12:19:51 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 10:14:05 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Lavancier", "Fr\u00e9d\u00e9ric", ""], ["Poinas", "Arnaud", ""], ["Waagepetersen", "Rasmus", ""]]}, {"id": "1806.06295", "submitter": "Nadezhda Gribkova Dr.", "authors": "Nadezhda Gribkova and Ri\\v{c}ardas Zitikis", "title": "Detecting intrusions in control systems: a rule of thumb, its\n  justification and illustrations", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control systems are exposed to unintentional errors, deliberate intrusions,\nfalse data injection attacks, and various other disruptions. In this paper we\npropose, justify, and illustrate a rule of thumb for detecting, or confirming\nthe absence of, such disruptions. To facilitate the use of the rule, we\nrigorously discuss background results that delineate the boundaries of the\nrule's applicability. We also discuss ways to further widen the applicability\nof the proposed intrusion-detection methodology.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 21:07:52 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Gribkova", "Nadezhda", ""], ["Zitikis", "Ri\u010dardas", ""]]}, {"id": "1806.06297", "submitter": "Susheela Singh", "authors": "Susheela P. Singh, Ana-Maria Staicu, Robert R. Dunn, Noah Fierer and\n  Brian J. Reich", "title": "A nonparametric spatial test to identify factors that shape a microbiome", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of high-throughput sequencing technologies has made data from DNA\nmaterial readily available, leading to a surge of microbiome-related research\nestablishing links between markers of microbiome health and specific outcomes.\nHowever, to harness the power of microbial communities we must understand not\nonly how they affect us, but also how they can be influenced to improve\noutcomes. This area has been dominated by methods that reduce community\ncomposition to summary metrics, which can fail to fully exploit the complexity\nof community data. Recently, methods have been developed to model the abundance\nof taxa in a community, but they can be computationally intensive and do not\naccount for spatial effects underlying microbial settlement. These spatial\neffects are particularly relevant in the microbiome setting because we expect\ncommunities that are close together to be more similar than those that are far\napart. In this paper, we propose a flexible Bayesian spike-and-slab variable\nselection model for presence-absence indicators that accounts for spatial\ndependence and cross-dependence between taxa while reducing dimensionality in\nboth directions. We show by simulation that in the presence of spatial\ndependence, popular distance-based hypothesis testing methods fail to preserve\ntheir advertised size, and the proposed method improves variable selection.\nFinally, we present an application of our method to an indoor fungal community\nfound with homes across the contiguous United States.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 21:11:20 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Singh", "Susheela P.", ""], ["Staicu", "Ana-Maria", ""], ["Dunn", "Robert R.", ""], ["Fierer", "Noah", ""], ["Reich", "Brian J.", ""]]}, {"id": "1806.06304", "submitter": "X. Jessie Jeng", "authors": "X. Jessie Jeng, Huimin Peng and Wenbin Lu", "title": "Post-Lasso Inference for High-Dimensional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the most popular variable selection procedures in high-dimensional\nregression, Lasso provides a solution path to rank the variables and determines\na cut-off position on the path to select variables and estimate coefficients.\nIn this paper, we consider variable selection from a new perspective motivated\nby the frequently occurred phenomenon that relevant variables are not\ncompletely distinguishable from noise variables on the solution path. We\npropose to characterize the positions of the first noise variable and the last\nrelevant variable on the path. We then develop a new variable selection\nprocedure to control over-selection of the noise variables ranking after the\nlast relevant variable, and, at the same time, retain a high proportion of\nrelevant variables ranking before the first noise variable. Our procedure\nutilizes the recently developed covariance test statistic and Q statistic in\npost-selection inference. In numerical examples, our method compares favorably\nwith other existing methods in selection accuracy and the ability to interpret\nits results.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 22:09:04 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Jeng", "X. Jessie", ""], ["Peng", "Huimin", ""], ["Lu", "Wenbin", ""]]}, {"id": "1806.06468", "submitter": "Liuhua Peng", "authors": "Liuhua Peng, Long Qu, Dan Nettleton", "title": "Variable Importance Assessments and Backward Variable Selection for\n  High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection in high-dimensional scenarios is of great interested in\nstatistics. One application involves identifying differentially expressed genes\nin genomic analysis. Existing methods for addressing this problem have some\nlimits or disadvantages. In this paper, we propose distance based variable\nimportance measures to deal with these problems, which is inspired by the\nMulti-Response Permutation Procedure (MRPP). The proposed variable importance\nassessments can effectively measure the importance of an individual dimension\nby quantifying its influence on the differences between multivariate\ndistributions. A backward selection algorithm is developed that can be used in\nhigh-dimensional variable selection to discover important variables. Both\nsimulations and real data applications demonstrate that our proposed method\nenjoys good properties and has advantages over other methods.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 00:45:38 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Peng", "Liuhua", ""], ["Qu", "Long", ""], ["Nettleton", "Dan", ""]]}, {"id": "1806.06489", "submitter": "Luca Tardella", "authors": "Danilo Alunni Fegatelli and Luca Tardella", "title": "Moment-based Bayesian Poisson Mixtures for inferring unobserved units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We exploit a suitable moment-based characterization of the mixture of Poisson\ndistribution for developing Bayesian inference for the unknown size of a finite\npopulation whose units are subject to multiple occurrences during an\nenumeration sampling stage. This is a particularly challenging setting for\nwhich many other attempts have been made for inferring the unknown\ncharacteristics of the population. Here we put particular emphasis on the\nconstruction of a default prior elicitation of the characteristics of the\nmixing distribution. We assess the comparative performance of our approach in\nreal data applications and in a simulation study.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 03:36:38 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Fegatelli", "Danilo Alunni", ""], ["Tardella", "Luca", ""]]}, {"id": "1806.06523", "submitter": "Efstathios Paparoditis", "authors": "Marco Meyer, Efstathios Paparoditis and Jens-Peter Kreiss", "title": "A Frequency Domain Bootstrap for General Stationary Processes", "comments": "38 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing frequency domain methods for bootstrapping time series have a\nlimited range. Consider for instance the class of spectral mean statistics\n(also called integrated periodograms) which includes many important statistics\nin time series analysis, such as sample autocovariances and autocorrelations\namong other things. Essentially, such frequency domain bootstrap procedures\ncover the case of linear time series with independent innovations, and some\neven require the time series to be Gaussian. In this paper we propose a new,\nfrequency domain bootstrap method which is consistent for a much wider range of\nstationary processes and can be applied to a large class of periodogram-based\nstatistics. It introduces a new concept of convolved periodograms of smaller\nsamples which uses pseudo periodograms of subsamples generated in a way that\ncorrectly imitates the weak dependence structure of the periodogram. %The new\nbootstrap procedure %corrects for those aspects of the distribution of spectral\nmeans that cannot be mimicked by existing procedures. We show consistency for\nthis procedure for a general class of stationary time series, ranging clearly\nbeyond linear processes, and for general spectral means and ratio statistics.\nFurthermore, and for the class of spectral means, we also show, how, using this\nnew approach, existing bootstrap methods, which replicate appropriately only\nthe dominant part of the distribution of interest, can be corrected. The finite\nsample performance of the new bootstrap procedure is illustrated via\nsimulations.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 07:24:59 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Meyer", "Marco", ""], ["Paparoditis", "Efstathios", ""], ["Kreiss", "Jens-Peter", ""]]}, {"id": "1806.06746", "submitter": "Jere Koskela", "authors": "Jere Koskela and Maite Wilke Berenguer", "title": "Robust model selection between population growth and multiple merger\n  coalescents", "comments": "21 pages, 8 figures", "journal-ref": "Mathematical Biosciences 311:1-12, 2019", "doi": "10.1016/j.mbs.2019.03.004", "report-no": null, "categories": "q-bio.PE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the effect of biological confounders on the model selection problem\nbetween Kingman coalescents with population growth, and Xi-coalescents\ninvolving simultaneous multiple mergers. We use a low dimensional,\ncomputationally tractable summary statistic, dubbed the singleton-tail\nstatistic, to carry out approximate likelihood ratio tests between these model\nclasses. The singleton-tail statistic has been shown to distinguish between\nthem with high power in the simple setting of neutrally evolving, panmictic\npopulations without recombination. We extend this work by showing that cryptic\nrecombination and selection do not diminish the power of the test, but that\nmisspecifying population structure does. Furthermore, we demonstrate that the\nsingleton-tail statistic can also solve the more challenging model selection\nproblem between multiple mergers due to selective sweeps, and multiple mergers\ndue to high fecundity with moderate power of up to 30%.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 14:57:13 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 21:17:51 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 11:36:03 GMT"}, {"version": "v4", "created": "Tue, 5 Mar 2019 18:29:29 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Koskela", "Jere", ""], ["Berenguer", "Maite Wilke", ""]]}, {"id": "1806.06761", "submitter": "HaiYing Wang", "authors": "Mingyao Ai, Jun Yu, Huiming Zhang and HaiYing Wang", "title": "Optimal Subsampling Algorithms for Big Data Regressions", "comments": null, "journal-ref": "Statist Sinica, 2021, 31(2), 749-772", "doi": "10.5705/ss.202018.0439", "report-no": "http://www3.stat.sinica.edu.tw/statistica/J31N2/j31n208/j31n208.html", "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To fast approximate maximum likelihood estimators with massive data, this\npaper studies the Optimal Subsampling Method under the A-optimality Criterion\n(OSMAC) for generalized linear models. The consistency and asymptotic normality\nof the estimator from a general subsampling algorithm are established, and\noptimal subsampling probabilities under the A- and L-optimality criteria are\nderived. Furthermore, using Frobenius norm matrix concentration inequalities,\nfinite sample properties of the subsample estimator based on optimal\nsubsampling probabilities are also derived. Since the optimal subsampling\nprobabilities depend on the full data estimate, an adaptive two-step algorithm\nis developed. Asymptotic normality and optimality of the estimator from this\nadaptive algorithm are established. The proposed methods are illustrated and\nevaluated through numerical experiments on simulated and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:15:41 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 09:05:27 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ai", "Mingyao", ""], ["Yu", "Jun", ""], ["Zhang", "Huiming", ""], ["Wang", "HaiYing", ""]]}, {"id": "1806.06777", "submitter": "Li Ma", "authors": "Shai Gorsky and Li Ma", "title": "Multiscale Fisher's Independence Test for Multivariate Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying dependency in multivariate data is a common inference task that\narises in numerous applications. However, existing nonparametric independence\ntests typically require computation that scales at least quadratically with the\nsample size, making it difficult to apply them to massive data. Moreover,\nresampling is usually necessary to evaluate the statistical significance of the\nresulting test statistics at finite sample sizes, further worsening the\ncomputational burden. We introduce a scalable, resampling-free approach to\ntesting the independence between two random vectors by breaking down the task\ninto simple univariate tests of independence on a collection of 2x2 contingency\ntables constructed through sequential coarse-to-fine discretization of the\nsample space, transforming the inference task into a multiple testing problem\nthat can be completed with almost linear complexity with respect to the sample\nsize. To address increasing dimensionality, we introduce a coarse-to-fine\nsequential adaptive procedure that exploits the spatial features of dependency\nstructures to more effectively examine the sample space. We derive a\nfinite-sample theory that guarantees the inferential validity of our adaptive\nprocedure at any given sample size. In particular, we show that our approach\ncan achieve strong control of the family-wise error rate without resampling or\nlarge-sample approximation. We demonstrate the substantial computational\nadvantage of the procedure in comparison to existing approaches as well as its\ndecent statistical power under various dependency scenarios through an\nextensive simulation study, and illustrate how the divide-and-conquer nature of\nthe procedure can be exploited to not just test independence but to learn the\nnature of the underlying dependency. Finally, we demonstrate the use of our\nmethod through analyzing a large data set from a flow cytometry experiment.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:38:54 GMT"}, {"version": "v2", "created": "Mon, 17 Sep 2018 19:02:19 GMT"}, {"version": "v3", "created": "Tue, 19 Mar 2019 13:48:05 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 14:13:48 GMT"}, {"version": "v5", "created": "Fri, 10 Jan 2020 17:21:40 GMT"}, {"version": "v6", "created": "Tue, 14 Jan 2020 19:09:36 GMT"}, {"version": "v7", "created": "Wed, 7 Jul 2021 11:37:00 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Gorsky", "Shai", ""], ["Ma", "Li", ""]]}, {"id": "1806.06784", "submitter": "Cheng Ju", "authors": "Cheng Ju and David Benkeser and Mark J. van der Laan", "title": "Robust inference on the average treatment effect using the outcome\n  highly adaptive lasso", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many estimators of the average effect of a treatment on an outcome require\nestimation of the propensity score, the outcome regression, or both. It is\noften beneficial to utilize flexible techniques such as semiparametric\nregression or machine learning to estimate these quantities. However, optimal\nestimation of these regressions does not necessarily lead to optimal estimation\nof the average treatment effect, particularly in settings with strong\ninstrumental variables. A recent proposal addressed these issues via the\noutcome-adaptive lasso, a penalized regression technique for estimating the\npropensity score that seeks to minimize the impact of instrumental variables on\ntreatment effect estimators. However, a notable limitation of this approach is\nthat its application is restricted to parametric models. We propose a more\nflexible alternative that we call the outcome highly adaptive lasso. We discuss\nlarge sample theory for this estimator and propose closed form confidence\nintervals based on the proposed estimator. We show via simulation that our\nmethod offers benefits over several popular approaches.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 15:47:37 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 18:01:26 GMT"}, {"version": "v3", "created": "Mon, 13 May 2019 00:04:41 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Ju", "Cheng", ""], ["Benkeser", "David", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1806.06799", "submitter": "Huijuan Ma", "authors": "Huijuan Ma, Limin Peng and Haoda Fu", "title": "Quantile Regression of Latent Longitudinal Trajectory Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression has demonstrated promising utility in longitudinal data\nanalysis. Existing work is primarily focused on modeling cross-sectional\noutcomes, while outcome trajectories often carry more substantive information\nin practice. In this work, we develop a trajectory quantile regression\nframework that is designed to robustly and flexibly investigate how latent\nindividual trajectory features are related to observed subject characteristics.\nThe proposed models are built under modeling with usual parametric assumptions\nlifted or relaxed. We derive our estimation procedure by novelly transforming\nthe problem at hand to quantile regression with perturbed responses and\nadapting the bias correction technique for handling covariate measurement\nerrors. We establish desirable asymptotic properties of the proposed estimator,\nincluding uniform consistency and weak convergence. Extensive simulation\nstudies confirm the validity of the proposed method as well as its robustness.\nAn application to the DURABLE trial uncovers sensible scientific findings and\nillustrates the practical value of our proposals.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 16:10:22 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Ma", "Huijuan", ""], ["Peng", "Limin", ""], ["Fu", "Haoda", ""]]}, {"id": "1806.06802", "submitter": "Yameng Liu", "authors": "Yameng Liu, Aw Dieng, Sudeepa Roy, Cynthia Rudin, Alexander Volfovsky", "title": "Interpretable Almost Matching Exactly for Causal Inference", "comments": "AISTATS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We aim to create the highest possible quality of treatment-control matches\nfor categorical data in the potential outcomes framework. Matching methods are\nheavily used in the social sciences due to their interpretability, but most\nmatching methods do not pass basic sanity checks: they fail when irrelevant\nvariables are introduced, and tend to be either computationally slow or produce\nlow-quality matches. The method proposed in this work aims to match units on a\nweighted Hamming distance, taking into account the relative importance of the\ncovariates; the algorithm aims to match units on as many relevant variables as\npossible. To do this, the algorithm creates a hierarchy of covariate\ncombinations on which to match (similar to downward closure), in the process\nsolving an optimization problem for each unit in order to construct the optimal\nmatches. The algorithm uses a single dynamic program to solve all of the\noptimization problems simultaneously. Notable advantages of our method over\nexisting matching procedures are its high-quality matches, versatility in\nhandling different data distributions that may have irrelevant variables, and\nability to handle missing data by matching on as many available covariates as\npossible.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 16:11:56 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 03:29:31 GMT"}, {"version": "v3", "created": "Thu, 22 Nov 2018 22:07:54 GMT"}, {"version": "v4", "created": "Wed, 5 Jun 2019 04:50:17 GMT"}, {"version": "v5", "created": "Thu, 6 Jun 2019 04:12:27 GMT"}, {"version": "v6", "created": "Sat, 8 Jun 2019 04:48:20 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Liu", "Yameng", ""], ["Dieng", "Aw", ""], ["Roy", "Sudeepa", ""], ["Rudin", "Cynthia", ""], ["Volfovsky", "Alexander", ""]]}, {"id": "1806.06853", "submitter": "Woundjiague Apollinaire", "authors": "Woundjiagu\\'e Apollinaire, Mbele Bidima Martin Le Doux, Waweru Mwangi\n  Ronald", "title": "A Hybrid Fuzzy Regression Model for Optimal Loss Reserving in Insurance", "comments": "arXiv admin note: substantial text overlap with arXiv:1806.04530", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a Hybrid Fuzzy Regression Model with Asymmetric Triangular\nFuzzy Coefficients and optimized $h-$value in Generalized Linear Models (GLM)\nframework have been developed. The weighted functions of Fuzzy Numbers rather\nthan the Expected value of Fuzzy Number is used as a defuzzification procedure.\nWe perform the new model on a numerical data (Taylor and Ashe, 1983) to predict\nincremental payments in loss reserving. We prove that the new Hybrid Model with\nthe optimized $h-$value produce better results than the classical GLM according\nto the Reserve Prediction Error and Reserve Standard Deviation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 08:21:13 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Apollinaire", "Woundjiagu\u00e9", ""], ["Doux", "Mbele Bidima Martin Le", ""], ["Ronald", "Waweru Mwangi", ""]]}, {"id": "1806.06959", "submitter": "Carsten Chong", "authors": "Carsten Chong", "title": "High-frequency analysis of parabolic stochastic PDEs", "comments": "Including supplementary material; accepted for publication in the\n  Annals of Statistics", "journal-ref": "The Annals of Statistics, Vol. 48, No. 2, 1143-1167, 2020", "doi": "10.1214/19-AOS1841", "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating stochastic volatility for a class of\nsecond-order parabolic stochastic PDEs. Assuming that the solution is observed\nat a high temporal frequency, we use limit theorems for multipower variations\nand related functionals to construct consistent nonparametric estimators and\nasymptotic confidence bounds for the integrated volatility process. As a\nbyproduct of our analysis, we also obtain feasible estimators for the\nregularity of the spatial covariance function of the noise.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 21:30:57 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 15:38:16 GMT"}, {"version": "v3", "created": "Fri, 25 Jan 2019 19:11:08 GMT"}, {"version": "v4", "created": "Mon, 25 Mar 2019 11:52:40 GMT"}], "update_date": "2020-06-02", "authors_parsed": [["Chong", "Carsten", ""]]}, {"id": "1806.06974", "submitter": "Glen DePalma", "authors": "Glen DePalma and Bruce A. Craig", "title": "Bayesian monotonic errors-in-variables models with applications to\n  pathogen susceptibility testing", "comments": null, "journal-ref": "Statistics in Medicine. 2018. 37:478-502", "doi": "10.1002/sim.7533", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drug dilution (MIC) and disk diffusion (DIA) are the two most common\nantimicrobial susceptibility assays used by hospitals and clinics to determine\nan unknown pathogen's susceptibility to various antibiotics. Since only one\nassay is commonly used, it is important that the two assays give similar\nresults. Calibration of the DIA assay to the MIC assay is typically done using\nthe error-rate bounded method, which selects DIA breakpoints that minimize the\nobserved discrepancies between the two assays. In 2000, Craig proposed a\nmodel-based approach that specifically models the measurement error and\nrounding processes of each assay, the underlying pathogen distribution, and the\ntrue monotonic relationship between the two assays. The two assays are then\ncalibrated by focusing on matching the probabilities of correct classification\n(susceptible, indeterminant, and resistant). This approach results in greater\nprecision and accuracy for estimating DIA breakpoints. In this paper, we expand\nthe flexibility of the model-based method by introducing a Bayesian\nfour-parameter logistic model (extending Craig's original three-parameter\nmodel) as well as a Bayesian nonparametric spline model to describe the\nrelationship between the two assays. We propose two ways to handle spline knot\nselection, considering many equally-spaced knots but restricting overfitting\nvia a random walk prior and treating the number and location of knots as\nadditional unknown parameters. We demonstrate the two approaches via a series\nof simulation studies and apply the methods to two real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 22:35:36 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["DePalma", "Glen", ""], ["Craig", "Bruce A.", ""]]}, {"id": "1806.07016", "submitter": "Michael Gechter", "authors": "Michael Gechter, Cyrus Samii, Rajeev Dehejia, Cristian Pop-Eleches", "title": "Evaluating Ex Ante Counterfactual Predictions Using Ex Post Causal\n  Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a formal, decision-based method for comparing the performance of\ncounterfactual treatment regime predictions using the results of experiments\nthat give relevant information on the distribution of treated outcomes. Our\napproach allows us to quantify and assess the statistical significance of\ndifferential performance for optimal treatment regimes estimated from\nstructural models, extrapolated treatment effects, expert opinion, and other\nmethods. We apply our method to evaluate optimal treatment regimes for\nconditional cash transfer programs across countries where predictions are\ngenerated using data from experimental evaluations in other countries and\npre-program data in the country of interest.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 02:33:42 GMT"}, {"version": "v2", "created": "Mon, 22 Jul 2019 19:02:26 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Gechter", "Michael", ""], ["Samii", "Cyrus", ""], ["Dehejia", "Rajeev", ""], ["Pop-Eleches", "Cristian", ""]]}, {"id": "1806.07161", "submitter": "Santtu Tikka", "authors": "Santtu Tikka and Juha Karvanen", "title": "Identifying Causal Effects with the R Package causaleffect", "comments": "This is the version published in the Journal of Statistical Software", "journal-ref": "Journal of Statistical Software, 76(12):1-30, 2017", "doi": "10.18637/jss.v076.i12", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Do-calculus is concerned with estimating the interventional distribution of\nan action from the observed joint probability distribution of the variables in\na given causal structure. All identifiable causal effects can be derived using\nthe rules of do-calculus, but the rules themselves do not give any direct\nindication whether the effect in question is identifiable or not. Shpitser and\nPearl constructed an algorithm for identifying joint interventional\ndistributions in causal models, which contain unobserved variables and induce\ndirected acyclic graphs. This algorithm can be seen as a repeated application\nof the rules of do-calculus and known properties of probabilities, and it\nultimately either derives an expression for the causal distribution, or fails\nto identify the effect, in which case the effect is non-identifiable. In this\npaper, the R package causaleffect is presented, which provides an\nimplementation of this algorithm. Functionality of causaleffect is also\ndemonstrated through examples.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 11:29:33 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Tikka", "Santtu", ""], ["Karvanen", "Juha", ""]]}, {"id": "1806.07172", "submitter": "Santtu Tikka", "authors": "Santtu Tikka and Juha Karvanen", "title": "Surrogate Outcomes and Transportability", "comments": "This is the version published in the International Journal of\n  Approximate Reasoning", "journal-ref": "International Journal of Approximate Reasoning, 2019; 108: 21-37", "doi": "10.1016/j.ijar.2019.02.007", "report-no": null, "categories": "cs.AI stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identification of causal effects is one of the most fundamental tasks of\ncausal inference. We consider an identifiability problem where some\nexperimental and observational data are available but neither data alone is\nsufficient for the identification of the causal effect of interest. Instead of\nthe outcome of interest, surrogate outcomes are measured in the experiments.\nThis problem is a generalization of identifiability using surrogate experiments\nand we label it as surrogate outcome identifiability. We show that the concept\nof transportability provides a sufficient criteria for determining surrogate\noutcome identifiability for a large class of queries.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 12:01:29 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 11:24:39 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 07:27:49 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2019 12:49:08 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Tikka", "Santtu", ""], ["Karvanen", "Juha", ""]]}, {"id": "1806.07176", "submitter": "Marco Geraci", "authors": "Marco Geraci", "title": "Letter to the Editor", "comments": "11 pages, 2, figures, 2 tables", "journal-ref": "Statistics and Its Interface, Volume 12, Issue 1, 2019, Pages\n  71-75", "doi": "10.4310/SII.2019.v12.n1.a7", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Galarza, Lachos and Bandyopadhyay (2017) have recently proposed a method of\nestimating linear quantile mixed models (Geraci and Bottai, 2014) based on a\nMonte Carlo EM algorithm. They assert that their procedure represents an\nimprovement over the numerical quadrature and non-smooth optimization approach\nimplemented by Geraci (2014). The objective of this note is to demonstrate that\nthis claim is incorrect. We also point out several inaccuracies and\nshortcomings in their paper which affect other results and conclusions that can\nbe drawn.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 12:13:53 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 12:10:24 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Geraci", "Marco", ""]]}, {"id": "1806.07244", "submitter": "Philippe Regnault", "authors": "Justine Lequesne, and Philippe Regnault", "title": "vsgoftest: An Package for Goodness-of-Fit Testing Based on\n  Kullback-Leibler Divergence", "comments": "25 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R-package vsgoftest performs goodness-of-fit (GOF) tests, based on\nShannon entropy and Kullback-Leibler divergence, developed by Vasicek (1976)\nand Song (2002), of various classical families of distributions. The\ntheoretical framework of the so-called Vasicek-Song (VS) tests is summarized\nand followed by a detailed description of the different features of the\npackage. The power and computational time performances of VS tests are studied\nthrough their comparison with other GOF tests. Application to real datasets\nillustrates the easy-to-use functionalities of the vsgoftest package.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 13:59:29 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Lequesne", "Justine", ""], ["Regnault", "Philippe", ""]]}, {"id": "1806.07274", "submitter": "Vincent Chin", "authors": "Vincent Chin, David Gunawan, Denzil G. Fiebig, Robert Kohn, Scott A.\n  Sisson", "title": "Efficient data augmentation for multivariate probit models with panel\n  data: An application to general practitioner decision-making about\n  contraceptives", "comments": null, "journal-ref": null, "doi": "10.1111/rssc.12393", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the problem of estimating a multivariate probit model\nin a panel data setting with emphasis on sampling a high-dimensional\ncorrelation matrix and improving the overall efficiency of the data\naugmentation approach. We reparameterise the correlation matrix in a principled\nway and then carry out efficient Bayesian inference using Hamiltonian Monte\nCarlo. We also propose a novel antithetic variable method to generate samples\nfrom the posterior distribution of the random effects and regression\ncoefficients, resulting in significant gains in efficiency. We apply the\nmethodology by analysing stated preference data obtained from Australian\ngeneral practitioners evaluating alternative contraceptive products. Our\nanalysis suggests that the joint probability of discussing combinations of\ncontraceptive products with a patient shows medical practice variation among\nthe general practitioners, which indicates some resistance to even discuss\nthese products, let alone recommend them.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:22:29 GMT"}, {"version": "v2", "created": "Wed, 28 Aug 2019 09:06:02 GMT"}], "update_date": "2020-05-07", "authors_parsed": [["Chin", "Vincent", ""], ["Gunawan", "David", ""], ["Fiebig", "Denzil G.", ""], ["Kohn", "Robert", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1806.07320", "submitter": "Muhammad Naveed Tabassum", "authors": "Muhammad Naveed Tabassum and Esa Ollila", "title": "Simultaneous Signal Subspace Rank and Model Selection with an\n  Application to Single-snapshot Source Localization", "comments": "5 pages, 4 figures, To appear in the Proceedings of the 26th European\n  Signal Processing Conference (EUSIPCO 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.CV math.OC stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method for model selection in linear regression\nby utilizing the solution path of $\\ell_1$ regularized least-squares (LS)\napproach (i.e., Lasso). This method applies the complex-valued least angle\nregression and shrinkage (c-LARS) algorithm coupled with a generalized\ninformation criterion (GIC) and referred to as the c-LARS-GIC method.\nc-LARS-GIC is a two-stage procedure, where firstly precise values of the\nregularization parameter, called knots, at which a new predictor variable\nenters (or leaves) the active set are computed in the Lasso solution path.\nActive sets provide a nested sequence of regression models and GIC then selects\nthe best model. The sparsity order of the chosen model serves as an estimate of\nthe model order and the LS fit based only on the active set of the model\nprovides an estimate of the regression parameter vector. We then consider a\nsource localization problem, where the aim is to detect the number of impinging\nsource waveforms at a sensor array as well to estimate their\ndirection-of-arrivals (DoA-s) using only a single-snapshot measurement. We\nillustrate via simulations that, after formulating the problem as a grid-based\nsparse signal reconstruction problem, the proposed c-LARS-GIC method detects\nthe number of sources with high probability while at the same time it provides\naccurate estimates of source locations.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 16:01:48 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Tabassum", "Muhammad Naveed", ""], ["Ollila", "Esa", ""]]}, {"id": "1806.07422", "submitter": "Lan Liu", "authors": "Lan Liu, Michael G. Hudgens, Bradley Saul, John D. Clemens, Mohammad\n  Ali, Michael E. Emch", "title": "Doubly Robust Estimation in Observational Studies with Partial\n  Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interference occurs when the treatment (or exposure) of one individual\naffects the outcomes of others. In some settings it may be reasonable to assume\nindividuals can be partitioned into clusters such that there is no interference\nbetween individuals in different clusters, i.e., there is partial interference.\nIn observational studies with partial interference, inverse probability\nweighted (IPW) estimators have been proposed of different possible treatment\neffects. However, the validity of IPW estimators depends on the propensity\nscore being known or correctly modeled. Alternatively, one can estimate the\ntreatment effect using an outcome regression model. In this paper, we propose\ndoubly robust (DR) estimators which utilize both models and are consistent and\nasymptotically normal if either model, but not necessarily both, is correctly\nspecified. Empirical results are presented to demonstrate the DR property of\nthe proposed estimators, as well as the efficiency gain of DR over IPW\nestimators when both models are correctly specified. The different estimators\nare illustrated using data from a study examining the effects of cholera\nvaccination in Bangladesh.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 18:48:18 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Liu", "Lan", ""], ["Hudgens", "Michael G.", ""], ["Saul", "Bradley", ""], ["Clemens", "John D.", ""], ["Ali", "Mohammad", ""], ["Emch", "Michael E.", ""]]}, {"id": "1806.07921", "submitter": "Fabio M. Bayer Ph.D", "authors": "F\\'abio M. Bayer, Renato J. Cintra, Francisco Cribari-Neto", "title": "Beta seasonal autoregressive moving average models", "comments": "26 pages, 5 figures, 4 tables", "journal-ref": "Journal of Statistical Computation and Simulation, 2018", "doi": "10.1080/00949655.2018.1491974", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce the class of beta seasonal autoregressive moving\naverage ($\\beta$SARMA) models for modeling and forecasting time series data\nthat assume values in the standard unit interval. It generalizes the class of\nbeta autoregressive moving average models [Rocha and Cribari-Neto, Test, 2009]\nby incorporating seasonal dynamics to the model dynamic structure. Besides\nintroducing the new class of models, we develop parameter estimation,\nhypothesis testing inference, and diagnostic analysis tools. We also discuss\nout-of-sample forecasting. In particular, we provide closed-form expressions\nfor the conditional score vector and for the conditional Fisher information\nmatrix. We also evaluate the finite sample performances of conditional maximum\nlikelihood estimators and white noise tests using Monte Carlo simulations. An\nempirical application is presented and discussed.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 18:35:02 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Bayer", "F\u00e1bio M.", ""], ["Cintra", "Renato J.", ""], ["Cribari-Neto", "Francisco", ""]]}, {"id": "1806.08144", "submitter": "Jorge Martin Arevalillo", "authors": "Jorge M. Arevalillo, Hilario Navarro", "title": "Maximal skewness projections for scale mixtures of skew-normal vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate scale mixtures of skew-normal (SMSN) variables are flexible\nmodels that account for non-normality in multivariate data scenarios by tail\nweight assessment and a shape vector representing the asymmetry of the model in\na directional fashion. Its stochastic representation involves a skew-normal\n(SN) vector and a non negative mixing scalar variable, independent of the SN\nvector, that injects kurtosis into the SMSN model. We address the problem of\nfinding the maximal skewness projection for vectors that follow a SMSN\ndistribution; when simple conditions on the moments of the mixing variable are\nfulfilled, it can be shown that the direction yielding the maximal skewness is\nproportional to the shape vector. This finding stresses the directional nature\nof the asymmetry in this class of distributions; it also provides the\ntheoretical foundations for solving the skewness model based projection pursuit\nfor SMSN vectors. Some examples that show the validity of our theoretical\nfindings for the most famous distributions within the SMSN family are also\ngiven. For the sake of completeness we carry out a simulation experiment with\nartificial data, which sheds light on the usefulness and implications of our\nresult in the statistical practice.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 09:52:08 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Arevalillo", "Jorge M.", ""], ["Navarro", "Hilario", ""]]}, {"id": "1806.08200", "submitter": "Isobel Claire Gormley Dr.", "authors": "Isobel Claire Gormley and Sylvia Fr\\\"uhwirth-Schnatter", "title": "Mixtures of Experts Models", "comments": "A chapter prepared for the forthcoming Handbook of Mixture Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of experts models provide a framework in which covariates may be\nincluded in mixture models. This is achieved by modelling the parameters of the\nmixture model as functions of the concomitant covariates. Given their mixture\nmodel foundation, mixtures of experts models possess a diverse range of\nanalytic uses, from clustering observations to capturing parameter\nheterogeneity in cross-sectional data. This chapter focuses on delineating the\nmixture of experts modelling framework and demonstrates the utility and\nflexibility of mixtures of experts models as an analytic tool.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 12:30:12 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Gormley", "Isobel Claire", ""], ["Fr\u00fchwirth-Schnatter", "Sylvia", ""]]}, {"id": "1806.08258", "submitter": "Jon Steingrimsson", "authors": "Jon Arni Steingrimsson and Jiabei Yang", "title": "Subgroup Identification using Covariate Adjusted Interaction Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying sub-groups of participants in a\nclinical trial that have enhanced treatment effect. Recursive partitioning\nmethods that recursively partition the covariate space based on some measure of\nbetween groups treatment effect difference are popular for such sub-group\nidentification. The most commonly used recursive partitioning method, the\nclassification and regression tree algorithm, first creates a large tree by\nrecursively partitioning the covariate space using some splitting criteria and\nthen selects the final tree from all subtrees of the large tree. In the context\nof subgroup identification, calculation of the splitting criteria and the\nevaluation measure used for final tree selection rely on comparing differences\nin means between the treatment and control arm. When covariates are prognostic\nfor the outcome, covariate adjusted estimators have the ability to improve\nefficiency compared to using differences in means between the treatment and\ncontrol group. This manuscript develops two covariate adjusted estimators that\ncan be used to both make splitting decisions and for final tree selection. The\nperformance of the resulting covariate adjusted recursive partitioning\nalgorithm is evaluated using simulations and by analyzing a clinical trial that\nevaluates if motivational interviews improve treatment engagement for substance\nabusers.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 14:19:18 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Steingrimsson", "Jon Arni", ""], ["Yang", "Jiabei", ""]]}, {"id": "1806.08456", "submitter": "Xuekui Zhang", "authors": "Yan Xu, Li Xing, Jessica Su, Xuekui Zhang, Weiliang Qiu", "title": "Model-based clustering for identifying disease-associated SNPs in\n  case-control genome-wide association studies", "comments": null, "journal-ref": null, "doi": "10.1038/s41598-019-50229-6", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genome-wide association studies (GWASs) aim to detect genetic risk factors\nfor complex human diseases by identifying disease-associated single-nucleotide\npolymorphisms (SNPs). The traditional SNP-wise approach along with multiple\ntesting adjustment is over-conservative and lack of power in many GWASs. In\nthis article, we proposed a model-based clustering method that transforms the\nchallenging high-dimension-small-sample-size problem to\nlow-dimension-large-sample-size problem and borrows information across SNPs by\ngrouping SNPs into three clusters. We pre-specify the patterns of clusters by\nminor allele frequencies of SNPs between cases and controls, and enforce the\npatterns with prior distributions. In the simulation studies our proposed novel\nmodel outperform traditional SNP-wise approach by showing better controls of\nfalse discovery rate (FDR) and higher sensitivity. We re-analyzed two real\nstudies to identifying SNPs associated with severe bortezomib-induced\nperipheral neuropathy (BiPN) in patients with multiple myeloma (MM). The\noriginal analysis in the literature failed to identify SNPs after FDR\nadjustment. Our proposed method not only detected the reported SNPs after FDR\nadjustment but also discovered a novel BiPN-associated SNP rs4351714 that has\nbeen reported to be related to MM in another study.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 23:43:03 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 08:06:28 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Xu", "Yan", ""], ["Xing", "Li", ""], ["Su", "Jessica", ""], ["Zhang", "Xuekui", ""], ["Qiu", "Weiliang", ""]]}, {"id": "1806.08807", "submitter": "Bjoern Bornkamp", "authors": "Bj\\\"orn Bornkamp and Georgina Bermann", "title": "Estimating the treatment effect in a subgroup defined by an early\n  post-baseline biomarker measurement in randomized clinical trials with\n  time-to-event endpoint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomarker measurements can be relatively easy and quick to obtain and they\nare useful to investigate whether a compound works as intended on a\nmechanistic, pharmacological level. In some situations, it is realistic to\nassume that patients, whose post-baseline biomarker levels indicate that they\ndo not sufficiently respond to the drug, are also unlikely to respond on\nclinically relevant long term outcomes (such as time-to-event). However the\ndetermination of the treatment effect in the subgroup of patients that\nsufficiently respond to the drug according to their biomarker levels is not\nstraightforward: It is unclear which patients on placebo would have responded\nhad they been given the treatment, so that naive comparisons between treatment\nand placebo will not estimate the treatment effect of interest. The purpose of\nthis paper is to investigate assumptions necessary to obtain causal conclusions\nin such a setting, utilizing the formalism of causal inference. Three\napproaches for estimation of subgroup effects will be developed and illustrated\nusing simulations and a case-study.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 18:16:57 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Bornkamp", "Bj\u00f6rn", ""], ["Bermann", "Georgina", ""]]}, {"id": "1806.08911", "submitter": "Qiang Wu", "authors": "Ning Zhang, Zhou Yu and Qiang Wu", "title": "Overlapping Sliced Inverse Regression for Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sliced inverse regression (SIR) is a pioneer tool for supervised dimension\nreduction. It identifies the effective dimension reduction space, the subspace\nof significant factors with intrinsic lower dimensionality. In this paper, we\npropose to refine the SIR algorithm through an overlapping slicing scheme. The\nnew algorithm, called overlapping sliced inverse regression (OSIR), is able to\nestimate the effective dimension reduction space and determine the number of\neffective factors more accurately. We show that such overlapping procedure has\nthe potential to identify the information contained in the derivatives of the\ninverse regression curve, which helps to explain the superiority of OSIR. We\nalso prove that OSIR algorithm is $\\sqrt n $-consistent and verify its\neffectiveness by simulations and real applications.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 05:39:11 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Zhang", "Ning", ""], ["Yu", "Zhou", ""], ["Wu", "Qiang", ""]]}, {"id": "1806.09000", "submitter": "Florian Maire", "authors": "Florian Maire and Pierre Vandekerkhove", "title": "On Markov chain Monte Carlo for sparse and filamentary distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel strategy that combines a given collection of reversible Markov\nkernels is proposed. It consists in a Markov chain that moves, at each\niteration, according to one of the available Markov kernels selected via a\nstate-dependent probability distribution which is thus dubbed locally informed.\nIn contrast to random-scan approaches that assume a constant selection\nprobability distribution, the state-dependent distribution is typically\nspecified so as to privilege moving according to a kernel which is relevant for\nthe local topology of the target distribution.\n  The second contribution is to characterize situations where a locally\ninformed strategy should be preferred to its random-scan counterpart. We find\nthat for a specific class of target distribution, referred to as sparse and\nfilamentary, that exhibits a strong correlation between some variables and/or\nwhich concentrates its probability mass on some low dimensional linear\nsubspaces or on thinned curved manifolds, a locally informed strategy converges\nsubstantially faster and yields smaller asymptotic variances than an equivalent\nrandom-scan algorithm.\n  The research is at this stage essentially speculative: this paper combines a\nseries of observations on this topic, both theoretical and empirical, that\ncould serve as a groundwork for further investigations.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 16:25:28 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Maire", "Florian", ""], ["Vandekerkhove", "Pierre", ""]]}, {"id": "1806.09014", "submitter": "Weijie J. Su", "authors": "Richard Berk, Andreas Buja, Lawrence Brown, Edward George, Arun Kumar\n  Kuchibhotla, Weijie J. Su, Linda Zhao", "title": "Assumption Lean Regression", "comments": "Submitted for review, 21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that models used in conventional regression analysis are\ncommonly misspecified. A standard response is little more than a shrug. Data\nanalysts invoke Box's maxim that all models are wrong and then proceed as if\nthe results are useful nevertheless. In this paper, we provide an alternative.\nRegression models are treated explicitly as approximations of a true response\nsurface that can have a number of desirable statistical properties, including\nestimates that are asymptotically unbiased. Valid statistical inference\nfollows. We generalize the formulation to include regression functionals, which\nbroadens substantially the range of potential applications. An empirical\napplication is provided to illustrate the paper's key concepts.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 18:01:18 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 06:34:08 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Berk", "Richard", ""], ["Buja", "Andreas", ""], ["Brown", "Lawrence", ""], ["George", "Edward", ""], ["Kuchibhotla", "Arun Kumar", ""], ["Su", "Weijie J.", ""], ["Zhao", "Linda", ""]]}, {"id": "1806.09043", "submitter": "Emilie Lebarbier", "authors": "Olivier Bock, Xavier Collilieux, Fran\\c{c}ois Guillamon, Emilie\n  Lebarbier, Claire Pascal", "title": "A breakpoint detection in the mean model with heterogeneous variance on\n  fixed time-intervals", "comments": "15 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by an application for the homogeneization of\nGNSS-derived IWV (Integrated Water Vapour) series. Indeed, these GPS series are\naffected by abrupt changes due to equipment changes or environemental effects.\nThe detection and correction of the series from these changes is a crucial step\nbefore any use for climate studies. In addition to these abrupt changes, it has\nbeen observed in the series a non-stationary of the variability. We propose in\nthis paper a new segmentation model that is a breakpoint detection in the mean\nmodel of a Gaussian process with heterogeneous variance on known\ntime-intervals. In this segmentation case, the dynamic programming (DP)\nalgorithm used classically to infer the breakpoints can not be applied anymore.\nWe propose a procedure in two steps: we first estimate robustly the variances\nand then apply the classical inference by plugging these estimators. The\nperformance of our proposed procedure is assessed through simulation\nexperiments. An application to real GNSS data is presented.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 21:54:10 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Bock", "Olivier", ""], ["Collilieux", "Xavier", ""], ["Guillamon", "Fran\u00e7ois", ""], ["Lebarbier", "Emilie", ""], ["Pascal", "Claire", ""]]}, {"id": "1806.09048", "submitter": "Alexis Derumigny", "authors": "Alexis Derumigny and Jean-David Fermanian", "title": "A classification point-of-view about conditional Kendall's tau", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the problem of estimating conditional Kendall's tau can be\nrewritten as a classification task. Conditional Kendall's tau is a conditional\ndependence parameter that is a characteristic of a given pair of random\nvariables. The goal is to predict whether the pair is concordant (value of $1$)\nor discordant (value of $-1$) conditionally on some covariates. We prove the\nconsistency and the asymptotic normality of a family of penalized approximate\nmaximum likelihood estimators, including the equivalent of the logit and probit\nregressions in our framework. Then, we detail specific algorithms adapting\nusual machine learning techniques, including nearest neighbors, decision trees,\nrandom forests and neural networks, to the setting of the estimation of\nconditional Kendall's tau. Finite sample properties of these estimators and\ntheir sensitivities to each component of the data-generating process are\nassessed in a simulation study. Finally, we apply all these estimators to a\ndataset of European stock indices.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 22:03:10 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 15:10:28 GMT"}, {"version": "v3", "created": "Mon, 26 Nov 2018 10:35:28 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Derumigny", "Alexis", ""], ["Fermanian", "Jean-David", ""]]}, {"id": "1806.09362", "submitter": "Virgilio Gomez-Rubio", "authors": "Elena L\\'azaro and Carmen Armero and Virgilio G\\'omez-Rubio", "title": "Approximate Bayesian inference for mixture cure models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cure models in survival analysis deal with populations in which a part of the\nindividuals cannot experience the event of interest. Mixture cure models\nconsider the target population as a mixture of susceptible and non-susceptible\nindividuals. The statistical analysis of these models focuses on examining the\nprobability of cure (incidence model) and inferring on the time-to-event in the\nsusceptible subpopulation (latency model).\n  Bayesian inference on mixture cure models has typically relied upon Markov\nchain Monte Carlo (MCMC) methods. The integrated nested Laplace approximation\n(INLA) is a recent and attractive approach for doing Bayesian inference. INLA\nin its natural definition cannot fit mixture models but recent research has new\nproposals that combine INLA and MCMC methods to extend its applicability to\nthem (Bivand et al., 2014, G\\'omez-Rubio et al., 2017, G\\'omez-Rubio and Rue,\n2018}.\n  This paper focuses on the implementation of INLA in mixture cure models. A\ngeneral mixture cure survival model with covariate information for the latency\nand the incidence model within a general scenario with censored and\nnon-censored information is discussed. The fact that non-censored individuals\nundoubtedly belong to the uncured population is a valuable information that was\nincorporated in the inferential process.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 10:20:51 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["L\u00e1zaro", "Elena", ""], ["Armero", "Carmen", ""], ["G\u00f3mez-Rubio", "Virgilio", ""]]}, {"id": "1806.09401", "submitter": "Shogo Nakakita", "authors": "Shogo H. Nakakita, Masayuki Uchida", "title": "Quasi-likelihood analysis of an ergodic diffusion plus noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider adaptive maximum-likelihood-type estimators and adaptive\nBayes-type ones for discretely observed ergodic diffusion processes with\nobservation noise whose variance is constant. The quasi-likelihood functions\nfor the diffusion and drift parameters are introduced and the polynomial-type\nlarge deviation inequalities for those quasi-likelihoods are shown to see the\nconvergence of moments for those estimators.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 12:00:56 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2018 15:44:52 GMT"}, {"version": "v3", "created": "Wed, 29 Aug 2018 17:27:30 GMT"}, {"version": "v4", "created": "Tue, 2 Apr 2019 02:25:02 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Nakakita", "Shogo H.", ""], ["Uchida", "Masayuki", ""]]}, {"id": "1806.09473", "submitter": "Henry Scharf", "authors": "Henry R. Scharf, Mevin B. Hooten, Ryan R. Wilson, George M. Durner,\n  Todd C. Atwood", "title": "Accounting for phenology in the analysis of animal movement", "comments": "Correction to caption of Figure 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of animal tracking data provides an important source of\nscientific understanding and discovery in ecology. Observations of animal\ntrajectories using telemetry devices provide researchers with information about\nthe way animals interact with their environment and each other. For many\nspecies, specific geographical features in the landscape can have a strong\neffect on behavior. Such features may correspond to a single point (e.g., dens\nor kill sites), or to higher-dimensional subspaces (e.g., rivers or lakes).\nFeatures may be relatively static in time (e.g., coastlines or home-range\ncenters), or may be dynamic (e.g., sea ice extent or areas of high-quality\nforage for herbivores). We introduce a novel model for animal movement that\nincorporates active selection for dynamic features in a landscape.\n  Our approach is motivated by the study of polar bear (Ursus maritimus)\nmovement. During the sea ice melt season, polar bears spend much of their time\non sea ice above shallow, biologically productive water where they hunt seals.\nThe changing distribution and characteristics of sea ice throughout the late\nspring through early fall means that the location of valuable habitat is\nconstantly shifting. We develop a model for the movement of polar bears that\naccounts for the effect of this important landscape feature. We introduce a\ntwo-stage procedure for approximate Bayesian inference that allows us to\nanalyze over 300,000 observed locations of 186 polar bears from 2012--2016. We\nuse our proposed model to answer a particular question posed by wildlife\nmanagers who seek to cluster polar bears from the Beaufort and Chukchi seas\ninto sub-populations.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 14:07:04 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 15:29:01 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 18:42:06 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Scharf", "Henry R.", ""], ["Hooten", "Mevin B.", ""], ["Wilson", "Ryan R.", ""], ["Durner", "George M.", ""], ["Atwood", "Todd C.", ""]]}, {"id": "1806.09529", "submitter": "Zhou Fan", "authors": "Zhou Fan and Iain M. Johnstone and Yi Sun", "title": "Spiked covariances and principal components analysis in high-dimensional\n  random effects models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study principal components analyses in multivariate random and mixed\neffects linear models, assuming a spherical-plus-spikes structure for the\ncovariance matrix of each random effect. We characterize the behavior of\noutlier sample eigenvalues and eigenvectors of MANOVA variance components\nestimators in such models under a high-dimensional asymptotic regime. Our\nresults show that an aliasing phenomenon may occur in high dimensions, in which\neigenvalues and eigenvectors of the MANOVA estimate for one variance component\nmay be influenced by the other components. We propose an alternative procedure\nfor estimating the true principal eigenvalues and eigenvectors that\nasymptotically corrects for this aliasing problem.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 15:35:10 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Fan", "Zhou", ""], ["Johnstone", "Iain M.", ""], ["Sun", "Yi", ""]]}, {"id": "1806.09611", "submitter": "Yijun Zuo", "authors": "Yijun Zuo", "title": "Robustness of deepest projection regression functional", "comments": "arXiv admin note: substantial text overlap with arXiv:1805.02046", "journal-ref": null, "doi": "10.1007/s00362-019-01129-4", "report-no": "26 pages, 3 figures", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth notions in regression have been systematically proposed and examined in\nZuo (2018). One of the prominent advantages of notion of depth is that it can\nbe directly utilized to introduce median-type deepest estimating functionals\n(or estimators in empirical distribution case) for location or regression\nparameters in a multi-dimensional setting.\n  Regression depth shares the advantage. Depth induced deepest estimating\nfunctionals are expected to inherit desirable and inherent robustness\nproperties ( e.g. bounded maximum bias and influence function and high\nbreakdown point) as their univariate location counterpart does. Investigating\nand verifying the robustness of the deepest projection estimating functional\n(in terms of maximum bias, asymptotic and finite sample breakdown point, and\ninfluence function) is the major goal of this article.\n  It turns out that the deepest projection estimating functional possesses a\nbounded influence function and the best possible asymptotic breakdown point as\nwell as the best finite sample breakdown point with robust choice of its\nunivariate regression and scale component.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 13:47:57 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 01:07:35 GMT"}, {"version": "v3", "created": "Tue, 18 Sep 2018 14:03:54 GMT"}, {"version": "v4", "created": "Fri, 2 Aug 2019 18:41:31 GMT"}, {"version": "v5", "created": "Mon, 12 Aug 2019 11:44:02 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Zuo", "Yijun", ""]]}, {"id": "1806.09734", "submitter": "Genevieve Robin", "authors": "Genevi\\`eve Robin, Olga Klopp, Julie Josse, \\'Eric Moulines and Robert\n  Tibshirani", "title": "Main effects and interactions in mixed and incomplete data frames", "comments": "25 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A mixed data frame (MDF) is a table collecting categorical, numerical and\ncount observations. The use of MDF is widespread in statistics and the\napplications are numerous from abundance data in ecology to recommender\nsystems. In many cases, an MDF exhibits simultaneously main effects, such as\nrow, column or group effects and interactions, for which a low-rank model has\noften been suggested. Although the literature on low-rank approximations is\nvery substantial, with few exceptions, existing methods do not allow to\nincorporate main effects and interactions while providing statistical\nguarantees. The present work fills this gap.\n  We propose an estimation method which allows to recover simultaneously the\nmain effects and the interactions. We show that our method is near optimal\nunder conditions which are met in our targeted applications. We also propose an\noptimization algorithm which provably converges to an optimal solution.\nNumerical experiments reveal that our method, mimi, performs well when the main\neffects are sparse and the interaction matrix has low-rank. We also show that\nmimi compares favorably to existing methods, in particular when the main\neffects are significantly large compared to the interactions, and when the\nproportion of missing entries is large. The method is available as an R package\non the Comprehensive R Archive Network.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 00:09:16 GMT"}, {"version": "v2", "created": "Tue, 26 Mar 2019 14:01:04 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Robin", "Genevi\u00e8ve", ""], ["Klopp", "Olga", ""], ["Josse", "Julie", ""], ["Moulines", "\u00c9ric", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1806.09762", "submitter": "Yichen Zhou", "authors": "Yichen Zhou, Giles Hooker", "title": "Boulevard: Regularized Stochastic Gradient Boosted Trees and Their\n  Limiting Distribution", "comments": "45 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper examines a novel gradient boosting framework for regression. We\nregularize gradient boosted trees by introducing subsampling and employ a\nmodified shrinkage algorithm so that at every boosting stage the estimate is\ngiven by an average of trees. The resulting algorithm, titled Boulevard, is\nshown to converge as the number of trees grows. We also demonstrate a central\nlimit theorem for this limit, allowing a characterization of uncertainty for\npredictions. A simulation study and real world examples provide support for\nboth the predictive accuracy of the model and its limiting behavior.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 02:22:13 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 15:11:58 GMT"}, {"version": "v3", "created": "Fri, 13 Sep 2019 04:11:02 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Zhou", "Yichen", ""], ["Hooker", "Giles", ""]]}, {"id": "1806.09949", "submitter": "Herv\\'e Cardot", "authors": "Herv\\'e Cardot and Anne De Moliner Anne and Camelia Goga", "title": "Conditional bias robust estimation of the total of curve data by\n  sampling in a finite population: an illustration on electricity load curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For marketing or power grid management purposes, many studies based on the\nanalysis of the total electricity consumption curves of groups of customers are\nnow carried out by electricity companies. Aggregated total or mean load curves\nare estimated using individual curves measured at fine time grid and collected\naccording to some sampling design. Due to the skewness of the distribution of\nelectricity consumptions, these samples often contain outlying curves which may\nhave an important impact on the usual estimation procedures. We introduce\nseveral robust estimators of the total consumption curve which are not\nsensitive to such outlying curves. These estimators are based on the\nconditional bias approach and robust functional methods. We also derive mean\nsquare error estimators of these robust estimators and finally, we evaluate and\ncompare the performance of the suggested estimators on Irish electricity data.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 12:49:52 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 11:08:14 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Cardot", "Herv\u00e9", ""], ["Anne", "Anne De Moliner", ""], ["Goga", "Camelia", ""]]}, {"id": "1806.10089", "submitter": "Guy Hawkins", "authors": "David Gunawan and Guy E. Hawkins and Minh-Ngoc Tran and Robert Kohn\n  and Scott Brown", "title": "New Estimation Approaches for the Hierarchical Linear Ballistic\n  Accumulator Model", "comments": "35 pages, 6 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Linear Ballistic Accumulator (Brown & Heathcote, 2008) model is used as a\nmeasurement tool to answer questions about applied psychology. The analyses\nbased on this model depend upon the model selected and its estimated\nparameters. Modern approaches use hierarchical Bayesian models and Markov chain\nMonte-Carlo (MCMC) methods to estimate the posterior distribution of the\nparameters. Although there are several approaches available for model\nselection, they are all based on the posterior samples produced via MCMC, which\nmeans that the model selection inference inherits the properties of the MCMC\nsampler. To improve on current approaches to LBA inference we propose two\nmethods that are based on recent advances in particle MCMC methodology; they\nare qualitatively different from existing approaches as well as from each\nother. The first approach is particle Metropolis-within-Gibbs; the second\napproach is density tempered sequential Monte Carlo. Both new approaches\nprovide very efficient sampling and can be applied to estimate the marginal\nlikelihood, which provides Bayes factors for model selection. The first\napproach is usually faster. The second approach provides a direct estimate of\nthe marginal likelihood, uses the first approach in its Markov move step and is\nvery efficient to parallelize on high performance computers. The new methods\nare illustrated by applying them to simulated and real data, and through pseudo\ncode. The code implementing the methods is freely available.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 16:07:37 GMT"}, {"version": "v2", "created": "Tue, 8 Oct 2019 06:31:16 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 23:44:47 GMT"}, {"version": "v4", "created": "Wed, 19 Feb 2020 05:21:44 GMT"}, {"version": "v5", "created": "Mon, 2 Mar 2020 06:16:03 GMT"}], "update_date": "2020-03-03", "authors_parsed": [["Gunawan", "David", ""], ["Hawkins", "Guy E.", ""], ["Tran", "Minh-Ngoc", ""], ["Kohn", "Robert", ""], ["Brown", "Scott", ""]]}, {"id": "1806.10093", "submitter": "Marie Perrot-Dockes", "authors": "Marie Perrot-Dock\\`es and C\\'eline L\\'evy-Leduc and Lo\\\"ic Rajjou", "title": "Estimation of large block structured covariance matrices: Application to\n  \"multi-omic\" approaches to study seed quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an application in high-throughput genomics and metabolomics, we\npropose a novel, efficient and fully data-driven approach for estimating large\nblock structured sparse covariance matrices in the case where the number of\nvariables is much larger than the number of samples without limiting ourselves\nto block diagonal matrices. Our approach consists in approximating such a\ncovariance matrix by the sum of a low-rank sparse matrix and a diagonal matrix.\nOur methodology also can deal with matrices for which the block structure\nappears only if the columns and rows are permuted according to an unknown\npermutation. Our technique is implemented in the R package \\texttt{BlockCov}\nwhich is available from the Comprehensive R Archive Network (CRAN) and from\nGitHub. In order to illustrate the statistical and numerical performance of our\npackage some numerical experiments are provided as well as a thorough\ncomparison with alternative methods. Finally, our approach is applied to the\nuse of \"multi-omic\" approaches for studying seed quality.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 16:18:24 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 16:57:34 GMT"}, {"version": "v3", "created": "Fri, 6 Dec 2019 18:08:16 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Perrot-Dock\u00e8s", "Marie", ""], ["L\u00e9vy-Leduc", "C\u00e9line", ""], ["Rajjou", "Lo\u00efc", ""]]}, {"id": "1806.10163", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban", "title": "FACT: Fast closed testing for exchangeable local tests", "comments": "This version fixes an indexing error in the calculation of adjusted\n  p-values, pointed out in arXiv:1912.06116. To appear in Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypothesis testing problems arise naturally in science. In this\npaper, we introduce the new Fast Closed Testing (FACT) method for multiple\ntesting, controlling the family-wise error rate. This error rate is state of\nthe art in many important application areas, and is preferred to false\ndiscovery rate control for many reasons, including that it leads to stronger\nreproducibility. The closure principle rejects an individual hypothesis if all\nglobal nulls of subsets containing it are rejected using some test statistics.\nIt takes exponential time in the worst case. When the tests are symmetric and\nmonotone, our method is an exact algorithm for computing the closure, quadratic\nin the number of tests, and linear in the number of discoveries. Our framework\ngeneralizes most examples of closed testing such as Holm's and the Bonferroni\nmethod. As a special case of our method, we propose the Simes-higher criticism\nfusion test, which is powerful for detecting both a few strong signals, and\nalso many moderate signals.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 18:35:08 GMT"}, {"version": "v2", "created": "Fri, 4 Oct 2019 15:19:46 GMT"}, {"version": "v3", "created": "Sat, 18 Jan 2020 00:38:00 GMT"}], "update_date": "2020-01-22", "authors_parsed": [["Dobriban", "Edgar", ""]]}, {"id": "1806.10234", "submitter": "Jonathan Huggins", "authors": "Jonathan H. Huggins, Trevor Campbell, Miko{\\l}aj Kasprzak, Tamara\n  Broderick", "title": "Scalable Gaussian Process Inference with Finite-data Mean and Variance\n  Guarantees", "comments": "20 pages, 7 figures, 1 table, including Appendix. Code available at\n  https://github.com/trevorcampbell/fishergp", "journal-ref": "Proceedings of the 22nd International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2019, Naha, Okinawa, Japan. PMLR:\n  Volume 89", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GPs) offer a flexible class of priors for nonparametric\nBayesian regression, but popular GP posterior inference methods are typically\nprohibitively slow or lack desirable finite-data guarantees on quality. We\ndevelop an approach to scalable approximate GP regression with finite-data\nguarantees on the accuracy of pointwise posterior mean and variance estimates.\nOur main contribution is a novel objective for approximate inference in the\nnonparametric setting: the preconditioned Fisher (pF) divergence. We show that\nunlike the Kullback--Leibler divergence (used in variational inference), the pF\ndivergence bounds the 2-Wasserstein distance, which in turn provides tight\nbounds the pointwise difference of the mean and variance functions. We\ndemonstrate that, for sparse GP likelihood approximations, we can minimize the\npF divergence efficiently. Our experiments show that optimizing the pF\ndivergence has the same computational requirements as variational sparse GPs\nwhile providing comparable empirical performance--in addition to our novel\nfinite-data quality guarantees.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 22:42:15 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2018 16:49:53 GMT"}, {"version": "v3", "created": "Sat, 2 Mar 2019 21:24:19 GMT"}, {"version": "v4", "created": "Wed, 27 Mar 2019 13:50:14 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Huggins", "Jonathan H.", ""], ["Campbell", "Trevor", ""], ["Kasprzak", "Miko\u0142aj", ""], ["Broderick", "Tamara", ""]]}, {"id": "1806.10403", "submitter": "Laura Anderlucci", "authors": "Christian Hennig, Cinzia Viroli and Laura Anderlucci", "title": "Quantile-based clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new cluster analysis method, $K$-quantiles clustering, is introduced.\n$K$-quantiles clustering can be computed by a simple greedy algorithm in the\nstyle of the classical Lloyd's algorithm for $K$-means. It can be applied to\nlarge and high-dimensional datasets. It allows for within-cluster skewness and\ninternal variable scaling based on within-cluster variation. Different versions\nallow for different levels of parsimony and computational efficiency. Although\n$K$-quantiles clustering is conceived as nonparametric, it can be connected to\na fixed partition model of generalized asymmetric Laplace-distributions. The\nconsistency of $K$-quantiles clustering is proved, and it is shown that\n$K$-quantiles clusters correspond to well separated mixture components in a\nnonparametric mixture. In a simulation, $K$-quantiles clustering is compared\nwith a number of popular clustering methods with good results. A\nhigh-dimensional microarray dataset is clustered by $K$-quantiles.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 10:46:29 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2019 19:06:16 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Hennig", "Christian", ""], ["Viroli", "Cinzia", ""], ["Anderlucci", "Laura", ""]]}, {"id": "1806.10483", "submitter": "Arthur Berg", "authors": "J G Liao, Arthur Berg, Timothy L McMurry", "title": "A robustified posterior for Bayesian inference on a large number of\n  parallel effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern experiments, such as microarray gene expression and genome-wide\nassociation studies, present the problem of estimating a large number of\nparallel effects. Bayesian inference is a popular approach for analyzing such\ndata by modeling the large number of unknown parameters as random effects from\na common prior distribution. However, misspecification of the prior\ndistribution can lead to erroneous estimates of the random effects, especially\nfor the largest and most interesting effects. This paper has two aims. First,\nwe propose a robustified posterior distribution for a parametric Bayesian\nhierarchical model that can substantially reduce the impact of a misspecified\nprior. Second, we conduct a systematic comparison of the standard parametric\nposterior, the proposed robustified parametric posterior, and a nonparametric\nBayesian posterior which uses a Dirichlet process mixture prior. The proposed\nrobustifed posterior when combined with a flexible parametric prior can be a\nsuperior alternative to nonparametric Bayesian methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 13:46:13 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 15:04:00 GMT"}, {"version": "v3", "created": "Thu, 25 Oct 2018 17:49:32 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Liao", "J G", ""], ["Berg", "Arthur", ""], ["McMurry", "Timothy L", ""]]}, {"id": "1806.10495", "submitter": "Maarten van Smeden", "authors": "Kim Luijken, Rolf H.H. Groenwold, Ben van Calster, Ewout W.\n  Steyerberg, Maarten van Smeden", "title": "Impact of predictor measurement heterogeneity across settings on\n  performance of prediction models: a measurement error perspective", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is widely acknowledged that the predictive performance of clinical\nprediction models should be studied in patients that were not part of the data\nin which the model was derived. Out-of-sample performance can be hampered when\npredictors are measured differently at derivation and external validation. This\nmay occur, for instance, when predictors are measured using different\nmeasurement protocols or when tests are produced by different manufacturers.\nAlthough such heterogeneity in predictor measurement between deriviation and\nvalidation data is common, the impact on the out-of-sample performance is not\nwell studied. Using analytical and simulation approaches, we examined\nout-of-sample performance of prediction models under various scenarios of\nheterogeneous predictor measurement. These scenarios were defined and clarified\nusing an established taxonomy of measurement error models. The results of our\nsimulations indicate that predictor measurement heterogeneity can induce\nmiscalibration of prediction and affects discrimination and overall predictive\naccuracy, to extents that the prediction model may no longer be considered\nclinically useful. The measurement error taxonomy was found to be helpful in\nidentifying and predicting effects of heterogeneous predictor measurements\nbetween settings of prediction model derivation and validation. Our work\nindicates that homogeneity of measurement strategies across settings is of\nparamount importance in prediction research.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 14:18:00 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 13:27:54 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2019 09:22:58 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Luijken", "Kim", ""], ["Groenwold", "Rolf H. H.", ""], ["van Calster", "Ben", ""], ["Steyerberg", "Ewout W.", ""], ["van Smeden", "Maarten", ""]]}, {"id": "1806.10582", "submitter": "Andrey Gorshenin", "authors": "Andrey K. Gorshenin and Victor Yu. Korolev", "title": "A functional approach to estimation of the parameters of generalized\n  negative binomial and gamma distributions", "comments": "13 pages, 6 figures, The XXI International Conference on Distributed\n  Computer and Communication Networks: Control, Computation, Communications\n  (DCCN 2018)", "journal-ref": "Communications in Computer and Information Science, 2018. Vol.\n  919. P. 353-364", "doi": "10.1007/978-3-319-99447-5_30", "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalized negative binomial distribution (GNB) is a new flexible family\nof discrete distributions that are mixed Poisson laws with the mixing\ngeneralized gamma (GG) distributions. This family of discrete distributions is\nvery wide and embraces Poisson distributions, negative binomial distributions,\nSichel distributions, Weibull--Poisson distributions and many other types of\ndistributions supplying descriptive statistics with many flexible models. These\ndistributions seem to be very promising for the statistical description of many\nreal phenomena. GG distributions are widely applied in signal and image\nprocessing and other practical problems. The statistical estimation of the\nparameters of GNB and GG distributions is quite complicated. To find estimates,\nthe methods of moments or maximum likelihood can be used as well as two-stage\ngrid EM-algorithms. The paper presents a methodology based on the search for\nthe best distribution using the minimization of $\\ell^p$-distances and\n$L^p$-metrics for GNB and GG distributions, respectively. This approach, first,\nallows to obtain parameter estimates without using grid methods and solving\nsystems of nonlinear equations and, second, yields not point estimates as the\nmethods of moments or maximum likelihood do, but the estimate for the density\nfunction. In other words, within this approach the set of decisions is not a\nEuclidean space, but a functional space.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 17:27:02 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Gorshenin", "Andrey K.", ""], ["Korolev", "Victor Yu.", ""]]}, {"id": "1806.10706", "submitter": "Nakahiro Yoshida", "authors": "Haruhiko Inatsugu and Nakahiro Yoshida", "title": "Global jump filters and quasi-likelihood analysis for volatility", "comments": "A condition has been relaxed", "journal-ref": "Annals of the Institute of Statistical Mathematics, on-line (2021)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new estimation scheme for estimation of the volatility\nparameters of a semimartingale with jumps based on a jump-detection filter. Our\nfilter uses all of data to analyze the relative size of increments and to\ndiscriminate jumps more precisely. We construct quasi-maximum likelihood\nestimators and quasi-Bayesian estimators, and show limit theorems for them\nincluding $L^p$-estimates of the error and asymptotic mixed normality based on\nthe framework of the quasi-likelihood analysis. The global jump filters do not\nneed a restrictive condition for the distribution of the small jumps. By\nnumerical simulation we show that our \"global\" method obtains better estimates\nof the volatility parameter than the previous \"local\" methods.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 22:39:10 GMT"}, {"version": "v2", "created": "Sun, 20 Sep 2020 06:39:50 GMT"}, {"version": "v3", "created": "Sun, 14 Feb 2021 19:16:15 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Inatsugu", "Haruhiko", ""], ["Yoshida", "Nakahiro", ""]]}, {"id": "1806.10761", "submitter": "Benjamin Peherstorfer", "authors": "Benjamin Peherstorfer, Karen Willcox, Max Gunzburger", "title": "Survey of multifidelity methods in uncertainty propagation, inference,\n  and optimization", "comments": "will appear in SIAM Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many situations across computational science and engineering, multiple\ncomputational models are available that describe a system of interest. These\ndifferent models have varying evaluation costs and varying fidelities.\nTypically, a computationally expensive high-fidelity model describes the system\nwith the accuracy required by the current application at hand, while\nlower-fidelity models are less accurate but computationally cheaper than the\nhigh-fidelity model. Outer-loop applications, such as optimization, inference,\nand uncertainty quantification, require multiple model evaluations at many\ndifferent inputs, which often leads to computational demands that exceed\navailable resources if only the high-fidelity model is used. This work surveys\nmultifidelity methods that accelerate the solution of outer-loop applications\nby combining high-fidelity and low-fidelity model evaluations, where the\nlow-fidelity evaluations arise from an explicit low-fidelity model (e.g., a\nsimplified physics approximation, a reduced model, a data-fit surrogate, etc.)\nthat approximates the same output quantity as the high-fidelity model. The\noverall premise of these multifidelity methods is that low-fidelity models are\nleveraged for speedup while the high-fidelity model is kept in the loop to\nestablish accuracy and/or convergence guarantees. We categorize multifidelity\nmethods according to three classes of strategies: adaptation, fusion, and\nfiltering. The paper reviews multifidelity methods in the outer-loop contexts\nof uncertainty propagation, inference, and optimization.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 04:06:17 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Peherstorfer", "Benjamin", ""], ["Willcox", "Karen", ""], ["Gunzburger", "Max", ""]]}, {"id": "1806.10947", "submitter": "Michail Tsagris", "authors": "Christina Chatzipantsiou, Marios Dimitriadis, Manos Papadakis and\n  Michail Tsagris", "title": "Extremely efficient permutation and bootstrap hypothesis tests using R", "comments": "Theis is a pre-print of the paper that was accepted in the Journal of\n  Modern Applied Statistical Methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Re-sampling based statistical tests are known to be computationally heavy,\nbut reliable when small sample sizes are available. Despite their nice\ntheoretical properties not much effort has been put to make them efficient. In\nthis paper we treat the case of Pearson correlation coefficient and two\nindependent samples t-test. We propose a highly computationally efficient\nmethod for calculating permutation based p-values in these two cases. The\nmethod is general and can be applied or be adopted to other similar two sample\nmean or two mean vectors cases.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 13:20:39 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Chatzipantsiou", "Christina", ""], ["Dimitriadis", "Marios", ""], ["Papadakis", "Manos", ""], ["Tsagris", "Michail", ""]]}, {"id": "1806.11032", "submitter": "Antonio El\\'ias Fern\\'andez", "authors": "Antonio El\\'ias and Ra\\'ul Jim\\'enez", "title": "A depth-based method for functional time series forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach is presented for making predictions about functional time series.\nThe method is applied to data coming from periodically correlated processes and\nelectricity demand, obtaining accurate point forecasts and narrow prediction\nbands that cover high proportions of the forecasted functional datum, for a\ngiven confidence level. The method is computationally efficient and\nsubstantially different to other functional time series methods, offering a new\ninsight for the analysis of these data structures.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 15:22:43 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["El\u00edas", "Antonio", ""], ["Jim\u00e9nez", "Ra\u00fal", ""]]}, {"id": "1806.11083", "submitter": "Jonas Krampe", "authors": "J. Krampe, J-P. Kreiss and E. Paparoditis", "title": "Bootstrap Based Inference for Sparse High-Dimensional Time Series Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitting sparse models to high-dimensional time series is an important area of\nstatistical inference. In this paper we consider sparse vector autoregressive\nmodels and develop appropriate bootstrap methods to infer properties of such\nprocesses. Our bootstrap methodology generates pseudo time series using a\nmodel-based bootstrap procedure which involves an estimated, sparsified version\nof the underlying vector autoregressive model. Inference is performed using\nso-called de-sparsified or de-biased estimators of the autoregressive model\nparameters. We derive the asymptotic distribution of such estimators in the\ntime series context and establish asymptotic validity of the bootstrap\nprocedure proposed for estimation and, appropriately modified, for testing\npurposes. In particular we focus on testing that large groups of autoregressive\ncoefficients equal zero. Our theoretical results are complemented by\nsimulations which investigate the finite sample performance of the bootstrap\nmethodology proposed. A real-life data application is also presented.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 16:57:55 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 12:51:19 GMT"}, {"version": "v3", "created": "Tue, 24 Sep 2019 13:18:27 GMT"}], "update_date": "2019-09-25", "authors_parsed": [["Krampe", "J.", ""], ["Kreiss", "J-P.", ""], ["Paparoditis", "E.", ""]]}, {"id": "1806.11153", "submitter": "Noemi Kreif", "authors": "No\\'emi Kreif, Oleg Sofrygin, Julie Schmittdiel, Alyce Adams, Richard\n  Grant, Zheng Zhu, Mark van der Laan, Romain Neugebauer", "title": "Evaluation of adaptive treatment strategies in an observational study\n  where time-varying covariates are not monitored systematically", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In studies based on electronic health records (EHR), the frequency of\ncovariate monitoring can vary by covariate type, across patients, and over\ntime. This can lead to major challenges: first, the difference in monitoring\nprotocols may invalidate the extrapolation of study results obtained in one\npopulation to the other, and second, monitoring can act as a time-varying\nconfounder of the causal effect of a time-varying treatment on the outcomes of\ninterest. This paper demonstrates how to account for non-systematic covariate\nmonitoring when evaluating dynamic treatment interventions, and how to evaluate\njoint dynamic treatment-censoring and static monitoring interventions, in a\nreal world, EHR-based, comparative effectiveness research (CER) study of\npatients with type II diabetes mellitus. First, we show that the effects of\ndynamic treatment-censoring regimes can be identified by including indicators\nof monitoring events in the adjustment set. Second, we demonstrate the poor\nperformance of the standard inverse probability weighting (IPW) estimator of\nthe effects of joint treatment-censoring-monitoring interventions, due to a\nlarge decrease in data support resulting in a large increase in standard errors\nand concerns over finite-sample bias from near-violations of the positivity\nassumption for the monitoring process. Finally, we detail an alternate IPW\nestimator of the effects of these interventions using the No Direct Effect\nassumption. We demonstrate that this estimator can result in improved\nefficiency but at the cost of increased bias concerns over structural\nnear-violations of the positivity assumption for the treatment process. To\nconclude, this paper develops and illustrates new tools that researchers can\nexploit to appropriately account for non-systematic covariate monitoring in\nCER, and to ask new causal questions about the joint effects of treatment and\nmonitoring interventions.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 19:35:22 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Kreif", "No\u00e9mi", ""], ["Sofrygin", "Oleg", ""], ["Schmittdiel", "Julie", ""], ["Adams", "Alyce", ""], ["Grant", "Richard", ""], ["Zhu", "Zheng", ""], ["van der Laan", "Mark", ""], ["Neugebauer", "Romain", ""]]}, {"id": "1806.11219", "submitter": "David Choi", "authors": "David Choi", "title": "Using Exposure Mappings as Side Information in Experiments with\n  Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exposure mappings are widely used to model potential outcomes in the presence\nof interference, where each unit's outcome may depend not only on its own\ntreatment, but also on the treatment of other units as well. However, in\npractice these models may be only a crude proxy for social dynamics. In this\nwork, we give estimands and estimators that are robust to the misspecification\nof an exposure model. In the first part, we require the treatment effect to be\nnonnegative (or \"monotone\") in both direct effects and spillovers. In the\nsecond part, we consider a weaker estimand (\"contrasts attributable to\ntreatment\") which makes no restrictions on the interference at all.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 22:24:10 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Choi", "David", ""]]}, {"id": "1806.11220", "submitter": "Sixing Chen", "authors": "Sixing Chen, Jukka-Pekka Onnela", "title": "A Bootstrap Method for Goodness of Fit and Model Selection with a Single\n  Observed Network", "comments": "21 pages, 6 figures, presented at Netsci 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network models are applied in numerous domains where data can be represented\nas a system of interactions among pairs of actors. While both statistical and\nmechanistic network models are increasingly capable of capturing various\ndependencies amongst these actors, these dependencies imply the lack of\nindependence. This poses statistical challenges for analyzing such data,\nespecially when there is only a single observed network, and often leads to\nintractable likelihoods regardless of the modeling paradigm, which limit the\napplication of existing statistical methods for networks. We explore a\nsubsampling bootstrap procedure to serve as the basis for goodness of fit and\nmodel selection with a single observed network that circumvents the\nintractability of such likelihoods. Our approach is based on flexible\nresampling distributions formed from the single observed network, allowing for\nfiner and higher dimensional comparisons than simply point estimates of\nquantities of interest. We include worked examples for model selection, with\nsimulation, and assessment of goodness of fit, with duplication-divergence\nmodel fits for yeast (S.cerevisiae) protein-protein interaction data from the\nliterature. The proposed procedure produces a flexible resampling distribution\nthat can be based on any statistics of one's choosing and can be employed\nregardless of choice of model.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 22:24:42 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Chen", "Sixing", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1806.11229", "submitter": "Bonifride Tuyishimire", "authors": "Bonifride Tuyishimire, Brent R Logan and Purushottam W Laud", "title": "Additivity Assessment in Nonparametric Models Using Ratio of Pseudo\n  Marginal Likelihoods", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric regression models such as Bayesian Additive Regression Trees\n(BART) can be useful in fitting flexible functions of a set of covariates to a\nresponse, while accounting for nonlinearities and interactions. However, they\nare often cumbersome to interpret. Breaking down the function into additive\ncomponents, if appropriate, could simplify the interpretation and improve the\nutility of the model. On the other hand, establishing nonadditivity can be\nuseful in determining the need for individualized predictions and treatment\nselection. Testing additivity of single covariates in nonparametric regression\nmodels has been extensively studied. However, additivity assessment of\nnonparametric functions of disjoint sets of variables has not received as much\nattention. We propose a method for detection of nonadditivity of two disjoint\nsets of variables by fitting the sum of two BART models, each using its own set\nof variables. We then compare the pseudo marginal likelihood (PsML) of this\nsum-of- BARTs model vs. a single-BART model with all the variables together, in\na ratio known as Pseudo Bayes Factor (PsBF). A special case of our method\nchecks additivity between one variable of interest and another set of\nvariables, where the additive model allows for direct interpretation of the\nvariable of interest while adjusting for the remaining variables in a flexible,\nnonparametric manner. We extended the above approaches to allow a binary\nresponse using a logit link. We also propose a systematic way to design\nsimulations that are used in additivity assessment. In simulation studies, PsBF\nshowed better performance compared to out-of-sample prediction error in\nchoosing the correct model, while avoiding computationally expensive\ncross-validation and providing an interpretable criterion for model selection.\nWe applied our approach to two different examples with a continuous and binary\noutcomes.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 23:20:18 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Tuyishimire", "Bonifride", ""], ["Logan", "Brent R", ""], ["Laud", "Purushottam W", ""]]}, {"id": "1806.11237", "submitter": "Rodney Sparapani", "authors": "Rodney Sparapani, Brent R. Logan, Robert E. McCulloch and Purushottam\n  W. Laud", "title": "Nonparametric competing risks analysis using Bayesian Additive\n  Regression Trees (BART)", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many time-to-event studies are complicated by the presence of competing\nrisks. Such data are often analyzed using Cox models for the cause specific\nhazard function or Fine-Gray models for the subdistribution hazard. In practice\nregression relationships in competing risks data with either strategy are often\ncomplex and may include nonlinear functions of covariates, interactions,\nhigh-dimensional parameter spaces and nonproportional cause specific or\nsubdistribution hazards. Model misspecification can lead to poor predictive\nperformance. To address these issues, we propose a novel approach to flexible\nprediction modeling of competing risks data using Bayesian Additive Regression\nTrees (BART). We study the simulation performance in two-sample scenarios as\nwell as a complex regression setting, and benchmark its performance against\nstandard regression techniques as well as random survival forests. We\nillustrate the use of the proposed method on a recently published study of\npatients undergoing hematopoietic stem cell transplantation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 00:49:03 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Sparapani", "Rodney", ""], ["Logan", "Brent R.", ""], ["McCulloch", "Robert E.", ""], ["Laud", "Purushottam W.", ""]]}, {"id": "1806.11294", "submitter": "Jose Jimenez", "authors": "Jose L Jimenez, Viktoriya Stalbovskaya, Byron Jones", "title": "Properties of the weighted log-rank test in the design of confirmatory\n  studies with delayed effects", "comments": null, "journal-ref": null, "doi": "10.1002/pst.1923", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proportional hazards are a common assumption when designing confirmatory\nclinical trials in oncology. This assumption not only affects the analysis part\nbut also the sample size calculation. The presence of delayed effects causes a\nchange in the hazard ratio while the trial is ongoing since at the beginning we\ndo not observe any difference between treatment arms and after some unknown\ntime point, the differences between treatment arms will start to appear. Hence,\nthe proportional hazards assumption no longer holds and both sample size\ncalculation and analysis methods to be used should be reconsidered. The\nweighted log-rank test allows a weighting for early, middle and late\ndifferences through the Fleming and Harrington class of weights, and is proven\nto be more efficient when the proportional hazards assumption does not hold.\nThe Fleming and Harrington class of weights, along with the estimated delay,\ncan be incorporated into the sample size calculation in order to maintain the\ndesired power once the treatment arm differences start to appear. In this\narticle, we explore the impact of delayed effects in group sequential and\nadaptive group sequential designs, and make an empirical evaluation in terms of\npower and type-I error rate of the of the weighted log-rank test in a simulated\nscenario with fixed values of the Fleming and Harrington class of weights. We\nalso give some practical recommendations regarding which methodology should be\nused in the presence of delayed effects depending on certain characteristics of\nthe trial.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 08:14:53 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 12:03:54 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Jimenez", "Jose L", ""], ["Stalbovskaya", "Viktoriya", ""], ["Jones", "Byron", ""]]}, {"id": "1806.11392", "submitter": "Richard Boys", "authors": "Stephen R. Johnson and Daniel A. Henderson and Richard J. Boys", "title": "Revealing subgroup structure in ranked data using a Bayesian WAND", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ranked data arise in many areas of application ranging from the ranking of\nup-regulated genes for cancer to the ranking of academic statistics journals.\nComplications can arise when rankers do not report a full ranking of all\nentities; for example, they might only report their top--$M$ ranked entities\nafter seeing some or all entities. It can also be useful to know whether\nrankers are equally informative, and whether some entities are effectively\njudged to be exchangeable. When there is important subgroup structure in the\ndata, summaries such as aggregate (overall) rankings can be misleading. In this\npaper we propose a flexible Bayesian nonparametric model for identifying\nheterogeneous structure and ranker reliability in ranked data. The model is a\nWeighted Adapted Nested Dirichlet (WAND) process mixture of Plackett-Luce\nmodels and inference proceeds through a simple and efficient Gibbs sampling\nscheme for posterior sampling. The richness of information in the posterior\ndistribution allows us to infer many details of the structure both between\nranker groups and between entity groups (within ranker groups), in contrast to\nmany other (Bayesian) analyses. We also examine how posterior predictive checks\ncan be used to identify lack of model fit. The methodology is illustrated using\nseveral simulation studies and real data examples.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 13:03:52 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 15:22:21 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Johnson", "Stephen R.", ""], ["Henderson", "Daniel A.", ""], ["Boys", "Richard J.", ""]]}, {"id": "1806.11544", "submitter": "Simon Lyddon", "authors": "S. P. Lyddon, S. G. Walker, C. C. Holmes", "title": "Nonparametric learning from Bayesian models with randomized objective\n  functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian learning is built on an assumption that the model space contains a\ntrue reflection of the data generating mechanism. This assumption is\nproblematic, particularly in complex data environments. Here we present a\nBayesian nonparametric approach to learning that makes use of statistical\nmodels, but does not assume that the model is true. Our approach has provably\nbetter properties than using a parametric model and admits a Monte Carlo\nsampling scheme that can afford massive scalability on modern computer\narchitectures. The model-based aspect of learning is particularly attractive\nfor regularizing nonparametric inference when the sample size is small, and\nalso for correcting approximate approaches such as variational Bayes (VB). We\ndemonstrate the approach on a number of examples including VB classifiers and\nBayesian random forests.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 17:18:28 GMT"}, {"version": "v2", "created": "Thu, 1 Nov 2018 22:03:43 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Lyddon", "S. P.", ""], ["Walker", "S. G.", ""], ["Holmes", "C. C.", ""]]}]