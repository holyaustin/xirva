[{"id": "1410.0163", "submitter": "Guido W. Imbens", "authors": "Guido W. Imbens", "title": "Instrumental Variables: An Econometrician's Perspective", "comments": "Published in at http://dx.doi.org/10.1214/14-STS480 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 3, 323-358", "doi": "10.1214/14-STS480", "report-no": "IMS-STS-STS480", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I review recent work in the statistics literature on instrumental variables\nmethods from an econometrics perspective. I discuss some of the older,\neconomic, applications including supply and demand models and relate them to\nthe recent applications in settings of randomized experiments with\nnoncompliance. I discuss the assumptions underlying instrumental variables\nmethods and in what settings these may be plausible. By providing context to\nthe current applications, a better understanding of the applicability of these\nmethods may arise.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 09:40:04 GMT"}], "update_date": "2014-10-02", "authors_parsed": [["Imbens", "Guido W.", ""]]}, {"id": "1410.0247", "submitter": "Johannes Lederer", "authors": "Micha\\\"el Chichignoud, Johannes Lederer, Martin Wainwright", "title": "A Practical Scheme and Fast Algorithm to Tune the Lasso With Optimality\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel scheme for choosing the regularization parameter in\nhigh-dimensional linear regression with Lasso. This scheme, inspired by\nLepski's method for bandwidth selection in non-parametric regression, is\nequipped with both optimal finite-sample guarantees and a fast algorithm. In\nparticular, for any design matrix such that the Lasso has low sup-norm error\nunder an \"oracle choice\" of the regularization parameter, we show that our\nmethod matches the oracle performance up to a small constant factor, and show\nthat it can be implemented by performing simple tests along a single Lasso\npath. By applying the Lasso to simulated and real data, we find that our novel\nscheme can be faster and more accurate than standard schemes such as\nCross-Validation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 14:55:34 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 17:25:15 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Chichignoud", "Micha\u00ebl", ""], ["Lederer", "Johannes", ""], ["Wainwright", "Martin", ""]]}, {"id": "1410.0386", "submitter": "Konstantinos Spiliopoulos", "authors": "Konstantinos Spiliopoulos", "title": "Rare event simulation for multiscale diffusions in random environments", "comments": "Final version, paper to appear in SIAM Journal Multiscale Modelling\n  and Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider systems of stochastic differential equations with multiple scales\nand small noise and assume that the coefficients of the equations are ergodic\nand stationary random fields. Our goal is to construct provably-efficient\nimportance sampling Monte Carlo methods that allow efficient computation of\nrare event probabilities or expectations of functionals that can be associated\nwith rare events. Standard Monte Carlo algorithms perform poorly in the small\nnoise limit and hence fast simulations algorithms become relevant. The presence\nof multiple scales complicates the design and the analysis of efficient\nimportance sampling schemes. An additional complication is the randomness of\nthe environment. We construct explicit changes of measures that are proven to\nbe logarithmic asymptotically efficient with probability one with respect to\nthe random environment (i.e., in the quenched sense). Numerical simulations\nsupport the theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 21:19:05 GMT"}, {"version": "v2", "created": "Mon, 23 Feb 2015 18:08:20 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2015 18:22:18 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1410.0403", "submitter": "Thomas Muehlenstaedt", "authors": "Thomas Muehlenstaedt, Jana Fruth, Olivier Roustant", "title": "Computer experiments with functional inputs and scalar outputs by a\n  norm-based approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework for designing and analyzing computer experiments is presented,\nwhich is constructed for dealing with functional and real number inputs and\nreal number outputs. For designing experiments with both functional and real\nnumber inputs a two stage approach is suggested. The first stage consists of\nconstructing a candidate set for each functional input and during the second\nstage an optimal combination of the found candidate sets and a Latin hypercube\nfor the real number inputs is searched for. The resulting designs can be\nconsidered to be generalizations of Latin hypercubes. GP models are explored as\nmetamodel. The functional inputs are incorporated into the kriging model by\napplying norms in order to define distances between two functional inputs. In\norder to make the calculation of these norms computationally feasible, the use\nof B-splines is promoted.\n", "versions": [{"version": "v1", "created": "Wed, 1 Oct 2014 22:37:08 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Muehlenstaedt", "Thomas", ""], ["Fruth", "Jana", ""], ["Roustant", "Olivier", ""]]}, {"id": "1410.0438", "submitter": "Jared Murray", "authors": "Jared S. Murray, Jerome P. Reiter", "title": "Multiple Imputation of Missing Categorical and Continuous Values via\n  Bayesian Mixture Models with Local Dependence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a nonparametric Bayesian joint model for multivariate continuous\nand categorical variables, with the intention of developing a flexible engine\nfor multiple imputation of missing values. The model fuses Dirichlet process\nmixtures of multinomial distributions for categorical variables with Dirichlet\nprocess mixtures of multivariate normal distributions for continuous variables.\nWe incorporate dependence between the continuous and categorical variables by\n(i) modeling the means of the normal distributions as component-specific\nfunctions of the categorical variables and (ii) forming distinct mixture\ncomponents for the categorical and continuous data with probabilities that are\nlinked via a hierarchical model. This structure allows the model to capture\ncomplex dependencies between the categorical and continuous data with minimal\ntuning by the analyst. We apply the model to impute missing values due to item\nnonresponse in an evaluation of the redesign of the Survey of Income and\nProgram Participation (SIPP). The goal is to compare estimates from a field\ntest with the new design to estimates from selected individuals from a panel\ncollected under the old design. We show that accounting for the missing data\nchanges some conclusions about the comparability of the distributions in the\ntwo datasets. We also perform an extensive repeated sampling simulation using\nsimilar data from complete cases in an existing SIPP panel, comparing our\nproposed model to a default application of multiple imputation by chained\nequations. Imputations based on the proposed model tend to have better repeated\nsampling properties than the default application of chained equations in this\nrealistic setting.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 02:19:41 GMT"}, {"version": "v2", "created": "Tue, 13 Oct 2015 00:01:21 GMT"}], "update_date": "2015-10-14", "authors_parsed": [["Murray", "Jared S.", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1410.0463", "submitter": "Toru Kitagawa", "authors": "Toru Kitagawa", "title": "Instrumental Variables Before and LATEr", "comments": "Published in at http://dx.doi.org/10.1214/14-STS494 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 3, 359-362", "doi": "10.1214/14-STS494", "report-no": "IMS-STS-STS494", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modern formulation of the instrumental variable methods initiated the\nvaluable interactions between economics and statistics literatures of causal\ninference and fueled new innovations of the idea. It helped resolving the\nlong-standing confusion that the statisticians used to have on the method, and\nencouraged the economists to rethink how to make use of instrumental variables\nin policy analysis. [arXiv:1410.0163]\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 07:33:58 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Kitagawa", "Toru", ""]]}, {"id": "1410.0470", "submitter": "Thomas S. Richardson", "authors": "Thomas S. Richardson, James M. Robins", "title": "ACE Bounds; SEMs with Equilibrium Conditions", "comments": "Published in at http://dx.doi.org/10.1214/14-STS485 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 3, 363-366", "doi": "10.1214/14-STS485", "report-no": "IMS-STS-STS485", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Instrumental Variables: An Econometrician's Perspective\" by\nGuido W. Imbens [arXiv:1410.0163].\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 08:04:47 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Richardson", "Thomas S.", ""], ["Robins", "James M.", ""]]}, {"id": "1410.0473", "submitter": "Ilya Shpitser", "authors": "Ilya Shpitser", "title": "Causal Graphs: Addressing the Confounding Problem Without Instruments or\n  Ignorability", "comments": "Published in at http://dx.doi.org/10.1214/14-STS488 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 3, 367-370", "doi": "10.1214/14-STS488", "report-no": "IMS-STS-STS488", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Instrumental Variables: An Econometrician's Perspective\" by\nGuido W. Imbens [arXiv:1410.0163].\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 08:12:06 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Shpitser", "Ilya", ""]]}, {"id": "1410.0477", "submitter": "Sonja A. Swanson", "authors": "Sonja A. Swanson, Miguel A. Hern\\'an", "title": "Think Globally, Act Globally: An Epidemiologist's Perspective on\n  Instrumental Variable Estimation", "comments": "Published in at http://dx.doi.org/10.1214/14-STS491 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 3, 371-374", "doi": "10.1214/14-STS491", "report-no": "IMS-STS-STS491", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discussion of \"Instrumental Variables: An Econometrician's Perspective\" by\nGuido W. Imbens [arXiv:1410.0163].\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 08:25:30 GMT"}], "update_date": "2016-08-10", "authors_parsed": [["Swanson", "Sonja A.", ""], ["Hern\u00e1n", "Miguel A.", ""]]}, {"id": "1410.0482", "submitter": "Guido Imbens", "authors": "Guido Imbens", "title": "Rejoinder of \"Instrumental Variables: An Econometrician's Perspective\"", "comments": "Published in at http://dx.doi.org/10.1214/14-STS496 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 3, 375-379", "doi": "10.1214/14-STS496", "report-no": "IMS-STS-STS496", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rejoinder of \"Instrumental Variables: An Econometrician's Perspective\" by\nGuido W. Imbens [arXiv:1410.0163].\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 08:35:25 GMT"}], "update_date": "2014-10-03", "authors_parsed": [["Imbens", "Guido", ""]]}, {"id": "1410.0580", "submitter": "Monia Lupparelli", "authors": "Monia Lupparelli and Alberto Roverato", "title": "Log-mean linear regression models for binary responses with an\n  application to multimorbidity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In regression models for categorical data a linear model is typically related\nto the response variables via a transformation of probabilities called the link\nfunction. We introduce an approach based on two link functions for binary data\nnamed log-mean (LM) and log-mean linear (LML), respectively. The choice of the\nlink function plays a key role for the interpretation of the model, and our\napproach is especially appealing in terms of interpretation of the effects of\ncovariates on the association of responses. Similarly to Poisson regression,\nthe LM and LML regression coefficients of single outcomes are log-relative\nrisks, and we show that the relative risk interpretation is maintained also in\nthe regressions of the association of responses. Furthermore, certain\ncollections of zero LML regression coefficients imply that the relative risks\nfor joint responses factorize with respect to the corresponding relative risks\nfor marginal responses. This work is motivated by the analysis of a dataset\nobtained from a case-control study aimed to investigate the effect of\nHIV-infection on multimorbidity, that is simultaneous presence of two or more\nnoninfectious commorbidities in one patient.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 14:58:59 GMT"}, {"version": "v2", "created": "Fri, 20 Feb 2015 13:06:14 GMT"}, {"version": "v3", "created": "Mon, 16 May 2016 15:07:34 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Lupparelli", "Monia", ""], ["Roverato", "Alberto", ""]]}, {"id": "1410.0611", "submitter": "Jim Griffin", "authors": "Jim E. Griffin and Fabrizio Leisen", "title": "Compound random measures and their use in Bayesian nonparametrics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new class of dependent random measures which we call {\\it compound random\nmeasures} are proposed and the use of normalized versions of these random\nmeasures as priors in Bayesian nonparametric mixture models is considered.\nTheir tractability allows the properties of both compound random measures and\nnormalized compound random measures to be derived. In particular, we show how\ncompound random measures can be constructed with gamma, $\\sigma$-stable and\ngeneralized gamma process marginals. We also derive several forms of the\nLaplace exponent and characterize dependence through both the L\\'evy copula and\ncorrelation function. A slice sampler and an augmented P\\'olya urn scheme\nsampler are described for posterior inference when a normalized compound random\nmeasure is used as the mixing measure in a nonparametric mixture model and a\ndata example is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 17:18:58 GMT"}, {"version": "v2", "created": "Thu, 9 Oct 2014 16:10:29 GMT"}, {"version": "v3", "created": "Wed, 2 Sep 2015 11:34:31 GMT"}], "update_date": "2015-09-03", "authors_parsed": [["Griffin", "Jim E.", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1410.0669", "submitter": "Alexander Wong", "authors": "Alexander Wong and Xiao Yu Wang", "title": "A Bayesian Residual Transform for Signal Processing", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-scale decomposition has been an invaluable tool for the processing of\nphysiological signals. Much focus in multi-scale decomposition for processing\nsuch signals have been based on scale-space theory and wavelet transforms. In\nthis study, we take a different perspective on multi-scale decomposition by\ninvestigating the feasibility of utilizing a Bayesian-based method for\nmulti-scale signal decomposition called Bayesian Residual Transform (BRT) for\nthe purpose of physiological signal processing. In BRT, a signal is modeled as\nthe summation of residual signals, each characterizing information from the\nsignal at different scales. A deep cascading framework is introduced as a\nrealization of the BRT. Signal-to-noise ratio (SNR) analysis using\nelectrocardiography (ECG) signals was used to illustrate the feasibility of\nusing the BRT for suppressing noise in physiological signals. Results in this\nstudy show that it is feasible to utilize the BRT for processing physiological\nsignals for tasks such as noise suppression.\n", "versions": [{"version": "v1", "created": "Thu, 2 Oct 2014 19:50:53 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 15:56:17 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Wong", "Alexander", ""], ["Wang", "Xiao Yu", ""]]}, {"id": "1410.0761", "submitter": "Ian Barnett", "authors": "Ian Barnett and Jukka-Pekka Onnela", "title": "Change Point Detection in Correlation Networks", "comments": "23 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many systems of interacting elements can be conceptualized as networks, where\nnetwork nodes represent the elements and network ties represent interactions\nbetween the elements. In systems where the underlying network evolves in time,\nit is useful to determine the points in time where the network structure\nchanges significantly as these may correspond also to functional change points.\nWe propose a method for detecting these change points in correlation networks\nthat, unlike previous change point detection methods designed for time series\ndata, requires no distributional assumptions. We investigate the difficulty of\nchange point detection near the boundaries of data in correlation networks and\ndemonstrate the power of our method and a competing method through simulation.\nWe also show the generalizable nature of our method by applying it to stock\nprice data as well as fMRI data.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 06:24:41 GMT"}, {"version": "v2", "created": "Mon, 4 May 2015 14:06:43 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Barnett", "Ian", ""], ["Onnela", "Jukka-Pekka", ""]]}, {"id": "1410.0813", "submitter": "John Aston", "authors": "Nathaniel Shiers, John A. D. Aston, Jim Q. Smith, and John S. Coleman", "title": "Gaussian Tree Constraints Applied to Acoustic Linguistic Functional Data", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary models of languages are usually considered to take the form of\ntrees. With the development of so-called tree constraints the plausibility of\nthe tree model assumptions can be addressed by checking whether the moments of\nobserved variables lie within regions consistent with trees. In our linguistic\napplication, the data set comprises acoustic samples (audio recordings) from\nspeakers of five Romance languages or dialects. We wish to assess these\nfunctional data for compatibility with a hereditary tree model at the language\nlevel. A novel combination of canonical function analysis (CFA) with a\nseparable covariance structure provides a method for generating a\nrepresentative basis for the data. This resulting basis is formed of components\nwhich emphasize language differences whilst maintaining the integrity of the\nobservational language-groupings. A previously unexploited Gaussian tree\nconstraint is then applied to component-by-component projections of the data to\ninvestigate adherence to an evolutionary tree. The results indicate that while\na tree model is unlikely to be suitable for modeling all aspects of the\nacoustic linguistic data, certain features of the spoken Romance languages\nhighlighted by the separable-CFA basis may indeed be suitably modeled as a\ntree.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 10:55:33 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Shiers", "Nathaniel", ""], ["Aston", "John A. D.", ""], ["Smith", "Jim Q.", ""], ["Coleman", "John S.", ""]]}, {"id": "1410.0827", "submitter": "Antonio Canale", "authors": "Antonio Canale and David B. Dunson", "title": "Multiscale Bernstein polynomials for densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our focus is on constructing a multiscale nonparametric prior for densities.\nThe Bayes density estimation literature is dominated by single scale methods,\nwith the exception of Polya trees, which favor overly-spiky densities even when\nthe truth is smooth. We propose a multiscale Bernstein polynomial family of\npriors, which produce smooth realizations that do not rely on hard partitioning\nof the support. At each level in an infinitely-deep binary tree, we place a\nbeta dictionary density; within a scale the densities are equivalent to\nBernstein polynomials. Using a stick-breaking characterization, stochastically\ndecreasing weights are allocated to the finer scale dictionary elements. A\nslice sampler is used for posterior computation, and properties are described.\nThe method characterizes densities with locally-varying smoothness, and can\nproduce a sequence of coarse to fine density estimates. An extension for\nBayesian testing of group differences is introduced and applied to DNA\nmethylation array data.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 12:05:04 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Canale", "Antonio", ""], ["Dunson", "David B.", ""]]}, {"id": "1410.0950", "submitter": "Dennis Wei", "authors": "Dennis Wei", "title": "Adaptive Sensing Resource Allocation Over Multiple Hypothesis Tests", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers multiple binary hypothesis tests with adaptive\nallocation of sensing resources from a shared budget over a small number of\nstages. A Bayesian formulation is provided for the multistage allocation\nproblem of minimizing the sum of Bayes risks, which is then recast as a dynamic\nprogram. In the single-stage case, the problem is a non-convex optimization,\nfor which an algorithm composed of a series of parallel one-dimensional\nminimizations is presented. This algorithm ensures a global minimum under a\nsufficient condition. In the multistage case, the approximate dynamic\nprogramming method of open-loop feedback control is employed. In numerical\nsimulations, the proposed allocation policies outperform alternative adaptive\nprocedures when the numbers of true null and alternative hypotheses are not too\nimbalanced. In the case of few alternative hypotheses, the proposed policies\nare competitive using only a few stages of adaptation. In all cases substantial\ngains over non-adaptive sensing are observed.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 19:40:53 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 21:47:36 GMT"}], "update_date": "2014-11-05", "authors_parsed": [["Wei", "Dennis", ""]]}, {"id": "1410.0989", "submitter": "Raja Giryes", "authors": "Raja Giryes, and Yaniv Plan and Roman Vershynin", "title": "On the Effective Measure of Dimension in the Analysis Cosparse Model", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.NA math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many applications have benefited remarkably from low-dimensional models in\nthe recent decade. The fact that many signals, though high dimensional, are\nintrinsically low dimensional has given the possibility to recover them stably\nfrom a relatively small number of their measurements. For example, in\ncompressed sensing with the standard (synthesis) sparsity prior and in matrix\ncompletion, the number of measurements needed is proportional (up to a\nlogarithmic factor) to the signal's manifold dimension.\n  Recently, a new natural low-dimensional signal model has been proposed: the\ncosparse analysis prior. In the noiseless case, it is possible to recover\nsignals from this model, using a combinatorial search, from a number of\nmeasurements proportional to the signal's manifold dimension. However, if we\nask for stability to noise or an efficient (polynomial complexity) solver, all\nthe existing results demand a number of measurements which is far removed from\nthe manifold dimension, sometimes far greater. Thus, it is natural to ask\nwhether this gap is a deficiency of the theory and the solvers, or if there\nexists a real barrier in recovering the cosparse signals by relying only on\ntheir manifold dimension. Is there an algorithm which, in the presence of\nnoise, can accurately recover a cosparse signal from a number of measurements\nproportional to the manifold dimension? In this work, we prove that there is no\nsuch algorithm. Further, we show through numerical simulations that even in the\nnoiseless case convex relaxations fail when the number of measurements is\ncomparable to the manifold dimension. This gives a practical counter-example to\nthe growing literature on compressed acquisition of signals based on manifold\ndimension.\n", "versions": [{"version": "v1", "created": "Fri, 3 Oct 2014 22:43:39 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 20:58:03 GMT"}], "update_date": "2015-07-29", "authors_parsed": [["Giryes", "Raja", ""], ["Plan", "Yaniv", ""], ["Vershynin", "Roman", ""]]}, {"id": "1410.1151", "submitter": "Vladimir Bochkarev", "authors": "Yulia S. Maslennikova, Vladimir V. Bochkarev", "title": "Training Algorithm for Neuro-Fuzzy Network Based on Singular Spectrum\n  Analysis", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we propose a combination of an noise-reduction algorithm\nbased on Singular Spectrum Analysis (SSA) and a standard feedforward neural\nprediction model. Basically, the proposed algorithm consists of two different\nsteps: data preprocessing based on the SSA filtering method and step-by-step\ntraining procedure in which we use a simple feedforward multilayer neural\nnetwork with backpropagation learning. The proposed noise-reduction procedure\nsuccessfully removes most of the noise. That increases long-term predictability\nof the processed dataset comparison with the raw dataset. The method was\napplied to predict the International sunspot number RZ time series. The results\nshow that our combined technique has better performances than those offered by\nthe same network directly applied to raw dataset.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 12:25:15 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Maslennikova", "Yulia S.", ""], ["Bochkarev", "Vladimir V.", ""]]}, {"id": "1410.1173", "submitter": "Yiyuan She", "authors": "Yiyuan She, Shijie Li, and Dapeng Wu", "title": "Robust Orthogonal Complement Principal Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the robustification of principal component analysis has attracted\nlots of attention from statisticians, engineers and computer scientists. In\nthis work we study the type of outliers that are not necessarily apparent in\nthe original observation space but can seriously affect the principal subspace\nestimation. Based on a mathematical formulation of such transformed outliers, a\nnovel robust orthogonal complement principal component analysis (ROC-PCA) is\nproposed. The framework combines the popular sparsity-enforcing and low rank\nregularization techniques to deal with row-wise outliers as well as\nelement-wise outliers. A non-asymptotic oracle inequality guarantees the\naccuracy and high breakdown performance of ROC-PCA in finite samples. To tackle\nthe computational challenges, an efficient algorithm is developed on the basis\nof Stiefel manifold optimization and iterative thresholding. Furthermore, a\nbatch variant is proposed to significantly reduce the cost in ultra high\ndimensions. The paper also points out a pitfall of a common practice of SVD\nreduction in robust PCA. Experiments show the effectiveness and efficiency of\nROC-PCA in both synthetic and real data.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 16:20:34 GMT"}, {"version": "v2", "created": "Mon, 13 Apr 2015 19:08:29 GMT"}, {"version": "v3", "created": "Thu, 28 Jan 2016 02:41:55 GMT"}], "update_date": "2016-01-29", "authors_parsed": [["She", "Yiyuan", ""], ["Li", "Shijie", ""], ["Wu", "Dapeng", ""]]}, {"id": "1410.1221", "submitter": "Noemi Petra", "authors": "Tobin Isaac, Noemi Petra, Georg Stadler, Omar Ghattas", "title": "Scalable and efficient algorithms for the propagation of uncertainty\n  from data through inference to prediction for large-scale problems, with\n  application to flow of the Antarctic ice sheet", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2015.04.047", "report-no": null, "categories": "math.OC math.NA stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of research on efficient and scalable algorithms in\ncomputational science and engineering has focused on the forward problem: given\nparameter inputs, solve the governing equations to determine output quantities\nof interest. In contrast, here we consider the broader question: given a\n(large-scale) model containing uncertain parameters, (possibly) noisy\nobservational data, and a prediction quantity of interest, how do we construct\nefficient and scalable algorithms to (1) infer the model parameters from the\ndata (the deterministic inverse problem), (2) quantify the uncertainty in the\ninferred parameters (the Bayesian inference problem), and (3) propagate the\nresulting uncertain parameters through the model to issue predictions with\nquantified uncertainties (the forward uncertainty propagation problem)? We\npresent efficient and scalable algorithms for this end-to-end,\ndata-to-prediction process under the Gaussian approximation and in the context\nof modeling the flow of the Antarctic ice sheet and its effect on sea level.\nThe ice is modeled as a viscous, incompressible, creeping, shear-thinning\nfluid. The observational data come from InSAR satellite measurements of surface\nice flow velocity, and the uncertain parameter field to be inferred is the\nbasal sliding parameter. The prediction quantity of interest is the present-day\nice mass flux from the Antarctic continent to the ocean. We show that the work\nrequired for executing this data-to-prediction process is independent of the\nstate dimension, parameter dimension, data dimension, and number of processor\ncores. The key to achieving this dimension independence is to exploit the fact\nthat the observational data typically provide only sparse information on model\nparameters. This property can be exploited to construct a low rank\napproximation of the linearized parameter-to-observable map.\n", "versions": [{"version": "v1", "created": "Sun, 5 Oct 2014 22:55:16 GMT"}, {"version": "v2", "created": "Wed, 2 Sep 2015 03:25:10 GMT"}], "update_date": "2015-11-05", "authors_parsed": [["Isaac", "Tobin", ""], ["Petra", "Noemi", ""], ["Stadler", "Georg", ""], ["Ghattas", "Omar", ""]]}, {"id": "1410.1242", "submitter": "Abraham Martin del Campo", "authors": "Abraham Martin del Campo, Sarah Cepeda, Caroline Uhler", "title": "Exact goodness-of-fit testing for the Ising model", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ising model is one of the simplest and most famous models of interacting\nsystems. It was originally proposed to model ferromagnetic interactions in\nstatistical physics and is now widely used to model spatial processes in many\nareas such as ecology, sociology, and genetics, usually without testing its\ngoodness of fit. Here, we propose various test statistics and an exact\ngoodness-of-fit test for the finite-lattice Ising model. The theory of Markov\nbases has been developed in algebraic statistics for exact goodness-of-fit\ntesting using a Monte Carlo approach. However, finding a Markov basis is often\ncomputationally intractable. Thus, we develop a Monte Carlo method for exact\ngoodness-of-fit testing for the Ising model which avoids computing a Markov\nbasis and also leads to a better connectivity of the Markov chain and hence to\na faster convergence. We show how this method can be applied to analyze the\nspatial organization of receptors on the cell membrane.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 02:59:48 GMT"}, {"version": "v2", "created": "Sun, 22 Feb 2015 09:05:50 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2016 01:17:23 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["del Campo", "Abraham Martin", ""], ["Cepeda", "Sarah", ""], ["Uhler", "Caroline", ""]]}, {"id": "1410.1263", "submitter": "Vladimir Minin", "authors": "Peter B. Chi, Sujay Chattopadhyay, Philippe Lemey, Evgeni V.\n  Sokurenko, Vladimir N. Minin", "title": "Synonymous and Nonsynonymous Distances Help Untangle Convergent\n  Evolution and Recombination", "comments": "21 pages, 8 figures, updated abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When estimating a phylogeny from a multiple sequence alignment, researchers\noften assume the absence of recombination. However, if recombination is\npresent, then tree estimation and all downstream analyses will be impacted,\nbecause different segments of the sequence alignment support different\nphylogenies. Similarly, convergent selective pressures at the molecular level\ncan also lead to phylogenetic tree incongruence across the sequence alignment.\nCurrent methods for detection of phylogenetic incongruence are not equipped to\ndistinguish between these two different mechanisms and assume that the\nincongruence is a result of recombination or other horizontal transfer of\ngenetic information. We propose a new recombination detection method that can\nmake this distinction, based on synonymous codon substitution distances.\nAlthough some power is lost by discarding the information contained in the\nnonsynonymous substitutions, our new method has lower false positive\nprobabilities than the comparable recombination detection method when the\nphylogenetic incongruence signal is due to convergent evolution. We apply our\nmethod to three empirical examples, where we analyze: 1) sequences from a\ntransmission network of the human immunodeficiency virus, 2) tlpB gene\nsequences from a geographically diverse set of 38 Helicobacter pylori strains,\nand 3) Hepatitis C virus sequences sampled longitudinally from one patient.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 05:54:57 GMT"}, {"version": "v2", "created": "Tue, 7 Oct 2014 04:53:59 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Chi", "Peter B.", ""], ["Chattopadhyay", "Sujay", ""], ["Lemey", "Philippe", ""], ["Sokurenko", "Evgeni V.", ""], ["Minin", "Vladimir N.", ""]]}, {"id": "1410.1494", "submitter": "Mark Risser", "authors": "Mark D. Risser and Catherine A. Calder", "title": "Regression-based covariance functions for nonstationary spatial modeling", "comments": null, "journal-ref": "Environmetrics, 2015. 26(4) 284-297", "doi": "10.1002/env.2336", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many environmental applications involving spatially-referenced data,\nlimitations on the number and locations of observations motivate the need for\npractical and efficient models for spatial interpolation, or kriging. A key\ncomponent of models for continuously-indexed spatial data is the covariance\nfunction, which is traditionally assumed to belong to a parametric class of\nstationary models. However, stationarity is rarely a realistic assumption.\nAlternative methods which more appropriately model the nonstationarity present\nin environmental processes often involve high-dimensional parameter spaces,\nwhich lead to difficulties in model fitting and interpretability. To overcome\nthis issue, we build on the growing literature of covariate-driven\nnonstationary spatial modeling. Using process convolution techniques, we\npropose a Bayesian model for continuously-indexed spatial data based on a\nflexible parametric covariance regression structure for a convolution-kernel\ncovariance matrix. The resulting model is a parsimonious representation of the\nkernel process, and we explore properties of the implied model, including a\ndescription of the resulting nonstationary covariance function and the\ninterpretational benefits in the kernel parameters. Furthermore, we demonstrate\nthat our model provides a practical compromise between stationary and highly\nparameterized nonstationary spatial covariance functions that do not perform\nwell in practice. We illustrate our approach through an analysis of annual\nprecipitation data.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 19:04:30 GMT"}, {"version": "v2", "created": "Wed, 4 Feb 2015 23:06:37 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Risser", "Mark D.", ""], ["Calder", "Catherine A.", ""]]}, {"id": "1410.1503", "submitter": "Xiaoming Huo", "authors": "Xiaoming Huo, Gabor J. Szekely", "title": "Fast Computing for Distance Covariance", "comments": "38 pages, 6 tables, 5 figures. arXiv admin note: text overlap with\n  arXiv:1205.4701 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distance covariance and distance correlation have been widely adopted in\nmeasuring dependence of a pair of random variables or random vectors. If the\ncomputation of distance covariance and distance correlation is implemented\ndirectly accordingly to its definition then its computational complexity is\nO($n^2$) which is a disadvantage compared to other faster methods. In this\npaper we show that the computation of distance covariance and distance\ncorrelation of real valued random variables can be implemented by an O(n log n)\nalgorithm and this is comparable to other computationally efficient algorithms.\nThe new formula we derive for an unbiased estimator for squared distance\ncovariance turns out to be a U-statistic. This fact implies some nice\nasymptotic properties that were derived before via more complex methods. We\napply the fast computing algorithm to some synthetic data. Our work will make\ndistance correlation applicable to a much wider class of applications.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 19:40:44 GMT"}], "update_date": "2014-10-07", "authors_parsed": [["Huo", "Xiaoming", ""], ["Szekely", "Gabor J.", ""]]}, {"id": "1410.1620", "submitter": "Bernard Ycart", "authors": "Konstantina Charmpi (LJK), Bernard Ycart (LJK)", "title": "Weighted Kolmogorov Smirnov testing: an alternative for Gene Set\n  Enrichment Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gene Set Enrichment Analysis (GSEA) is a basic tool for genomic data\ntreatment. From a statistical point of view, the centering of its test\nstatistic does not allow the derivation of asymptotic results. A test statistic\nwith a different centering is proposed. Under the null hypothesis, the\nconvergence in distribution of the new test statistic is proved, using the\ntheory of empirical processes. The limiting distribution can be computed by\nMonte-Carlo simulation. The test defined in this way has been called Weighted\nKolmogorov Smirnov (WKS) test. The fact that the evaluation of the asymptotic\ndistribution serves for many different gene sets results in shorter computing\ntimes. Using expression data from the GEO repository, tested against the MSig\nDatabase C2, a comparison between the classical GSEA test and the new procedure\nhas been conducted. Our conclusion is that, beyond its mathematical and\nalgorithmic advantages, the WKS test could be more informative in many cases,\nthan the classical GSEA test.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2014 05:53:53 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Charmpi", "Konstantina", "", "LJK"], ["Ycart", "Bernard", "", "LJK"]]}, {"id": "1410.1932", "submitter": "Wei-Yin Loh", "authors": "Wei-Yin Loh, Xu He and Michael Man", "title": "A regression tree approach to identifying subgroups with differential\n  treatment effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fight against hard-to-treat diseases such as cancer, it is often\ndifficult to discover new treatments that benefit all subjects. For regulatory\nagency approval, it is more practical to identify subgroups of subjects for\nwhom the treatment has an enhanced effect. Regression trees are natural for\nthis task because they partition the data space. We briefly review existing\nregression tree algorithms. Then we introduce three new ones that are\npractically free of selection bias and are applicable to two or more\ntreatments, censored response variables, and missing values in the predictor\nvariables. The algorithms extend the GUIDE approach by using three key ideas:\n(i) treatment as a linear predictor, (ii) chi-squared tests to detect residual\npatterns and lack of fit, and (iii) proportional hazards modeling via Poisson\nregression. Importance scores with thresholds for identifying influential\nvariables are obtained as by-products. A bootstrap technique is used to\nconstruct confidence intervals for the treatment effects in each node. Real and\nsimulated data are used to compare the methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Oct 2014 22:08:23 GMT"}], "update_date": "2014-10-09", "authors_parsed": [["Loh", "Wei-Yin", ""], ["He", "Xu", ""], ["Man", "Michael", ""]]}, {"id": "1410.2296", "submitter": "Paul von Hippel", "authors": "Paul T. von Hippel", "title": "Estimates of heterogeneity (I2) can be biased in small meta-analyses", "comments": "7 pages + 3 figures", "journal-ref": null, "doi": "10.1186/s12874-015-0024-z", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In meta-analysis, the fraction of variance that is due to heterogeneity is\nknown as I2. We show that the usual estimator of I2 is biased. The bias is\nlargest when a meta-analysis has few studies and little heterogeneity. For\nexample, with 7 studies and the true value of I2 at 0, the average estimate of\nI2 is .124. Estimates of I2 should be interpreted cautiously when the\nmeta-analysis is small and the null hypothesis of homogeneity (I2=0) has not\nbeen rejected. In small meta-analyses, confidence intervals may be preferable\nto point estimates for I2.\n", "versions": [{"version": "v1", "created": "Wed, 8 Oct 2014 22:13:05 GMT"}, {"version": "v2", "created": "Fri, 10 Oct 2014 02:40:23 GMT"}], "update_date": "2017-03-24", "authors_parsed": [["von Hippel", "Paul T.", ""]]}, {"id": "1410.2323", "submitter": "Jinyuan Chang", "authors": "Jinyuan Chang, Bin Guo, Qiwei Yao", "title": "Principal component analysis for second-order stationary vector time\n  series", "comments": "The original title dated back to October 2014 is \"Segmenting Multiple\n  Time Series by Contemporaneous Linear Transformation: PCA for Time Series\"", "journal-ref": "Annals of Statistics 2018, Vol. 46, No. 5, 2094-2124", "doi": "10.1214/17-AOS1613", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the principal component analysis (PCA) to second-order stationary\nvector time series in the sense that we seek for a contemporaneous linear\ntransformation for a $p$-variate time series such that the transformed series\nis segmented into several lower-dimensional subseries, and those subseries are\nuncorrelated with each other both contemporaneously and serially. Therefore\nthose lower-dimensional series can be analysed separately as far as the linear\ndynamic structure is concerned. Technically it boils down to an eigenanalysis\nfor a positive definite matrix. When $p$ is large, an additional step is\nrequired to perform a permutation in terms of either maximum cross-correlations\nor FDR based on multiple tests. The asymptotic theory is established for both\nfixed $p$ and diverging $p$ when the sample size $n$ tends to infinity.\nNumerical experiments with both simulated and real data sets indicate that the\nproposed method is an effective initial step in analysing multiple time series\ndata, which leads to substantial dimension reduction in modelling and\nforecasting high-dimensional linear dynamical structures. Unlike PCA for\nindependent data, there is no guarantee that the required linear transformation\nexists. When it does not, the proposed method provides an approximate\nsegmentation which leads to the advantages in, for example, forecasting for\nfuture values. The method can also be adapted to segment multiple volatility\nprocesses.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 00:53:06 GMT"}, {"version": "v2", "created": "Fri, 5 Dec 2014 13:09:47 GMT"}, {"version": "v3", "created": "Tue, 2 Aug 2016 05:07:15 GMT"}, {"version": "v4", "created": "Wed, 12 Apr 2017 21:04:13 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Chang", "Jinyuan", ""], ["Guo", "Bin", ""], ["Yao", "Qiwei", ""]]}, {"id": "1410.2392", "submitter": "Chris Oates", "authors": "Chris J. Oates, Mark Girolami, Nicolas Chopin", "title": "Control functionals for Monte Carlo integration", "comments": "Accepted for publication in J. R. Statist. Soc. B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-parametric extension of control variates is presented. These leverage\ngradient information on the sampling density to achieve substantial variance\nreduction. It is not required that the sampling density be normalised. The\nnovel contribution of this work is based on two important insights; (i) a\ntrade-off between random sampling and deterministic approximation and (ii) a\nnew gradient-based function space derived from Stein's identity. Unlike\nclassical control variates, our estimators achieve super-root-$n$ convergence,\noften requiring orders of magnitude fewer simulations to achieve a fixed level\nof precision. Theoretical and empirical results are presented, the latter\nfocusing on integration problems arising in hierarchical models and models\nbased on non-linear ordinary differential equations.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 09:05:22 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 19:49:19 GMT"}, {"version": "v3", "created": "Fri, 1 May 2015 17:50:56 GMT"}, {"version": "v4", "created": "Tue, 27 Oct 2015 01:51:50 GMT"}, {"version": "v5", "created": "Sat, 2 Apr 2016 02:13:52 GMT"}], "update_date": "2016-04-05", "authors_parsed": [["Oates", "Chris J.", ""], ["Girolami", "Mark", ""], ["Chopin", "Nicolas", ""]]}, {"id": "1410.2477", "submitter": "Rams\\'{e}s H. Mena", "authors": "Rams\\'es H. Mena, Matteo Ruggiero", "title": "Dynamic density estimation with diffusive Dirichlet mixtures", "comments": "Published at http://dx.doi.org/10.3150/14-BEJ681 in the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2016, Vol. 22, No. 2, 901-926", "doi": "10.3150/14-BEJ681", "report-no": "IMS-BEJ-BEJ681", "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of nonparametric prior distributions on the space of\ncontinuously varying densities, induced by Dirichlet process mixtures which\ndiffuse in time. These select time-indexed random functions without jumps,\nwhose sections are continuous or discrete distributions depending on the choice\nof kernel. The construction exploits the widely used stick-breaking\nrepresentation of the Dirichlet process and induces the time dependence by\nreplacing the stick-breaking components with one-dimensional Wright-Fisher\ndiffusions. These features combine appealing properties of the model, inherited\nfrom the Wright-Fisher diffusions and the Dirichlet mixture structure, with\ngreat flexibility and tractability for posterior computation. The construction\ncan be easily extended to multi-parameter GEM marginal states, which include,\nfor example, the Pitman--Yor process. A full inferential strategy is detailed\nand illustrated on simulated and real data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 14:13:07 GMT"}, {"version": "v2", "created": "Tue, 9 Feb 2016 10:48:15 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Mena", "Rams\u00e9s H.", ""], ["Ruggiero", "Matteo", ""]]}, {"id": "1410.2505", "submitter": "Ping Li", "authors": "Jian Wang, Ping Li", "title": "Recovery of Sparse Signals Using Multiple Orthogonal Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of recovering sparse signals from compressed linear\nmeasurements. This problem, often referred to as sparse recovery or sparse\nreconstruction, has generated a great deal of interest in recent years. To\nrecover the sparse signals, we propose a new method called multiple orthogonal\nleast squares (MOLS), which extends the well-known orthogonal least squares\n(OLS) algorithm by allowing multiple $L$ indices to be chosen per iteration.\nOwing to inclusion of multiple support indices in each selection, the MOLS\nalgorithm converges in much fewer iterations and improves the computational\nefficiency over the conventional OLS algorithm. Theoretical analysis shows that\nMOLS ($L > 1$) performs exact recovery of all $K$-sparse signals within $K$\niterations if the measurement matrix satisfies the restricted isometry property\n(RIP) with isometry constant $\\delta_{LK} < \\frac{\\sqrt{L}}{\\sqrt{K} + 2\n\\sqrt{L}}.$ The recovery performance of MOLS in the noisy scenario is also\nstudied. It is shown that stable recovery of sparse signals can be achieved\nwith the MOLS algorithm when the signal-to-noise ratio (SNR) scales linearly\nwith the sparsity level of input signals.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 15:17:54 GMT"}, {"version": "v2", "created": "Thu, 31 Dec 2015 04:27:22 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Wang", "Jian", ""], ["Li", "Ping", ""]]}, {"id": "1410.2535", "submitter": "Jeremie Houssineau", "authors": "Jeremie Houssineau, Daniel Clark, Spela Ivekovic, Chee Sing Lee, Jose\n  Franco", "title": "A unified approach for multi-object triangulation, tracking and camera\n  calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object triangulation, 3-D object tracking, feature correspondence, and camera\ncalibration are key problems for estimation from camera networks. This paper\naddresses these problems within a unified Bayesian framework for joint\nmulti-object tracking and sensor registration. Given that using standard\nfiltering approaches for state estimation from cameras is problematic, an\nalternative parametrisation is exploited, called disparity space. The disparity\nspace-based approach for triangulation and object tracking is shown to be more\neffective than non-linear versions of the Kalman filter and particle filtering\nfor non-rectified cameras. The approach for feature correspondence is based on\nthe Probability Hypothesis Density (PHD) filter, and hence inherits the ability\nto update without explicit measurement association, to initiate new targets,\nand to discriminate between target and clutter. The PHD filtering approach then\nforms the basis of a camera calibration method from static or moving objects.\nResults are shown on simulated data.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 17:13:29 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Houssineau", "Jeremie", ""], ["Clark", "Daniel", ""], ["Ivekovic", "Spela", ""], ["Lee", "Chee Sing", ""], ["Franco", "Jose", ""]]}, {"id": "1410.2568", "submitter": "Hailiang Du", "authors": "Hailiang Du and Leonard A. Smith", "title": "Rising Above Chaotic Likelihoods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an nlin.CD stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Berliner (Likelihood and Bayesian prediction for chaotic systems, J. Am.\nStat. Assoc. 1991) identified a number of difficulties in using the likelihood\nfunction within the Bayesian paradigm which arise both for state estimation and\nfor parameter estimation of chaotic systems. Even when the equations of the\nsystem are given, he demonstrated \"chaotic likelihood functions\" both of\ninitial conditions and of parameter values in the Logistic Map. Chaotic\nlikelihood functions, while ultimately smooth, have such complicated small\nscale structure as to cast doubt on the possibility of identifying high\nlikelihood states in practice. In this paper, the challenge of chaotic\nlikelihoods is overcome by embedding the observations in a higher dimensional\nsequence-space; this allows good state estimation with finite computational\npower. An importance sampling approach is introduced, where Pseudo-orbit Data\nAssimilation is employed in the sequence-space, first to identify relevant\npseudo-orbits and then relevant trajectories. Estimates are identified with\nlikelihoods orders of magnitude higher than those previously identified in the\nexamples given by Berliner. Pseudo-orbit Data Assimilation importance sampler\nexploits the information both from the model dynamics and from the\nobservations. While sampling from the relevant prior (here, the natural\nmeasure) will, of course, eventually yield an accountable sample, given the\nrealistic computational resource this traditional approach would provide no\nhigh likelihood points at all. While one of the challenges Berliner posed is\novercome, his central conclusion is supported. \"Chaotic likelihood functions\"\nfor parameter estimation still pose a challenge; this fact helps clarify why\nphysical scientists maintain a strong distinction between the initial condition\nuncertainty and parameter uncertainty.\n", "versions": [{"version": "v1", "created": "Mon, 6 Oct 2014 21:18:06 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 15:28:30 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Du", "Hailiang", ""], ["Smith", "Leonard A.", ""]]}, {"id": "1410.2585", "submitter": "Bernard Ycart", "authors": "Bernard Ycart (LJK), Konstantina Charmpi (LJK), Sophie Rousseaux,\n  Jean-Jacques Fourni\\'e (CRCT)", "title": "Large scale statistical analysis of GEO datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem addressed here is that of simultaneous treatment of several gene\nexpression datasets, possibly collected under different experimental conditions\nand/or platforms. Using robust statistics, a large scale statistical analysis\nhas been conducted over $20$ datasets downloaded from the Gene Expression\nOmnibus repository. The differences between datasets are compared to the\nvariability inside a given dataset. Evidence that meaningful biological\ninformation can be extracted by merging different sources is provided.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 19:26:19 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Ycart", "Bernard", "", "LJK"], ["Charmpi", "Konstantina", "", "LJK"], ["Rousseaux", "Sophie", "", "CRCT"], ["Fourni\u00e9", "Jean-Jacques", "", "CRCT"]]}, {"id": "1410.2596", "submitter": "Trevor Hastie", "authors": "Trevor Hastie, Rahul Mazumder, Jason Lee, and Reza Zadeh", "title": "Matrix Completion and Low-Rank SVD via Fast Alternating Least Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The matrix-completion problem has attracted a lot of attention, largely as a\nresult of the celebrated Netflix competition. Two popular approaches for\nsolving the problem are nuclear-norm-regularized matrix approximation (Candes\nand Tao, 2009, Mazumder, Hastie and Tibshirani, 2010), and maximum-margin\nmatrix factorization (Srebro, Rennie and Jaakkola, 2005). These two procedures\nare in some cases solving equivalent problems, but with quite different\nalgorithms. In this article we bring the two approaches together, leading to an\nefficient algorithm for large matrix factorization and completion that\noutperforms both of these. We develop a software package \"softImpute\" in R for\nimplementing our approaches, and a distributed version for very large matrices\nusing the \"Spark\" cluster programming environment.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 19:48:00 GMT"}], "update_date": "2014-10-10", "authors_parsed": [["Hastie", "Trevor", ""], ["Mazumder", "Rahul", ""], ["Lee", "Jason", ""], ["Zadeh", "Reza", ""]]}, {"id": "1410.2597", "submitter": "William Fithian", "authors": "William Fithian, Dennis Sun, and Jonathan Taylor", "title": "Optimal Inference After Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform inference after model selection, we propose controlling the\nselective type I error; i.e., the error rate of a test given that it was\nperformed. By doing so, we recover long-run frequency properties among selected\nhypotheses analogous to those that apply in the classical (non-adaptive)\ncontext. Our proposal is closely related to data splitting and has a similar\nintuitive justification, but is more powerful. Exploiting the classical theory\nof Lehmann and Scheff\\'e (1955), we derive most powerful unbiased selective\ntests and confidence intervals for inference in exponential family models after\narbitrary selection procedures. For linear regression, we derive new selective\nz-tests that generalize recent proposals for inference after model selection\nand improve on their power, and new selective t-tests that do not require\nknowledge of the error variance.\n", "versions": [{"version": "v1", "created": "Thu, 9 Oct 2014 19:53:37 GMT"}, {"version": "v2", "created": "Wed, 29 Apr 2015 20:05:11 GMT"}, {"version": "v3", "created": "Sat, 15 Apr 2017 08:32:58 GMT"}, {"version": "v4", "created": "Tue, 18 Apr 2017 05:05:35 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Fithian", "William", ""], ["Sun", "Dennis", ""], ["Taylor", "Jonathan", ""]]}, {"id": "1410.2838", "submitter": "Ender Konukoglu", "authors": "Ender Konukoglu and Melanie Ganz", "title": "Approximate False Positive Rate Control in Selection Frequency for\n  Random Forest", "comments": "26 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random Forest has become one of the most popular tools for feature selection.\nIts ability to deal with high-dimensional data makes this algorithm especially\nuseful for studies in neuroimaging and bioinformatics. Despite its popularity\nand wide use, feature selection in Random Forest still lacks a crucial\ningredient: false positive rate control. To date there is no efficient,\nprincipled and computationally light-weight solution to this shortcoming. As a\nresult, researchers using Random Forest for feature selection have to resort to\nusing heuristically set thresholds on feature rankings. This article builds an\napproximate probabilistic model for the feature selection process in random\nforest training, which allows us to compute an estimated false positive rate\nfor a given threshold on selection frequency. Hence, it presents a principled\nway to determine thresholds for the selection of relevant features without any\nadditional computational load. Experimental analysis with synthetic data\ndemonstrates that the proposed approach can limit false positive rates on the\norder of the desired values and keep false negative rates low. Results show\nthat this holds even in the presence of a complex correlation structure between\nfeatures. Its good statistical properties and light-weight computational needs\nmake this approach widely applicable to feature selection for a wide-range of\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 16:43:16 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Konukoglu", "Ender", ""], ["Ganz", "Melanie", ""]]}, {"id": "1410.2839", "submitter": "Jun Li", "authors": "Jun Li and Ping-Shou Zhong", "title": "A rate optimal procedure for sparse signal recovery under dependence", "comments": "56 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers the problem of identifying the sparse different\ncomponents between two high dimensional means of column-wise dependent random\nvectors. We show that the dependence can be utilized to lower the\nidentification boundary for signal recovery. Moreover, an optimal convergence\nrate for the marginal false non-discovery rate (mFNR) is established under the\ndependence. The convergence rate is faster than the optimal rate without\ndependence. To recover the sparse signal bearing dimensions, we propose a\nDependence-Assisted Thresholding and Excising (DATE) procedure, which is shown\nto be rate optimal for the mFNR with the marginal false discovery rate (mFDR)\ncontrolled at a pre-specified level. Simulation studies and case study are\ngiven to demonstrate the performance of the proposed signal identification\nprocedure.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 16:49:04 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Li", "Jun", ""], ["Zhong", "Ping-Shou", ""]]}, {"id": "1410.2840", "submitter": "Pengsheng Ji", "authors": "Pengsheng Ji and Jiashun Jin", "title": "Coauthorship and Citation Networks for Statisticians", "comments": null, "journal-ref": "Annals of Applied Statistics 2016, 10(4): 1779-1812", "doi": "10.1214/15-AOAS896", "report-no": null, "categories": "stat.AP cs.DL physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have collected and cleaned two network data sets: Coauthorship and\nCitation networks for statisticians. The data sets are based on all research\npapers published in four of the top journals in statistics from $2003$ to the\nfirst half of $2012$. We analyze the data sets from many different\nperspectives, focusing on (a) centrality, (b) community structures, and (c)\nproductivity, patterns and trends.\n  For (a), we have identified the most prolific/collaborative/highly cited\nauthors. We have also identified a handful of \"hot\" papers, suggesting\n\"Variable Selection\" as one of the \"hot\" areas.\n  For (b), we have identified about $15$ meaningful communities or research\ngroups, including large-size ones such as \"Spatial Statistics\", \"Large-Scale\nMultiple Testing\", \"Variable Selection\" as well as small-size ones such as\n\"Dimensional Reduction\", \"Objective Bayes\", \"Quantile Regression\", and\n\"Theoretical Machine Learning\".\n  For (c), we find that over the 10-year period, both the average number of\npapers per author and the fraction of self citations have been decreasing, but\nthe proportion of distant citations has been increasing. These suggest that the\nstatistics community has become increasingly more collaborative, competitive,\nand globalized.\n  Our findings shed light on research habits, trends, and topological patterns\nof statisticians. The data sets provide a fertile ground for future researches\non or related to social networks of statisticians.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 16:49:31 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 07:51:07 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Ji", "Pengsheng", ""], ["Jin", "Jiashun", ""]]}, {"id": "1410.2843", "submitter": "Shemra Rizzo", "authors": "Shemra Rizzo and Robert E. Weiss", "title": "Meta-Analysis of Odds Ratios With Incomplete Extracted Data", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A typical random effects meta-analysis of odds-ratios assumes binomially\ndistributed numbers of events in a treatment and control group and requires the\nproportion of deaths to be extracted from published papers. This data is often\nnot available in the publications due to loss to follow-up. When the Kaplan\nMeier survival plot is available, it is common practice to manually measure the\nneeded information from the plot and infer the probability of survival and then\nto infer a best-guess of the number of deaths. Uncertainty introduced from\ntheses guesses is not accounted for in current models. This naive approach\nleads to over-certain results and potentially inaccurate conclusions. We\npropose the Uncertain Reading-Estimated Events model to construct each study's\ncontribution to the meta-analysis separately using the data available for\nextraction in the publications. We use real and simulated data to illustrate\nour methods. Meta-analysis based on the observed number of deaths lead to\nbiased estimates while our proposed model does not. Our results show increases\nin the standard deviation of the log-odds as compared to a naive meta-analysis\nthat assumes ideal extracted data, equivalent to a reduction of the overall\nsample size of 43% in our example.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 17:22:41 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Rizzo", "Shemra", ""], ["Weiss", "Robert E.", ""]]}, {"id": "1410.2848", "submitter": "Jun Li", "authors": "Song Xi Chen, Jun Li and Ping-Shou Zhong", "title": "Two-Sample Tests for High Dimensional Means with Thresholding and Data\n  Transformation", "comments": "64 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider testing for two-sample means of high dimensional populations by\nthresholding. Two tests are investigated, which are designed for better power\nperformance when the two population mean vectors differ only in sparsely\npopulated coordinates. The first test is constructed by carrying out\nthresholding to remove the non-signal bearing dimensions. The second test\ncombines data transformation via the precision matrix with the thresholding.\nThe benefits of the thresholding and the data transformations are showed by a\nreduced variance of the test thresholding statistics, the improved power and a\nwider detection region of the tests. Simulation experiments and an empirical\nstudy are performed to confirm the theoretical findings and to demonstrate the\npractical implementations.\n", "versions": [{"version": "v1", "created": "Fri, 10 Oct 2014 17:45:18 GMT"}], "update_date": "2014-10-13", "authors_parsed": [["Chen", "Song Xi", ""], ["Li", "Jun", ""], ["Zhong", "Ping-Shou", ""]]}, {"id": "1410.3155", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou and Stephen G Walker", "title": "Sample Size Dependent Species Models", "comments": "21 pages + 5 page appendix. arXiv admin note: text overlap with\n  arXiv:1310.1800", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the fundamental problem of measuring species diversity, this\npaper introduces the concept of a cluster structure to define an exchangeable\ncluster probability function that governs the joint distribution of a random\ncount and its exchangeable random partitions. A cluster structure, naturally\narising from a completely random measure mixed Poisson process, allows the\nprobability distribution of the random partitions of a subset of a sample to be\ndependent on the sample size, a distinct and motivated feature that differs it\nfrom a partition structure. A generalized negative binomial process model is\nproposed to generate a cluster structure, where in the prior the number of\nclusters is finite and Poisson distributed, and the cluster sizes follow a\ntruncated negative binomial distribution. We construct a nonparametric Bayesian\nestimator of Simpson's index of diversity under the generalized negative\nbinomial process. We illustrate our results through the analysis of two real\nsequencing count datasets.\n", "versions": [{"version": "v1", "created": "Sun, 12 Oct 2014 21:14:27 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Zhou", "Mingyuan", ""], ["Walker", "Stephen G", ""]]}, {"id": "1410.3163", "submitter": "Jay Ver Hoef", "authors": "Jay M. Ver Hoef and John K. Jansen", "title": "Estimating Abundance from Counts in Large Data Sets of\n  Irregularly-Spaced Plots using Spatial Basis Functions", "comments": "37 pages, 7 figures, 4 tables, keywords: sampling, change-of-support,\n  spatial point processes, intensity function, random effects, Poisson process,\n  overdispersion", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Monitoring plant and animal populations is an important goal for both\nacademic research and management of natural resources. Successful management of\npopulations often depends on obtaining estimates of their mean or total over a\nregion. The basic problem considered in this paper is the estimation of a total\nfrom a sample of plots containing count data, but the plot placements are\nspatially irregular and non randomized. Our application had counts from\nthousands of irregularly-spaced aerial photo images. We used change-of-support\nmethods to model counts in images as a realization of an inhomogeneous Poisson\nprocess that used spatial basis functions to model the spatial intensity\nsurface. The method was very fast and took only a few seconds for thousands of\nimages. The fitted intensity surface was integrated to provide an estimate from\nall unsampled areas, which is added to the observed counts. The proposed method\nalso provides a finite area correction factor to variance estimation. The\nintensity surface from an inhomogeneous Poisson process tends to be too smooth\nfor locally clustered points, typical of animal distributions, so we introduce\nseveral new overdispersion estimators due to poor performance of the classic\none. We used simulated data to examine estimation bias and to investigate\nseveral variance estimators with overdispersion. A real example is given of\nharbor seal counts from aerial surveys in an Alaskan glacial fjord.\n", "versions": [{"version": "v1", "created": "Sun, 12 Oct 2014 22:50:01 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Hoef", "Jay M. Ver", ""], ["Jansen", "John K.", ""]]}, {"id": "1410.3293", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy, Derek Bingham, James Paul Holloway, Michael J.\n  Grosskopf, Carolyn C. Kuranz, Erica Rutter, Matt Trantham, R. Paul Drake", "title": "Calibrating a large computer experiment simulating radiative shock\n  hydrodynamics", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS850 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1141-1168", "doi": "10.1214/15-AOAS850", "report-no": "IMS-AOAS-AOAS850", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider adapting a canonical computer model calibration apparatus,\ninvolving coupled Gaussian process (GP) emulators, to a computer experiment\nsimulating radiative shock hydrodynamics that is orders of magnitude larger\nthan what can typically be accommodated. The conventional approach calls for\nthousands of large matrix inverses to evaluate the likelihood in an MCMC\nscheme. Our approach replaces that costly ideal with a thrifty take on\nessential ingredients, synergizing three modern ideas in emulation, calibration\nand optimization: local approximate GP regression, modularization, and mesh\nadaptive direct search. The new methodology is motivated both by necessity -\nconsidering our particular application - and by recent trends in the\nsupercomputer simulation literature. A synthetic data application allows us to\nexplore the merits of several variations in a controlled environment and,\ntogether with results on our motivating real-data experiment, lead to\nnoteworthy insights into the dynamics of radiative shocks as well as the\nlimitations of the calibration enterprise generally.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 13:08:18 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2015 18:45:10 GMT"}, {"version": "v3", "created": "Sat, 6 Jun 2015 13:29:36 GMT"}, {"version": "v4", "created": "Thu, 5 Nov 2015 06:56:17 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Bingham", "Derek", ""], ["Holloway", "James Paul", ""], ["Grosskopf", "Michael J.", ""], ["Kuranz", "Carolyn C.", ""], ["Rutter", "Erica", ""], ["Trantham", "Matt", ""], ["Drake", "R. Paul", ""]]}, {"id": "1410.3355", "submitter": "Johanna Hardin", "authors": "Jacob Coleman, Joseph Replogle, Gabriel Chandler, Johanna Hardin", "title": "Resistant Multiple Sparse Canonical Correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Canonical Correlation Analysis (CCA) is a multivariate technique that takes\ntwo datasets and forms the most highly correlated possible pairs of linear\ncombinations between them. Each subsequent pair of linear combinations is\northogonal to the preceding pair, meaning that new information is gleaned from\neach pair. By looking at the magnitude of coefficient values, we can find out\nwhich variables can be grouped together, thus better understanding multiple\ninteractions that are otherwise difficult to compute or grasp intuitively.\n  CCA appears to have quite powerful applications to high throughput data, as\nwe can use it to discover, for example, relationships between gene expression\nand gene copy number variation. One of the biggest problems of CCA is that the\nnumber of variables (often upwards of 10,000) makes biological interpretation\nof linear combinations nearly impossible. To limit variable output, we have\nemployed a method known as Sparse Canonical Correlation Analysis (SCCA), while\nadding estimation which is resistant to extreme observations or other types of\ndeviant data. In this paper, we have demonstrated the success of resistant\nestimation in variable selection using SCCA. Additionally, we have used SCCA to\nfind multiple canonical pairs for extended knowledge about the datasets at\nhand. Again, using resistant estimators provided more accurate estimates than\nstandard estimators in the multiple canonical correlation setting.\n  R code is available and documented at https://github.com/hardin47/rmscca.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 15:39:24 GMT"}, {"version": "v2", "created": "Sun, 20 Dec 2015 15:59:00 GMT"}], "update_date": "2015-12-22", "authors_parsed": [["Coleman", "Jacob", ""], ["Replogle", "Joseph", ""], ["Chandler", "Gabriel", ""], ["Hardin", "Johanna", ""]]}, {"id": "1410.3370", "submitter": "Johanna Hardin", "authors": "Ciaran Evans, Johanna Hardin, Mark Huber, Daniel Stoebel, Garrett Wong", "title": "Differential expression analysis for multiple conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As high-throughput sequencing has become common practice, the cost of\nsequencing large amounts of genetic data has been drastically reduced, leading\nto much larger data sets for analysis. One important task is to identify\nbiological conditions that lead to unusually high or low expression of a\nparticular gene. Packages such as DESeq implement a simple method for testing\ndifferential signal when exactly two biological conditions are possible. For\nmore than two conditions, pairwise testing is typically used. Here the DESeq\nmethod is extended so that three or more biological conditions can be assessed\nsimultaneously. Because the computation time grows exponentially in the number\nof conditions, a Monte Carlo approach provides a fast way to approximate the\n$p$-values for the new test. The approach is studied on both simulated data and\na data set of {\\em C. jejuni}, the bacteria responsible for most food poisoning\nin the United States.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 15:59:36 GMT"}], "update_date": "2014-10-14", "authors_parsed": [["Evans", "Ciaran", ""], ["Hardin", "Johanna", ""], ["Huber", "Mark", ""], ["Stoebel", "Daniel", ""], ["Wong", "Garrett", ""]]}, {"id": "1410.3533", "submitter": "Igor Kheifets", "authors": "Igor L. Kheifets", "title": "Specification tests for nonlinear dynamic models", "comments": null, "journal-ref": "The Econometrics Journal, Vol. 18, 2015, pp. 67-94", "doi": "10.1111/ectj.12040", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new adequacy test and a graphical evaluation tool for nonlinear\ndynamic models. The proposed techniques can be applied in any setup where\nparametric conditional distribution of the data is specified, in particular to\nmodels involving conditional volatility, conditional higher moments,\nconditional quantiles, asymmetry, Value at Risk models, duration models,\ndiffusion models, etc. Compared to other tests, the new test properly controls\nthe nonlinear dynamic behavior in conditional distribution and does not rely on\nsmoothing techniques which require a choice of several tuning parameters. The\ntest is based on a new kind of multivariate empirical process of\ncontemporaneous and lagged probability integral transforms. We establish weak\nconvergence of the process under parameter uncertainty and local alternatives.\nWe justify a parametric bootstrap approximation that accounts for parameter\nestimation effects often ignored in practice. Monte Carlo experiments show that\nthe test has good finite-sample size and power properties. Using the new test\nand graphical tools we check the adequacy of various popular heteroscedastic\nmodels for stock exchange index data.\n", "versions": [{"version": "v1", "created": "Mon, 13 Oct 2014 22:54:06 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Kheifets", "Igor L.", ""]]}, {"id": "1410.3561", "submitter": "Hung Hung", "authors": "Hung Hung, Chih-Yen Liu, and Henry Horng-Shing Lu", "title": "Sufficient dimension reduction with additional information", "comments": "26 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sufficient dimension reduction is widely applied to help model building\nbetween the response $Y$ and covariate $X$. While the target of interest is the\nrelationship between $(Y,X)$, in some applications we also collect additional\nvariable $W$ that is strongly correlated with $Y$. From a statistical point of\nview, making inference about $(Y,X)$ without using $W$ will lose efficiency.\nHowever, it is not trivial to incorporate the information of $W$ to infer\n$(Y,X)$. In this article, we propose a two-stage dimension reduction method for\n$(Y,X)$, that is able to utilize the additional information from $W$. The main\nidea is to confine the searching space, by constructing an envelope subspace\nfor the target of interest. In the analysis of breast cancer data, the risk\nscore constructed from the two-stage method can well separate patients with\ndifferent survival experiences. In the Pima data, the two-stage method requires\nfewer components to infer the diabetes status, while achieving higher\nclassification accuracy than conventional method.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 03:44:35 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Hung", "Hung", ""], ["Liu", "Chih-Yen", ""], ["Lu", "Henry Horng-Shing", ""]]}, {"id": "1410.3566", "submitter": "Yuehan Yang", "authors": "Yuehan Yang, Hu Yang", "title": "Adaptive elastic net and Separate Selection from Least Squares for\n  ultra-high dimensional regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the asymptotic properties of the adaptive elastic net in\nultra-high dimensional sparse linear regression models and proposes a new\nmethod called SSLS (Separate Selection from Least Squares) to improve\nprediction accuracy. Besides, we prove that SSLS has the superior performance\nboth in the theoretical part and empirical part.\n  In this paper, we prove that the probability of adaptive elastic net\nselecting wrong variables can decays at an exponential rate with very few\nconditions. Irrepresentable Condition or similar constraint isn't necessary in\nour proof. We derive accurate bounds of bias and mean squared error (MSE) which\nboth depend on the choice of parameters, and also show that there exists a bias\nof asymptotic normality of the adaptive elastic net. Furthermore, simulations\nand empirical part both show that the prediction accuracy of the penalized\nleast squares requires more improvement.\n  Therefore, we propose SSLS to improve the prediction. It selects variable\nfirst, reducing high dimension to low dimension by using the adaptive elastic\nnet in this paper. In the second step, the coefficients are constructed based\non the OLS estimation. We show that the bias of SSLS can decays at an\nexponential rate. Also, MSE decays to zero. Finally, we prove that the variable\nselection consistency of SSLS implies the asymptotic normality of SSLS.\nSimulations given in this paper illustrate the performance of the SSLS,\nadaptive elastic net and other penalized least squares. The index tracking\nproblem in stock market is studied in the empirical part with other methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 04:15:29 GMT"}], "update_date": "2014-10-15", "authors_parsed": [["Yang", "Yuehan", ""], ["Yang", "Hu", ""]]}, {"id": "1410.3687", "submitter": "Jianfeng Yao", "authors": "Zeng Li, Qinwen Wang and Jianfeng Yao", "title": "Identifying the number of factors from singular values of a large sample\n  auto-covariance matrix", "comments": "This is a largely revised version of the previous manuscript (v1 &\n  v2)", "journal-ref": "The Annals of Statistics 45(1):257-288, February 2017", "doi": "10.1214/16-AOS1452", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying the number of factors in a high-dimensional factor model has\nattracted much attention in recent years and a general solution to the problem\nis still lacking. A promising ratio estimator based on the singular values of\nthe lagged autocovariance matrix has been recently proposed in the literature\nand is shown to have a good performance under some specific assumption on the\nstrength of the factors. Inspired by this ratio estimator and as a first main\ncontribution, this paper proposes a complete theory of such sample singular\nvalues for both the factor part and the noise part under the large-dimensional\nscheme where the dimension and the sample size proportionally grow to infinity.\nIn particular, we provide the exact description of the phase transition\nphenomenon that determines whether a factor is strong enough to be detected\nwith the observed sample singular values. Based on these findings and as a\nsecond main contribution of the paper, we propose a new estimator of the number\nof factors which is strongly consistent for the detection of all significant\nfactors (which are the only theoretically detectable ones). In particular,\nfactors are assumed to have the minimum strength above the phase transition\nboundary which is of the order of a constant; they are thus not required to\ngrow to infinity together with the dimension (as assumed in most of the\nexisting papers on high-dimensional factor models). Empirical Monte-Carlo study\nas well as the analysis of stock returns data attest a very good performance of\nthe proposed estimator. In all the tested cases, the new estimator largely\noutperforms the existing estimator using the same ratios of singular values.\n", "versions": [{"version": "v1", "created": "Tue, 14 Oct 2014 13:36:36 GMT"}, {"version": "v2", "created": "Sat, 25 Oct 2014 05:09:12 GMT"}, {"version": "v3", "created": "Mon, 19 Oct 2015 06:24:37 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Li", "Zeng", ""], ["Wang", "Qinwen", ""], ["Yao", "Jianfeng", ""]]}, {"id": "1410.3958", "submitter": "Kwun Chuen Gary Chan", "authors": "Kwun Chuen Gary Chan, Sheung Chi Phillip Yam", "title": "Oracle, Multiple Robust and Multipurpose Calibration in a Missing\n  Response Problem", "comments": "Published in at http://dx.doi.org/10.1214/13-STS461 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 3, 380-396", "doi": "10.1214/13-STS461", "report-no": "IMS-STS-STS461", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the presence of a missing response, reweighting the complete case\nsubsample by the inverse of nonmissing probability is both intuitive and easy\nto implement. When the population totals of some auxiliary variables are known\nand when the inclusion probabilities are known by design, survey statisticians\nhave developed calibration methods for improving efficiencies of the inverse\nprobability weighting estimators and the methods can be applied to missing data\nanalysis. Model-based calibration has been proposed in the survey sampling\nliterature, where multidimensional auxiliary variables are first summarized\ninto a predictor function from a working regression model. Usually, one working\nmodel is being proposed for each parameter of interest and results in different\nsets of calibration weights for estimating different parameters. This paper\nconsiders calibration using multiple working regression models for estimating a\nsingle or multiple parameters. Contrary to a common belief that overfitting\nhurts efficiency, we present three rather unexpected results. First, when the\nmissing probability is correctly specified and multiple working regression\nmodels for the conditional mean are posited, calibration enjoys an oracle\nproperty: the same semiparametric efficiency bound is attained as if the true\noutcome model is known in advance. Second, when the missing data mechanism is\nmisspecified, calibration can still be a consistent estimator when any one of\nthe outcome regression models is correctly specified. Third, a common set of\ncalibration weights can be used to improve efficiency in estimating multiple\nparameters of interest and can simultaneously attain semiparametric efficiency\nbounds for all parameters of interest. We provide connections of a wide class\nof calibration estimators, constructed based on generalized empirical\nlikelihood, to many existing estimators in biostatistics, econometrics and\nsurvey sampling and perform simulation studies to show that the finite sample\nproperties of calibration estimators conform well with the theoretical results\nbeing studied.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 08:17:10 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Chan", "Kwun Chuen Gary", ""], ["Yam", "Sheung Chi Phillip", ""]]}, {"id": "1410.3989", "submitter": "Kenneth F. Wallis", "authors": "Kenneth F. Wallis", "title": "Revisiting Francis Galton's Forecasting Competition", "comments": "Published in at http://dx.doi.org/10.1214/14-STS468 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 3, 420-424", "doi": "10.1214/14-STS468", "report-no": "IMS-STS-STS468", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note reexamines the data from a weight-judging competition described in\nan article by Francis Galton published in 1907. Following the correction of\nsome errors, it is shown that this forecasting competition is an interesting\nprecursor of two more recent developments in the statistical forecasting\nliterature. One is forecast combination, with the mean forecast here exactly\ncoinciding with the outcome, and the second is the use of two-piece frequency\nand probability distributions to describe asymmetry.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 09:46:13 GMT"}], "update_date": "2014-10-16", "authors_parsed": [["Wallis", "Kenneth F.", ""]]}, {"id": "1410.4093", "submitter": "Dexter Cahoy", "authors": "Dexter Cahoy", "title": "An estimation procedure for the Linnik distribution", "comments": null, "journal-ref": "Statistical Papers Statistical Papers, August 2012, Volume 53,\n  Issue 3, pp 617-628", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose estimators for the parameters of the Linnik L$(\\alpha,\\gamma)$\ndistribution. The estimators are derived from the moments of the\nlog-transformed Linnik distributed random variable, and are shown to be\nasymptotically unbiased. The estimation algorithm is computationally simple and\nless restrictive. Our procedure is also tested using simulated data.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 15:10:35 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 17:18:19 GMT"}, {"version": "v3", "created": "Fri, 24 Mar 2017 15:12:52 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["Cahoy", "Dexter", ""]]}, {"id": "1410.4105", "submitter": "Herv\\'e Cardot", "authors": "Herv\\'e Cardot and Anne De Moliner and Camelia Goga", "title": "Estimating with kernel smoothers the mean of functional data in a finite\n  population setting. A note on variance estimation in presence of partially\n  observed trajectories", "comments": "Version revised for Statistics and Probability Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the near future, millions of load curves measuring the electricity\nconsumption of French households in small time grids (probably half hours) will\nbe available. All these collected load curves represent a huge amount of\ninformation which could be exploited using survey sampling techniques. In\nparticular, the total consumption of a specific cus- tomer group (for example\nall the customers of an electricity supplier) could be estimated using unequal\nprobability random sampling methods. Unfortunately, data collection may undergo\ntechnical problems resulting in missing values. In this paper we study a new\nestimation method for the mean curve in the presence of missing values which\nconsists in extending kernel estimation techniques developed for longitudinal\ndata analysis to sampled curves. Three nonparametric estimators that take\naccount of the missing pieces of trajectories are suggested. We also study\npointwise variance estimators which are based on linearization techniques. The\nparticular but very important case of stratified sampling is then specifically\nstudied. Finally, we discuss some more practical aspects such as choosing the\nbandwidth values for the kernel and estimating the probabilities of observation\nof the trajectories.\n", "versions": [{"version": "v1", "created": "Wed, 15 Oct 2014 15:38:06 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2015 08:21:07 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Cardot", "Herv\u00e9", ""], ["De Moliner", "Anne", ""], ["Goga", "Camelia", ""]]}, {"id": "1410.4274", "submitter": "Xiongzhi Chen", "authors": "Xiongzhi Chen, Rebecca W. Doerge and Joseph F. Heyse", "title": "Multiple testing with discrete data: proportion of true null hypotheses\n  and two adaptive FDR procedures", "comments": "This version is essentially a different paper than the previous one.\n  A new estimator of the proportion of true null hypotheses has been developed,\n  and it induces two adaptive FDR procedures. One such procedure has been\n  proved to be conservative non-asymptotically, and the other has been\n  empirically shown to be conservative", "journal-ref": "Biometrical journal 2018", "doi": "10.1002/bimj.201700157", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider multiple testing with false discovery rate (FDR) control when\np-values have discrete and heterogeneous null distributions. We propose a new\nestimator of the proportion of true null hypotheses and demonstrate that it is\nless upwardly biased than Storey's estimator and two other estimators. The new\nestimator induces two adaptive procedures, i.e., an adaptive Benjamini-Hochberg\n(BH) procedure and an adaptive Benjamini-Hochberg-Heyse (BHH) procedure. We\nprove that the the adaptive BH procedure is conservative non-asymptotically.\nThrough simulation studies, we show that these procedures are usually more\npowerful than their non-adaptive counterparts and that the adaptive BHH\nprocedure is usually more powerful than the adaptive BH procedure and a\nprocedure based on randomized p-value. The adaptive procedures are applied to a\nstudy of HIV vaccine efficacy, where they identify more differentially\npolymorphic positions than the BH procedure at the same FDR level.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 02:00:36 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 05:21:28 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Chen", "Xiongzhi", ""], ["Doerge", "Rebecca W.", ""], ["Heyse", "Joseph F.", ""]]}, {"id": "1410.4275", "submitter": "Xiongzhi Chen", "authors": "Xiongzhi Chen", "title": "Consistent FDR estimation for adaptive multiple testing Normal means\n  under principal correlation structure", "comments": "20 pages; extracted multiple testing from the two-stage procedure:\n  estimate principal factors and then implement multiple testing; consistency\n  of the estimator and FDR estimation is shown under principle correlation\n  structure now; simulation studies are different", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We consider multiple testing means of many dependent Normal random variables\nwhen these random variables have a principal correlation structure and\ndifferent variances. We extend Jin's estimator of the proportion of nonzero\nNormal means to this setting and show that the extended estimator is\nconsistent. We also show that the false discovery rate of the adaptive\nsingle-step multiple testing procedure that employs this estimator can be\nconsistently estimated by its false discovery proportion and that the rejection\nthreshold of the procedure can be explicitly determined to ensure the\nconservativeness of the procedure. The extended estimator and adaptive\nprocedure are applied to multiple testing in an association study based on\nbrain imaging data.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 02:11:15 GMT"}, {"version": "v2", "created": "Fri, 16 Oct 2015 01:15:53 GMT"}, {"version": "v3", "created": "Wed, 29 Jun 2016 00:35:43 GMT"}, {"version": "v4", "created": "Wed, 31 Jan 2018 02:06:52 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Chen", "Xiongzhi", ""]]}, {"id": "1410.4282", "submitter": "Weidong Liu", "authors": "Weidong Liu", "title": "Incorporation of Sparsity Information in Large-scale Multiple Two-sample\n  $t$ Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale multiple two-sample {\\em Student}'s $t$ testing problems often\narise from the statistical analysis of scientific data. To detect components\nwith different values between two mean vectors, a well-known procedure is to\napply the Benjamini and Hochberg (B-H) method and two-sample {\\em Student}'s\n$t$ statistics to control the false discovery rate (FDR). In many applications,\nmean vectors are expected to be sparse or asymptotically sparse. When dealing\nwith such type of data, {\\em can we gain more power than the standard procedure\nsuch as the B-H method with Student's $t$ statistics while keeping the FDR\nunder control?} The answer is positive. By exploiting the possible sparsity\ninformation in mean vectors, we present an uncorrelated screening-based (US)\nFDR control procedure, which is shown to be more powerful than the B-H method.\nThe US testing procedure depends on a novel construction of screening\nstatistics, which are asymptotically uncorrelated with two-sample {\\em\nStudent}'s $t$ statistics. The US testing procedure is different from some\nexisting {\\em testing following screening} methods (Reiner, et al., 2007;\nYekutieli, 2008) in which independence between screening and testing is crucial\nto control the FDR, while the independence often requires additional data or\nsplitting of samples. An inappropriate splitting of samples may result in a\nloss rather than an improvement of statistical power. Instead, the uncorrelated\nscreening US is based on the original data and does not need to split the\nsamples. Theoretical results show that the US testing procedure controls the\ndesired FDR asymptotically.\n  Numerical studies are conducted and indicate that the proposed procedure\nworks quite well.\n", "versions": [{"version": "v1", "created": "Thu, 16 Oct 2014 02:52:12 GMT"}], "update_date": "2014-10-17", "authors_parsed": [["Liu", "Weidong", ""]]}, {"id": "1410.4723", "submitter": "Luke Keele", "authors": "Luke Keele, Sam Pimentel, Frank Yoon", "title": "Variable-Ratio Matching with Fine Balance in a Study of Peer Health\n  Exchange", "comments": "30 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies of treatment effects, matched samples are created so\ntreated and control groups are similar in terms of observable covariates.\nTraditionally such matched samples consist of matched pairs. If a pair match\nfails to make treated and control units sufficiently comparable, alternative\nforms of matching may be necessary. One general strategy to improve balance is\nto match a variable number of control units to each treated unit. A more\ntailored strategy is to adopt a fine balance constraint. Under a fine balance\nconstraint, a nominal covariate is exactly balanced, but it does not require\nindividually matched treated and control subjects for this variable. In the\nexample, we seek to construct a matched sample for an ongoing evaluation of\nPeer Health Exchange, an intervention in schools designed to decrease risky\nhealth behaviors among youth. We find that an optimal pair match that minimizes\ndistances between pairs creates a matched sample where balance is poor. Here we\npropose a method to allow for fine balance constraints when each treated unit\nis matched to a variable number of control units, which is not currently\npossible using existing matching algorithms. Our approach uses the entire\nnumber to first determine the optimal number of controls for each treated unit.\nFor each strata of matched treated units, we can then apply a fine balance\nconstraint. We then demonstrate that a matched sample for the evaluation of the\nPeer Health Exchange based on a variable number of controls and fine balance\nconstraint is superior to simply using a variable-ratio match.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 13:52:10 GMT"}, {"version": "v2", "created": "Mon, 20 Oct 2014 21:28:40 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Keele", "Luke", ""], ["Pimentel", "Sam", ""], ["Yoon", "Frank", ""]]}, {"id": "1410.4726", "submitter": "Anestis Touloumis", "authors": "Anestis Touloumis", "title": "Nonparametric Stein-type Shrinkage Covariance Matrix Estimators in\n  High-Dimensional Settings", "comments": "To appear in Computational Statistics and Data Analysis", "journal-ref": "Computational Statistics and Data Analysis (2015), pp. 251--261", "doi": "10.1016/j.csda.2014.10.018", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating a covariance matrix is an important task in applications where the\nnumber of variables is larger than the number of observations. Shrinkage\napproaches for estimating a high-dimensional covariance matrix are often\nemployed to circumvent the limitations of the sample covariance matrix. A new\nfamily of nonparametric Stein-type shrinkage covariance estimators is proposed\nwhose members are written as a convex linear combination of the sample\ncovariance matrix and of a predefined invertible target matrix. Under the\nFrobenius norm criterion, the optimal shrinkage intensity that defines the best\nconvex linear combination depends on the unobserved covariance matrix and it\nmust be estimated from the data. A simple but effective estimation process that\nproduces nonparametric and consistent estimators of the optimal shrinkage\nintensity for three popular target matrices is introduced. In simulations, the\nproposed Stein-type shrinkage covariance matrix estimator based on a scaled\nidentity matrix appeared to be up to 80% more efficient than existing ones in\nextreme high-dimensional settings. A colon cancer dataset was analyzed to\ndemonstrate the utility of the proposed estimators. A rule of thumb for adhoc\nselection among the three commonly used target matrices is recommended.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 14:01:04 GMT"}, {"version": "v2", "created": "Mon, 3 Nov 2014 11:42:30 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Touloumis", "Anestis", ""]]}, {"id": "1410.4732", "submitter": "Antonello Maruotti", "authors": "Alessandra Marcelletti, Antonello Maruotti, Giovanni Trovato", "title": "A flexible bivariate location-scale finite mixture approach to economic\n  growth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a multivariate multidimensional mixed-effects regression model\nin a finite mixture framework. We relax the usual unidimensionality assumption\non the random effects multivariate distribution. Thus, we introduce a\nmultidimensional multivariate discrete distribution for the random terms, with\na possibly different number of support points in each univariate profile,\nallowing for a full association structure. Our approach is motivated by the\nanalysis of economic growth. Accordingly, we define an extended version of the\naugmented Solow model. Indeed, we allow all model parameters, and not only the\nmean, to vary according to a regression model. Moreover, we argue that\ncountries do not follow the same growth process, and that a mixture-based\napproach can provide a natural framework for the detection of similar growth\npatterns. Our empirical findings provide evidence of heterogenous behaviors and\nsuggest the need of a flexible approach to properly reflect the heterogeneity\nin the data. We further test the behavior of the proposed approach via a\nsimulation study, considering several factors such as the number of observed\nunits, times and levels of heterogeneity in the data.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 14:16:45 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Marcelletti", "Alessandra", ""], ["Maruotti", "Antonello", ""], ["Trovato", "Giovanni", ""]]}, {"id": "1410.4778", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Tatsuya Kubokawa", "title": "Parametric Transformed Fay-Herriot Model for Small Area Estimation", "comments": null, "journal-ref": "Journal of Multivariate Analysis, 2015", "doi": "10.1016/j.jmva.2015.04.001", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider parametric transformed Fay-Herriot models, and\nclarify conditions on transformations under which the estimator of the\ntransformation is consistent. It is shown that the dual power transformation\nsatisfies the conditions. Based on asymptotic properties for estimators of\nparameters, we derive a second-order approximation of the prediction error of\nthe empirical best linear unbiased predictors (EBLUP) and obtain a second-order\nunbiased estimator of the prediction error. Finally, performances of the\nproposed procedures are investigated through simulation and empirical studies.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 15:57:56 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1410.4792", "submitter": "Tamara Broderick", "authors": "Tamara Broderick, Rebecca C. Steorts", "title": "Variational Bayes for Merging Noisy Databases", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian entity resolution merges together multiple, noisy databases and\nreturns the minimal collection of unique individuals represented, together with\ntheir true, latent record values. Bayesian methods allow flexible generative\nmodels that share power across databases as well as principled quantification\nof uncertainty for queries of the final, resolved database. However, existing\nBayesian methods for entity resolution use Markov monte Carlo method (MCMC)\napproximations and are too slow to run on modern databases containing millions\nor billions of records. Instead, we propose applying variational approximations\nto allow scalable Bayesian inference in these models. We derive a\ncoordinate-ascent approximation for mean-field variational Bayes, qualitatively\ncompare our algorithm to existing methods, note unique challenges for inference\nthat arise from the expected distribution of cluster sizes in entity\nresolution, and discuss directions for future work in this domain.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 16:46:45 GMT"}], "update_date": "2014-10-20", "authors_parsed": [["Broderick", "Tamara", ""], ["Steorts", "Rebecca C.", ""]]}, {"id": "1410.4827", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss, Andreas Neudecker, Simon Anders, Julien Gagneur", "title": "BADER: Bayesian analysis of differential expression in RNA sequencing\n  data", "comments": "14 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying differentially expressed genes from RNA sequencing data remains a\nchallenging task because of the considerable uncertainties in parameter\nestimation and the small sample sizes in typical applications. Here we\nintroduce Bayesian Analysis of Differential Expression in RNA-sequencing data\n(BADER). Due to our choice of data and prior distributions, full posterior\ninference for BADER can be carried out efficiently. The method appropriately\ntakes uncertainty in gene variance into account, leading to higher power than\nexisting methods in detecting differentially expressed genes. Moreover, we show\nthat the posterior samples can be naturally integrated into downstream gene set\nenrichment analyses, with excellent performance in detecting enriched sets. An\nopen-source R package (BADER) that provides a user-friendly interface to a C++\nback-end is available on Bioconductor.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 19:17:44 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 23:53:19 GMT"}], "update_date": "2014-11-11", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Neudecker", "Andreas", ""], ["Anders", "Simon", ""], ["Gagneur", "Julien", ""]]}, {"id": "1410.4856", "submitter": "Silvia Bacci Dr", "authors": "Silvia Bacci and Francesco Bartolucci", "title": "A multidimensional latent class IRT model for non-ignorable missing\n  responses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a structural equation model, which reduces to a multidimensional\nlatent class item response theory model, for the analysis of binary item\nresponses with non-ignorable missingness. The missingness mechanism is driven\nby two sets of latent variables: one describing the propensity to respond and\nthe other referred to the abilities measured by the test items. These latent\nvariables are assumed to have a discrete distribution, so as to reduce the\nnumber of parametric assumptions regarding the latent structure of the model.\nIndividual covariates may also be included through a multinomial logistic\nparametrization of the probabilities of each support point of the distribution\nof the latent variables. Given the discrete nature of this distribution, the\nproposed model is efficiently estimated by the Expectation-Maximization\nalgorithm. A simulation study is performed to evaluate the finite sample\nproperties of the parameter estimates. Moreover, an application is illustrated\nto data coming from a Students' Entry Test for the admission to some university\ncourses.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 20:40:52 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Bacci", "Silvia", ""], ["Bartolucci", "Francesco", ""]]}, {"id": "1410.4871", "submitter": "Herwig Wendt", "authors": "S\\'ebastien Combrexelle, Herwig Wendt, Nicolas Dobigeon, Jean-Yves\n  Tourneret, Steve McLaughlin and Patrice Abry", "title": "Bayesian estimation of the multifractality parameter for image texture\n  using a Whittle approximation", "comments": null, "journal-ref": "IEEE T. Image Proces., vol. 24, no. 8, pp. 2540-2551, Aug. 2015", "doi": "10.1109/TIP.2015.2426021", "report-no": null, "categories": "physics.data-an cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Texture characterization is a central element in many image processing\napplications. Multifractal analysis is a useful signal and image processing\ntool, yet, the accurate estimation of multifractal parameters for image texture\nremains a challenge. This is due in the main to the fact that current\nestimation procedures consist of performing linear regressions across frequency\nscales of the two-dimensional (2D) dyadic wavelet transform, for which only a\nfew such scales are computable for images. The strongly non-Gaussian nature of\nmultifractal processes, combined with their complicated dependence structure,\nmakes it difficult to develop suitable models for parameter estimation. Here,\nwe propose a Bayesian procedure that addresses the difficulties in the\nestimation of the multifractality parameter. The originality of the procedure\nis threefold: The construction of a generic semi-parametric statistical model\nfor the logarithm of wavelet leaders; the formulation of Bayesian estimators\nthat are associated with this model and the set of parameter values admitted by\nmultifractal theory; the exploitation of a suitable Whittle approximation\nwithin the Bayesian model which enables the otherwise infeasible evaluation of\nthe posterior distribution associated with the model. Performance is assessed\nnumerically for several 2D multifractal processes, for several image sizes and\na large range of process parameters. The procedure yields significant benefits\nover current benchmark estimators in terms of estimation performance and\nability to discriminate between the two most commonly used classes of\nmultifractal process models. The gains in performance are particularly\npronounced for small image sizes, notably enabling for the first time the\nanalysis of image patches as small as 64x64 pixels.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 21:32:28 GMT"}, {"version": "v2", "created": "Thu, 9 Apr 2015 09:13:28 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Combrexelle", "S\u00e9bastien", ""], ["Wendt", "Herwig", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""], ["McLaughlin", "Steve", ""], ["Abry", "Patrice", ""]]}, {"id": "1410.5011", "submitter": "Michail Tsagris", "authors": "Michail Tsagris and Connie Stewart", "title": "A Dirichlet Regression Model for Compositional Data with Zeros", "comments": "Research article consisting of 18 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compositional data are met in many different fields, such as economics,\narchaeometry, ecology, geology and political sciences. Regression where the\ndependent variable is a composition is usually carried out via a log-ratio\ntransformation of the composition or via the Dirichlet distribution. However,\nwhen there are zero values in the data these two ways are not readily\napplicable. Suggestions for this problem exist, but most of them rely on\nsubstituting the zero values. In this paper we adjust the Dirichlet\ndistribution when covariates are present, in order to allow for zero values to\nbe present in the data, without modifying any values. To do so, we modify the\nlog-likelihood of the Dirichlet distribution to account for zero values.\nExamples and simulation studies exhibit the performance of the zero adjusted\nDirichlet regression.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 21:56:40 GMT"}, {"version": "v2", "created": "Wed, 22 Oct 2014 19:16:10 GMT"}, {"version": "v3", "created": "Fri, 6 Mar 2015 00:41:26 GMT"}, {"version": "v4", "created": "Mon, 9 Jan 2017 19:28:50 GMT"}, {"version": "v5", "created": "Tue, 6 Jun 2017 09:58:33 GMT"}, {"version": "v6", "created": "Wed, 7 Jun 2017 04:47:19 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Tsagris", "Michail", ""], ["Stewart", "Connie", ""]]}, {"id": "1410.5014", "submitter": "Johannes Lederer", "authors": "Didier Ch\\'etelat, Johannes Lederer, Joseph Salmon", "title": "Optimal Two-Step Prediction in Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional prediction typically comprises two steps: variable selection\nand subsequent least-squares refitting on the selected variables. However, the\nstandard variable selection procedures, such as the lasso, hinge on tuning\nparameters that need to be calibrated. Cross-validation, the most popular\ncalibration scheme, is computationally costly and lacks finite sample\nguarantees. In this paper, we introduce an alternative scheme, easy to\nimplement and both computationally and theoretically efficient.\n", "versions": [{"version": "v1", "created": "Sat, 18 Oct 2014 22:55:57 GMT"}, {"version": "v2", "created": "Thu, 6 Nov 2014 01:54:37 GMT"}, {"version": "v3", "created": "Tue, 6 Jun 2017 01:11:46 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Ch\u00e9telat", "Didier", ""], ["Lederer", "Johannes", ""], ["Salmon", "Joseph", ""]]}, {"id": "1410.5110", "submitter": "Michael Betancourt", "authors": "M. J. Betancourt, Simon Byrne, Samuel Livingstone, Mark Girolami", "title": "The Geometric Foundations of Hamiltonian Monte Carlo", "comments": "45 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Hamiltonian Monte Carlo has proven an empirical success, the lack of\na rigorous theoretical understanding of the algorithm has in many ways impeded\nboth principled developments of the method and use of the algorithm in\npractice. In this paper we develop the formal foundations of the algorithm\nthrough the construction of measures on smooth manifolds, and demonstrate how\nthe theory naturally identifies efficient implementations and motivates\npromising generalizations.\n", "versions": [{"version": "v1", "created": "Sun, 19 Oct 2014 20:16:02 GMT"}], "update_date": "2014-10-21", "authors_parsed": [["Betancourt", "M. J.", ""], ["Byrne", "Simon", ""], ["Livingstone", "Samuel", ""], ["Girolami", "Mark", ""]]}, {"id": "1410.5170", "submitter": "Abhik Ghosh", "authors": "Abhik Ghosh and Ayanendranath Basu", "title": "Robust and Efficient Parameter Estimation based on Censored Data with\n  Stochastic Covariates", "comments": "Pre-print, 29 pages", "journal-ref": "Statistics A Journal of Theoretical and Applied Statistics (2017)\n  Volume 51, Issue 4", "doi": "10.1080/02331888.2017.1318139", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of random censored life-time data along with some related stochastic\ncovariables is of great importance in many applied sciences like medical\nresearch, population studies and planning etc. The parametric estimation\ntechnique commonly used under this set-up is based on the efficient but\nnon-robust likelihood approach. In this paper, we propose a robust parametric\nestimator for the censored data with stochastic covariates based on the minimum\ndensity power divergence approach. The resulting estimator also has competitive\nefficiency with respect to the maximum likelihood estimator under pure data.\nThe strong robustness property of the proposed estimator with respect to the\npresence of outliers is examined and illustrated through an appropriate\nsimulation study in the context of censored regression with stochastic\ncovariates. Further, the theoretical asymptotic properties of the proposed\nestimator are also derived in terms of a general class of M-estimators based on\nthe estimating equation.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 07:30:42 GMT"}, {"version": "v2", "created": "Wed, 24 Dec 2014 09:42:09 GMT"}, {"version": "v3", "created": "Fri, 1 Jul 2016 07:31:06 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Ghosh", "Abhik", ""], ["Basu", "Ayanendranath", ""]]}, {"id": "1410.5356", "submitter": "Ping He", "authors": "Ning Sui, Min Li, Ping He", "title": "Statistical computation of Boltzmann entropy and estimation of the\n  optimal probability density function from statistical sample", "comments": "8 pages, 6 figures, MNRAS, in the press", "journal-ref": "MNRAS (2014), 445, 4211 - 4217", "doi": "10.1093/mnras/stu2040", "report-no": null, "categories": "stat.ME physics.comp-ph physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate the statistical computation of the Boltzmann\nentropy of statistical samples. For this purpose, we use both histogram and\nkernel function to estimate the probability density function of statistical\nsamples. We find that, due to coarse-graining, the entropy is a monotonic\nincreasing function of the bin width for histogram or bandwidth for kernel\nestimation, which seems to be difficult to select an optimal bin\nwidth/bandwidth for computing the entropy. Fortunately, we notice that there\nexists a minimum of the first derivative of entropy for both histogram and\nkernel estimation, and this minimum point of the first derivative\nasymptotically points to the optimal bin width or bandwidth. We have verified\nthese findings by large amounts of numerical experiments. Hence, we suggest\nthat the minimum of the first derivative of entropy be used as a selector for\nthe optimal bin width or bandwidth of density estimation. Moreover, the optimal\nbandwidth selected by the minimum of the first derivative of entropy is purely\ndata-based, independent of the unknown underlying probability density\ndistribution, which is obviously superior to the existing estimators. Our\nresults are not restricted to one-dimensional, but can also be extended to\nmultivariate cases. It should be emphasized, however, that we do not provide a\nrobust mathematical proof of these findings, and we leave these issues with\nthose who are interested in them.\n", "versions": [{"version": "v1", "created": "Fri, 17 Oct 2014 16:08:02 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Sui", "Ning", ""], ["Li", "Min", ""], ["He", "Ping", ""]]}, {"id": "1410.5801", "submitter": "Saptarshi Das", "authors": "Valentina Bono, Wasifa Jamal, Saptarshi Das, Koushik Maharatna", "title": "Artifact reduction in multichannel pervasive EEG using hybrid WPT-ICA\n  and WPT-EMD signal decomposition techniques", "comments": "5 pages, 6 figures", "journal-ref": "Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE\n  International Conference on, pp. 5864 - 5868, May 2014", "doi": "10.1109/ICASSP.2014.6854728", "report-no": null, "categories": "physics.med-ph cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to reduce the muscle artifacts in multi-channel pervasive\nElectroencephalogram (EEG) signals, we here propose and compare two hybrid\nalgorithms by combining the concept of wavelet packet transform (WPT),\nempirical mode decomposition (EMD) and Independent Component Analysis (ICA).\nThe signal cleaning performances of WPT-EMD and WPT-ICA algorithms have been\ncompared using a signal-to-noise ratio (SNR)-like criterion for artifacts. The\nalgorithms have been tested on multiple trials of four different artifact cases\nviz. eye-blinking and muscle artifacts including left and right hand movement\nand head-shaking.\n", "versions": [{"version": "v1", "created": "Mon, 20 Oct 2014 16:57:17 GMT"}], "update_date": "2014-10-22", "authors_parsed": [["Bono", "Valentina", ""], ["Jamal", "Wasifa", ""], ["Das", "Saptarshi", ""], ["Maharatna", "Koushik", ""]]}, {"id": "1410.5885", "submitter": "Ju Hyun Kim", "authors": "Ju Hyun Kim", "title": "Identifying the Distribution of Treatment Effects under Support\n  Restrictions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distribution of treatment effects (DTE) is often of interest in the\ncontext of welfare policy evaluation. In this paper, I consider partial\nidentification of the DTE under known marginal distributions and support\nrestrictions on the potential outcomes. Examples of such support restrictions\ninclude monotone treatment response, concave treatment response, convex\ntreatment response, and the Roy model of self-selection. To establish\ninformative bounds on the DTE, I formulate the problem as an optimal\ntransportation linear program and develop a new dual representation to\ncharacterize the identification region with respect to the known marginal\ndistributions. I use this result to derive informative bounds for concrete\neconomic examples. I also propose an estimation procedure and illustrate the\nusefulness of my approach in the context of an empirical analysis of the\neffects of smoking on infant birth weight. The empirical results show that\nmonotone treatment response has a substantial identifying power for the DTE\nwhen the marginal distributions of the potential outcomes are given.\n", "versions": [{"version": "v1", "created": "Tue, 21 Oct 2014 23:41:02 GMT"}], "update_date": "2014-10-23", "authors_parsed": [["Kim", "Ju Hyun", ""]]}, {"id": "1410.5899", "submitter": "Alen Alexanderian", "authors": "Alen Alexanderian, Noemi Petra, Georg Stadler, and Omar Ghattas", "title": "A Fast and Scalable Method for A-Optimal Design of Experiments for\n  Infinite-dimensional Bayesian Nonlinear Inverse Problems", "comments": "30 pages; minor revisions; accepted for publication in SIAM Journal\n  on Scientific Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of optimal experimental design (OED) for Bayesian\nnonlinear inverse problems governed by PDEs. The goal is to find a placement of\nsensors, at which experimental data are collected, so as to minimize the\nuncertainty in the inferred parameter field. We formulate the OED objective\nfunction by generalizing the classical A-optimal experimental design criterion\nusing the expected value of the trace of the posterior covariance. We seek a\nmethod that solves the OED problem at a cost (measured in the number of forward\nPDE solves) that is independent of both the parameter and sensor dimensions. To\nfacilitate this, we construct a Gaussian approximation to the posterior at the\nmaximum a posteriori probability (MAP) point, and use the resulting covariance\noperator to define the OED objective function. We use randomized trace\nestimation to compute the trace of this (implicitly defined) covariance\noperator. The resulting OED problem includes as constraints the PDEs\ncharacterizing the MAP point, and the PDEs describing the action of the\ncovariance operator to vectors. The sparsity of the sensor configurations is\ncontrolled using sparsifying penalty functions. We elaborate our OED method for\nthe problem of determining the sensor placement to best infer the coefficient\nof an elliptic PDE. Adjoint methods are used to compute the gradient of the\nPDE-constrained OED objective function. We provide numerical results for\ninference of the permeability field in a porous medium flow problem, and\ndemonstrate that the number of PDE solves required for the evaluation of the\nOED objective function and its gradient is essentially independent of both the\nparameter and sensor dimensions. The number of quasi-Newton iterations for\ncomputing an OED also exhibits the same dimension invariance properties.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 02:31:07 GMT"}, {"version": "v2", "created": "Tue, 3 Nov 2015 05:01:12 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Alexanderian", "Alen", ""], ["Petra", "Noemi", ""], ["Stadler", "Georg", ""], ["Ghattas", "Omar", ""]]}, {"id": "1410.6002", "submitter": "J. Martin van Zyl", "authors": "J. Martin van Zyl", "title": "Estimating the Tail Index by using Model Averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.OT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ideas of model averaging are used to find weights in peak-over-threshold\nproblems using a possible range of thresholds. A range of the largest\nobservations are chosen and considered as possible thresholds, each time\nperforming estimation. Weights based on an information criterion for each\nthreshold are calculated. A weighted estimate of the threshold and shape\nparameter can be calculated.\n", "versions": [{"version": "v1", "created": "Wed, 22 Oct 2014 11:20:45 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 10:17:21 GMT"}], "update_date": "2014-10-30", "authors_parsed": [["van Zyl", "J. Martin", ""]]}, {"id": "1410.6413", "submitter": "Vladimir Bochkarev", "authors": "Vladimir V. Bochkarev and Yulia S. Maslennikova", "title": "Initialization of multilayer forecasting artifical neural networks", "comments": "9 pages, 3 figures", "journal-ref": "Uchenye Zapiski Kazanskogo Universiteta. Seriya\n  Fiziko-Matematicheskie Nauki, 2010, vol. 152, no. 1, pp. 7-14. (In Russian)", "doi": null, "report-no": null, "categories": "cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new method was developed for initialising artificial neural\nnetworks predicting dynamics of time series. Initial weighting coefficients\nwere determined for neurons analogously to the case of a linear prediction\nfilter. Moreover, to improve the accuracy of the initialization method for a\nmultilayer neural network, some variants of decomposition of the transformation\nmatrix corresponding to the linear prediction filter were suggested. The\nefficiency of the proposed neural network prediction method by forecasting\nsolutions of the Lorentz chaotic system is shown in this paper.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 16:54:39 GMT"}], "update_date": "2014-10-24", "authors_parsed": [["Bochkarev", "Vladimir V.", ""], ["Maslennikova", "Yulia S.", ""]]}, {"id": "1410.6556", "submitter": "Jin-Ting  Zhang", "authors": "Ming-Yen Cheng, Toshio Honda, and Jin-Ting Zhang", "title": "Forward variable selection for sparse ultra-high dimensional varying\n  coefficient models", "comments": "33 pages, 5 figures and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Varying coefficient models have numerous applications in a wide scope of\nscientific areas. While enjoying nice interpretability, they also allow\nflexibility in modeling dynamic impacts of the covariates. But, in the new era\nof big data, it is challenging to select the relevant variables when there are\na large number of candidates. Recently several work are focused on this\nimportant problem based on sparsity assumptions; they are subject to some\nlimitations, however. We introduce an appealing forward variable selection\nprocedure. It selects important variables sequentially according to a sum of\nsquares criterion, and it employs an EBIC- or BIC-based stopping rule. Clearly\nit is simple to implement and fast to compute, and it possesses many other\ndesirable properties from both theoretical and numerical viewpoints. We\nestablish rigorous selection consistency results when either EBIC or BIC is\nused as the stopping criterion, under some mild regularity conditions. Notably,\nunlike existing methods, an extra screening step is not required to ensure\nselection consistency. Even if the regularity conditions fail to hold, our\nprocedure is still useful as an effective screening procedure in a less\nrestrictive setup. We carried out simulation and empirical studies to show the\nefficacy and usefulness of our procedure.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 03:23:00 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Cheng", "Ming-Yen", ""], ["Honda", "Toshio", ""], ["Zhang", "Jin-Ting", ""]]}, {"id": "1410.6558", "submitter": "Raja Giryes", "authors": "Raja Giryes", "title": "Sampling in the Analysis Transform Domain", "comments": "13 Pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many signal and image processing applications have benefited remarkably from\nthe fact that the underlying signals reside in a low dimensional subspace. One\nof the main models for such a low dimensionality is the sparsity one. Within\nthis framework there are two main options for the sparse modeling: the\nsynthesis and the analysis ones, where the first is considered the standard\nparadigm for which much more research has been dedicated. In it the signals are\nassumed to have a sparse representation under a given dictionary. On the other\nhand, in the analysis approach the sparsity is measured in the coefficients of\nthe signal after applying a certain transformation, the analysis dictionary, on\nit. Though several algorithms with some theory have been developed for this\nframework, they are outnumbered by the ones proposed for the synthesis\nmethodology.\n  Given that the analysis dictionary is either a frame or the two dimensional\nfinite difference operator, we propose a new sampling scheme for signals from\nthe analysis model that allows to recover them from their samples using any\nexisting algorithm from the synthesis model. The advantage of this new sampling\nstrategy is that it makes the existing synthesis methods with their theory also\navailable for signals from the analysis framework.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 03:32:07 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 22:35:05 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Giryes", "Raja", ""]]}, {"id": "1410.6604", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, Peichao Peng, David Dunson", "title": "Median Selection Subset Aggregation for Parallel Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For massive data sets, efficient computation commonly relies on distributed\nalgorithms that store and process subsets of the data on different machines,\nminimizing communication costs. Our focus is on regression and classification\nproblems involving many features. A variety of distributed algorithms have been\nproposed in this context, but challenges arise in defining an algorithm with\nlow communication, theoretical guarantees and excellent practical performance\nin general settings. We propose a MEdian Selection Subset AGgregation Estimator\n(message) algorithm, which attempts to solve these problems. The algorithm\napplies feature selection in parallel for each subset using Lasso or another\nmethod, calculates the `median' feature inclusion index, estimates coefficients\nfor the selected features in parallel for each subset, and then averages these\nestimates. The algorithm is simple, involves very minimal communication, scales\nefficiently in both sample and feature size, and has theoretical guarantees. In\nparticular, we show model selection consistency and coefficient estimation\nefficiency. Extensive experiments show excellent performance in variable\nselection, estimation, prediction, and computation time relative to usual\ncompetitors.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 07:52:55 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["Wang", "Xiangyu", ""], ["Peng", "Peichao", ""], ["Dunson", "David", ""]]}, {"id": "1410.6733", "submitter": "Jennifer Wadsworth", "authors": "J. L. Wadsworth", "title": "On the occurrence times of componentwise maxima and bias in likelihood\n  inference for multivariate max-stable distributions", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Full likelihood-based inference for high-dimensional multivariate extreme\nvalue distributions, or max-stable processes, is feasible when incorporating\noccurrence times of the maxima; without this information, $d$-dimensional\nlikelihood inference is usually precluded due to the large number of terms in\nthe likelihood. However, some studies have noted bias when performing\nhigh-dimensional inference that incorporates such event information,\nparticularly when dependence is weak. We elucidate this phenomenon, showing\nthat for unbiased inference in moderate dimensions, dimension $d$ should be of\na magnitude smaller than the square root of the number of vectors over which\none takes the componentwise maximum. A bias reduction technique is suggested\nand illustrated on the extreme value logistic model.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 16:43:28 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 17:04:20 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Wadsworth", "J. L.", ""]]}, {"id": "1410.6758", "submitter": "Ruth Heller", "authors": "Ruth Heller, Yair Heller, Shachar Kaufman, Barak Brill, Malka Gorfine", "title": "Consistent distribution-free $K$-sample and independence tests for\n  univariate random variables", "comments": "arXiv admin note: substantial text overlap with arXiv:1308.1559", "journal-ref": "Journal of Machine Learning Research (JMLR) 2016, vol. 17, No. 29,\n  1-54", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular approach for testing if two univariate random variables are\nstatistically independent consists of partitioning the sample space into bins,\nand evaluating a test statistic on the binned data. The partition size matters,\nand the optimal partition size is data dependent. While for detecting simple\nrelationships coarse partitions may be best, for detecting complex\nrelationships a great gain in power can be achieved by considering finer\npartitions. We suggest novel consistent distribution-free tests that are based\non summation or maximization aggregation of scores over all partitions of a\nfixed size. We show that our test statistics based on summation can serve as\ngood estimators of the mutual information. Moreover, we suggest regularized\ntests that aggregate over all partition sizes, and prove those are consistent\ntoo. We provide polynomial-time algorithms, which are critical for computing\nthe suggested test statistics efficiently. We show that the power of the\nregularized tests is excellent compared to existing tests, and almost as\npowerful as the tests based on the optimal (yet unknown in practice) partition\nsize, in simulations as well as on a real data example.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 18:07:15 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 12:58:57 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Heller", "Ruth", ""], ["Heller", "Yair", ""], ["Kaufman", "Shachar", ""], ["Brill", "Barak", ""], ["Gorfine", "Malka", ""]]}, {"id": "1410.6782", "submitter": "Michael Kolonko", "authors": "Bj\\\"orn G\\\"order and Michael Kolonko", "title": "Ranking and Selection: A New Sequential Bayesian Procedure for Use with\n  Common Random Numbers", "comments": "28 pages, 11 figures, extended discussion of literature, improved\n  arguments in section 2.5 on approximate distribution, extended empirical\n  comparisons", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We want to select the best systems out of a given set of systems (or rank\nthem) with respect to their expected performance. The systems allow random\nobservations only and we assume that the joint observation of the systems has a\nmultivariate normal distribution with unknown mean and covariance. We allow\ndependent marginal observations as they occur when common random numbers are\nused for the simulation of the systems. In particular, we focus on positively\ndependent observations as they might be expected in heuristic optimization\nwhere `systems' are different solutions to an optimization problem with common\nrandom inputs. In each iteration, we allocate a fixed budget of simulation runs\nto the solutions. We use a Bayesian setup and allocate the simulation effort\naccording to the posterior covariances of the solutions until the ranking and\nselection decision is correct with a given high probability. Here, the complex\nposterior distributions are approximated only but we give extensive empirical\nevidence that the observed error probabilities are well below the given bounds\nin most cases. We also use a generalized scheme for the target of the ranking\nand selection that allows to bound the error probabilities with a Bonferroni\napproach. Our test results show that our procedure uses less simulations than\ncomparable procedures from literature even in most of the cases where the\nobservations are not positively correlated.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 18:59:43 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2017 17:07:54 GMT"}], "update_date": "2017-01-23", "authors_parsed": [["G\u00f6rder", "Bj\u00f6rn", ""], ["Kolonko", "Michael", ""]]}, {"id": "1410.6784", "submitter": "Ivan Kojadinovic", "authors": "Axel B\\\"ucher and Ivan Kojadinovic", "title": "An overview of nonparametric tests of extreme-value dependence and of\n  some related statistical procedures", "comments": "21 pages, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An overview of existing nonparametric tests of extreme-value dependence is\npresented. Given an i.i.d.\\ sample of random vectors from a continuous\ndistribution, such tests aim at assessing whether the underlying unknown copula\nis of the {\\em extreme-value} type or not. The existing approaches available in\nthe literature are summarized according to how departure from extreme-value\ndependence is assessed. Related statistical procedures useful when modeling\ndata with this type of dependence are briefly described next. Two illustrations\non real data sets are then carried out using some of the statistical procedures\nunder consideration implemented in the \\textsf{R} package {\\tt copula}.\nFinally, the related problem of testing the {\\em maximum domain of attraction}\ncondition is discussed.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 19:04:29 GMT"}], "update_date": "2014-10-27", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Kojadinovic", "Ivan", ""]]}, {"id": "1410.6843", "submitter": "Tamara Broderick", "authors": "Tamara Broderick, Ashia C. Wilson, and Michael I. Jordan", "title": "Posteriors, conjugacy, and exponential families for completely random\n  measures", "comments": "42 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We demonstrate how to calculate posteriors for general CRM-based priors and\nlikelihoods for Bayesian nonparametric models. We further show how to represent\nBayesian nonparametric priors as a sequence of finite draws using a\nsize-biasing approach---and how to represent full Bayesian nonparametric models\nvia finite marginals. Motivated by conjugate priors based on exponential family\nrepresentations of likelihoods, we introduce a notion of exponential families\nfor CRMs, which we call exponential CRMs. This construction allows us to\nspecify automatic Bayesian nonparametric conjugate priors for exponential CRM\nlikelihoods. We demonstrate that our exponential CRMs allow particularly\nstraightforward recipes for size-biased and marginal representations of\nBayesian nonparametric models. Along the way, we prove that the gamma process\nis a conjugate prior for the Poisson likelihood process and the beta prime\nprocess is a conjugate prior for a process we call the odds Bernoulli process.\nWe deliver a size-biased representation of the gamma process and a marginal\nrepresentation of the gamma process coupled with a Poisson likelihood process.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 21:40:45 GMT"}, {"version": "v2", "created": "Fri, 22 Apr 2016 17:44:00 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Broderick", "Tamara", ""], ["Wilson", "Ashia C.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1410.6853", "submitter": "Ryan Giordano", "authors": "Ryan Giordano, Tamara Broderick", "title": "Covariance Matrices for Mean Field Variational Bayes", "comments": "14 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean Field Variational Bayes (MFVB) is a popular posterior approximation\nmethod due to its fast runtime on large-scale data sets. However, it is well\nknown that a major failing of MFVB is its (sometimes severe) underestimates of\nthe uncertainty of model variables and lack of information about model variable\ncovariance. We develop a fast, general methodology for exponential families\nthat augments MFVB to deliver accurate uncertainty estimates for model\nvariables -- both for individual variables and coherently across variables.\nMFVB for exponential families defines a fixed-point equation in the means of\nthe approximating posterior, and our approach yields a covariance estimate by\nperturbing this fixed point. Inspired by linear response theory, we call our\nmethod linear response variational Bayes (LRVB). We demonstrate the accuracy of\nour method on simulated data sets.\n", "versions": [{"version": "v1", "created": "Fri, 24 Oct 2014 23:31:44 GMT"}, {"version": "v2", "created": "Mon, 8 Dec 2014 22:07:47 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Giordano", "Ryan", ""], ["Broderick", "Tamara", ""]]}, {"id": "1410.6870", "submitter": "Robert Weiss", "authors": "Jihey Lee, Robert E. Weiss, Marc A. Suchard", "title": "Using a Birth-Death Process to Account for Reporting Errors in\n  Longitudinal Self-reported Counts of Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze longitudinal self-reported counts of sexual partners from youth\nliving with HIV. In self-reported survey data, subjects recall counts of events\nor behaviors such as the number of sexual partners or the number of drug uses\nin the past three months. Subjects with small counts may report the exact\nnumber, whereas subjects with large counts may have difficulty recalling the\nexact number. Thus, self-reported counts are noisy, and mis-reporting induces\nerrors in the count variable. As a naive method for analyzing self-reported\ncounts, the Poisson random effects model treats the observed counts as true\ncounts and reporting errors in the outcome variable are ignored. Inferences are\ntherefore based on incorrect information and may lead to conclusions\nunsupported by the data. We describe a Bayesian model for analyzing\nlongitudinal self-reported count data that formally accounts for reporting\nerror. We model reported counts conditional on underlying true counts using a\nlinear birth-death process and use a Poisson random effects model to model the\nunderlying true counts. A regression version of our model can identify\ncharacteristics of subjects with greater or lesser reporting error. We\ndemonstrate several approaches to prior specification.\n", "versions": [{"version": "v1", "created": "Sat, 25 Oct 2014 02:21:20 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Lee", "Jihey", ""], ["Weiss", "Robert E.", ""], ["Suchard", "Marc A.", ""]]}, {"id": "1410.7056", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts", "title": "Smoothing, Clustering, and Benchmarking for Small Area Estimation", "comments": "24 pages, 4 figures, Submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop constrained Bayesian estimation methods for small area problems:\nthose requiring smoothness with respect to similarity across areas, such as\ngeographic proximity or clustering by covariates; and benchmarking constraints,\nrequiring (weighted) means of estimates to agree across levels of aggregation.\nWe develop methods for constrained estimation decision-theoretically and\ndiscuss their geometric interpretation. Our constrained estimators are the\nsolutions to tractable optimization problems and have closed-form solutions.\nMean squared errors of the constrained estimators are calculated via\nbootstrapping. Our techniques are free of distributional assumptions and apply\nwhether the estimator is linear or non-linear, univariate or multivariate. We\nillustrate our methods using data from the U.S. Census's Small Area Income and\nPoverty Estimates program.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 16:24:41 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Steorts", "Rebecca C.", ""]]}, {"id": "1410.7074", "submitter": "Oscar Beijbom Mr", "authors": "Oscar Beijbom", "title": "Random Sampling in an Age of Automation: Minimizing Expenditures through\n  Balanced Collection and Annotation", "comments": "PDF contains 9 pages of manuscript followed by 7 pages of\n  Supplementary Information", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for automated collection and annotation are changing the\ncost-structures of sampling surveys for a wide range of applications. Digital\nsamples in the form of images or audio recordings can be collected rapidly, and\nannotated by computer programs or crowd workers. We consider the problem of\nestimating a population mean under these new cost-structures, and propose a\nHybrid-Offset sampling design. This design utilizes two annotators: a primary,\nwhich is accurate but costly (e.g. a human expert) and an auxiliary which is\nnoisy but cheap (e.g. a computer program), in order to minimize total sampling\nexpenditures. Our analysis gives necessary conditions for the Hybrid-Offset\ndesign and specifies optimal sample sizes for both annotators. Simulations on\ndata from a coral reef survey program indicate that the Hybrid-Offset design\noutperforms several alternative sampling designs. In particular, sampling\nexpenditures are reduced 50% compared to the Conventional design currently\ndeployed by the coral ecologists.\n", "versions": [{"version": "v1", "created": "Sun, 26 Oct 2014 20:12:32 GMT"}, {"version": "v2", "created": "Wed, 29 Oct 2014 15:59:30 GMT"}, {"version": "v3", "created": "Wed, 19 Nov 2014 01:19:14 GMT"}, {"version": "v4", "created": "Fri, 21 Aug 2015 16:18:15 GMT"}], "update_date": "2015-08-24", "authors_parsed": [["Beijbom", "Oscar", ""]]}, {"id": "1410.7241", "submitter": "Fabian Wauthier", "authors": "Fabian L. Wauthier and Peter Donnelly", "title": "A Greedy Homotopy Method for Regression with Nonconvex Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained least squares regression is an essential tool for\nhigh-dimensional data analysis. Given a partition $\\mathcal{G}$ of input\nvariables, this paper considers a particular class of nonconvex constraint\nfunctions that encourage the linear model to select a small number of variables\nfrom a small number of groups in $\\mathcal{G}$. Such constraints are relevant\nin many practical applications, such as Genome-Wide Association Studies (GWAS).\nMotivated by the efficiency of the Lasso homotopy method, we present RepLasso,\na greedy homotopy algorithm that tries to solve the induced sequence of\nnonconvex problems by solving a sequence of suitably adapted convex surrogate\nproblems. We prove that in some situations RepLasso recovers the global minima\nof the nonconvex problem. Moreover, even if it does not recover global minima,\nwe prove that in relevant cases it will still do no worse than the Lasso in\nterms of support and signed support recovery, while in practice outperforming\nit. We show empirically that the strategy can also be used to improve over\nother Lasso-style algorithms. Finally, a GWAS of ankylosing spondylitis\nhighlights our method's practical utility.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 13:54:59 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Wauthier", "Fabian L.", ""], ["Donnelly", "Peter", ""]]}, {"id": "1410.7279", "submitter": "Johannes Lederer", "authors": "Johannes Lederer, Christian M\\\"uller", "title": "Topology Adaptive Graph Estimation in High Dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Graphical TREX (GTREX), a novel method for graph estimation in\nhigh-dimensional Gaussian graphical models. By conducting neighborhood\nselection with TREX, GTREX avoids tuning parameters and is adaptive to the\ngraph topology. We compare GTREX with standard methods on a new simulation\nset-up that is designed to assess accurately the strengths and shortcomings of\ndifferent methods. These simulations show that a neighborhood selection scheme\nbased on Lasso and an optimal (in practice unknown) tuning parameter\noutperforms other standard methods over a large spectrum of scenarios.\nMoreover, we show that GTREX can rival this scheme and, therefore, can provide\ncompetitive graph estimation without the need for tuning parameter calibration.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 15:41:23 GMT"}], "update_date": "2014-10-28", "authors_parsed": [["Lederer", "Johannes", ""], ["M\u00fcller", "Christian", ""]]}, {"id": "1410.7424", "submitter": "Hans De Raedt", "authors": "Marian Kupczynski and Hans De Raedt", "title": "Breakdown of statistical inference from some random experiments", "comments": "Accepted for publication in Computer Physics Communications", "journal-ref": "Computer Physics Communications 200 168 (2016)", "doi": "10.1016/j.cpc.2015.11.010", "report-no": null, "categories": "physics.data-an quant-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many experiments can be interpreted in terms of random processes operating\naccording to some internal protocols. When experiments are costly or cannot be\nrepeated only one or a few finite samples are available. In this paper we study\ndata generated by pseudo-random computer experiments operating according to\nparticular internal protocols. We show that the standard statistical analysis\nperformed on a sample, containing 100000 data points or more, may sometimes be\nhighly misleading and statistical errors largely underestimated. Our results\nconfirm in a dramatic way the dangers of standard asymptotic statistical\ninference if a sample is not homogenous. We demonstrate that analyzing various\nsubdivisions of samples by multiple chi-square tests and chi-square frequency\ngraphs is very effective in detecting sample inhomogeneity. Therefore to assure\ncorrectness of the statistical inference the above mentioned chi-square tests\nand other non-parametric sample homogeneity tests should be incorporated in any\nstatistical analysis of experimental data. If such tests are not performed the\nreported conclusions and estimates of the errors cannot be trusted.\n", "versions": [{"version": "v1", "created": "Mon, 27 Oct 2014 20:41:31 GMT"}, {"version": "v2", "created": "Sat, 25 Apr 2015 17:24:09 GMT"}, {"version": "v3", "created": "Thu, 10 Sep 2015 12:14:31 GMT"}, {"version": "v4", "created": "Sun, 22 Nov 2015 07:58:08 GMT"}], "update_date": "2016-02-02", "authors_parsed": [["Kupczynski", "Marian", ""], ["De Raedt", "Hans", ""]]}, {"id": "1410.7554", "submitter": "Nicolas Brunel", "authors": "Nicolas Brunel and Quentin Clairon", "title": "A Tracking Approach to Parameter Estimation in Linear Ordinary\n  Differential Equations", "comments": "41 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary Differential Equations are widespread tools to model chemical,\nphysical, biological process but they usually rely on parameters which are of\ncritical importance in terms of dynamic and need to be estimated directly from\nthe data. Classical statistical approaches (nonlinear least squares, maximum\nlikelihood estimator) can give unsatisfactory results because of computational\ndifficulties and ill-posedness of the statistical problem. New estimation\nmethods that use some nonparametric devices have been proposed to circumvent\nthese issues. We present a new estimator that shares properties with Two-Step\nestimator and Generalized Smoothing (introduced by Ramsay et al, 2007). We\nintroduce a perturbed model and we use optimal control theory for constructing\na criterion that aims at minimizing the discrepancy with data and the model.\nHere, we focus on the case of linear Ordinary Differential Equations as our\ncriterion has a closed-form expression that permits a detailed analysis. Our\napproach avoids the use of a nonparametric estimator of the derivative, which\nis one of the main cause of inaccuracy in Two-Step estimators. Moreover, we\ntake into account model discrepancy and our estimator is more robust to model\nmisspecification than classical methods. The discrepancy with the parametric\nODE model correspond to the minimum perturbation (or control) to apply to the\ninitial model. Its qualitative analysis can be informative for misspecification\ndiagnosis. In the case of well-specified model, we show the consistency of our\nestimator and that we reach the parametric root-n rate when regression splines\nare used in the first step.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 08:51:47 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Brunel", "Nicolas", ""], ["Clairon", "Quentin", ""]]}, {"id": "1410.7558", "submitter": "Nicolas Brunel", "authors": "Quentin Clairon and Nicolas Brunel", "title": "State and Parameter Estimation of Partially Observed Linear Ordinary\n  Differential Equations with Deterministic Optimal Control", "comments": "45 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary Differential Equations are a simple but powerful framework for\nmodeling complex systems. Parameter estimation from times series can be done by\nNonlinear Least Squares (or other classical approaches), but this can give\nunsatisfactory results because the inverse problem can be ill-posed, even when\nthe differential equation is linear.\n  Following recent approaches that use approximate solutions of the ODE model,\nwe propose a new method that converts parameter estimation into an optimal\ncontrol problem: our objective is to determine a control and a parameter that\nare as close as possible to the data. We derive then a criterion that makes a\nbalance between discrepancy with data and with the model, and we minimize it by\nusing optimization in functions spaces: our approach is related to the\nso-called Deterministic Kalman Filtering, but different from the usual\nstatistical Kalman filtering. e show the root-$n$ consistency and asymptotic\nnormality of the estimators for the parameter and for the states. Experiments\nin a toy model and in a real case shows that our approach is generally more\naccurate and more reliable than Nonlinear Least Squares and Generalized\nSmoothing, even in misspecified cases.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 09:01:48 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Clairon", "Quentin", ""], ["Brunel", "Nicolas", ""]]}, {"id": "1410.7566", "submitter": "Nicolas Brunel", "authors": "Nicolas J-B Brunel, Quentin Clairon, Florence d'Alche-Buc", "title": "Parametric Estimation of Ordinary Differential Equations with\n  Orthogonality Conditions", "comments": "35 pages, 5 figures", "journal-ref": "Brunel, N. JB, Clairon, Q., d Alche Buc, F. (2014). Parametric\n  Estimation of Ordinary Differential Equations With Orthogonality Conditions.\n  Journal of the American Statistical Association, 109(505), 173-185", "doi": "10.1080/01621459.2013.841583", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential equations are commonly used to model dynamical deterministic\nsystems in applications. When statistical parameter estimation is required to\ncalibrate theoretical models to data, classical statistical estimators are\noften confronted to complex and potentially ill-posed optimization problem. As\na consequence, alternative estimators to classical parametric estimators are\nneeded for obtaining reliable estimates. We propose a gradient matching\napproach for the estimation of parametric Ordinary Differential Equations\nobserved with noise. Starting from a nonparametric proxy of a true solution of\nthe ODE, we build a parametric estimator based on a variational\ncharacterization of the solution. As a Generalized Moment Estimator, our\nestimator must satisfy a set of orthogonal conditions that are solved in the\nleast squares sense. Despite the use of a nonparametric estimator, we prove the\nroot-$n$ consistency and asymptotic normality of the Orthogonal Conditions\nestimator. We can derive confidence sets thanks to a closed-form expression for\nthe asymptotic variance. Finally, the OC estimator is compared to classical\nestimators in several (simulated and real) experiments and ODE models in order\nto show its versatility and relevance with respect to classical Gradient\nMatching and Nonlinear Least Squares estimators. In particular, we show on a\nreal dataset of influenza infection that the approach gives reliable estimates.\nMoreover, we show that our approach can deal directly with more elaborated\nmodels such as Delay Differential Equation (DDE).\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 09:42:46 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Brunel", "Nicolas J-B", ""], ["Clairon", "Quentin", ""], ["d'Alche-Buc", "Florence", ""]]}, {"id": "1410.7616", "submitter": "Holger Dette", "authors": "Holger Dette and Kirsten Schorning", "title": "Optimal designs for comparing curves", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the optimal design problem for a comparison of two regression\ncurves, which is used to establish the similarity between the dose response\nrelationships of two groups. An optimal pair of designs minimizes the width of\nthe confidence band for the difference between the two regression functions.\nOptimal design theory (equivalence theorems, efficiency bounds) is developed\nfor this non standard design problem and for some commonly used dose response\nmodels optimal designs are found explicitly. The results are illustrated in\nseveral examples modeling dose response relationships. It is demonstrated that\nthe optimal pair of designs for the comparison of the regression curves is not\nthe pair of the optimal designs for the individual models. In particular it is\nshown that the use of the optimal designs proposed in this paper instead of\ncommonly used \"non-optimal\" designs yields a reduction of the width of the\nconfidence band by more than 50%.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 13:38:14 GMT"}, {"version": "v2", "created": "Tue, 18 Nov 2014 13:13:00 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Dette", "Holger", ""], ["Schorning", "Kirsten", ""]]}, {"id": "1410.7690", "submitter": "Yu-Xiang Wang", "authors": "Yu-Xiang Wang, James Sharpnack, Alex Smola, Ryan J. Tibshirani", "title": "Trend Filtering on Graphs", "comments": "A short version appeared in AISTATS'2015", "journal-ref": "Journal of Machine Learning Research Volume (2016) Volume 17\n  Article 15-147", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of adaptive estimators on graphs, based on penalizing\nthe $\\ell_1$ norm of discrete graph differences. This generalizes the idea of\ntrend filtering [Kim et al. (2009), Tibshirani (2014)], used for univariate\nnonparametric regression, to graphs. Analogous to the univariate case, graph\ntrend filtering exhibits a level of local adaptivity unmatched by the usual\n$\\ell_2$-based graph smoothers. It is also defined by a convex minimization\nproblem that is readily solved (e.g., by fast ADMM or Newton algorithms). We\ndemonstrate the merits of graph trend filtering through examples and theory.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 16:22:32 GMT"}, {"version": "v2", "created": "Wed, 12 Nov 2014 01:21:54 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2015 06:16:02 GMT"}, {"version": "v4", "created": "Wed, 12 Aug 2015 00:42:15 GMT"}, {"version": "v5", "created": "Sat, 4 Jun 2016 17:03:24 GMT"}], "update_date": "2016-06-07", "authors_parsed": [["Wang", "Yu-Xiang", ""], ["Sharpnack", "James", ""], ["Smola", "Alex", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1410.7692", "submitter": "Ye Wang", "authors": "Ye Wang, Antonio Canale, David Dunson", "title": "Scalable multiscale density estimation", "comments": "9 pages with references, 7 pages appendix, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although Bayesian density estimation using discrete mixtures has good\nperformance in modest dimensions, there is a lack of statistical and\ncomputational scalability to high-dimensional multivariate cases. To combat the\ncurse of dimensionality, it is necessary to assume the data are concentrated\nnear a lower-dimensional subspace. However, Bayesian methods for learning this\nsubspace along with the density of the data scale poorly computationally. To\nsolve this problem, we propose an empirical Bayes approach, which estimates a\nmultiscale dictionary using geometric multiresolution analysis in a first\nstage. We use this dictionary within a multiscale mixture model, which allows\nuncertainty in component allocation, mixture weights and scaling factors over a\nbinary tree. A computational algorithm is proposed, which scales efficiently to\nmassive dimensional problems. We provide some theoretical support for this\ngeometric density estimation (GEODE) method, and illustrate the performance\nthrough simulated and real data examples.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 16:29:29 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Wang", "Ye", ""], ["Canale", "Antonio", ""], ["Dunson", "David", ""]]}, {"id": "1410.7748", "submitter": "Jonathan Bradley", "authors": "Jonathan R. Bradley, Noel Cressie, Tao Shi", "title": "A Comparison of Spatial Predictors when Datasets Could be Very Large", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we review and compare a number of methods of spatial\nprediction. To demonstrate the breadth of available choices, we consider both\ntraditional and more-recently-introduced spatial predictors. Specifically, in\nour exposition we review: traditional stationary kriging, smoothing splines,\nnegative-exponential distance-weighting, Fixed Rank Kriging, modified\npredictive processes, a stochastic partial differential equation approach, and\nlattice kriging. This comparison is meant to provide a service to practitioners\nwishing to decide between spatial predictors. Hence, we provide technical\nmaterial for the unfamiliar, which includes the definition and motivation for\neach (deterministic and stochastic) spatial predictor. We use a benchmark\ndataset of $\\mathrm{CO}_{2}$ data from NASA's AIRS instrument to address\ncomputational efficiencies that include CPU time and memory usage. Furthermore,\nthe predictive performance of each spatial predictor is assessed empirically\nusing a hold-out subset of the AIRS data.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 19:21:16 GMT"}], "update_date": "2014-10-29", "authors_parsed": [["Bradley", "Jonathan R.", ""], ["Cressie", "Noel", ""], ["Shi", "Tao", ""]]}, {"id": "1410.7812", "submitter": "Mingyuan Zhou", "authors": "Mingyuan Zhou", "title": "Beta-Negative Binomial Process and Exchangeable Random Partitions for\n  Mixed-Membership Modeling", "comments": "in Neural Information Processing Systems (NIPS) 2014. 9 pages + 3\n  page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The beta-negative binomial process (BNBP), an integer-valued stochastic\nprocess, is employed to partition a count vector into a latent random count\nmatrix. As the marginal probability distribution of the BNBP that governs the\nexchangeable random partitions of grouped data has not yet been developed,\ncurrent inference for the BNBP has to truncate the number of atoms of the beta\nprocess. This paper introduces an exchangeable partition probability function\nto explicitly describe how the BNBP clusters the data points of each group into\na random number of exchangeable partitions, which are shared across all the\ngroups. A fully collapsed Gibbs sampler is developed for the BNBP, leading to a\nnovel nonparametric Bayesian topic model that is distinct from existing ones,\nwith simple implementation, fast convergence, good mixing, and state-of-the-art\npredictive performance.\n", "versions": [{"version": "v1", "created": "Tue, 28 Oct 2014 21:08:39 GMT"}, {"version": "v2", "created": "Wed, 31 Dec 2014 22:52:17 GMT"}], "update_date": "2015-01-05", "authors_parsed": [["Zhou", "Mingyuan", ""]]}, {"id": "1410.7874", "submitter": "James Sharpnack", "authors": "James Sharpnack and Mladen Kolar", "title": "Mean and variance estimation in high-dimensional heteroscedastic models\n  with non-convex penalties", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite its prevalence in statistical datasets, heteroscedasticity\n(non-constant sample variances) has been largely ignored in the\nhigh-dimensional statistics literature. Recently, studies have shown that the\nLasso can accommodate heteroscedastic errors, with minor algorithmic\nmodifications (Belloni et al., 2012; Gautier and Tsybakov, 2013). In this work,\nwe study heteroscedastic regression with linear mean model and log-linear\nvariances model with sparse high-dimensional parameters. In this work, we\npropose estimating variances in a post-Lasso fashion, which is followed by\nweighted-least squares mean estimation. These steps employ non-convex penalties\nas in Fan and Li (2001), which allows us to prove oracle properties for both\npost-Lasso variance and mean parameter estimates. We reinforce our theoretical\nfindings with experiments.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 05:06:52 GMT"}, {"version": "v2", "created": "Thu, 30 Oct 2014 01:51:38 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Sharpnack", "James", ""], ["Kolar", "Mladen", ""]]}, {"id": "1410.8093", "submitter": "Cinzia Viroli", "authors": "Elisabetta Bonafede, Franck Picard, St\\'ephane Robin and Cinzia Viroli", "title": "Modelling overdispersion heterogeneity in differential expression\n  analysis using mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Next-generation sequencing technologies now constitute a method of choice to\nmeasure gene expression. Data to analyze are read counts, commonly modeled\nusing Negative Binomial distributions. A relevant issue associated with this\nprobabilistic framework is the reliable estimation of the overdispersion\nparameter, reinforced by the limited number of replicates generally observable\nfor each gene. Many strategies have been proposed to estimate this parameter,\nbut when differential analysis is the purpose, they often result in procedures\nbased on plug-in estimates, and we show here that this discrepancy between the\nestimation framework and the testing framework can lead to uncontrolled type-I\nerrors. Instead we propose a mixture model that allows each gene to share\ninformation with other genes that exhibit similar variability. Three consistent\nstatistical tests are developed for differential expression analysis. We show\nthat the proposed method improves the sensitivity of detecting differentially\nexpressed genes with respect to the common procedures, since it is the best one\nin reaching the nominal value for the first-type error, while keeping elevate\npower. The method is finally illustrated on prostate cancer RNA-seq data.\n", "versions": [{"version": "v1", "created": "Thu, 23 Oct 2014 17:52:32 GMT"}, {"version": "v2", "created": "Fri, 7 Nov 2014 08:26:00 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Bonafede", "Elisabetta", ""], ["Picard", "Franck", ""], ["Robin", "St\u00e9phane", ""], ["Viroli", "Cinzia", ""]]}, {"id": "1410.8165", "submitter": "Ali Akbar Jafari", "authors": "Mohammad Reza Kazemi and Ali Akbar Jafari", "title": "Comparing Seventeen Interval Estimates for a Bivariate Normal\n  Correlation Coefficient", "comments": "Accepted for publication Journal of Statistics Applications &\n  Probability Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of constructing confidence interval\nfor the correlation coefficient in a bivariate normal distribution. For this\nproblem, we found fifteen approaches in literatures. Also, we have proposed a\ngeneralized confidence interval and a parametric bootstrap confidence interval.\nThe coverage probabilities and expected lengths of these seventeen approaches\nare evaluated and compared via simulation study. In addition, robustness of the\nmethods is considered in the comparisons by the non-normal distributions. Two\nreal examples are given to illustrate the approaches.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 21:04:27 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Kazemi", "Mohammad Reza", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1410.8167", "submitter": "Ali Akbar Jafari", "authors": "Mohammad Reza Kazemi and Ali Akbar Jafari", "title": "Characterization of Order Statistics in Two Runs Using Conditional\n  Expectation", "comments": "Accepted for publication in Model Assisted Statistics and\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The runs test is a well-known test that is used for checking independence\nbetween elements of a sample data sequence. Some of runs tests are based on the\nlongest run and others based on the total runs. In this paper, we consider\norder statistics of two runs statistics, and obtain their probability mass\nfunctions. In addition, the means and variances of the order statistics are\nderived using traditional conditional expectation.\n", "versions": [{"version": "v1", "created": "Wed, 29 Oct 2014 21:14:15 GMT"}], "update_date": "2014-10-31", "authors_parsed": [["Kazemi", "Mohammad Reza", ""], ["Jafari", "Ali Akbar", ""]]}, {"id": "1410.8209", "submitter": "Shirin Golchi", "authors": "Shirin Golchi, David A. Campbell", "title": "Sequentially Constrained Monte Carlo", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraints can be interpreted in a broad sense as any kind of explicit\nrestriction over the parameters. While some constraints are defined directly on\nthe parameter space, when they are instead defined by known behaviour on the\nmodel, transformation of constraints into features on the parameter space may\nnot be possible. Difficulties in sampling from the posterior distribution as a\nresult of incorporation of constraints into the model is a common challenge\nleading to truncations in the parameter space and inefficient sampling\nalgorithms. We propose a variant of sequential Monte Carlo algorithm for\nposterior sampling in presence of constraints by defining a sequence of\ndensities through the imposition of the constraint. Particles generated from an\nunconstrained or mildly constrained distribution are filtered and moved through\nsampling and resampling steps to obtain a sample from the fully constrained\ntarget distribution. General and model specific forms of constraints enforcing\nstrategies are defined. The Sequentially Constrained Monte Carlo algorithm is\ndemonstrated on constraints defined by monotonicity of a function, densities\nconstrained to low dimensional manifolds, adherence to a theoretically derived\nmodel, and model feature matching.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 00:28:16 GMT"}, {"version": "v2", "created": "Wed, 25 Feb 2015 21:38:16 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Golchi", "Shirin", ""], ["Campbell", "David A.", ""]]}, {"id": "1410.8249", "submitter": "Stefan Siegert", "authors": "Stefan Siegert, Christopher A.T. Ferro, David B. Stephenson", "title": "Evaluating ensemble forecasts by the Ignorance score -- Correcting the\n  finite-ensemble bias", "comments": "31 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study considers the application of the Ignorance Score (also known as\nthe Logarithmic Score) in the context of ensemble verification. In particular,\nwe consider the case where an ensemble forecast is transformed to a Normal\nforecast distribution, and this distribution is evaluated by the Ignorance\nScore. It is shown that the standard Ignorance score is biased with respect to\nthe ensemble size, such that larger ensembles yield systematically better\nexpected scores. A new estimator of the Ignorance score is derived which is\nunbiased with respect to the ensemble size. In an application to seasonal\nclimate predictions it is shown that the standard Ignorance score assigns\nbetter expected scores to simple climatological ensembles or biased ensembles\nthat have many members, than to physical dynamical and unbiased ensembles with\nfewer members. By contrast, the new bias-corrected Ignorance score ranks the\nphysical dynamical and unbiased ensembles better than the climatological and\nbiased ones, independent of ensemble size. It is shown that the unbiased\nestimator has smaller estimator variance and error than the standard estimator,\nand that it is a fair verification score, which is optimized if the ensemble\nmembers are statistically consistent with the observations. The finite ensemble\nbias of ensemble verification scores is discussed more broadly. It is argued\nthat a bias-correction is appropriate when forecast systems with different\nensemble sizes are compared, and when an evaluation of the underlying\ndistribution of the ensemble is of interest; possible applications to unbiased\nparameter estimation are discussed.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 04:27:37 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2015 12:48:46 GMT"}], "update_date": "2015-04-09", "authors_parsed": [["Siegert", "Stefan", ""], ["Ferro", "Christopher A. T.", ""], ["Stephenson", "David B.", ""]]}, {"id": "1410.8260", "submitter": "Yunjin Choi", "authors": "Yunjin Choi, Jonathan Taylor, Robert Tibshirani", "title": "Selecting the number of principal components: estimation of the true\n  rank of a noisy matrix", "comments": "29 pages, 9 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal component analysis (PCA) is a well-known tool in multivariate\nstatistics. One significant challenge in using PCA is the choice of the number\nof components. In order to address this challenge, we propose an exact\ndistribution-based method for hypothesis testing and construction of confidence\nintervals for signals in a noisy matrix. Assuming Gaussian noise, we use the\nconditional distribution of the singular values of a Wishart matrix and derive\nexact hypothesis tests and confidence intervals for the true signals. Our paper\nis based on the approach of Taylor, Loftus and Tibshirani (2013) for testing\nthe global null: we generalize it to test for any number of principal\ncomponents, and derive an integrated version with greater power. In simulation\nstudies we find that our proposed methods compare well to existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 05:36:57 GMT"}, {"version": "v2", "created": "Fri, 31 Oct 2014 05:10:31 GMT"}, {"version": "v3", "created": "Sun, 31 May 2015 02:43:39 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Choi", "Yunjin", ""], ["Taylor", "Jonathan", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1410.8269", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa and Tatsuya Kubokawa", "title": "Estimation and Prediction in Transformed Nested Error Regression Models", "comments": "This manuscript is superseded by \"Adaptively transformed mixed model\n  prediction of general finite population parameters\" by Sugasawa and Kubokawa\n  (arXiv:1705.04136)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper suggests parametrically transformed nested error regression models\n(TNERM), which transform the data flexibly to follow the normal linear mixed\nregression. We provide a procedure for estimating consistently the parameters\nof the proposed model and a predictor based on the consistent estimators. Then,\nin order to calibrate uncertainty of the transformed empirical best linear\nunbiased predictor, we derive prediction intervals with second-order accuracy\nbased on the parametric bootstrap method. The proposed methods are investigated\nthrough simulation and empirical studies.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 07:04:50 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 13:42:56 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kubokawa", "Tatsuya", ""]]}, {"id": "1410.8275", "submitter": "Stefan Wager", "authors": "Julie Josse and Stefan Wager", "title": "Bootstrap-Based Regularization for Low-Rank Matrix Estimation", "comments": "To appear in the Journal of Machine Learning Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a flexible framework for low-rank matrix estimation that allows us\nto transform noise models into regularization schemes via a simple bootstrap\nalgorithm. Effectively, our procedure seeks an autoencoding basis for the\nobserved matrix that is stable with respect to the specified noise model; we\ncall the resulting procedure a stable autoencoder. In the simplest case, with\nan isotropic noise model, our method is equivalent to a classical singular\nvalue shrinkage estimator. For non-isotropic noise models, e.g., Poisson noise,\nthe method does not reduce to singular value shrinkage, and instead yields new\nestimators that perform well in experiments. Moreover, by iterating our stable\nautoencoding scheme, we can automatically generate low-rank estimates without\nspecifying the target rank as a tuning parameter.\n", "versions": [{"version": "v1", "created": "Thu, 30 Oct 2014 07:22:33 GMT"}, {"version": "v2", "created": "Sat, 22 Nov 2014 02:11:14 GMT"}, {"version": "v3", "created": "Tue, 28 Jun 2016 05:13:33 GMT"}], "update_date": "2016-06-29", "authors_parsed": [["Josse", "Julie", ""], ["Wager", "Stefan", ""]]}, {"id": "1410.8597", "submitter": "Kevin Xu", "authors": "Qiuyi Han, Kevin S. Xu, and Edoardo M. Airoldi", "title": "Consistent estimation of dynamic and multi-layer block models", "comments": "To appear at ICML 2015", "journal-ref": "Proceedings of the 32nd International Conference on Machine\n  Learning (2015) 1511-1520", "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST physics.soc-ph stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant progress has been made recently on theoretical analysis of\nestimators for the stochastic block model (SBM). In this paper, we consider the\nmulti-graph SBM, which serves as a foundation for many application settings\nincluding dynamic and multi-layer networks. We explore the asymptotic\nproperties of two estimators for the multi-graph SBM, namely spectral\nclustering and the maximum-likelihood estimate (MLE), as the number of layers\nof the multi-graph increases. We derive sufficient conditions for consistency\nof both estimators and propose a variational approximation to the MLE that is\ncomputationally feasible for large networks. We verify the sufficient\nconditions via simulation and demonstrate that they are practical. In addition,\nwe apply the model to two real data sets: a dynamic social network and a\nmulti-layer social network with several types of relations.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 00:34:50 GMT"}, {"version": "v2", "created": "Tue, 11 Nov 2014 19:16:06 GMT"}, {"version": "v3", "created": "Tue, 19 May 2015 18:15:07 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Han", "Qiuyi", ""], ["Xu", "Kevin S.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1410.8679", "submitter": "Kristoffer H. Hellton", "authors": "Kristoffer Hellton and Magne Thoresen", "title": "Integrative clustering of high-dimensional data with joint and\n  individual clusters, with an application to the Metabric study", "comments": "20 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When measuring a range of different genomic, epigenomic, transcriptomic and\nother variables, an integrative approach to analysis can strengthen inference\nand give new insights. This is also the case when clustering patient samples,\nand several integrative cluster procedures have been proposed. Common for these\nmethodologies is the restriction of a joint cluster structure, which is equal\nfor all data layers. We instead present Joint and Individual Clustering (JIC),\nwhich estimates both joint and data type-specific clusters simultaneously, as\nan extension of the JIVE algorithm (Lock et. al, 2013). The method is compared\nto iCluster, another integrative clustering method, and simulations show that\nJIC is clearly advantageous when both individual and joint clusters are\npresent. The method is used to cluster patients in the Metabric study,\nintegrating gene expression data and copy number aberrations (CNA). The\nanalysis suggests a division into three joint clusters common for both data\ntypes and seven independent clusters specific for CNA. Both the joint and\nCNA-specific clusters are significantly different with respect to survival,\nalso when adjusting for age and treatment.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 09:09:18 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Hellton", "Kristoffer", ""], ["Thoresen", "Magne", ""]]}, {"id": "1410.8688", "submitter": "Holger Dette", "authors": "Holger Dette, Katrin Kettelhake, Frank Bretz", "title": "Designing dose finding studies with an active control for exponential\n  families", "comments": "24 pages 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent paper Dette et al. (2014) introduced optimal design problems for\ndose fnding studies with an active control. These authors concentrated on\nregression models with normal distributed errors (with known variance) and the\nproblem of determining optimal designs for estimating the smallest dose, which\nachieves the same treatment effect as the active control. This paper discusses\nthe problem of designing active-controlled dose fnding studies from a broader\nperspective. In particular, we consider a general class of optimality criteria\nand models arising from an exponential family, which are frequently used\nanalyzing count data. We investigate under which circumstances optimal designs\nfor dose fnding studies including a placebo can be used to obtain optimal\ndesigns for studies with an active control. Optimal designs are constructed for\nseveral situations and the differences arising from different distributional\nassumptions are investigated in detail. In particular, our results are\napplicable for constructing optimal experimental designs to analyze\nactive-controlled dose fnding studies with discrete data, and we illustrate the\nefficiency of the new optimal designs with two recent examples from our\nconsulting projects.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 09:58:19 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Dette", "Holger", ""], ["Kettelhake", "Katrin", ""], ["Bretz", "Frank", ""]]}, {"id": "1410.8765", "submitter": "Hongzhong Zhang", "authors": "Hongzhong Zhang, Neofytos Rodosthenous, Olympia Hadjiliadis", "title": "Robustness of the ${N}$-CUSUM stopping rule in a Wiener disorder problem", "comments": "Published at http://dx.doi.org/10.1214/14-AAP1078 in the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2015, Vol. 25, No. 6, 3405-3433", "doi": "10.1214/14-AAP1078", "report-no": "IMS-AAP-AAP1078", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a Wiener disorder problem of detecting the minimum of $N$\nchange-points in $N$ observation channels coupled by correlated noises. It is\nassumed that the observations in each dimension can have different strengths\nand that the change-points may differ from channel to channel. The objective is\nthe quickest detection of the minimum of the $N$ change-points. We adopt a\nmin-max approach and consider an extended Lorden's criterion, which is\nminimized subject to a constraint on the mean time to the first false alarm. It\nis seen that, under partial information of the post-change drifts and a general\nnonsingular stochastic correlation structure in the noises, the minimum of $N$\ncumulative sums (CUSUM) stopping rules is asymptotically optimal as the mean\ntime to the first false alarm increases without bound. We further discuss\napplications of this result with emphasis on its implications to the efficiency\nof the decentralized versus the centralized systems of observations which arise\nin engineering.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 15:10:05 GMT"}, {"version": "v2", "created": "Wed, 28 Oct 2015 10:56:01 GMT"}], "update_date": "2015-10-29", "authors_parsed": [["Zhang", "Hongzhong", ""], ["Rodosthenous", "Neofytos", ""], ["Hadjiliadis", "Olympia", ""]]}, {"id": "1410.8766", "submitter": "Apratim Ganguly", "authors": "Apratim Ganguly and Wolfgang Polonik", "title": "Local Neighborhood Fusion in Locally Constant Gaussian Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we penetrate and extend the notion of local constancy in\ngraphical models that has been introduced by Honorio et al. (2009). We propose\nNeighborhood-Fused Lasso, a method for model selection in high-dimensional\ngraphical models, leveraging locality information. Our approach is based on an\nextension of the idea of node-wise regression (Meinshausen-B\\\"{u}hlmann, 2006)\nby adding a fusion penalty. We propose a fast numerical algorithm for our\napproach, and provide theoretical and numerical evidence for the fact that our\nmethodology outperforms related approaches that are ignoring the locality\ninformation. We further investigate the compatibility issues in our proposed\nmethodology and derive bound for the quadratic prediction error and\n$l_1$-bounds on the estimated coefficients.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 15:10:20 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Ganguly", "Apratim", ""], ["Polonik", "Wolfgang", ""]]}, {"id": "1410.8861", "submitter": "Priyantha Wijayatunga", "authors": "Priyantha Wijayatunga", "title": "Causal Effect Estimation Methods", "comments": null, "journal-ref": "Journal of Statistical and Econometric Methods, vol.3, no.2, 2014,\n  153-170", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relationship between two popular modeling frameworks of causal inference from\nobservational data, namely, causal graphical model and potential outcome causal\nmodel is discussed. How some popular causal effect estimators found in\napplications of the potential outcome causal model, such as inverse probability\nof treatment weighted estimator and doubly robust estimator can be obtained by\nusing the causal graphical model is shown. We confine to the simple case of\nbinary outcome and treatment variables with discrete confounders and it is\nshown how to generalize results to cases of continuous variables.\n", "versions": [{"version": "v1", "created": "Fri, 31 Oct 2014 19:36:47 GMT"}], "update_date": "2014-11-03", "authors_parsed": [["Wijayatunga", "Priyantha", ""]]}]