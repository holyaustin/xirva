[{"id": "1704.00183", "submitter": "Manuel Batram", "authors": "Manuel Batram and Dietmar Bauer", "title": "Model selection and model averaging in MACML-estimated MNP models", "comments": "Presented at International Choice Modelling Conference (ICMC) 2017,\n  Cape Town, South Africa", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a review of model selection and model averaging methods\nfor multinomial probit models estimated using the MACML approach. The proposed\napproaches are partitioned into test based methods (mostly derived from the\nlikelihood ratio paradigm), methods based on information criteria and model\naveraging methods.\n  Many of the approaches first have been derived for models estimated using\nmaximum likelihood and later adapted to the composite marginal likelihood\nframework. In this paper all approaches are applied to the MACML approach for\nestimation. The investigation lists advantages and disadvantages of the various\nmethods in terms of asymptotic properties as well as computational aspects. We\nfind that likelihood-ratio-type tests and information criteria have a spotty\nperformance when applied to MACML models and instead propose the use of an\nempirical likelihood test.\n  Furthermore, we show that model averaging is easily adaptable to CML\nestimation and has promising performance w.r.t to parameter recovery. Finally\nmodel averaging is applied to a real world example in order to demonstrate the\nfeasibility of the method in real world sized problems.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 15:33:40 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Batram", "Manuel", ""], ["Bauer", "Dietmar", ""]]}, {"id": "1704.00211", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy", "title": "Nonparametric causal effects based on incremental propensity score\n  interventions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most work in causal inference considers deterministic interventions that set\neach unit's treatment to some fixed value. However, under positivity violations\nthese interventions can lead to non-identification, inefficiency, and effects\nwith little practical relevance. Further, corresponding effects in longitudinal\nstudies are highly sensitive to the curse of dimensionality, resulting in\nwidespread use of unrealistic parametric models. We propose a novel solution to\nthese problems: incremental interventions that shift propensity score values\nrather than set treatments to fixed values. Incremental interventions have\nseveral crucial advantages. First, they avoid positivity assumptions entirely.\nSecond, they require no parametric assumptions and yet still admit a simple\ncharacterization of longitudinal effects, independent of the number of\ntimepoints. For example, they allow longitudinal effects to be visualized with\na single curve instead of lists of coefficients. After characterizing these\nincremental interventions and giving identifying conditions for corresponding\neffects, we also develop general efficiency theory, propose efficient\nnonparametric estimators that can attain fast convergence rates even when\nincorporating flexible machine learning, and propose a bootstrap-based\nconfidence band and simultaneous test of no treatment effect. Finally we\nexplore finite-sample performance via simulation, and apply the methods to\nstudy time-varying sociological effects of incarceration on entry into\nmarriage.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 18:52:43 GMT"}, {"version": "v2", "created": "Wed, 20 Sep 2017 03:56:39 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 19:03:03 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Kennedy", "Edward H.", ""]]}, {"id": "1704.00247", "submitter": "Gautam Sabnis", "authors": "Gautam Sabnis, Debdeep Pati, Anirban Bhattacharya", "title": "Compressed Covariance Estimation With Automated Dimension Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for estimating a covariance matrix that can be\nrepresented as a sum of a low-rank matrix and a diagonal matrix. The proposed\nmethod compresses high-dimensional data, computes the sample covariance in the\ncompressed space, and lifts it back to the ambient space via a decompression\noperation. A salient feature of our approach relative to existing literature on\ncombining sparsity and low-rank structures in covariance matrix estimation is\nthat we do not require the low-rank component to be sparse. A principled\nframework for estimating the compressed dimension using Stein's Unbiased Risk\nEstimation theory is demonstrated. Experimental simulation results demonstrate\nthe efficacy and scalability of our proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 2 Apr 2017 03:05:02 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Sabnis", "Gautam", ""], ["Pati", "Debdeep", ""], ["Bhattacharya", "Anirban", ""]]}, {"id": "1704.00391", "submitter": "Ming Cao", "authors": "Ming Cao", "title": "A two-stage working model strategy for network analysis under\n  Hierarchical Exponential Random Graph Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social networks as a representation of relational data, often possess\nmultiple types of dependency structures at the same time. There could be\nclustering (beyond homophily) at a macro level as well as transitivity (a\nfriend's friend is more likely to be also a friend) at a micro level. Motivated\nby \\cite{schweinberger2015local} which constructed a family of Exponential\nRandom Graph Models (ERGM) with local dependence assumption, we argue that this\nkind of hierarchical models has potential to better fit real networks. To\ntackle the non-scalable estimation problem, the cost paid for modeling power,\nwe propose a two-stage working model strategy that first utilize Latent Space\nModels (LSM) for their strength on clustering, and then further tune ERGM to\narchive goodness of fit.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 00:06:56 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Cao", "Ming", ""]]}, {"id": "1704.00402", "submitter": "Ming Cao", "authors": "Ming Cao", "title": "A Class of Temporal Hierarchical Exponential Random Graph Models for\n  Longitudinal Network Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a representation of relational data over time series, longitudinal\nnetworks provide opportunities to study link formation processes. However,\nnetworks at scale often exhibits community structure (i.e. clustering), which\nmay confound local structural effects if it is not considered appropriately in\nstatistical analysis. To infer the (possibly) evolving clusters and other\nnetwork structures (e.g. degree distribution and/or transitivity) within each\ncommunity, simultaneously, we propose a class of statistical models named\nTemporal Hierarchical Exponential Random Graph Models (THERGM). Our generative\nmodel imposes a Markovian transition matrix for nodes to change their\nmembership, and assumes they join new community in a preferential attachment\nway. For those remaining in the same cluster, they follow a specific temporal\nERG model (TERGM). While a direct MCMC based Bayesian estimation is\ncomputational infeasible, we propose a two-stage strategy. At the first stage,\na specific dynamic latent space model will be used as the working model for\nclustering. At the second stage, estimated memberships are taken as given to\nfit a TERG model in each cluster. We evaluate our methods on simulated data in\nterms of the mis-clustering rate, as well as the goodness of fit and link\nprediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 01:48:15 GMT"}], "update_date": "2017-04-04", "authors_parsed": [["Cao", "Ming", ""]]}, {"id": "1704.00520", "submitter": "Marko J\\\"arvenp\\\"a\\\"a", "authors": "Marko J\\\"arvenp\\\"a\\\"a, Michael U. Gutmann, Arijus Pleska, Aki Vehtari,\n  Pekka Marttinen", "title": "Efficient acquisition rules for model-based approximate Bayesian\n  computation", "comments": "30 pages, 10 figures", "journal-ref": null, "doi": "10.1214/18-BA1121", "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a method for Bayesian inference\nwhen the likelihood is unavailable but simulating from the model is possible.\nHowever, many ABC algorithms require a large number of simulations, which can\nbe costly. To reduce the computational cost, Bayesian optimisation (BO) and\nsurrogate models such as Gaussian processes have been proposed. Bayesian\noptimisation enables one to intelligently decide where to evaluate the model\nnext but common BO strategies are not designed for the goal of estimating the\nposterior distribution. Our paper addresses this gap in the literature. We\npropose to compute the uncertainty in the ABC posterior density, which is due\nto a lack of simulations to estimate this quantity accurately, and define a\nloss function that measures this uncertainty. We then propose to select the\nnext evaluation location to minimise the expected loss. Experiments show that\nthe proposed method often produces the most accurate approximations as compared\nto common BO strategies.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 10:40:15 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 18:43:02 GMT"}, {"version": "v3", "created": "Wed, 8 Aug 2018 13:57:47 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["J\u00e4rvenp\u00e4\u00e4", "Marko", ""], ["Gutmann", "Michael U.", ""], ["Pleska", "Arijus", ""], ["Vehtari", "Aki", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1704.00566", "submitter": "Jinyuan Chang", "authors": "Jinyuan Chang, Cheng Yong Tang, Tong Tong Wu", "title": "A new scope of penalized empirical likelihood with high-dimensional\n  estimating equations", "comments": null, "journal-ref": "Annals of Statistics 2018, Vol. 46, No. 6B, 3185-3216", "doi": "10.1214/17-AOS1655", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods with empirical likelihood (EL) are appealing and\neffective especially in conjunction with estimating equations through which\nuseful data information can be adaptively and flexibly incorporated. It is also\nknown in the literature that EL approaches encounter difficulties when dealing\nwith problems having high-dimensional model parameters and estimating\nequations. To overcome the challenges, we begin our study with a careful\ninvestigation on high-dimensional EL from a new scope targeting at estimating a\nhigh-dimensional sparse model parameters. We show that the new scope provides\nan opportunity for relaxing the stringent requirement on the dimensionality of\nthe model parameter. Motivated by the new scope, we then propose a new\npenalized EL by applying two penalty functions respectively regularizing the\nmodel parameters and the associated Lagrange multipliers in the optimizations\nof EL. By penalizing the Lagrange multiplier to encourage its sparsity, we show\nthat drastic dimension reduction in the number of estimating equations can be\neffectively achieved without compromising the validity and consistency of the\nresulting estimators. Most attractively, such a reduction in dimensionality of\nestimating equations is actually equivalent to a selection among those\nhigh-dimensional estimating equations, resulting in a highly parsimonious and\neffective device for high-dimensional sparse model parameters. Allowing both\nthe dimensionalities of model parameters and estimating equations growing\nexponentially with the sample size, our theory demonstrates that the estimator\nfrom our new penalized EL is sparse and consistent with asymptotically normally\ndistributed nonzero components. Numerical simulations and a real data analysis\nshow that the proposed penalized EL works promisingly.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 13:18:59 GMT"}, {"version": "v2", "created": "Sun, 28 May 2017 03:25:14 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Chang", "Jinyuan", ""], ["Tang", "Cheng Yong", ""], ["Wu", "Tong Tong", ""]]}, {"id": "1704.00642", "submitter": "Timothy Cannings", "authors": "Timothy I. Cannings, Thomas B. Berrett and Richard J. Samworth", "title": "Local nearest neighbour classification with applications to\n  semi-supervised learning", "comments": "60 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.CV cs.LG stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive a new asymptotic expansion for the global excess risk of a\nlocal-$k$-nearest neighbour classifier, where the choice of $k$ may depend upon\nthe test point. This expansion elucidates conditions under which the dominant\ncontribution to the excess risk comes from the decision boundary of the optimal\nBayes classifier, but we also show that if these conditions are not satisfied,\nthen the dominant contribution may arise from the tails of the marginal\ndistribution of the features. Moreover, we prove that, provided the\n$d$-dimensional marginal distribution of the features has a finite $\\rho$th\nmoment for some $\\rho > 4$ (as well as other regularity conditions), a local\nchoice of $k$ can yield a rate of convergence of the excess risk of\n$O(n^{-4/(d+4)})$, where $n$ is the sample size, whereas for the standard\n$k$-nearest neighbour classifier, our theory would require $d \\geq 5$ and $\\rho\n> 4d/(d-4)$ finite moments to achieve this rate. These results motivate a new\n$k$-nearest neighbour classifier for semi-supervised learning problems, where\nthe unlabelled data are used to obtain an estimate of the marginal feature\ndensity, and fewer neighbours are used for classification when this density\nestimate is small. Our worst-case rates are complemented by a minimax lower\nbound, which reveals that the local, semi-supervised $k$-nearest neighbour\nclassifier attains the minimax optimal rate over our classes for the excess\nrisk, up to a subpolynomial factor in $n$. These theoretical improvements over\nthe standard $k$-nearest neighbour classifier are also illustrated through a\nsimulation study.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 15:34:11 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 11:16:30 GMT"}, {"version": "v3", "created": "Sat, 18 May 2019 10:49:46 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Cannings", "Timothy I.", ""], ["Berrett", "Thomas B.", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1704.00666", "submitter": "Shu Yang", "authors": "Shu Yang and Peng Ding", "title": "Asymptotic causal inference with observational studies trimmed by the\n  estimated propensity scores", "comments": "21 pages, 1 figures and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference with observational studies often relies on the assumptions\nof unconfoundedness and overlap of covariate distributions in different\ntreatment groups. The overlap assumption is violated when some units have\npropensity scores close to 0 or 1, and therefore both practical and theoretical\nresearchers suggest dropping units with extreme estimated propensity scores.\nHowever, existing trimming methods ignore the uncertainty in this design stage\nand restrict inference only to the trimmed sample, due to the non-smoothness of\nthe trimming. We propose a smooth weighting, which approximates the existing\nsample trimming but has better asymptotic properties. An advantage of the new\nsmoothly weighted estimator is its asymptotic linearity, which ensures that the\nbootstrap can be used to make inference for the target population,\nincorporating uncertainty arising from both the design and analysis stages. We\nalso extend the theory to the average treatment effect on the treated,\nsuggesting trimming samples with estimated propensity scores close to 1.\n", "versions": [{"version": "v1", "created": "Mon, 3 Apr 2017 16:26:10 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 14:34:16 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Yang", "Shu", ""], ["Ding", "Peng", ""]]}, {"id": "1704.00850", "submitter": "Qian Qin", "authors": "Qian Qin, James P. Hobert, Kshitij Khare", "title": "Estimating the spectral gap of a trace-class Markov operator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The utility of a Markov chain Monte Carlo algorithm is, in large part,\ndetermined by the size of the spectral gap of the corresponding Markov\noperator. However, calculating (and even approximating) the spectral gaps of\npractical Monte Carlo Markov chains in statistics has proven to be an extremely\ndifficult and often insurmountable task, especially when these chains move on\ncontinuous state spaces. In this paper, a method for accurate estimation of the\nspectral gap is developed for general state space Markov chains whose operators\nare non-negative and trace-class. The method is based on the fact that the\nsecond largest eigenvalue (and hence the spectral gap) of such operators can be\nbounded above and below by simple functions of the power sums of the\neigenvalues. These power sums often have nice integral representations. A\nclassical Monte Carlo method is proposed to estimate these integrals, and a\nsimple sufficient condition for finite variance is provided. This leads to\nasymptotically valid confidence intervals for the second largest eigenvalue\n(and the spectral gap) of the Markov operator. In contrast with previously\nexisting techniques, our method is not based on a near-stationary version of\nthe Markov chain, which, paradoxically, cannot be obtained in a principled\nmanner without bounds on the spectral gap. On the other hand, it can be quite\nexpensive from a computational standpoint. The efficiency of the method is\nstudied both theoretically and empirically.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 01:52:24 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 03:55:20 GMT"}, {"version": "v3", "created": "Thu, 4 Apr 2019 23:58:26 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Qin", "Qian", ""], ["Hobert", "James P.", ""], ["Khare", "Kshitij", ""]]}, {"id": "1704.00946", "submitter": "Hien Nguyen", "authors": "Hien D. Nguyen and Faicel Chamroukhi and Florence Forbes", "title": "Approximation results regarding the multiple-output mixture of linear\n  experts model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of experts (MoE) models are a class of artificial neural networks\nthat can be used for functional approximation and probabilistic modeling. An\nimportant class of MoE models is the class of mixture of linear experts (MoLE)\nmodels, where the expert functions map to real topological output spaces. There\nare a number of powerful approximation results regarding MoLE models, when the\noutput space is univariate. These results guarantee the ability of MoLE mean\nfunctions to approximate arbitrary continuous functions, and MoLE models\nthemselves to approximate arbitrary conditional probability density functions.\nWe utilize and extend upon the univariate approximation results in order to\nprove a pair of useful results for situations where the output spaces are\nmultivariate.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 10:33:10 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 14:09:03 GMT"}, {"version": "v3", "created": "Tue, 4 Sep 2018 14:20:02 GMT"}, {"version": "v4", "created": "Tue, 28 May 2019 03:42:02 GMT"}], "update_date": "2019-05-29", "authors_parsed": [["Nguyen", "Hien D.", ""], ["Chamroukhi", "Faicel", ""], ["Forbes", "Florence", ""]]}, {"id": "1704.00963", "submitter": "Eero Siivola", "authors": "Eero Siivola, Aki Vehtari, Jarno Vanhatalo, Javier Gonz\\'alez, Michael\n  Riis Andersen", "title": "Correcting boundary over-exploration deficiencies in Bayesian\n  optimization with virtual derivative sign observations", "comments": "6 pages, 7 figures", "journal-ref": "2018 IEEE 28th International Workshop on Machine Learning for\n  Signal Processing (MLSP)", "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian optimization (BO) is a global optimization strategy designed to find\nthe minimum of an expensive black-box function, typically defined on a compact\nsubset of $\\mathcal{R}^d$, by using a Gaussian process (GP) as a surrogate\nmodel for the objective. Although currently available acquisition functions\naddress this goal with different degree of success, an over-exploration effect\nof the contour of the search space is typically observed. However, in problems\nlike the configuration of machine learning algorithms, the function domain is\nconservatively large and with a high probability the global minimum does not\nsit on the boundary of the domain. We propose a method to incorporate this\nknowledge into the search process by adding virtual derivative observations in\nthe \\gp at the boundary of the search space. We use the properties of GPs to\nimpose conditions on the partial derivatives of the objective. The method is\napplicable with any acquisition function, it is easy to use and consistently\nreduces the number of evaluations required to optimize the objective\nirrespective of the acquisition used. We illustrate the benefits of our\napproach in an extensive experimental comparison.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 11:40:20 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 14:50:39 GMT"}, {"version": "v3", "created": "Fri, 21 Sep 2018 11:49:01 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Siivola", "Eero", ""], ["Vehtari", "Aki", ""], ["Vanhatalo", "Jarno", ""], ["Gonz\u00e1lez", "Javier", ""], ["Andersen", "Michael Riis", ""]]}, {"id": "1704.01053", "submitter": "Dane Taylor", "authors": "Zichao Li, Peter J. Mucha, Dane Taylor", "title": "Network-ensemble comparisons with stochastic rewiring and von Neumann\n  entropy", "comments": "24 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cond-mat.dis-nn cs.IT math.IT math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing whether a given network is typical or atypical for a random-network\nensemble (i.e., network-ensemble comparison) has widespread applications\nranging from null-model selection and hypothesis testing to clustering and\nclassifying networks. We develop a framework for network-ensemble comparison by\nsubjecting the network to stochastic rewiring. We study two rewiring processes,\nuniform and degree-preserved rewiring, which yield random-network ensembles\nthat converge to the Erdos-Renyi and configuration-model ensembles,\nrespectively. We study convergence through von Neumann entropy (VNE), a network\nsummary statistic measuring information content based on the spectra of a\nLaplacian matrix, and develop a perturbation analysis for the expected effect\nof rewiring on VNE. Our analysis yields an estimate for how many rewires are\nrequired for a given network to resemble a typical network from an ensemble,\noffering a computationally efficient quantity for network-ensemble comparison\nthat does not require simulation of the corresponding rewiring process.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 15:23:01 GMT"}, {"version": "v2", "created": "Wed, 29 Nov 2017 22:28:20 GMT"}], "update_date": "2017-12-01", "authors_parsed": [["Li", "Zichao", ""], ["Mucha", "Peter J.", ""], ["Taylor", "Dane", ""]]}, {"id": "1704.01066", "submitter": "Fabian Dunker", "authors": "Fabian Dunker, Konstantin Eckle, Katharina Proksch, Johannes\n  Schmidt-Hieber", "title": "Tests for qualitative features in the random coefficients model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The random coefficients model is an extension of the linear regression model\nthat allows for unobserved heterogeneity in the population by modeling the\nregression coefficients as random variables. Given data from this model, the\nstatistical challenge is to recover information about the joint density of the\nrandom coefficients which is a multivariate and ill-posed problem. Because of\nthe curse of dimensionality and the ill-posedness, pointwise nonparametric\nestimation of the joint density is difficult and suffers from slow convergence\nrates. Larger features, such as an increase of the density along some direction\nor a well-accentuated mode can, however, be much easier detected from data by\nmeans of statistical tests. In this article, we follow this strategy and\nconstruct tests and confidence statements for qualitative features of the joint\ndensity, such as increases, decreases and modes. We propose a multiple testing\napproach based on aggregating single tests which are designed to extract shape\ninformation on fixed scales and directions. Using recent tools for Gaussian\napproximations of multivariate empirical processes, we derive expressions for\nthe critical value. We apply our method to simulated and real data.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 15:33:19 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 03:17:06 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 03:36:50 GMT"}], "update_date": "2018-03-15", "authors_parsed": [["Dunker", "Fabian", ""], ["Eckle", "Konstantin", ""], ["Proksch", "Katharina", ""], ["Schmidt-Hieber", "Johannes", ""]]}, {"id": "1704.01190", "submitter": "Jean Pouget-Abadie", "authors": "Jean Pouget-Abadie, Martin Saveski, Guillaume Saint-Jacques, Weitao\n  Duan, Ya Xu, Souvik Ghosh, Edoardo Maria Airoldi", "title": "Testing for arbitrary interference on experimentation platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimentation platforms are essential to modern large technology companies,\nas they are used to carry out many randomized experiments daily. The classic\nassumption of no interference among users, under which the outcome of one user\ndoes not depend on the treatment assigned to other users, is rarely tenable on\nsuch platforms. Here, we introduce an experimental design strategy for testing\nwhether this assumption holds. Our approach is in the spirit of the\nDurbin-Wu-Hausman test for endogeneity in econometrics, where multiple\nestimators return the same estimate if and only if the null hypothesis holds.\nThe design that we introduce makes no assumptions on the interference model\nbetween units, nor on the network among the units, and has a sharp bound on the\nvariance and an implied analytical bound on the type I error rate. We discuss\nhow to apply the proposed design strategy to large experimentation platforms,\nand we illustrate it in the context of an experiment on the LinkedIn platform.\n", "versions": [{"version": "v1", "created": "Tue, 4 Apr 2017 21:16:30 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 18:17:00 GMT"}, {"version": "v3", "created": "Thu, 13 Apr 2017 20:18:08 GMT"}, {"version": "v4", "created": "Tue, 29 Jan 2019 02:35:27 GMT"}], "update_date": "2019-01-30", "authors_parsed": [["Pouget-Abadie", "Jean", ""], ["Saveski", "Martin", ""], ["Saint-Jacques", "Guillaume", ""], ["Duan", "Weitao", ""], ["Xu", "Ya", ""], ["Ghosh", "Souvik", ""], ["Airoldi", "Edoardo Maria", ""]]}, {"id": "1704.01233", "submitter": "Jeremie Houssineau", "authors": "Jeremie Houssineau and Adrian N. Bishop", "title": "Smoothing and filtering with a class of outer measures", "comments": null, "journal-ref": "SIAM/ASA Journal on Uncertainty Quantification, Volume 6, Issue 2,\n  pages: 845-866, 2018", "doi": "10.1137/17M1124383", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Filtering and smoothing with a generalised representation of uncertainty is\nconsidered. Here, uncertainty is represented using a class of outer measures.\nIt is shown how this representation of uncertainty can be propagated using\nouter-measure-type versions of Markov kernels and generalised Bayesian-like\nupdate equations. This leads to a system of generalised smoothing and filtering\nequations where integrals are replaced by supremums and probability density\nfunctions are replaced by positive functions with supremum equal to one.\nInterestingly, these equations retain most of the structure found in the\nclassical Bayesian filtering framework. It is additionally shown that the\nKalman filter recursion can be recovered from weaker assumptions on the\navailable information on the corresponding hidden Markov model.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 01:11:23 GMT"}, {"version": "v2", "created": "Fri, 20 Apr 2018 04:57:14 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Houssineau", "Jeremie", ""], ["Bishop", "Adrian N.", ""]]}, {"id": "1704.01538", "submitter": "Iv\\'an D\\'iaz", "authors": "Iv\\'an D\\'iaz and Mark J. van der Laan", "title": "Doubly Robust Inference for Targeted Minimum Loss Based Estimation in\n  Randomized Trials with Missing Outcome Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing outcome data is one of the principal threats to the validity of\ntreatment effect estimates from randomized trials. The outcome distributions of\nparticipants with missing and observed data are often different, which\nincreases the risk of bias. Causal inference methods may aid in reducing the\nbias and improving efficiency by incorporating baseline variables into the\nanalysis. In particular, doubly robust estimators incorporate estimates of two\nnuisance parameters: the outcome regression and the missingness mechanism, to\nadjust for differences in the observed and unobserved groups that can be\nexplained by observed covariates. Such nuisance parameters are traditionally\nestimated using parametric models, which generally preclude consistent\nestimation, particularly in moderate to high dimensions. Recent research on\nmissing data has focused on data-adaptive estimation of the nuisance parameters\nin order to achieve consistency, but the large sample properties of such\nestimators are poorly understood. In this article we discuss a doubly robust\nestimator that is consistent and asymptotically normal (CAN) under\ndata-adaptive consistent estimation of the outcome regression or the\nmissingness mechanism. We provide a formula for an asymptotically valid\nconfidence interval under minimal assumptions. We show that our proposed\nestimator has smaller finite-sample bias compared to standard doubly robust\nestimators. We present a simulation study demonstrating the enhanced\nperformance of our estimators in terms of bias, efficiency, and coverage of the\nconfidence intervals. We present the results of an illustrative example: a\nrandomized, double-blind phase II/III trial of antiretroviral therapy in\nHIV-infected persons, and provide R code implementing our proposed estimators.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 17:31:07 GMT"}], "update_date": "2017-04-06", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1704.01649", "submitter": "Giovanni Marchetti", "authors": "Nanny Wermuth and Giovanni M. Marchetti", "title": "Generating large Ising models with Markov structure via simple linear\n  relations", "comments": "27 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the notion of a tree graph to sequences of prime graphs which are\ncycles and edges and name these non-chordal graphs hollow trees. These\nstructures are especially attractive for palindromic Ising models, which mimic\na symmetry of joint Gaussian distributions. We show that for an Ising model all\ndefining independences are captured by zero partial correlations and\nconditional correlations agree with partial correlations within each prime\ngraph if and only if the model is palindromic and has a hollow-tree structure.\nThis implies that the strength of dependences can be assessed locally. We use\nthe results to find a well-fitting general Ising model with hollow-tree\nstructure for a set of longitudinal data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 20:42:51 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Wermuth", "Nanny", ""], ["Marchetti", "Giovanni M.", ""]]}, {"id": "1704.01664", "submitter": "Cheng Ju", "authors": "Cheng Ju and Aur\\'elien Bibaut and Mark J. van der Laan", "title": "The Relative Performance of Ensemble Methods with Deep Convolutional\n  Neural Networks for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks have been successfully applied to a variety of\nmachine learning tasks, including image recognition, semantic segmentation, and\nmachine translation. However, few studies fully investigated ensembles of\nartificial neural networks. In this work, we investigated multiple widely used\nensemble methods, including unweighted averaging, majority voting, the Bayes\nOptimal Classifier, and the (discrete) Super Learner, for image recognition\ntasks, with deep neural networks as candidate algorithms. We designed several\nexperiments, with the candidate algorithms being the same network structure\nwith different model checkpoints within a single training process, networks\nwith same structure but trained multiple times stochastically, and networks\nwith different structure. In addition, we further studied the over-confidence\nphenomenon of the neural networks, as well as its impact on the ensemble\nmethods. Across all of our experiments, the Super Learner achieved best\nperformance among all the ensemble methods in this study.\n", "versions": [{"version": "v1", "created": "Wed, 5 Apr 2017 23:04:43 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Ju", "Cheng", ""], ["Bibaut", "Aur\u00e9lien", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1704.01864", "submitter": "Ioan Gabriel Bucur", "authors": "Ioan Gabriel Bucur, Tom Claassen, Tom Heskes", "title": "Robust Causal Estimation in the Large-Sample Limit without Strict\n  Faithfulness", "comments": "10 pages, 12 figures, Proceedings of the 20th International\n  Conference on Artificial Intelligence and Statistics (AISTATS) 2017", "journal-ref": "PMLR 54:1523-1531, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal effect estimation from observational data is an important and much\nstudied research topic. The instrumental variable (IV) and local causal\ndiscovery (LCD) patterns are canonical examples of settings where a closed-form\nexpression exists for the causal effect of one variable on another, given the\npresence of a third variable. Both rely on faithfulness to infer that the\nlatter only influences the target effect via the cause variable. In reality, it\nis likely that this assumption only holds approximately and that there will be\nat least some form of weak interaction. This brings about the paradoxical\nsituation that, in the large-sample limit, no predictions are made, as\ndetecting the weak edge invalidates the setting. We introduce an alternative\napproach by replacing strict faithfulness with a prior that reflects the\nexistence of many 'weak' (irrelevant) and 'strong' interactions. We obtain a\nposterior distribution over the target causal effect estimator which shows\nthat, in many cases, we can still make good estimates. We demonstrate the\napproach in an application on a simple linear-Gaussian setting, using the\nMultiNest sampling algorithm, and compare it with established techniques to\nshow our method is robust even when strict faithfulness is violated.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 14:39:54 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Bucur", "Ioan Gabriel", ""], ["Claassen", "Tom", ""], ["Heskes", "Tom", ""]]}, {"id": "1704.01910", "submitter": "Bernd Sturmfels", "authors": "Elina Robeva, Bernd Sturmfels, and Caroline Uhler", "title": "Geometry of Log-Concave Density Estimation", "comments": "22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shape-constrained density estimation is an important topic in mathematical\nstatistics. We focus on densities on $\\mathbb{R}^d$ that are log-concave, and\nwe study geometric properties of the maximum likelihood estimator (MLE) for\nweighted samples. Cule, Samworth, and Stewart showed that the logarithm of the\noptimal log-concave density is piecewise linear and supported on a regular\nsubdivision of the samples. This defines a map from the space of weights to the\nset of regular subdivisions of the samples, i.e. the face poset of their\nsecondary polytope. We prove that this map is surjective. In fact, every\nregular subdivision arises in the MLE for some set of weights with positive\nprobability, but coarser subdivisions appear to be more likely to arise than\nfiner ones. To quantify these results, we introduce a continuous version of the\nsecondary polytope, whose dual we name the Samworth body. This article\nestablishes a new link between geometric combinatorics and nonparametric\nstatistics, and it suggests numerous open problems.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 16:23:41 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Robeva", "Elina", ""], ["Sturmfels", "Bernd", ""], ["Uhler", "Caroline", ""]]}, {"id": "1704.01978", "submitter": "Marina Valdora", "authors": "Julieta Molina, Mariela Sued, Marina Valdora", "title": "Models for the Propensity Score that Contemplate the Positivity\n  Assumption and their Application to Missing Data and Causality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models are often assumed to fit propensity scores, which\nare used to compute inverse probability weighted (IPW) estimators. In order to\nderive the asymptotic properties of IPW estimators, the propensity score is\nsupposed to be bounded away from cero. This condition is known in the\nliterature as strict positivity (or positivity assumption) and, in practice,\nwhen it does not hold, IPW estimators are very unstable and have a large\nvariability. Although strict positivity is often assumed, it is not upheld when\nsome of the covariates are continuous. In this work, we attempt to conciliate\nbetween the strict positivity condition and the theory of generalized linear\nmodels by incorporating an extra parameter, which results in an explicit lower\nbound for the propensity scores.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 18:10:45 GMT"}, {"version": "v2", "created": "Mon, 10 Apr 2017 12:15:20 GMT"}, {"version": "v3", "created": "Sun, 23 Apr 2017 12:31:11 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Molina", "Julieta", ""], ["Sued", "Mariela", ""], ["Valdora", "Marina", ""]]}, {"id": "1704.02030", "submitter": "Yuling Yao", "authors": "Yuling Yao, Aki Vehtari, Daniel Simpson, Andrew Gelman", "title": "Using stacking to average Bayesian predictive distributions", "comments": null, "journal-ref": null, "doi": "10.1214/17-BA1091", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The widely recommended procedure of Bayesian model averaging is flawed in the\nM-open setting in which the true data-generating process is not one of the\ncandidate models being fit. We take the idea of stacking from the point\nestimation literature and generalize to the combination of predictive\ndistributions, extending the utility function to any proper scoring rule, using\nPareto smoothed importance sampling to efficiently compute the required\nleave-one-out posterior distributions and regularization to get more stability.\nWe compare stacking of predictive distributions to several alternatives:\nstacking of means, Bayesian model averaging (BMA), pseudo-BMA using AIC-type\nweighting, and a variant of pseudo-BMA that is stabilized using the Bayesian\nbootstrap. Based on simulations and real-data applications, we recommend\nstacking of predictive distributions, with BB-pseudo-BMA as an approximate\nalternative when computation cost is an issue.\n", "versions": [{"version": "v1", "created": "Thu, 6 Apr 2017 21:49:24 GMT"}, {"version": "v2", "created": "Tue, 11 Apr 2017 05:09:48 GMT"}, {"version": "v3", "created": "Sat, 16 Sep 2017 02:31:39 GMT"}], "update_date": "2018-10-15", "authors_parsed": [["Yao", "Yuling", ""], ["Vehtari", "Aki", ""], ["Simpson", "Daniel", ""], ["Gelman", "Andrew", ""]]}, {"id": "1704.02084", "submitter": "Isabel Schlangen", "authors": "Isabel Schlangen and Emmanuel D. Delande and Jeremie Houssineau and\n  Daniel E. Clark", "title": "A second-order PHD filter with mean and variance in target number", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2017.2757905", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Probability Hypothesis Density (PHD) and Cardinalized PHD (CPHD) filters\nare popular solutions to the multi-target tracking problem due to their low\ncomplexity and ability to estimate the number and states of targets in\ncluttered environments. The PHD filter propagates the first-order moment (i.e.\nmean) of the number of targets while the CPHD propagates the cardinality\ndistribution in the number of targets, albeit for a greater computational cost.\nIntroducing the Panjer point process, this paper proposes a second-order PHD\nfilter, propagating the second-order moment (i.e. variance) of the number of\ntargets alongside its mean. The resulting algorithm is more versatile in the\nmodelling choices than the PHD filter, and its computational cost is\nsignificantly lower compared to the CPHD filter. The paper compares the three\nfilters in statistical simulations which demonstrate that the proposed filter\nreacts more quickly to changes in the number of targets, i.e., target births\nand target deaths, than the CPHD filter. In addition, a new statistic for\nmulti-object filters is introduced in order to study the correlation between\nthe estimated number of targets in different regions of the state space, and\npropose a quantitative analysis of the spooky effect for the three filters.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 03:37:57 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Schlangen", "Isabel", ""], ["Delande", "Emmanuel D.", ""], ["Houssineau", "Jeremie", ""], ["Clark", "Daniel E.", ""]]}, {"id": "1704.02097", "submitter": "Konstantinos  Fokianos", "authors": "Paul Doukhan and Konstantinos Fokianos and B{\\aa}rd St{\\o}ve and Dag\n  Tj{\\o}stheim", "title": "Multivariate Count Autoregression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are studying the problems of modeling and inference for multivariate count\ntime series data with Poisson marginals. The focus is on linear and log-linear\nmodels. For studying the properties of such processes we develop a novel\nconceptual framework which is based on copulas. However, our approach does not\nimpose the copula on a vector of counts; instead the joint distribution is\ndetermined by imposing a copula function on a vector of associated continuous\nrandom variables. This specific construction avoids conceptual difficulties\nresulting from the joint distribution of discrete random variables yet it keeps\nthe properties of the Poisson process marginally. We employ Markov chain theory\nand the notion of weak dependence to study ergodicity and stationarity of the\nmodels we consider. We obtain easily verifiable conditions for both linear and\nlog-linear models under both theoretical frameworks. Suitable estimating\nequations are suggested for estimating unknown model parameters. The large\nsample properties of the resulting estimators are studied in detail. The work\nconcludes with some simulations and a real data example.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 05:50:10 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["Doukhan", "Paul", ""], ["Fokianos", "Konstantinos", ""], ["St\u00f8ve", "B\u00e5rd", ""], ["Tj\u00f8stheim", "Dag", ""]]}, {"id": "1704.02207", "submitter": "Noel Erp van", "authors": "H.R.N. van Erp, R.O. Linger, and P.H.A.J.M. van Gelder", "title": "Introducing Inner Nested Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we will give a Monte Carlo algorithm by which the moments of a\nfunctions of Dirichlet probability distributions can be estimated. This\nalgorithm is called Inner Nested Sampling and is an implementation of\nSkilling's general Nested Sampling framework.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 12:42:00 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["van Erp", "H. R. N.", ""], ["Linger", "R. O.", ""], ["van Gelder", "P. H. A. J. M.", ""]]}, {"id": "1704.02220", "submitter": "Maria Francesca  Marino", "authors": "Maria Francesca Marino and Maria Giovanna Ranalli and Nicola Salvati\n  and Marco Alfo'", "title": "Semi-Parametric Empirical Best Prediction for small area estimation of\n  unemployment indicators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Italian National Institute for Statistics regularly provides estimates of\nunemployment indicators using data from the Labor Force Survey. However, direct\nestimates of unemployment incidence cannot be released for Local Labor Market\nAreas. These are unplanned domains defined as clusters of municipalities; many\nare out-of-sample areas and the majority is characterized by a small sample\nsize, which render direct estimates inadequate. The Empirical Best Predictor\nrepresents an appropriate, model-based, alternative. However, for non-Gaussian\nresponses, its computation and the computation of the analytic approximation to\nits Mean Squared Error require the solution of (possibly) multiple integrals\nthat, generally, have not a closed form. To solve the issue, Monte Carlo\nmethods and parametric bootstrap are common choices, even though the\ncomputational burden is a non trivial task. In this paper, we propose a\nSemi-Parametric Empirical Best Predictor for a (possibly) non-linear mixed\neffect model by leaving the distribution of the area-specific random effects\nunspecified and estimating it from the observed data. This approach is known to\nlead to a discrete mixing distribution which helps avoid unverifiable\nparametric assumptions and heavy integral approximations. We also derive a\nsecond-order, bias-corrected, analytic approximation to the corresponding Mean\nSquared Error. Finite sample properties of the proposed approach are tested via\na large scale simulation study. Furthermore, the proposal is applied to\nunit-level data from the 2012 Italian Labor Force Survey to estimate\nunemployment incidence for 611 Local Labor Market Areas using auxiliary\ninformation from administrative registers and the 2011 Census.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 13:22:38 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 07:29:47 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Marino", "Maria Francesca", ""], ["Ranalli", "Maria Giovanna", ""], ["Salvati", "Nicola", ""], ["Alfo'", "Marco", ""]]}, {"id": "1704.02381", "submitter": "Xin Bing", "authors": "Xin Bing and Marten Wegkamp", "title": "Adaptive estimation of the rank of the coefficient matrix in high\n  dimensional multivariate response regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the multivariate response regression problem with a regression\ncoefficient matrix of low, unknown rank. In this setting, we analyze a new\ncriterion for selecting the optimal reduced rank. This criterion differs\nnotably from the one proposed in Bunea, She and Wegkamp [7] in that it does not\nrequire estimation of the unknown variance of the noise, nor depends on a\ndelicate choice of a tuning parameter. We develop an iterative, fully\ndata-driven procedure, that adapts to the optimal signal to noise ratio. This\nprocedure finds the true rank in a few steps with overwhelming probability. At\neach step, our estimate increases, while at the same time it does not exceed\nthe true rank. Our finite sample results hold for any sample size and any\ndimension, even when the number of responses and of covariates grow much faster\nthan the number of observations. We perform an extensive simulation study that\nconfirms our theoretical findings. The new method performs better and more\nstable than that in [7] in both low- and high-dimensional settings.\n", "versions": [{"version": "v1", "created": "Fri, 7 Apr 2017 21:51:45 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 18:16:32 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 16:26:22 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Bing", "Xin", ""], ["Wegkamp", "Marten", ""]]}, {"id": "1704.02479", "submitter": "Quentin Frederik Gronau", "authors": "Quentin F. Gronau, Alexander Ly, and Eric-Jan Wagenmakers", "title": "Informed Bayesian T-Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across the empirical sciences, few statistical procedures rival the\npopularity of the frequentist t-test. In contrast, the Bayesian versions of the\nt-test have languished in obscurity. In recent years, however, the theoretical\nand practical advantages of the Bayesian t-test have become increasingly\napparent and various Bayesian t-tests have been proposed, both objective ones\n(based on general desiderata) and subjective ones (based on expert knowledge).\nHere we propose a flexible t-prior for standardized effect size that allows\ncomputation of the Bayes factor by evaluating a single numerical integral. This\nspecification contains previous objective and subjective t-test Bayes factors\nas special cases. Furthermore, we propose two measures for informed prior\ndistributions that quantify the departure from the objective Bayes factor\ndesiderata of predictive matching and information consistency. We illustrate\nthe use of informed prior distributions based on an expert prior elicitation\neffort.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 11:36:53 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 15:03:38 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 09:32:54 GMT"}, {"version": "v4", "created": "Fri, 14 Dec 2018 10:35:09 GMT"}], "update_date": "2018-12-17", "authors_parsed": [["Gronau", "Quentin F.", ""], ["Ly", "Alexander", ""], ["Wagenmakers", "Eric-Jan", ""]]}, {"id": "1704.02519", "submitter": "Alexander Tank", "authors": "Alex Tank, Emily B. Fox, Ali Shojaie", "title": "Identifiability and Estimation of Structural Vector Autoregressive\n  Models for Subsampled and Mixed Frequency Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference in multivariate time series is challenging due to the fact\nthat the sampling rate may not be as fast as the timescale of the causal\ninteractions. In this context, we can view our observed series as a subsampled\nversion of the desired series. Furthermore, due to technological and other\nlimitations, series may be observed at different sampling rates, representing a\nmixed frequency setting. To determine instantaneous and lagged effects between\ntime series at the true causal scale, we take a model-based approach based on\nstructural vector autoregressive (SVAR) models. In this context, we present a\nunifying framework for parameter identifiability and estimation under both\nsubsampling and mixed frequencies when the noise, or shocks, are non-Gaussian.\nImportantly, by studying the SVAR case, we are able to both provide\nidentifiability and estimation methods for the causal structure of both lagged\nand instantaneous effects at the desired time scale. We further derive an exact\nEM algorithm for inference in both subsampled and mixed frequency settings. We\nvalidate our approach in simulated scenarios and on two real world data sets.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 18:29:02 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Tank", "Alex", ""], ["Fox", "Emily B.", ""], ["Shojaie", "Ali", ""]]}, {"id": "1704.02531", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "Three Skewed Matrix Variate Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-way data can be conveniently modelled by using matrix variate\ndistributions. Although there has been a lot of work for the matrix variate\nnormal distribution, there is little work in the area of matrix skew\ndistributions. Three matrix variate distributions that incorporate skewness, as\nwell as other flexible properties such as concentration, are discussed.\nEquivalences to multivariate analogues are presented, and moment generating\nfunctions are derived. Maximum likelihood parameter estimation is discussed,\nand simulated data is used for illustration.\n", "versions": [{"version": "v1", "created": "Sat, 8 Apr 2017 20:03:01 GMT"}, {"version": "v2", "created": "Sun, 23 Apr 2017 22:05:55 GMT"}, {"version": "v3", "created": "Sat, 8 Jul 2017 20:00:10 GMT"}, {"version": "v4", "created": "Sat, 4 Nov 2017 22:20:45 GMT"}, {"version": "v5", "created": "Mon, 13 Aug 2018 21:19:08 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1704.02568", "submitter": "Wenlin Dai", "authors": "Wenlin Dai, Marc G. Genton", "title": "An Outlyingness Matrix for Multivariate Functional Data Classification", "comments": "26 pages, 7 figures, Statistica Sinica, 2017", "journal-ref": null, "doi": "10.5705/ss.202016.0537", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The classification of multivariate functional data is an important task in\nscientific research. Unlike point-wise data, functional data are usually\nclassified by their shapes rather than by their scales. We define an\noutlyingness matrix by extending directional outlyingness, an effective measure\nof the shape variation of curves that combines the direction of outlyingness\nwith conventional depth. We propose two classifiers based on directional\noutlyingness and the outlyingness matrix, respectively. Our classifiers provide\nbetter performance compared with existing depth-based classifiers when applied\non both univariate and multivariate functional data from simulation studies. We\nalso test our methods on two data problems: speech recognition and gesture\nclassification, and obtain results that are consistent with the findings from\nthe simulated data.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 07:17:08 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 06:34:13 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Dai", "Wenlin", ""], ["Genton", "Marc G.", ""]]}, {"id": "1704.02671", "submitter": "Dhaker Hamza", "authors": "Hamza Dhaker, Papa Ngom, Malick Mbodj", "title": "Overlap Coefficients Based on Kullback-Leibler Divergence: Exponential\n  Populations Case", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is devoted to the study of overlap measures of densities of two\nexponential populations. Various Overlapping Coefficients, namely: Matusita's\nmeasure $\\rho$, Morisita's measure $\\lambda$ and Weitzman's measure $\\Delta$. A\nnew overlap measure $\\Lambda$ based on Kullback-Leibler measure is proposed.\nThe invariance property and a method of statistical inference of these\ncoefficients also are presented. Taylor series approximation are used to\nconstruct confidence intervals for the overlap measures. The bias and mean\nsquare error properties of the estimators are studied through a simulation\nstudy.\n", "versions": [{"version": "v1", "created": "Sun, 9 Apr 2017 23:29:35 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Dhaker", "Hamza", ""], ["Ngom", "Papa", ""], ["Mbodj", "Malick", ""]]}, {"id": "1704.02739", "submitter": "Yunqi Bu", "authors": "Yunqi Bu and Johannes Lederer", "title": "Integrating Additional Knowledge Into Estimation of Graphical Models", "comments": "16 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In applications of graphical models, we typically have more information than\njust the samples themselves. A prime example is the estimation of brain\nconnectivity networks based on fMRI data, where in addition to the samples\nthemselves, the spatial positions of the measurements are readily available.\nWith particular regard for this application, we are thus interested in ways to\nincorporate additional knowledge most effectively into graph estimation. Our\napproach to this is to make neighborhood selection receptive to additional\nknowledge by strengthening the role of the tuning parameters. We demonstrate\nthat this concept (i) can improve reproducibility, (ii) is computationally\nconvenient and efficient, and (iii) carries a lucid Bayesian interpretation. We\nspecifically show that the approach provides effective estimations of brain\nconnectivity graphs from fMRI data. However, providing a general scheme for the\ninclusion of additional knowledge, our concept is expected to have applications\nin a wide range of domains.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 07:33:54 GMT"}, {"version": "v2", "created": "Thu, 20 Apr 2017 04:26:11 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Bu", "Yunqi", ""], ["Lederer", "Johannes", ""]]}, {"id": "1704.02771", "submitter": "Luca Martino", "authors": "L. Martino, V. Elvira, G. Camps-Valls", "title": "Group Importance Sampling for Particle Filtering and MCMC", "comments": "To appear in Digital Signal Processing. Related Matlab demos are\n  provided at https://github.com/lukafree/GIS.git", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.CE cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods and their implementations by means of sophisticated Monte\nCarlo techniques have become very popular in signal processing over the last\nyears. Importance Sampling (IS) is a well-known Monte Carlo technique that\napproximates integrals involving a posterior distribution by means of weighted\nsamples. In this work, we study the assignation of a single weighted sample\nwhich compresses the information contained in a population of weighted samples.\nPart of the theory that we present as Group Importance Sampling (GIS) has been\nemployed implicitly in different works in the literature. The provided analysis\nyields several theoretical and practical consequences. For instance, we discuss\nthe application of GIS into the Sequential Importance Resampling framework and\nshow that Independent Multiple Try Metropolis schemes can be interpreted as a\nstandard Metropolis-Hastings algorithm, following the GIS approach. We also\nintroduce two novel Markov Chain Monte Carlo (MCMC) techniques based on GIS.\nThe first one, named Group Metropolis Sampling method, produces a Markov chain\nof sets of weighted samples. All these sets are then employed for obtaining a\nunique global estimator. The second one is the Distributed Particle\nMetropolis-Hastings technique, where different parallel particle filters are\njointly used to drive an MCMC algorithm. Different resampled trajectories are\ncompared and then tested with a proper acceptance probability. The novel\nschemes are tested in different numerical experiments such as learning the\nhyperparameters of Gaussian Processes, two localization problems in a wireless\nsensor network (with synthetic and real data) and the tracking of vegetation\nparameters given satellite observations, where they are compared with several\nbenchmark Monte Carlo techniques. Three illustrative Matlab demos are also\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 09:20:47 GMT"}, {"version": "v2", "created": "Mon, 17 Apr 2017 14:22:22 GMT"}, {"version": "v3", "created": "Fri, 28 Apr 2017 20:51:47 GMT"}, {"version": "v4", "created": "Sat, 4 Aug 2018 09:19:51 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Martino", "L.", ""], ["Elvira", "V.", ""], ["Camps-Valls", "G.", ""]]}, {"id": "1704.02778", "submitter": "Sezen Cekic", "authors": "Sezen Cekic, Didier Grandjean and Olivier Renaud", "title": "Multiscale Bayesian State Space Model for Granger Causality Analysis of\n  Brain Signal", "comments": null, "journal-ref": "Journal of Applied Statistics, 2018", "doi": "10.1080/02664763.2018.1455814", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling time-varying and frequency-specific relationships between two brain\nsignals is becoming an essential methodological tool to answer heoretical\nquestions in experimental neuroscience. In this article, we propose to estimate\na frequency Granger causality statistic that may vary in time in order to\nevaluate the functional connections between two brain regions during a task. We\nuse for that purpose an adaptive Kalman filter type of estimator of a linear\nGaussian vector autoregressive model with coefficients evolving over time. The\nestimation procedure is achieved through variational Bayesian approximation and\nis extended for multiple trials. This Bayesian State Space (BSS) model provides\na dynamical Granger-causality statistic that is quite natural. We propose to\nextend the BSS model to include the \\`{a} trous Haar decomposition. This\nwavelet-based forecasting method is based on a multiscale resolution\ndecomposition of the signal using the redundant \\`{a} trous wavelet transform\nand allows us to capture short- and long-range dependencies between signals.\nEqually importantly it allows us to derive the desired dynamical and\nfrequency-specific Granger-causality statistic. The application of these models\nto intracranial local field potential data recorded during a psychological\nexperimental task shows the complex frequency based cross-talk between amygdala\nand medial orbito-frontal cortex.\n  Keywords: \\`{a} trous Haar wavelets; Multiple trials; Neuroscience data;\nNonstationarity; Time-frequency; Variational methods\n  The published version of this article is\n  Cekic, S., Grandjean, D., Renaud, O. (2018). Multiscale Bayesian state-space\nmodel for Granger causality analysis of brain signal. Journal of Applied\nStatistics. https://doi.org/10.1080/02664763.2018.1455814\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 09:36:07 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 12:30:30 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Cekic", "Sezen", ""], ["Grandjean", "Didier", ""], ["Renaud", "Olivier", ""]]}, {"id": "1704.02917", "submitter": "Gustavo Pereira", "authors": "Gustavo H. A. Pereira", "title": "On quantile residuals in beta regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Beta regression is often used to model the relationship between a dependent\nvariable that assumes values on the open interval (0,1) and a set of predictor\nvariables. An important challenge in beta regression is to find residuals whose\ndistribution is well approximated by the standard normal distribution. Two\nprevious works compared residuals in beta regression, but the authors did not\ninclude the quantile residual. Using Monte Carlo simulation techniques, this\npaper studies the behavior of certain residuals in beta regression in several\nscenarios. Overall, the results suggest that the distribution of the quantile\nresidual is better approximated by the standard normal distribution than that\nof the other residuals in most scenarios. Three applications illustrate the\neffectiveness of the quantile residual.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 15:45:58 GMT"}], "update_date": "2017-04-11", "authors_parsed": [["Pereira", "Gustavo H. A.", ""]]}, {"id": "1704.02999", "submitter": "Kyungchul Song", "authors": "Nathan Canen, Jacob Schwartz, Kyungchul Song", "title": "Estimating Local Interactions Among Many Agents Who Observe Their\n  Neighbors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In various economic environments, people observe other people with whom they\nstrategically interact. We can model such information-sharing relations as an\ninformation network, and the strategic interactions as a game on the network.\nWhen any two agents in the network are connected either directly or indirectly\nin a large network, empirical modeling using an equilibrium approach can be\ncumbersome, since the testable implications from an equilibrium generally\ninvolve all the players of the game, whereas a researcher's data set may\ncontain only a fraction of these players in practice. This paper develops a\ntractable empirical model of linear interactions where each agent, after\nobserving part of his neighbors' types, not knowing the full information\nnetwork, uses best responses that are linear in his and other players' types\nthat he observes, based on simple beliefs about the other players' strategies.\nWe provide conditions on information networks and beliefs such that the best\nresponses take an explicit form with multiple intuitive features. Furthermore,\nthe best responses reveal how local payoff interdependence among agents is\ntranslated into local stochastic dependence of their actions, allowing the\neconometrician to perform asymptotic inference without having to observe all\nthe players in the game or having to know the precise sampling process.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 18:18:46 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 16:43:15 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 02:44:33 GMT"}, {"version": "v4", "created": "Tue, 26 Nov 2019 18:06:56 GMT"}], "update_date": "2019-11-27", "authors_parsed": [["Canen", "Nathan", ""], ["Schwartz", "Jacob", ""], ["Song", "Kyungchul", ""]]}, {"id": "1704.03005", "submitter": "Zhenhua Lin", "authors": "Zhenhua Lin and Fang Yao", "title": "Functional Regression on Manifold with Contamination", "comments": "49 pages, 2 figures; to appear in Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for functional nonparametric regression with a\npredictor that resides on a finite-dimensional manifold but is only observable\nin an infinite-dimensional space. Contamination of the predictor due to\ndiscrete/noisy measurements is also accounted for. By using functional local\nlinear manifold smoothing, the proposed estimator enjoys a polynomial rate of\nconvergence that adapts to the intrinsic manifold dimension and the\ncontamination level. This is in contrast to the logarithmic convergence rate in\nthe literature of functional nonparametric regression. We also observe a phase\ntransition phenomenon regarding the interplay of the manifold dimension and the\ncontamination level. We demonstrate that the proposed method has favorable\nnumerical performance relative to commonly used methods via simulated and real\ndata examples.\n", "versions": [{"version": "v1", "created": "Mon, 10 Apr 2017 18:28:27 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 23:40:23 GMT"}, {"version": "v3", "created": "Fri, 12 Apr 2019 02:47:57 GMT"}, {"version": "v4", "created": "Fri, 5 Jun 2020 05:13:11 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Lin", "Zhenhua", ""], ["Yao", "Fang", ""]]}, {"id": "1704.03106", "submitter": "David Lester", "authors": "K. D. Yao, V. Patrangenaru, D. Lester", "title": "3D mean Projective Shape Difference for Face Differentiation from\n  Multiple Digital Camera Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a nonparametric methodology for hypothesis testing for equality of\nextrinsic mean objects on a manifold embedded in a numerical spaces. The\nresults obtained in the general setting are detailed further in the case of 3D\nprojective shapes represented in a space of symmetric matrices via the\nquadratic Veronese-Whitney (VW) embedding. Large sample and nonparametric\nbootstrap confidence regions are derived for the common VW-mean of random\nprojective shapes for finite 3D configurations. As an example, the VW MANOVA\ntesting methodology is applied to the multi-sample mean problem for independent\nprojective shapes of $3D$ facial configurations retrieved from digital images,\nvia Agisoft PhotoScan technology.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 01:08:00 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 22:26:18 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Yao", "K. D.", ""], ["Patrangenaru", "V.", ""], ["Lester", "D.", ""]]}, {"id": "1704.03127", "submitter": "Dibyendu Bhaumik Mr.", "authors": "Dibyendu Bhaumik and Radhendushka Srivastava and Debasis Sengupta", "title": "Feature Sensitive Curve Registration by Kernel Matching", "comments": "29 pages, 10 figures; Changes over the earlier version: Some of the\n  sections are modified; a new section on \"Standard error of the estimator\" has\n  been added. More analysis are reported in Simulation of performance", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we argue that the problem of registering two sets of\nfunctional data, where the underlying mean function has sharp features, is not\nproperly addressed by methods designed to align a bunch of growth curves data.\nWe provide a new method, which is able to pool local information without\nsmoothing and to match sharp landmarks without manual identification. This\nmethod, which we refer to as kernel-matched registration, is based on\nmaximizing a kernel-based measure of alignment. We prove that the proposed\nmethod is consistent under fairly general conditions. Simulation results show\nsuperiority of the performance of the proposed method over two existing\nmethods. The proposed method is illustrated through the analysis of three sets\nof paleoclimatic data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 03:25:34 GMT"}, {"version": "v2", "created": "Sun, 17 Dec 2017 17:49:59 GMT"}], "update_date": "2017-12-19", "authors_parsed": [["Bhaumik", "Dibyendu", ""], ["Srivastava", "Radhendushka", ""], ["Sengupta", "Debasis", ""]]}, {"id": "1704.03239", "submitter": "Gregor Kastner", "authors": "Gregor Kastner and Florian Huber", "title": "Sparse Bayesian vector autoregressions in huge dimensions", "comments": null, "journal-ref": "Journal of Forecasting (2020)", "doi": "10.1002/for.2680", "report-no": null, "categories": "stat.CO econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian vector autoregressive (VAR) model with multivariate\nstochastic volatility that is capable of handling vast dimensional information\nsets. Three features are introduced to permit reliable estimation of the model.\nFirst, we assume that the reduced-form errors in the VAR feature a factor\nstochastic volatility structure, allowing for conditional equation-by-equation\nestimation. Second, we apply recently developed global-local shrinkage priors\nto the VAR coefficients to cure the curse of dimensionality. Third, we utilize\nrecent innovations to efficiently sample from high-dimensional multivariate\nGaussian distributions. This makes simulation-based fully Bayesian inference\nfeasible when the dimensionality is large but the time series length is\nmoderate. We demonstrate the merits of our approach in an extensive simulation\nstudy and apply the model to US macroeconomic data to evaluate its forecasting\ncapabilities.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 11:14:41 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 16:04:35 GMT"}, {"version": "v3", "created": "Wed, 4 Dec 2019 07:29:27 GMT"}], "update_date": "2020-03-12", "authors_parsed": [["Kastner", "Gregor", ""], ["Huber", "Florian", ""]]}, {"id": "1704.03331", "submitter": "Julien Stoehr", "authors": "Julien Stoehr", "title": "A review on statistical inference methods for discrete Markov random\n  fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing satisfactory methodology for the analysis of Markov random field\nis a very challenging task. Indeed, due to the Markovian dependence structure,\nthe normalizing constant of the fields cannot be computed using standard\nanalytical or numerical methods. This forms a central issue for any statistical\napproach as the likelihood is an integral part of the procedure. Furthermore,\nsuch unobserved fields cannot be integrated out and the likelihood evaluation\nbecomes a doubly intractable problem. This report gives an overview of some of\nthe methods used in the literature to analyse such observed or unobserved\nrandom fields.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 14:54:34 GMT"}], "update_date": "2017-04-12", "authors_parsed": [["Stoehr", "Julien", ""]]}, {"id": "1704.03459", "submitter": "Edward Higson", "authors": "Edward Higson, Will Handley, Mike Hobson and Anthony Lasenby", "title": "Dynamic nested sampling: an improved algorithm for parameter estimation\n  and evidence calculation", "comments": "14 pages + appendix, 16 figures. Added signal reconstruction\n  numerical example using dyPolyChord. Moderate updates to text and appendices", "journal-ref": "Statistics and Computing 29, 5 (2019) p891-913", "doi": "10.1007/s11222-018-9844-0", "report-no": null, "categories": "stat.CO astro-ph.IM physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce dynamic nested sampling: a generalisation of the nested sampling\nalgorithm in which the number of \"live points\" varies to allocate samples more\nefficiently. In empirical tests the new method significantly improves\ncalculation accuracy compared to standard nested sampling with the same number\nof samples; this increase in accuracy is equivalent to speeding up the\ncomputation by factors of up to ~72 for parameter estimation and ~7 for\nevidence calculations. We also show that the accuracy of both parameter\nestimation and evidence calculations can be improved simultaneously. In\naddition, unlike in standard nested sampling, more accurate results can be\nobtained by continuing the calculation for longer. Popular standard nested\nsampling implementations can be easily adapted to perform dynamic nested\nsampling, and several dynamic nested sampling software packages are now\npublicly available.\n", "versions": [{"version": "v1", "created": "Tue, 11 Apr 2017 18:00:00 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 18:59:59 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 18:00:02 GMT"}, {"version": "v4", "created": "Sun, 7 Oct 2018 17:32:57 GMT"}], "update_date": "2019-08-27", "authors_parsed": [["Higson", "Edward", ""], ["Handley", "Will", ""], ["Hobson", "Mike", ""], ["Lasenby", "Anthony", ""]]}, {"id": "1704.03590", "submitter": "Luke Gandolfo", "authors": "Luke C. Gandolfo and Terence P. Speed", "title": "RLE Plots: Visualising Unwanted Variation in High Dimensional Data", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": "10.1371/journal.pone.0191629", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unwanted variation can be highly problematic and so its detection is often\ncrucial. Relative log expression (RLE) plots are a powerful tool for\nvisualising such variation in high dimensional data. We provide a detailed\nexamination of these plots, with the aid of examples and simulation, explaining\nwhat they are and what they can reveal. RLE plots are particularly useful for\nassessing whether a procedure aimed at removing unwanted variation, i.e. a\nnormalisation procedure, has been successful. These plots, while originally\ndevised for gene expression data from microarrays, can also be used to reveal\nunwanted variation in many other kinds of high dimensional data, where such\nvariation can be problematic.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 01:43:31 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Gandolfo", "Luke C.", ""], ["Speed", "Terence P.", ""]]}, {"id": "1704.03639", "submitter": "Caifa Zhou", "authors": "Caifa Zhou, Lin Ma, and Xuezhi Tan", "title": "Joint Semi-supervised RSS Dimensionality Reduction and Fingerprint Based\n  Algorithm for Indoor Localization", "comments": "14 figures. Institute of Navigation (ION GNSS+ 2014), 27th\n  International Technical Meeting of The Satellite Division Conference on", "journal-ref": "Zhou, C.F. et al., 2016. Joint Semi-supervised RSS Dimensionality\n  Reduction and Fingerprint Based Algorithm for Indoor Localization. ,\n  (SEPTEMBER 2014)", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent development in mobile computing devices and as the ubiquitous\ndeployment of access points(APs) of Wireless Local Area Networks(WLANs), WLAN\nbased indoor localization systems(WILSs) are of mounting concentration and are\nbecoming more and more prevalent for they do not require additional\ninfrastructure. As to the localization methods in WILSs, for the approaches\nused to localization in satellite based global position systems are difficult\nto achieve in indoor environments, fingerprint based localization\nalgorithms(FLAs) are predominant in the RSS based schemes. However, the\nperformance of FLAs has close relationship with the number of APs and the\nnumber of reference points(RPs) in WILSs, especially as the redundant\ndeployment of APs and RPs in the system. There are two fatal problems, curse of\ndimensionality (CoD) and asymmetric matching(AM), caused by increasing number\nof APs and breaking down APs during online stage. In this paper, a\nsemi-supervised RSS dimensionality reduction algorithm is proposed to solve\nthese two dilemmas at the same time and there are numerous analyses about the\ntheoretical realization of the proposed method. Another significant innovation\nof this paper is jointing the fingerprint based algorithm with CM-SDE algorithm\nto improve the localization accuracy of indoor localization.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 06:35:18 GMT"}], "update_date": "2017-04-13", "authors_parsed": [["Zhou", "Caifa", ""], ["Ma", "Lin", ""], ["Tan", "Xuezhi", ""]]}, {"id": "1704.03731", "submitter": "Sarah Friedrich", "authors": "Sarah Friedrich and Markus Pauly", "title": "MATS: Inference for potentially Singular and Heteroscedastic MANOVA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many experiments in the life sciences, several endpoints are recorded per\nsubject. The analysis of such multivariate data is usually based on MANOVA\nmodels assuming multivariate normality and covariance homogeneity. These\nassumptions, however, are often not met in practice. Furthermore, test\nstatistics should be invariant under scale transformations of the data, since\nthe endpoints may be measured on different scales. In the context of\nhigh-dimensional data, Srivastava and Kubokawa (2013) proposed such a test\nstatistic for a specific one-way model, which, however, relies on the\nassumption of a common non-singular covariance matrix. We modify and extend\nthis test statistic to factorial MANOVA designs, incorporating general\nheteroscedastic models. In particular, our only distributional assumption is\nthe existence of the group-wise covariance matrices, which may even be\nsingular. We base inference on quantiles of resampling distributions, and\nderive confidence regions and ellipsoids based on these quantiles. In a\nsimulation study, we extensively analyze the behavior of these procedures.\nFinally, the methods are applied to a data set containing information on the\n2016 presidential elections in the USA with unequal and singular empirical\ncovariance matrices.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 12:43:29 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 10:03:05 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Friedrich", "Sarah", ""], ["Pauly", "Markus", ""]]}, {"id": "1704.03907", "submitter": "Mehdi Maadooliat", "authors": "Mehdi Maadooliat, Ying Sun and Tianbo Chen", "title": "Nonparametric collective spectral density estimation with an application\n  to clustering the brain signals", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop a method for the simultaneous estimation of\nspectral density functions (SDFs) for a collection of stationary time series\nthat share some common features. Due to the similarities among the SDFs, the\nlog-SDF can be represented using a common set of basis functions. The basis\nshared by the collection of the log-SDFs is estimated as a low-dimensional\nmanifold of a large space spanned by a pre-specified rich basis. A collective\nestimation approach pools information and borrows strength across the SDFs to\nachieve better estimation efficiency. Also, each estimated spectral density has\na concise representation using the coefficients of the basis expansion, and\nthese coefficients can be used for visualization, clustering, and\nclassification purposes. The Whittle pseudo-maximum likelihood approach is used\nto fit the model and an alternating blockwise Newton-type algorithm is\ndeveloped for the computation. A web-based shiny App found at\n\"https://ncsde.shinyapps.io/NCSDE\" is developed for visualization, training and\nlearning the SDFs collectively using the proposed technique. Finally, we apply\nour method to cluster similar brain signals recorded by the\nelectroencephalogram for identifying synchronized brain regions according to\ntheir spectral densities.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 19:19:30 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 18:28:14 GMT"}, {"version": "v3", "created": "Sat, 21 Oct 2017 17:02:58 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Maadooliat", "Mehdi", ""], ["Sun", "Ying", ""], ["Chen", "Tianbo", ""]]}, {"id": "1704.03924", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen", "title": "A Tutorial on Kernel Density Estimation and Recent Advances", "comments": "A tutorial paper; accepted to Biostatistics & Epidemiology. Main\n  article: 26 pages, 8 figures. R implementations: 11 pages, generated by\n  Rmarkdown", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This tutorial provides a gentle introduction to kernel density estimation\n(KDE) and recent advances regarding confidence bands and geometric/topological\nfeatures. We begin with a discussion of basic properties of KDE: the\nconvergence rate under various metrics, density derivative estimation, and\nbandwidth selection. Then, we introduce common approaches to the construction\nof confidence intervals/bands, and we discuss how to handle bias. Next, we talk\nabout recent advances in the inference of geometric and topological features of\na density function using KDE. Finally, we illustrate how one can use KDE to\nestimate a cumulative distribution function and a receiver operating\ncharacteristic curve. We provide R implementations related to this tutorial at\nthe end.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 20:45:38 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 13:40:54 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Chen", "Yen-Chi", ""]]}, {"id": "1704.03942", "submitter": "Marco Scutari", "authors": "Marco Scutari", "title": "Beyond Uniform Priors in Bayesian Network Structure Learning", "comments": "37 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian network structure learning is often performed in a Bayesian setting,\nevaluating candidate structures using their posterior probabilities for a given\ndata set. Score-based algorithms then use those posterior probabilities as an\nobjective function and return the maximum a posteriori network as the learned\nmodel. For discrete Bayesian networks, the canonical choice for a posterior\nscore is the Bayesian Dirichlet equivalent uniform (BDeu) marginal likelihood\nwith a uniform (U) graph prior, which assumes a uniform prior both on the\nnetwork structures and on the parameters of the networks. In this paper, we\ninvestigate the problems arising from these assumptions, focusing on those\ncaused by small sample sizes and sparse data. We then propose an alternative\nposterior score: the Bayesian Dirichlet sparse (BDs) marginal likelihood with a\nmarginal uniform (MU) graph prior. Like U+BDeu, MU+BDs does not require any\nprior information on the probabilistic structure of the data and can be used as\na replacement noninformative score. We study its theoretical properties and we\nevaluate its performance in an extensive simulation study, showing that MU+BDs\nis both more accurate than U+BDeu in learning the structure of the network and\ncompetitive in predicting power, while not being computationally more complex\nto estimate.\n", "versions": [{"version": "v1", "created": "Wed, 12 Apr 2017 22:01:09 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Scutari", "Marco", ""]]}, {"id": "1704.03995", "submitter": "Satoshi Kuriki", "authors": "Satoshi Kuriki, Henry P. Wynn", "title": "Optimal experimental design that minimizes the width of simultaneous\n  confidence bands", "comments": "34 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimal experimental design for a curvilinear regression model\nthat minimizes the band-width of simultaneous confidence bands. Simultaneous\nconfidence bands for curvilinear regression are constructed by evaluating the\nvolume of a tube about a curve that is defined as a trajectory of a regression\nbasis vector (Naiman, 1986). The proposed criterion is constructed based on the\nvolume of a tube, and the corresponding optimal design that minimizes the\nvolume of tube is referred to as the tube-volume optimal (TV-optimal) design.\nFor Fourier and weighted polynomial regressions, the problem is formalized as\none of minimization over the cone of Hankel positive definite matrices, and the\ncriterion to minimize is expressed as an elliptic integral. We show that the\nM\\\"obius group keeps our problem invariant, and hence, minimization can be\nconducted over cross-sections of orbits. We demonstrate that for the weighted\npolynomial regression and the Fourier regression with three bases, the\ntube-volume optimal design forms an orbit of the M\\\"obius group containing\nD-optimal designs as representative elements.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 05:16:33 GMT"}, {"version": "v2", "created": "Mon, 26 Mar 2018 14:16:47 GMT"}, {"version": "v3", "created": "Fri, 1 Mar 2019 04:13:12 GMT"}, {"version": "v4", "created": "Sat, 30 Mar 2019 07:17:28 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Kuriki", "Satoshi", ""], ["Wynn", "Henry P.", ""]]}, {"id": "1704.04050", "submitter": "Caifa Zhou", "authors": "Lin Ma, Caifa Zhou, Xi Liu, Yubin Xu", "title": "Adaptive Neighboring Selection Algorithm Based on Curvature Prediction\n  in Manifold Learning", "comments": "3 figures, from Journal of Harbin Institute of Technology", "journal-ref": "Journal of Harbin Institute of Technology, 20(3), pp.119--123\n  (2013)", "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently manifold learning algorithm for dimensionality reduction attracts\nmore and more interests, and various linear and nonlinear, global and local\nalgorithms are proposed. The key step of manifold learning algorithm is the\nneighboring region selection. However, so far for the references we know, few\nof which propose a generally accepted algorithm to well select the neighboring\nregion. So in this paper, we propose an adaptive neighboring selection\nalgorithm, which successfully applies the LLE and ISOMAP algorithms in the\ntest. It is an algorithm that can find the optimal K nearest neighbors of the\ndata points on the manifold. And the theoretical basis of the algorithm is the\napproximated curvature of the data point on the manifold. Based on Riemann\nGeometry, Jacob matrix is a proper mathematical concept to predict the\napproximated curvature. By verifying the proposed algorithm on embedding Swiss\nroll from R3 to R2 based on LLE and ISOMAP algorithm, the simulation results\nshow that the proposed adaptive neighboring selection algorithm is feasible and\nable to find the optimal value of K, making the residual variance relatively\nsmall and better visualization of the results. By quantitative analysis, the\nembedding quality measured by residual variance is increased 45.45% after using\nthe proposed algorithm in LLE.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 09:33:56 GMT"}], "update_date": "2017-04-14", "authors_parsed": [["Ma", "Lin", ""], ["Zhou", "Caifa", ""], ["Liu", "Xi", ""], ["Xu", "Yubin", ""]]}, {"id": "1704.04070", "submitter": "Michele Nguyen", "authors": "Michele Nguyen and Almut E. D. Veraart", "title": "Bridging between short-range and long-range dependence with mixed\n  spatio-temporal Ornstein-Uhlenbeck processes", "comments": null, "journal-ref": null, "doi": "10.1080/17442508.2018.1466886", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While short-range dependence is widely assumed in the literature for its\nsimplicity, long-range dependence is a feature that has been observed in data\nfrom finance, hydrology, geophysics and economics. In this paper, we extend a\nL\\'evy-driven spatio-temporal Ornstein-Uhlenbeck process by randomly varying\nits rate parameter to model both short-range and long-range dependence. This\nparticular set-up allows for non-separable spatio-temporal correlations which\nare desirable for real applications, as well as flexible spatial covariances\nwhich arise from the shapes of influence regions. Theoretical properties such\nas spatio-temporal stationarity and second-order moments are established. An\nisotropic g-class is also used to illustrate how the memory of the process is\nrelated to the probability distribution of the rate parameter. We develop a\nsimulation algorithm for the compound Poisson case which can be used to\napproximate other L\\'evy bases. The generalised method of moments is used for\ninference and simulation experiments are conducted with a view towards\nasymptotic properties.\n", "versions": [{"version": "v1", "created": "Thu, 13 Apr 2017 11:07:47 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Nguyen", "Michele", ""], ["Veraart", "Almut E. D.", ""]]}, {"id": "1704.04315", "submitter": "Youngjun Choe", "authors": "Youngjun Choe, Yen-Chi Chen, Nick Terry", "title": "Information Criterion for Boltzmann Approximation Problems", "comments": "67 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of approximating a density when it can be\nevaluated up to a normalizing constant at a limited number of points. We call\nthis problem the Boltzmann approximation (BA) problem. The BA problem is\nubiquitous in statistics, such as approximating a posterior density for\nBayesian inference and estimating an optimal density for importance sampling.\nApproximating the density with a parametric model can be cast as a model\nselection problem. This problem cannot be addressed with traditional approaches\nthat maximize the (marginal) likelihood of a model, for example, using the\nAkaike information criterion (AIC) or Bayesian information criterion (BIC). We\ninstead aim to minimize the cross-entropy that gauges the deviation of a\nparametric model from the target density. We propose a novel information\ncriterion called the cross-entropy information criterion (CIC) and prove that\nthe CIC is an asymptotically unbiased estimator of the cross-entropy (up to a\nmultiplicative constant) under some regularity conditions. We propose an\niterative method to approximate the target density by minimizing the CIC. We\ndemonstrate that the proposed method selects a parametric model that well\napproximates the target density.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 00:42:46 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 18:06:16 GMT"}, {"version": "v3", "created": "Wed, 7 Oct 2020 17:44:41 GMT"}], "update_date": "2020-10-08", "authors_parsed": [["Choe", "Youngjun", ""], ["Chen", "Yen-Chi", ""], ["Terry", "Nick", ""]]}, {"id": "1704.04356", "submitter": "Kiheiji Nishida", "authors": "Kiheiji Nishida", "title": "Skewing Methods for Variance-Stabilizing Local Linear Regression\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well-known that kernel regression estimators do not produce a constant\nestimator variance over a domain. To correct this problem, Nishida and Kanazawa\n(2015) proposed a variance-stabilizing (VS) local variable bandwidth for Local\nLinear (LL) regression estimator. In contrast, Choi and Hall (1998) proposed\nthe skewing (SK) methods for a univariate LL estimator and constructed a convex\ncombination of one LL estimator and two SK estimators that are symmetrically\nplaced on both sides of the LL estimator (the convex combination (CC)\nestimator) to eliminate higher-order terms in its asymptotic bias. To obtain a\nCC estimator with a constant estimator variance without employing the VS local\nvariable bandwidth, the weight in the convex combination must be determined\nlocally to produce a constant estimator variance. In this study, we compare the\nperformances of two VS methods for a CC estimator and find cases in which the\nweighting method can superior to the VS bandwidth method in terms of the degree\nof variance stabilization.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 05:55:53 GMT"}], "update_date": "2017-04-17", "authors_parsed": [["Nishida", "Kiheiji", ""]]}, {"id": "1704.04415", "submitter": "Yongqiang Tang", "authors": "Yongqiang Tang", "title": "Sample size for comparing negative binomial rates in noninferiority and\n  equivalence trials with unequal follow-up times", "comments": null, "journal-ref": null, "doi": "10.1080/10543406.2017.1333998", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive the sample size formulae for comparing two negative binomial rates\nbased on both the relative and absolute rate difference metrics in\nnoninferiority and equivalence trials with unequal follow-up times, and\nestablish an approximate relationship between the sample sizes required for the\ntreatment comparison based on the two treatment effect metrics. The proposed\nmethod allows the dispersion parameter to vary by treatment groups. The\naccuracy of these methods is assessed by simulations. It is demonstrated that\nignoring the between-subject variation in the follow-up time by setting the\nfollow-up time for all individuals to be the mean follow-up time may greatly\nunderestimate the required size, resulting in underpowered studies. Methods are\nprovided for back-calculating the dispersion parameter based on the published\nsummary results.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 13:11:54 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Tang", "Yongqiang", ""]]}, {"id": "1704.04536", "submitter": "Gane Samb Lo", "authors": "Gane Samb Lo, Amadou Diadi\\'e Ba, Diam Ba", "title": "Divergence Measures Estimation and Its Asymptotic Normality Theory Using\n  Wavelets Empirical Processes", "comments": "33 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide the asymptotic theory of the general of\n$\\phi$-divergences measures, which includes the most common divergence measures\n: Renyi and Tsallis families and the Kullback-Leibler measure. Instead of using\nthe Parzen nonparametric estimators of the probability density functions whose\ndiscrepancy is estimated, we use the wavelets approach and the geometry of\nBesov spaces. One-sided and two-sided statistical tests are derived as well as\nsymmetrized estimators. Almost sure rates of convergence and asymptotic\nnormality theorem are obtained in the general case, and next particularized for\nthe Renyi and Tsallis families and for the Kullback-Leibler measure as well.\nThe applicability of the results to usual distribution functions is addressed.\n", "versions": [{"version": "v1", "created": "Fri, 14 Apr 2017 20:37:03 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Lo", "Gane Samb", ""], ["Ba", "Amadou Diadi\u00e9", ""], ["Ba", "Diam", ""]]}, {"id": "1704.04629", "submitter": "Luca Martino", "authors": "Luca Martino, Victor Elvira", "title": "Metropolis Sampling", "comments": "Wiley StatsRef-Statistics Reference Online, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo (MC) sampling methods are widely applied in Bayesian inference,\nsystem simulation and optimization problems. The Markov Chain Monte Carlo\n(MCMC) algorithms are a well-known class of MC methods which generate a Markov\nchain with the desired invariant distribution. In this document, we focus on\nthe Metropolis-Hastings (MH) sampler, which can be considered as the atom of\nthe MCMC techniques, introducing the basic notions and different properties. We\ndescribe in details all the elements involved in the MH algorithm and the most\nrelevant variants. Several improvements and recent extensions proposed in the\nliterature are also briefly discussed, providing a quick but exhaustive\noverview of the current Metropolis-based sampling's world.\n", "versions": [{"version": "v1", "created": "Sat, 15 Apr 2017 12:15:30 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Martino", "Luca", ""], ["Elvira", "Victor", ""]]}, {"id": "1704.04736", "submitter": "Marcelo Hartmann", "authors": "Marcelo Hartmann", "title": "Extending Owen's integral table and a new multivariate Bernoulli\n  distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We extend some equatilites in the Owen's table of normal integrals (A table\nof normal integrals in Communication in Statistics-Simulation and Computation,\n1980). Furthermore a new probabilistic model for a vector of binary random\nvariables is proposed.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 08:56:17 GMT"}, {"version": "v2", "created": "Fri, 21 Apr 2017 11:14:32 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 12:28:49 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Hartmann", "Marcelo", ""]]}, {"id": "1704.04820", "submitter": "Aaron Molstad", "authors": "Aaron J. Molstad, Adam J. Rothman", "title": "Shrinking characteristics of precision matrix estimators", "comments": null, "journal-ref": "Biometrika, Volume 105, Issue 3, 2018, 563-574", "doi": "10.1093/biomet/asy023", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework to shrink a user-specified characteristic of a\nprecision matrix estimator that is needed to fit a predictive model. Estimators\nin our framework minimize the Gaussian negative loglikelihood plus an $L_1$\npenalty on a linear or affine function evaluated at the optimization variable\ncorresponding to the precision matrix. We establish convergence rate bounds for\nthese estimators and propose an alternating direction method of multipliers\nalgorithm for their computation. Our simulation studies show that our\nestimators can perform better than competitors when they are used to fit\npredictive models. In particular, we illustrate cases where our precision\nmatrix estimators perform worse at estimating the population precision matrix\nbut better at prediction.\n", "versions": [{"version": "v1", "created": "Sun, 16 Apr 2017 21:07:02 GMT"}, {"version": "v2", "created": "Wed, 11 Sep 2019 18:00:58 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Molstad", "Aaron J.", ""], ["Rothman", "Adam J.", ""]]}, {"id": "1704.04839", "submitter": "Li Ma", "authors": "Jacopo Soriano and Li Ma", "title": "Mixture modeling on related samples by $\\psi$-stick breaking and kernel\n  perturbation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been great interest recently in applying nonparametric kernel\nmixtures in a hierarchical manner to model multiple related data samples\njointly. In such settings several data features are commonly present: (i) the\nrelated samples often share some, if not all, of the mixture components but\nwith differing weights, (ii) only some, not all, of the mixture components vary\nacross the samples, and (iii) often the shared mixture components across\nsamples are not aligned perfectly in terms of their location and spread, but\nrather display small misalignments either due to systematic cross-sample\ndifference or more often due to uncontrolled, extraneous causes. Properly\nincorporating these features in mixture modeling will enhance the efficiency of\ninference, whereas ignoring them not only reduces efficiency but can jeopardize\nthe validity of the inference due to issues such as confounding. We introduce\ntwo techniques for incorporating these features in modeling related data\nsamples using kernel mixtures. The first technique, called $\\psi$-stick\nbreaking, is a joint generative process for the mixing weights through the\nbreaking of both a stick shared by all the samples for the components that do\nnot vary in size across samples and an idiosyncratic stick for each sample for\nthose components that do vary in size. The second technique is to imbue random\nperturbation into the kernels, thereby accounting for cross-sample\nmisalignment. These techniques can be used either separately or together in\nboth parametric and nonparametric kernel mixtures. We derive efficient Bayesian\ninference recipes based on MCMC sampling for models featuring these techniques,\nand illustrate their work through both simulated data and a real flow cytometry\ndata set in prediction/estimation, cross-sample calibration, and testing\nmulti-sample differences.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 00:58:37 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Soriano", "Jacopo", ""], ["Ma", "Li", ""]]}, {"id": "1704.04858", "submitter": "Zach Branson", "authors": "Zach Branson, Maxime Rischard, Luke Bornn, and Luke Miratrix", "title": "A Nonparametric Bayesian Methodology for Regression Discontinuity\n  Designs", "comments": "40 pages, 5 figures, 5 tables", "journal-ref": null, "doi": "10.1016/j.jspi.2019.01.003", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most popular methodologies for estimating the average treatment\neffect at the threshold in a regression discontinuity design is local linear\nregression (LLR), which places larger weight on units closer to the threshold.\nWe propose a Gaussian process regression methodology that acts as a Bayesian\nanalog to LLR for regression discontinuity designs. Our methodology provides a\nflexible fit for treatment and control responses by placing a general prior on\nthe mean response functions. Furthermore, unlike LLR, our methodology can\nincorporate uncertainty in how units are weighted when estimating the treatment\neffect. We prove our method is consistent in estimating the average treatment\neffect at the threshold. Furthermore, we find via simulation that our method\nexhibits promising coverage, interval length, and mean squared error properties\ncompared to standard LLR and state-of-the-art LLR methodologies. Finally, we\nexplore the performance of our method on a real-world example by studying the\nimpact of being a first-round draft pick on the performance and playing time of\nbasketball players in the National Basketball Association.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 03:22:13 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 03:47:07 GMT"}, {"version": "v3", "created": "Wed, 13 Dec 2017 16:28:07 GMT"}, {"version": "v4", "created": "Wed, 14 Mar 2018 16:06:50 GMT"}, {"version": "v5", "created": "Sun, 30 Sep 2018 18:46:14 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Branson", "Zach", ""], ["Rischard", "Maxime", ""], ["Bornn", "Luke", ""], ["Miratrix", "Luke", ""]]}, {"id": "1704.04926", "submitter": "Fabio Rapallo", "authors": "Cristiano Bocci and Fabio Rapallo", "title": "Exact tests to compare contingency tables under quasi-independence and\n  quasi-symmetry", "comments": "14 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we define log-linear models to compare several square\ncontingency tables under the quasi-independence or the quasi-symmetry model,\nand the relevant Markov bases are theoretically characterized. Through Markov\nbases, an exact test to evaluate if two or more tables fit a common model is\nintroduced. Two real-data examples illustrate the use of these models in\ndifferent fields of applications.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 10:49:12 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Bocci", "Cristiano", ""], ["Rapallo", "Fabio", ""]]}, {"id": "1704.05029", "submitter": "Gianluca Mastrantonio", "authors": "Gianluca Mastrantonio, Giovanna Jona Lasinio, and Alan E. Gelfand", "title": "Spatio-temporal circular models with non-separable covariance structure", "comments": null, "journal-ref": null, "doi": "10.1007/s11749-015-0458-y", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Circular data arise in many areas of application. Recently, there has been\ninterest in looking at circular data collected separately over time and over\nspace. Here, we extend some of this work to the spatio-temporal setting,\nintroducing space-time dependence. We accommodate covariates, implement full\nkriging and forecasting, and also allow for a nugget which can be time\ndependent. We work within a Bayesian framework, introducing suitable latent\nvariables to facilitate Markov chain Monte Carlo (MCMC) model fitting. The\nBayesian framework enables us to implement full inference, obtaining predictive\ndistributions for kriging and forecasting. We offer comparison between the less\nflexible but more interpretable wrapped Gaussian process and the more flexible\nbut less interpretable projected Gaussian process. We do this illustratively\nusing both simulated data and data from computer model output for wave\ndirections in the Adriatic Sea off the coast of Italy.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 16:42:39 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Mastrantonio", "Gianluca", ""], ["Lasinio", "Giovanna Jona", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "1704.05032", "submitter": "Gianluca Mastrantonio", "authors": "Gianluca Mastrantonio, Giovanna Jona Lasinio, and Alan E. Gelfand", "title": "The wrapped skew Gaussian process for analyzing spatio-temporal data", "comments": null, "journal-ref": null, "doi": "10.1007/s00477-015-1163-9", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider modeling of angular or directional data viewed as a linear\nvariable wrapped onto a unit circle. In particular, we focus on the\nspatio-temporal context, motivated by a collection of wave directions obtained\nas computer model output developed dynamically over a collection of spatial\nlocations. We propose a novel wrapped skew Gaussian process which enriches the\nclass of wrapped Gaussian process. The wrapped skew Gaussian process enables\nmore flexible marginal distributions than the symmetric ones arising under the\nwrapped Gaussian process and it allows straightforward interpretation of\nparameters. We clarify that replication through time enables criticism of the\nwrapped process in favor of the wrapped skew process. We formulate a\nhierarchical model incorporating this process and show how to introduce\nappropriate latent variables in order to enable efficient fitting to dynamic\nspatial directional data. We also show how to implement kriging and forecasting\nunder this model. We provide a simulation example as a proof of concept as well\nas a real data example. Both examples reveal consequential improvement in\npredictive performance for the wrapped skew Gaussian specification compared\nwith the earlier wrapped Gaussian version.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 16:56:29 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Mastrantonio", "Gianluca", ""], ["Lasinio", "Giovanna Jona", ""], ["Gelfand", "Alan E.", ""]]}, {"id": "1704.05037", "submitter": "Gianluca Mastrantonio", "authors": "Gianluca Mastrantonio, Gianfranco Calise", "title": "Hidden Markov model for discrete circular-linear wind data time series", "comments": null, "journal-ref": null, "doi": "10.1080/00949655.2016.1142544", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we deal with a bivariate time series of wind speed and\ndirection. Our observed data have peculiar features, such as informative\nmissing values, non-reliable measures under a specific condition and\ninterval-censored data, that we take into account in the model specification.\nWe analyze the time series with a non-parametric Bayesian hidden Markov model,\nintroducing a new emission distribution based on the invariant wrapped Poisson,\nthe Poisson and the hurdle density, suitable to model our data. The model is\nestimated on simulated datasets and on the real data example that motivated\nthis work.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 17:10:28 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Mastrantonio", "Gianluca", ""], ["Calise", "Gianfranco", ""]]}, {"id": "1704.05046", "submitter": "Ruoqing Zhu", "authors": "Qiang Sun, Ruoqing Zhu, Tao Wang, Donglin Zeng", "title": "Counting Process Based Dimension Reduction Methods for Censored Outcomes", "comments": "First version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a class of dimension reduction methods for right censored survival\ndata using a counting process representation of the failure process.\nSemiparametric estimating equations are constructed to estimate the dimension\nreduction subspace for the failure time model. The proposed method addresses\ntwo fundamental limitations of existing approaches. First, using the counting\nprocess formulation, it does not require any estimation of the censoring\ndistribution to compensate the bias in estimating the dimension reduction\nsubspace. Second, the nonparametric part in the estimating equations is\nadaptive to the structural dimension, hence the approach circumvents the curse\nof dimensionality. Asymptotic normality is established for the obtained\nestimators. We further propose a computationally efficient approach that\nsimplifies the estimation equation formulations and requires only a singular\nvalue decomposition to estimate the dimension reduction subspace. Numerical\nstudies suggest that our new approaches exhibit significantly improved\nperformance for estimating the true dimension reduction subspace. We further\nconduct a real data analysis on a skin cutaneous melanoma dataset from The\nCancer Genome Atlas. The proposed method is implemented in the R package\n\"orthoDr\".\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 17:57:51 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 02:45:51 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Sun", "Qiang", ""], ["Zhu", "Ruoqing", ""], ["Wang", "Tao", ""], ["Zeng", "Donglin", ""]]}, {"id": "1704.05074", "submitter": "Mohammad Arashi", "authors": "B. Yuzbasi, M. Arashi and S. E. Ahmed", "title": "Big Data Analysis Using Shrinkage Strategies", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we apply shrinkage strategies to estimate regression\ncoefficients efficiently for the high-dimensional multiple regression model,\nwhere the number of samples is smaller than the number of predictors. We assume\nin the sparse linear model some of the predictors have very weak influence on\nthe response of interest. We propose to shrink estimators more than usual.\nSpecifically, we use integrated estimation strategies in sub and full models\nand shrink the integrated estimators by incorporating a bounded measurable\nfunction of some weights. The exhibited double shrunken estimators improve the\nprediction performance of sub models significantly selected from existing\nLasso-type variable selection methods. Monte Carlo simulation studies as well\nas real examples of eye data and Riboavin data confirm the superior performance\nof the estimators in the high-dimensional regression model.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 18:02:29 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Yuzbasi", "B.", ""], ["Arashi", "M.", ""], ["Ahmed", "S. E.", ""]]}, {"id": "1704.05098", "submitter": "Yun Yang", "authors": "Yun Yang", "title": "Statistical inference for high dimensional regression via Constrained\n  Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new method for estimation and constructing\nconfidence intervals for low-dimensional components in a high-dimensional\nmodel. The proposed estimator, called Constrained Lasso (CLasso) estimator, is\nobtained by simultaneously solving two estimating equations---one imposing a\nzero-bias constraint for the low-dimensional parameter and the other forming an\n$\\ell_1$-penalized procedure for the high-dimensional nuisance parameter. By\ncarefully choosing the zero-bias constraint, the resulting estimator of the low\ndimensional parameter is shown to admit an asymptotically normal limit\nattaining the Cram\\'{e}r-Rao lower bound in a semiparametric sense. We propose\na tuning-free iterative algorithm for implementing the CLasso. We show that\nwhen the algorithm is initialized at the Lasso estimator, the de-sparsified\nestimator proposed in van de Geer et al. [\\emph{Ann. Statist.} {\\bf 42} (2014)\n1166--1202] is asymptotically equivalent to the first iterate of the algorithm.\nWe analyse the asymptotic properties of the CLasso estimator and show the\nglobally linear convergence of the algorithm. We also demonstrate encouraging\nempirical performance of the CLasso through numerical studies.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 19:21:33 GMT"}], "update_date": "2017-04-19", "authors_parsed": [["Yang", "Yun", ""]]}, {"id": "1704.05268", "submitter": "Valerio Lucarini", "authors": "Vera Melinda Galfi, Tamas Bodai, Valerio Lucarini", "title": "Convergence of extreme value statistics in a two-layer quasi-geostrophic\n  atmospheric model", "comments": "28 pages, 8 figures; improved version following the referees'\n  comments", "journal-ref": null, "doi": "10.1155/2017/5340858", "report-no": null, "categories": "nlin.CD cond-mat.stat-mech physics.ao-ph physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We search for the signature of universal properties of extreme events,\ntheoretically predicted for Axiom A flows, in a chaotic and high dimensional\ndynamical system by studying the convergence of GEV (Generalized Extreme Value)\nand GP (Generalized Pareto) shape parameter estimates to a theoretical value,\nexpressed in terms of partial dimensions of the attractor, which are global\nproperties. We consider a two layer quasi-geostrophic (QG) atmospheric model\nusing two forcing levels, and analyse extremes of different types of physical\nobservables (local, zonally-averaged energy, and the average value of energy\nover the mid-latitudes). Regarding the predicted universality, we find closer\nagreement in the shape parameter estimates only in the case of strong forcing,\nproducing a highly chaotic behaviour, for some observables (the local energy at\nevery latitude). Due to the limited (though very large) data size and the\npresence of serial correlations, it is difficult to obtain robust statistics of\nextremes in case of the other observables. In the case of weak forcing,\ninducing a less pronounced chaotic flow with regime behaviour, we find worse\nagreement with the theory developed for Axiom A flows, which is unsurprising\nconsidering the properties of the system.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 11:01:23 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 14:24:26 GMT"}], "update_date": "2020-11-16", "authors_parsed": [["Galfi", "Vera Melinda", ""], ["Bodai", "Tamas", ""], ["Lucarini", "Valerio", ""]]}, {"id": "1704.05318", "submitter": "Mickael Binois", "authors": "Micka\\\"el Binois, David Ginsbourger (3MI-ENSMSE), Olivier Roustant\n  (GdR MASCOT-NUM)", "title": "On the choice of the low-dimensional domain for global optimization via\n  random embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge of taking many variables into account in optimization problems\nmay be overcome under the hypothesis of low effective dimensionality. Then, the\nsearch of solutions can be reduced to the random embedding of a low dimensional\nspace into the original one, resulting in a more manageable optimization\nproblem. Specifically, in the case of time consuming black-box functions and\nwhen the budget of evaluations is severely limited, global optimization with\nrandom embeddings appears as a sound alternative to random search. Yet, in the\ncase of box constraints on the native variables, defining suitable bounds on a\nlow dimensional domain appears to be complex. Indeed, a small search domain\ndoes not guarantee to find a solution even under restrictive hypotheses about\nthe function, while a larger one may slow down convergence dramatically. Here\nwe tackle the issue of low-dimensional domain selection based on a detailed\nstudy of the properties of the random embedding, giving insight on the\naforementioned difficulties. In particular, we describe a minimal\nlow-dimensional set in correspondence with the embedded search space. We\nadditionally show that an alternative equivalent embedding procedure yields\nsimultaneously a simpler definition of the low-dimensional minimal set and\nbetter properties in practice. Finally, the performance and robustness gains of\nthe proposed enhancements for Bayesian optimization are illustrated on\nnumerical examples.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 13:10:41 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 13:55:21 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 14:16:16 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Binois", "Micka\u00ebl", "", "3MI-ENSMSE"], ["Ginsbourger", "David", "", "3MI-ENSMSE"], ["Roustant", "Olivier", "", "GdR MASCOT-NUM"]]}, {"id": "1704.05627", "submitter": "Benjamin Taylor", "authors": "Benjamin M. Taylor, Ricardo Andrade-Pacheco, Hugh J. W. Sturrock", "title": "Continuous Inference for Aggregated Point Process Data", "comments": "31 pages, 11 Figures. Note: figure quality substantially reduced in\n  order to meet arXiv requirements", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces new methods for inference with count data registered\non a set of aggregation units. Such data are omnipresent in epidemiology due to\nconfidentiality issues: it is much more common to know the county in which an\nindividual resides, say, than know their exact location in space. Inference for\naggregated data has traditionally made use of models for discrete spatial\nvariation, for example conditional autoregressive models (CAR). We argue that\nsuch discrete models can be improved from both a scientific and inferential\nperspective by using spatiotemporally continuous models to directly model the\naggregated counts. We introduce methods for delivering (limiting) continuous\ninference with spatitemporal aggregated count data in which the aggregation\nunits might change over time and are subject to uncertainty. We illustrate our\nmethods using two examples: from epidemiology, spatial prediction malaria\nincidence in Namibia; and from politics, forecasting voting under the proposed\nchanges to parlimentary boundaries in the United Kingdom.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 06:48:29 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Taylor", "Benjamin M.", ""], ["Andrade-Pacheco", "Ricardo", ""], ["Sturrock", "Hugh J. W.", ""]]}, {"id": "1704.05706", "submitter": "Edward Kennedy", "authors": "Edward H. Kennedy, Steve Harris, Luke J. Keele", "title": "Survivor-complier effects in the presence of selection on treatment,\n  with application to a study of prompt ICU admission", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pre-treatment selection or censoring (`selection on treatment') can occur\nwhen two treatment levels are compared ignoring the third option of neither\ntreatment, in `censoring by death' settings where treatment is only defined for\nthose who survive long enough to receive it, or in general in studies where the\ntreatment is only defined for a subset of the population. Unfortunately, the\nstandard instrumental variable (IV) estimand is not defined in the presence of\nsuch selection, so we consider estimating a new survivor-complier causal\neffect. Although this effect is generally not identified under standard IV\nassumptions, it is possible to construct sharp bounds. We derive these bounds\nand give a corresponding data-driven sensitivity analysis, along with\nnonparametric yet efficient estimation methods. Importantly, our approach\nallows for high-dimensional confounding adjustment, and valid inference even\nafter employing machine learning. Incorporating covariates can tighten bounds\ndramatically, especially when they are strong predictors of the selection\nprocess. We apply the methods in a UK cohort study of critical care patients to\nexamine the mortality effects of prompt admission to the intensive care unit,\nusing ICU bed availability as an instrument.\n", "versions": [{"version": "v1", "created": "Wed, 19 Apr 2017 12:26:18 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 00:16:02 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Kennedy", "Edward H.", ""], ["Harris", "Steve", ""], ["Keele", "Luke J.", ""]]}, {"id": "1704.05761", "submitter": "Bin Liu", "authors": "Bin Liu, Ke-Jia Chen", "title": "Maximum Likelihood Estimation based on Random Subspace EDA: Application\n  to Extrasolar Planet Detection", "comments": "12 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses maximum likelihood (ML) estimation based model fitting\nin the context of extrasolar planet detection. This problem is featured by the\nfollowing properties: 1) the candidate models under consideration are highly\nnonlinear; 2) the likelihood surface has a huge number of peaks; 3) the\nparameter space ranges in size from a few to dozens of dimensions. These\nproperties make the ML search a very challenging problem, as it lacks any\nanalytical or gradient based searching solution to explore the parameter space.\nA population based searching method, called estimation of distribution\nalgorithm (EDA), is adopted to explore the model parameter space starting from\na batch of random locations. EDA is featured by its ability to reveal and\nutilize problem structures. This property is desirable for characterizing the\ndetections. However, it is well recognized that EDAs can not scale well to\nlarge scale problems, as it consists of iterative random sampling and model\nfitting procedures, which results in the well-known dilemma curse of\ndimensionality. A novel mechanism to perform EDAs in interactive random\nsubspaces spanned by correlated variables is proposed and the hope is to\nalleviate the curse of dimensionality for EDAs by performing the operations of\nsampling and model fitting in lower dimensional subspaces. The effectiveness of\nthe proposed algorithm is verified via both benchmark numerical studies and\nreal data analysis.\n", "versions": [{"version": "v1", "created": "Tue, 18 Apr 2017 05:37:32 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 14:14:00 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Liu", "Bin", ""], ["Chen", "Ke-Jia", ""]]}, {"id": "1704.05993", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa, Genya Kobayashi, Yuki Kawakubo", "title": "Latent Mixture Modeling for Clustered Data", "comments": "17 pages", "journal-ref": "Statistics and Computing (2019) 29, 537-548", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes a mixture modeling approach to estimating cluster-wise\nconditional distributions in clustered (grouped) data. We adapt the\nmixture-of-experts model to the latent distributions, and propose a model in\nwhich each cluster-wise density is represented as a mixture of latent experts\nwith cluster-wise mixing proportions distributed as Dirichlet distribution. The\nmodel parameters are estimated by maximizing the marginal likelihood function\nusing a newly developed Monte Carlo Expectation-Maximization algorithm. We also\nextend the model such that the distribution of cluster-wise mixing proportions\ndepends on some cluster-level covariates. The finite sample performance of the\nproposed model is compared with some existing mixture modeling approaches as\nwell as linear mixed model through the simulation studies. The proposed model\nis also illustrated with the posted land price data in Japan.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 03:33:52 GMT"}], "update_date": "2019-09-10", "authors_parsed": [["Sugasawa", "Shonosuke", ""], ["Kobayashi", "Genya", ""], ["Kawakubo", "Yuki", ""]]}, {"id": "1704.05995", "submitter": "David Sinclair", "authors": "David G. Sinclair and Giles Hooker", "title": "An Expectation Maximization Algorithm for High-Dimensional Model\n  Selection for the Ising Model with Misclassified States", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the misclassified Ising Model; a framework for analyzing dependent\nbinary data where the binary state is susceptible to error. We extend the\ntheoretical results of the model selection method presented in Ravikumar et.\nal. (2010) to show that the method will still correctly identify edges in the\nunderlying graphical model under suitable misclassification settings. With\nknowledge of the misclassification process, an expectation maximization\nalgorithm is developed that accounts for misclassification during model\nselection. We illustrate the increase of performance of the proposed\nexpectation maximization algorithm with simulated data, and using data from a\nfunctional magnetic resonance imaging analysis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 03:43:31 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Sinclair", "David G.", ""], ["Hooker", "Giles", ""]]}, {"id": "1704.06022", "submitter": "Zimu Chen", "authors": "Zhanfeng Wang and Zhuojian Chen and Zimu Chen", "title": "H-relative error estimation approach for multiplicative regression model\n  with random effect", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative error approaches are more of concern compared to absolute error ones\nsuch as the least square and least absolute deviation, when it needs scale\ninvariant of output variable, for example with analyzing stock and survival\ndata. An h-relative error estimation method via the h-likelihood is developed\nto avoid heavy and intractable integration for a multiplicative regression\nmodel with random effect. Statistical properties of the parameters and random\neffect in the model are studied. To estimate the parameters, we propose an\nh-relative error computation procedure. Numerical studies including simulation\nand real examples show the proposed method performs well.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 06:21:22 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Wang", "Zhanfeng", ""], ["Chen", "Zhuojian", ""], ["Chen", "Zimu", ""]]}, {"id": "1704.06210", "submitter": "Kirsty Rhodes", "authors": "Kirsty Rhodes, Rebecca Turner, Rupert Payne, Ian White", "title": "Computationally efficient methods for fitting mixed models to electronic\n  health records data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by two case studies using primary care records from the Clinical\nPractice Research Datalink, we describe statistical methods that facilitate the\nanalysis of tall data, with very large numbers of observations. Our focus is on\ninvestigating the association between patient characteristics and an outcome of\ninterest, while allowing for variation among general practices. We explore ways\nto fit mixed effects models to tall data, including predictors of interest and\nconfounding factors as covariates, and including random intercepts to allow for\nheterogeneity in outcome among practices. We introduce: (1) weighted regression\nand (2) meta-analysis of estimated regression coefficients from each practice.\nBoth methods reduce the size of the dataset, thus decreasing the time required\nfor statistical analysis. We compare the methods to an existing subsampling\napproach. All methods give similar point estimates, and weighted regression and\nmeta-analysis give similar standard errors for point estimates to analysis of\nthe entire dataset, but the subsampling method gives larger standard errors.\nWhere all data are discrete, weighted regression is equivalent to fitting the\nmixed model to the entire dataset. In the presence of a continuous covariate,\nmeta-analysis is useful. Both methods are easy to implement in standard\nstatistical software.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 16:16:31 GMT"}, {"version": "v2", "created": "Fri, 11 May 2018 10:58:07 GMT"}], "update_date": "2018-05-14", "authors_parsed": [["Rhodes", "Kirsty", ""], ["Turner", "Rebecca", ""], ["Payne", "Rupert", ""], ["White", "Ian", ""]]}, {"id": "1704.06230", "submitter": "Ansgar Steland", "authors": "Ansgar Steland and Rainer von Sachs", "title": "Large-sample approximations for variance-covariance matrices of\n  high-dimensional time series", "comments": null, "journal-ref": "Bernoulli, 23, Number 4A (2017), 2299-2329", "doi": "10.3150/16-BEJ811", "report-no": null, "categories": "math.PR math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributional approximations of (bi--) linear functions of sample\nvariance-covariance matrices play a critical role to analyze vector time\nseries, as they are needed for various purposes, especially to draw inference\non the dependence structure in terms of second moments and to analyze\nprojections onto lower dimensional spaces as those generated by principal\ncomponents. This particularly applies to the high-dimensional case, where the\ndimension $d$ is allowed to grow with the sample size $n$ and may even be\nlarger than $n$. We establish large-sample approximations for such bilinear\nforms related to the sample variance-covariance matrix of a high-dimensional\nvector time series in terms of strong approximations by Brownian motions. The\nresults cover weakly dependent as well as many long-range dependent linear\nprocesses and are valid for uniformly $ \\ell_1 $-bounded projection vectors,\nwhich arise, either naturally or by construction, in many statistical problems\nextensively studied for high-dimensional series. Among those problems are\nsparse financial portfolio selection, sparse principal components, the LASSO,\nshrinkage estimation and change-point analysis for high--dimensional time\nseries, which matter for the analysis of big data and are discussed in greater\ndetail.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 16:52:23 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Steland", "Ansgar", ""], ["von Sachs", "Rainer", ""]]}, {"id": "1704.06250", "submitter": "Selden Crary", "authors": "Selden Crary, Tatiana Nizhegorodova, and Michael Saunders", "title": "The Nu Class of Low-Degree-Truncated, Rational, Generalized Functions.\n  Ia. MINOS for IMSPE Evaluation and Optimal-IMSPE-Design Search", "comments": "12 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide compact algebraic expressions that replace the lengthy\nsymbolic-algebra-generated integrals I6 and I8 in Part I of this series of\npapers [1]. The MRSE entries of Part I, Table 4.3 are thus updated to simpler\nalgebraic expressions. We use MINOS [2-4] to tabulate several IMSPE-optimal\ndesigns with one factor and one or two design points, for the exponential-, two\nof the Mat\\'ern-, and the Gaussian-correlation functions, i.e. for the class of\nproblems considered in Part I. The tabulated results can be used as standards\nfor optimal-design-software developers and users.\n", "versions": [{"version": "v1", "created": "Thu, 20 Apr 2017 17:53:03 GMT"}, {"version": "v2", "created": "Mon, 20 May 2019 04:08:24 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Crary", "Selden", ""], ["Nizhegorodova", "Tatiana", ""], ["Saunders", "Michael", ""]]}, {"id": "1704.06374", "submitter": "Scott Sisson", "authors": "G. S. Rodrigues and D. Prangle and S. A. Sisson", "title": "Recalibration: A post-processing method for approximate Bayesian\n  computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new recalibration post-processing method is presented to improve the\nquality of the posterior approximation when using Approximate Bayesian\nComputation (ABC) algorithms. Recalibration may be used in conjunction with\nexisting post-processing methods, such as regression-adjustments. In addition,\nthis work extends and strengthens the links between ABC and indirect inference\nalgorithms, allowing more extensive use of misspecified auxiliary models in the\nABC context. The method is illustrated using simulated examples to demonstrate\nthe effects of recalibration under various conditions, and through an\napplication to an analysis of stereological extremes both with and without the\nuse of auxiliary models. Code to implement recalibration post-processing is\navailable in the R package, abctools.\n", "versions": [{"version": "v1", "created": "Fri, 21 Apr 2017 01:08:14 GMT"}], "update_date": "2017-04-24", "authors_parsed": [["Rodrigues", "G. S.", ""], ["Prangle", "D.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1704.06742", "submitter": "Chao Gao", "authors": "Chao Gao and John Lafferty", "title": "Testing Network Structure Using Relations Between Small Subgraph\n  Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of testing for structure in networks using relations\nbetween the observed frequencies of small subgraphs. We consider the statistics\n\\begin{align*} T_3 & =(\\text{edge frequency})^3 - \\text{triangle frequency}\\\\\nT_2 & =3(\\text{edge frequency})^2(1-\\text{edge frequency}) - \\text{V-shape\nfrequency} \\end{align*} and prove a central limit theorem for $(T_2, T_3)$\nunder an Erd\\H{o}s-R\\'{e}nyi null model. We then analyze the power of the\nassociated $\\chi^2$ test statistic under a general class of alternative models.\nIn particular, when the alternative is a $k$-community stochastic block model,\nwith $k$ unknown, the power of the test approaches one. Moreover, the\nsignal-to-noise ratio required is strictly weaker than that required for\ncommunity detection. We also study the relation with other statistics over\nthree-node subgraphs, and analyze the error under two natural algorithms for\nsampling small subgraphs. Together, our results show how global structural\ncharacteristics of networks can be inferred from local subgraph frequencies,\nwithout requiring the global community structure to be explicitly estimated.\n", "versions": [{"version": "v1", "created": "Sat, 22 Apr 2017 03:34:30 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Gao", "Chao", ""], ["Lafferty", "John", ""]]}, {"id": "1704.06977", "submitter": "Xin Bing", "authors": "Xin Bing, Florentina Bunea, Yang Ning, Marten Wegkamp", "title": "Adaptive Estimation in Structured Factor Models with Applications to\n  Overlapping Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces a novel estimation method, called LOVE, of the entries\nand structure of a loading matrix A in a sparse latent factor model X = AZ + E,\nfor an observable random vector X in Rp, with correlated unobservable factors Z\n\\in RK, with K unknown, and independent noise E. Each row of A is scaled and\nsparse. In order to identify the loading matrix A, we require the existence of\npure variables, which are components of X that are associated, via A, with one\nand only one latent factor. Despite the fact that the number of factors K, the\nnumber of the pure variables, and their location are all unknown, we only\nrequire a mild condition on the covariance matrix of Z, and a minimum of only\ntwo pure variables per latent factor to show that A is uniquely defined, up to\nsigned permutations. Our proofs for model identifiability are constructive, and\nlead to our novel estimation method of the number of factors and of the set of\npure variables, from a sample of size n of observations on X. This is the first\nstep of our LOVE algorithm, which is optimization-free, and has low\ncomputational complexity of order p2. The second step of LOVE is an easily\nimplementable linear program that estimates A. We prove that the resulting\nestimator is minimax rate optimal up to logarithmic factors in p. The model\nstructure is motivated by the problem of overlapping variable clustering,\nubiquitous in data science. We define the population level clusters as groups\nof those components of X that are associated, via the sparse matrix A, with the\nsame unobservable latent factor, and multi-factor association is allowed.\nClusters are respectively anchored by the pure variables, and form overlapping\nsub-groups of the p-dimensional random vector X. The Latent model approach to\nOVErlapping clustering is reflected in the name of our algorithm, LOVE.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 20:43:44 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 04:06:07 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 03:01:20 GMT"}, {"version": "v4", "created": "Thu, 20 Jun 2019 23:02:35 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Bing", "Xin", ""], ["Bunea", "Florentina", ""], ["Ning", "Yang", ""], ["Wegkamp", "Marten", ""]]}, {"id": "1704.06988", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss, Jonathan R. Stroud, Christopher K. Wikle", "title": "Ensemble Kalman methods for high-dimensional hierarchical dynamic\n  space-time models", "comments": null, "journal-ref": "Journal of the American Statistical Association, Theory & Methods\n  (2019+)", "doi": "10.1080/01621459.2019.1592753", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new class of filtering and smoothing methods for inference in\nhigh-dimensional, nonlinear, non-Gaussian, spatio-temporal state-space models.\nThe main idea is to combine the ensemble Kalman filter and smoother, developed\nin the geophysics literature, with state-space algorithms from the statistics\nliterature. Our algorithms address a variety of estimation scenarios, including\non-line and off-line state and parameter estimation. We take a Bayesian\nperspective, for which the goal is to generate samples from the joint posterior\ndistribution of states and parameters. The key benefit of our approach is the\nuse of ensemble Kalman methods for dimension reduction, which allows inference\nfor high-dimensional state vectors. We compare our methods to existing ones,\nincluding ensemble Kalman filters, particle filters, and particle MCMC. Using a\nreal data example of cloud motion and data simulated under a number of\nnonlinear and non-Gaussian scenarios, we show that our approaches outperform\nthese existing methods.\n", "versions": [{"version": "v1", "created": "Sun, 23 Apr 2017 21:51:54 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 22:03:51 GMT"}], "update_date": "2019-03-22", "authors_parsed": [["Katzfuss", "Matthias", ""], ["Stroud", "Jonathan R.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1704.07008", "submitter": "Weixin Cai", "authors": "Weixin Cai, Nima S. Hejazi, Alan E. Hubbard", "title": "Data-adaptive statistics for multiple hypothesis testing in\n  high-dimensional settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current statistical inference problems in areas like astronomy, genomics, and\nmarketing routinely involve the simultaneous testing of thousands -- even\nmillions -- of null hypotheses. For high-dimensional multivariate\ndistributions, these hypotheses may concern a wide range of parameters, with\ncomplex and unknown dependence structures among variables. In analyzing such\nhypothesis testing procedures, gains in efficiency and power can be achieved by\nperforming variable reduction on the set of hypotheses prior to testing. We\npresent in this paper an approach using data-adaptive multiple testing that\nserves exactly this purpose. This approach applies data mining techniques to\nscreen the full set of covariates on equally sized partitions of the whole\nsample via cross-validation. This generalized screening procedure is used to\ncreate average ranks for covariates, which are then used to generate a reduced\n(sub)set of hypotheses, from which we compute test statistics that are\nsubsequently subjected to standard multiple testing corrections. The principal\nadvantage of this methodology lies in its providing valid statistical inference\nwithout the \\textit{a priori} specifying which hypotheses will be tested. Here,\nwe present the theoretical details of this approach, confirm its validity via a\nsimulation study, and exemplify its use by applying it to the analysis of data\non microRNA differential expression.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 00:50:29 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Cai", "Weixin", ""], ["Hejazi", "Nima S.", ""], ["Hubbard", "Alan E.", ""]]}, {"id": "1704.07016", "submitter": "Minzhe Wang", "authors": "Zheng Tracy Ke, Minzhe Wang", "title": "A new SVD approach to optimal topic estimation", "comments": "73 pages, 8 figures, 6 tables; considered two different VH algorithm,\n  OVH and GVH, and provided theoretical analysis for each algorithm;\n  re-organized upper bound theory part; added the subsection of comparing error\n  rate with other existing methods; provided another improved version of error\n  analysis through Bernstein inequality for martingales", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the probabilistic topic models, the quantity of interest---a low-rank\nmatrix consisting of topic vectors---is hidden in the text corpus matrix,\nmasked by noise, and Singular Value Decomposition (SVD) is a potentially useful\ntool for learning such a matrix. However, different rows and columns of the\nmatrix are usually in very different scales and the connection between this\nmatrix and the singular vectors of the text corpus matrix are usually\ncomplicated and hard to spell out, so how to use SVD for learning topic models\nfaces challenges.\n  We overcome the challenges by introducing a proper Pre-SVD normalization of\nthe text corpus matrix and a proper column-wise scaling for the matrix of\ninterest, and by revealing a surprising Post-SVD low-dimensional {\\it simplex}\nstructure. The simplex structure, together with the Pre-SVD normalization and\ncolumn-wise scaling, allows us to conveniently reconstruct the matrix of\ninterest, and motivates a new SVD-based approach to learning topic models.\n  We show that under the popular probabilistic topic model \\citep{hofmann1999},\nour method has a faster rate of convergence than existing methods in a wide\nvariety of cases. In particular, for cases where documents are long or $n$ is\nmuch larger than $p$, our method achieves the optimal rate. At the heart of the\nproofs is a tight element-wise bound on singular vectors of a multinomially\ndistributed data matrix, which do not exist in literature and we have to derive\nby ourself.\n  We have applied our method to two data sets, Associated Process (AP) and\nStatistics Literature Abstract (SLA), with encouraging results. In particular,\nthere is a clear simplex structure associated with the SVD of the data\nmatrices, which largely validates our discovery.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 01:45:51 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 16:05:12 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Ke", "Zheng Tracy", ""], ["Wang", "Minzhe", ""]]}, {"id": "1704.07040", "submitter": "Daniel Eck", "authors": "Daniel J. Eck", "title": "Bootstrapping for multivariate linear regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate linear regression model is an important tool for\ninvestigating relationships between several response variables and several\npredictor variables. The primary interest is in inference about the unknown\nregression coefficient matrix. We propose multivariate bootstrap techniques as\na means for making inferences about the unknown regression coefficient matrix.\nThese bootstrapping techniques are extensions of those developed in Freedman\n(1981), which are only appropriate for univariate responses. Extensions to the\nmultivariate linear regression model are made without proof. We formalize this\nextension and prove its validity. A real data example and two simulated data\nexamples which offer some finite sample verification of our theoretical results\nare provided.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 04:55:16 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 16:11:19 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Eck", "Daniel J.", ""]]}, {"id": "1704.07272", "submitter": "Kody Law", "authors": "Ajay Jasra, Kody Law, and Carina Suciu", "title": "Advanced Multilevel Monte Carlo Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article reviews the application of advanced Monte Carlo techniques in\nthe context of Multilevel Monte Carlo (MLMC). MLMC is a strategy employed to\ncompute expectations which can be biased in some sense, for instance, by using\nthe discretization of a associated probability law. The MLMC approach works\nwith a hierarchy of biased approximations which become progressively more\naccurate and more expensive. Using a telescoping representation of the most\naccurate approximation, the method is able to reduce the computational cost for\na given level of error versus i.i.d. sampling from this latter approximation.\nAll of these ideas originated for cases where exact sampling from couples in\nthe hierarchy is possible. This article considers the case where such exact\nsampling is not currently possible. We consider Markov chain Monte Carlo and\nsequential Monte Carlo methods which have been introduced in the literature and\nwe describe different strategies which facilitate the application of MLMC\nwithin these methods.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 15:08:32 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Jasra", "Ajay", ""], ["Law", "Kody", ""], ["Suciu", "Carina", ""]]}, {"id": "1704.07453", "submitter": "Daniel Lizotte", "authors": "Daniel J. Lizotte and Arezoo Tahmasebi", "title": "On Prediction and Tolerance Intervals for Dynamic Treatment Regimes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop and evaluate tolerance interval methods for dynamic treatment\nregimes (DTRs) that can provide more detailed prognostic information to\npatients who will follow an estimated optimal regime. Although the problem of\nconstructing confidence intervals for DTRs has been extensively studied,\nprediction and tolerance intervals have received little attention. We begin by\nreviewing in detail different interval estimation and prediction methods and\nthen adapting them to the DTR setting. We illustrate some of the challenges\nassociated with tolerance interval estimation stemming from the fact that we do\nnot typically have data that were generated from the estimated optimal regime.\nWe give an extensive empirical evaluation of the methods and discussed several\npractical aspects of method choice, and we present an example application using\ndata from a clinical trial. Finally, we discuss future directions within this\nimportant emerging area of DTR research.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 20:28:38 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Lizotte", "Daniel J.", ""], ["Tahmasebi", "Arezoo", ""]]}, {"id": "1704.07457", "submitter": "Thomas Nagler", "authors": "Thomas Nagler", "title": "A generic approach to nonparametric function estimation with mixed data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, data often contain discrete variables. But most of the popular\nnonparametric estimation methods have been developed in a purely continuous\nframework. A common trick among practitioners is to make discrete variables\ncontinuous by adding a small amount of noise. We show that this approach is\njustified if the noise distribution belongs to a certain class. In this case,\nany estimator developed in a purely continuous framework extends naturally to\nthe mixed data setting. Estimators defined that way will be called continuous\nconvolution estimators. They are extremely easy to implement and their\nasymptotic properties transfer directly from the continuous to the mixed data\nsetting.\n", "versions": [{"version": "v1", "created": "Mon, 24 Apr 2017 20:33:01 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 16:21:50 GMT"}, {"version": "v3", "created": "Thu, 4 Jan 2018 01:00:16 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Nagler", "Thomas", ""]]}, {"id": "1704.07531", "submitter": "Longshaokan Wang", "authors": "Longshaokan Wang, Eric B. Laber, Katie Witkiewitz", "title": "Sufficient Markov Decision Processes with Alternating Deep Neural\n  Networks", "comments": "31 pages, 3 figures, extended abstract in the proceedings of\n  RLDM2017. (v2 revisions: Fixed a minor bug in the code w.r.t. setting seed,\n  as a result numbers in the simulation experiments had some slight changes,\n  but conclusions stayed the same. Corrected typos. Improved notations.)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in mobile computing technologies have made it possible to monitor\nand apply data-driven interventions across complex systems in real time. Markov\ndecision processes (MDPs) are the primary model for sequential decision\nproblems with a large or indefinite time horizon. Choosing a representation of\nthe underlying decision process that is both Markov and low-dimensional is\nnon-trivial. We propose a method for constructing a low-dimensional\nrepresentation of the original decision process for which: 1. the MDP model\nholds; 2. a decision strategy that maximizes mean utility when applied to the\nlow-dimensional representation also maximizes mean utility when applied to the\noriginal process. We use a deep neural network to define a class of potential\nprocess representations and estimate the process of lowest dimension within\nthis class. The method is illustrated using data from a mobile study on heavy\ndrinking and smoking among college students.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 04:10:37 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2018 05:59:04 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Wang", "Longshaokan", ""], ["Laber", "Eric B.", ""], ["Witkiewitz", "Katie", ""]]}, {"id": "1704.07532", "submitter": "Joseph Antonelli", "authors": "Joseph Antonelli, Giovanni Parmigiani, Francesca Dominici", "title": "High-dimensional confounding adjustment using continuous spike and slab\n  priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In observational studies, estimation of a causal effect of a treatment on an\noutcome relies on proper adjustment for confounding. If the number of the\npotential confounders ($p$) is larger than the number of observations ($n$),\nthen direct control for all potential confounders is infeasible. Existing\napproaches for dimension reduction and penalization are generally aimed at\npredicting the outcome, and are less suited for estimation of causal effects.\nUnder standard penalization approaches (e.g. Lasso), if a variable $X_j$ is\nstrongly associated with the treatment $T$ but weakly with the outcome $Y$, the\ncoefficient $\\beta_j$ will be shrunk towards zero thus leading to confounding\nbias.\n  Under the assumption of a linear model for the outcome and sparsity, we\npropose continuous spike and slab priors on the regression coefficients\n$\\beta_j$ corresponding to the potential confounders $X_j$. Specifically, we\nintroduce a prior distribution that does not heavily shrink to zero the\ncoefficients ($\\beta_j$s) of the $X_j$s that are strongly associated with $T$\nbut weakly associated with $Y$. We compare our proposed approach to several\nstate of the art methods proposed in the literature. Our proposed approach has\nthe following features: 1) it reduces confounding bias in high dimensional\nsettings; 2) it shrinks towards zero coefficients of instrumental variables;\nand 3) it achieves good coverages even in small sample sizes. We apply our\napproach to the National Health and Nutrition Examination Survey (NHANES) data\nto estimate the causal effects of persistent pesticide exposure on triglyceride\nlevels.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 04:31:45 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 23:14:27 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 17:19:04 GMT"}, {"version": "v4", "created": "Thu, 14 Jun 2018 20:54:14 GMT"}, {"version": "v5", "created": "Tue, 16 Oct 2018 21:48:57 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Antonelli", "Joseph", ""], ["Parmigiani", "Giovanni", ""], ["Dominici", "Francesca", ""]]}, {"id": "1704.07584", "submitter": "Maksim Butsenko", "authors": "Maksim Butsenko, Johan Sw\\\"ard, Andreas Jakobsson", "title": "Estimating Sparse Signals Using Integrated Wideband Dictionaries", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2018.2835426", "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a wideband dictionary framework for estimating\nsparse signals. By formulating integrated dictionary elements spanning bands of\nthe considered parameter space, one may efficiently find and discard large\nparts of the parameter space not active in the signal. After each iteration,\nthe zero-valued parts of the dictionary may be discarded to allow a refined\ndictionary to be formed around the active elements, resulting in a zoomed\ndictionary to be used in the following iterations. Implementing this scheme\nallows for more accurate estimates, at a much lower computational cost, as\ncompared to directly forming a larger dictionary spanning the whole parameter\nspace or performing a zooming procedure using standard dictionary elements.\nDifferent from traditional dictionaries, the wideband dictionary allows for the\nuse of dictionaries with fewer elements than the number of available samples\nwithout loss of resolution. The technique may be used on both one- and\nmulti-dimensional signals, and may be exploited to refine several traditional\nsparse estimators, here illustrated with the LASSO and the SPICE estimators.\nNumerical examples illustrate the improved performance.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 08:34:44 GMT"}, {"version": "v2", "created": "Sun, 15 Oct 2017 13:41:45 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Butsenko", "Maksim", ""], ["Sw\u00e4rd", "Johan", ""], ["Jakobsson", "Andreas", ""]]}, {"id": "1704.07645", "submitter": "Fabrizio Leisen", "authors": "Alan Riva Palacio and Fabrizio Leisen", "title": "Bayesian nonparametric estimation of survival functions with\n  multiple-samples information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real problems, dependence structures more general than\nexchangeability are required. For instance, in some settings partial\nexchangeability is a more reasonable assumption. For this reason, vectors of\ndependent Bayesian nonparametric priors have recently gained popularity. They\nprovide flexible models which are tractable from a computational and\ntheoretical point of view. In this paper, we focus on their use for estimating\nsurvival functions with multiple-samples information. Our methodology allows to\nmodel the dependence among survival times of different groups of observations\nand extend previous work to an arbitrary dimension . Theoretical results about\nthe posterior behaviour of the underlying dependent vector of completely random\nmeasures are provided. The performance of the model is tested on a simulated\ndataset arising from a distributional Clayton copula.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 11:52:11 GMT"}, {"version": "v2", "created": "Sun, 18 Mar 2018 13:52:33 GMT"}], "update_date": "2018-03-20", "authors_parsed": [["Palacio", "Alan Riva", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1704.07789", "submitter": "Ziyue Chen", "authors": "Ziyue Chen and Eloise Kaizar", "title": "On variance estimation for generalizing from a trial to a target\n  population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized controlled trials (RCTs) provide strong internal validity compared\nwith observational studies. However, selection bias threatens the external\nvalidity of randomized trials. Thus, RCT results may not apply to either broad\npublic policy populations or narrow populations, such as specific insurance\npools. Some researchers use propensity scores (PSs) to generalize results from\nan RCT to a target population. In this scenario, a PS is defined as the\nprobability of participating in the trial conditioning on observed covariates.\nWe study a model-free inverse probability weighted estimator (IPWE) of the\naverage treatment effect in a target population with data from a randomized\ntrial. We present variance estimators and compare the performance of our method\nwith that of model-based approaches. We examine the robustness of the\nmodel-free estimators to heterogeneous treatment effects.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 17:10:23 GMT"}], "update_date": "2017-04-26", "authors_parsed": [["Chen", "Ziyue", ""], ["Kaizar", "Eloise", ""]]}, {"id": "1704.07848", "submitter": "Abhirup Datta", "authors": "Abhirup Datta, Sudipto Banerjee, James S. Hodges", "title": "Spatial disease mapping using Directed Acyclic Graph Auto-Regressive\n  (DAGAR) models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical models for regionally aggregated disease incidence data commonly\ninvolve region specific latent random effects that are modeled jointly as\nhaving a multivariate Gaussian distribution. The covariance or precision matrix\nincorporates the spatial dependence between the regions. Common choices for the\nprecision matrix include the widely used ICAR model, which is singular, and its\nnonsingular extension which lacks interpretability. We propose a new parametric\nmodel for the precision matrix based on a directed acyclic graph (DAG)\nrepresentation of the spatial dependence. Our model guarantees positive\ndefiniteness and, hence, in addition to being a valid prior for regional\nspatially correlated random effects, can also directly model the outcome from\ndependent data like images and networks. Theoretical results establish a link\nbetween the parameters in our model and the variance and covariances of the\nrandom effects. Substantive simulation studies demonstrate that the improved\ninterpretability of our model reaps benefits in terms of accurately recovering\nthe latent spatial random effects as well as for inference on the spatial\ncovariance parameters. Under modest spatial correlation, our model far\noutperforms the CAR models, while the performances are similar when the spatial\ncorrelation is strong. We also assess sensitivity to the choice of the ordering\nin the DAG construction using theoretical and empirical results which testify\nto the robustness of our model. We also present a large-scale public health\napplication demonstrating the competitive performance of the model.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 18:05:04 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 21:29:53 GMT"}, {"version": "v3", "created": "Sun, 28 Apr 2019 22:16:51 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Datta", "Abhirup", ""], ["Banerjee", "Sudipto", ""], ["Hodges", "James S.", ""]]}, {"id": "1704.07865", "submitter": "Nirian Mart\\'in", "authors": "N. Balakrishnan, E. Castilla, N. Martin and L.Pardo", "title": "Robust Estimators and Test-Statistics for One-Shot Device Testing Under\n  the Exponential Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a new family of estimators, the minimum density power\ndivergence estimators (MDPDEs), for the parameters of the one-shot device model\nas well as a new family of test statistics, Z-type test statistics based on\nMDPDEs, for testing the corresponding model parameters. The family of MDPDEs\ncontains as a particular case the maximum likelihood estimator (MLE) considered\nin Balakrishnan and Ling (2012). Through a simulation study, it is shown that\nsome MDPDEs have a better behavior than the MLE in relation to robustness. At\nthe same time, it can be seen that some Z-type tests based on MDPDEs have a\nbetter behavior than the classical Z-test statistic also in terms of\nrobustness.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 19:02:21 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Balakrishnan", "N.", ""], ["Castilla", "E.", ""], ["Martin", "N.", ""], ["Pardo", "L.", ""]]}, {"id": "1704.07868", "submitter": "Nirian Mart\\'in", "authors": "E. Castilla, A. Ghosh, N. Mart\\'in and L. Pardo", "title": "New robust statistical procedures for polytomous logistic regression\n  models", "comments": "Some typos were corrected from the published version", "journal-ref": "Biometrics, 2018", "doi": "10.1111/biom.12890", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper derives a new family of estimators, namely the minimum density\npower divergence estimators, as a robust generalization of the maximum\nlikelihood estimator for the polytomous logistic regression model. Based on\nthese estimators, a family of Wald-type test statistics for linear hypotheses\nis introduced. Robustness properties of both the proposed estimators and the\ntest statistics are theoretically studied through the classical influence\nfunction analysis. Appropriate real life examples are presented to justify the\nrequirement of suitable robust statistical procedures in place of the\nlikelihood based inference for the polytomous logistic regression model. The\nvalidity of the theoretical results established in the paper are further\nconfirmed empirically through suitable simulation studies. Finally, an approach\nfor the data-driven selection of the robustness tuning parameter is proposed\nwith empirical justifications.\n", "versions": [{"version": "v1", "created": "Tue, 25 Apr 2017 19:10:11 GMT"}, {"version": "v2", "created": "Wed, 14 Mar 2018 10:11:54 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 08:50:54 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Castilla", "E.", ""], ["Ghosh", "A.", ""], ["Mart\u00edn", "N.", ""], ["Pardo", "L.", ""]]}, {"id": "1704.07969", "submitter": "Tejal Bhamre", "authors": "Tejal Bhamre, Teng Zhang, Amit Singer", "title": "Anisotropic twicing for single particle reconstruction using\n  autocorrelation analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV q-bio.BM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The missing phase problem in X-ray crystallography is commonly solved using\nthe technique of molecular replacement, which borrows phases from a previously\nsolved homologous structure, and appends them to the measured Fourier\nmagnitudes of the diffraction patterns of the unknown structure. More recently,\nmolecular replacement has been proposed for solving the missing orthogonal\nmatrices problem arising in Kam's autocorrelation analysis for single particle\nreconstruction using X-ray free electron lasers and cryo-EM. In classical\nmolecular replacement, it is common to estimate the magnitudes of the unknown\nstructure as twice the measured magnitudes minus the magnitudes of the\nhomologous structure, a procedure known as `twicing'. Mathematically, this is\nequivalent to finding an unbiased estimator for a complex-valued scalar. We\ngeneralize this scheme for the case of estimating real or complex valued\nmatrices arising in single particle autocorrelation analysis. We name this\napproach \"Anisotropic Twicing\" because unlike the scalar case, the unbiased\nestimator is not obtained by a simple magnitude isotropic correction. We\ncompare the performance of the least squares, twicing and anisotropic twicing\nestimators on synthetic and experimental datasets. We demonstrate 3D homology\nmodeling in cryo-EM directly from experimental data without iterative\nrefinement or class averaging, for the first time.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 04:47:01 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Bhamre", "Tejal", ""], ["Zhang", "Teng", ""], ["Singer", "Amit", ""]]}, {"id": "1704.07971", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Jason D. Lee", "title": "A Flexible Framework for Hypothesis Testing in High-dimensions", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing in the linear regression model is a fundamental\nstatistical problem. We consider linear regression in the high-dimensional\nregime where the number of parameters exceeds the number of samples ($p> n$).\nIn order to make informative inference, we assume that the model is\napproximately sparse, that is the effect of covariates on the response can be\nwell approximated by conditioning on a relatively small number of covariates\nwhose identities are unknown. We develop a framework for testing very general\nhypotheses regarding the model parameters. Our framework encompasses testing\nwhether the parameter lies in a convex cone, testing the signal strength, and\ntesting arbitrary functionals of the parameter. We show that the proposed\nprocedure controls the type I error, and also analyze the power of the\nprocedure. Our numerical experiments confirm our theoretical findings and\ndemonstrate that we control false positive rate (type I error) near the nominal\nlevel, and have high power. By duality between hypotheses testing and\nconfidence intervals, the proposed framework can be used to obtain valid\nconfidence intervals for various functionals of the model parameters. For\nlinear functionals, the length of confidence intervals is shown to be minimax\nrate optimal.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 05:01:16 GMT"}, {"version": "v2", "created": "Tue, 19 Feb 2019 07:35:33 GMT"}, {"version": "v3", "created": "Sun, 14 Jul 2019 08:10:47 GMT"}, {"version": "v4", "created": "Sat, 21 Sep 2019 06:11:57 GMT"}], "update_date": "2019-09-24", "authors_parsed": [["Javanmard", "Adel", ""], ["Lee", "Jason D.", ""]]}, {"id": "1704.08066", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo and Michael Jansson and Kenichi Nagasawa", "title": "Bootstrap-Based Inference for Cube Root Asymptotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a valid bootstrap-based distributional approximation for\nM-estimators exhibiting a Chernoff (1964)-type limiting distribution. For\nestimators of this kind, the standard nonparametric bootstrap is inconsistent.\nThe method proposed herein is based on the nonparametric bootstrap, but\nrestores consistency by altering the shape of the criterion function defining\nthe estimator whose distribution we seek to approximate. This modification\nleads to a generic and easy-to-implement resampling method for inference that\nis conceptually distinct from other available distributional approximations. We\nillustrate the applicability of our results with four examples in econometrics\nand machine learning.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 11:41:13 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2019 14:14:59 GMT"}, {"version": "v3", "created": "Fri, 29 May 2020 13:59:24 GMT"}], "update_date": "2020-06-01", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Jansson", "Michael", ""], ["Nagasawa", "Kenichi", ""]]}, {"id": "1704.08095", "submitter": "Rafael Izbicki Rafael Izbicki", "authors": "Rafael Izbicki and Ann B. Lee", "title": "Converting High-Dimensional Regression to High-Dimensional Conditional\n  Density Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing demand for nonparametric conditional density estimators\n(CDEs) in fields such as astronomy and economics. In astronomy, for example,\none can dramatically improve estimates of the parameters that dictate the\nevolution of the Universe by working with full conditional densities instead of\nregression (i.e., conditional mean) estimates. More generally, standard\nregression falls short in any prediction problem where the distribution of the\nresponse is more complex with multi-modality, asymmetry or heteroscedastic\nnoise. Nevertheless, much of the work on high-dimensional inference concerns\nregression and classification only, whereas research on density estimation has\nlagged behind. Here we propose FlexCode, a fully nonparametric approach to\nconditional density estimation that reformulates CDE as a non-parametric\northogonal series problem where the expansion coefficients are estimated by\nregression. By taking such an approach, one can efficiently estimate\nconditional densities and not just expectations in high dimensions by drawing\nupon the success in high-dimensional regression. Depending on the choice of\nregression procedure, our method can adapt to a variety of challenging\nhigh-dimensional settings with different structures in the data (e.g., a large\nnumber of irrelevant components and nonlinear manifold structure) as well as\ndifferent data types (e.g., functional data, mixed data types and sample sets).\nWe study the theoretical and empirical performance of our proposed method, and\nwe compare our approach with traditional conditional density estimators on\nsimulated as well as real-world data, such as photometric galaxy data, Twitter\ndata, and line-of-sight velocities in a galaxy cluster.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 13:21:43 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Izbicki", "Rafael", ""], ["Lee", "Ann B.", ""]]}, {"id": "1704.08160", "submitter": "Saharon Rosset", "authors": "Saharon Rosset and Ryan J. Tibshirani", "title": "From Fixed-X to Random-X Regression: Bias-Variance Decompositions,\n  Covariance Penalties, and Prediction Error Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical prediction, classical approaches for model selection and model\nevaluation based on covariance penalties are still widely used. Most of the\nliterature on this topic is based on what we call the \"Fixed-X\" assumption,\nwhere covariate values are assumed to be nonrandom. By contrast, it is often\nmore reasonable to take a \"Random-X\" view, where the covariate values are\nindependently drawn for both training and prediction. To study the\napplicability of covariance penalties in this setting, we propose a\ndecomposition of Random-X prediction error in which the randomness in the\ncovariates contributes to both the bias and variance components. This\ndecomposition is general, but we concentrate on the fundamental case of least\nsquares regression. We prove that in this setting the move from Fixed-X to\nRandom-X prediction results in an increase in both bias and variance. When the\ncovariates are normally distributed and the linear model is unbiased, all terms\nin this decomposition are explicitly computable, which yields an extension of\nMallows' Cp that we call $RCp$. $RCp$ also holds asymptotically for certain\nclasses of nonnormal covariates. When the noise variance is unknown, plugging\nin the usual unbiased estimate leads to an approach that we call $\\hat{RCp}$,\nwhich is closely related to Sp (Tukey 1967), and GCV (Craven and Wahba 1978).\nFor excess bias, we propose an estimate based on the \"shortcut-formula\" for\nordinary cross-validation (OCV), resulting in an approach we call $RCp^+$.\nTheoretical arguments and numerical simulations suggest that $RCP^+$ is\ntypically superior to OCV, though the difference is small. We further examine\nthe Random-X error of other popular estimators. The surprising result we get\nfor ridge regression is that, in the heavily-regularized regime, Random-X\nvariance is smaller than Fixed-X variance, which can lead to smaller overall\nRandom-X error.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 15:22:54 GMT"}, {"version": "v2", "created": "Sat, 10 Jun 2017 19:16:24 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Rosset", "Saharon", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1704.08192", "submitter": "Sarah Fletcher Mercaldo", "authors": "Sarah Fletcher Mercaldo and Jeffrey D. Blume", "title": "Missing Data and Prediction", "comments": "26 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data are a common problem for both the construction and\nimplementation of a prediction algorithm. Pattern mixture kernel submodels\n(PMKS) - a series of submodels for every missing data pattern that are fit\nusing only data from that pattern - are a computationally efficient remedy for\nboth stages. Here we show that PMKS yield the most predictive algorithm among\nall standard missing data strategies. Specifically, we show that the expected\nloss of a forecasting algorithm is minimized when each pattern-specific loss is\nminimized. Simulations and a re-analysis of the SUPPORT study confirms that\nPMKS generally outperforms zero-imputation, mean-imputation, complete-case\nanalysis, complete-case submodels, and even multiple imputation (MI). The\ndegree of improvement is highly dependent on the missingness mechanism and the\neffect size of missing predictors. When the data are Missing at Random (MAR) MI\ncan yield comparable forecasting performance but generally requires a larger\ncomputational cost. We see that predictions from the PMKS are equivalent to the\nlimiting predictions for a MI procedure that uses a mean model dependent on\nmissingness indicators (the MIMI model). Consequently, the MIMI model can be\nused to assess the MAR assumption in practice. The focus of this paper is on\nout-of-sample prediction behavior, implications for model inference are only\nbriefly explored.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 16:28:31 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Mercaldo", "Sarah Fletcher", ""], ["Blume", "Jeffrey D.", ""]]}, {"id": "1704.08229", "submitter": "Michael Wallace", "authors": "M. P. Wallace, E. E. M. Moodie, and D. A. Stephens", "title": "Generalized G-estimation and Model Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic treatment regimes (DTRs) aim to formalize personalized medicine by\ntailoring treatment decisions to individual patient characteristics.\nG-estimation for DTR identification targets the parameters of a structural\nnested mean model known as the blip function from which the optimal DTR is\nderived. Despite considerable work deriving such estimation methods, there has\nbeen little focus on extending G-estimation to the case of non-additive\neffects, non-continuous outcomes or on model selection. We demonstrate how\nG-estimation can be more widely applied through the use of\niteratively-reweighted least squares procedures, and illustrate this for\nlog-linear models. We then derive a quasi-likelihood function for G-estimation\nwithin the DTR framework, and show how it can be used to form an information\ncriterion for blip model selection. These developments are demonstrated through\napplication to a variety of simulation studies as well as data from the\nSequenced Treatment Alternatives to Relieve Depression study.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 17:32:33 GMT"}], "update_date": "2017-04-27", "authors_parsed": [["Wallace", "M. P.", ""], ["Moodie", "E. E. M.", ""], ["Stephens", "D. A.", ""]]}, {"id": "1704.08248", "submitter": "Sarit Agami", "authors": "Robert J. Adler, Sarit Agami, and Pratyush Pranav", "title": "Modeling and replicating statistical topology, and evidence for CMB\n  non-homogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Under the banner of `Big Data', the detection and classification of structure\nin extremely large, high dimensional, data sets, is, one of the central\nstatistical challenges of our times. Among the most intriguing approaches to\nthis challenge is `TDA', or `Topological Data Analysis', one of the primary\naims of which is providing non-metric, but topologically informative,\npre-analyses of data sets which make later, more quantitative analyses\nfeasible. While TDA rests on strong mathematical foundations from Topology, in\napplications it has faced challenges due to an inability to handle issues of\nstatistical reliability and robustness and, most importantly, in an inability\nto make scientific claims with verifiable levels of statistical confidence. We\npropose a methodology for the parametric representation, estimation, and\nreplication of persistence diagrams, the main diagnostic tool of TDA. The power\nof the methodology lies in the fact that even if only one persistence diagram\nis available for analysis -- the typical case for big data applications --\nreplications can be generated to allow for conventional statistical hypothesis\ntesting. The methodology is conceptually simple and computationally practical,\nand provides a broadly effective statistical procedure for persistence diagram\nTDA analysis. We demonstrate the basic ideas on a toy example, and the power of\nthe approach in a novel and revealing analysis of CMB non-homogeneity.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 10:57:42 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Adler", "Robert J.", ""], ["Agami", "Sarit", ""], ["Pranav", "Pratyush", ""]]}, {"id": "1704.08349", "submitter": "Yoshimasa Uematsu", "authors": "Yoshimasa Uematsu, Yingying Fan, Kun Chen, Jinchi Lv and Wei Lin", "title": "SOFAR: large-scale association network learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern big data applications feature large scale in both numbers of\nresponses and predictors. Better statistical efficiency and scientific insights\ncan be enabled by understanding the large-scale response-predictor association\nnetwork structures via layers of sparse latent factors ranked by importance.\nYet sparsity and orthogonality have been two largely incompatible goals. To\naccommodate both features, in this paper we suggest the method of sparse\northogonal factor regression (SOFAR) via the sparse singular value\ndecomposition with orthogonality constrained optimization to learn the\nunderlying association networks, with broad applications to both unsupervised\nand supervised learning tasks such as biclustering with sparse singular value\ndecomposition, sparse principal component analysis, sparse factor analysis, and\nspare vector autoregression analysis. Exploiting the framework of\nconvexity-assisted nonconvex optimization, we derive nonasymptotic error bounds\nfor the suggested procedure characterizing the theoretical advantages. The\nstatistical guarantees are powered by an efficient SOFAR algorithm with\nconvergence property. Both computational and theoretical advantages of our\nprocedure are demonstrated with several simulation and real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 26 Apr 2017 21:05:18 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Uematsu", "Yoshimasa", ""], ["Fan", "Yingying", ""], ["Chen", "Kun", ""], ["Lv", "Jinchi", ""], ["Lin", "Wei", ""]]}, {"id": "1704.08440", "submitter": "Shonosuke Sugasawa", "authors": "Shonosuke Sugasawa", "title": "On Bootstrap Averaging Empirical Bayes Estimators", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric empirical Bayes (EB) estimators have been widely used in variety\nof fields including small area estimation, disease mapping. Since EB estimator\nis constructed by plugging in the estimator of parameters in prior\ndistributions, it might perform poorly if the estimator of parameters is\nunstable. This can happen when the number of samples are small or moderate.\nThis paper suggests bootstrapping averaging approach, known as \"bagging\" in\nmachine learning literatures, to improve the performances of EB estimators. We\nconsider two typical hierarchical models, two-stage normal hierarchical model\nand Poisson-gamma model, and compare the proposed method with the classical\nparametric EB method through simulation and empirical studies.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 05:49:48 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Sugasawa", "Shonosuke", ""]]}, {"id": "1704.08444", "submitter": "Kattumannil Sudheesh Dr", "authors": "Sreelakshmi N", "title": "A note on Quantile curves based bivariate reliability concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the univariate quantile based reliability concepts to the bivariate\ncase using quantile curves. We propose quantile curves based bivariate hazard\nrate and bivariate mean residual life function and establish a relationship\nbetween them. We study the uniqueness properties of these concepts to determine\nthe underlying quantile curve. We also study the quantile curves based\nreliability concepts in reverse time.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 06:16:46 GMT"}, {"version": "v2", "created": "Tue, 13 Mar 2018 11:25:52 GMT"}], "update_date": "2018-03-14", "authors_parsed": [["N", "Sreelakshmi", ""]]}, {"id": "1704.08447", "submitter": "Linda Mhalla", "authors": "Linda Mhalla, Miguel de Carvalho, Val\\'erie Chavez-Demoulin", "title": "Regression Type Models for Extremal Dependence", "comments": "29 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a vector generalized additive modeling framework for taking into\naccount the effect of covariates on angular density functions in a multivariate\nextreme value context. The proposed methods are tailored for settings where the\ndependence between extreme values may change according to covariates. We devise\na maximum penalized log-likelihood estimator, discuss details of the estimation\nprocedure, and derive its consistency and asymptotic normality. The simulation\nstudy suggests that the proposed methods perform well in a wealth of simulation\nscenarios by accurately recovering the true covariate-adjusted angular density.\nOur empirical analysis reveals relevant dynamics of the dependence between\nextreme air temperatures in two alpine resorts during the winter season.\nSupplementary materials for this article are available online.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 06:35:17 GMT"}, {"version": "v2", "created": "Mon, 27 Nov 2017 08:20:55 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Mhalla", "Linda", ""], ["de Carvalho", "Miguel", ""], ["Chavez-Demoulin", "Val\u00e9rie", ""]]}, {"id": "1704.08729", "submitter": "Dorian Cazau", "authors": "Dorian Cazau and Yuancheng Wang and Olivier Adam and Qiao Wang and\n  Gr\\'egory Nuel", "title": "Calibration of a two-state pitch-wise HMM method for note segmentation\n  in Automatic Music Transcription systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many methods for automatic music transcription involves a multi-pitch\nestimation method that estimates an activity score for each pitch. A second\nprocessing step, called note segmentation, has to be performed for each pitch\nin order to identify the time intervals when the notes are played. In this\nstudy, a pitch-wise two-state on/off firstorder Hidden Markov Model (HMM) is\ndeveloped for note segmentation. A complete parametrization of the HMM sigmoid\nfunction is proposed, based on its original regression formulation, including a\nparameter alpha of slope smoothing and beta? of thresholding contrast. A\ncomparative evaluation of different note segmentation strategies was performed,\ndifferentiated according to whether they use a fixed threshold, called \"Hard\nThresholding\" (HT), or a HMM-based thresholding method, called \"Soft\nThresholding\" (ST). This evaluation was done following MIREX standards and\nusing the MAPS dataset. Also, different transcription scenarios and recording\nnatures were tested using three units of the Degradation toolbox. Results show\nthat note segmentation through a HMM soft thresholding with a data-based\noptimization of the {alpha,beta} parameter couple significantly enhances\ntranscription performance.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 19:48:09 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Cazau", "Dorian", ""], ["Wang", "Yuancheng", ""], ["Adam", "Olivier", ""], ["Wang", "Qiao", ""], ["Nuel", "Gr\u00e9gory", ""]]}, {"id": "1704.08756", "submitter": "Piotr Szyma\\'nski", "authors": "Piotr Szyma\\'nski, Tomasz Kajdanowicz", "title": "A Network Perspective on Stratification of Multi-Label Data", "comments": "submitted for ECML2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, we have witnessed the development of multi-label\nclassification methods which utilize the structure of the label space in a\ndivide and conquer approach to improve classification performance and allow\nlarge data sets to be classified efficiently. Yet most of the available data\nsets have been provided in train/test splits that did not account for\nmaintaining a distribution of higher-order relationships between labels among\nsplits or folds. We present a new approach to stratifying multi-label data for\nclassification purposes based on the iterative stratification approach proposed\nby Sechidis et. al. in an ECML PKDD 2011 paper. Our method extends the\niterative approach to take into account second-order relationships between\nlabels. Obtained results are evaluated using statistical properties of obtained\nstrata as presented by Sechidis. We also propose new statistical measures\nrelevant to second-order quality: label pairs distribution, the percentage of\nlabel pairs without positive evidence in folds and label pair - fold pairs that\nhave no positive evidence for the label pair. We verify the impact of new\nmethods on classification performance of Binary Relevance, Label Powerset and a\nfast greedy community detection based label space partitioning classifier.\nRandom Forests serve as base classifiers. We check the variation of the number\nof communities obtained per fold, and the stability of their modularity score.\nSecond-Order Iterative Stratification is compared to standard k-fold, label\nset, and iterative stratification. The proposed approach lowers the variance of\nclassification quality, improves label pair oriented measures and example\ndistribution while maintaining a competitive quality in label-oriented\nmeasures. We also witness an increase in stability of network characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 27 Apr 2017 21:43:53 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Szyma\u0144ski", "Piotr", ""], ["Kajdanowicz", "Tomasz", ""]]}, {"id": "1704.08810", "submitter": "Yi Yang", "authors": "Yanjia Yu, Yi Yang and Yuhong Yang", "title": "Performance Assessment of High-dimensional Variable Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since model selection is ubiquitous in data analysis, reproducibility of\nstatistical results demands a serious evaluation of reliability of the employed\nmodel selection method, no matter what label it may have in terms of good\nproperties. Instability measures have been proposed for evaluating model\nselection uncertainty. However, low instability does not necessarily indicate\nthat the selected model is trustworthy, since low instability can also arise\nwhen a certain method tends to select an overly parsimonious model. F- and\nG-measures have become increasingly popular for assessing variable selection\nperformance in theoretical studies and simulation results. However, they are\nnot computable in practice. In this work, we propose an estimation method for\nF- and G-measures and prove their desirable properties of uniform consistency.\nThis gives the data analyst a valuable tool to compare different variable\nselection methods based on the data at hand. Extensive simulations are\nconducted to show the very good finite sample performance of our approach. We\nfurther demonstrate the application of our methods using several micro-array\ngene expression data sets, with intriguing findings.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 05:24:27 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Yu", "Yanjia", ""], ["Yang", "Yi", ""], ["Yang", "Yuhong", ""]]}, {"id": "1704.08851", "submitter": "Sebastian Weichwald", "authors": "Sebastian Weichwald and Moritz Grosse-Wentrup", "title": "The right tool for the right question --- beyond the encoding versus\n  decoding dichotomy", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two major questions that neuroimaging studies attempt to answer:\nFirst, how are sensory stimuli represented in the brain (which we term the\nstimulus-based setting)? And, second, how does the brain generate cognition\n(termed the response-based setting)? There has been a lively debate in the\nneuroimaging community whether encoding and decoding models can provide\ninsights into these questions. In this commentary, we construct two simple and\nanalytically tractable examples to demonstrate that while an encoding model\nanalysis helps with the former, neither model is appropriate to satisfactorily\nanswer the latter question. Consequently, we argue that if we want to\nunderstand how the brain generates cognition, we need to move beyond the\nencoding versus decoding dichotomy and instead discuss and develop tools that\nare specifically tailored to our endeavour.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 08:56:47 GMT"}], "update_date": "2017-05-01", "authors_parsed": [["Weichwald", "Sebastian", ""], ["Grosse-Wentrup", "Moritz", ""]]}, {"id": "1704.08891", "submitter": "Edouard Ollier", "authors": "Gersende Fort, Edouard Ollier, Adeline Samson", "title": "Stochastic Proximal Gradient Algorithms for Penalized Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by penalized likelihood maximization in complex models, we study\noptimization problems where neither the function to optimize nor its gradient\nhave an explicit expression, but its gradient can be approximated by a Monte\nCarlo technique. We propose a new algorithm based on a stochastic approximation\nof the Proximal-Gradient (PG) algorithm. This new algorithm, named Stochastic\nApproximation PG (SAPG) is the combination of a stochastic gradient descent\nstep which - roughly speaking - computes a smoothed approximation of the past\ngradient along the iterations, and a proximal step. The choice of the step size\nand the Monte Carlo batch size for the stochastic gradient descent step in SAPG\nare discussed. Our convergence results cover the cases of biased and unbiased\nMonte Carlo approximations. While the convergence analysis of the Monte\nCarlo-PG is already addressed in the literature (see Atchad\\'e et al. [2016]),\nthe convergence analysis of SAPG is new. The two algorithms are compared on a\nlinear mixed effect model as a toy example. A more challenging application is\nproposed on non-linear mixed effect models in high dimension with a\npharmacokinetic data set including genomic covariates. To our best knowledge,\nour work provides the first convergence result of a numerical method designed\nto solve penalized Maximum Likelihood in a non-linear mixed effect model.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 12:08:39 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 08:55:47 GMT"}], "update_date": "2017-09-28", "authors_parsed": [["Fort", "Gersende", ""], ["Ollier", "Edouard", ""], ["Samson", "Adeline", ""]]}]