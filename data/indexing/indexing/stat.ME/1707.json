[{"id": "1707.00013", "submitter": "Malayaja Chutani", "authors": "Neelima Gupte, N. Nirmal Thyagu, and Malayaja Chutani", "title": "The Simplicial Characterisation of TS networks: Theory and applications", "comments": "11 pages, 2 figures, 4 tables. Accepted for publication in\n  Proceedings of the 4th International Conference on Applications in Nonlinear\n  Dynamics (ICAND 2016)", "journal-ref": null, "doi": "10.1007/978-3-319-52621-8_25", "report-no": null, "categories": "stat.ME physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the visibility algorithm to construct the time series networks\nobtained from the time series of different dynamical regimes of the logistic\nmap. We define the simplicial characterisers of networks which can analyse the\nsimplicial structure at both the global and local levels. These characterisers\nare used to analyse the TS networks obtained in different dynamical regimes of\nthe logisitic map. It is seen that the simplicial characterisers are able to\ndistinguish between distinct dynamical regimes. We also apply the simplicial\ncharacterisers to time series networks constructed from fMRI data, where the\npreliminary results indicate that the characterisers are able to differentiate\nbetween distinct TS networks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 08:57:49 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Gupte", "Neelima", ""], ["Thyagu", "N. Nirmal", ""], ["Chutani", "Malayaja", ""]]}, {"id": "1707.00114", "submitter": "Guy Katriel", "authors": "Tamar Gadrich, Guy Katriel", "title": "Estimating the rate of defects under imperfect sampling inspection - a\n  new approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the rate of defects (mean number of\ndefects per item), given the counts of defects detected by two independent\nimperfect inspectors on one sample of items. In contrast with the setting for\nthe well-known method of Capture-Recapture, we {\\it{do not}} have information\nregarding the number of defects jointly detected by {\\it{both}} inspectors. We\nsolve this problem by constructing two types of estimators - a simple\nmoment-type estimator, and a complicated maximum-likelihood estimator. The\nperformance of these estimators is studied analytically and by means of\nsimulations. It is shown that the maximum-likelihood estimator is superior to\nthe moment-type estimator. A systematic comparison with the Capture-Recapture\nmethod is also made.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 08:39:19 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 11:50:17 GMT"}, {"version": "v3", "created": "Mon, 22 Apr 2019 11:10:02 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Gadrich", "Tamar", ""], ["Katriel", "Guy", ""]]}, {"id": "1707.00167", "submitter": "Hao Chen", "authors": "Lynna Chu and Hao Chen", "title": "Asymptotic Distribution-Free Change-Point Detection for Multivariate and\n  non-Euclidean Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the testing and estimation of change-points, locations where the\ndistribution abruptly changes, in a sequence of multivariate or non-Euclidean\nobservations. We study a nonparametric framework that utilizes similarity\ninformation among observations, which can be applied to various data types as\nlong as an informative similarity measure on the sample space can be defined.\nThe existing approach along this line has low power and/or biased estimates for\nchange-points under some common scenarios. We address these problems by\nconsidering new tests based on similarity information. Simulation studies show\nthat the new approaches exhibit substantial improvements in detecting and\nestimating change-points. In addition, under some mild conditions, the new test\nstatistics are asymptotically distribution free under the null hypothesis of no\nchange. Analytic p-value approximations to the significance of the new test\nstatistics for the single change-point alternative and changed interval\nalternative are derived, making the new approaches easy off-the-shelf tools for\nlarge datasets. The new approaches are illustrated in an analysis of New York\ntaxi data.\n", "versions": [{"version": "v1", "created": "Sat, 1 Jul 2017 15:26:23 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 18:10:36 GMT"}], "update_date": "2018-02-23", "authors_parsed": [["Chu", "Lynna", ""], ["Chen", "Hao", ""]]}, {"id": "1707.00306", "submitter": "Michael Fop", "authors": "Michael Fop and Thomas Brendan Murphy", "title": "Variable Selection Methods for Model-based Clustering", "comments": null, "journal-ref": "Statistics Surveys, 12 (2018) 1-48", "doi": "10.1214/18-SS119", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering is a popular approach for clustering multivariate data\nwhich has seen applications in numerous fields. Nowadays, high-dimensional data\nare more and more common and the model-based clustering approach has adapted to\ndeal with the increasing dimensionality. In particular, the development of\nvariable selection techniques has received a lot of attention and research\neffort in recent years. Even for small size problems, variable selection has\nbeen advocated to facilitate the interpretation of the clustering results. This\nreview provides a summary of the methods developed for variable selection in\nmodel-based clustering. Existing R packages implementing the different methods\nare indicated and illustrated in application to two data analysis examples.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 15:29:13 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 07:52:56 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Fop", "Michael", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1707.00332", "submitter": "Flavio Goncalves", "authors": "Fl\\'avio B. Gon\\c{c}alves, Krzysztof G. {\\L}atuszy\\'nski, Gareth O.\n  Roberts", "title": "Exact Monte Carlo likelihood-based inference for jump-diffusion\n  processes", "comments": "40 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference for discretely observed jump-diffusion processes is a\ncomplex problem which motivates new methodological challenges. Thus existing\napproaches invariably resort to time-discretisations which inevitably lead to\napproximations in inference. In this paper, we give the first general\ncollection of methodologies for exact (in this context meaning\ndiscretisation-free) likelihood-based inference for discretely observed finite\nactivity jump-diffusions. The only sources of error involved are Monte Carlo\nerror and convergence of EM or MCMC algorithms. We shall introduce both\nfrequentist and Bayesian approaches, illustrating the methodology through\nsimulated and real examples.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jul 2017 18:34:03 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 23:51:52 GMT"}, {"version": "v3", "created": "Sun, 5 Apr 2020 22:07:59 GMT"}, {"version": "v4", "created": "Wed, 26 May 2021 13:02:25 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Gon\u00e7alves", "Fl\u00e1vio B.", ""], ["\u0141atuszy\u0144ski", "Krzysztof G.", ""], ["Roberts", "Gareth O.", ""]]}, {"id": "1707.00453", "submitter": "Eardi Lila", "authors": "Eardi Lila, John A. D. Aston", "title": "Statistical Analysis of Functions on Surfaces, with an application to\n  Medical Imaging", "comments": "42 pages", "journal-ref": null, "doi": "10.1080/01621459.2019.1635479", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Functional Data Analysis, data are commonly assumed to be smooth functions\non a fixed interval of the real line. In this work, we introduce a\ncomprehensive framework for the analysis of functional data, whose domain is a\ntwo-dimensional manifold and the domain itself is subject to variability from\nsample to sample. We formulate a statistical model for such data, here called\nFunctions on Surfaces, which enables a joint representation of the geometric\nand functional aspects, and propose an associated estimation framework. We\nassess the validity of the framework by performing a simulation study and we\nfinally apply it to the analysis of neuroimaging data of cortical thickness,\nacquired from the brains of different subjects, and thus lying on domains with\ndifferent geometries.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 09:15:47 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 10:34:39 GMT"}, {"version": "v3", "created": "Thu, 1 Aug 2019 10:31:02 GMT"}], "update_date": "2019-08-02", "authors_parsed": [["Lila", "Eardi", ""], ["Aston", "John A. D.", ""]]}, {"id": "1707.00460", "submitter": "Nicolas Brosse", "authors": "Nicolas Brosse, Alain Durmus and \\'Eric Moulines", "title": "Normalizing constants of log-concave densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive explicit bounds for the computation of normalizing constants $Z$\nfor log-concave densities $\\pi = \\exp(-U)/Z$ with respect to the Lebesgue\nmeasure on $\\mathbb{R}^d$. Our approach relies on a Gaussian annealing combined\nwith recent and precise bounds on the Unadjusted Langevin Algorithm\n(High-dimensional Bayesian inference via the Unadjusted Langevin Algorithm, A.\nDurmus and E. Moulines). Polynomial bounds in the dimension $d$ are obtained\nwith an exponent that depends on the assumptions made on $U$. The algorithm\nalso provides a theoretically grounded choice of the annealing sequence of\nvariances. A numerical experiment supports our findings. Results of independent\ninterest on the mean squared error of the empirical average of locally\nLipschitz functions are established.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 09:26:42 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 08:01:06 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Brosse", "Nicolas", ""], ["Durmus", "Alain", ""], ["Moulines", "\u00c9ric", ""]]}, {"id": "1707.00705", "submitter": "Selden Crary", "authors": "Selden Crary, Richard Diehl Martinez, and Michael Saunders", "title": "The Nu Class of Low-Degree-Truncated, Rational, Generalized Functions.\n  Ib. Integrals of Matern-correlation functions for all odd-half-integer class\n  parameters", "comments": "30 pages, 3 tables, 1 appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is an extension of Parts I and Ia of a series about Nu-class\ngeneralized functions. We provide hand-generated algebraic expressions for\nintegrals of single Matern-covariance functions, as well as for products of two\nMatern-covariance functions, for all odd-half-integer class parameters. These\nare useful both for IMSPE-optimal design software and for testing universality\nof Nu-class generalized-function properties, across covariance classes.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 18:00:18 GMT"}, {"version": "v2", "created": "Wed, 22 May 2019 05:59:07 GMT"}], "update_date": "2019-05-23", "authors_parsed": [["Crary", "Selden", ""], ["Martinez", "Richard Diehl", ""], ["Saunders", "Michael", ""]]}, {"id": "1707.00731", "submitter": "Jie Sun", "authors": "Jie Sun, Fernando J. Quevedo, Erik Bollt", "title": "Data Fusion Reconstruction of Spatially Embedded Complex Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an nlin.CG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a kernel Lasso (kLasso) optimization that simultaneously\naccounts for spatial regularity and network sparsity to reconstruct spatial\ncomplex networks from data. Through a kernel function, the proposed approach\nexploits spatial embedding distances to penalize overabundance of spatially\nlong-distance connections. Examples of both synthetic and real-world spatial\nnetworks show that the proposed method improves significantly upon existing\nnetwork reconstruction techniques that mainly concerns sparsity but not spatial\nregularity. Our results highlight the promise of data fusion in the\nreconstruction of complex networks, by utilizing both microscopic node-level\ndynamics (e.g., time series data) and macroscopic network-level information\n(metadata).\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 19:00:48 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Sun", "Jie", ""], ["Quevedo", "Fernando J.", ""], ["Bollt", "Erik", ""]]}, {"id": "1707.00763", "submitter": "Daniel Kowal", "authors": "Daniel R. Kowal, David S. Matteson, and David Ruppert", "title": "Dynamic Shrinkage Processes", "comments": null, "journal-ref": null, "doi": "10.1111/rssb.12325", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel class of dynamic shrinkage processes for Bayesian time\nseries and regression analysis. Building upon a global-local framework of prior\nconstruction, in which continuous scale mixtures of Gaussian distributions are\nemployed for both desirable shrinkage properties and computational\ntractability, we model dependence among the local scale parameters. The\nresulting processes inherit the desirable shrinkage behavior of popular\nglobal-local priors, such as the horseshoe prior, but provide additional\nlocalized adaptivity, which is important for modeling time series data or\nregression functions with local features. We construct a computationally\nefficient Gibbs sampling algorithm based on a P\\'olya-Gamma scale mixture\nrepresentation of the proposed process. Using dynamic shrinkage processes, we\ndevelop a Bayesian trend filtering model that produces more accurate estimates\nand tighter posterior credible intervals than competing methods, and apply the\nmodel for irregular curve-fitting of minute-by-minute Twitter CPU usage data.\nIn addition, we develop an adaptive time-varying parameter regression model to\nassess the efficacy of the Fama-French five-factor asset pricing model with\nmomentum added as a sixth factor. Our dynamic analysis of manufacturing and\nhealthcare industry data shows that with the exception of the market risk, no\nother risk factors are significant except for brief periods.\n", "versions": [{"version": "v1", "created": "Mon, 3 Jul 2017 21:20:08 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 01:13:29 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Kowal", "Daniel R.", ""], ["Matteson", "David S.", ""], ["Ruppert", "David", ""]]}, {"id": "1707.00819", "submitter": "Sebastian Weichwald", "authors": "Paul K. Rubenstein, Sebastian Weichwald, Stephan Bongers, Joris M.\n  Mooij, Dominik Janzing, Moritz Grosse-Wentrup, Bernhard Sch\\\"olkopf", "title": "Causal Consistency of Structural Equation Models", "comments": "equal contribution between Rubenstein and Weichwald; accepted\n  manuscript", "journal-ref": "Proceedings of the Annual Conference on Uncertainty in Artificial\n  Intelligence, UAI 2017 ( http://auai.org/uai2017/proceedings/papers/11.pdf )", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex systems can be modelled at various levels of detail. Ideally, causal\nmodels of the same system should be consistent with one another in the sense\nthat they agree in their predictions of the effects of interventions. We\nformalise this notion of consistency in the case of Structural Equation Models\n(SEMs) by introducing exact transformations between SEMs. This provides a\ngeneral language to consider, for instance, the different levels of description\nin the following three scenarios: (a) models with large numbers of variables\nversus models in which the `irrelevant' or unobservable variables have been\nmarginalised out; (b) micro-level models versus macro-level models in which the\nmacro-variables are aggregate features of the micro-variables; (c) dynamical\ntime series models versus models of their stationary behaviour. Our analysis\nstresses the importance of well specified interventions in the causal modelling\nprocess and sheds light on the interpretation of cyclic SEMs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 06:05:31 GMT"}], "update_date": "2020-09-03", "authors_parsed": [["Rubenstein", "Paul K.", ""], ["Weichwald", "Sebastian", ""], ["Bongers", "Stephan", ""], ["Mooij", "Joris M.", ""], ["Janzing", "Dominik", ""], ["Grosse-Wentrup", "Moritz", ""], ["Sch\u00f6lkopf", "Bernhard", ""]]}, {"id": "1707.00833", "submitter": "Debarghya Ghoshdastidar", "authors": "Debarghya Ghoshdastidar, Maurilio Gutzeit, Alexandra Carpentier,\n  Ulrike von Luxburg", "title": "Two-sample Hypothesis Testing for Inhomogeneous Random Graphs", "comments": "To appear in the Annals of Statistics. This 54-page version includes\n  the supplementary material (appendix to the main paper)", "journal-ref": "Ann. Statist. Volume 48, Number 4 (2020), 2208-2229", "doi": "10.1214/19-AOS1884", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of networks leads to a wide range of high dimensional inference\nproblems. In many practical applications, one needs to draw inference from one\nor few large sparse networks. The present paper studies hypothesis testing of\ngraphs in this high-dimensional regime, where the goal is to test between two\npopulations of inhomogeneous random graphs defined on the same set of $n$\nvertices. The size of each population $m$ is much smaller than $n$, and can\neven be a constant as small as 1. The critical question in this context is\nwhether the problem is solvable for small $m$.\n  We answer this question from a minimax testing perspective. Let $P,Q$ be the\npopulation adjacencies of two sparse inhomogeneous random graph models, and $d$\nbe a suitably defined distance function. Given a population of $m$ graphs from\neach model, we derive minimax separation rates for the problem of testing $P=Q$\nagainst $d(P,Q)>\\rho$. We observe that if $m$ is small, then the minimax\nseparation is too large for some popular choices of $d$, including total\nvariation distance between corresponding distributions. This implies that some\nmodels that are widely separated in $d$ cannot be distinguished for small $m$,\nand hence, the testing problem is generally not solvable in these cases.\n  We also show that if $m>1$, then the minimax separation is relatively small\nif $d$ is the Frobenius norm or operator norm distance between $P$ and $Q$. For\n$m=1$, only the latter distance provides small minimax separation. Thus, for\nthese distances, the problem is solvable for small $m$. We also present\nnear-optimal two-sample tests in both cases, where tests are adaptive with\nrespect to sparsity level of the graphs.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 07:25:45 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 13:05:26 GMT"}, {"version": "v3", "created": "Tue, 11 Dec 2018 12:56:03 GMT"}, {"version": "v4", "created": "Wed, 17 Jul 2019 13:32:07 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Ghoshdastidar", "Debarghya", ""], ["Gutzeit", "Maurilio", ""], ["Carpentier", "Alexandra", ""], ["von Luxburg", "Ulrike", ""]]}, {"id": "1707.00842", "submitter": "Julyan Arbel", "authors": "Julyan Arbel (1), Marco Battiston (2), Stefano Favaro (3), Antonio\n  Lijoi (4), Igor Pr\\\"unster (4), Rams\\'es H. Mena (5), Yang Ni (6), Peter\n  M\\\"uller (6) ((1) Inria Grenoble (2) Oxford University (3) University of\n  Turin (4) Bocconi University (5) IIMAS, UNAM (6) University of Texas at\n  Austin)", "title": "Discussions of the paper \"Sparse graphs using exchangeable random\n  measures\" by F. Caron and E. B. Fox", "comments": "To be published in the Journal of the Royal Statistical Society,\n  Series B, volume 79. 4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These are written discussions of the paper \"Sparse graphs using exchangeable\nrandom measures\" by Fran\\c{c}ois Caron and Emily B. Fox, contributed to the\nJournal of the Royal Statistical Society Series B.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 08:09:14 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Arbel", "Julyan", ""], ["Battiston", "Marco", ""], ["Favaro", "Stefano", ""], ["Lijoi", "Antonio", ""], ["Pr\u00fcnster", "Igor", ""], ["Mena", "Rams\u00e9s H.", ""], ["Ni", "Yang", ""], ["M\u00fcller", "Peter", ""]]}, {"id": "1707.00877", "submitter": "Manuele Leonelli", "authors": "Manuele Leonelli and Dani Gamerman", "title": "A semiparametric approach for bivariate extreme exceedances", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference over tails is performed by applying only the results of extreme\nvalue theory. Whilst such theory is well defined and flexible enough in the\nunivariate case, multivariate inferential methods often require the imposition\nof arbitrary constraints not fully justifed by the underlying theory. In\ncontrast, our approach uses only the constraints imposed by theory. We build on\nprevious, theoretically justified work for marginal exceedances over a high,\nunknown threshold, by combining it with flexible, semiparametric copulae\nspecifications to investigate extreme dependence. Whilst giving probabilistic\njudgements about the extreme regime of all marginal variables, our approach\nformally uses the full dataset and allows for a variety of patterns of\ndependence, be them extremal or not. A new probabilistic criterion quantifying\nthe possibility that the data exhibits asymptotic independence is introduced\nand its robustness empirically studied. Estimation of functions of interest in\nextreme value analyses is performed via MCMC algorithms. Attention is also\ndevoted to the prediction of new extreme observations. Our approach is\nevaluated through a series of simulations, applied to real data sets and\nassessed against competing approaches. Evidence demonstrates that the bulk of\nthe data does not bias and improves the inferential process for the extremal\ndependence.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 09:35:54 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 09:49:40 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Leonelli", "Manuele", ""], ["Gamerman", "Dani", ""]]}, {"id": "1707.00911", "submitter": "Ola H\\\"{o}ssjer", "authors": "Ola H\\\"ossjer, Lars Alfredsson, Anna Karin Hedstr\\\"om, Magnus Lekman,\n  Ingrid Kockum, Tomas Olsson", "title": "Quantifying and estimating additive measures of interaction from\n  case-control data", "comments": "Published at http://dx.doi.org/10.15559/17-VMSTA77 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)", "journal-ref": "Modern Stochastics: Theory and Applications 2017, Vol. 4, No. 2,\n  109-125", "doi": "10.15559/17-VMSTA77", "report-no": "VTeX-VMSTA-VMSTA77", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a general framework for quantifying how binary risk\nfactors jointly influence a binary outcome. Our key result is an additive\nexpansion of odds ratios as a sum of marginal effects and interaction terms of\nvarying order. These odds ratio expansions are used for estimating the excess\nodds ratio, attributable proportion and synergy index for a case-control\ndataset by means of maximum likelihood from a logistic regression model. The\nconfidence intervals associated with these estimates of joint effects and\ninteraction of risk factors rely on the delta method. Our methodology is\nillustrated with a large Nordic meta dataset for multiple sclerosis. It\ncombines four studies, with a total of 6265 cases and 8401 controls. It has\nthree risk factors (smoking and two genetic factors) and a number of other\nconfounding variables.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 11:12:37 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["H\u00f6ssjer", "Ola", ""], ["Alfredsson", "Lars", ""], ["Hedstr\u00f6m", "Anna Karin", ""], ["Lekman", "Magnus", ""], ["Kockum", "Ingrid", ""], ["Olsson", "Tomas", ""]]}, {"id": "1707.00974", "submitter": "Shu Yang", "authors": "Shu Yang and Jae Kwang Kim", "title": "Nearest neighbor imputation for general parameter estimation in survey\n  sampling", "comments": "25 pages. arXiv admin note: substantial text overlap with\n  arXiv:1703.10256", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nearest neighbor imputation is popular for handling item nonresponse in\nsurvey sampling. In this article, we study the asymptotic properties of the\nnearest neighbor imputation estimator for general population parameters,\nincluding population means, proportions and quantiles. For variance estimation,\nthe conventional bootstrap inference for matching estimators with fixed number\nof matches has been shown to be invalid due to the nonsmoothness nature of the\nmatching estimator. We propose asymptotically valid replication variance\nestimation. The key strategy is to construct replicates of the estimator\ndirectly based on linear terms, instead of individual records of variables. A\nsimulation study confirms that the new procedure provides valid variance\nestimation.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 23:33:05 GMT"}], "update_date": "2017-07-05", "authors_parsed": [["Yang", "Shu", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1707.01164", "submitter": "Jianbo Chen", "authors": "Jianbo Chen, Mitchell Stern, Martin J. Wainwright, Michael I. Jordan", "title": "Kernel Feature Selection via Conditional Covariance Minimization", "comments": "The first two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for feature selection that employs kernel-based measures\nof independence to find a subset of covariates that is maximally predictive of\nthe response. Building on past work in kernel dimension reduction, we show how\nto perform feature selection via a constrained optimization problem involving\nthe trace of the conditional covariance operator. We prove various consistency\nresults for this procedure, and also demonstrate that our method compares\nfavorably with other state-of-the-art algorithms on a variety of synthetic and\nreal data sets.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 22:00:58 GMT"}, {"version": "v2", "created": "Sat, 20 Oct 2018 19:34:49 GMT"}], "update_date": "2018-10-23", "authors_parsed": [["Chen", "Jianbo", ""], ["Stern", "Mitchell", ""], ["Wainwright", "Martin J.", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1707.01225", "submitter": "Zhigang Yao", "authors": "Zhigang Yao, Ye Zhang, Zhidong Bai and William F. Eddy", "title": "Estimating the Number of Sources in Magnetoencephalography Using Spiked\n  Population Eigenvalues", "comments": "38 pages, 8 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Magnetoencephalography (MEG) is an advanced imaging technique used to measure\nthe magnetic fields outside the human head produced by the electrical activity\ninside the brain. Various source localization methods in MEG require the\nknowledge of the underlying active sources, which are identified by a priori.\nCommon methods used to estimate the number of sources include principal\ncomponent analysis or information criterion methods, both of which make use of\nthe eigenvalue distribution of the data, thus avoiding solving the\ntime-consuming inverse problem. Unfortunately, all these methods are very\nsensitive to the signal-to-noise ratio (SNR), as examining the sample extreme\neigenvalues does not necessarily reflect the perturbation of the population\nones. To uncover the unknown sources from the very noisy MEG data, we introduce\na framework, referred to as the intrinsic dimensionality (ID) of the optimal\ntransformation for the SNR rescaling functional. It is defined as the number of\nthe spiked population eigenvalues of the associated transformed data matrix. It\nis shown that the ID yields a more reasonable estimate for the number of\nsources than its sample counterparts, especially when the SNR is small. By\nmeans of examples, we illustrate that the new method is able to capture the\nnumber of signal sources in MEG that can escape PCA or other information\ncriterion based methods.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 06:39:39 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Yao", "Zhigang", ""], ["Zhang", "Ye", ""], ["Bai", "Zhidong", ""], ["Eddy", "William F.", ""]]}, {"id": "1707.01254", "submitter": "Michael GB Blum", "authors": "Michael GB Blum", "title": "Regression approaches for Approximate Bayesian Computation", "comments": "Book chapter, published in Handbook of Approximate Bayesian\n  Computation 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This book chapter introduces regression approaches and regression adjustment\nfor Approximate Bayesian Computation (ABC). Regression adjustment adjusts\nparameter values after rejection sampling in order to account for the imperfect\nmatch between simulations and observations. Imperfect match between simulations\nand observations can be more pronounced when there are many summary statistics,\na phenomenon coined as the curse of dimensionality. Because of this imperfect\nmatch, credibility intervals obtained with regression approaches can be\ninflated compared to true credibility intervals. The chapter presents the main\nconcepts underlying regression adjustment. A theorem that compares theoretical\nproperties of posterior distributions obtained with and without regression\nadjustment is presented. Last, a practical application of regression adjustment\nin population genetics shows that regression adjustment shrinks posterior\ndistributions compared to rejection approaches, which is a solution to avoid\ninflated credibility intervals.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 08:17:58 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Blum", "Michael GB", ""]]}, {"id": "1707.01308", "submitter": "Pavlina Jordanova", "authors": "Pavlina K. Jordanova, Monika P. Petkova", "title": "Measuring heavy-tailedness of distributions", "comments": "Submitted in AIP Conference porceedings", "journal-ref": null, "doi": "10.1063/1.5013996", "report-no": null, "categories": "stat.ME math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Different questions related with analysis of extreme values and outliers\narise frequently in practice. To exclude extremal observations and outliers is\nnot a good decision because they contain important information about the\nobserved distribution. The difficulties with their usage are usually related to\nthe estimation of the tail index in case it exists. There are many measures for\nthe center of the distribution, e.g. mean, mode, median. There are many\nmeasures of the variance, asymmetry, and kurtosis, but there is no easy\ncharacteristic for heavy-tailedness of the observed distribution. Here we\npropose such a measure, give some examples and explore some of its properties.\nThis allows us to introduce a classification of the distributions, with respect\nto their heavy-tailedness. The idea is to help and navigate practitioners for\naccurate and easier work in the field of probability distributions.\n  Using the properties of the defined characteristics some distribution\nsensitive extremal index estimators are proposed and their properties are\npartially investigated.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 10:43:20 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Jordanova", "Pavlina K.", ""], ["Petkova", "Monika P.", ""]]}, {"id": "1707.01311", "submitter": "Sylvain Le", "authors": "Ngoc Minh Nguyen, Sylvain Le Corff, Eric Moulines", "title": "Particle rejuvenation of Rao-Blackwellized Sequential Monte Carlo\n  smoothers for Conditionally Linear and Gaussian models", "comments": null, "journal-ref": null, "doi": "10.1186/s13634-017-0489-5", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on Sequential Monte Carlo approximations of smoothing\ndistributions in conditionally linear and Gaussian state spaces. To reduce\nMonte Carlo variance of smoothers, it is typical in these models to use\nRao-Blackwellization: particle approximation is used to sample sequences of\nhidden regimes while the Gaussian states are explicitly integrated conditional\non the sequence of regimes and observations, using variants of the Kalman\nfilter / smoother. The first successful attempt to use Rao-Blackwellization for\nsmoothing extends the Bryson-Frazier smoother for Gaussian linear state space\nmodels using the generalized two-filter formula together with Kalman filters /\nsmoothers. More recently, a forward backward decomposition of smoothing\ndistributions mimicking the Rauch-Tung-Striebel smoother for the regimes\ncombined with backward Kalman updates has been introduced. This paper\ninvestigates the benefit of introducing additional rejuvenation steps in all\nthese algorithms to sample at each time instant new regimes conditional on the\nforward and backward particles. This defines particle based approximations of\nthe smoothing distributions whose support is not restricted to the set of\nparticles sampled in the forward or backward filter. These procedures are\napplied to commodity markets which are described using a two factor model based\non the spot price and a convenience yield for crude oil data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 10:46:26 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Nguyen", "Ngoc Minh", ""], ["Corff", "Sylvain Le", ""], ["Moulines", "Eric", ""]]}, {"id": "1707.01370", "submitter": "Nassim Nicholas Taleb", "authors": "Andrea Fontanari, Nassim Nicholas Taleb, Pasquale Cirillo", "title": "Gini estimation under infinite variance", "comments": null, "journal-ref": "Physica A: Statistical Mechanics and its Applications 502,\n  256-269, 2018", "doi": "10.1016/j.physa.2018.02.102", "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problems related to the estimation of the Gini index in presence\nof a fat-tailed data generating process, i.e. one in the stable distribution\nclass with finite mean but infinite variance (i.e. with tail index\n$\\alpha\\in(1,2)$). We show that, in such a case, the Gini coefficient cannot be\nreliably estimated using conventional nonparametric methods, because of a\ndownward bias that emerges under fat tails. This has important implications for\nthe ongoing discussion about economic inequality.\n  We start by discussing how the nonparametric estimator of the Gini index\nundergoes a phase transition in the symmetry structure of its asymptotic\ndistribution, as the data distribution shifts from the domain of attraction of\na light-tailed distribution to that of a fat-tailed one, especially in the case\nof infinite variance. We also show how the nonparametric Gini bias increases\nwith lower values of $\\alpha$. We then prove that maximum likelihood estimation\noutperforms nonparametric methods, requiring a much smaller sample size to\nreach efficiency.\n  Finally, for fat-tailed data, we provide a simple correction mechanism to the\nsmall sample bias of the nonparametric estimator based on the distance between\nthe mode and the mean of its asymptotic distribution.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 12:46:35 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 11:40:38 GMT"}, {"version": "v3", "created": "Mon, 17 Jul 2017 21:38:37 GMT"}, {"version": "v4", "created": "Sun, 17 Dec 2017 23:17:47 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Fontanari", "Andrea", ""], ["Taleb", "Nassim Nicholas", ""], ["Cirillo", "Pasquale", ""]]}, {"id": "1707.01473", "submitter": "Jann Spiess", "authors": "Jens Ludwig, Sendhil Mullainathan, Jann Spiess", "title": "Machine-Learning Tests for Effects on Multiple Outcomes", "comments": "Minor update: new abstract, introduction, and section on representing\n  distribution differences", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present tools for applied researchers that re-purpose\noff-the-shelf methods from the computer-science field of machine learning to\ncreate a \"discovery engine\" for data from randomized controlled trials (RCTs).\nThe applied problem we seek to solve is that economists invest vast resources\ninto carrying out RCTs, including the collection of a rich set of candidate\noutcome measures. But given concerns about inference in the presence of\nmultiple testing, economists usually wind up exploring just a small subset of\nthe hypotheses that the available data could be used to test. This prevents us\nfrom extracting as much information as possible from each RCT, which in turn\nimpairs our ability to develop new theories or strengthen the design of policy\ninterventions. Our proposed solution combines the basic intuition of reverse\nregression, where the dependent variable of interest now becomes treatment\nassignment itself, with methods from machine learning that use the data\nthemselves to flexibly identify whether there is any function of the outcomes\nthat predicts (or has signal about) treatment group status. This leads to\ncorrectly-sized tests with appropriate $p$-values, which also have the\nimportant virtue of being easy to implement in practice. One open challenge\nthat remains with our work is how to meaningfully interpret the signal that\nthese methods find.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 17:13:08 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 18:10:56 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Ludwig", "Jens", ""], ["Mullainathan", "Sendhil", ""], ["Spiess", "Jann", ""]]}, {"id": "1707.01517", "submitter": "Francisco Traversaro Prof.", "authors": "Francisco Traversaro, Marcelo Risk, Osvaldo Rosso, Francisco Redelico", "title": "An empirical evaluation of alternative methods of estimation for\n  Permutation Entropy in time series with tied values", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandt and Pompe introduced Permutation Entropy in 2002 for Time Series where\nequal values, xt1 = xt2, t1 = t2, were neglected and only inequalities between\nthe xt were considered. Since then, this measure has been modified and\nextended, in particular in cases when the amount of equal values in the series\ncan not be neglected, (i.e. heart rate variability (HRV) time series). We\nreview the different existing methodologies that treats this subject by\nclassifying them according to their different strategies. In addition, a novel\nBayesian Missing Data Imputation is presented that proves to outperform the\nexisting methodologies that deals with type of time series. All this facts are\nillustrated by simulations and also by distinguishing patients suffering from\nCongestive Heart Failure from a (healthy) control group using HRV time series\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 00:31:27 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Traversaro", "Francisco", ""], ["Risk", "Marcelo", ""], ["Rosso", "Osvaldo", ""], ["Redelico", "Francisco", ""]]}, {"id": "1707.01522", "submitter": "Yakov Nikitin", "authors": "Ya. Yu. Nikitin", "title": "Tests based on characterizations, and their efficiencies: a survey", "comments": "Open access in Acta et Commentationes Universitatis Tartuensis de\n  Mathematica", "journal-ref": "Acta et Commentationes Universitatis Tartuensis de Mathematica,17,\n  N 1, 3-24 (2017)", "doi": "10.12697/ACUTM.2017.21.01", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A survey of goodness-of-fit and symmetry tests based on the characterization\nproperties of distributions is presented. This approach became popular in\nrecent years. In most cases the test statistics are functionals of\n$U$-empirical processes. The limiting distributions and large deviations of new\nstatistics under the null hypothesis are described. Their local Bahadur\nefficiency for various parametric alternatives is calculated and compared with\neach other as well as with diverse previously known tests. We also describe new\ndirections of possible research in this domain.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 18:20:25 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Nikitin", "Ya. Yu.", ""]]}, {"id": "1707.01585", "submitter": "Daniel Fraiman", "authors": "Daniel Fraiman and Ricardo Fraiman", "title": "Statistical comparison of (brain) networks", "comments": "Three references added. A new paragraph was added in the\n  Resting-state fMRI functional networks section", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of random networks in a neuroscientific context has developed\nextensively over the last couple of decades. By contrast, techniques for the\nstatistical analysis of these networks are less developed. In this paper, we\nfocus on the statistical comparison of brain networks in a nonparametric\nframework and discuss the associated detection and identification problems. We\ntested network differences between groups with an analysis of variance (ANOVA)\ntest we developed specifically for networks. We also propose and analyse the\nbehaviour of a new statistical procedure designed to identify different\nsubnetworks. As an example, we show the application of this tool in\nresting-state fMRI data obtained from the Human Connectome Project. Finally, we\ndiscuss the potential bias in neuroimaging findings that is generated by some\nbehavioural and brain structure variables. Our method can also be applied to\nother kind of networks such as protein interaction networks, gene networks or\nsocial networks.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jul 2017 21:26:58 GMT"}, {"version": "v2", "created": "Sun, 9 Jul 2017 22:12:41 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Fraiman", "Daniel", ""], ["Fraiman", "Ricardo", ""]]}, {"id": "1707.01694", "submitter": "Juho Piironen", "authors": "Juho Piironen, Aki Vehtari", "title": "Sparsity information and regularization in the horseshoe and other\n  shrinkage priors", "comments": null, "journal-ref": "Electronic Journal of Statistics, Volume 11, Number 2 (2017),\n  5018-5051. https://projecteuclid.org/euclid.ejs/1513306866", "doi": "10.1214/17-EJS1337SI", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The horseshoe prior has proven to be a noteworthy alternative for sparse\nBayesian estimation, but has previously suffered from two problems. First,\nthere has been no systematic way of specifying a prior for the global shrinkage\nhyperparameter based on the prior information about the degree of sparsity in\nthe parameter vector. Second, the horseshoe prior has the undesired property\nthat there is no possibility of specifying separately information about\nsparsity and the amount of regularization for the largest coefficients, which\ncan be problematic with weakly identified parameters, such as the logistic\nregression coefficients in the case of data separation. This paper proposes\nsolutions to both of these problems. We introduce a concept of effective number\nof nonzero parameters, show an intuitive way of formulating the prior for the\nglobal hyperparameter based on the sparsity assumptions, and argue that the\nprevious default choices are dubious based on their tendency to favor solutions\nwith more unshrunk parameters than we typically expect a priori. Moreover, we\nintroduce a generalization to the horseshoe prior, called the regularized\nhorseshoe, that allows us to specify a minimum level of regularization to the\nlargest values. We show that the new prior can be considered as the continuous\ncounterpart of the spike-and-slab prior with a finite slab width, whereas the\noriginal horseshoe resembles the spike-and-slab with an infinitely wide slab.\nNumerical experiments on synthetic and real world data illustrate the benefit\nof both of these theoretical advances.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 08:59:03 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Piironen", "Juho", ""], ["Vehtari", "Aki", ""]]}, {"id": "1707.01723", "submitter": "Cedric Ginestet", "authors": "Cedric E. Ginestet and Richard Emsley and Sabine Landau", "title": "Stein-like Estimators for Causal Mediation Analysis in Randomized Trials", "comments": "26 pages, 6 figures, 1 table. Presented at the conference of the\n  International Society for Clinical Biostatistics (ISCB 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis aims to estimate the natural direct and indirect\neffects under clearly specified assumptions. Traditional mediation analysis\nbased on Ordinary Least Squares (OLS) relies on the absence of unmeasured\ncauses of the putative mediator and outcome. When this assumption cannot be\njustified, Instrumental Variables (IV) estimators can be used in order to\nproduce an asymptotically unbiased estimator of the mediator-outcome link.\nHowever, provided that valid instruments exist, bias removal comes at the cost\nof variance inflation for standard IV procedures such as Two-Stage Least\nSquares (TSLS). A Semi-Parametric Stein-Like (SPSL) estimator has been proposed\nin the literature that strikes a natural trade-off between the unbiasedness of\nthe TSLS procedure and the relatively small variance of the OLS estimator.\nMoreover, the SPSL has the advantage that its shrinkage parameter can be\ndirectly estimated from the data. In this paper, we demonstrate how this\nStein-like estimator can be implemented in the context of the estimation of\nnatural direct and natural indirect effects of treatments in randomized\ncontrolled trials. The performance of the competing methods is studied in a\nsimulation study, in which both the strength of hidden confounding and the\nstrength of the instruments are independently varied. These considerations are\nmotivated by a trial in mental health evaluating the impact of a primary\ncare-based intervention to reduce depression in the elderly.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 10:32:46 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Ginestet", "Cedric E.", ""], ["Emsley", "Richard", ""], ["Landau", "Sabine", ""]]}, {"id": "1707.01822", "submitter": "Bowen Li", "authors": "Bowen Li", "title": "Nonparametric Marginal Analysis of Recurrent Events Data under Competing\n  Risks", "comments": "PhD Dissertation, 73 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project was motivated by a dialysis study in northern Taiwan. Dialysis\npatients, after shunt implantation, may experience two types (\"acute\" or\n\"non-acute\") of shunt thrombosis, both of which may recur. We formulate the\nproblem under the framework of recurrent events data in the presence of\ncompeting risks. In particular we focus on marginal inference for the gap time\nvariable of specific type. The functions of interest are the cumulative\nincidence function and cause-specific hazard function. The major challenge of\nnonparametric inference is the problem of induced dependent censoring. We apply\nthe technique of inverse probability of censoring weighting (IPCW) to adjust\nfor the selection bias. Besides point estimation, we apply the bootstrap\nre-sampling method for further inference. Large sample properties of the\nproposed estimators are derived. Simulations are performed to examine the\nfinite-sample performances of the proposed methods. Finally we apply the\nproposed methodology to analyze the dialysis data.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 14:49:42 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Li", "Bowen", ""]]}, {"id": "1707.01951", "submitter": "Marina Valdora", "authors": "Julieta Molina, Mariela Sued, Marina Valdora and V\\'ictor Yohai", "title": "Robust Doubly Protected Estimators for Quantiles with Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Doubly protected estimators are widely used for estimating the population\nmean of an outcome Y from a sample where the response is missing in some\nindividuals. To compensate for the missing responses, a vector X of covariates\nis observed at each individual, and the missing mechanism is assumed to be\nindependent of the response, conditioned on X (missing at random). In recent\nyears, many authors have moved from the mean to the median, and more generally,\ndoubly protected estimators of the quantiles have been proposed, assuming a\nparametric regression model for the relationship between X and Y and a\nparametric form for the propensity score. In this work, we present doubly\nprotected estimators for the quantiles that are also robust, in the sense that\nthey are resistant to the presence of outliers in the sample. We also\nflexibilize the model for the relationship between X and Y . Thus we present\nrobust doubly protected estimators for the quantiles of the response in the\npresence of missing observations, postulating a semiparametric regression model\nfor the relationship between the response and the covariates and a parametric\nmodel for the propensity score.\n", "versions": [{"version": "v1", "created": "Thu, 6 Jul 2017 20:00:28 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 12:53:21 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Molina", "Julieta", ""], ["Sued", "Mariela", ""], ["Valdora", "Marina", ""], ["Yohai", "V\u00edctor", ""]]}, {"id": "1707.02014", "submitter": "Zhanfeng Wang", "authors": "Wang Zhanfeng and Noh Maengseok and Lee Youngjo and Shi Jianqing", "title": "A Robust t-process Regression Model with Independent Errors", "comments": "27 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian process regression (GPR) model is well-known to be susceptible to\noutliers. Robust process regression models based on t-process or other\nheavy-tailed processes have been developed to address the problem. However, due\nto the nature of the current definition for heavy-tailed processes, the unknown\nprocess regression function and the random errors are always defined jointly\nand thus dependently. This definition, mainly owing to the dependence\nassumption involved, is not justified in many practical problems and thus\nlimits the application of those robust approaches. It also results in a\nlimitation of the theory of robust analysis. In this paper, we propose a new\nrobust process regression model enabling independent random errors. An\nefficient estimation procedure is developed. Statistical properties, such as\nunbiasness and information consistency, are provided. Numerical studies show\nthat the proposed method is robust against outliers and has a better\nperformance in prediction compared with the existing models. We illustrate that\nthe estimated random-effects are useful in detecting outlying curves.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 01:10:59 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Zhanfeng", "Wang", ""], ["Maengseok", "Noh", ""], ["Youngjo", "Lee", ""], ["Jianqing", "Shi", ""]]}, {"id": "1707.02055", "submitter": "Kyungchul Song", "authors": "Kyungchul Song and Zhengfei Yu", "title": "Estimation and Inference on Treatment Effects Under Treatment-Based\n  Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal inference in a program evaluation setting faces the problem of\nexternal validity when the treatment effect in the target population is\ndifferent from the treatment effect identified from the population of which the\nsample is representative. This paper focuses on a situation where such\ndiscrepancy arises by a stratified sampling design based on the individual\ntreatment status and other characteristics. In such settings, the design\nprobability is known from the sampling design but the target population depends\non the underlying population share vector which is often unknown, and except\nfor special cases, the treatment effect parameters are not identified. First,\nwe propose a method of constructing confidence sets that are valid for a given\nrange of population shares. Second, when a benchmark population share vector\nand a corresponding estimator of a treatment effect parameter are given, we\npropose a method to discover the scope of external validity with familywise\nerror rate control. Third, we propose an optimal sampling design which\nminimizes the semiparametric efficiency bound given a population share\nassociated with a target population. We provide Monte Carlo simulation results\nand an empirical application to demonstrate the usefulness of our proposals.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 06:44:20 GMT"}, {"version": "v2", "created": "Thu, 24 Dec 2020 20:27:27 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Song", "Kyungchul", ""], ["Yu", "Zhengfei", ""]]}, {"id": "1707.02127", "submitter": "Jiao Song", "authors": "Jiao Song and Jiang-lun Wu", "title": "Critical link of self-similarity and visualisation for jump-diffusions\n  driven by $\\alpha$-stable noise", "comments": "27 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to derive a critical link of parameters for the\nself-similar trajectories of jump-diffusions which are described as solutions\nof stochastic differential equations driven by $\\alpha$-stable noise. This is\ndone by a multivariate Lagrange interpolation approach. To this end, we utilise\ncomputer simulation algorithm in MATLAB to visualise the trajectories of the\njump-diffusions for various combinations of parameters arising in the\nstochastic differential equations.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 11:44:44 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Song", "Jiao", ""], ["Wu", "Jiang-lun", ""]]}, {"id": "1707.02147", "submitter": "Amelia Sim\\'o", "authors": "Sonia Barahona, Pablo Centella, Ximo Gual-Arnau, Maria Victoria\n  Ib\\'a\\~nez and Amelia Sim\\'o", "title": "Classification of geometrical objects by integrating currents and\n  functional data analysis. An application to a 3D database of Spanish child\n  population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the application of Discriminant Analysis to a set of\ngeometrical objects (bodies) characterized by currents. A current is a relevant\nmathematical object to model geometrical data, like hypersurfaces, through\nintegration of vector fields along them. As a consequence of the choice of a\nvector-valued Reproducing Kernel Hilbert Space (RKHS) as a test space to\nintegrate hypersurfaces, it is possible to consider that hypersurfaces are\nembedded in this Hilbert space. This embedding enables us to consider\nclassification algorithms of geometrical objects. A method to apply Functional\nDiscriminant Analysis in the obtained vector-valued RKHS is given. This method\nis based on the eigenfunction decomposition of the kernel. So, the novelty of\nthis paper is the reformulation of a size and shape classification problem in\nFunctional Data Analysis terms using the theory of currents and vector-valued\nRKHS. This approach is applied to a 3D database obtained from an anthropometric\nsurvey of the Spanish child population with a potential application to online\nsales of children's wear.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 12:37:50 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 19:42:25 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Barahona", "Sonia", ""], ["Centella", "Pablo", ""], ["Gual-Arnau", "Ximo", ""], ["Ib\u00e1\u00f1ez", "Maria Victoria", ""], ["Sim\u00f3", "Amelia", ""]]}, {"id": "1707.02182", "submitter": "Maria Francesca  Marino", "authors": "Alessandra Spagnoli, Maria Francesca Marino, Marco Alf\\`o", "title": "A bi-dimensional finite mixture model for longitudinal data subject to\n  dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In longitudinal studies, subjects may be lost to follow-up, or miss some of\nthe planned visits, leading to incomplete response sequences. When the\nprobability of non-response, conditional on the available covariates and the\nobserved responses, still depends on unobserved outcomes, the dropout mechanism\nis said to be non ignorable. A common objective is to build a reliable\nassociation structure to account for dependence between the longitudinal and\nthe dropout processes. Starting from the existing literature, we introduce a\nrandom coefficient based dropout model where the association between outcomes\nis modeled through discrete latent effects. These effects are outcome-specific\nand account for heterogeneity in the univariate profiles. Dependence between\nprofiles is introduced by using a bi-dimensional representation for the\ncorresponding distribution. In this way, we define a flexible latent class\nstructure which allows to efficiently describe both dependence within the two\nmargins of interest and dependence between them. By using this representation\nwe show that, unlike standard (unidimensional) finite mixture models, the non\nignorable dropout model properly nests its ignorable counterpart. We detail the\nproposed modeling approach by analyzing data from a longitudinal study on the\ndynamics of cognitive functioning in the elderly. Further, the effects of\nassumptions about non ignorability of the dropout process on model parameter\nestimates are (locally) investigated using the index of (local) sensitivity to\nnon-ignorability.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 13:59:47 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Spagnoli", "Alessandra", ""], ["Marino", "Maria Francesca", ""], ["Alf\u00f2", "Marco", ""]]}, {"id": "1707.02215", "submitter": "Stephen Burgess", "authors": "Stephen Burgess, Verena Zuber, Elsa Valdes-Marquez, Benjamin B Sun,\n  Jemma C Hopewell", "title": "Mendelian randomization with fine-mapped genetic data: choosing from\n  large numbers of correlated instrumental variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mendelian randomization uses genetic variants to make causal inferences about\nthe effect of a risk factor on an outcome. With fine-mapped genetic data, there\nmay be hundreds of genetic variants in a single gene region any of which could\nbe used to assess this causal relationship. However, using too many genetic\nvariants in the analysis can lead to spurious estimates and inflated Type 1\nerror rates. But if only a few genetic variants are used, then the majority of\nthe data is ignored and estimates are highly sensitive to the particular choice\nof variants. We propose an approach based on summarized data only (genetic\nassociation and correlation estimates) that uses principal components analysis\nto form instruments. This approach has desirable theoretical properties: it\ntakes the totality of data into account and does not suffer from numerical\ninstabilities. It also has good properties in simulation studies: it is not\nparticularly sensitive to varying the genetic variants included in the analysis\nor the genetic correlation matrix, and it does not have greatly inflated Type 1\nerror rates. Overall, the method gives estimates that are not so precise as\nthose from variable selection approaches (such as using a conditional analysis\nor pruning approach to select variants), but are more robust to seemingly\narbitrary choices in the variable selection step. Methods are illustrated by an\nexample using genetic associations with testosterone for 320 genetic variants\nto assess the effect of sex hormone-related pathways on coronary artery disease\nrisk, in which variable selection approaches give inconsistent inferences.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 15:07:06 GMT"}], "update_date": "2017-07-10", "authors_parsed": [["Burgess", "Stephen", ""], ["Zuber", "Verena", ""], ["Valdes-Marquez", "Elsa", ""], ["Sun", "Benjamin B", ""], ["Hopewell", "Jemma C", ""]]}, {"id": "1707.02233", "submitter": "Clara Happ", "authors": "Clara Happ, Sonja Greven, Volker J. Schmid", "title": "The Impact of Model Assumptions in Scalar-on-Image Regression", "comments": null, "journal-ref": "Statistics in Medicine 2018 (19), pp. 4298-4317", "doi": "10.1002/sim.7915", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex statistical models such as scalar-on-image regression often require\nstrong assumptions to overcome the issue of non-identifiability. While in\ntheory it is well understood that model assumptions can strongly influence the\nresults, this seems to be underappreciated, or played down, in practice.\n  The article gives a systematic overview of the main approaches for\nscalar-on-image regression with a special focus on their assumptions. We\ncategorize the assumptions and develop measures to quantify the degree to which\nthey are met. The impact of model assumptions and the practical usage of the\nproposed measures are illustrated in a simulation study and in an application\nto neuroimaging data. The results show that different assumptions indeed lead\nto quite different estimates with similar predictive ability, raising the\nquestion of their interpretability. We give recommendations for making modeling\nand interpretation decisions in practice, based on the new measures and\nsimulations using hypothetic coefficient images and the observed data.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 15:41:09 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 16:48:10 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 13:49:10 GMT"}], "update_date": "2020-05-04", "authors_parsed": [["Happ", "Clara", ""], ["Greven", "Sonja", ""], ["Schmid", "Volker J.", ""]]}, {"id": "1707.02247", "submitter": "Paul McNicholas", "authors": "Paula M. Murray, Ryan P. Browne and Paul D. McNicholas", "title": "Hidden Truncation Hyperbolic Distributions, Finite Mixtures Thereof, and\n  Their Application for Clustering", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmva.2017.07.008", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A hidden truncation hyperbolic (HTH) distribution is introduced and finite\nmixtures thereof are applied for clustering. A stochastic representation of the\nHTH distribution is given and a density is derived. A hierarchical\nrepresentation is described, which aids in parameter estimation. Finite\nmixtures of HTH distributions are presented and their identifiability is\nproved. The convexity of the HTH distribution is discussed, which is important\nin clustering applications, and some theoretical results in this direction are\npresented. The relationship between the HTH distribution and other skewed\ndistributions in the literature is discussed. Illustrations are provided ---\nboth of the HTH distribution and application of finite mixtures thereof for\nclustering.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 16:10:00 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 22:53:30 GMT"}], "update_date": "2018-11-02", "authors_parsed": [["Murray", "Paula M.", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1707.02333", "submitter": "Abhik Ghosh", "authors": "Ayanendranath Basu, Abhik Ghosh, Nirian Martin, Leandro Pardo", "title": "Robust Wald-type tests for non-homogeneous observations based on minimum\n  density power divergence estimator", "comments": "Pre-print, Under review", "journal-ref": "Metrika (2018) 81: 493", "doi": "10.1007/s00184-018-0653-4", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of robust hypothesis testing under\nnon-identically distributed data. We propose Wald-type tests for both simple\nand composite hypothesis for independent but non-homogeneous observations based\non the robust minimum density power divergence estimator of the common\nunderlying parameter. Asymptotic and theoretical robustness properties of the\nproposed tests have been discussed. Application to the problem of testing the\ngeneral linear hypothesis in a generalized linear model with fixed-design has\nbeen considered in detail with specific illustrations for its special cases\nunder normal and Poisson distributions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jul 2017 18:52:44 GMT"}], "update_date": "2019-05-09", "authors_parsed": [["Basu", "Ayanendranath", ""], ["Ghosh", "Abhik", ""], ["Martin", "Nirian", ""], ["Pardo", "Leandro", ""]]}, {"id": "1707.02430", "submitter": "Hamed Masnadi-Shirazi", "authors": "Hamed Masnadi-Shirazi", "title": "Combining Forecasts Using Ensemble Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of combining individual forecasters to produce a forecaster with\nimproved performance is considered. The connections between probability\nelicitation and classification are used to pose the combining forecaster\nproblem as that of ensemble learning. With this connection in place, a number\nof theoretically sound ensemble learning methods such as Bagging and Boosting\nare adapted for combining forecasters. It is shown that the simple yet\neffective method of averaging the forecasts is equivalent to Bagging. This\nprovides theoretical insight into why the well established averaging of\nforecasts method works so well. Also, a nonlinear combination of forecasters\ncan be attained through Boosting which is shown to theoretically produce\ncombined forecasters that are both calibrated and highly refined. Finally, the\nproposed methods of combining forecasters are applied to the Good Judgment\nProject data set and are shown to outperform the individual forecasters.\n", "versions": [{"version": "v1", "created": "Sat, 8 Jul 2017 12:23:05 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Masnadi-Shirazi", "Hamed", ""]]}, {"id": "1707.02548", "submitter": "Duncan Ermini Leaf", "authors": "Duncan Ermini Leaf", "title": "Unified Method for Markov Chain Transition Model Estimation Using\n  Incomplete Survey Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Future Elderly Model and related microsimulations are modeled as Markov\nchains. These simulations rely on longitudinal survey data to estimate their\ntransition models. The use of survey data presents several incomplete data\nproblems, including coarse and irregular spacing of interviews, data collection\nfrom subsamples, and structural changes to surveys over time. The\nExpectation-Maximization algorithm is adapted to create a method for maximum\nlikelihood estimation of Markov chain transition models using incomplete data.\nThe method is demonstrated on a simplified version of the Future Elderly Model.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 09:32:52 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Leaf", "Duncan Ermini", ""]]}, {"id": "1707.02587", "submitter": "Wilson Ye Chen", "authors": "Wilson Ye Chen, Gareth W. Peters, Richard H. Gerlach, Scott A. Sisson", "title": "Dynamic Quantile Function Models", "comments": "MATLAB code: https://github.com/wilson-ye-chen/aqua", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the need for effectively summarising, modelling, and forecasting\nthe distributional characteristics of intra-daily returns, as well as the\nrecent work on forecasting histogram-valued time-series in the area of symbolic\ndata analysis, we develop a time-series model for forecasting\nquantile-function-valued (QF-valued) daily summaries for intra-daily returns.\nWe call this model the dynamic quantile function (DQF) model. Instead of a\nhistogram, we propose to use a $g$-and-$h$ quantile function to summarise the\ndistribution of intra-daily returns. We work with a Bayesian formulation of the\nDQF model in order to make statistical inference while accounting for parameter\nuncertainty; an efficient MCMC algorithm is developed for sampling-based\nposterior inference. Using ten international market indices and approximately\n2,000 days of out-of-sample data from each market, the performance of the DQF\nmodel compares favourably, in terms of forecasting VaR of intra-daily returns,\nagainst the interval-valued and histogram-valued time-series models.\nAdditionally, we demonstrate that the QF-valued forecasts can be used to\nforecast VaR measures at the daily timescale via a simple quantile regression\nmodel on daily returns (QR-DQF). In certain markets, the resulting QR-DQF model\nis able to provide competitive VaR forecasts for daily returns.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 14:31:09 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 07:19:26 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 05:59:04 GMT"}, {"version": "v4", "created": "Fri, 22 Nov 2019 03:49:30 GMT"}, {"version": "v5", "created": "Tue, 4 May 2021 15:24:03 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Chen", "Wilson Ye", ""], ["Peters", "Gareth W.", ""], ["Gerlach", "Richard H.", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1707.02597", "submitter": "Hao Wu", "authors": "Jolynn Pek and Hao Wu", "title": "Parameter uncertainty in structural equation models: Confidence sets and\n  fungible estimates", "comments": "39 pages, 6 figures, 6 tables. Accepted by Psychological Methods on\n  July 1, 2017 but not the final version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current concerns regarding the dependability of psychological findings call\nfor methodological developments to provide additional evidence in support of\nscientific conclusions. This paper highlights the value and importance of two\ndistinct kinds of parameter uncertainty which are quantified by confidence sets\n(CSs) and fungible parameter estimates (FPEs); both provide essential\ninformation regarding the defensibility of scientific findings. Using the\nstructural equation model, we introduce a general perturbation framework based\non the likelihood function that unifies CSs and FPEs and sheds new light on the\nconceptual distinctions between them. A targeted illustration is then presented\nto demonstrate the factors which differentially influence CSs and FPEs, further\nhighlighting their theoretical differences. With three empirical examples on\ninitiating a conversation with a stranger, posttraumatic growth of caregivers\nin the context of pediatric palliative care, and the direct and indirect\neffects of spirituality on thriving among youth, we illustrate how CSs and FPEs\nprovide unique information which lead to better informed scientific\nconclusions. Finally, we discuss the importance of considering information\nafforded by CSs and FPEs in strengthening the basis of interpreting statistical\nresults in substantive research, conclude with future research directions, and\nprovide example OpenMx code for the computation of CSs and FPEs.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 16:04:40 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Pek", "Jolynn", ""], ["Wu", "Hao", ""]]}, {"id": "1707.02641", "submitter": "Vincent Dorie", "authors": "Vincent Dorie, Jennifer Hill, Uri Shalit, Marc Scott, and Dan Cervone", "title": "Automated versus do-it-yourself methods for causal inference: Lessons\n  learned from a data analysis competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statisticians have made great progress in creating methods that reduce our\nreliance on parametric assumptions. However this explosion in research has\nresulted in a breadth of inferential strategies that both create opportunities\nfor more reliable inference as well as complicate the choices that an applied\nresearcher has to make and defend. Relatedly, researchers advocating for new\nmethods typically compare their method to at best 2 or 3 other causal inference\nstrategies and test using simulations that may or may not be designed to\nequally tease out flaws in all the competing methods. The causal inference data\nanalysis challenge, \"Is Your SATT Where It's At?\", launched as part of the 2016\nAtlantic Causal Inference Conference, sought to make progress with respect to\nboth of these issues. The researchers creating the data testing grounds were\ndistinct from the researchers submitting methods whose efficacy would be\nevaluated. Results from 30 competitors across the two versions of the\ncompetition (black box algorithms and do-it-yourself analyses) are presented\nalong with post-hoc analyses that reveal information about the characteristics\nof causal inference strategies and settings that affect performance. The most\nconsistent conclusion was that methods that flexibly model the response surface\nperform better overall than methods that fail to do so. Finally new methods are\nproposed that combine features of several of the top-performing submitted\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 9 Jul 2017 21:24:25 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 18:40:08 GMT"}, {"version": "v3", "created": "Fri, 6 Oct 2017 19:27:46 GMT"}, {"version": "v4", "created": "Thu, 8 Mar 2018 16:38:31 GMT"}, {"version": "v5", "created": "Fri, 20 Jul 2018 16:18:04 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Dorie", "Vincent", ""], ["Hill", "Jennifer", ""], ["Shalit", "Uri", ""], ["Scott", "Marc", ""], ["Cervone", "Dan", ""]]}, {"id": "1707.03057", "submitter": "Hyungsuk Tak", "authors": "Hyungsuk Tak, Justin A. Ellis, Sujit K. Ghosh", "title": "Robust and Accurate Inference via a Mixture of Gaussian and Student's t\n  Errors", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2018.1537925", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Gaussian measurement error assumption, i.e., an assumption that the data\nare observed up to Gaussian noise, can bias any parameter estimation in the\npresence of outliers. A heavy tailed error assumption based on Student's t\ndistribution helps reduce the bias. However, it may be less efficient in\nestimating parameters if the heavy tailed assumption is uniformly applied to\nall of the data when most of them are normally observed. We propose a mixture\nerror assumption that selectively converts Gaussian errors into Student's t\nerrors according to latent outlier indicators, leveraging the best of the\nGaussian and Student's t errors; a parameter estimation can be not only robust\nbut also accurate. Using simulated hospital profiling data and astronomical\ntime series of brightness data, we demonstrate the potential for the proposed\nmixture error assumption to estimate parameters accurately in the presence of\noutliers.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 20:42:00 GMT"}, {"version": "v2", "created": "Sat, 24 Feb 2018 21:47:33 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2018 18:46:14 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Tak", "Hyungsuk", ""], ["Ellis", "Justin A.", ""], ["Ghosh", "Sujit K.", ""]]}, {"id": "1707.03117", "submitter": "Andrew Holbrook", "authors": "Andrew Holbrook, Shiwei Lan, Jeffrey Streets, and Babak Shahbaba", "title": "The nonparametric Fisher geometry and the chi-square process density\n  prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that the Fisher information induces a Riemannian geometry on\nparametric families of probability density functions. Following recent work, we\nconsider the nonparametric generalization of the Fisher geometry. The resulting\nnonparametric Fisher geometry is shown to be equivalent to a familiar, albeit\ninfinite-dimensional, geometric object---the sphere. By shifting focus away\nfrom density functions and toward \\emph{square-root} density functions, one may\ncalculate theoretical quantities of interest with ease. More importantly, the\nsphere of square-root densities is much more computationally tractable. This\ninsight leads to a novel Bayesian nonparametric density estimation model. We\nconstruct the $\\chi^2$-process density prior by modeling the square-root\ndensity with a restricted Gaussian process prior. Inference over square-root\ndensities is fast, and the model retains the flexibility characteristic of\nBayesian nonparametric models. Finally, we formalize the relationship between\nspherical HMC in the infinite-dimensional limit and standard Riemannian HMC.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 03:16:38 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 20:27:15 GMT"}], "update_date": "2017-11-16", "authors_parsed": [["Holbrook", "Andrew", ""], ["Lan", "Shiwei", ""], ["Streets", "Jeffrey", ""], ["Shahbaba", "Babak", ""]]}, {"id": "1707.03119", "submitter": "Agatha Rodrigues Mrs.", "authors": "Agatha S. Rodrigues, Felipe Bhering, Carlos Alberto de Braganca\n  Pereira and Adriano Polpo", "title": "Estimation of Component Reliability in Coherent Systems", "comments": null, "journal-ref": null, "doi": "10.1109/ACCESS.2018.2821102", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The first step in statistical reliability studies of coherent systems is the\nestimation of the reliability of each system component. For the cases of\nparallel and series systems the literature is abundant. It seems that the\npresent paper is the first that presents the general case of component\ninferences in coherent systems. The failure time model considered here is the\nthree-parameter Weibull distribution. Furthermore, neither independence nor\nidentically distributed failure times are required restrictions. The proposed\nmodel is general in the sense that it can be used for any coherent system, from\nthe simplest to the more complex structures. It can be considered for all kinds\nof censored data; including interval-censored data. An important property\nobtained for the Weibull model is the fact that the posterior distributions are\nproper, even for non-informative priors. Using several simulations, the\nexcellent performance of the model is illustrated. As a real example, boys\nfirst use of marijuana is considered to show the efficiency of the solution\neven when censored data occurs.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 03:19:11 GMT"}], "update_date": "2018-10-05", "authors_parsed": [["Rodrigues", "Agatha S.", ""], ["Bhering", "Felipe", ""], ["Pereira", "Carlos Alberto de Braganca", ""], ["Polpo", "Adriano", ""]]}, {"id": "1707.03165", "submitter": "Alexander Kreuzer", "authors": "A. Kreuzer, T. Erhardt, T. Nagler, C. Czado", "title": "Heavy tailed spatial autocorrelation models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appropriate models for spatially autocorrelated data account for the fact\nthat observations are not independent. A popular model in this context is the\nsimultaneous autoregressive (SAR) model that allows to model the spatial\ndependency structure of a response variable and the influence of covariates on\nthis variable. This spatial regression model assumes that the error follows a\nnormal distribution. Since this assumption cannot always be met, it is\nnecessary to extend this model to other error distributions. We propose the\nextension to the $t$-distribution, the tSAR model, which can be used if we\nobserve heavy tails in the fitted residuals of the SAR model. In addition, we\nprovide a variance estimate that considers the spatial structure of a variable\nwhich helps us to specify inputs for our models. An extended simulation study\nshows that the proposed estimators of the tSAR model are performing well and in\nan application to fire danger we see that the tSAR model is a notable\nimprovement compared to the SAR model.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 08:21:25 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Kreuzer", "A.", ""], ["Erhardt", "T.", ""], ["Nagler", "T.", ""], ["Czado", "C.", ""]]}, {"id": "1707.03173", "submitter": "Agatha Rodrigues Mrs.", "authors": "Agatha Sacramento Rodrigues, Carlos Alberto de Braganca Pereira and\n  Adriano Polpo", "title": "Reliability of components of coherent systems: estimates in presence of\n  masked data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliability of a system of components depends on reliability of each\ncomponent. Thus, the initial statistical work should be the estimation of the\nreliability of each component of the system. This is not an easy task because\nwhen the system fails, the failure time of a given component can not be\nobserved, that is, censored data. Rodrigues et al. (2017) presented a solution\nfor reliability estimation of components when it is avaliable the system\nfailure time and the status of each component at the time of system failure (if\nit had failed before, after or it is responsible for system failure). However,\nthere are situations it may be difficult to identify the status of components\nat the moment of system failure.\n  Such cases are systems with masked causes of failure. Since parallel and\nseries systems are the simplest systems, innumerous alternative solutions for\nthese two systems have been appeared in the literature. To the best of our\nknowledge, this seems to be the first work that considers the general case of\ncoherent systems. The three-parameter Weibull distribution is considered as the\ncomponent failure time model. Identically distributed failure times is not\nrequired restrictions. Furthermore, there is no restriction on the subjective\nchoice of prior distributions but preference has been given to continuous prior\ndistributions; these priors represent well the nuances of the environment that\nthe system operates. The statistical work of obtaining quantities of the\nposterior distribution is supported by the Metropolis within Gibbs algorithm.\nWith several simulations, the excellent performance of the model was evaluated.\nWe also consider a computer hard-drives real dataset in order to present the\npractical relevance of the proposed model.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 08:37:40 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 16:25:48 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Rodrigues", "Agatha Sacramento", ""], ["Pereira", "Carlos Alberto de Braganca", ""], ["Polpo", "Adriano", ""]]}, {"id": "1707.03247", "submitter": "Filip Elvander", "authors": "Johan Sw\\\"ard, Filip Elvander, and Andreas Jakobsson", "title": "Designing Sampling Schemes for Multi-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a method for determining a non-uniform sampling\nscheme for multi-dimensional signals by solving a convex optimization problem\nreminiscent of the sensor selection problem. The resulting sampling scheme\nminimizes the sum of the Cram\\'er-Rao lower bound for the parameters of\ninterest, given a desired number of sampling points. The proposed framework\nallows for selecting an arbitrary subset of the parameters detailing the model,\nas well as weighing the importance of the different parameters. Also presented\nis a scheme for incorporating any imprecise a priori knowledge of the locations\nof the parameters, as well as defining estimation performance bounds for the\nparameters of interest. Numerical examples illustrate the efficiency of the\nproposed scheme.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 12:39:43 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Sw\u00e4rd", "Johan", ""], ["Elvander", "Filip", ""], ["Jakobsson", "Andreas", ""]]}, {"id": "1707.03307", "submitter": "Matteo Fasiolo", "authors": "M. Fasiolo, S. N. Wood, M. Zaffran, R. Nedellec and Y.Goude", "title": "Fast calibrated additive quantile regression", "comments": null, "journal-ref": null, "doi": "10.1080/01621459.2020.1725521", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel framework for fitting additive quantile regression models,\nwhich provides well calibrated inference about the conditional quantiles and\nfast automatic estimation of the smoothing parameters, for model structures as\ndiverse as those usable with distributional GAMs, while maintaining equivalent\nnumerical efficiency and stability. The proposed methods are at once\nstatistically rigorous and computationally efficient, because they are based on\nthe general belief updating framework of Bissiri et al. (2016) to loss based\ninference, but compute by adapting the stable fitting methods of Wood et al.\n(2016). We show how the pinball loss is statistically suboptimal relative to a\nnovel smooth generalisation, which also gives access to fast estimation\nmethods. Further, we provide a novel calibration method for efficiently\nselecting the 'learning rate' balancing the loss with the smoothing priors\nduring inference, thereby obtaining reliable quantile uncertainty estimates.\nOur work was motivated by a probabilistic electricity load forecasting\napplication, used here to demonstrate the proposed approach. The methods\ndescribed here are implemented by the qgam R package, available on the\nComprehensive R Archive Network (CRAN).\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 14:47:43 GMT"}, {"version": "v2", "created": "Fri, 25 May 2018 10:44:08 GMT"}, {"version": "v3", "created": "Thu, 13 Jun 2019 17:48:43 GMT"}, {"version": "v4", "created": "Thu, 12 Mar 2020 10:18:31 GMT"}], "update_date": "2020-03-13", "authors_parsed": [["Fasiolo", "M.", ""], ["Wood", "S. N.", ""], ["Zaffran", "M.", ""], ["Nedellec", "R.", ""], ["Goude", "Y.", ""]]}, {"id": "1707.03436", "submitter": "David Kaplan", "authors": "Luciano de Castro (1), Antonio F. Galvao (2), David M. Kaplan (3), Xin\n  Liu (3) ((1) University of Iowa, (2) University of Arizona, (3) University of\n  Missouri)", "title": "Smoothed GMM for quantile models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops theory for feasible estimators of finite-dimensional\nparameters identified by general conditional quantile restrictions, under much\nweaker assumptions than previously seen in the literature. This includes\ninstrumental variables nonlinear quantile regression as a special case. More\nspecifically, we consider a set of unconditional moments implied by the\nconditional quantile restrictions, providing conditions for local\nidentification. Since estimators based on the sample moments are generally\nimpossible to compute numerically in practice, we study feasible estimators\nbased on smoothed sample moments. We propose a method of moments estimator for\nexactly identified models, as well as a generalized method of moments estimator\nfor over-identified models. We establish consistency and asymptotic normality\nof both estimators under general conditions that allow for weakly dependent\ndata and nonlinear structural models. Simulations illustrate the finite-sample\nproperties of the methods. Our in-depth empirical application concerns the\nconsumption Euler equation derived from quantile utility maximization.\nAdvantages of the quantile Euler equation include robustness to fat tails,\ndecoupling of risk attitude from the elasticity of intertemporal substitution,\nand log-linearization without any approximation error. For the four countries\nwe examine, the quantile estimates of discount factor and elasticity of\nintertemporal substitution are economically reasonable for a range of quantiles\nabove the median, even when two-stage least squares estimates are not\nreasonable.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 19:01:38 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 16:36:44 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["de Castro", "Luciano", ""], ["Galvao", "Antonio F.", ""], ["Kaplan", "David M.", ""], ["Liu", "Xin", ""]]}, {"id": "1707.03487", "submitter": "Runze Tang", "authors": "Runze Tang, Minh Tang, Joshua T. Vogelstein, Carey E. Priebe", "title": "Robust Estimation from Multiple Graphs under Gross Error Contamination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of graph parameters based on a collection of graphs is essential\nfor a wide range of graph inference tasks. In practice, weighted graphs are\ngenerally observed with edge contamination. We consider a weighted latent\nposition graph model contaminated via an edge weight gross error model and\npropose an estimation methodology based on robust Lq estimation followed by\nlow-rank adjacency spectral decomposition. We demonstrate that, under\nappropriate conditions, our estimator both maintains Lq robustness and wins the\nbias-variance tradeoff by exploiting low-rank graph structure. We illustrate\nthe improvement offered by our estimator via both simulations and a human\nconnectome data experiment.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 23:06:03 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Tang", "Runze", ""], ["Tang", "Minh", ""], ["Vogelstein", "Joshua T.", ""], ["Priebe", "Carey E.", ""]]}, {"id": "1707.03494", "submitter": "Mikhail Langovoy", "authors": "Mikhail A. Langovoy, Akhilesh Gotmare, Martin Jaggi", "title": "Unsupervised robust nonparametric learning of hidden community\n  properties", "comments": "Experiments with new types of adversaries added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.SI stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider learning of fundamental properties of communities in large noisy\nnetworks, in the prototypical situation where the nodes or users are split into\ntwo classes according to a binary property, e.g., according to their opinions\nor preferences on a topic. For learning these properties, we propose a\nnonparametric, unsupervised, and scalable graph scan procedure that is, in\naddition, robust against a class of powerful adversaries. In our setup, one of\nthe communities can fall under the influence of a knowledgeable adversarial\nleader, who knows the full network structure, has unlimited computational\nresources and can completely foresee our planned actions on the network. We\nprove strong consistency of our results in this setup with minimal assumptions.\nIn particular, the learning procedure estimates the baseline activity of normal\nusers asymptotically correctly with probability 1; the only assumption being\nthe existence of a single implicit community of asymptotically negligible\nlogarithmic size. We provide experiments on real and synthetic data to\nillustrate the performance of our method, including examples with adversaries.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jul 2017 23:28:52 GMT"}, {"version": "v2", "created": "Mon, 23 Jul 2018 16:22:55 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Langovoy", "Mikhail A.", ""], ["Gotmare", "Akhilesh", ""], ["Jaggi", "Martin", ""]]}, {"id": "1707.03593", "submitter": "Olivier Bouaziz", "authors": "G Nuel (UPMC, LPMA), Antoine Lefebvre (UPMC, LPMA), O Bouaziz (MAP5)", "title": "Computing Individual Risks based on Family History in Genetic Disease in\n  the Presence of Competing Risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering a genetic disease with variable age at onset (ex: diabetes ,\nfamilial amyloid neuropathy, cancers, etc.), computing the individual risk of\nthe disease based on family history (FH) is of critical interest both for\nclinicians and patients. Such a risk is very challenging to compute because: 1)\nthe genotype X of the individual of interest is in general unknown; 2) the\nposterior distribution P(X|FH, T > t) changes with t (T is the age at disease\nonset for the targeted individual); 3) the competing risk of death is not\nnegligible. In this work, we present a modeling of this problem using a\nBayesian network mixed with (right-censored) survival outcomes where hazard\nrates only depend on the genotype of each individual. We explain how belief\npropagation can be used to obtain posterior distribution of genotypes given the\nFH, and how to obtain a time-dependent posterior hazard rate for any individual\nin the pedigree. Finally, we use this posterior hazard rate to compute\nindividual risk, with or without the competing risk of death. Our method is\nillustrated using the Claus-Easton model for breast cancer (BC). This model\nassumes an autosomal dominant genetic risk factor such as non-carriers\n(genotype 00) have a BC hazard rate $\\lambda$ 0 (t) while carriers (genotypes\n01, 10 and 11) have a (much greater) hazard rate $\\lambda$ 1 (t). Both hazard\nrates are assumed to be piecewise constant with known values (cuts at 20, 30,.\n.. , 80 years). The competing risk of death is derived from the national French\nregistry.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 08:23:48 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 08:19:02 GMT"}], "update_date": "2017-09-15", "authors_parsed": [["Nuel", "G", "", "UPMC, LPMA"], ["Lefebvre", "Antoine", "", "UPMC, LPMA"], ["Bouaziz", "O", "", "MAP5"]]}, {"id": "1707.03916", "submitter": "Evgeny Burnaev", "authors": "Evgeny Burnaev, Alexey Zaytsev", "title": "Large Scale Variable Fidelity Surrogate Modeling", "comments": "21 pages, 4 figures, Ann Math Artif Intell (2017)", "journal-ref": null, "doi": "10.1007/s10472-017-9545-y", "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineers widely use Gaussian process regression framework to construct\nsurrogate models aimed to replace computationally expensive physical models\nwhile exploring design space. Thanks to Gaussian process properties we can use\nboth samples generated by a high fidelity function (an expensive and accurate\nrepresentation of a physical phenomenon) and a low fidelity function (a cheap\nand coarse approximation of the same physical phenomenon) while constructing a\nsurrogate model. However, if samples sizes are more than few thousands of\npoints, computational costs of the Gaussian process regression become\nprohibitive both in case of learning and in case of prediction calculation. We\npropose two approaches to circumvent this computational burden: one approach is\nbased on the Nystr\\\"om approximation of sample covariance matrices and another\nis based on an intelligent usage of a blackbox that can evaluate a~low fidelity\nfunction on the fly at any point of a design space. We examine performance of\nthe proposed approaches using a number of artificial and real problems,\nincluding engineering optimization of a rotating disk shape.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jul 2017 21:21:32 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Burnaev", "Evgeny", ""], ["Zaytsev", "Alexey", ""]]}, {"id": "1707.03971", "submitter": "Cai Wu", "authors": "Cai Wu, Liang Li", "title": "Quantifying and Estimating the Predictive Accuracy for Censored\n  Time-to-Event Data with Competing Risks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on quantifying and estimating the predictive accuracy of\nprognostic models for time-to-event outcomes with competing events. We consider\nthe time-dependent discrimination and calibration metrics, including the\nreceiver operating characteristics curve and the Brier score, in the context of\ncompeting risks. To address censoring, we propose a unified nonparametric\nestimation framework for both discrimination and calibration measures, by\nweighting the censored subjects with the conditional probability of the event\nof interest given the observed data. We demonstrate through simulations that\nthe proposed estimator is unbiased, efficient and robust against model\nmisspecification in comparison to other methods published in the literature. In\naddition, the proposed method can be extended to time-dependent predictive\naccuracy metrics constructed from a general class of loss functions. We apply\nthe methodology to a data set from the African American Study of Kidney Disease\nand Hypertension to evaluate the predictive accuracy of a prognostic risk score\nin predicting end-stage renal disease (ESRD), accounting for the competing risk\nof pre-ESRD death.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 04:17:43 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Wu", "Cai", ""], ["Li", "Liang", ""]]}, {"id": "1707.04136", "submitter": "Zach Branson", "authors": "Zach Branson and Marie-Abele Bind", "title": "Randomization-based Inference for Bernoulli-Trial Experiments and\n  Implications for Observational Studies", "comments": "39 Pages, 4 Figures, 3 Tables", "journal-ref": null, "doi": "10.1177/0962280218756689", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a randomization-based inferential framework for experiments\ncharacterized by a strongly ignorable assignment mechanism where units have\nindependent probabilities of receiving treatment. Previous works on\nrandomization tests often assume these probabilities are equal within blocks of\nunits. We consider the general case where they differ across units and show how\nto perform randomization tests and obtain point estimates and confidence\nintervals. Furthermore, we develop a rejection-sampling algorithm to conduct\nrandomization-based inference conditional on ancillary statistics, covariate\nbalance, or other statistics of interest. Through simulation we demonstrate how\nour algorithm can yield powerful randomization tests and thus precise\ninference. Our work also has implications for observational studies, which\ncommonly assume a strongly ignorable assignment mechanism. Most methodologies\nfor observational studies make additional modeling or asymptotic assumptions,\nwhile our framework only assumes the strongly ignorable assignment mechanism,\nand thus can be considered a minimal-assumption approach.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 14:13:23 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 21:57:43 GMT"}], "update_date": "2019-02-01", "authors_parsed": [["Branson", "Zach", ""], ["Bind", "Marie-Abele", ""]]}, {"id": "1707.04141", "submitter": "Timoth\\'ee Tabouy", "authors": "Timoth\\'ee Tabouy, Pierre Barbillon and Julien Chiquet", "title": "Variational Inference for Stochastic Block Models from Sampled Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with non-observed dyads during the sampling of a network and\nconsecutive issues in the inference of the Stochastic Block Model (SBM). We\nreview sampling designs and recover Missing At Random (MAR) and Not Missing At\nRandom (NMAR) conditions for the SBM. We introduce variants of the variational\nEM algorithm for inferring the SBM under various sampling designs (MAR and\nNMAR) all available as an R package. Model selection criteria based on\nIntegrated Classification Likelihood are derived for selecting both the number\nof blocks and the sampling design. We investigate the accuracy and the range of\napplicability of these algorithms with simulations. We explore two real-world\nnetworks from ethnology (seed circulation network) and biology (protein-protein\ninteraction network), where the interpretations considerably depends on the\nsampling designs considered.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 14:23:36 GMT"}, {"version": "v2", "created": "Tue, 29 Aug 2017 12:12:15 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2018 06:30:49 GMT"}, {"version": "v4", "created": "Fri, 21 Dec 2018 15:37:03 GMT"}, {"version": "v5", "created": "Wed, 2 Jan 2019 18:53:35 GMT"}, {"version": "v6", "created": "Wed, 9 Jan 2019 09:37:09 GMT"}], "update_date": "2019-01-10", "authors_parsed": [["Tabouy", "Timoth\u00e9e", ""], ["Barbillon", "Pierre", ""], ["Chiquet", "Julien", ""]]}, {"id": "1707.04235", "submitter": "Susanne Ditlevsen", "authors": "Susanne Ditlevsen, Adeline Samson", "title": "Hypoelliptic diffusions: discretization, filtering and inference from\n  complete and partial observations", "comments": "36 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The statistical problem of parameter estimation in partially observed\nhypoelliptic diffusion processes is naturally occurring in many applications.\nHowever, due to the noise structure, where the noise components of the\ndifferent coordinates of the multi-dimensional process operate on different\ntime scales, standard inference tools are ill conditioned. In this paper, we\npropose to use a higher order scheme to approximate the likelihood, such that\nthe different time scales are appropriately accounted for. We show consistency\nand asymptotic normality with non-typical convergence rates. When only partial\nobservations are available, we embed the approximation into a filtering\nalgorithm for the unobserved coordinates, and use this as a building block in a\nStochastic Approximation Expectation Maximization algorithm. We illustrate on\nsimulated data from three models; the Harmonic Oscillator, the FitzHugh-Nagumo\nmodel used to model the membrane potential evolution in neuroscience, and the\nSynaptic Inhibition and Excitation model used for determination of neuronal\nsynaptic input.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 17:34:06 GMT"}, {"version": "v2", "created": "Sun, 11 Nov 2018 10:03:42 GMT"}], "update_date": "2018-11-13", "authors_parsed": [["Ditlevsen", "Susanne", ""], ["Samson", "Adeline", ""]]}, {"id": "1707.04246", "submitter": "Matthew Dunlop", "authors": "Daniela Calvetti, Matthew M. Dunlop, Erkki Somersalo, Andrew M. Stuart", "title": "Iterative Updating of Model Error for Bayesian Inversion", "comments": "39 pages, 9 figures", "journal-ref": null, "doi": "10.1088/1361-6420/aaa34d", "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational inverse problems, it is common that a detailed and accurate\nforward model is approximated by a computationally less challenging substitute.\nThe model reduction may be necessary to meet constraints in computing time when\noptimization algorithms are used to find a single estimate, or to speed up\nMarkov chain Monte Carlo (MCMC) calculations in the Bayesian framework. The use\nof an approximate model introduces a discrepancy, or modeling error, that may\nhave a detrimental effect on the solution of the ill-posed inverse problem, or\nit may severely distort the estimate of the posterior distribution. In the\nBayesian paradigm, the modeling error can be considered as a random variable,\nand by using an estimate of the probability distribution of the unknown, one\nmay estimate the probability distribution of the modeling error and incorporate\nit into the inversion. We introduce an algorithm which iterates this idea to\nupdate the distribution of the model error, leading to a sequence of posterior\ndistributions that are demonstrated empirically to capture the underlying truth\nwith increasing accuracy. Since the algorithm is not based on rejections, it\nrequires only limited full model evaluations.\n  We show analytically that, in the linear Gaussian case, the algorithm\nconverges geometrically fast with respect to the number of iterations. For more\ngeneral models, we introduce particle approximations of the iteratively\ngenerated sequence of distributions; we also prove that each element of the\nsequence converges in the large particle limit. We show numerically that, as in\nthe linear case, rapid convergence occurs with respect to the number of\niterations. Additionally, we show through computed examples that point\nestimates obtained from this iterative algorithm are superior to those obtained\nby neglecting the model error.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 17:54:56 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Calvetti", "Daniela", ""], ["Dunlop", "Matthew M.", ""], ["Somersalo", "Erkki", ""], ["Stuart", "Andrew M.", ""]]}, {"id": "1707.04301", "submitter": "Gery Geenens", "authors": "Gery Geenens", "title": "Mellin-Meijer-kernel density estimation on $\\mathbb{R}^+$", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric kernel density estimation is a very natural procedure which\nsimply makes use of the smoothing power of the convolution operation. Yet, it\nperforms poorly when the density of a positive variable is to be estimated\n(boundary issues, spurious bumps in the tail). So various extensions of the\nbasic kernel estimator allegedly suitable for $\\mathbb{R}^+$-supported\ndensities, such as those using Gamma or other asymmetric kernels, abound in the\nliterature. Those, however, are not based on any valid smoothing operation\nanalogous to the convolution, which typically leads to inconsistencies. By\ncontrast, in this paper a kernel estimator for $\\mathbb{R}^+$-supported\ndensities is defined by making use of the Mellin convolution, the natural\nanalogue of the usual convolution on $\\mathbb{R}^+$. From there, a very\ntransparent theory flows and leads to new type of asymmetric kernels strongly\nrelated to Meijer's $G$-functions. The numerous pleasant properties of this\n`Mellin-Meijer-kernel density estimator' are demonstrated in the paper. Its\npointwise and $L_2$-consistency (with optimal rate of convergence) is\nestablished for a large class of densities, including densities unbounded at 0\nand showing power-law decay in their right tail. Its practical behaviour is\ninvestigated further through simulations and some real data analyses.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:24:00 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Geenens", "Gery", ""]]}, {"id": "1707.04306", "submitter": "Leland Bybee", "authors": "Yves Atchade and Leland Bybee", "title": "A Scalable Algorithm for Gaussian Graphical Models with Change-Points", "comments": "39 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical models with change-points are computationally challenging to fit,\nparticularly in cases where the number of observation points and the number of\nnodes in the graph are large. Focusing on Gaussian graphical models, we\nintroduce an approximate majorize-minimize (MM) algorithm that can be useful\nfor computing change-points in large graphical models. The proposed algorithm\nis an order of magnitude faster than a brute force search. Under some\nregularity conditions on the data generating process, we show that with high\nprobability, the algorithm converges to a value that is within statistical\nerror of the true change-point. A fast implementation of the algorithm using\nMarkov Chain Monte Carlo is also introduced. The performances of the proposed\nalgorithms are evaluated on synthetic data sets and the algorithm is also used\nto analyze structural changes in the S&P 500 over the period 2000-2016.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jul 2017 20:31:13 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Atchade", "Yves", ""], ["Bybee", "Leland", ""]]}, {"id": "1707.04360", "submitter": "Xiongtao Dai", "authors": "Xiongtao Dai, Hans-Georg M\\\"uller, Wenwen Tao", "title": "Derivative Principal Component Analysis for Representing the Time\n  Dynamics of Longitudinal and Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric method to explicitly model and represent the\nderivatives of smooth underlying trajectories for longitudinal data. This\nrepresentation is based on a direct Karhunen--Lo\\`eve expansion of the\nunobserved derivatives and leads to the notion of derivative principal\ncomponent analysis, which complements functional principal component analysis,\none of the most popular tools of functional data analysis. The proposed\nderivative principal component scores can be obtained for irregularly spaced\nand sparsely observed longitudinal data, as typically encountered in biomedical\nstudies, as well as for functional data which are densely measured. Novel\nconsistency results and asymptotic convergence rates for the proposed estimates\nof the derivative principal component scores and other components of the model\nare derived under a unified scheme for sparse or dense observations and mild\nconditions. We compare the proposed representations for derivatives with\nalternative approaches in simulation settings and also in a wallaby growth\ncurve application. It emerges that representations using the proposed\nderivative principal component analysis recover the underlying derivatives more\naccurately compared to principal component analysis-based approaches especially\nin settings where the functional data are represented with only a very small\nnumber of components or are densely sampled. In a second wheat spectra\nclassification example, derivative principal component scores were found to be\nmore predictive for the protein content of wheat than the conventional\nfunctional principal component scores.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 00:45:45 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Dai", "Xiongtao", ""], ["M\u00fcller", "Hans-Georg", ""], ["Tao", "Wenwen", ""]]}, {"id": "1707.04372", "submitter": "Itai Dattner", "authors": "Rami Yaari, Itai Dattner, and Amit Huppert", "title": "A two-stage approach for estimating the parameters of an age-group\n  epidemic model from incidence data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Age-dependent dynamics is an important characteristic of many infectious\ndiseases. Age-group epidemic models describe the infection dynamics in\ndifferent age-groups by allowing to set distinct parameter values for each.\nHowever, such models are highly nonlinear and may have a large number of\nunknown parameters. Thus, parameter estimation of age-group models, while\nbecoming a fundamental issue for both the scientific study and policy making in\ninfectious diseases, is not a trivial task in practice. In this paper, we\nexamine the estimation of the so called next-generation matrix using incidence\ndata of a single entire outbreak, and extend the approach to deal with\nrecurring outbreaks. Unlike previous studies, we do not assume any constraints\nregarding the structure of the matrix. A novel two-stage approach is developed,\nwhich allows for efficient parameter estimation from both statistical and\ncomputational perspectives. Simulation studies corroborate the ability to\nestimate accurately the parameters of the model for several realistic\nscenarios. The model and estimation method are applied to real data of\ninfluenza-like-illness in Israel. The parameter estimates of the key relevant\nepidemiological parameters and the recovered structure of the estimated\nnext-generation matrix are in line with results obtained in previous studies.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 02:39:44 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Yaari", "Rami", ""], ["Dattner", "Itai", ""], ["Huppert", "Amit", ""]]}, {"id": "1707.04400", "submitter": "Antonio Punzo", "authors": "Antonio Punzo", "title": "A new look at the inverse Gaussian distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inverse Gaussian (IG) is one of the most famous and considered\ndistributions with positive support. We propose a convenient mode-based\nparameterization yielding the reparametrized IG (rIG) distribution; it\nallows/simplifies the use of the IG distribution in various statistical fields,\nand we give some examples in nonparametric statistics, robust statistics, and\nmodel-based clustering. In nonparametric statistics, we define a smoother based\non rIG kernels. By construction, the estimator is well-defined and free of\nboundary bias. We adopt likelihood cross-validation to select the smoothing\nparameter. In robust statistics, we propose the contaminated IG distribution, a\nheavy-tailed generalization of the rIG distribution to accommodate mild\noutliers; they can be automatically detected by the model via maximum a\nposteriori probabilities. To obtain maximum likelihood estimates of the\nparameters, we illustrate an expectation-maximization (EM) algorithm. Finally,\nfor model-based clustering and semiparametric density estimation, we present\nfinite mixtures of rIG distributions. We use the EM algorithm to obtain ML\nestimates of the parameters of the mixture model. Applications to economic and\ninsurance data are finally illustrated to exemplify and enhance the use of the\nproposed models.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 07:26:09 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Punzo", "Antonio", ""]]}, {"id": "1707.04405", "submitter": "Takuya Ishihara", "authors": "Takuya Ishihara", "title": "Partial Identification of Nonseparable Models using Binary Instruments", "comments": null, "journal-ref": null, "doi": "10.1017/S0266466620000353", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we explore the partial identification of nonseparable models\nwith continuous endogenous and binary instrumental variables. We show that the\nstructural function is partially identified when it is monotone or concave in\nthe explanatory variable. D'Haultfoeuille and Fevrier (2015) and Torgovitsky\n(2015) prove the point identification of the structural function under a key\nassumption that the conditional distribution functions of the endogenous\nvariable for different values of the instrumental variables have intersections.\nWe demonstrate that, even if this assumption does not hold, monotonicity and\nconcavity provide identifying power. Point identification is achieved when the\nstructural function is flat or linear with respect to the explanatory variable\nover a given interval. We compute the bounds using real data and show that our\nbounds are informative.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 07:35:34 GMT"}, {"version": "v2", "created": "Thu, 12 Oct 2017 01:13:58 GMT"}, {"version": "v3", "created": "Wed, 12 Jun 2019 07:15:15 GMT"}, {"version": "v4", "created": "Fri, 29 May 2020 12:43:20 GMT"}], "update_date": "2020-11-04", "authors_parsed": [["Ishihara", "Takuya", ""]]}, {"id": "1707.04434", "submitter": "Ana Ferreira", "authors": "Ana Ferreira, Petra Friederichs, Laurens de Haan, Cl\\'audia Neves,\n  Martin Schlather", "title": "Estimating space-time trend and dependence of heavy rainfall", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach for evaluating time-trends in extreme values accounting also\nfor spatial dependence is proposed. Based on exceedances over a space-time\nthreshold, estimators for a trend function and for extreme value parameters are\ngiven, leading to a homogenization procedure for then applying stationary\nextreme value processes. Extremal dependence over space is further evaluated\nthrough variogram analysis including anisotropy. We detect significant\ninhomogeneities and trends in the extremal behaviour of daily precipitation\ndata over a time period of 84 years and from 68 observational weather stations\nin North-West Germany. We observe that the trend is not monotonous over time in\ngeneral.\n  Asymptotic normality of the estimators under maximum domain of attraction\nconditions are proven.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 09:43:12 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Ferreira", "Ana", ""], ["Friederichs", "Petra", ""], ["de Haan", "Laurens", ""], ["Neves", "Cl\u00e1udia", ""], ["Schlather", "Martin", ""]]}, {"id": "1707.04464", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey, Debasis Kundu and Tumati Kiran Kumar", "title": "Hierarchical EM algorithm for estimating the parameters of Mixture of\n  Bivariate Generalized Exponential distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a mixture modeling framework using the bivariate\ngeneralized exponential distribution. We study different properties of this\nmixture distribution. Hierarchical EM algorithm is developed for finding the\nestimates of the parameters. The algorithm takes very large sample size to work\nas it contains many stages of approximation. Numerical Results are provided for\nmore illustration.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 11:28:41 GMT"}, {"version": "v2", "created": "Sun, 1 Apr 2018 05:27:04 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["Kundu", "Debasis", ""], ["Kumar", "Tumati Kiran", ""]]}, {"id": "1707.04465", "submitter": "Jonathan Bartlett", "authors": "Jonathan W. Bartlett", "title": "Covariate adjustment and prediction of mean response in randomised\n  trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyses of randomised trials are often based on regression models which\nadjust for baseline covariates, in addition to randomised group. Based on such\nmodels, one can obtain estimates of the marginal mean outcome for the\npopulation under assignment to each treatment, by averaging the model based\npredictions across the empirical distribution of the baseline covariates in the\ntrial. We identify under what conditions such estimates are consistent, and in\nparticular show that for canonical generalised linear models, the resulting\nestimates are always consistent. We show that a recently proposed variance\nestimator underestimates the true variance when the baseline covariates are not\nfixed in repeated sampling, and provide a simple adjustment to remedy this. We\nalso describe an alternative semiparametric estimator which is consistent even\nwhen the outcome regression model used is misspecified. The different\nestimators are compared through simulations and application to a recently\nconducted trial in asthma.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 11:28:54 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Bartlett", "Jonathan W.", ""]]}, {"id": "1707.04485", "submitter": "Fabian Schroeder", "authors": "Fabian Schroeder", "title": "On an Exact and Nonparametric Test for the Separability of Two Classes\n  by Means of a Simple Threshold", "comments": "17 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a statistical test inferring whether a variable allows\nseparating two classes by means of a single critical value. Its test statistic\nis the prediction error of a nonparametric threshold classifier. While this\napproach is adequate for univariate classification tasks, it is especially\nadvantageous for filter-type variable selection. It constitutes a robust and\nnonparametric method which may identify important otherwise neglected\nvariables. It can incorporate the operating conditions of the classification\ntask. Last but not least, the exact finite sample distribution of the test\nstatistic under the null hypothesis can be calculated using a fast recursive\nalgorithm.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 12:35:04 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Schroeder", "Fabian", ""]]}, {"id": "1707.04602", "submitter": "Cheng Huang", "authors": "Cheng Huang and Xiaoming Huo", "title": "An Efficient and Distribution-Free Two-Sample Test Based on Energy\n  Statistics and Random Projections", "comments": "27 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common disadvantage in existing distribution-free two-sample testing\napproaches is that the computational complexity could be high. Specifically, if\nthe sample size is $N$, the computational complexity of those two-sample tests\nis at least $O(N^2)$. In this paper, we develop an efficient algorithm with\ncomplexity $O(N \\log N)$ for computing energy statistics in univariate cases.\nFor multivariate cases, we introduce a two-sample test based on energy\nstatistics and random projections, which enjoys the $O(K N \\log N)$\ncomputational complexity, where $K$ is the number of random projections. We\nname our method for multivariate cases as Randomly Projected Energy Statistics\n(RPES). We can show RPES achieves nearly the same test power with energy\nstatistics both theoretically and empirically. Numerical experiments also\ndemonstrate the efficiency of the proposed method over the competitors.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 18:38:41 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Huang", "Cheng", ""], ["Huo", "Xiaoming", ""]]}, {"id": "1707.04635", "submitter": "Johannes Bracher", "authors": "Johannes Bracher and Leonhard Held", "title": "Periodically stationary multivariate autoregressive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of multivariate periodic autoregressive models is proposed where\ncoupling between time series is achieved through linear mean functions. Various\nresponse distributions with quadratic mean-variance relationships fit into the\nframework, including the negative binomial, gamma and Gaussian distributions.\nWe develop an iterative algorithm to obtain unconditional means, variances and\nauto-/cross-covariances for models with higher order lags. Analytical solutions\nare given for the univariate model with lag one and multivariate models with\nlinear mean-variance relationship. A special case of the model class is an\nestablished framework for modelling multivariate time series of counts from\nroutine surveillance of infectious diseases. We extend this model class to\nallow for distributed lags and apply it to a dataset on norovirus\ngastroenteritis in two German states. The availability of unconditional moments\nand auto/cross-correlations enhances model assessment and interpretation.\n", "versions": [{"version": "v1", "created": "Fri, 14 Jul 2017 20:54:02 GMT"}, {"version": "v2", "created": "Fri, 15 Dec 2017 10:21:25 GMT"}], "update_date": "2017-12-18", "authors_parsed": [["Bracher", "Johannes", ""], ["Held", "Leonhard", ""]]}, {"id": "1707.04721", "submitter": "Ashwin Seshadri", "authors": "Ashwin K Seshadri", "title": "Statistics of spatial averages and optimal averaging in the presence of\n  missing data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistics of spatial averages estimated by weighting\nobservations over an arbitrary spatial domain using identical and independent\nmeasuring devices, and derive an account of bias and variance in the presence\nof missing observations. We test the model relative to simulations, and the\napproximations for bias and variance with missing data are shown to compare\nwell even when the probability of missing data is large. Previous authors have\nexamined optimal averaging strategies for minimizing bias, variance and mean\nsquared error of the spatial average, and we extend the analysis to the case of\nmissing observations. Minimizing variance mainly requires higher weights where\nlocal variance and covariance is small, whereas minimizing bias requires higher\nweights where the field is closer to the true spatial average. Missing data\nincreases variance and contributes to bias, and reducing both effects involves\nemphasizing locations with mean value nearer to the spatial average. The\nframework is applied to study spatially averaged rainfall over India. We use\nour model to estimate standard error in all-India rainfall as the combined\neffect of measurement uncertainty and bias, when weights are chosen so as to\nyield minimum mean squared error.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 10:11:59 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 10:43:32 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Seshadri", "Ashwin K", ""]]}, {"id": "1707.04723", "submitter": "Martin Ehler", "authors": "Martin Ehler and Manuel Graef and Chris. J. Oates", "title": "Optimal Monte Carlo integration on closed manifolds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The worst case integration error in reproducing kernel Hilbert spaces of\nstandard Monte Carlo methods with n random points decays as $n^{-1/2}$.\nHowever, re-weighting of random points can sometimes be used to improve the\nconvergence order. This paper contributes general theoretical results for\nSobolev spaces on closed Riemannian manifolds, where we verify that such\nre-weighting yields optimal approximation rates up to a logarithmic factor. We\nalso provide numerical experiments matching the theoretical results for some\nSobolev spaces on the unit sphere and on the Grassmannian manifold. Our\ntheoretical findings also cover function spaces on more general sets such as\nthe unit ball, the cube, and the simplex.\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 10:48:35 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 07:41:52 GMT"}, {"version": "v3", "created": "Thu, 28 Sep 2017 12:32:07 GMT"}, {"version": "v4", "created": "Fri, 29 Sep 2017 18:41:15 GMT"}, {"version": "v5", "created": "Wed, 24 Jan 2018 17:08:45 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Ehler", "Martin", ""], ["Graef", "Manuel", ""], ["Oates", "Chris. J.", ""]]}, {"id": "1707.04800", "submitter": "Jonathan Stewart", "authors": "Michael Schweinberger, Pavel N. Krivitsky, Carter T. Butts, Jonathan\n  Stewart", "title": "Exponential-Family Models of Random Graphs: Inference in Finite-,\n  Super-, and Infinite Population Scenarios", "comments": null, "journal-ref": "Statistical Science 35(4): 627 - 662 (2020)", "doi": "10.1214/19-STS743", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential-family Random Graph Models (ERGMs) constitute a large statistical\nframework for modeling sparse and dense random graphs, short- and long-tailed\ndegree distributions, covariates, and a wide range of complex dependencies.\nSpecial cases of ERGMs are generalized linear models (GLMs), Bernoulli random\ngraphs, $\\beta$-models, $p_1$-models, and models related to Markov random\nfields in spatial statistics and other areas of statistics. While widely used\nin practice, questions have been raised about the theoretical properties of\nERGMs. These include concerns that some ERGMs are near-degenerate and that many\nERGMs are non-projective. To address them, careful attention must be paid to\nmodel specifications and their underlying assumptions, and in which inferential\nsettings models are employed. As we discuss, near-degeneracy can affect\nsimplistic ERGMs lacking structure, but well-posed ERGMs with additional\nstructure can be well-behaved. Likewise, lack of projectivity can affect\nnon-likelihood-based inference, but likelihood-based inference does not require\nprojectivity. Here, we review well-posed ERGMs along with likelihood-based\ninference. We first clarify the core statistical notions of \"sample\" and\n\"population\" in the ERGM framework, and separate the process that generates the\npopulation graph from the observation process. We then review likelihood-based\ninference in finite-, super-, and infinite-population scenarios. We conclude\nwith consistency results, and an application to human brain networks\n", "versions": [{"version": "v1", "created": "Sat, 15 Jul 2017 23:44:12 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 22:39:39 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 18:17:14 GMT"}, {"version": "v4", "created": "Thu, 12 Sep 2019 19:07:32 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Schweinberger", "Michael", ""], ["Krivitsky", "Pavel N.", ""], ["Butts", "Carter T.", ""], ["Stewart", "Jonathan", ""]]}, {"id": "1707.04928", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Ali Shojaie, Eric Shea-Brown, and Daniela Witten", "title": "The Multivariate Hawkes Process in High Dimensions: Beyond Mutual\n  Excitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Hawkes process is a class of point processes whose future depends on\ntheir own history. Previous theoretical work on the Hawkes process is limited\nto a special case in which a past event can only increase the occurrence of\nfuture events, and the link function is linear. However, in neuronal networks\nand other real-world applications, inhibitory relationships may be present, and\nthe link function may be non-linear. In this paper, we develop a new approach\nfor investigating the properties of the Hawkes process without the restriction\nto mutual excitation or linear link functions. To this end, we employ a\nthinning process representation and a coupling construction to bound the\ndependence coefficient of the Hawkes process. Using recent developments on\nweakly dependent sequences, we establish a concentration inequality for\nsecond-order statistics of the Hawkes process. We apply this concentration\ninequality to cross-covariance analysis in the high-dimensional regime, and we\nverify the theoretical claims with simulation studies.\n", "versions": [{"version": "v1", "created": "Sun, 16 Jul 2017 18:51:48 GMT"}, {"version": "v2", "created": "Wed, 19 Jun 2019 18:08:29 GMT"}], "update_date": "2019-06-21", "authors_parsed": [["Chen", "Shizhe", ""], ["Shojaie", "Ali", ""], ["Shea-Brown", "Eric", ""], ["Witten", "Daniela", ""]]}, {"id": "1707.04998", "submitter": "Kattumannil Sudheesh Dr", "authors": "Sreelakshmi N, Sudheesh K Kattumannil and Rituparna Sen", "title": "Jackknife Empirical Likelihood-based inference for S-Gini indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widely used income inequality measure, Gini index is extended to form a\nfamily of income inequality measures known as Single-Series Gini (S-Gini)\nindices. In this study, we develop empirical likelihood (EL) and jackknife\nempirical likelihood (JEL) based inference for S-Gini indices. We prove that\nthe limiting distribution of both EL and JEL ratio statistics are Chi-square\ndistribution with one degree of freedom. Using the asymptotic distribution we\nconstruct EL and JEL based confidence intervals for realtive S-Gini indices. We\nalso give bootstrap-t and bootstrap calibrated empirical likelihood confidence\nintervals for S-Gini indices. A numerical study is carried out to compare the\nperformances of the proposed confidence interval with the bootstrap methods. A\ntest for S-Gini indices based on jackknife empirical likelihood ratio is also\nproposed. Finally we illustrate the proposed method using an income data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 04:17:07 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 05:56:05 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["N", "Sreelakshmi", ""], ["Kattumannil", "Sudheesh K", ""], ["Sen", "Rituparna", ""]]}, {"id": "1707.05034", "submitter": "Yifan Cui", "authors": "Yifan Cui, Jan Hannig", "title": "Nonparametric generalized fiducial inference for survival functions\n  under censoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fiducial Inference, introduced by Fisher in the 1930s, has a long history,\nwhich at times aroused passionate disagreements. However, its application has\nbeen largely confined to relatively simple parametric problems. In this paper,\nwe present what might be the first time fiducial inference, as generalized by\nHannig et al. (2016), is systematically applied to estimation of a\nnonparametric survival function under right censoring. We find that the\nresulting fiducial distribution gives rise to surprisingly good statistical\nprocedures applicable to both one sample and two sample problems. In\nparticular, we use the fiducial distribution of a survival function to\nconstruct pointwise and curvewise confidence intervals for the survival\nfunction, and propose tests based on the curvewise confidence interval. We\nestablish a functional Bernstein-von Mises theorem, and perform thorough\nsimulation studies in scenarios with different levels of censoring. The\nproposed fiducial based confidence intervals maintain coverage in situations\nwhere asymptotic methods often have substantial coverage problems. Furthermore,\nthe average length of the proposed confidence intervals is often shorter than\nthe length of competing methods that maintain coverage. Finally, the proposed\nfiducial test is more powerful than various types of log-rank tests and sup\nlog-rank tests in some scenarios. We illustrate the proposed fiducial test\ncomparing chemotherapy against chemotherapy combined with radiotherapy using\ndata from the treatment of locally unresectable gastric cancer.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 08:10:39 GMT"}, {"version": "v2", "created": "Fri, 24 Nov 2017 07:22:39 GMT"}, {"version": "v3", "created": "Sat, 24 Mar 2018 20:18:37 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Cui", "Yifan", ""], ["Hannig", "Jan", ""]]}, {"id": "1707.05200", "submitter": "Chris Sherlock Dr.", "authors": "Chris Sherlock and Alexandre H. Thiery", "title": "A Discrete Bouncy Particle Sampler", "comments": "New theory for anisotropic targets", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most Markov chain Monte Carlo methods operate in discrete time and are\nreversible with respect to the target probability. Nevertheless, it is now\nunderstood that the use of non-reversible Markov chains can be beneficial in\nmany contexts. In particular, the recently-proposed Bouncy Particle Sampler\nleverages a continuous-time and non-reversible Markov process and empirically\nshows state-of-the-art performances when used to explore certain probability\ndensities; however, its implementation typically requires the computation of\nlocal upper bounds on the gradient of the log target density.\n  We present the Discrete Bouncy Particle Sampler, a general algorithm based\nupon a guided random walk, a partial refreshment of direction, and a\ndelayed-rejection step. We show that the Bouncy Particle Sampler can be\nunderstood as a scaling limit of a special case of our algorithm. In contrast\nto the Bouncy Particle Sampler, implementing the Discrete Bouncy Particle\nSampler only requires point-wise evaluation of the target density and its\ngradient. We propose extensions of the basic algorithm for situations when the\nexact gradient of the target density is not available. In a Gaussian setting,\nwe establish a scaling limit for the radial process as dimension increases to\ninfinity. We leverage this result to obtain the theoretical efficiency of the\nDiscrete Bouncy Particle Sampler as a function of the partial-refreshment\nparameter, which leads to a simple and robust tuning criterion. A further\nanalysis in a more general setting suggests that this tuning criterion applies\nmore generally. Theoretical and empirical efficiency curves are then compared\nfor different targets and algorithm variations.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 14:57:50 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 16:12:33 GMT"}, {"version": "v3", "created": "Thu, 18 Jul 2019 12:04:02 GMT"}, {"version": "v4", "created": "Mon, 22 Feb 2021 14:33:01 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Sherlock", "Chris", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "1707.05296", "submitter": "Paul Vanetti", "authors": "Paul Vanetti, Alexandre Bouchard-C\\^ot\\'e, George Deligiannidis and\n  Arnaud Doucet", "title": "Piecewise-Deterministic Markov Chain Monte Carlo", "comments": "42 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel class of non-reversible Markov chain Monte Carlo schemes relying on\ncontinuous-time piecewise-deterministic Markov Processes has recently emerged.\nIn these algorithms, the state of the Markov process evolves according to a\ndeterministic dynamics which is modified using a Markov transition kernel at\nrandom event times. These methods enjoy remarkable features including the\nability to update only a subset of the state components while other components\nimplicitly keep evolving and the ability to use an unbiased estimate of the\ngradient of the log-target while preserving the target as invariant\ndistribution. However, they also suffer from important limitations. The\ndeterministic dynamics used so far do not exploit the structure of the target.\nMoreover, exact simulation of the event times is feasible for an important yet\nrestricted class of problems and, even when it is, it is application specific.\nThis limits the applicability of these techniques and prevents the development\nof a generic software implementation of them. We introduce novel MCMC methods\naddressing these shortcomings. In particular, we introduce novel\ncontinuous-time algorithms relying on exact Hamiltonian flows and novel\nnon-reversible discrete-time algorithms which can exploit complex dynamics such\nas approximate Hamiltonian dynamics arising from symplectic integrators while\npreserving the attractive features of continuous-time algorithms. We\ndemonstrate the performance of these schemes on a variety of applications.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 17:49:19 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 16:56:47 GMT"}], "update_date": "2018-05-16", "authors_parsed": [["Vanetti", "Paul", ""], ["Bouchard-C\u00f4t\u00e9", "Alexandre", ""], ["Deligiannidis", "George", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1707.05360", "submitter": "Paul von Hippel", "authors": "Paul T. von Hippel", "title": "Should a Normal Imputation Model Be Modified to Impute Skewed Variables?", "comments": "38 pages, 6 figures, 2 Tables + 2 Appendix Tables", "journal-ref": "Sociological Methods and Research, 2013, 42(1), 105-138", "doi": "10.1177/0049124112464866", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers often impute continuous variables under an assumption of\nnormality, yet many incomplete variables are skewed. We find that imputing\nskewed continuous variables under a normal model can lead to bias; the bias is\nusually mild for popular estimands such as means, standard deviations, and\nlinear regression coefficients, but the bias can be severe for more\nshape-dependent estimands such as percentiles or the coefficient of skewness.\nWe test several methods for adapting a normal imputation model to accommodate\nskewness, including methods that transform, truncate, or censor (round)\nnormally imputed values, as well as methods that impute values from a quadratic\nor truncated regression. None of these modifications reliably reduces the\nbiases of the normal model, and some modifications can make the biases much\nworse. We conclude that, if one has to impute a skewed variable under a normal\nmodel, it is usually safest to do so without modifications -- unless you are\nmore interested in estimating percentiles and shape that in estimated means,\nvariance, and regressions. In the conclusion, we briefly discuss promising\ndevelopments in the area of continuous imputation models that do not assume\nnormality.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jul 2017 18:39:29 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["von Hippel", "Paul T.", ""]]}, {"id": "1707.05807", "submitter": "Ioannis Mitliagkas", "authors": "Ioannis Mitliagkas and Lester Mackey", "title": "Improving Gibbs Sampler Scan Quality with DoGS", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pairwise influence matrix of Dobrushin has long been used as an\nanalytical tool to bound the rate of convergence of Gibbs sampling. In this\nwork, we use Dobrushin influence as the basis of a practical tool to certify\nand efficiently improve the quality of a discrete Gibbs sampler. Our\nDobrushin-optimized Gibbs samplers (DoGS) offer customized variable selection\norders for a given sampling budget and variable subset of interest, explicit\nbounds on total variation distance to stationarity, and certifiable\nimprovements over the standard systematic and uniform random scan Gibbs\nsamplers. In our experiments with joint image segmentation and object\nrecognition, Markov chain Monte Carlo maximum likelihood estimation, and Ising\nmodel inference, DoGS consistently deliver higher-quality inferences with\nsignificantly smaller sampling budgets than standard Gibbs samplers.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 18:17:55 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Mitliagkas", "Ioannis", ""], ["Mackey", "Lester", ""]]}, {"id": "1707.05825", "submitter": "Jenkin Tsui", "authors": "Jenkin Tsui, Abel Dasylva, Kenneth Chu", "title": "Optimal Estimating Equation for Logistic Regression with Linked Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an optimal estimating equation for logistic regression with linked\ndata while accounting for false positives. It builds on a previous solution but\nestimates the regression coefficients with a smaller variance, in large\nsamples.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 19:19:15 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 17:49:24 GMT"}, {"version": "v3", "created": "Tue, 15 Aug 2017 01:22:55 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Tsui", "Jenkin", ""], ["Dasylva", "Abel", ""], ["Chu", "Kenneth", ""]]}, {"id": "1707.05833", "submitter": "Sarah Fletcher Mercaldo", "authors": "Sarah Fletcher Mercaldo and Jeffrey D. Blume", "title": "Bagged Empirical Null p-values: A Method to Account for Model\n  Uncertainty in Large Scale Inference", "comments": "27 Pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When conducting large scale inference, such as genome-wide association\nstudies or image analysis, nominal $p$-values are often adjusted to improve\ncontrol over the family-wise error rate (FWER). When the majority of tests are\nnull, procedures controlling the False discovery rate (Fdr) can be improved by\nreplacing the theoretical global null with its empirical estimate. However,\nthese other adjustment procedures remain sensitive to the working model\nassumption. Here we propose two key ideas to improve inference in this space.\nFirst, we propose $p$-values that are standardized to the empirical null\ndistribution (instead of the theoretical null). Second, we propose model\naveraging $p$-values by bootstrap aggregation (Bagging) to account for model\nuncertainty and selection procedures. The combination of these two key ideas\nyields bagged empirical null $p$-values (BEN $p$-values) that often\ndramatically alter the rank ordering of significant findings. Moreover, we find\nthat a multidimensional selection criteria based on BEN $p$-values and bagged\nmodel fit statistics is more likely to yield reproducible findings. A\nre-analysis of the famous Golub Leukemia data is presented to illustrate these\nideas. We uncovered new findings in these data, not detected previously, that\nare backed by published bench work pre-dating the Gloub experiment. A\npseudo-simulation using the leukemia data is also presented to explore the\nstability of this approach under broader conditions, and illustrates the\nsuperiority of the BEN $p$-values compared to the other approaches.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 19:31:15 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Mercaldo", "Sarah Fletcher", ""], ["Blume", "Jeffrey D.", ""]]}, {"id": "1707.05835", "submitter": "Fan Li", "authors": "Jing Dong, Junni L Zhang, Fan Li", "title": "Subgroup Balancing Propensity Score", "comments": "30 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the estimation of subgroup treatment effects with\nobservational data. Existing propensity score matching and weighting methods\nare mostly developed for estimating overall treatment effect. Although the true\npropensity score should balance covariates for the subgroup populations, the\nestimated propensity score may not balance covariates for the subgroup samples.\nWe propose the subgroup balancing propensity score (SBPS) method, which\nselects, for each subgroup, to use either the overall sample or the subgroup\nsample to estimate propensity scores for units within that subgroup, in order\nto optimize a criterion accounting for a set of covariate-balancing conditions\nfor both the overall sample and the subgroup samples. We develop a stochastic\nsearch algorithm for the estimation of SBPS when the number of subgroups is\nlarge. We demonstrate through simulations that the SBPS can improve the\nperformance of propensity score matching in estimating subgroup treatment\neffects. We then apply the SBPS method to data from the Italy Survey of\nHousehold Income and Wealth (SHIW) to estimate the treatment effects of having\ndebit card on household consumption for different income groups.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 19:33:45 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Dong", "Jing", ""], ["Zhang", "Junni L", ""], ["Li", "Fan", ""]]}, {"id": "1707.05861", "submitter": "Cheng Ju", "authors": "Cheng Ju, Joshua Schwab, Mark J. van der Laan", "title": "On Adaptive Propensity Score Truncation in Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The positivity assumption, or the experimental treatment assignment (ETA)\nassumption, is important for identifiability in causal inference. Even if the\npositivity assumption holds, practical violations of this assumption may\njeopardize the finite sample performance of the causal estimator. One of the\nconsequences of practical violations of the positivity assumption is extreme\nvalues in the estimated propensity score (PS). A common practice to address\nthis issue is truncating the PS estimate when constructing PS-based estimators.\nIn this study, we propose a novel adaptive truncation method,\nPositivity-C-TMLE, based on the collaborative targeted maximum likelihood\nestimation (C-TMLE) methodology. We demonstrate the outstanding performance of\nour novel approach in a variety of simulations by comparing it with other\ncommonly studied estimators. Results show that by adaptively truncating the\nestimated PS with a more targeted objective function, the Positivity-C-TMLE\nestimator achieves the best performance for both point estimation and\nconfidence interval coverage among all estimators considered.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 21:17:55 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Ju", "Cheng", ""], ["Schwab", "Joshua", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1707.05884", "submitter": "Olga Morozova", "authors": "Olga Morozova, Ted Cohen and Forrest W. Crawford", "title": "Risk ratios for contagious outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risk ratio is a popular tool for summarizing the relationship between a\nbinary covariate and outcome, even when outcomes may be dependent.\nInvestigations of infectious disease outcomes in cohort studies of individuals\nembedded within clusters -- households, villages, or small groups -- often\nreport risk ratios. Epidemiologists have warned that risk ratios may be\nmisleading when outcomes are contagious, but the nature and severity of this\nerror is not well understood. In this study, we assess the epidemiologic\nmeaning of the risk ratio when outcomes are contagious. We first give a\nstructural definition of infectious disease transmission within clusters, based\non the canonical susceptible-infective epidemic model. From this standard\ncharacterization, we define the individual-level ratio of instantaneous risks\n(hazard ratio) as the inferential target, and evaluate the properties of the\nrisk ratio as an estimate of this quantity. We exhibit analytically and by\nsimulation the circumstances under which the risk ratio implies an effect whose\ndirection is opposite that of the true individual-level hazard ratio. In\nparticular, the risk ratio can be greater than one even when the covariate of\ninterest reduces both individual-level susceptibility to infection, and\ntransmissibility once infected. We explain these findings in the epidemiologic\nlanguage of confounding and relate the direction bias to Simpson's paradox.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jul 2017 22:37:31 GMT"}], "update_date": "2017-07-20", "authors_parsed": [["Morozova", "Olga", ""], ["Cohen", "Ted", ""], ["Crawford", "Forrest W.", ""]]}, {"id": "1707.05916", "submitter": "Olanrewaju Akande", "authors": "Olanrewaju Akande, Jerome Reiter and Andr\\'es F. Barrientos", "title": "Multiple Imputation of Missing Values in Household Data with Structural\n  Zeros", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for imputation of missing items in multivariate\ncategorical data nested within households. The approach relies on a latent\nclass model that (i) allows for household level and individual level variables,\n(ii) ensures that impossible household configurations have zero probability in\nthe model, and (iii) can preserve multivariate distributions both within\nhouseholds and across households. We present a Gibbs sampler for estimating the\nmodel and generating imputations. We also describe strategies for improving the\ncomputational efficiency of the model estimation. We illustrate the performance\nof the approach with data that mimic the variables collected in typical\npopulation censuses.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 01:55:14 GMT"}, {"version": "v2", "created": "Wed, 4 Jul 2018 15:11:07 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Akande", "Olanrewaju", ""], ["Reiter", "Jerome", ""], ["Barrientos", "Andr\u00e9s F.", ""]]}, {"id": "1707.05917", "submitter": "Henry Lam", "authors": "Henry Lam, Huajie Qian", "title": "Optimization-based Quantification of Simulation Input Uncertainty via\n  Empirical Likelihood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an optimization-based approach to construct statistically accurate\nconfidence intervals for simulation performance measures under nonparametric\ninput uncertainty. This approach computes confidence bounds from simulation\nruns driven by probability weights defined on the data, which are obtained from\nsolving optimization problems under suitably posited averaged divergence\nconstraints. We illustrate how this approach offers benefits in computational\nefficiency and finite-sample performance compared to the bootstrap and the\ndelta method. While resembling robust optimization, we explain the procedural\ndesign and develop tight statistical guarantees of this approach via a\ngeneralization of the empirical likelihood method.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 01:58:59 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 00:00:21 GMT"}], "update_date": "2019-02-14", "authors_parsed": [["Lam", "Henry", ""], ["Qian", "Huajie", ""]]}, {"id": "1707.06318", "submitter": "Hyeon-Ah Kang", "authors": "Hyeon-Ah Kang, Jingchen Liu, Zhiliang Ying", "title": "A Graphical Diagnostic Classification Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework is presented to model instances and degrees of local item\ndependence within the context of diagnostic classification models (DCMs). The\nstudy considers an undirected graphical model to describe dependent structure\nof test items and draws inference based on pseudo-likelihood. The new modeling\nframework explicitly addresses item interactions beyond those explained by\nlatent classes and thus is more flexible and robust against the violation of\nlocal independence. It also facilitates concise interpretation of item\nrelations by regulating complexity of a network underlying the test items. The\nviability and effectiveness are demonstrated via simulation and a real data\nexample. Results from the simulation study suggest that the proposed methods\nadequately recover the model parameters in the presence of locally dependent\nitems and lead to a substantial improvement in estimation accuracy compared to\nthe standard DCM approach. The analysis of real data demonstrates that the\ngraphical DCM provides a useful summary of item interactions in regards to the\nexistence and extent of local dependence.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 22:54:41 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Kang", "Hyeon-Ah", ""], ["Liu", "Jingchen", ""], ["Ying", "Zhiliang", ""]]}, {"id": "1707.06476", "submitter": "Elie Wolfe", "authors": "Miguel Navascues and Elie Wolfe", "title": "The Inflation Technique Completely Solves the Causal Compatibility\n  Problem", "comments": "Updated to match forthcoming journal publication as closely as\n  possible. Some content removed for brevity. Expanded citations. Most\n  footnotes moved into the main text. Significant changes to subsection 4.1,\n  where we corrected an error in the example of second order inflation not\n  converging, and added an converse example where second order inflation\n  outperforms other techniques", "journal-ref": null, "doi": "10.1515/jci-2018-0008", "report-no": null, "categories": "quant-ph math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal compatibility question asks whether a given causal structure graph\n-- possibly involving latent variables -- constitutes a genuinely plausible\ncausal explanation for a given probability distribution over the graph's\nobserved variables. Algorithms predicated on merely necessary constraints for\ncausal compatibility typically suffer from false negatives, i.e. they admit\nincompatible distributions as apparently compatible with the given graph. In\n[arXiv:1609.00672], one of us introduced the inflation technique for\nformulating useful relaxations of the causal compatibility problem in terms of\nlinear programming. In this work, we develop a formal hierarchy of such causal\ncompatibility relaxations. We prove that inflation is asymptotically tight,\ni.e., that the hierarchy converges to a zero-error test for causal\ncompatibility. In this sense, the inflation technique fulfills a longstanding\ndesideratum in the field of causal inference. We quantify the rate of\nconvergence by showing that any distribution which passes the $n^{th}$-order\ninflation test must be $O\\left(n^{-1/2}\\right)$-close in Euclidean norm to some\ndistribution genuinely compatible with the given causal structure. Furthermore,\nwe show that for many causal structures, the (unrelaxed) causal compatibility\nproblem is faithfully formulated already by either the first or second order\ninflation test.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:38:06 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 17:37:07 GMT"}, {"version": "v3", "created": "Tue, 4 Aug 2020 19:06:47 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Navascues", "Miguel", ""], ["Wolfe", "Elie", ""]]}, {"id": "1707.06485", "submitter": "Gen Li", "authors": "Gen Li, Irina Gaynanova", "title": "A General Framework for Association Analysis of Heterogeneous Data", "comments": null, "journal-ref": "Annals of Applied Statistics 2018, Vol. 12, No. 3, 1700-1726", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate association analysis is of primary interest in many\napplications. Despite the prevalence of high-dimensional and non-Gaussian data\n(such as count-valued or binary), most existing methods only apply to\nlow-dimensional data with continuous measurements. Motivated by the Computer\nAudition Lab 500-song (CAL500) music annotation study, we develop a new\nframework for the association analysis of two sets of high-dimensional and\nheterogeneous (continuous/binary/count) data. We model heterogeneous random\nvariables using exponential family distributions, and exploit a structured\ndecomposition of the underlying natural parameter matrices to identify shared\nand individual patterns for two data sets. We also introduce a new measure of\nthe strength of association, and a permutation-based procedure to test its\nsignificance. An alternating iteratively reweighted least squares algorithm is\ndevised for model fitting, and several variants are developed to expedite\ncomputation and achieve variable selection. The application to the CAL500 data\nsheds light on the relationship between acoustic features and semantic\nannotations, and provides effective means for automatic music annotation and\nretrieval.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 12:59:13 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Li", "Gen", ""], ["Gaynanova", "Irina", ""]]}, {"id": "1707.06529", "submitter": "Alan Heavens", "authors": "Alan Heavens, Elena Sellentin, Damien de Mijolla, Alvise Vianello", "title": "Massive data compression for parameter-dependent covariance matrices", "comments": "8 pages. Accepted by MNRAS", "journal-ref": null, "doi": "10.1093/mnras/stx2326", "report-no": null, "categories": "astro-ph.CO astro-ph.IM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the massive data compression algorithm MOPED can be used to\nreduce, by orders of magnitude, the number of simulated datasets that are\nrequired to estimate the covariance matrix required for the analysis of\ngaussian-distributed data. This is relevant when the covariance matrix cannot\nbe calculated directly. The compression is especially valuable when the\ncovariance matrix varies with the model parameters. In this case, it may be\nprohibitively expensive to run enough simulations to estimate the full\ncovariance matrix throughout the parameter space. This compression may be\nparticularly valuable for the next-generation of weak lensing surveys, such as\nproposed for Euclid and LSST, for which the number of summary data (such as\nband power or shear correlation estimates) is very large, $\\sim 10^4$, due to\nthe large number of tomographic redshift bins that the data will be divided\ninto. In the pessimistic case where the covariance matrix is estimated\nseparately for all points in an MCMC analysis, this may require an unfeasible\n$10^9$ simulations. We show here that MOPED can reduce this number by a factor\nof 1000, or a factor of $\\sim 10^6$ if some regularity in the covariance matrix\nis assumed, reducing the number of simulations required to a manageable $10^3$,\nmaking an otherwise intractable analysis feasible.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 14:18:50 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 15:36:00 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Heavens", "Alan", ""], ["Sellentin", "Elena", ""], ["de Mijolla", "Damien", ""], ["Vianello", "Alvise", ""]]}, {"id": "1707.06544", "submitter": "Henry Lam", "authors": "Matthew Plumlee, Henry Lam", "title": "An Uncertainty Quantification Method for Inexact Simulation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of stochastic simulation models are imperfect in that they\nfail to exactly emulate real system dynamics. The inexactness of the simulation\nmodel, or model discrepancy, can impact the predictive accuracy and usefulness\nof the simulation for decision-making. This paper proposes a systematic\nframework to integrate data from both the simulation responses and the real\nsystem responses to learn this discrepancy and quantify the resulting\nuncertainty. Our framework addresses the theoretical and computational\nrequirements for stochastic estimation in a Bayesian setting. It involves an\noptimization-based procedure to compute confidence bounds on the target outputs\nthat elicit desirable large-sample statistical properties. We illustrate the\npractical value of our framework with a call center example and a manufacturing\nline case study.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 14:30:47 GMT"}], "update_date": "2017-07-21", "authors_parsed": [["Plumlee", "Matthew", ""], ["Lam", "Henry", ""]]}, {"id": "1707.06573", "submitter": "Irina Gaynanova", "authors": "Irina Gaynanova and Gen Li", "title": "Structural Learning and Integrative Decomposition of Multi-View Data", "comments": null, "journal-ref": "Biometrics 2019, Vol. 75, No. 4, 1121-1132", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increased availability of the multi-view data (data on the same samples\nfrom multiple sources) has led to strong interest in models based on low-rank\nmatrix factorizations. These models represent each data view via shared and\nindividual components, and have been successfully applied for exploratory\ndimension reduction, association analysis between the views, and further\nlearning tasks such as consensus clustering. Despite these advances, there\nremain significant challenges in modeling partially-shared components, and\nidentifying the number of components of each type\n(shared/partially-shared/individual). In this work, we formulate a novel linked\ncomponent model that directly incorporates partially-shared structures. We call\nthis model SLIDE for Structural Learning and Integrative DEcomposition of\nmulti-view data. We prove the existence of SLIDE decomposition and explicitly\ncharacterize the identifiability conditions. The proposed model fitting and\nselection techniques allow for joint identification of the number of components\nof each type, in contrast to existing sequential approaches. In our empirical\nstudies, SLIDE demonstrates excellent performance in both signal estimation and\ncomponent selection. We further illustrate the methodology on the breast cancer\ndata from The Cancer Genome Atlas repository.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 15:30:54 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gaynanova", "Irina", ""], ["Li", "Gen", ""]]}, {"id": "1707.06578", "submitter": "Joydeep Chowdhury", "authors": "Joydeep Chowdhury and Probal Chaudhuri", "title": "Depth based inference on conditional distribution with infinite\n  dimensional data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop inference and testing procedures for conditional dispersion and\nskewness in a nonparametric regression setup based on statistical depth\nfunctions. The methods developed can be applied in situations, where the\nresponse is multivariate and the covariate is a random element in a metric\nspace. This includes regression with functional covariate as a special case. We\nconstruct measures of the center, the spread and the skewness of the\nconditional distribution of the response given the covariate using depth based\nnonparametric regression procedures. We establish the asymptotic consistency of\nthose measures and develop a test for heteroscedasticity and a test for\nconditional skewness. We present level and power study for the tests in several\nsimulated models. The usefulness of the methodology is also demonstrated in a\nreal dataset. In that dataset, our responses are the nutritional contents of\ndifferent meat samples measured by their protein, fat and moisture contents,\nand the functional covariate is the absorbance spectra of the meat samples.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 15:47:35 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 12:43:53 GMT"}, {"version": "v3", "created": "Sat, 14 Sep 2019 14:13:48 GMT"}], "update_date": "2019-09-17", "authors_parsed": [["Chowdhury", "Joydeep", ""], ["Chaudhuri", "Probal", ""]]}, {"id": "1707.06661", "submitter": "Yunfan Li", "authors": "Yunfan Li, Bruce A. Craig, Anindya Bhadra", "title": "The Graphical Horseshoe Estimator for Inverse Covariance Matrices", "comments": "29 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new estimator of the inverse covariance matrix for\nhigh-dimensional multivariate normal data using the horseshoe prior. The\nproposed graphical horseshoe estimator has attractive properties compared to\nother popular estimators, such as the graphical lasso and graphical Smoothly\nClipped Absolute Deviation (SCAD). The most prominent benefit is that when the\ntrue inverse covariance matrix is sparse, the graphical horseshoe provides\nestimates with small information divergence from the true sampling\ndistribution. The posterior mean under the graphical horseshoe prior can also\nbe almost unbiased under certain conditions. In addition to these theoretical\nresults, we also provide a full Gibbs sampler for implementing our estimator.\nMATLAB code is available for download from github at\nhttp://github.com/liyf1988/GHS. The graphical horseshoe estimator compares\nfavorably to existing techniques in simulations and in a human gene network\ndata analysis.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 18:03:42 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 22:12:37 GMT"}, {"version": "v3", "created": "Sun, 6 Jan 2019 19:15:56 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Li", "Yunfan", ""], ["Craig", "Bruce A.", ""], ["Bhadra", "Anindya", ""]]}, {"id": "1707.06697", "submitter": "Rafael Erbisti", "authors": "Rafael S. Erbisti and Thais C. O. Fonseca and Mariane B. Alves", "title": "Bayesian covariance modeling of multivariate spatial random fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present full Bayesian inference for a new flexible\nnonseparable class of cross-covariance functions for multivariate spatial data.\nA Bayesian test is proposed for separability of covariance functions which is\nmuch more interpretable than parameters related to separability. Spatial models\nhave been increasingly applied in several areas, such as environmental science,\nclimate science and agriculture. These data are usually available in space,\ntime and possibly for several processes. In this context the modeling of\ndependence is crucial for correct uncertainty quantification and reliable\npredictions. In particular, for multivariate spatial data we need to specify a\nvalid cross-covariance function, which defines the dependence between the\ncomponents of a response vector for all locations in the spatial domain.\nHowever, cross-covariance functions are not easily specified and the\ncomputational burden is a limitation for model complexity. In this work, we\npropose a nonseparable covariance function that is based on the convex\ncombination of separable covariance functions and on latent dimensions\nrepresentation of the vector components. The covariance structure proposed is\nvalid and flexible. We simulate four different scenarios for different degrees\nof separability and compute the posterior probability of separability. It turns\nout that the posterior probability is much easier to interpret than actual\nmodel parameters. We illustrate our methodology with a weather dataset from\nCear\\'a, Brazil.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 20:13:27 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Erbisti", "Rafael S.", ""], ["Fonseca", "Thais C. O.", ""], ["Alves", "Mariane B.", ""]]}, {"id": "1707.06706", "submitter": "Hong Zhou", "authors": "Huajiang Li and Hong Zhou", "title": "The Covering Principle: A New Approach to Address Multiplicity in\n  Hypotheses Testing", "comments": null, "journal-ref": "The revised manuscript of this article has been published by\n  Taylor & Francis in \"Communications in Statistics - Theory and Methods\" on\n  June 04, 2019", "doi": "10.1080/03610926.2019.1628989", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The closure and the partitioning principles have been used to build various\nmultiple testing procedures in the past three decades. The essence of these two\nprinciples is based on parameter space partitioning. In this article, we\npropose a novel approach coined the covering principle from the perspective of\nrejection region coverage in the sample space. The covering principle divides\nthe whole family of null hypotheses into a few overlapped sub-families when\nthere is a priority of making decisions for hypothesis testing. We have proven\nthat the multiple testing procedure constructed by the covering principle\nstrongly controls the familywise error rate as long as the multiple tests for\neach sub-familiy strongly control the type I error. We have illustrated the\ncovering principle can be applied to solve the general gate-keeping problems.\n", "versions": [{"version": "v1", "created": "Thu, 20 Jul 2017 20:55:19 GMT"}], "update_date": "2019-11-20", "authors_parsed": [["Li", "Huajiang", ""], ["Zhou", "Hong", ""]]}, {"id": "1707.06756", "submitter": "Clayton Morrison", "authors": "Colin Reimer Dawson, Chaofan Huang, Clayton T. Morrison", "title": "An Infinite Hidden Markov Model With Similarity-Biased Transitions", "comments": "16 pages, 4 figures, accepted to ICML 2017, includes supplemental\n  appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a generalization of the Hierarchical Dirichlet Process Hidden\nMarkov Model (HDP-HMM) which is able to encode prior information that state\ntransitions are more likely between \"nearby\" states. This is accomplished by\ndefining a similarity function on the state space and scaling transition\nprobabilities by pair-wise similarities, thereby inducing correlations among\nthe transition distributions. We present an augmented data representation of\nthe model as a Markov Jump Process in which: (1) some jump attempts fail, and\n(2) the probability of success is proportional to the similarity between the\nsource and destination states. This augmentation restores conditional conjugacy\nand admits a simple Gibbs sampler. We evaluate the model and inference method\non a speaker diarization task and a \"harmonic parsing\" task using four-part\nchorale data, as well as on several synthetic datasets, achieving favorable\ncomparisons to existing models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 04:39:10 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Dawson", "Colin Reimer", ""], ["Huang", "Chaofan", ""], ["Morrison", "Clayton T.", ""]]}, {"id": "1707.06768", "submitter": "Fabrizio Leisen", "authors": "Alan Riva Palacio and Fabrizio Leisen", "title": "Integrability conditions for Compound Random Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compound random measures (CoRM's) are a flexible and tractable framework for\nvectors of completely random measure. In this paper, we provide conditions to\nguarantee the existence of a CoRM. Furthermore, we prove some interesting\nproperties of CoRM's when exponential scores and regularly varying L\\'evy\nintensities are considered.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 06:33:29 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 17:31:23 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Palacio", "Alan Riva", ""], ["Leisen", "Fabrizio", ""]]}, {"id": "1707.06837", "submitter": "Akihiko Noda", "authors": "Mikio Ito, Akihiko Noda, Tatsuma Wada", "title": "An Alternative Estimation Method of a Time-Varying Parameter Model", "comments": "35 pages, 6 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.EC q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A non-Bayesian, regression-based or generalized least squares (GLS)-based\napproach is formally proposed to estimate a class of time-varying AR parameter\nmodels. This approach has partly been used by Ito et al. (2014, 2016a,b), and\nis proven to be efficient because, unlike conventional methods, it does not\nrequire Kalman filtering and smoothing procedures, but yields a smoothed\nestimate that is identical to the Kalman-smoothed estimate. Unlike the maximum\nlikelihood estimator, the possibility of the pile-up problem is negligible. In\naddition, this approach enables us to deal with stochastic volatility models,\nmodels with a time-dependent variance-covariance matrix, and models with\nnon-Gaussian errors that allow us to deal with abrupt changes or structural\nbreaks in time-varying parameters.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 10:52:28 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 09:19:53 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Ito", "Mikio", ""], ["Noda", "Akihiko", ""], ["Wada", "Tatsuma", ""]]}, {"id": "1707.06842", "submitter": "Simon Michael Papalexiou Ph.D", "authors": "Simon Michael Papalexiou", "title": "A unified theory for exact stochastic modelling of univariate and\n  multivariate processes with continuous, mixed type, or discrete marginal\n  distributions and any correlation structure", "comments": "46 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hydroclimatic processes are characterized by heterogeneous spatiotemporal\ncorrelation structures and marginal distributions that can be continuous,\nmixed-type, discrete or even binary. Simulating exactly such processes can\ngreatly improve hydrological analysis and design. Yet this challenging task is\naccomplished often by ad hoc and approximate methodologies that are devised for\nspecific variables and purposes. In this study, a single framework is proposed\nallowing the exact simulation of processes with any marginal and any\ncorrelation structure. We unify, extent, and improve of a general-purpose\nmodelling strategy based on the assumption that any process can emerge by\ntransforming a parent Gaussian process with a specific correlation structure. A\nnovel mathematical representation of the parent-Gaussian scheme provides a\nconsistent and fully general description that supersedes previous specific\nparameterizations, resulting in a simple, fast and efficient simulation\nprocedure for every spatiotemporal process. In particular, introducing a simple\nbut flexible procedure we obtain a parametric expression of the correlation\ntransformation function, allowing to assess the correlation structure of the\nparent-Gaussian process that yields the prescribed correlation of the target\nprocess after marginal back transformation. The same framework is also\napplicable for cyclostationary and multivariate modelling. The simulation of a\nvariety of hydroclimatic variables with very different correlation structures\nand marginals, such as precipitation, stream flow, wind speed, humidity,\nextreme events per year, etc., as well as a multivariate application,\nhighlights the flexibility, advantages, and complete generality of the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 11:11:03 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Papalexiou", "Simon Michael", ""]]}, {"id": "1707.06852", "submitter": "Sourabh Bhattacharya", "authors": "Debashis Chatterjee and Sourabh Bhattacharya", "title": "A Statistical Perspective on Inverse and Inverse Regression Problems", "comments": "To appear in RASHI", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inverse problems, where in broad sense the task is to learn from the noisy\nresponse about some unknown function, usually represented as the argument of\nsome known functional form, has received wide attention in the general\nscientific disciplines. How- ever, in mainstream statistics such inverse\nproblem paradigm does not seem to be as popular. In this article we provide a\nbrief overview of such problems from a statistical, particularly Bayesian,\nperspective.\n  We also compare and contrast the above class of problems with the perhaps\nmore statistically familiar inverse regression problems, arguing that this\nclass of problems contains the traditional class of inverse problems. In course\nof our review we point out that the statistical literature is very scarce with\nrespect to both the inverse paradigms, and substantial research work is still\nnecessary to develop the fields.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 11:30:54 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Chatterjee", "Debashis", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1707.06888", "submitter": "Martin Hazelton", "authors": "Tilman M. Davies, Jonathan C. Marshall, Martin L. Hazelton", "title": "Tutorial on kernel estimation of continuous spatial and spatiotemporal\n  relative risk with accompanying instruction in R", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel smoothing is a highly flexible and popular approach for estimation of\nprobability density and intensity functions of continuous spatial data. In this\nrole it also forms an integral part of estimation of functionals such as the\ndensity-ratio or \"relative risk\" surface. Originally developed with the\nepidemiological motivation of examining fluctuations in disease risk based on\nsamples of cases and controls collected over a given geographical region, such\nfunctions have also been successfully employed across a diverse range of\ndisciplines where a relative comparison of spatial density functions has been\nof interest. This versatility has demanded ongoing developments and\nimprovements to the relevant methodology, including use spatially adaptive\nsmoothers; tests of significantly elevated risk based on asymptotic theory;\nextension to the spatiotemporal domain; and novel computational methods for\ntheir evaluation. In this tutorial paper we review the current methodology,\nincluding the most recent developments in estimation, computation and\ninference. All techniques are implemented in the new software package sparr,\npublicly available for the R language, and we illustrate its use with a pair of\nepidemiological examples.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 10:03:32 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Davies", "Tilman M.", ""], ["Marshall", "Jonathan C.", ""], ["Hazelton", "Martin L.", ""]]}, {"id": "1707.06897", "submitter": "Nicholas Heard", "authors": "Nicholas Heard and Patrick Rubin-Delanchy", "title": "Choosing Between Methods of Combining p-values", "comments": null, "journal-ref": null, "doi": "10.1093/biomet/asx076", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining p-values from independent statistical tests is a popular approach\nto meta-analysis, particularly when the data underlying the tests are either no\nlonger available or are difficult to combine. A diverse range of p-value\ncombination methods appear in the literature, each with different statistical\nproperties. Yet all too often the final choice used in a meta-analysis can\nappear arbitrary, as if all effort has been expended building the models that\ngave rise to the p-values. Birnbaum (1954) showed that any reasonable p-value\ncombiner must be optimal against some alternative hypothesis. Starting from\nthis perspective and recasting each method of combining p-values as a\nlikelihood ratio test, we present theoretical results for some of the standard\ncombiners which provide guidance about how a powerful combiner might be chosen\nin practice.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 13:52:21 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 14:37:59 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 23:53:47 GMT"}, {"version": "v4", "created": "Thu, 14 Dec 2017 13:45:16 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Heard", "Nicholas", ""], ["Rubin-Delanchy", "Patrick", ""]]}, {"id": "1707.06904", "submitter": "Hamdi Raissi", "authors": "Ben Hajria Raja, Khardani Salah and Ra\\\"issi Hamdi", "title": "Testing for breaks in variance structures with smooth changes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting variance breaks in the case of smooth time-varying\nvariance structure is studied. It is highlighted that the tests based on\n(piecewise) constant specification of the variance are not able to distinguish\nbetween smooth non constant variance and the case where an abrupt change is\npresent. Consequently, a new procedure for detecting variance breaks taking\ninto account for smooth changes of the variance is proposed. The finite sample\nproperties of the tests introduced in the paper are investigated by Monte Carlo\nexperiments. The theoretical outputs are illustrated using U.S. macroeconomic\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 14:03:59 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Raja", "Ben Hajria", ""], ["Salah", "Khardani", ""], ["Hamdi", "Ra\u00efssi", ""]]}, {"id": "1707.06953", "submitter": "Dario Gasbarra", "authors": "Dario Gasbarra, Sinisa Pajevic and Peter J. Basser", "title": "Eigenvalues of random matrices with isotropic Gaussian noise and the\n  design of Diffusion Tensor Imaging experiments", "comments": "40+4 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor-valued and matrix-valued measurements of different physical properties\nare increasingly available in material sciences and medical imaging\napplications. The eigenvalues and eigenvectors of such multivariate data\nprovide novel and unique information, but at the cost of requiring a more\ncomplex statistical analysis. In this work we derive the distributions of\neigenvalues and eigenvectors in the special but important case of $m \\times m$\nsymmetric random matrices, $D$, observed with isotropic matrix-variate Gaussian\nnoise. The properties of these distributions depend strongly on the symmetries\nof the mean tensor/matrix, $\\bar D$. When $\\bar D$ has repeated eigenvalues,\nthe eigenvalues of $D$ are not asymptotically Gaussian, and repulsion is\nobserved between the eigenvalues corresponding to the same $\\bar D$\neigenspaces. We apply these results to diffusion tensor imaging (DTI), with\n$m=3$, addressing an important problem of detecting the symmetries of the\ndiffusion tensor, and seeking an experimental design that could potentially\nyield an isotropic Gaussian distribution. In the 3-dimensional case, when the\nmean tensor is spherically symmetric and the noise is Gaussian and isotropic,\nthe asymptotic distribution of the first three eigenvalue central moment\nstatistics is simple and can be used to test for isotropy. In order to apply\nsuch tests, we use quadrature rules of order $t \\ge 4$ with constant weights on\nthe unit sphere to design a DTI-experiment with the property that isotropy of\nthe underlying true tensor implies isotropy of the Fisher information. We also\nexplain the potential implications of the methods using simulated DTI data with\na Rician noise model.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jul 2017 16:04:38 GMT"}], "update_date": "2017-07-24", "authors_parsed": [["Gasbarra", "Dario", ""], ["Pajevic", "Sinisa", ""], ["Basser", "Peter J.", ""]]}, {"id": "1707.07149", "submitter": "Marjolein Fokkema", "authors": "Marjolein Fokkema", "title": "Fitting Prediction Rule Ensembles with R Package pre", "comments": null, "journal-ref": "Journal of Statistical Software 92 (2020) 12 1-30", "doi": "10.18637/jss.v092.i12", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction rule ensembles (PREs) are sparse collections of rules, offering\nhighly interpretable regression and classification models. This paper presents\nthe R package pre, which derives PREs through the methodology of Friedman and\nPopescu (2008). The implementation and functionality of package pre is\ndescribed and illustrated through application on a dataset on the prediction of\ndepression. Furthermore, accuracy and sparsity of PREs is compared with that of\nsingle trees, random forest and lasso regression in four benchmark datasets.\nResults indicate that pre derives ensembles with predictive accuracy comparable\nto that of random forests, while using a smaller number of variables for\nprediction.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 12:03:59 GMT"}, {"version": "v2", "created": "Sat, 5 Aug 2017 10:34:16 GMT"}, {"version": "v3", "created": "Tue, 5 Sep 2017 15:49:23 GMT"}, {"version": "v4", "created": "Tue, 30 Oct 2018 12:24:09 GMT"}, {"version": "v5", "created": "Sat, 28 Mar 2020 01:01:55 GMT"}], "update_date": "2020-03-31", "authors_parsed": [["Fokkema", "Marjolein", ""]]}, {"id": "1707.07215", "submitter": "Weinan Wang", "authors": "Weinan Wang, Wenguang Sun", "title": "Sparse Recovery With Multiple Data Streams: A Sequential Adaptive\n  Testing Approach", "comments": "34 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multistage design has been used in a wide range of scientific fields. By\nallocating sensing resources adaptively, one can effectively eliminate null\nlocations and localize signals with a smaller study budget. We formulate a\ndecision-theoretic framework for simultaneous multi-stage adaptive testing and\nstudy how to minimize the total number of measurements while meeting\npre-specified constraints on both the false positive rate (FPR) and missed\ndiscovery rate (MDR). The new procedure, which effectively pools information\nacross individual tests using a simultaneous multistage adaptive ranking and\nthresholding (SMART) approach, can achieve precise error rates control and lead\nto great savings in total study costs. Numerical studies confirm the\neffectiveness of SMART for FPR and MDR control and show that it achieves\nsubstantial power gain over existing methods. The SMART procedure is\ndemonstrated through the analysis of high-throughput screening data and spatial\nimaging data.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jul 2017 21:02:20 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 01:08:32 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wang", "Weinan", ""], ["Sun", "Wenguang", ""]]}, {"id": "1707.07275", "submitter": "Giacomo Aletti", "authors": "Giacomo Aletti", "title": "Likelihood test in permutations with bias. Premier League and La Liga:\n  surprises during the last 25 seasons", "comments": "Bibliography updated. Thanks to Prof Karlsson to have suggested the\n  paper [8] H. Stern. Models for distributions on permutations. JASA (1990)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce the models of permutations with bias, which are\nrandom permutations of a set, biased by some preference values. We present a\nnew parametric test, together with an efficient way to calculate its p-value.\nThe final tables of the English and Spanish major soccer leagues are tested\naccording to this new procedure, to discover whether these results were aligned\nwith expectations.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 10:17:40 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 10:23:34 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Aletti", "Giacomo", ""]]}, {"id": "1707.07332", "submitter": "Farouk Nathoo", "authors": "Farouk S. Nathoo, Linglong Kong, Hongtu Zhu", "title": "A Review of Statistical Methods in Imaging Genetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid growth of modern technology, many large-scale biomedical\nstudies have been/are being/will be conducted to collect massive datasets with\nlarge volumes of multi-modality imaging, genetic, neurocognitive, and clinical\ninformation from increasingly large cohorts. Simultaneously extracting and\nintegrating rich and diverse heterogeneous information in neuroimaging and/or\ngenomics from these big datasets could transform our understanding of how\ngenetic variants impact brain structure and function, cognitive function, and\nbrain-related disease risk across the lifespan. Such understanding is critical\nfor diagnosis, prevention, and treatment of numerous complex brain-related\ndisorders (e.g., schizophrenia and Alzheimer). However, the development of\nanalytical methods for the joint analysis of both high-dimensional imaging\nphenotypes and high-dimensional genetic data, called big data squared (BD$^2$),\npresents major computational and theoretical challenges for existing analytical\nmethods. Besides the high-dimensional nature of BD$^2$, various neuroimaging\nmeasures often exhibit strong spatial smoothness and dependence and genetic\nmarkers may have a natural dependence structure arising from linkage\ndisequilibrium. We review some recent developments of various statistical\ntechniques for the joint analysis of BD$^2$, including massive univariate and\nvoxel-wise approaches, reduced rank regression, mixture models, and group\nsparse multi-task regression. By doing so, we hope that this review may\nencourage others in the statistical community to enter into this new and\nexciting field of research.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jul 2017 19:11:15 GMT"}, {"version": "v2", "created": "Tue, 6 Mar 2018 00:32:08 GMT"}], "update_date": "2018-03-07", "authors_parsed": [["Nathoo", "Farouk S.", ""], ["Kong", "Linglong", ""], ["Zhu", "Hongtu", ""]]}, {"id": "1707.07506", "submitter": "Yasin Asar", "authors": "Jibo Wu and Yasin Asar", "title": "Efficiency of the principal component Liu-type estimator in logistic\n  regression model", "comments": "16 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a principal component Liu-type logistic estimator by\ncombining the principal component logistic regression estimator and Liu-type\nlogistic estimator to overcome the multicollinearity problem. The superiority\nof the new estimator over some related estimators are studied under the\nasymptotic mean squared error matrix. A Monte Carlo simulation experiment is\ndesigned to compare the performances of the estimators using mean squared error\ncriterion. Finally, a conclusion section is presented.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 12:09:15 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Wu", "Jibo", ""], ["Asar", "Yasin", ""]]}, {"id": "1707.07560", "submitter": "Marco Felix Eigenmann", "authors": "Marco F. Eigenmann, Preetam Nandy, Marloes H. Maathuis", "title": "Structure Learning of Linear Gaussian Structural Equation Models with\n  Weak Edges", "comments": "18 pages, 17 figures, UAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider structure learning of linear Gaussian structural equation models\nwith weak edges. Since the presence of weak edges can lead to a loss of edge\norientations in the true underlying CPDAG, we define a new graphical object\nthat can contain more edge orientations. We show that this object can be\nrecovered from observational data under a type of strong faithfulness\nassumption. We present a new algorithm for this purpose, called aggregated\ngreedy equivalence search (AGES), that aggregates the solution path of the\ngreedy equivalence search (GES) algorithm for varying values of the penalty\nparameter. We prove consistency of AGES and demonstrate its performance in a\nsimulation study and on single cell data from Sachs et al. (2005). The\nalgorithm will be made available in the R-package pcalg.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jul 2017 13:56:48 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Eigenmann", "Marco F.", ""], ["Nandy", "Preetam", ""], ["Maathuis", "Marloes H.", ""]]}, {"id": "1707.07971", "submitter": "Sophie Donnet", "authors": "Sophie Donnet (1), St\\'ephane Robin (1) ((1) MIA-Paris)", "title": "Using deterministic approximations to accelerate SMC for posterior\n  sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential Monte Carlo has become a standard tool for Bayesian Inference of\ncomplex models. This approach can be computationally demanding, especially when\ninitialized from the prior distribution. On the other hand, deter-ministic\napproximations of the posterior distribution are often available with no\ntheoretical guaranties. We propose a bridge sampling scheme starting from such\na deterministic approximation of the posterior distribution and targeting the\ntrue one. The resulting Shortened Bridge Sampler (SBS) relies on a sequence of\ndistributions that is determined in an adaptive way. We illustrate the\nrobustness and the efficiency of the methodology on a large simulation study.\nWhen applied to network datasets, SBS inference leads to different statistical\nconclusions from the one supplied by the standard variational Bayes\napproximation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 13:04:51 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Donnet", "Sophie", "", "MIA-Paris"], ["Robin", "St\u00e9phane", "", "MIA-Paris"]]}, {"id": "1707.08036", "submitter": "Andi Wang", "authors": "Andi Q. Wang, Martin Kolb, Gareth O. Roberts, David Steinsaltz", "title": "Theoretical properties of quasi-stationary Monte Carlo methods", "comments": "27 pages, 1 figure. Final version of accepted paper. Minor typos\n  corrected", "journal-ref": "Annals of Applied Probability 2019, Vol. 29, No. 1, 434-457", "doi": "10.1214/18-AAP1422", "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives foundational results for the application of\nquasi-stationarity to Monte Carlo inference problems. We prove natural\nsufficient conditions for the quasi-limiting distribution of a killed diffusion\nto coincide with a target density of interest. We also quantify the rate of\nconvergence to quasi-stationarity by relating the killed diffusion to an\nappropriate Langevin diffusion. As an example, we consider in detail a killed\nOrnstein--Uhlenbeck process with Gaussian quasi-stationary distribution.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 15:16:01 GMT"}, {"version": "v2", "created": "Wed, 2 May 2018 18:21:05 GMT"}, {"version": "v3", "created": "Thu, 6 Dec 2018 16:44:44 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Wang", "Andi Q.", ""], ["Kolb", "Martin", ""], ["Roberts", "Gareth O.", ""], ["Steinsaltz", "David", ""]]}, {"id": "1707.08053", "submitter": "Julyan Arbel", "authors": "Julyan Arbel (1) and Stefano Favaro (2) ((1) Inria Grenoble\n  Rh\\^one-Alpes (2) University of Torino)", "title": "Approximating predictive probabilities of Gibbs-type priors", "comments": "22 pages, 6 figures. Added posterior simulation study, corrected\n  typos", "journal-ref": "Sankhya, 2020", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gibbs-type random probability measures, or Gibbs-type priors, are arguably\nthe most \"natural\" generalization of the celebrated Dirichlet prior. Among them\nthe two parameter Poisson-Dirichlet prior certainly stands out for the\nmathematical tractability and interpretability of its predictive probabilities,\nwhich made it the natural candidate in several applications. Given a sample of\nsize $n$, in this paper we show that the predictive probabilities of any\nGibbs-type prior admit a large $n$ approximation, with an error term vanishing\nas $o(1/n)$, which maintains the same desirable features as the predictive\nprobabilities of the two parameter Poisson-Dirichlet prior.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 15:47:12 GMT"}, {"version": "v2", "created": "Fri, 28 Jul 2017 08:16:13 GMT"}, {"version": "v3", "created": "Tue, 24 Mar 2020 15:12:25 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Arbel", "Julyan", ""], ["Favaro", "Stefano", ""]]}, {"id": "1707.08143", "submitter": "Vivekananda Roy", "authors": "Run Wang, Somak Dutta and Vivekananda Roy", "title": "A note on marginal correlation based screening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independence screening methods such as the two sample $t$-test and the\nmarginal correlation based ranking are among the most widely used techniques\nfor variable selection in ultrahigh dimensional data sets. In this short note,\nsimple examples are used to demonstrate potential problems with the\nindependence screening methods in the presence of correlated predictors. Also,\nan example is considered where all important variables are independent among\nthemselves and all but one important variables are independent with the\nunimportant variables. Furthermore, a real data example from a genome wide\nassociation study is used to illustrate inferior performance of marginal\ncorrelation screening compared to another screening method.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 18:23:31 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2020 02:16:44 GMT"}, {"version": "v3", "created": "Mon, 16 Nov 2020 16:52:55 GMT"}], "update_date": "2020-11-17", "authors_parsed": [["Wang", "Run", ""], ["Dutta", "Somak", ""], ["Roy", "Vivekananda", ""]]}, {"id": "1707.08215", "submitter": "Mengyang Gu", "authors": "Mengyang Gu and Long Wang", "title": "Scaled Gaussian Stochastic Process for Computer Model Calibration and\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of calibrating an imperfect computer model using\nexperimental data. To compensate the misspecification of the computer model and\nmake more accurate predictions, a discrepancy function is often included and\nmodeled via a Gaussian stochastic process (GaSP). The calibrated computer model\nalone, however, sometimes fits the experimental data poorly, as the calibration\nparameters become unidentifiable. In this work, we propose the scaled Gaussian\nstochastic process (S-GaSP), a novel stochastic process that bridges the gap\nbetween two predominant methods, namely the $L_2$ calibration and the GaSP\ncalibration. It is shown that our approach performs well in both calibration\nand prediction. A computationally feasible approach is introduced for this new\nmodel under the Bayesian paradigm. Compared with the GaSP calibration, the\nS-GaSP calibration enables the calibrated computer model itself to predict the\nreality well, based on the posterior distribution of the calibration\nparameters. Numerical comparisons of the simulated and real data are provided\nto illustrate the connections and differences between the proposed S-GaSP and\nother alternative approaches.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 20:54:50 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 03:36:44 GMT"}, {"version": "v3", "created": "Thu, 3 May 2018 13:14:33 GMT"}], "update_date": "2018-05-04", "authors_parsed": [["Gu", "Mengyang", ""], ["Wang", "Long", ""]]}, {"id": "1707.08220", "submitter": "Yajuan Si", "authors": "Yajuan Si, Rob Trangucci, Jonah Sol Gabry and Andrew Gelman", "title": "Bayesian hierarchical weighting adjustment and survey inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We combine Bayesian prediction and weighted inference as a unified approach\nto survey inference. The general principles of Bayesian analysis imply that\nmodels for survey outcomes should be conditional on all variables that affect\nthe probability of inclusion. We incorporate the weighting variables under the\nframework of multilevel regression and poststratification, as a byproduct\ngenerating model-based weights after smoothing. We investigate deep\ninteractions and introduce structured prior distributions for smoothing and\nstability of estimates. The computation is done via Stan and implemented in the\nopen source R package \"rstanarm\" ready for public use. Simulation studies\nillustrate that model-based prediction and weighting inference outperform\nclassical weighting. We apply the proposal to the New York Longitudinal Study\nof Wellbeing. The new approach generates robust weights and increases\nefficiency for finite population inference, especially for subsets of the\npopulation.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jul 2017 21:02:26 GMT"}, {"version": "v2", "created": "Tue, 23 Jun 2020 15:51:53 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Si", "Yajuan", ""], ["Trangucci", "Rob", ""], ["Gabry", "Jonah Sol", ""], ["Gelman", "Andrew", ""]]}, {"id": "1707.08298", "submitter": "Vitara Pungpapong", "authors": "Vitara Pungpapong, Min Zhang and Dabao Zhang", "title": "Variable Selection for High-dimensional Generalized Linear Models using\n  an Iterated Conditional Modes/Medians Algorithm", "comments": "47 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional linear and nonlinear models have been extensively used to\nidentify associations between response and explanatory variables. The variable\nselection problem is commonly of interest in the presence of massive and\ncomplex data. An empirical Bayes model for high-dimensional generalized linear\nmodels (GLMs) is considered in this paper. The extension of the Iterated\nConditional Modes/Medians (ICM/M) algorithm is proposed to build up a GLM. With\nthe construction of pseudodata and pseudovariances based on iteratively\nreweighted least squares (IRLS), conditional modes are employed to obtain\ndata-drive optimal values for hyperparameters and conditional medians are used\nto estimate regression coefficients. With a spike-and-slab prior for each\ncoefficient, a conditional median can enforce variable estimation and selection\nat the same time. The ICM/M algorithm can also incorporate more complicated\nprior by taking the network structural information into account through the\nIsing model prior. Here we focus on two extensively used models for genomic\ndata: binary logistic and Cox's proportional hazards models. The performance of\nthe proposed method is demonstrated through both simulation studies and real\ndata examples. The implementation of the ICM/M algorithm for both linear and\nnonlinear models can be found in the icmm R package which is freely available\non CRAN.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 06:35:52 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 10:34:38 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Pungpapong", "Vitara", ""], ["Zhang", "Min", ""], ["Zhang", "Dabao", ""]]}, {"id": "1707.08339", "submitter": "H{\\aa}kon Tjelmeland", "authors": "H{\\aa}kon Tjelmeland and Xin Luo", "title": "Prior specification for binary Markov mesh models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose prior distributions for all parts of the specification of a Markov\nmesh model. In the formulation we define priors for the sequential\nneighborhood, for the parametric form of the conditional distributions and for\nthe parameter values. By simulating from the resulting posterior distribution\nwhen conditioning on an observed scene, we thereby obtain an automatic model\nselection procedure for Markov mesh models. To sample from such a posterior\ndistribution, we construct a reversible jump Markov chain Monte Carlo algorithm\n(RJMCMC). We demonstrate the usefulness of our prior formulation and the\nlimitations of our RJMCMC algorithm in two examples.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 09:45:18 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Tjelmeland", "H\u00e5kon", ""], ["Luo", "Xin", ""]]}, {"id": "1707.08384", "submitter": "Julien Bect", "authors": "R\\'emi Stroh (1), S\\'everine Demeyer (1), Nicolas Fischer (1), Julien\n  Bect (2), Emmanuel Vazquez (3) ((1) LNE, (2) L2S, (3) GdR MASCOT-NUM)", "title": "Sequential design of experiments to estimate a probability of exceeding\n  a threshold in a multi-fidelity stochastic simulator", "comments": "61th World Statistics Congress of the International Statistical\n  Institute (ISI 2017), Jul 2017, Marrakech, Morocco", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider a stochastic numerical simulator to assess the\nimpact of some factors on a phenomenon. The simulator is seen as a black box\nwith inputs and outputs. The quality of a simulation, hereafter referred to as\nfidelity, is assumed to be tunable by means of an additional input of the\nsimulator (e.g., a mesh size parameter): high-fidelity simulations provide more\naccurate results, but are time-consuming. Using a limited computation-time\nbudget, we want to estimate, for any value of the physical inputs, the\nprobability that a certain scalar output of the simulator will exceed a given\ncritical threshold at the highest fidelity level. The problem is addressed in a\nBayesian framework, using a Gaussian process model of the multi-fidelity\nsimulator. We consider a Bayesian estimator of the probability, together with\nan associated measure of uncertainty, and propose a new multi-fidelity\nsequential design strategy, called Maximum Speed of Uncertainty Reduction\n(MSUR), to select the value of physical inputs and the fidelity level of new\nsimulations. The MSUR strategy is tested on an example.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 11:35:58 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Stroh", "R\u00e9mi", "", "LNE"], ["Demeyer", "S\u00e9verine", "", "LNE"], ["Fischer", "Nicolas", "", "LNE"], ["Bect", "Julien", "", "L2S"], ["Vazquez", "Emmanuel", "", "GdR MASCOT-NUM"]]}, {"id": "1707.08405", "submitter": "Evgeniy Riabenko", "authors": "Pavel Shvechikov and Evgeniy Riabenko", "title": "Gaussian Processes for Individualized Continuous Treatment Rule\n  Estimation", "comments": "26 pages, 2 figures, presented at American Statistical Association\n  Joint Statistical Meetings 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individualized treatment rule (ITR) recommends treatment on the basis of\nindividual patient characteristics and the previous history of applied\ntreatments and their outcomes. Despite the fact there are many ways to estimate\nITR with binary treatment, algorithms for continuous treatment have only just\nstarted to emerge. We propose a novel approach to continuous ITR estimation\nbased on explicit modelling of uncertainty in the subject's outcome as well as\ndirect estimation of the mean outcome using gaussian process regression. Our\nmethod incorporates two intuitively appealing properties - it is more inclined\nto give a treatment with the outcome of higher expected value and lower\nvariance. Experiments show that this direct incorporation of the uncertainty\ninto ITR estimation process allows to select better treatment than standard\nindirect approach that just models the average. Compared to the competitors\n(including OWL), the proposed method shows improved performance in terms of\nvalue function maximization, has better interpretability, and could be easier\ngeneralized to multiple interdependent continuous treatments setting.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 12:17:43 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 12:05:25 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Shvechikov", "Pavel", ""], ["Riabenko", "Evgeniy", ""]]}, {"id": "1707.08407", "submitter": "Sean Simpson", "authors": "Sean L. Simpson, Min Zhu, Keith E. Muller", "title": "A Note on Implementing a Special Case of the LEAR Covariance Model in\n  Standard Software", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Repeated measures analyses require proper choice of the correlation model to\nensure accurate inference and optimal efficiency. The linear exponent\nautoregressive (LEAR) correlation model provides a flexible two-parameter\ncorrelation structure that accommodates a variety of data types in which the\ncorrelation within-sampling unit decreases exponentially in time or space. The\nLEAR model subsumes three classic temporal correlation structures, namely\ncompound symmetry, continuous-time AR(1), and MA(1), while maintaining\nparsimony and providing appealing statistical and computational properties. It\nalso supplies a plausible correlation structure for power analyses across many\nexperimental designs. However, no commonly used statistical packages provide a\nstraightforward way to implement the model, limiting its use to those with the\nappropriate programming skills. Here we present a reparameterization of the\nLEAR model that allows easily implementing it in standard software for the\nspecial case of data with equally spaced temporal or spatial intervals.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 12:28:51 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Simpson", "Sean L.", ""], ["Zhu", "Min", ""], ["Muller", "Keith E.", ""]]}, {"id": "1707.08538", "submitter": "Jarod Yan Liang Lee", "authors": "Jarod Y.L. Lee, Peter J. Green and Louise M. Ryan", "title": "On the \"Poisson Trick\" and its Extensions for Fitting Multinomial\n  Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is concerned with the fitting of multinomial regression models\nusing the so-called \"Poisson Trick\". The work is motivated by Chen & Kuo (2001)\nand Malchow-M{\\o}ller & Svarer (2003) which have been criticized for being\ncomputationally inefficient and sometimes producing nonsense results. We first\ndiscuss the case of independent data and offer a parsimonious fitting strategy\nwhen all covariates are categorical. We then propose a new approach for\nmodelling correlated responses based on an extension of the Gamma-Poisson\nmodel, where the likelihood can be expressed in closed-form. The parameters are\nestimated via an Expectation/Conditional Maximization (ECM) algorithm, which\ncan be implemented using functions for fitting generalized linear models\nreadily available in standard statistical software packages. Compared to\nexisting methods, our approach avoids the need to approximate the intractable\nintegrals and thus the inference is exact with respect to the approximating\nGamma-Poisson model. The proposed method is illustrated via a reanalysis of the\nyogurt data discussed by Chen & Kuo (2001).\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 16:55:48 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Lee", "Jarod Y. L.", ""], ["Green", "Peter J.", ""], ["Ryan", "Louise M.", ""]]}, {"id": "1707.08658", "submitter": "Nikita Pronko", "authors": "Nikita Pronko", "title": "Change Point Detection with Optimal Transport and Geometric Discrepancy", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present novel retrospective change point detection approach based on\noptimal transport and geometric discrepancy. The method does not require any\nparametric assumptions about distributions separated by change points. It can\nbe used both for single and multiple change point detection and estimation,\nwhile the number of change points is either known or unknown. This result is\nachieved by construction of a certain sliding window statistic from which\nchange points can be derived with elementary convex geometry in a specific\nHilbert space. The work is illustrated with computational examples, both\nartificially constructed and based on actual data.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 22:39:38 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Pronko", "Nikita", ""]]}, {"id": "1707.08692", "submitter": "Ryan Tibshirani", "authors": "Trevor Hastie, Robert Tibshirani, Ryan J. Tibshirani", "title": "Extended Comparisons of Best Subset Selection, Forward Stepwise\n  Selection, and the Lasso", "comments": "18 pages main paper, 34 pages supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In exciting new work, Bertsimas et al. (2016) showed that the classical best\nsubset selection problem in regression modeling can be formulated as a mixed\ninteger optimization (MIO) problem. Using recent advances in MIO algorithms,\nthey demonstrated that best subset selection can now be solved at much larger\nproblem sizes that what was thought possible in the statistics community. They\npresented empirical comparisons of best subset selection with other popular\nvariable selection procedures, in particular, the lasso and forward stepwise\nselection. Surprisingly (to us), their simulations suggested that best subset\nselection consistently outperformed both methods in terms of prediction\naccuracy. Here we present an expanded set of simulations to shed more light on\nthese comparisons.\n  The summary is roughly as follows: (a) neither best subset selection nor the\nlasso uniformly dominate the other, with best subset selection generally\nperforming better in high signal-to-noise (SNR) ratio regimes, and the lasso\nbetter in low SNR regimes; (b) best subset selection and forward stepwise\nperform quite similarly throughout; (c) the relaxed lasso (actually, a\nsimplified version of the original relaxed estimator defined in Meinshausen,\n2007) is the overall winner, performing just about as well as the lasso in low\nSNR scenarios, and as well as best subset selection in high SNR scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 03:04:28 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 23:46:57 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Hastie", "Trevor", ""], ["Tibshirani", "Robert", ""], ["Tibshirani", "Ryan J.", ""]]}, {"id": "1707.08868", "submitter": "Konstantinos Spiliopoulos", "authors": "Konstantinos Spiliopoulos", "title": "Importance sampling for metastable and multiscale dynamical systems", "comments": "Will appear as a chapter in Springer book", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we address the issues that come up in the design of\nimportance sampling schemes for rare events associated to stochastic dynamical\nsystems. We focus on the issue of metastability and on the effect of multiple\nscales. We discuss why seemingly reasonable schemes that follow large\ndeviations optimal paths may perform poorly in practice, even though they are\nasymptotically optimal. Pre-asymptotic optimality is important when one deals\nwith metastable dynamics and we discuss possible ways as to how to address this\nissue. Moreover, we discuss how the effect of the multiple scales (either in\nperiodic or random environments) on the efficient design of importance sampling\nshould be addressed. We discuss the mathematical and practical issues that come\nup, how to overcome some of the issues and discuss future challenges.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 14:01:30 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Spiliopoulos", "Konstantinos", ""]]}, {"id": "1707.08933", "submitter": "Brian Segal", "authors": "Brian D. Segal, Michael R. Elliott, Thomas Braun, Hui Jiang", "title": "P-splines with an l1 penalty for repeated measures", "comments": "54 pages, 26 figures, 5 tables", "journal-ref": "Electronic Journal of Statistics. 12 (2018) 3554-3600", "doi": "10.1214/18-EJS1487", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  P-splines are penalized B-splines, in which finite order differences in\ncoefficients are typically penalized with an $\\ell_2$ norm. P-splines can be\nused for semiparametric regression and can include random effects to account\nfor within-subject variability. In addition to $\\ell_2$ penalties,\n$\\ell_1$-type penalties have been used in nonparametric and semiparametric\nregression to achieve greater flexibility, such as in locally adaptive\nregression splines, $\\ell_1$ trend filtering, and the fused lasso additive\nmodel. However, there has been less focus on using $\\ell_1$ penalties in\nP-splines, particularly for estimating conditional means.\n  In this paper, we demonstrate the potential benefits of using an $\\ell_1$\npenalty in P-splines with an emphasis on fitting non-smooth functions. We\npropose an estimation procedure using the alternating direction method of\nmultipliers and cross validation, and provide degrees of freedom and\napproximate confidence bands based on a ridge approximation to the $\\ell_1$\npenalized fit. We also demonstrate potential uses through simulations and an\napplication to electrodermal activity data collected as part of a stress study.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 16:55:22 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 21:55:47 GMT"}], "update_date": "2018-11-01", "authors_parsed": [["Segal", "Brian D.", ""], ["Elliott", "Michael R.", ""], ["Braun", "Thomas", ""], ["Jiang", "Hui", ""]]}, {"id": "1707.09021", "submitter": "Kara Rudolph", "authors": "Kara E. Rudolph, Oleg Sofrygin, Wenjing Zheng, Mark J. van der Laan", "title": "Robust and Flexible Estimation of Stochastic Mediation Effects: A\n  Proposed Method and Example in a Randomized Trial Setting", "comments": "24 pages, 2 tables, 2 figures", "journal-ref": "Epidemiologic Methods. 2017; 7(1)", "doi": "10.1515/em-2017-0007", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal mediation analysis can improve understanding of the mechanisms\nunderlying epidemiologic associations. However, the utility of natural direct\nand indirect effect estimation has been limited by the assumption of no\nconfounder of the mediator-outcome relationship that is affected by prior\nexposure---an assumption frequently violated in practice. We build on recent\nwork that identified alternative estimands that do not require this assumption\nand propose a flexible and double robust semiparametric targeted minimum\nloss-based estimator for data-dependent stochastic direct and indirect effects.\nThe proposed method treats the intermediate confounder affected by prior\nexposure as a time-varying confounder and intervenes stochastically on the\nmediator using a distribution which conditions on baseline covariates and\nmarginalizes over the intermediate confounder. In addition, we assume the\nstochastic intervention is given, conditional on observed data, which results\nin a simpler estimator and weaker identification assumptions. We demonstrate\nthe estimator's finite sample and robustness properties in a simple simulation\nstudy. We apply the method to an example from the Moving to Opportunity\nexperiment. In this application, randomization to receive a housing voucher is\nthe treatment/instrument that influenced moving to a low-poverty neighborhood,\nwhich is the intermediate confounder. We estimate the data-dependent stochastic\ndirect effect of randomization to the voucher group on adolescent marijuana use\nnot mediated by change in school district and the stochastic indirect effect\nmediated by change in school district. We find no evidence of mediation. Our\nestimator is easy to implement in standard statistical software, and we provide\nannotated R code to further lower implementation barriers.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 19:44:44 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 23:41:55 GMT"}], "update_date": "2020-06-16", "authors_parsed": [["Rudolph", "Kara E.", ""], ["Sofrygin", "Oleg", ""], ["Zheng", "Wenjing", ""], ["van der Laan", "Mark J.", ""]]}, {"id": "1707.09076", "submitter": "Maya Mathur", "authors": "Maya B. Mathur and Tyler J. VanderWeele", "title": "Sensitivity Analysis for Unmeasured Confounding in Meta-Analyses", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random-effects meta-analyses of observational studies can produce biased\nestimates if the synthesized studies are subject to unmeasured confounding. We\npropose sensitivity analyses quantifying the extent to which unmeasured\nconfounding of specified magnitude could reduce to below a certain threshold\nthe proportion of true effect sizes that are scientifically meaningful. We also\ndevelop converse methods to estimate the strength of confounding capable of\nreducing the proportion of scientifically meaningful true effects to below a\nchosen threshold. These methods apply when a \"bias factor\" is assumed to be\nnormally distributed across studies or is assessed across a range of fixed\nvalues. Our estimators are derived using recently proposed sharp bounds on\nconfounding bias within a single study that do not make assumptions regarding\nthe unmeasured confounders themselves or the functional form of their\nrelationships to the exposure and outcome of interest. We provide an R package,\nConfoundedMeta, and a freely available online graphical user interface that\ncompute point estimates and inference and produce plots for conducting such\nsensitivity analyses. These methods facilitate principled use of random-effects\nmeta-analyses of observational studies to assess the strength of causal\nevidence for a hypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 00:06:44 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Mathur", "Maya B.", ""], ["VanderWeele", "Tyler J.", ""]]}, {"id": "1707.09208", "submitter": "Ines Wilms", "authors": "Ines Wilms, Sumanta Basu, Jacob Bien, David S. Matteson", "title": "Sparse Identification and Estimation of Large-Scale Vector\n  AutoRegressive Moving Averages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Vector AutoRegressive Moving Average (VARMA) model is fundamental to the\ntheory of multivariate time series; however, identifiability issues have led\npractitioners to abandon it in favor of the simpler but more restrictive Vector\nAutoRegressive (VAR) model. We narrow this gap with a new optimization-based\napproach to VARMA identification built upon the principle of parsimony. Among\nall equivalent data-generating models, we use convex optimization to seek the\nparameterization that is \"simplest\" in a certain sense. A user-specified\nstrongly convex penalty is used to measure model simplicity, and that same\npenalty is then used to define an estimator that can be efficiently computed.\nWe establish consistency of our estimators in a double-asymptotic regime. Our\nnon-asymptotic error bound analysis accommodates both model specification and\nparameter estimation steps, a feature that is crucial for studying large-scale\nVARMA algorithms. Our analysis also provides new results on penalized\nestimation of infinite-order VAR, and elastic net regression under a singular\ncovariance structure of regressors, which may be of independent interest. We\nillustrate the advantage of our method over VAR alternatives on three real data\nexamples.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 12:43:23 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 16:01:52 GMT"}, {"version": "v3", "created": "Wed, 24 Apr 2019 16:07:26 GMT"}, {"version": "v4", "created": "Tue, 8 Jun 2021 06:50:26 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Wilms", "Ines", ""], ["Basu", "Sumanta", ""], ["Bien", "Jacob", ""], ["Matteson", "David S.", ""]]}, {"id": "1707.09272", "submitter": "Jose Ameijeiras-Alonso", "authors": "Jose Ameijeiras-Alonso, Christophe Ley, Arthur Pewsey, Thomas\n  Verdebout", "title": "Optimal tests for circular reflective symmetry about an unknown central\n  direction", "comments": null, "journal-ref": null, "doi": "10.1007/s00362-019-01150-7", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parametric and semiparametric tests of circular reflective symmetry about an\nunknown central direction are developed that are locally and asymptotically\noptimal in the Le Cam sense against asymmetric $k$-sine-skewed alternatives.\nThe results from Monte Carlo studies comparing the rejection rates of tests\nwith those of previously proposed tests lead to recommendations regarding the\nuse of the various tests with small- to medium-sized samples. Analyses of data\non the directions of cracks in cemented femoral components and the times of gun\ncrimes in Pittsburgh illustrate the proposed methodology and its bootstrap\nextension.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 15:05:46 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Ameijeiras-Alonso", "Jose", ""], ["Ley", "Christophe", ""], ["Pewsey", "Arthur", ""], ["Verdebout", "Thomas", ""]]}, {"id": "1707.09363", "submitter": "Shikui Tu", "authors": "Jin-Xiong Lv, Shikui Tu, Lei Xu", "title": "A Comparative Study of Joint-SNVs Analysis Methods and Detection of\n  Susceptibility Genes for Gastric Cancer in Korean Population", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many joint-SNVs (single-nucleotide variants) analysis methods were proposed\nto tackle the \"missing heritability\" problem, which emphasizes that the joint\ngenetic variants can explain more heritability of traits and diseases. However,\nthere is still lack of a systematic comparison and investigation on the\nrelative strengths and weaknesses of these methods. In this paper, we evaluated\ntheir performance on extensive simulated data generated by varying sample size,\nlinkage disequilibrium (LD), odds ratios (OR), and minor allele frequency\n(MAF), which aims to cover almost all scenarios encountered in practical\napplications. Results indicated that a method called Statistics-space Boundary\nBased Test (S-space BBT) showed stronger detection power than other methods.\nResults on a real dataset of gastric cancer for Korean population also validate\nthe effectiveness of the S-space BBT method.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jul 2017 05:27:24 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Lv", "Jin-Xiong", ""], ["Tu", "Shikui", ""], ["Xu", "Lei", ""]]}, {"id": "1707.09461", "submitter": "Antonio Linero", "authors": "Antonio Ricardo Linero and Yun Yang", "title": "Bayesian Regression Tree Ensembles that Adapt to Smoothness and Sparsity", "comments": "47 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensembles of decision trees are a useful tool for obtaining for obtaining\nflexible estimates of regression functions. Examples of these methods include\ngradient boosted decision trees, random forests, and Bayesian CART. Two\npotential shortcomings of tree ensembles are their lack of smoothness and\nvulnerability to the curse of dimensionality. We show that these issues can be\novercome by instead considering sparsity inducing soft decision trees in which\nthe decisions are treated as probabilistic. We implement this in the context of\nthe Bayesian additive regression trees framework, and illustrate its promising\nperformance through testing on benchmark datasets. We provide strong\ntheoretical support for our methodology by showing that the posterior\ndistribution concentrates at the minimax rate (up-to a logarithmic factor) for\nsparse functions and functions with additive structures in the high-dimensional\nregime where the dimensionality of the covariate space is allowed to grow near\nexponentially in the sample size. Our method also adapts to the unknown\nsmoothness and sparsity levels, and can be implemented by making minimal\nmodifications to existing BART algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 04:25:41 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 23:08:08 GMT"}, {"version": "v3", "created": "Sat, 15 Sep 2018 06:00:40 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Linero", "Antonio Ricardo", ""], ["Yang", "Yun", ""]]}, {"id": "1707.09506", "submitter": "Manabu Kuroki", "authors": "Manabu Kuroki", "title": "Counterfactual Reasoning with Disjunctive Knowledge in a Linear\n  Structural Equation Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating counterfactual quantities when prior\nknowledge is available in the form of disjunctive statements. These include\ndisjunction of conditions (e.g., \"the patient is more than 60 years of age\") as\nwell as disjuction of antecedants (e.g., \"had the patient taken either drug A\nor drug B\"). Focusing on linear structural equation models (SEM) and imperfect\ncontrol plans, we extend the counterfactual framework of Balke and Pearl (1995)\n, Chen and Pearl (2015), and Pearl (2009, pp. 389-391) from unconditional to\nconditional plans, from a univariate treatment to a set of treatments, and from\npoint type knowledge to disjunctive knowledge. Finally, we provide improved\nmatrix representations of the resulting counterfactual parameters, and improved\ncomputational methods of their evaluation.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 12:48:51 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 02:32:28 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Kuroki", "Manabu", ""]]}, {"id": "1707.09508", "submitter": "Julie Josse", "authors": "Jean-Louis Foulley, Gilles Celeux and Julie Josse", "title": "Empirical Bayes approaches to PageRank type algorithms for rating\n  scientific journals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following criticisms against the journal Impact Factor, new journal influence\nscores have been developed such as the Eigenfactor or the Prestige Scimago\nJournal Rank. They are based on PageRank type algorithms on the cross-citations\ntransition matrix of the citing-cited network. The PageRank algorithm performs\na smoothing of the transition matrix combining a random walk on the data\nnetwork and a teleportation to all possible nodes with fixed probabilities (the\ndamping factor being $\\alpha= 0.85$). We reinterpret this smoothing matrix as\nthe mean of a posterior distribution of a Dirichlet-multinomial model in an\nempirical Bayes perspective. We suggest a simple yet efficient way to make a\nclear distinction between structural and sampling zeroes. This allows us to\ncontrast cases with self-citations included or excluded to avoid overvalued\njournal bias. We estimate the model parameters by maximizing the marginal\nlikelihood with a Majorize-Minimize algorithm. The procedure ends up with a\nscore similar to the PageRank ones but with a damping factor depending on each\njournal. The procedures are illustrated with an example about cross-citations\namong 47 statistical journals studied by Varin et. al. (2016).\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 12:53:33 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 15:58:45 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Foulley", "Jean-Louis", ""], ["Celeux", "Gilles", ""], ["Josse", "Julie", ""]]}, {"id": "1707.09549", "submitter": "Raiden Hasegawa", "authors": "Raiden B. Hasegawa and Dylan S. Small", "title": "Sensitivity Analysis for matched pair analysis of binary data: From\n  worst case to average case analysis", "comments": "minor corrections/clarifications made", "journal-ref": "Biometrics, Volume 73, Issue 4, p.1424-1432, 2017", "doi": "10.1111/biom.12688", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In matched observational studies where treatment assignment is not\nrandomized, sensitivity analysis helps investigators determine how sensitive\ntheir estimated treatment effect is to some unmeasured con- founder. The\nstandard approach calibrates the sensitivity analysis according to the worst\ncase bias in a pair. This approach will result in a conservative sensitivity\nanalysis if the worst case bias does not hold in every pair. In this paper, we\nshow that for binary data, the standard approach can be calibrated in terms of\nthe average bias in a pair rather than worst case bias. When the worst case\nbias and average bias differ, the average bias interpretation results in a less\nconservative sensitivity analysis and more power. In many studies, the average\ncase calibration may also carry a more natural interpretation than the worst\ncase calibration and may also allow researchers to incorporate additional data\nto establish an empirical basis with which to calibrate a sensitivity analysis.\nWe illustrate this with a study of the effects of cellphone use on the\nincidence of automobile accidents. Finally, we extend the average case\ncalibration to the sensitivity analysis of confidence intervals for\nattributable effects.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 18:49:34 GMT"}, {"version": "v2", "created": "Wed, 16 May 2018 20:54:21 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Hasegawa", "Raiden B.", ""], ["Small", "Dylan S.", ""]]}, {"id": "1707.09561", "submitter": "Jelena Bradic", "authors": "Jue Hou, Jelena Bradic, Ronghui Xu", "title": "Fine-Gray competing risks model with high-dimensional covariates:\n  estimation and Inference", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to construct confidence intervals for the\nregression coefficients in the Fine-Gray model for competing risks data with\nrandom censoring, where the number of covariates can be larger than the sample\nsize. Despite strong motivation from biomedical applications, a\nhigh-dimensional Fine-Gray model has attracted relatively little attention\namong the methodological or theoretical literature. We fill in this gap by\ndeveloping confidence intervals based on a one-step bias-correction for a\nregularized estimation. We develop a theoretical framework for the partial\nlikelihood, which does not have independent and identically distributed entries\nand therefore presents many technical challenges. We also study the\napproximation error from the weighting scheme under random censoring for\ncompeting risks and establish new concentration results for time-dependent\nprocesses. In addition to the theoretical results and algorithms, we present\nextensive numerical experiments and an application to a study of non-cancer\nmortality among prostate cancer patients using the linked Medicare-SEER data.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 21:53:35 GMT"}, {"version": "v2", "created": "Mon, 8 Apr 2019 20:03:15 GMT"}], "update_date": "2019-04-10", "authors_parsed": [["Hou", "Jue", ""], ["Bradic", "Jelena", ""], ["Xu", "Ronghui", ""]]}, {"id": "1707.09563", "submitter": "Alexandre Poirier", "authors": "Matthew A. Masten and Alexandre Poirier", "title": "Identification of Treatment Effects under Conditional Partial\n  Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conditional independence of treatment assignment from potential outcomes is a\ncommonly used but nonrefutable assumption. We derive identified sets for\nvarious treatment effect parameters under nonparametric deviations from this\nconditional independence assumption. These deviations are defined via a\nconditional treatment assignment probability, which makes it straightforward to\ninterpret. Our results can be used to assess the robustness of empirical\nconclusions obtained under the baseline conditional independence assumption.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 22:26:56 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Masten", "Matthew A.", ""], ["Poirier", "Alexandre", ""]]}, {"id": "1707.09565", "submitter": "Mohamad Elmasri", "authors": "Kalyan Das, Mohamad Elmasri, Arusharka Sen", "title": "A Skew-Normal Copula-Driven GLMM", "comments": "20 pages, 5 figures, 4 tables", "journal-ref": "Statistica Neerlandica, 70, 396-413, 2016", "doi": "10.1111/stan.12092", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a method for fitting a copula-driven generalized linear\nmixed models. For added flexibility, the skew-normal copula is adopted for\nfitting. The correlation matrix of the skew-normal copula is used to capture\nthe dependence structure within units, while the fixed and random effects\ncoefficients are estimated through the mean of the copula. For estimation, a\nMonte Carlo expectation-maximization algorithm is developed. Simulations are\nshown alongside a real data example from the Framingham Heart Study.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jul 2017 22:38:43 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Das", "Kalyan", ""], ["Elmasri", "Mohamad", ""], ["Sen", "Arusharka", ""]]}, {"id": "1707.09631", "submitter": "Ruoqing Zhu", "authors": "Yifan Cui, Ruoqing Zhu, Mai Zhou, Michael Kosorok", "title": "Consistency of survival tree and forest models: splitting bias and\n  correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random survival forest and survival trees are popular models in statistics\nand machine learning. However, there is a lack of general understanding\nregarding consistency, splitting rules and influence of the censoring\nmechanism. In this paper, we investigate the statistical properties of existing\nmethods from several interesting perspectives. First, we show that traditional\nsplitting rules with censored outcomes rely on a biased estimation of the\nwithin-node failure distribution. To exactly quantify this bias, we develop a\nconcentration bound of the within-node estimation based on non i.i.d. samples\nand apply it to the entire forest. Second, we analyze the entanglement between\nthe failure and censoring distributions caused by univariate splits, and show\nthat without correcting the bias at an internal node, survival tree and forest\nmodels can still enjoy consistency under suitable conditions. In particular, we\ndemonstrate this property under two cases: a finite-dimensional case where the\nsplitting variables and cutting points are chosen randomly, and a\nhigh-dimensional case where the covariates are weakly correlated. Our results\ncan also degenerate into an independent covariate setting, which is commonly\nused in the random forest literature for high-dimensional sparse models.\nHowever, it may not be avoidable that the convergence rate depends on the total\nnumber of variables in the failure and censoring distributions. Third, we\npropose a new splitting rule that compares bias-corrected cumulative hazard\nfunctions at each internal node. We show that the rate of consistency of this\nnew model depends only on the number of failure variables, which improves from\nnon-bias-corrected versions. We perform simulation studies to confirm that this\ncan substantially benefit the prediction error.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 15:24:02 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 03:56:30 GMT"}, {"version": "v3", "created": "Fri, 11 Aug 2017 00:19:56 GMT"}, {"version": "v4", "created": "Mon, 4 Feb 2019 03:17:26 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Cui", "Yifan", ""], ["Zhu", "Ruoqing", ""], ["Zhou", "Mai", ""], ["Kosorok", "Michael", ""]]}, {"id": "1707.09632", "submitter": "Ruoqing Zhu", "authors": "Yifan Cui, Ruoqing Zhu, Michael Kosorok", "title": "Tree based weighted learning for estimating individualized treatment\n  rules with censored data", "comments": "Accepted by EJS", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating individualized treatment rules is a central task for personalized\nmedicine. [zhao2012estimating] and [zhang2012robust] proposed outcome weighted\nlearning to estimate individualized treatment rules directly through maximizing\nthe expected outcome without modeling the response directly. In this paper, we\nextend the outcome weighted learning to right censored survival data without\nrequiring either an inverse probability of censoring weighting or a\nsemiparametric modeling of the censoring and failure times as done in\n[zhao2015doubly]. To accomplish this, we take advantage of the tree based\napproach proposed in [zhu2012recursively] to nonparametrically impute the\nsurvival time in two different ways. The first approach replaces the reward of\neach individual by the expected survival time, while in the second approach\nonly the censored observations are imputed by their conditional expected\nfailure times. We establish consistency and convergence rates for both\nestimators. In simulation studies, our estimators demonstrate improved\nperformance compared to existing methods. We also illustrate the proposed\nmethod on a phase III clinical trial of non-small cell lung cancer.\n", "versions": [{"version": "v1", "created": "Sun, 30 Jul 2017 15:24:14 GMT"}, {"version": "v2", "created": "Fri, 29 Sep 2017 05:15:02 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Cui", "Yifan", ""], ["Zhu", "Ruoqing", ""], ["Kosorok", "Michael", ""]]}, {"id": "1707.09714", "submitter": "Sebastian Kurtek", "authors": "Abhijoy Saha, Karthik Bharath, Sebastian Kurtek", "title": "A Geometric Variational Approach to Bayesian Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Riemannian geometric framework for variational inference\nin Bayesian models based on the nonparametric Fisher-Rao metric on the manifold\nof probability density functions. Under the square-root density representation,\nthe manifold can be identified with the positive orthant of the unit\nhypersphere in L2, and the Fisher-Rao metric reduces to the standard L2 metric.\nExploiting such a Riemannian structure, we formulate the task of approximating\nthe posterior distribution as a variational problem on the hypersphere based on\nthe alpha-divergence. This provides a tighter lower bound on the marginal\ndistribution when compared to, and a corresponding upper bound unavailable\nwith, approaches based on the Kullback-Leibler divergence. We propose a novel\ngradient-based algorithm for the variational problem based on Frechet\nderivative operators motivated by the geometry of the Hilbert sphere, and\nexamine its properties. Through simulations and real-data applications, we\ndemonstrate the utility of the proposed geometric framework and algorithm on\nseveral Bayesian models.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 03:59:00 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 18:32:39 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Saha", "Abhijoy", ""], ["Bharath", "Karthik", ""], ["Kurtek", "Sebastian", ""]]}, {"id": "1707.09867", "submitter": "Nicolas Dobigeon", "authors": "Yanna Cruz Cavalcanti, Thomas Oberlin, Nicolas Dobigeon, Simon Stute,\n  Maria Ribeiro, Clovis Tauber", "title": "Unmixing dynamic PET images with variable specific binding kinetics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To analyze dynamic positron emission tomography (PET) images, various generic\nmultivariate data analysis techniques have been considered in the literature,\nsuch as principal component analysis (PCA), independent component analysis\n(ICA), factor analysis and nonnegative matrix factorization (NMF).\nNevertheless, these conventional approaches neglect any possible nonlinear\nvariations in the time activity curves describing the kinetic behavior of\ntissues with specific binding, which limits their ability to recover a\nreliable, understandable and interpretable description of the data. This paper\nproposes an alternative analysis paradigm that accounts for spatial\nfluctuations in the exchange rate of the tracer between a free compartment and\na specifically bound ligand compartment. The method relies on the concept of\nlinear unmixing, usually applied on the hyperspectral domain, which combines\nNMF with a sum-to-one constraint that ensures an exhaustive description of the\nmixtures. The spatial variability of the signature corresponding to the\nspecific binding tissue is explicitly modeled through a perturbed component.\nThe performance of the method is assessed on both synthetic and real data and\nis shown to compete favorably when compared to other conventional analysis\nmethods. The proposed method improved both factor estimation and proportions\nextraction for specific binding. Modeling the variability of the specific\nbinding factor has a strong potential impact for dynamic PET image analysis.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jul 2017 12:17:57 GMT"}, {"version": "v2", "created": "Sat, 9 Dec 2017 13:33:48 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Cavalcanti", "Yanna Cruz", ""], ["Oberlin", "Thomas", ""], ["Dobigeon", "Nicolas", ""], ["Stute", "Simon", ""], ["Ribeiro", "Maria", ""], ["Tauber", "Clovis", ""]]}, {"id": "1707.09902", "submitter": "Christopher Marcum", "authors": "Carter T. Butts and Christopher Steven Marcum", "title": "A Relational Event Approach to Modeling Behavioral Dynamics", "comments": null, "journal-ref": "In Andrew Pilny and Marshall Scott Poole, eds. Group Processes,\n  pp. 51-92. Springer International Publishing, 2017", "doi": "10.1007/978-3-319-48941-4_4", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter provides an introduction to the analysis of relational event\ndata (i.e., actions, interactions, or other events involving multiple actors\nthat occur over time) within the R/statnet platform. We begin by reviewing the\nbasics of relational event modeling, with an emphasis on models with piecewise\nconstant hazards. We then discuss estimation for dyadic and more general\nrelational event models using the relevent package, with an emphasis on\nhands-on applications of the methods and interpretation of results. Statnet is\na collection of packages for the R statistical computing system that supports\nthe representation, manipulation, visualization, modeling, simulation, and\nanalysis of relational data. Statnet packages are contributed by a team of\nvolunteer developers, and are made freely available under the GNU Public\nLicense. These packages are written for the R statistical computing\nenvironment, and can be used with any computing platform that supports R\n(including Windows, Linux, and Mac).\"\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 14:58:00 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Butts", "Carter T.", ""], ["Marcum", "Christopher Steven", ""]]}, {"id": "1707.09974", "submitter": "Arabin Kumar Dey", "authors": "Arabin Kumar Dey and Biplab Paul", "title": "Some variations of EM algorithms for Marshall-Olkin bivariate Pareto\n  distribution with location and scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently Asimit et. al used an EM algorithm to estimate Marshall-Olkin\nbivariate Pareto distribution. The distribution has seven parameters. We\ndescribe few alternative approaches of EM algorithm. A numerical simulation is\nperformed to verify the performance of different proposed algorithms. A\nreal-life data analysis is also shown for illustrative purposes.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 17:40:40 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Dey", "Arabin Kumar", ""], ["Paul", "Biplab", ""]]}]