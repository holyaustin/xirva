[{"id": "1506.00053", "submitter": "Panagiotis Tsilifis", "authors": "Panagiotis Tsilifis, Roger G. Ghanem and Paris Hajali", "title": "Efficient Bayesian experimentation using an expected information gain\n  lower bound", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.geo-ph stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experimental design is crucial for inference where limitations in the data\ncollection procedure are present due to cost or other restrictions. Optimal\nexperimental designs determine parameters that in some appropriate sense make\nthe data the most informative possible. In a Bayesian setting this is\ntranslated to updating to the best possible posterior. Information theoretic\narguments have led to the formation of the expected information gain as a\ndesign criterion. This can be evaluated mainly by Monte Carlo sampling and\nmaximized by using stochastic approximation methods, both known for being\ncomputationally expensive tasks. We propose a framework where a lower bound of\nthe expected information gain is used as an alternative design criterion. In\naddition to alleviating the computational burden, this also addresses issues\nconcerning estimation bias. The problem of permeability inference in a large\ncontaminated area is used to demonstrate the validity of our approach where we\nemploy the massively parallel version of the multiphase multicomponent\nsimulator TOUGH2 to simulate contaminant transport and a Polynomial Chaos\napproximation of the forward model that further accelerates the objective\nfunction evaluations. The proposed methodology is demonstrated to a setting\nwhere field measurements are available.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 01:17:38 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 18:34:41 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Tsilifis", "Panagiotis", ""], ["Ghanem", "Roger G.", ""], ["Hajali", "Paris", ""]]}, {"id": "1506.00137", "submitter": "Daniel Gervini", "authors": "Daniel Gervini", "title": "Independent component models for replicated point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a semiparametric independent-component model for the intensity\nfunctions of a point process. When independent replications of the process are\navailable, we show that the estimators are consistent and asymptotically\nnormal. We study the finite-sample behavior of the estimators by simulation,\nand as an example of application we analyze the spatial distribution of street\nrobberies in the city of Chicago.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 16:10:31 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Gervini", "Daniel", ""]]}, {"id": "1506.00219", "submitter": "Nikolai Slavov", "authors": "Alexander Franks, Edoardo Airoldi, Nikolai Slavov", "title": "Post-transcriptional regulation across human tissues", "comments": "30 pages, 4 figures", "journal-ref": "PLoS Comput Biol 13(5): e1005535 (2017)", "doi": "10.1371/journal.pcbi.1005535", "report-no": null, "categories": "q-bio.GN q-bio.QM q-bio.TO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transcriptional and post-transcriptional regulation shape\ntissue-type-specific proteomes, but their relative contributions remain\ncontested. Estimates of the factors determining protein levels in human tissues\ndo not distinguish between (i) the factors determining the variability between\nthe abundances of different proteins, i.e., mean-level-variability and, (ii)\nthe factors determining the physiological variability of the same protein\nacross different tissue types, i.e., across-tissues variability. We sought to\nestimate the contribution of transcript levels to these two orthogonal sources\nof variability, and found that scaled mRNA levels can account for most of the\nmean-level-variability but not necessarily for across-tissues variability. The\nreliable quantification of the latter estimate is limited by substantial\nmeasurement noise. However, protein-to-mRNA ratios exhibit substantial\nacross-tissues variability that is functionally concerted and reproducible\nacross different datasets, suggesting extensive post-transcriptional\nregulation. These results caution against estimating protein fold-changes from\nmRNA fold-changes between different cell-types, and highlight the contribution\nof post-transcriptional regulation to shaping tissue-type-specific proteomes.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 11:52:05 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 15:38:55 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Franks", "Alexander", ""], ["Airoldi", "Edoardo", ""], ["Slavov", "Nikolai", ""]]}, {"id": "1506.00438", "submitter": "Aravind Rajeswaran", "authors": "Aravind Rajeswaran and Shankar Narasimhan", "title": "Network Topology Identification using PCA and its Graph Theoretic\n  Interpretations", "comments": "Structure of paper is changed to improve presentation. Methods and\n  results are unchanged. A more detailed literature survey has been added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.DM cs.SY stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve the problem of identifying (reconstructing) network topology from\nsteady state network measurements. Concretely, given only a data matrix\n$\\mathbf{X}$ where the $X_{ij}$ entry corresponds to flow in edge $i$ in\nconfiguration (steady-state) $j$, we wish to find a network structure for which\nflow conservation is obeyed at all the nodes. This models many network problems\ninvolving conserved quantities like water, power, and metabolic networks. We\nshow that identification is equivalent to learning a model $\\mathbf{A_n}$ which\ncaptures the approximate linear relationships between the different variables\ncomprising $\\mathbf{X}$ (i.e. of the form $\\mathbf{A_n X \\approx 0}$) such that\n$\\mathbf{A_n}$ is full rank (highest possible) and consistent with a network\nnode-edge incidence structure. The problem is solved through a sequence of\nsteps like estimating approximate linear relationships using Principal\nComponent Analysis, obtaining f-cut-sets from these approximate relationships,\nand graph realization from f-cut-sets (or equivalently f-circuits). Each step\nand the overall process is polynomial time. The method is illustrated by\nidentifying topology of a water distribution network. We also study the extent\nof identifiability from steady-state data.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 10:57:00 GMT"}, {"version": "v2", "created": "Thu, 21 Jan 2016 18:31:42 GMT"}], "update_date": "2016-01-22", "authors_parsed": [["Rajeswaran", "Aravind", ""], ["Narasimhan", "Shankar", ""]]}, {"id": "1506.00461", "submitter": "Chu Mai", "authors": "Chu V. Mai, Bruno Sudret", "title": "Hierarchical adaptive polynomial chaos expansions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Polynomial chaos expansions (PCE) are widely used in the framework of\nuncertainty quantification. However, when dealing with high dimensional complex\nproblems, challenging issues need to be faced. For instance, high-order\npolynomials may be required, which leads to a large polynomial basis whereas\nusually only a few of the basis functions are in fact significant. Taking into\naccount the sparse structure of the model, advanced techniques such as sparse\nPCE (SPCE), have been recently proposed to alleviate the computational issue.\nIn this paper, we propose a novel approach to SPCE, which allows one to exploit\nthe model's hierarchical structure. The proposed approach is based on the\nadaptive enrichment of the polynomial basis using the so-called principle of\nheredity. As a result, one can reduce the computational burden related to a\nlarge pre-defined candidate set while obtaining higher accuracy with the same\ncomputational budget.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 12:06:57 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Mai", "Chu V.", ""], ["Sudret", "Bruno", ""]]}, {"id": "1506.00559", "submitter": "Faustino Prieto", "authors": "Jos\\'e Mar\\'ia Sarabia, Emilio G\\'omez-D\\'eniz, Faustino Prieto,\n  Vanesa Jord\\'a", "title": "Risks aggregation in multivariate dependent Pareto distributions", "comments": "This is a preprint (26 pages, 4 tables, 6 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we obtain closed expressions for the probability distribution\nfunction, when we consider aggregated risks with multivariate dependent Pareto\ndistributions. We work with the dependent multivariate Pareto type II proposed\nby Arnold (1983, 2015), which is widely used in insurance and risk analysis. We\nbegin with the individual risk model, where we obtain the probability density\nfunction (PDF), which corresponds to a second kind beta distribution. We obtain\nseveral risk measures including the VaR, TVaR and other tail measures. Then, we\nconsider collective risk model based on dependence, where several general\nproperties are studied. We study in detail some relevant collective models with\nPoisson, negative binomial and logarithmic distributions as primary\ndistributions. In the collective Pareto-Poisson model, the PDF is a function of\nthe Kummer confluent hypergeometric function, and in the Pareto-negative\nbinomial is a function of the Gauss hypergeometric function. Using the data set\nbased on one-year vehicle insurance policies taken out in 2004-2005 (Jong and\nHeller, 2008), we conclude that our collective dependent models outperform the\nclassical collective models Poisson-exponential and geometric-exponential in\nterms of the AIC and CAIC statistics.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 16:19:20 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Sarabia", "Jos\u00e9 Mar\u00eda", ""], ["G\u00f3mez-D\u00e9niz", "Emilio", ""], ["Prieto", "Faustino", ""], ["Jord\u00e1", "Vanesa", ""]]}, {"id": "1506.00691", "submitter": "Chao Gao", "authors": "Mengjie Chen, Chao Gao, Zhao Ren", "title": "Robust Covariance and Scatter Matrix Estimation under Huber's\n  Contamination Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Covariance matrix estimation is one of the most important problems in\nstatistics. To accommodate the complexity of modern datasets, it is desired to\nhave estimation procedures that not only can incorporate the structural\nassumptions of covariance matrices, but are also robust to outliers from\narbitrary sources. In this paper, we define a new concept called matrix depth\nand then propose a robust covariance matrix estimator by maximizing the\nempirical depth function. The proposed estimator is shown to achieve minimax\noptimal rate under Huber's $\\epsilon$-contamination model for estimating\ncovariance/scatter matrices with various structures including bandedness and\nsparsity.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 22:08:52 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 16:06:57 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2015 19:38:24 GMT"}, {"version": "v4", "created": "Mon, 12 Jun 2017 15:58:11 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Chen", "Mengjie", ""], ["Gao", "Chao", ""], ["Ren", "Zhao", ""]]}, {"id": "1506.00728", "submitter": "Li Liu", "authors": "Li Liu, Jing Lei, Kathryn Roeder", "title": "Network assisted analysis to reveal the genetic basis of autism", "comments": "Published at http://dx.doi.org/10.1214/15-AOAS844 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 3, 1571-1600", "doi": "10.1214/15-AOAS844", "report-no": "IMS-AOAS-AOAS844", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While studies show that autism is highly heritable, the nature of the genetic\nbasis of this disorder remains illusive. Based on the idea that highly\ncorrelated genes are functionally interrelated and more likely to affect risk,\nwe develop a novel statistical tool to find more potentially autism risk genes\nby combining the genetic association scores with gene co-expression in specific\nbrain regions and periods of development. The gene dependence network is\nestimated using a novel partial neighborhood selection (PNS) algorithm, where\nnode specific properties are incorporated into network estimation for improved\nstatistical and computational efficiency. Then we adopt a hidden Markov random\nfield (HMRF) model to combine the estimated network and the genetic association\nscores in a systematic manner. The proposed modeling framework can be naturally\nextended to incorporate additional structural information concerning the\ndependence between genes. Using currently available genetic association data\nfrom whole exome sequencing studies and brain gene expression levels, the\nproposed algorithm successfully identified 333 genes that plausibly affect\nautism risk.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 02:21:49 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 03:37:51 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 12:58:52 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Liu", "Li", ""], ["Lei", "Jing", ""], ["Roeder", "Kathryn", ""]]}, {"id": "1506.00829", "submitter": "Sarah Filippi", "authors": "Sarah Filippi and Chris Holmes", "title": "A Bayesian nonparametric approach to testing for dependence between\n  random variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonparametric and nonlinear measures of statistical dependence between pairs\nof random variables are important tools in modern data analysis. In particular\nthe emergence of large data sets can now support the relaxation of linearity\nassumptions implicit in traditional association scores such as correlation.\nHere we describe a Bayesian nonparametric procedure that leads to a tractable,\nexplicit and analytic quantification of the relative evidence for dependence vs\nindependence. Our approach uses Polya tree priors on the space of probability\nmeasures which can then be embedded within a decision theoretic test for\ndependence. Polya tree priors can accommodate known uncertainty in the form of\nthe underlying sampling distribution and provides an explicit posterior\nprobability measure of both dependence and independence. Well known advantages\nof having an explicit probability measure include: easy comparison of evidence\nacross different studies; encoding prior information; quantifying changes in\ndependence across different experimental conditions, and; the integration of\nresults within formal decision analysis.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 10:24:46 GMT"}, {"version": "v2", "created": "Thu, 12 May 2016 13:44:51 GMT"}], "update_date": "2016-05-13", "authors_parsed": [["Filippi", "Sarah", ""], ["Holmes", "Chris", ""]]}, {"id": "1506.00878", "submitter": "Ganggang Xu", "authors": "Ganggang Xu, Marc G. Genton", "title": "Efficient Maximum Approximated Likelihood Inference for Tukey's g-and-h\n  Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tukey's $g$-and-$h$ distribution has been a powerful tool for data\nexploration and modeling since its introduction. However, two long standing\nchallenges associated with this distribution family have remained unsolved\nuntil this day: how to find an optimal estimation procedure and how to make\nvalid statistical inference on unknown parameters. To overcome these two\nchallenges, a computationally efficient estimation procedure based on\nmaximizing an approximated likelihood function of the Tukey's $g$-and-$h$\ndistribution is proposed and is shown to have the same estimation efficiency as\nthe maximum likelihood estimator under mild conditions. The asymptotic\ndistribution of the proposed estimator is derived and a series of approximated\nlikelihood ratio test statistics are developed to conduct hypothesis tests\ninvolving two shape parameters of Tukey's $g$-and-$h$ distribution. Simulation\nexamples and an analysis of air pollution data are used to demonstrate the\neffectiveness of the proposed estimation and testing procedures.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 13:38:22 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Xu", "Ganggang", ""], ["Genton", "Marc G.", ""]]}, {"id": "1506.01194", "submitter": "Marie-Colette van Lieshout", "authors": "M.N.M. van Lieshout", "title": "State estimation for temporal point processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with combined inference for point processes on the\nreal line observed in a broken interval. For such processes, the classic\nhistory-based approach cannot be used. Instead, we adapt tools from sequential\nspatial point processes. For a range of models, the marginal and conditional\ndistributions are derived. We discuss likelihood based inference as well as\nparameter estimation using the method of moments, conduct a simulation study\nfor the important special case of renewal processes and analyse a data set\ncollected by Diggle and Hawtin.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 10:18:20 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["van Lieshout", "M. N. M.", ""]]}, {"id": "1506.01223", "submitter": "Viktoria \\\"Ollerer", "authors": "Viktoria \\\"Ollerer, Andreas Alfons, Christophe Croux", "title": "The shooting S-estimator for robust regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform multiple regression, the least squares estimator is commonly used.\nHowever, this estimator is not robust to outliers. Therefore, robust methods\nsuch as S-estimation have been proposed. These estimators flag any observation\nwith a large residual as an outlier and downweight it in the further procedure.\nHowever, a large residual may be caused by an outlier in only one single\npredictor variable, and downweighting the complete observation results in a\nloss of information.\n  Therefore, we propose the shooting S-estimator, a regression estimator that\nis especially designed for situations where a large number of observations\nsuffer from contamination in a small number of predictor variables. The\nshooting S-estimator combines the ideas of the coordinate descent algorithm\nwith simple S-regression, which makes it robust against componentwise\ncontamination, at the cost of failing the regression equivariance property.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 12:00:29 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["\u00d6llerer", "Viktoria", ""], ["Alfons", "Andreas", ""], ["Croux", "Christophe", ""]]}, {"id": "1506.01332", "submitter": "James Long", "authors": "James P. Long, Jianhua Z. Huang", "title": "A Study of Functional Depths", "comments": "25 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional depth is used for ranking functional observations from most\noutlying to most typical. The ranks produced by functional depth have been\nproposed as the basis for functional classifiers, rank tests, and data\nvisualization procedures. Many of the proposed functional depths are invariant\nto domain permutation, an unusual property for a functional data analysis\nprocedure. Essentially these depths treat functional data as if it were\nmultivariate data. In this work, we compare the performance of several existing\nfunctional depths to a simple adaptation of an existing multivariate depth\nnotion, $L^\\infty$ depth ($L^{\\infty}D$). On simulated and real data, we show\n$L^{\\infty}D$ has performance comparable or superior to several existing\nnotions of functional depth. In addition, we review how depth functions are\nevaluated and propose some improvements. In particular, we show that empirical\ndepth function asymptotics can be mis--leading and instead propose a new\nmethod, the rank--rank plot, for evaluating empirical depth rank stability.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 17:50:28 GMT"}, {"version": "v2", "created": "Mon, 28 Sep 2015 19:38:58 GMT"}, {"version": "v3", "created": "Tue, 1 Nov 2016 14:45:20 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Long", "James P.", ""], ["Huang", "Jianhua Z.", ""]]}, {"id": "1506.01407", "submitter": "Shaojun Guo", "authors": "Shaojun Guo, John Box and Wenyang Zhang", "title": "A Dynamic Structure for High Dimensional Covariance Matrices and its\n  Application in Portfolio Allocation", "comments": "38 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of high dimensional covariance matrices is an interesting and\nimportant research topic. In this paper, we propose a dynamic structure and\ndevelop an estimation procedure for high dimensional covariance matrices.\nAsymptotic properties are derived to justify the estimation procedure and\nsimulation studies are conducted to demonstrate its performance when the sample\nsize is finite. By exploring a financial application, an empirical study shows\nthat portfolio allocation based on dynamic high dimensional covariance matrices\ncan significantly outperform the market from 1995 to 2014. Our proposed method\nalso outperforms portfolio allocation based on the sample covariance matrix and\nthe portfolio allocation proposed in Fan, Fan and Lv (2008).\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 21:07:05 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Guo", "Shaojun", ""], ["Box", "John", ""], ["Zhang", "Wenyang", ""]]}, {"id": "1506.01567", "submitter": "Felix Abramovich", "authors": "Felix Abramovich and Marianna Pensky", "title": "Classification with many classes: challenges and pluses", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of the paper is to study accuracy of multi-class classification\nin high-dimensional setting, where the number of classes is also large (\"large\n$L$, large $p$, small $n$\" model). While this problem arises in many practical\napplications and many techniques have been recently developed for its solution,\nto the best of our knowledge nobody provided a rigorous theoretical analysis of\nthis important setup. The purpose of the present paper is to fill in this gap.\n  We consider one of the most common settings, classification of\nhigh-dimensional normal vectors where, unlike standard assumptions, the number\nof classes could be large. We derive non-asymptotic conditions on effects of\nsignificant features, and the low and the upper bounds for distances between\nclasses required for successful feature selection and classification with a\ngiven accuracy. Furthermore, we study an asymptotic setup where the number of\nclasses is diverging with the dimension of feature space and while the number\nof samples per class is possibly limited. We point out on an interesting and,\nat first glance, somewhat counter-intuitive phenomenon that a large number of\nclasses may be a \"blessing\" rather than a \"curse\" since, in certain settings,\nthe precision of classification can improve as the number of classes grows.\nThis is due to more accurate feature selection since even weaker significant\nfeatures, which are not sufficiently strong to be manifested in a coarse\nclassification, being shared across the classes, have a stronger impact as the\nnumber of classes increases. We supplement our theoretical investigation by a\nsimulation study and a real data example where we again observe the above\nphenomenon.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 12:57:19 GMT"}, {"version": "v2", "created": "Wed, 29 Mar 2017 07:32:26 GMT"}, {"version": "v3", "created": "Sun, 12 Nov 2017 10:48:37 GMT"}, {"version": "v4", "created": "Wed, 17 Jul 2019 12:25:41 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Abramovich", "Felix", ""], ["Pensky", "Marianna", ""]]}, {"id": "1506.01583", "submitter": "Mireille Schnitzer", "authors": "Mireille E. Schnitzer, Russell J. Steele, Mich\\`ele Bally, Ian Shrier", "title": "A causal inference approach to network meta-analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While standard meta-analysis pools the results from randomized trials that\ncompare two treatments, network meta-analysis aggregates the results of\nrandomized trials comparing a wider variety of treatment options. However, it\nis unclear whether the aggregation of effect estimates across heterogeneous\npopulations will be consistent for a meaningful parameter when not all\ntreatments are evaluated on each population. Drawing from counterfactual theory\nand the causal inference framework, we define the population of interest in a\nnetwork meta-analysis and define the target parameter under a series of\nnonparametric structural assumptions. This allows us to determine the\nrequirements for identifiability of this parameter, enabling a description of\nthe conditions under which network meta-analysis is appropriate and when it\nmight mislead decision making. We then adapt several modeling strategies from\nthe causal inference literature to obtain consistent estimation of the\nintervention-specific mean outcome and model-independent contrasts between\ntreatments. Finally, we perform a reanalysis of a systematic review to compare\nthe efficacy of antibiotics on suspected or confirmed methicillin-resistant\n\\emph{Staphylococcus aureus} in hospitalized patients.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 13:27:39 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2016 21:48:42 GMT"}], "update_date": "2016-08-15", "authors_parsed": [["Schnitzer", "Mireille E.", ""], ["Steele", "Russell J.", ""], ["Bally", "Mich\u00e8le", ""], ["Shrier", "Ian", ""]]}, {"id": "1506.01646", "submitter": "Tom\\'a\\v{s} Mrkvi\\v{c}ka", "authors": "Tom\\'a\\v{s} Mrkvi\\v{c}ka, Mari Myllym\\\"aki, Ute Hahn", "title": "Multiple Monte Carlo Testing with Applications in Spatial Point\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rank envelope test (Myllym\\\"aki et al., Global envelope tests for spatial\nprocesses, arXiv:1307.0239 [stat.ME]) is proposed as a solution to multiple\ntesting problem for Monte Carlo tests. Three different situations are\nrecognized: 1) a few univariate Monte Carlo tests, 2) a Monte Carlo test with a\nfunction as the test statistic, 3) several Monte Carlo tests with functions as\ntest statistics. The rank test has correct (global) type I error in each case\nand it is accompanied with a $p$-value and with a graphical interpretation\nwhich shows which subtest or which distances of the used test function(s) lead\nto the rejection at the prescribed significance level of the test. Examples of\nnull hypothesis from point process and random set statistics are used to\ndemonstrate the strength of the rank envelope test. The examples include\ngoodness-of-fit test with several test functions, goodness-of-fit test for one\ngroup of point patterns, comparison of several groups of point patterns, test\nof dependence of components in a multi-type point pattern, and test of Boolean\nassumption for random closed sets.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 16:40:31 GMT"}], "update_date": "2015-06-05", "authors_parsed": [["Mrkvi\u010dka", "Tom\u00e1\u0161", ""], ["Myllym\u00e4ki", "Mari", ""], ["Hahn", "Ute", ""]]}, {"id": "1506.01782", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang and Chenlei Leng", "title": "High-dimensional Ordinary Least-squares Projection for Screening\n  Variables", "comments": "To appear in JRSS-B", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection is a challenging issue in statistical applications when\nthe number of predictors $p$ far exceeds the number of observations $n$. In\nthis ultra-high dimensional setting, the sure independence screening (SIS)\nprocedure was introduced to significantly reduce the dimensionality by\npreserving the true model with overwhelming probability, before a refined\nsecond stage analysis. However, the aforementioned sure screening property\nstrongly relies on the assumption that the important variables in the model\nhave large marginal correlations with the response, which rarely holds in\nreality. To overcome this, we propose a novel and simple screening technique\ncalled the high-dimensional ordinary least-squares projection (HOLP). We show\nthat HOLP possesses the sure screening property and gives consistent variable\nselection without the strong correlation assumption, and has a low\ncomputational complexity. A ridge type HOLP procedure is also discussed.\nSimulation study shows that HOLP performs competitively compared to many other\nmarginal correlation based methods. An application to a mammalian eye disease\ndata illustrates the attractiveness of HOLP.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 05:39:38 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Wang", "Xiangyu", ""], ["Leng", "Chenlei", ""]]}, {"id": "1506.01917", "submitter": "Patrick Schmidt", "authors": "Patrick Schmidt, Matthias Katzfu{\\ss}, Tilmann Gneiting", "title": "Interpretation of point forecasts with unkown directive", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point forecasts can be interpreted as functionals (i.e., point summaries) of\npredictive distributions. We consider the situation where forecasters'\ndirectives are hidden and develop methodology for the identification of the\nunknown functional based on time series data of point forecasts and associated\nrealizations. Focusing on the natural cases of state-dependent quantiles and\nexpectiles, we provide a generalized method of moments estimator for the\nfunctional, along with tests of optimality relative to information sets that\nare specified by instrumental variables. Using simulation, we demonstrate that\nour optimality test is better calibrated and more powerful than existing\nsolutions. In empirical examples, Greenbook gross domestic product (GDP)\nforecasts of the US Federal Reserve and model output for precipitation from the\nEuropean Centre for Medium-Range Weather Forecasts (ECMWF) are indicative of\noverstatement in anticipation of extreme events.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 13:55:08 GMT"}, {"version": "v2", "created": "Mon, 12 Sep 2016 09:47:24 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 17:54:27 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Schmidt", "Patrick", ""], ["Katzfu\u00df", "Matthias", ""], ["Gneiting", "Tilmann", ""]]}, {"id": "1506.02166", "submitter": "Diaa Al Mohamad", "authors": "Diaa Al Mohamad", "title": "Towards a better understanding of the dual representation of phi\n  divergences", "comments": "Working paper. It only lacks some revisions before submitting", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to study different estimation procedures based on\n$\\varphi-$divergences. The dual representation of $\\varphi-$divergences based\non the Fenchel-Legendre duality is the main interest of this study. It provides\na way to estimate $\\varphi-$divergences by a simple plug-in of the empirical\ndistribution without any smoothing technique. Resulting estimators are\nthoroughly studied theoretically and with simulations showing that the so\ncalled minimum $\\varphi-$divergence estimator (MD$\\varphi$DE) is generally non\nrobust and behaves similarly to the maximum likelihood estimator. We give some\narguments supporting the non robustness property, and give insights on how to\nmodify the classical approach. An alternative class of $\\varphi-$divergences\nrobust estimators based on the dual representation is presented. We study\nconsistency and robustness properties from an influence function point of view\nof the new estimators. In a second part, we invoke the Basu-Lindsay approach\nfor approximating $\\varphi-$divergences and provide a comparison between these\napproaches. The so called dual $\\varphi-$divergence is also discussed and\ncompared to our new estimator. A full simulation study of all these approaches\nis given in order to compare efficiency and robustness of all mentioned\nestimators against the so-called minimum density power divergence, showing\nencouraging results in favor of our new class of minimum dual\n$\\varphi-$divergences.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 15:53:11 GMT"}, {"version": "v2", "created": "Sat, 10 Oct 2015 12:05:34 GMT"}], "update_date": "2015-10-13", "authors_parsed": [["Mohamad", "Diaa Al", ""]]}, {"id": "1506.02222", "submitter": "Xiangyu Wang", "authors": "Xiangyu Wang, David Dunson and Chenlei Leng", "title": "No penalty no tears: Least squares in high-dimensional linear models", "comments": "Added results for non-sparse models; Added results for elliptical\n  distribution; Added simulations for adaptive lasso", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordinary least squares (OLS) is the default method for fitting linear models,\nbut is not applicable for problems with dimensionality larger than the sample\nsize. For these problems, we advocate the use of a generalized version of OLS\nmotivated by ridge regression, and propose two novel three-step algorithms\ninvolving least squares fitting and hard thresholding. The algorithms are\nmethodologically simple to understand intuitively, computationally easy to\nimplement efficiently, and theoretically appealing for choosing models\nconsistently. Numerical exercises comparing our methods with penalization-based\napproaches in simulations and data analyses illustrate the great potential of\nthe proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 05:45:24 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2015 03:31:06 GMT"}, {"version": "v3", "created": "Fri, 9 Oct 2015 21:30:39 GMT"}, {"version": "v4", "created": "Mon, 23 Nov 2015 09:21:37 GMT"}, {"version": "v5", "created": "Thu, 16 Jun 2016 07:13:40 GMT"}], "update_date": "2016-06-17", "authors_parsed": [["Wang", "Xiangyu", ""], ["Dunson", "David", ""], ["Leng", "Chenlei", ""]]}, {"id": "1506.02273", "submitter": "Michael Betancourt", "authors": "Michael Betancourt", "title": "A Unified Treatment of Predictive Model Comparison", "comments": "20 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predictive performance of any inferential model is critical to its\npractical success, but quantifying predictive performance is a subtle\nstatistical problem. In this paper I show how the natural structure of any\ninferential problem defines a canonical measure of relative predictive\nperformance and then demonstrate how approximations of this measure yield many\nof the model comparison techniques popular in statistics and machine learning.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 15:12:40 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Betancourt", "Michael", ""]]}, {"id": "1506.02278", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Shirley Ho, Larry Wasserman", "title": "Optimal Ridge Detection using Coverage Risk", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of coverage risk as an error measure for density\nridge estimation. The coverage risk generalizes the mean integrated square\nerror to set estimation. We propose two risk estimators for the coverage risk\nand we show that we can select tuning parameters by minimizing the estimated\nrisk. We study the rate of convergence for coverage risk and prove consistency\nof the risk estimators. We apply our method to three simulated datasets and to\ncosmology data. In all the examples, the proposed method successfully recover\nthe underlying density structure.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 15:52:36 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Ho", "Shirley", ""], ["Wasserman", "Larry", ""]]}, {"id": "1506.02494", "submitter": "Christina Heinze", "authors": "Dominik Rothenh\\\"ausler, Christina Heinze, Jonas Peters and Nicolai\n  Meinshausen", "title": "backShift: Learning causal cyclic graphs from unknown shift\n  interventions", "comments": null, "journal-ref": "Advances in Neural Information Processing Systems 28 (2015)\n  1513-1521", "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple method to learn linear causal cyclic models in the\npresence of latent variables. The method relies on equilibrium data of the\nmodel recorded under a specific kind of interventions (\"shift interventions\").\nThe location and strength of these interventions do not have to be known and\ncan be estimated from the data. Our method, called backShift, only uses second\nmoments of the data and performs simple joint matrix diagonalization, applied\nto differences between covariance matrices. We give a sufficient and necessary\ncondition for identifiability of the system, which is fulfilled almost surely\nunder some quite general assumptions if and only if there are at least three\ndistinct experimental settings, one of which can be pure observational data. We\ndemonstrate the performance on some simulated data and applications in flow\ncytometry and financial time series. The code is made available as R-package\nbackShift.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 13:41:22 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 11:12:23 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2015 12:13:03 GMT"}], "update_date": "2016-01-11", "authors_parsed": [["Rothenh\u00e4usler", "Dominik", ""], ["Heinze", "Christina", ""], ["Peters", "Jonas", ""], ["Meinshausen", "Nicolai", ""]]}, {"id": "1506.02578", "submitter": "Daniel Vogel", "authors": "Alexander D\\\"urre, Daniel Vogel", "title": "Asymptotics of the two-stage spatial sign correlation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spatial sign correlation (D\\\"urre, Vogel and Fried, 2015) is a highly\nrobust and easy-to-compute, bivariate correlation estimator based on the\nspatial sign covariance matrix. Since the estimator is inefficient when the\nmarginal scales strongly differ, a two-stage version was proposed. In the first\nstep, the observations are marginally standardized by means of a robust scale\nestimator, and in the second step, the spatial sign correlation of the thus\ntransformed data set is computed. D\\\"urre et al. (2015) give some evidence that\nthe asymptotic distribution of the two-stage estimator equals that of the\nspatial sign correlation at equal marginal scales by comparing their influence\nfunctions and presenting simulation results, but give no formal proof. In the\npresent paper, we close this gap and establish the asymptotic normality of the\ntwo-stage spatial sign correlation and compute its asymptotic variance for\nelliptical population distributions. We further derive a variance-stabilizing\ntransformation, similar to Fisher's z-transform, and numerically compare the\nsmall-sample coverage probabilities of several confidence intervals.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 16:36:12 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["D\u00fcrre", "Alexander", ""], ["Vogel", "Daniel", ""]]}, {"id": "1506.02685", "submitter": "Jaewoo Park", "authors": "Joshua Goldstein, Jaewoo Park, Murali Haran, Andrew Liebhold and Ottar\n  N. Bjornstad", "title": "Quantifying Spatio-Temporal Variation of Invasion Spread", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spread of invasive species can have far reaching environmental and\necological consequences. Understanding invasion spread patterns and the\nunderlying process driving invasions are key to predicting and managing\ninvasions. We combine a set of statistical methods in a novel way to\ncharacterize local spread properties and demonstrate their application using\nsimulated and historical data on invasive insects. Our method uses a Gaussian\nprocess fit to the surface of waiting times to invasion in order to\ncharacterize the vector field of spread. Using this method we estimate with\nstatistical uncertainties the speed and direction of spread at each location.\nSimulations from a stratified diffusion model verify the accuracy of our\nmethod. We show how we may link local rates of spread to environmental\ncovariates for two case studies: the spread of the gypsy moth (Lymantria\ndispar), and hemlock wolly adelgid (Adelges tsugae) in North America. We\nprovide an R-package that automates the calculations for any spatially\nreferenced waiting time data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 20:24:54 GMT"}, {"version": "v2", "created": "Sun, 4 Mar 2018 23:15:56 GMT"}, {"version": "v3", "created": "Wed, 7 Mar 2018 17:20:42 GMT"}, {"version": "v4", "created": "Wed, 10 Oct 2018 19:28:17 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Goldstein", "Joshua", ""], ["Park", "Jaewoo", ""], ["Haran", "Murali", ""], ["Liebhold", "Andrew", ""], ["Bjornstad", "Ottar N.", ""]]}, {"id": "1506.02824", "submitter": "Fredrik S\\\"avje", "authors": "Fredrik S\\\"avje", "title": "The performance and efficiency of Threshold Blocking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common method to reduce the uncertainty of causal inferences from\nexperiments is to assign treatments in fixed proportions within groups of\nsimilar units: blocking. Previous results indicate that one can expect\nsubstantial reductions in variance if these groups are formed so to contain\nexactly as many units as treatment conditions. This approach can be contrasted\nto threshold blocking which, instead of specifying a fixed size, requires that\nthe groups contain a minimum number of units. In this paper, I investigate the\nadvantages of respective method. In particular, I show that threshold blocking\nis superior to fixed-sized blocking in the sense that it always finds a weakly\nbetter grouping for any objective and sample. However, this does not\nnecessarily hold when the objective function of the blocking problem is\nunknown, and a fixed-sized design can perform better in that case. I\nspecifically examine the factors that govern how the methods perform in the\ncommon situation where the objective is to reduce the estimator's variance, but\nwhere groups are constructed based on covariates. This reveals that the\nrelative performance of threshold blocking improves when the covariates become\nmore predictive of the outcome.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 08:55:36 GMT"}, {"version": "v2", "created": "Fri, 28 Aug 2015 15:06:00 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["S\u00e4vje", "Fredrik", ""]]}, {"id": "1506.02886", "submitter": "Angelina Roche", "authors": "Angelina Roche (MAP5, CEREMADE)", "title": "Local Optimization of Black-Box Function with High or\n  Infinite-Dimensional Inputs", "comments": null, "journal-ref": null, "doi": null, "report-no": "MAP5 2015-11", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An adaptation of Response Surface Methodology (RSM) when the covariate is of\nhigh or infinite dimensional is proposed, providing a tool for black-box\noptimization in this context. We combine dimension reduction techniques with\nclassical multivariate Design of Experiments (DoE). We propose a method to\ngenerate experimental designs and extend usual properties (orthogonality,\nrotatability,...) of multivariate designs to general high or infinite\ndimensional contexts. Different dimension reduction basis are considered\n(including data-driven basis). The methodology is illustrated on simulated\nfunctional data and we discuss the choice of the different parameters, in\nparticular the dimension of the approximation space. The method is finally\napplied to a problem of nuclear safety.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 12:40:53 GMT"}, {"version": "v2", "created": "Fri, 12 Jun 2015 20:47:55 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2015 12:15:24 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Roche", "Angelina", "", "MAP5, CEREMADE"]]}, {"id": "1506.02916", "submitter": "Timothy W. Waite", "authors": "Timothy W. Waite", "title": "Singular prior distributions and ill-conditioning in Bayesian D-optimal\n  design for several nonlinear models", "comments": "38 pages, 1 figure", "journal-ref": "Statistica Sinica, 28, 505-525, 2018", "doi": "10.5705/ss.202015.0293", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Bayesian D-optimal design, we define a singular prior distribution for\nthe model parameters as a prior distribution such that the determinant of the\nFisher information matrix has a prior geometric mean of zero for all designs.\nFor such a prior distribution, the Bayesian D-optimality criterion fails to\nselect a design. For the exponential decay model, we characterize singularity\nof the prior distribution in terms of the expectations of a few elementary\ntransformations of the parameter. For a compartmental model and several\nmulti-parameter generalized linear models, we establish sufficient conditions\nfor singularity of a prior distribution. For the generalized linear models we\nalso obtain sufficient conditions for non-singularity. In the existing\nliterature, weakly informative prior distributions are commonly recommended as\na default choice for inference in logistic regression. Here it is shown that\nsome of the recommended prior distributions are singular, and hence should not\nbe used for Bayesian D-optimal design. Additionally, methods are developed to\nderive and assess Bayesian D-efficient designs when numerical evaluation of the\nobjective function fails due to ill-conditioning, as often occurs for\nheavy-tailed prior distributions. These numerical methods are illustrated for\nlogistic regression.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 14:04:10 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 13:10:44 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 16:40:10 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Waite", "Timothy W.", ""]]}, {"id": "1506.02927", "submitter": "Juliette Spinnato", "authors": "Juliette Spinnato (LNC, I2M), Marie-Christine Roubaud (I2M), Margaux\n  Perrin, Emmanuel Maby, Jeremie Mattout, Boris Burle (LNC), Bruno Torr\\'esani\n  (I2M)", "title": "Analyse discriminante matricielle descriptive. Application a l'\\'etude\n  de signaux EEG", "comments": "in French, Journ{\\'e}es de statistique de la SFDS, Jun 2015, Lille,\n  France", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the descriptive approach to linear discriminant analysis for\nmatrix-variate data in the binary case. Under a separability assumption on row\nand column variability, the most discriminant linear combinations of rows and\ncolumns are determined by the singular value decomposition of the difference of\nthe class-averages with the Mahalanobis metric in the row and column spaces.\nThis approach provides data representations of data in two-dimensional or\nthree-dimensional plots and singles out discriminant components. An application\nto electroencephalographic multi-sensor signals illustrates the relevance of\nthe method.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 14:24:17 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Spinnato", "Juliette", "", "LNC, I2M"], ["Roubaud", "Marie-Christine", "", "I2M"], ["Perrin", "Margaux", "", "LNC"], ["Maby", "Emmanuel", "", "LNC"], ["Mattout", "Jeremie", "", "LNC"], ["Burle", "Boris", "", "LNC"], ["Torr\u00e9sani", "Bruno", "", "I2M"]]}, {"id": "1506.02940", "submitter": "Luca Di Persio", "authors": "Luca Di Persio", "title": "Autoregressive approaches to import-export time series I: basic\n  techniques", "comments": "Published at http://dx.doi.org/10.15559/15-VMSTA22 in the Modern\n  Stochastics: Theory and Applications (https://www.i-journals.org/vtxpp/VMSTA)\n  by VTeX (http://www.vtex.lt/)", "journal-ref": "Modern Stochastics: Theory and Applications 2015, Vol. 2, No. 1,\n  51-65", "doi": "10.15559/15-VMSTA22", "report-no": "VTeX-VMSTA-VMSTA22", "categories": "stat.AP math.PR q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is the first part of a project dealing with an in-depth study of\neffective techniques used in econometrics in order to make accurate forecasts\nin the concrete framework of one of the major economies of the most productive\nItalian area, namely the province of Verona. In particular, we develop an\napproach mainly based on vector autoregressions, where lagged values of two or\nmore variables are considered, Granger causality, and the stochastic trend\napproach useful to work with the cointegration phenomenon. Latter techniques\nconstitute the core of the present paper, whereas in the second part of the\nproject, we present how these approaches can be applied to economic data at our\ndisposal in order to obtain concrete analysis of import--export behavior for\nthe considered productive area of Verona.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 07:23:44 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Di Persio", "Luca", ""]]}, {"id": "1506.03039", "submitter": "Jack Gorham", "authors": "Jackson Gorham and Lester Mackey", "title": "Measuring Sample Quality with Stein's Method", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the efficiency of Monte Carlo estimation, practitioners are\nturning to biased Markov chain Monte Carlo procedures that trade off asymptotic\nexactness for computational speed. The reasoning is sound: a reduction in\nvariance due to more rapid sampling can outweigh the bias introduced. However,\nthe inexactness creates new challenges for sampler and parameter selection,\nsince standard measures of sample quality like effective sample size do not\naccount for asymptotic bias. To address these challenges, we introduce a new\ncomputable quality measure based on Stein's method that quantifies the maximum\ndiscrepancy between sample and target expectations over a large class of test\nfunctions. We use our tool to compare exact, biased, and deterministic sample\nsequences and illustrate applications to hyperparameter selection, convergence\nrate assessment, and quantifying bias-variance tradeoffs in posterior\ninference.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 18:48:58 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 05:15:18 GMT"}, {"version": "v3", "created": "Sat, 12 Sep 2015 23:31:21 GMT"}, {"version": "v4", "created": "Mon, 11 Jan 2016 03:47:27 GMT"}, {"version": "v5", "created": "Mon, 6 Mar 2017 18:59:16 GMT"}, {"version": "v6", "created": "Tue, 1 Jan 2019 03:07:44 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Gorham", "Jackson", ""], ["Mackey", "Lester", ""]]}, {"id": "1506.03159", "submitter": "Dustin Tran", "authors": "Dustin Tran, David M. Blei, Edoardo M. Airoldi", "title": "Copula variational inference", "comments": "Appears in Neural Information Processing Systems, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general variational inference method that preserves dependency\namong the latent variables. Our method uses copulas to augment the families of\ndistributions used in mean-field and structured approximations. Copulas model\nthe dependency that is not captured by the original variational distribution,\nand thus the augmented variational family guarantees better approximations to\nthe posterior. With stochastic optimization, inference on the augmented\ndistribution is scalable. Furthermore, our strategy is generic: it can be\napplied to any inference procedure that currently uses the mean-field or\nstructured approach. Copula variational inference has many advantages: it\nreduces bias; it is less sensitive to local optima; it is less sensitive to\nhyperparameters; and it helps characterize and interpret the dependency among\nthe latent variables.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 04:14:22 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2015 06:52:07 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Tran", "Dustin", ""], ["Blei", "David M.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1506.03162", "submitter": "Erin Conlon", "authors": "Alexey Miroshnikov and Erin M. Conlon", "title": "Parallel Markov Chain Monte Carlo for Non-Gaussian Posterior\n  Distributions", "comments": "32 pages, 3 figures", "journal-ref": "Stat (2015), 4, 304-319", "doi": "10.1002/sta4.97", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in big data and analytics research have produced an\nabundance of large data sets that are too big to be analyzed in their entirety,\ndue to limits on computer memory or storage capacity. To address these issues,\ncommunication-free parallel Markov chain Monte Carlo (MCMC) methods have been\ndeveloped for Bayesian analysis of big data. These methods partition data into\nmanageable subsets, perform independent Bayesian MCMC analysis on each subset,\nand combine the subset posterior samples to estimate the full data posterior.\nCurrent approaches to combining subset posterior samples include sample\naveraging, weighted averaging, and kernel smoothing techniques. Although these\nmethods work well for Gaussian posteriors, they are not well-suited to\nnon-Gaussian posterior distributions. Here, we develop a new direct density\nproduct method for combining subset marginal posterior samples to estimate full\ndata marginal posterior densities. Using a commonly-implemented distance\nmetric, we show in simulation studies of Bayesian models with non-Gaussian\nposteriors that our method outperforms the existing methods in approximating\nthe full data marginal posteriors. Since our method estimates only marginal\ndensities, there is no limitation on the number of model parameters analyzed.\nOur procedure is suitable for Bayesian models with unknown parameters with\nfixed dimension in continuous parameter spaces.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 04:28:22 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Miroshnikov", "Alexey", ""], ["Conlon", "Erin M.", ""]]}, {"id": "1506.03481", "submitter": "Wentao Li", "authors": "Wentao Li and Paul Fearnhead", "title": "On the Asymptotic Efficiency of Approximate Bayesian Computation\n  Estimators", "comments": "Main text shortened and proof revised. To appear in Biometrika", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many statistical applications involve models for which it is difficult to\nevaluate the likelihood, but from which it is relatively easy to sample.\nApproximate Bayesian computation is a likelihood-free method for implementing\nBayesian inference in such cases. We present results on the asymptotic variance\nof estimators obtained using approximate Bayesian computation in a large-data\nlimit. Our key assumption is that the data are summarized by a\nfixed-dimensional summary statistic that obeys a central limit theorem. We\nprove asymptotic normality of the mean of the approximate Bayesian computation\nposterior. This result also shows that, in terms of asymptotic variance, we\nshould use a summary statistic that is the same dimension as the parameter\nvector, p; and that any summary statistic of higher dimension can be reduced,\nthrough a linear transformation, to dimension p in a way that can only reduce\nthe asymptotic variance of the posterior mean. We look at how the Monte Carlo\nerror of an importance sampling algorithm that samples from the approximate\nBayesian computation posterior affects the accuracy of estimators. We give\nconditions on the importance sampling proposal distribution such that the\nvariance of the estimator will be the same order as that of the maximum\nlikelihood estimator based on the summary statistics used. This suggests an\niterative importance sampling algorithm, which we evaluate empirically on a\nstochastic volatility model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 21:05:59 GMT"}, {"version": "v2", "created": "Fri, 4 Mar 2016 13:34:12 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2016 12:01:30 GMT"}, {"version": "v4", "created": "Tue, 28 Nov 2017 10:23:09 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Li", "Wentao", ""], ["Fearnhead", "Paul", ""]]}, {"id": "1506.03486", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani, Aaditya Ramdas", "title": "Sequential Nonparametric Testing with the Law of the Iterated Logarithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithmic framework for sequential hypothesis testing with\ni.i.d. data, which includes A/B testing, nonparametric two-sample testing, and\nindependence testing as special cases. It is novel in several ways: (a) it\ntakes linear time and constant space to compute on the fly, (b) it has the same\npower guarantee as a non-sequential version of the test with the same\ncomputational constraints up to a small factor, and (c) it accesses only as\nmany samples as are required - its stopping time adapts to the unknown\ndifficulty of the problem. All our test statistics are constructed to be\nzero-mean martingales under the null hypothesis, and the rejection threshold is\ngoverned by a uniform non-asymptotic law of the iterated logarithm (LIL). For\nthe case of nonparametric two-sample mean testing, we also provide a finite\nsample power analysis, and the first non-asymptotic stopping time calculations\nfor this class of problems. We verify our predictions for type I and II errors\nand stopping times using simulations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 21:28:38 GMT"}, {"version": "v2", "created": "Wed, 2 Mar 2016 02:11:19 GMT"}], "update_date": "2016-03-03", "authors_parsed": [["Balsubramani", "Akshay", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1506.03541", "submitter": "Yan Sun", "authors": "Yan Sun, Dan Ralescu", "title": "A Linear Model for Interval-valued Data", "comments": "18 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval-valued linear regression has been investigated for some time. One of\nthe critical issues is optimizing the balance between model flexibility and\ninterpretability. This paper proposes a linear model for interval-valued data\nbased on the affine operators in the cone $\\mathcal{C} = \\{ (x, y) \\in\n\\mathbb{R}^2 | x \\leq y\\}$. The resulting new model is shown to have improved\nflexibility over typical models in the literature, while maintaining a good\ninterpretability. The least squares (LS) estimators of the model parameters are\nprovided in a simple explicit form, which possesses a series of nice\nproperties. Further investigations into the LS estimators shed light on the\npositive restrictions of a subset of the parameters and their implications on\nthe model validity. A simulation study is presented that supports the\ntheoretical findings. An application to a real data set is also provided to\ndemonstrate the applicability of our model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 03:55:29 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Sun", "Yan", ""], ["Ralescu", "Dan", ""]]}, {"id": "1506.03571", "submitter": "Enea Giuseppe Bongiorno", "authors": "Enea G. Bongiorno and Aldo Goia", "title": "Classification methods for Hilbert data based on surrogate density", "comments": "33 pages, 11 figures, 6 tables", "journal-ref": "Computational Statistics & Data Analysis, 99 (2016) pp. 204-222", "doi": "10.1016/j.csda.2016.01.019", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An unsupervised and a supervised classification approaches for Hilbert random\ncurves are studied. Both rest on the use of a surrogate of the probability\ndensity which is defined, in a distribution-free mixture context, from an\nasymptotic factorization of the small-ball probability. That surrogate density\nis estimated by a kernel approach from the principal components of the data.\nThe focus is on the illustration of the classification algorithms and the\ncomputational implications, with particular attention to the tuning of the\nparameters involved. Some asymptotic results are sketched. Applications on\nsimulated and real datasets show how the proposed methods work.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 07:31:45 GMT"}, {"version": "v2", "created": "Tue, 29 Mar 2016 08:57:16 GMT"}], "update_date": "2016-03-30", "authors_parsed": [["Bongiorno", "Enea G.", ""], ["Goia", "Aldo", ""]]}, {"id": "1506.03627", "submitter": "Fabian Scheipl", "authors": "Fabian Scheipl and Sonja Greven", "title": "Identifiability in penalized function-on-function regression models", "comments": "27 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models with functional responses and covariates constitute a\npowerful and increasingly important model class. However, regression with\nfunctional data poses well known and challenging problems of\nnon-identifiability. This non-identifiability can manifest itself in\narbitrarily large errors for coefficient surface estimates despite accurate\npredictions of the responses, thus invalidating substantial interpretations of\nthe fitted models. We offer an accessible rephrasing of these identifiability\nissues in realistic applications of penalized linear\nfunction-on-function-regression and delimit the set of circumstances under\nwhich they are likely to occur in practice. Specifically, non-identifiability\nthat persists under smoothness assumptions on the coefficient surface can occur\nif the functional covariate's empirical covariance has a kernel which overlaps\nthat of the roughness penalty of the spline estimator. Extensive simulation\nstudies validate the theoretical insights, explore the extent of the problem\nand allow us to evaluate their practical consequences under varying assumptions\nabout the data generating processes. A case study illustrates the practical\nsignificance of the problem. Based on theoretical considerations and our\nempirical evaluation, we provide immediately applicable diagnostics for lack of\nidentifiability and give recommendations for avoiding estimation artifacts in\npractice.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 11:16:22 GMT"}, {"version": "v2", "created": "Fri, 19 Feb 2016 11:14:23 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Scheipl", "Fabian", ""], ["Greven", "Sonja", ""]]}, {"id": "1506.03784", "submitter": "M{\\aa}ns Magnusson", "authors": "M{\\aa}ns Magnusson, Leif Jonsson, Mattias Villani, David Broman", "title": "Sparse Partially Collapsed MCMC for Parallel Inference in Topic Models", "comments": "Accepted for publication in Journal of Computational and Graphical\n  Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models, and more specifically the class of Latent Dirichlet Allocation\n(LDA), are widely used for probabilistic modeling of text. MCMC sampling from\nthe posterior distribution is typically performed using a collapsed Gibbs\nsampler. We propose a parallel sparse partially collapsed Gibbs sampler and\ncompare its speed and efficiency to state-of-the-art samplers for topic models\non five well-known text corpora of differing sizes and properties. In\nparticular, we propose and compare two different strategies for sampling the\nparameter block with latent topic indicators. The experiments show that the\nincrease in statistical inefficiency from only partial collapsing is smaller\nthan commonly assumed, and can be more than compensated by the speedup from\nparallelization and sparsity on larger corpora. We also prove that the\npartially collapsed samplers scale well with the size of the corpus. The\nproposed algorithm is fast, efficient, exact, and can be used in more modeling\nsituations than the ordinary collapsed sampler.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 19:16:01 GMT"}, {"version": "v2", "created": "Wed, 19 Oct 2016 21:22:53 GMT"}, {"version": "v3", "created": "Tue, 15 Aug 2017 05:36:07 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Magnusson", "M\u00e5ns", ""], ["Jonsson", "Leif", ""], ["Villani", "Mattias", ""], ["Broman", "David", ""]]}, {"id": "1506.03824", "submitter": "Ephraim Hanks", "authors": "Ephraim M. Hanks", "title": "A Constructive Spatio-Temporal Approach to Modeling Spatial Covariance", "comments": "32 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present an approach for modeling areal spatial covariance by considering\nthe stationary distribution of a spatio-temporal Markov random walk. In the\nareal data case, this stationary distribution corresponds to an intrinsic\nsimultaneous autoregressive (SAR) model for spatial correlation, and provides a\nprincipled approach to specifying areal spatial models when a spatio-temporal\ngenerating process can be assumed. I apply the approach to a study of spatial\ngenetic variation of trout in a stream network in Connecticut, USA, and a study\nof crime rates in neighborhoods of Columbus, OH, USA.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 20:04:02 GMT"}, {"version": "v2", "created": "Fri, 3 Jul 2015 14:20:12 GMT"}], "update_date": "2015-07-06", "authors_parsed": [["Hanks", "Ephraim M.", ""]]}, {"id": "1506.03909", "submitter": "Xiaohui Chen", "authors": "Xiaohui Chen, Yifeng He", "title": "Inference of high-dimensional linear models with time-varying\n  coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a pointwise inference algorithm for high-dimensional linear models\nwith time-varying coefficients. The method is based on a novel combination of\nthe nonparametric kernel smoothing technique and a Lasso bias-corrected ridge\nregression estimator. Due to the non-stationarity feature of the model, dynamic\nbias-variance decomposition of the estimator is obtained. With a\nbias-correction procedure, the local null distribution of the estimator of the\ntime-varying coefficient vector is characterized for iid Gaussian and\nheavy-tailed errors. The limiting null distribution is also established for\nGaussian process errors, and we show that the asymptotic properties differ\nbetween short-range and long-range dependent errors. Here, p-values are\nadjusted by a Bonferroni-type correction procedure to control the familywise\nerror rate (FWER) in the asymptotic sense at each time point. The finite sample\nsize performance of the proposed inference algorithm is illustrated with\nsynthetic data and an application to learn brain connectivity by using the\nresting-state fMRI data for Parkinson's disease.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 06:26:23 GMT"}, {"version": "v2", "created": "Mon, 12 Dec 2016 03:04:54 GMT"}, {"version": "v3", "created": "Thu, 16 Mar 2017 04:37:11 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Chen", "Xiaohui", ""], ["He", "Yifeng", ""]]}, {"id": "1506.03918", "submitter": "Paul Kabaila", "authors": "Paul Kabaila, Rheanna Mainzer and Davide Farchione", "title": "The impact of a Hausman pretest on the coverage probability and expected\n  length of confidence intervals", "comments": "arXiv admin note: substantial text overlap with arXiv:1412.5262", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the analysis of clustered and longitudinal data, which includes a\ncovariate that varies both between and within clusters (e.g. time-varying\ncovariate in longitudinal data), a Hausman pretest is commonly used to decide\nwhether subsequent inference is made using the linear random intercept model or\nthe fixed effects model. We assess the effect of this pretest on the coverage\nprobability and expected length of a confidence interval for the slope\nparameter. Our results show that for the small levels of significance of the\nHausman pretest commonly used in applications, the minimum coverage probability\nof this confidence interval can be far below nominal. Furthermore, the expected\nlength of this confidence interval is, on average, larger than the expected\nlength of a confidence interval for the slope parameter based on the fixed\neffects model with the same minimum coverage.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 07:13:05 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Kabaila", "Paul", ""], ["Mainzer", "Rheanna", ""], ["Farchione", "Davide", ""]]}, {"id": "1506.03920", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "A vine copula mixed effect model for trivariate meta-analysis of\n  diagnostic test accuracy studies accounting for disease prevalence", "comments": "arXiv admin note: substantial text overlap with arXiv:1502.07505", "journal-ref": "Statistical Methods in Medical Research, 2017, 26(5):2270--2286", "doi": "10.1177/0962280215596769", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bivariate copula mixed model has been recently proposed to synthesize\ndiagnostic test accuracy studies and it has been shown that is superior to the\nstandard generalized linear mixed model (GLMM) in this context. Here we call\ntrivariate vine copulas to extend the bivariate meta-analysis of diagnostic\ntest accuracy studies by accounting for disease prevalence. Our vine copula\nmixed model includes the trivariate GLMM as a special case and can also operate\non the original scale of sensitivity, specificity, and disease prevalence. Our\ngeneral methodology is illustrated by re-analysing the data of two published\nmeta-analyses. Our study suggests that there can be an improvement on\ntrivariate GLMM in fit to data and makes the argument for moving to vine copula\nrandom effects models especially because of their richness including reflection\nasymmetric tail dependence, and, computational feasibility despite their\nthree-dimensionality.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 07:50:24 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1506.03993", "submitter": "Fabio M. Bayer Ph.D", "authors": "La\\'is H. Loose, F\\'abio M. Bayer, Tarciana L. Pereira", "title": "Bootstrap Bartlett correction in inflated beta regression", "comments": "17 pages, 2 figures, 3 tables", "journal-ref": null, "doi": "10.1080/03610918.2015.1065326", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The inflated beta regression model aims to enable the modeling of responses\nin the intervals $(0,1]$, $[0,1)$ or $[0,1]$. In this model, hypothesis testing\nis often performed based on the likelihood ratio statistic. The critical values\nare obtained from asymptotic approximations, which may lead to distortions of\nsize in small samples. In this sense, this paper proposes the bootstrap\nBartlett correction to the statistic of likelihood ratio in the inflated beta\nregression model. The proposed adjustment only requires a simple Monte Carlo\nsimulation. Through extensive Monte Carlo simulations the finite sample\nperformance (size and power) of the proposed corrected test is compared to the\nusual likelihood ratio test and the Skovgaard adjustment already proposed in\nthe literature. The numerical results evidence that inference based on the\nproposed correction is much more reliable than that based on the usual\nlikelihood ratio statistics and the Skovgaard adjustment. At the end of the\nwork, an application to real data is also presented.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 11:47:26 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Loose", "La\u00eds H.", ""], ["Bayer", "F\u00e1bio M.", ""], ["Pereira", "Tarciana L.", ""]]}, {"id": "1506.04017", "submitter": "Serena Ng", "authors": "Jean-Jacques Forneron and Serena Ng", "title": "A Likelihood-Free Reverse Sampler of the Posterior Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers properties of an optimization based sampler for\ntargeting the posterior distribution when the likelihood is intractable and\nauxiliary statistics are used to summarize information in the data. Our reverse\nsampler approximates the likelihood-based posterior distribution by solving a\nsequence of simulated minimum distance problems. By a change of variable\nargument, these estimates are reweighted with a prior and the volume of the\njacobian matrix to serve as draws from the desired posterior distribution. The\nsampler provides a conceptual framework to understand the difference between\ntwo types of likelihood free estimation. Because simulated minimum distance\nestimation always results in acceptable draws, the reverse sampler is\npotentially an alternative to existing approximate Bayesian methods that are\ncomputationally demanding because of a low acceptance rate.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 13:55:20 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 15:53:47 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Forneron", "Jean-Jacques", ""], ["Ng", "Serena", ""]]}, {"id": "1506.04137", "submitter": "Utkarsh Dang", "authors": "Utkarsh J. Dang, Ryan P. Browne, and Paul D. McNicholas", "title": "Mixtures of Multivariate Power Exponential Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An expanded family of mixtures of multivariate power exponential\ndistributions is introduced. While fitting heavy-tails and skewness has\nreceived much attention in the model-based clustering literature recently, we\ninvestigate the use of a distribution that can deal with both varying\ntail-weight and peakedness of data. A family of parsimonious models is proposed\nusing an eigen-decomposition of the scale matrix. A generalized\nexpectation-maximization algorithm is presented that combines convex\noptimization via a minorization-maximization approach and optimization based on\naccelerated line search algorithms on the Stiefel manifold. Lastly, the utility\nof this family of models is illustrated using both toy and benchmark data.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 19:59:14 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Dang", "Utkarsh J.", ""], ["Browne", "Ryan P.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1506.04147", "submitter": "Jacob Andreas", "authors": "Jacob Andreas, Maxim Rabinovich, Dan Klein, Michael I. Jordan", "title": "On the accuracy of self-normalized log-linear models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CL cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Calculation of the log-normalizer is a major computational obstacle in\napplications of log-linear models with large output spaces. The problem of fast\nnormalizer computation has therefore attracted significant attention in the\ntheoretical and applied machine learning literature. In this paper, we analyze\na recently proposed technique known as \"self-normalization\", which introduces a\nregularization term in training to penalize log normalizers for deviating from\nzero. This makes it possible to use unnormalized model scores as approximate\nprobabilities. Empirical evidence suggests that self-normalization is extremely\neffective, but a theoretical understanding of why it should work, and how\ngenerally it can be applied, is largely lacking. We prove generalization bounds\non the estimated variance of normalizers and upper bounds on the loss in\naccuracy due to self-normalization, describe classes of input distributions\nthat self-normalize easily, and construct explicit examples of high-variance\ninput distributions. Our theoretical results make predictions about the\ndifficulty of fitting self-normalized models to several classes of\ndistributions, and we conclude with empirical validation of these predictions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 20:00:29 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 15:22:50 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Andreas", "Jacob", ""], ["Rabinovich", "Maxim", ""], ["Klein", "Dan", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1506.04321", "submitter": "Daniel Vogel", "authors": "Daniel Vogel and Roland Fried", "title": "Elliptical graphical modelling", "comments": null, "journal-ref": "Biometrika (2011) 98 (4): 935-951 first published online October\n  13, 2011", "doi": "10.1093/biomet/asr037", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose elliptical graphical models based on conditional uncorrelatedness\nas a general- ization of Gaussian graphical models by letting the population\ndistribution be elliptical instead of normal, allowing the fitting of data with\narbitrarily heavy tails. We study the class of propor- tionally affine\nequivariant scatter estimators and show how they can be used to perform\nelliptical graphical modelling, leading to a new class of partial correlation\nestimators and analogues of the classical deviance test. General expressions\nfor the asymptotic variance of partial correla- tion estimators, unconstrained\nand under decomposable models, are given, and the asymptotic chi square\napproximation of the pseudo-deviance test statistic is proved. The feasibility\nof our approach is demonstrated by a simulation study, using, among others,\nTyler's scatter estimator, which is distribution-free within the elliptical\nmodel. Our approach provides a robustification of Gaussian graphical modelling.\nThe latter is likelihood-based and known to be very sensitive to model\nmisspecification and outlying observations.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 21:25:12 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Vogel", "Daniel", ""], ["Fried", "Roland", ""]]}, {"id": "1506.04347", "submitter": "Helene Massam", "authors": "Helene Massam, Qiong Li, Xin Gao", "title": "Bayesian precision matrix estimation for graphical Gaussian models with\n  edge and vertex symmetries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graphical Gaussian models with edge and vertex symmetries were introduced by\n\\citet{HojLaur:2008} who also gave an algorithm to compute the maximum\nlikelihood estimate of the precision matrix for such models. In this paper, we\ntake a Bayesian approach to the estimation of the precision matrix. We consider\nonly those models where the symmetry constraints are imposed on the precision\nmatrix and which thus form a natural exponential family with the precision\nmatrix as the canonical parameter.\n  We first identify the Diaconis-Ylvisaker conjugate prior for these models and\ndevelop a scheme to sample from the prior and posterior distributions. We thus\nobtain estimates of the posterior mean of the precision matrix.\n  Second, in order to verify the precision of our estimate, we derive the\nexplicit analytic expression of the expected value of the precision matrix when\nthe graph underlying our model is a tree, a complete graph on three vertices\nand a decomposable graph on four vertices with various symmetries. In those\ncases, we compare our estimates with the exact value of the mean of the prior\ndistribution. We also verify the accuracy of our estimates of the posterior\nmean on simulated data for graphs with up to thirty vertices and various\nsymmetries.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 02:10:28 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Massam", "Helene", ""], ["Li", "Qiong", ""], ["Gao", "Xin", ""]]}, {"id": "1506.04430", "submitter": "Clement Dombry", "authors": "Cl\\'ement Dombry and Sebastian Engelke and Marco Oesting", "title": "Exact simulation of max-stable processes", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable processes play an important role as models for spatial extreme\nevents. Their complex structure as the pointwise maximum over an infinite\nnumber of random functions makes simulation highly nontrivial. Algorithms based\non finite approximations that are used in practice are often not exact and\ncomputationally inefficient. We will present two algorithms for exact\nsimulation of a max-stable process at a finite number of locations. The first\nalgorithm generalizes the approach by \\citet{DM-2014} for Brown--Resnick\nprocesses and it is based on simulation from the spectral measure. The second\nalgorithm relies on the idea to simulate only the extremal functions, that is,\nthose functions in the construction of a max-stable process that effectively\ncontribute to the pointwise maximum. We study the complexity of both algorithms\nand prove that the second procedure is always more efficient. Moreover, we\nprovide closed expressions for their implementation that cover the most popular\nmodels for max-stable processes and extreme value copulas. For simulation on\ndense grids, an adaptive design of the second algorithm is proposed.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 19:10:22 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Dombry", "Cl\u00e9ment", ""], ["Engelke", "Sebastian", ""], ["Oesting", "Marco", ""]]}, {"id": "1506.04451", "submitter": "Jose Luiz Padilha Da Silva", "authors": "Jos\\'e Luiz P. da Silva, Enrico A. Colosimo, F\\'abio N. Demarqui", "title": "Doubly Robust-Based Generalized Estimating Equations for the Analysis of\n  Longitudinal Ordinal Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Generalized Estimation Equations (GEE) are a well-known method for the\nanalysis of non-Gaussian longitudinal data. This method has computational\nsimplicity and marginal parameter interpretation. However, in the presence of\nmissing data, it is only valid under the strong assumption of missing\ncompletely at random (MCAR). Some corrections can be done when the missing data\nmechanism is missing at random (MAR): inverse probability weighting (WGEE) and\nmultiple imputation (MIGEE). In order to obtain consistent estimates, it is\nnecessary the correct specification of the weight model for WGEE or the\nimputation model for the MIGEE. A recent method combining ideas of these two\napproaches has doubly robust property. For consistency, it requires only the\nweight or the imputation model to be correct. In this work it is assumed a\nproportional odds model and it is proposed a doubly robust estimator for the\nanalysis of ordinal longitudinal data with intermittently missing response and\ncovariate under the MAR mechanism. Simulation results revealed better\nperformance of the proposed method compared to WGEE and MIGEE. The method is\napplied to a data set related to Analgesia Pain in Childbirth study.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 00:33:35 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["da Silva", "Jos\u00e9 Luiz P.", ""], ["Colosimo", "Enrico A.", ""], ["Demarqui", "F\u00e1bio N.", ""]]}, {"id": "1506.04452", "submitter": "Jose Luiz Padilha Da Silva", "authors": "Jos\\'e Luiz P. da Silva, Enrico A. Colosimo, F\\'abio N. Demarqui", "title": "Modeling the Association Structure in Doubly Robust GEE for Longitudinal\n  Ordinal Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Generalized Estimation Equations (GEE) are a well-known method for the\nanalysis of categorical longitudinal responses. GEE method has computational\nsimplicity and population parameter interpretation. In the presence of missing\ndata it is only valid under the strong assumption of missing completely at\nrandom. A doubly robust estimator (DRGEE) for correlated ordinal longitudinal\ndata is a nice approach for handling intermittently missing response and\ncovariate under the MAR mechanism. Independent working correlation is the\nstandard way in DRGEE. However, when covariate is not time stationary,\nefficiency can be gained using a structured association. The goal of this paper\nis to extend the DRGEE estimator to allow modeling the association structure by\nmeans of either the correlation coefficient or local odds ratio. Simulation\nresults revealed better performance of the local odds ratio parametrization,\nspecially for small samples. The method is applied to a data set related to\nRheumatic Mitral Stenosis.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 00:37:25 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["da Silva", "Jos\u00e9 Luiz P.", ""], ["Colosimo", "Enrico A.", ""], ["Demarqui", "F\u00e1bio N.", ""]]}, {"id": "1506.04489", "submitter": "Antony Overstall", "authors": "Antony Overstall and David Woods", "title": "Multivariate emulation of computer simulators: model selection and\n  diagnostics with application to a humanitarian relief model", "comments": "Minor revisions - methodology remains unchanged", "journal-ref": "Journal of the Royal Statistical Society Series C, 2016, 65,\n  483-505", "doi": "10.1111/rssc.12141", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a common framework for Bayesian emulation methodologies for\nmultivariate-output simulators, or computer models, that employ either\nparametric linear models or nonparametric Gaussian processes. Novel diagnostics\nsuitable for multivariate covariance-separable emulators are developed and\ntechniques to improve the adequacy of an emulator are discussed and\nimplemented. A variety of emulators are compared for a humanitarian relief\nsimulator, modelling aid missions to Sicily after a volcanic eruption and\nearthquake, and a sensitivity analysis is conducted to determine the\nsensitivity of the simulator output to changes in the input variables. The\nresults from parametric and nonparametric emulators are compared in terms of\nprediction accuracy, uncertainty quantification and scientific\ninterpretability.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 06:57:42 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 15:00:50 GMT"}], "update_date": "2016-10-28", "authors_parsed": [["Overstall", "Antony", ""], ["Woods", "David", ""]]}, {"id": "1506.04493", "submitter": "Julien Bect", "authors": "H\\'elo\\\"ise Dutrieux (EDF R\\&D, L2EP), Ivana Aleksovska (L2S), Julien\n  Bect (L2S,(M\\'ethodes d'Analyse Stochastique des Codes et Traitements\n  Num\\'eriques)), Emmanuel Vazquez (L2S,(M\\'ethodes d'Analyse Stochastique des\n  Codes et Traitements Num\\'eriques)), Delille Gauthier (EDF R\\&D), Bruno\n  Fran\\c{c}ois (L2EP)", "title": "The Informational Approach to Global Optimization in presence of very\n  noisy evaluation results. Application to the optimization of renewable energy\n  integration strategies", "comments": null, "journal-ref": "47\\`emes Journ\\'ees de Statistique de la SFdS (JdS 2015), Jun\n  2015, Lille, France", "doi": null, "report-no": null, "categories": "stat.CO math.OC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of global optimization of a function f from very\nnoisy evaluations. We adopt a Bayesian sequential approach: evaluation points\nare chosen so as to reduce the uncertainty about the position of the global\noptimum of f, as measured by the entropy of the corresponding random variable\n(Informational Approach to Global Optimization, Villemonteix et al., 2009).\nWhen evaluations are very noisy, the error coming from the estimation of the\nentropy using conditional simulations becomes non negligible compared to its\nvariations on the input domain. We propose a solution to this problem by\nchoosing evaluation points as if several evaluations were going to be made at\nthese points. The method is applied to the optimization of a strategy for the\nintegration of renewable energies into an electrical distribution network.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 07:25:16 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Dutrieux", "H\u00e9lo\u00efse", "", "EDF R\\&D, L2EP"], ["Aleksovska", "Ivana", "", "L2S"], ["Bect", "Julien", "", "L2S,"], ["Vazquez", "Emmanuel", "", "L2S,"], ["Gauthier", "Delille", "", "EDF R\\&D"], ["Fran\u00e7ois", "Bruno", "", "L2EP"]]}, {"id": "1506.04592", "submitter": "Patrick Conrad", "authors": "Patrick R. Conrad, Mark Girolami, Simo S\\\"arkk\\\"a, Andrew Stuart,\n  Konstantinos Zygalakis", "title": "Probability Measures for Numerical Solutions of Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a formal quantification of epistemic uncertainty\ninduced by numerical solutions of ordinary and partial differential equation\nmodels. Numerical solutions of differential equations contain inherent\nuncertainties due to the finite dimensional approximation of an unknown and\nimplicitly defined function. When statistically analysing models based on\ndifferential equations describing physical, or other naturally occurring,\nphenomena, it is therefore important to explicitly account for the uncertainty\nintroduced by the numerical method. This enables objective determination of its\nimportance relative to other uncertainties, such as those caused by data\ncontaminated with noise or model error induced by missing physical or\ninadequate descriptors. To this end we show that a wide variety of existing\nsolvers can be randomised, inducing a probability measure over the solutions of\nsuch differential equations. These measures exhibit contraction to a Dirac\nmeasure around the true unknown solution, where the rates of convergence are\nconsistent with the underlying deterministic numerical method. Ordinary\ndifferential equations and elliptic partial differential equations are used to\nillustrate the approach to quantifying uncertainty in both the statistical\nanalysis of the forward and inverse problems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 13:44:41 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Conrad", "Patrick R.", ""], ["Girolami", "Mark", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Stuart", "Andrew", ""], ["Zygalakis", "Konstantinos", ""]]}, {"id": "1506.04696", "submitter": "Yi-An Ma", "authors": "Yi-An Ma, Tianqi Chen and Emily B. Fox", "title": "A Complete Recipe for Stochastic Gradient MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent Markov chain Monte Carlo (MCMC) samplers leverage continuous\ndynamics to define a transition kernel that efficiently explores a target\ndistribution. In tandem, a focus has been on devising scalable variants that\nsubsample the data and use stochastic gradients in place of full-data gradients\nin the dynamic simulations. However, such stochastic gradient MCMC samplers\nhave lagged behind their full-data counterparts in terms of the complexity of\ndynamics considered since proving convergence in the presence of the stochastic\ngradient noise is non-trivial. Even with simple dynamics, significant physical\nintuition is often required to modify the dynamical system to account for the\nstochastic gradient noise. In this paper, we provide a general recipe for\nconstructing MCMC samplers--including stochastic gradient versions--based on\ncontinuous Markov processes specified via two matrices. We constructively prove\nthat the framework is complete. That is, any continuous Markov process that\nprovides samples from the target distribution can be written in our framework.\nWe show how previous continuous-dynamic samplers can be trivially \"reinvented\"\nin our framework, avoiding the complicated sampler-specific proofs. We likewise\nuse our recipe to straightforwardly propose a new state-adaptive sampler:\nstochastic gradient Riemann Hamiltonian Monte Carlo (SGRHMC). Our experiments\non simulated data and a streaming Wikipedia analysis demonstrate that the\nproposed SGRHMC sampler inherits the benefits of Riemann HMC, with the\nscalability of stochastic gradient methods.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 18:32:37 GMT"}, {"version": "v2", "created": "Sun, 1 Nov 2015 00:18:32 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Ma", "Yi-An", ""], ["Chen", "Tianqi", ""], ["Fox", "Emily B.", ""]]}, {"id": "1506.04731", "submitter": "Georgiy Shevchenko", "authors": "Yuliya Mishura", "title": "Maximum likelihood drift estimation for the mixing of two fractional\n  Brownian motions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct the maximum likelihood estimator (MLE) of the unknown drift\nparameter $\\theta\\in \\mathbb{R}$ in the linear model $X_t=\\theta t+\\sigma\nB^{H_1}(t)+B^{H_2}(t),\\;t\\in[0,T],$ where $B^{H_1}$ and $B^{H_2}$ are two\nindependent fractional Brownian motions with Hurst indices $\\frac12<H_1<H_2<1.$\nThe formula for MLE is based on the solution of the integral equation with weak\npolar kernel.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 19:55:38 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Mishura", "Yuliya", ""]]}, {"id": "1506.04842", "submitter": "Giovanni Montana", "authors": "Da Ruan, Alastair Young and Giovanni Montana", "title": "Differential analysis of biological networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cancer research, the comparison of gene expression or DNA methylation\nnetworks inferred from healthy controls and patients can lead to the discovery\nof biological pathways associated to the disease. As a cancer progresses, its\nsignalling and control networks are subject to some degree of localised\nre-wiring. Being able to detect disrupted interaction patterns induced by the\npresence or progression of the disease can lead to the discovery of novel\nmolecular diagnostic and prognostic signatures. Currently there is a lack of\nscalable statistical procedures for two-network comparisons aimed at detecting\nlocalised topological differences. We propose the dGHD algorithm, a methodology\nfor detecting differential interaction patterns in two-network comparisons. The\nalgorithm relies on a statistic, the Generalised Hamming Distance (GHD), for\nassessing the degree of topological difference between networks and evaluating\nits statistical significance. dGHD builds on a non-parametric permutation\ntesting framework but achieves computationally efficiency through an asymptotic\nnormal approximation. We show that the GHD is able to detect more subtle\ntopological differences compared to a standard Hamming distance between\nnetworks. This results in the dGHD algorithm achieving high performance in\nsimulation studies as measured by sensitivity and specificity. An application\nto the problem of detecting differential DNA co-methylation subnetworks\nassociated to ovarian cancer demonstrates the potential benefits of the\nproposed methodology for discovering network-derived biomarkers associated with\na trait of interest.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 06:03:20 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Ruan", "Da", ""], ["Young", "Alastair", ""], ["Montana", "Giovanni", ""]]}, {"id": "1506.04854", "submitter": "Xing He", "authors": "Xinyi Xu, Xing He, Qian Ai, Robert C. Qiu", "title": "A Correlation Analysis Method for Power Systems Based on Random Matrix\n  Theory", "comments": "9 pages, 9 figures, Accepted by IEEE Trans on Smart Grid", "journal-ref": "IEEE Transactions on Smart Grid , vol.PP, no.99, pp.1-10, 2015", "doi": "10.1109/TSG.2015.2508506", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The operating status of power systems is influenced by growing varieties of\nfactors, resulting from the developing sizes and complexity of power systems;\nin this situation, the modelbased methods need be revisited. A data-driven\nmethod, as the novel alternative, on the other hand, is proposed in this paper:\nit reveals the correlations between the factors and the system status through\nstatistical properties of data. An augmented matrix, as the data source, is the\nkey trick for this method; it is formulated by two parts: 1) status data as the\nbasic part, and 2) factor data as the augmented part. The random matrix theory\n(RMT) is applied as the mathematical framework. The linear eigenvalue\nstatistics (LESs), such as the mean spectral radius (MSR), are defined to study\ndata correlations through large random matrices. Compared with model-based\nmethods, the proposed method is inspired by a pure statistical approach,\nwithout a prior knowledge of operation and interaction mechanism models for\npower systems and factors. In general, this method is direct in analysis,\nrobust against bad data, universal to various factors, and applicable for\nreal-time analysis. A case study, based on the standard IEEE 118-bus system,\nvalidates the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 07:02:19 GMT"}, {"version": "v2", "created": "Tue, 22 Dec 2015 14:41:21 GMT"}, {"version": "v3", "created": "Wed, 6 Jul 2016 12:54:11 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Xu", "Xinyi", ""], ["He", "Xing", ""], ["Ai", "Qian", ""], ["Qiu", "Robert C.", ""]]}, {"id": "1506.04915", "submitter": "Bernardo Nipoti", "authors": "Julyan Arbel, Stefano Favaro, Bernardo Nipoti, Yee Whye Teh", "title": "Bayesian nonparametric inference for discovery probabilities: credible\n  intervals and large sample asymptotics", "comments": null, "journal-ref": "Statistica Sinica, 27:839--858, 2017", "doi": "10.5705/ss.202015.0250", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a sample of size $n$ from a population of individuals belonging to\ndifferent species with unknown proportions, a popular problem of practical\ninterest consists in making inference on the probability $D_{n}(l)$ that the\n$(n+1)$-th draw coincides with a species with frequency $l$ in the sample, for\nany $l=0,1,\\ldots,n$. This paper contributes to the methodology of Bayesian\nnonparametric inference for $D_{n}(l)$. Specifically, under the general\nframework of Gibbs-type priors we show how to derive credible intervals for a\nBayesian nonparametric estimation of $D_{n}(l)$, and we investigate the large\n$n$ asymptotic behaviour of such an estimator. Of particular interest are\nspecial cases of our results obtained under the specification of the two\nparameter Poisson--Dirichlet prior and the normalized generalized Gamma prior,\nwhich are two of the most commonly used Gibbs-type priors. With respect to\nthese two prior specifications, the proposed results are illustrated through a\nsimulation study and a benchmark Expressed Sequence Tags dataset. To the best\nour knowledge, this illustration provides the first comparative study between\nthe two parameter Poisson--Dirichlet prior and the normalized generalized Gamma\nprior in the context of Bayesian nonparemetric inference for $D_{n}(l)$.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 11:02:18 GMT"}, {"version": "v2", "created": "Sat, 2 Jul 2016 10:44:11 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Arbel", "Julyan", ""], ["Favaro", "Stefano", ""], ["Nipoti", "Bernardo", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1506.04967", "submitter": "Shravan Vasishth", "authors": "Douglas Bates and Reinhold Kliegl and Shravan Vasishth and Harald\n  Baayen", "title": "Parsimonious Mixed Models", "comments": "ArXiv preprint. 21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of experimental data with mixed-effects models requires\ndecisions about the specification of the appropriate random-effects structure.\nRecently, Barr, Levy, Scheepers, and Tily, 2013 recommended fitting `maximal'\nmodels with all possible random effect components included. Estimation of\nmaximal models, however, may not converge. We show that failure to converge\ntypically is not due to a suboptimal estimation algorithm, but is a consequence\nof attempting to fit a model that is too complex to be properly supported by\nthe data, irrespective of whether estimation is based on maximum likelihood or\non Bayesian hierarchical modeling with uninformative or weakly informative\npriors. Importantly, even under convergence, overparameterization may lead to\nuninterpretable models. We provide diagnostic tools for detecting\noverparameterization and guiding model simplification.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 13:42:03 GMT"}, {"version": "v2", "created": "Sat, 26 May 2018 09:06:47 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Bates", "Douglas", ""], ["Kliegl", "Reinhold", ""], ["Vasishth", "Shravan", ""], ["Baayen", "Harald", ""]]}, {"id": "1506.04976", "submitter": "Tsagris Michail", "authors": "Michail Tsagris, Simon Preston and Andrew T.A. Wood", "title": "Improved classification for compositional data using the\n  $\\alpha$-transformation", "comments": "This is a 17-page preprint and has been accepted for publication at\n  the Journal of Classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compositional data analysis an observation is a vector containing\nnon-negative values, only the relative sizes of which are considered to be of\ninterest. Without loss of generality, a compositional vector can be taken to be\na vector of proportions that sum to one. Data of this type arise in many areas\nincluding geology, archaeology, biology, economics and political science. In\nthis paper we investigate methods for classification of compositional data. Our\napproach centres on the idea of using the $\\alpha$-transformation to transform\nthe data and then to classify the transformed data via regularised discriminant\nanalysis and the k-nearest neighbours algorithm. Using the\n$\\alpha$-transformation generalises two rival approaches in compositional data\nanalysis, one (when $\\alpha=1$) that treats the data as though they were\nEuclidean, ignoring the compositional constraint, and another (when $\\alpha=0$)\nthat employs Aitchison's centred log-ratio transformation. A numerical study\nwith several real datasets shows that whether using $\\alpha=1$ or $\\alpha=0$\ngives better classification performance depends on the dataset, and moreover\nthat using an intermediate value of $\\alpha$ can sometimes give better\nperformance than using either 1 or 0.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 13:59:49 GMT"}, {"version": "v2", "created": "Wed, 17 Jun 2015 07:14:50 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Tsagris", "Michail", ""], ["Preston", "Simon", ""], ["Wood", "Andrew T. A.", ""]]}, {"id": "1506.05216", "submitter": "Tsagris Michail", "authors": "Michail Tsagris", "title": "The k-NN algorithm for compositional data: a revised approach with and\n  without zero values present", "comments": "This manuscript will appear at the.\n  http://www.jds-online.com/volume-12-number-3-july-2014", "journal-ref": "Journal of Data Science, Vol 12, Number 3, July 2014", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In compositional data, an observation is a vector with non-negative\ncomponents which sum to a constant, typically 1. Data of this type arise in\nmany areas, such as geology, archaeology, biology, economics and political\nscience among others. The goal of this paper is to extend the taxicab metric\nand a newly suggested metric for compositional data by employing a power\ntransformation. Both metrics are to be used in the k-nearest neighbours\nalgorithm regardless of the presence of zeros. Examples with real data are\nexhibited.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 06:25:09 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Tsagris", "Michail", ""]]}, {"id": "1506.05269", "submitter": "Julyan Arbel", "authors": "Julyan Arbel, Antonio Lijoi, Bernardo Nipoti", "title": "Bayesian Survival Model based on Moment Characterization", "comments": "12 pages, 3 figures. arXiv admin note: substantial text overlap with\n  arXiv:1405.6628", "journal-ref": "Bayesian Statistics from Methods to Models and Applications,\n  (2015) 3-14", "doi": "10.1007/978-3-319-16238-6", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian nonparametric marginal methods are very popular since they lead to\nfairly easy implementation due to the formal marginalization of the\ninfinite-dimensional parameter of the model. However, the straightforwardness\nof these methods also entails some limitations: they typically yield point\nestimates in the form of posterior expectations, but cannot be used to estimate\nnon-linear functionals of the posterior distribution, such as median, mode or\ncredible intervals. This is particularly relevant in survival analysis where\nnon-linear functionals such as e.g. the median survival time, play a central\nrole for clinicians and practitioners. The main goal of this paper is to\nsummarize the methodology introduced in [Arbel et al., Comput. Stat. Data. An.,\n2015] for hazard mixture models in order to draw approximate Bayesian inference\non survival functions that is not limited to the posterior mean. In addition,\nwe propose a practical implementation of an R package called momentify designed\nfor moment-based density approximation, and, by means of an extensive\nsimulation study, we thoroughly compare the introduced methodology with\nstandard marginal methods and empirical estimation.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 10:24:27 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Arbel", "Julyan", ""], ["Lijoi", "Antonio", ""], ["Nipoti", "Bernardo", ""]]}, {"id": "1506.05275", "submitter": "Sokbae Lee", "authors": "Le-Yu Chen, Sokbae Lee", "title": "Breaking the curse of dimensionality in conditional moment inequalities\n  for discrete choice models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies inference of preference parameters in semiparametric\ndiscrete choice models when these parameters are not point-identified and the\nidentified set is characterized by a class of conditional moment inequalities.\nExploring the semiparametric modeling restrictions, we show that the identified\nset can be equivalently formulated by moment inequalities conditional on only\ntwo continuous indexing variables. Such formulation holds regardless of the\ncovariate dimension, thereby breaking the curse of dimensionality for\nnonparametric inference based on the underlying conditional moment\ninequalities. We further apply this dimension reducing characterization\napproach to the monotone single index model and to a variety of semiparametric\nmodels under which the sign of conditional expectation of a certain\ntransformation of the outcome is the same as that of the indexing variable.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 10:48:46 GMT"}, {"version": "v2", "created": "Mon, 11 Apr 2016 11:50:20 GMT"}, {"version": "v3", "created": "Fri, 23 Nov 2018 16:19:34 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Chen", "Le-Yu", ""], ["Lee", "Sokbae", ""]]}, {"id": "1506.05384", "submitter": "Fabian Scheipl", "authors": "Fabian Scheipl and Jan Gertheiss and Sonja Greven", "title": "Generalized Functional Additive Mixed Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a comprehensive framework for additive regression models for\nnon-Gaussian functional responses, allowing for multiple (partially) nested or\ncrossed functional random effects with flexible correlation structures for,\ne.g., spatial, temporal, or longitudinal functional data as well as linear and\nnonlinear effects of functional and scalar covariates that may vary smoothly\nover the index of the functional response. Our implementation handles\nfunctional responses from any exponential family distribution as well as many\nothers like Beta- or scaled non-central $t$-distributions. Development is\nmotivated by and evaluated on an application to large-scale longitudinal\nfeeding records of pigs. Results in extensive simulation studies as well as\nreplications of two previously published simulation studies for generalized\nfunctional mixed models demonstrate the good performance of our proposal. The\napproach is implemented in well-documented open source software in the \"pffr()\"\nfunction in R-package \"refund\".\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 16:47:11 GMT"}, {"version": "v2", "created": "Fri, 2 Oct 2015 14:17:36 GMT"}, {"version": "v3", "created": "Fri, 6 May 2016 12:09:36 GMT"}], "update_date": "2016-05-09", "authors_parsed": [["Scheipl", "Fabian", ""], ["Gertheiss", "Jan", ""], ["Greven", "Sonja", ""]]}, {"id": "1506.05446", "submitter": "Weijie Su", "authors": "Weijie Su, Junyang Qian, Linxi Liu", "title": "Communication-Efficient False Discovery Rate Control via Knockoff\n  Aggregation", "comments": "Generalized to the case that linear models can have different\n  parameters; changed title; updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The false discovery rate (FDR)---the expected fraction of spurious\ndiscoveries among all the discoveries---provides a popular statistical\nassessment of the reproducibility of scientific studies in various disciplines.\nIn this work, we introduce a new method for controlling the FDR in\nmeta-analysis of many decentralized linear models. Our method targets the\nscenario where many research groups---possibly the number of which is\nrandom---are independently testing a common set of hypotheses and then sending\nsummary statistics to a coordinating center in an online manner. Built on the\nknockoffs framework introduced by Barber and Candes (2015), our procedure\nstarts by applying the knockoff filter to each linear model and then aggregates\nthe summary statistics via one-shot communication in a novel way. This method\ngives exact FDR control non-asymptotically without any knowledge of the noise\nvariances or making any assumption about sparsity of the signal. In certain\nsettings, it has a communication complexity that is optimal up to a logarithmic\nfactor.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 19:56:59 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 18:24:26 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Su", "Weijie", ""], ["Qian", "Junyang", ""], ["Liu", "Linxi", ""]]}, {"id": "1506.05506", "submitter": "Yuzo Maruyama", "authors": "Yuzo Maruyama, Ryoko Tone, Yasushi Asami", "title": "Noise Addition for Individual Records to Preserve Privacy and\n  Statistical Characteristics: Case Study of Real Estate Transaction Data", "comments": "16 pages; minor change in title; revised version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method of perturbing a major variable by adding noise such\nthat results of regression analysis are unaffected. The extent of the\nperturbation can be controlled using a single parameter, which eases an actual\nperturbation application. On the basis of results of a numerical experiment, we\nrecommend an appropriate value of the parameter that can achieve both\nsufficient perturbation to mask original values and sufficient coherence\nbetween perturbed and original data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 22:08:37 GMT"}, {"version": "v2", "created": "Fri, 7 Aug 2015 01:17:11 GMT"}], "update_date": "2015-08-10", "authors_parsed": [["Maruyama", "Yuzo", ""], ["Tone", "Ryoko", ""], ["Asami", "Yasushi", ""]]}, {"id": "1506.05710", "submitter": "Amy Willis", "authors": "Amy Willis, John Bunge, Thea Whitman", "title": "Inference for changes in biodiversity", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We wish to formally test for changes in the taxonomic diversity of a\ncommunity, especially in the presence of high latent diversity. Drawing on the\nmeta-analysis literature, we construct a model for diversity that accounts for\ncovariate effects as well as sampling variability. This permits inference for\nchanges in richness with covariates and also a test for homogeneity. We argue\nthat we can use the principles of shrinkage estimation to improve richness\nestimation in this nonstandard context, which is especially important given the\nhigh variance of richness estimators and the increasing abundance of community\ncomposition data. We demonstrate the methodology under simulation, in a gut\nmicrobiome study (testing for a decrease in richness with antibiotics), and in\na soil microbiome study (testing for homogeneity of replicates). We believe\nthat this is the first formal procedure for analyzing changes in species\nrichness.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 15:15:25 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Willis", "Amy", ""], ["Bunge", "John", ""], ["Whitman", "Thea", ""]]}, {"id": "1506.05741", "submitter": "Kody Law", "authors": "Yuxin Chen, David Keyes, Kody J.H. Law, and Hatem Ltaief", "title": "Accelerated dimension-independent adaptive Metropolis", "comments": null, "journal-ref": "SIAM J. Sci. Comput., 38(5), S539--S565, (2016)", "doi": "10.1137/15M1026432", "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers black-box Bayesian inference over high-dimensional\nparameter spaces. The well-known adaptive Metropolis (AM) algorithm of (Haario\netal. 2001) is extended herein to scale asymptotically uniformly with respect\nto the underlying parameter dimension for Gaussian targets, by respecting the\nvariance of the target. The resulting algorithm, referred to as the\ndimension-independent adaptive Metropolis (DIAM) algorithm, also shows improved\nperformance with respect to adaptive Metropolis on non-Gaussian targets. This\nalgorithm is further improved, and the possibility of probing high-dimensional\ntargets is enabled, via GPU-accelerated numerical libraries and periodically\nsynchronized concurrent chains (justified a posteriori). Asymptotically in\ndimension, this GPU implementation exhibits a factor of four improvement versus\na competitive CPU-based Intel MKL parallel version alone. Strong scaling to\nconcurrent chains is exhibited, through a combination of longer time per sample\nbatch (weak scaling) and yet fewer necessary samples to convergence. The\nalgorithm performance is illustrated on several Gaussian and non-Gaussian\ntarget examples, in which the dimension may be in excess of one thousand.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 16:30:55 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Chen", "Yuxin", ""], ["Keyes", "David", ""], ["Law", "Kody J. H.", ""], ["Ltaief", "Hatem", ""]]}, {"id": "1506.05757", "submitter": "Mathieu Gerber", "authors": "Mathieu Gerber and Florian Pelgrin", "title": "Bayesian Inference for the Multivariate Extended-Skew Normal\n  Distribution", "comments": "38 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multivariate extended skew-normal distribution allows for accommodating\nraw data which are skewed and heavy tailed, and has at least three appealing\nstatistical properties, namely closure under conditioning, affine\ntransformations, and marginalization. In this paper we propose a Bayesian\ncomputational approach based on a sequential Monte Carlo (SMC) sampler to\nestimate such distributions. The practical implementation of each step of the\nalgorithm is discussed and the elicitation of prior distributions takes into\nconsideration some unusual behaviour of the likelihood function and the\ncorresponding Fisher information matrix. Using Monte Carlo simulations, we\nprovide strong evidence regarding the performances of the SMC sampler as well\nas some new insights regarding the parametrizations of the extended skew-normal\ndistribution. A generalization to the extended skew-normal sample selection\nmodel is also presented. Finally we proceed with the analysis of two real\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 17:45:53 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Gerber", "Mathieu", ""], ["Pelgrin", "Florian", ""]]}, {"id": "1506.05822", "submitter": "Jakob Runge", "authors": "Jakob Runge, Reik V. Donner, and J\\\"urgen Kurths", "title": "Optimal model-free prediction from multivariate time series", "comments": "14 pages, 9 figures", "journal-ref": "Phys. Rev. E 91, 052909, May 2015", "doi": "10.1103/PhysRevE.91.052909", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting a time series from multivariate predictors constitutes a\nchallenging problem, especially using model-free approaches. Most techniques,\nsuch as nearest-neighbor prediction, quickly suffer from the curse of\ndimensionality and overfitting for more than a few predictors which has limited\ntheir application mostly to the univariate case. Therefore, selection\nstrategies are needed that harness the available information as efficiently as\npossible. Since often the right combination of predictors matters, ideally all\nsubsets of possible predictors should be tested for their predictive power, but\nthe exponentially growing number of combinations makes such an approach\ncomputationally prohibitive. Here a prediction scheme that overcomes this\nstrong limitation is introduced utilizing a causal pre-selection step which\ndrastically reduces the number of possible predictors to the most predictive\nset of causal drivers making a globally optimal search scheme tractable. The\ninformation-theoretic optimality is derived and practical selection criteria\nare discussed. As demonstrated for multivariate nonlinear stochastic delay\nprocesses, the optimal scheme can even be less computationally expensive than\ncommonly used sub-optimal schemes like forward selection. The method suggests a\ngeneral framework to apply the optimal model-free approach to select variables\nand subsequently fit a model to further improve a prediction or learn\nstatistical dependencies. The performance of this framework is illustrated on a\nclimatological index of El Ni\\~no Southern Oscillation.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 21:17:11 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Runge", "Jakob", ""], ["Donner", "Reik V.", ""], ["Kurths", "J\u00fcrgen", ""]]}, {"id": "1506.05886", "submitter": "Bailey Fosdick", "authors": "Bailey K. Fosdick, Maria DeYoreo, Jerome P. Reiter", "title": "Categorical Data Fusion Using Auxiliary Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data fusion analysts seek to combine information from two databases\ncomprised of disjoint sets of individuals, in which some variables appear in\nboth databases and other variables appear in only one database. Most data\nfusion techniques rely on variants of conditional independence assumptions.\nWhen inappropriate, these assumptions can result in unreliable inferences. We\npropose a data fusion technique that allows analysts to easily incorporate\nauxiliary information on the dependence structure of variables not observed\njointly; we refer to this auxiliary information as glue. With this technique,\nwe fuse two marketing surveys from the book publisher HarperCollins using glue\nfrom the online, rapid-response polling company CivicScience. The fused data\nenable estimation of associations between people's preferences for authors and\nfor learning about new books. The analysis also serves as a case study on the\npotential for using online surveys to aid data fusion.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 06:34:14 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Fosdick", "Bailey K.", ""], ["DeYoreo", "Maria", ""], ["Reiter", "Jerome P.", ""]]}, {"id": "1506.06040", "submitter": "Esin Karahan", "authors": "Esin Karahan, Pedro A. Rojas-Lopez, Maria L. Bringas-Vega, Pedro A.\n  Valdes-Hernandez, Pedro A. Valdes-Sosa", "title": "Tensor Analysis and Fusion of Multimodal Brain Images", "comments": "23 pages, 15 figures, submitted to Proceedings of the IEEE", "journal-ref": null, "doi": "10.1109/JPROC.2015.2455028", "report-no": null, "categories": "stat.ME cs.NA stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current high-throughput data acquisition technologies probe dynamical systems\nwith different imaging modalities, generating massive data sets at different\nspatial and temporal resolutions posing challenging problems in multimodal data\nfusion. A case in point is the attempt to parse out the brain structures and\nnetworks that underpin human cognitive processes by analysis of different\nneuroimaging modalities (functional MRI, EEG, NIRS etc.). We emphasize that the\nmultimodal, multi-scale nature of neuroimaging data is well reflected by a\nmulti-way (tensor) structure where the underlying processes can be summarized\nby a relatively small number of components or \"atoms\". We introduce\nMarkov-Penrose diagrams - an integration of Bayesian DAG and tensor network\nnotation in order to analyze these models. These diagrams not only clarify\nmatrix and tensor EEG and fMRI time/frequency analysis and inverse problems,\nbut also help understand multimodal fusion via Multiway Partial Least Squares\nand Coupled Matrix-Tensor Factorization. We show here, for the first time, that\nGranger causal analysis of brain networks is a tensor regression problem, thus\nallowing the atomic decomposition of brain networks. Analysis of EEG and fMRI\nrecordings shows the potential of the methods and suggests their use in other\nscientific domains.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 15:03:22 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Karahan", "Esin", ""], ["Rojas-Lopez", "Pedro A.", ""], ["Bringas-Vega", "Maria L.", ""], ["Valdes-Hernandez", "Pedro A.", ""], ["Valdes-Sosa", "Pedro A.", ""]]}, {"id": "1506.06101", "submitter": "Jeffrey Miller", "authors": "Jeffrey W. Miller and David B. Dunson", "title": "Robust Bayesian inference via coarsening", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard approach to Bayesian inference is based on the assumption that\nthe distribution of the data belongs to the chosen model class. However, even a\nsmall violation of this assumption can have a large impact on the outcome of a\nBayesian procedure. We introduce a simple, coherent approach to Bayesian\ninference that improves robustness to perturbations from the model: rather than\ncondition on the data exactly, one conditions on a neighborhood of the\nempirical distribution. When using neighborhoods based on relative entropy\nestimates, the resulting \"coarsened\" posterior can be approximated by simply\ntempering the likelihood---that is, by raising it to a fractional power---thus,\ninference is often easily implemented with standard methods, and one can even\nobtain analytical solutions when using conjugate priors. Some theoretical\nproperties are derived, and we illustrate the approach with real and simulated\ndata, using mixture models, autoregressive models of unknown order, and\nvariable selection in linear regression.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 18:02:48 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Miller", "Jeffrey W.", ""], ["Dunson", "David B.", ""]]}, {"id": "1506.06169", "submitter": "Patrick McDermott", "authors": "Patrick L. McDermott and Christopher K. Wikle", "title": "A Model-Based Approach for Analog Spatio-Temporal Dynamic Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analog forecasting has been applied in a variety of fields for predicting\nfuture states of complex nonlinear systems that require flexible forecasting\nmethods. Past analog methods have almost exclu- sively been used in an\nempirical framework without the structure of a model-based approach. We propose\na Bayesian model framework for analog forecasting, building upon previous\nanalog methods but accounting for parameter uncertainty. Thus, unlike\ntraditional analog forecasting methods, the use of Bayesian modeling allows one\nto rigorously quantify uncertainty to obtain realistic posterior predictive\ndistributions. The model is applied to the long-lead time forecasting of\nmid-May averaged soil moisture anomalies in Iowa over a high-resolution grid of\nspatial locations. Sea Surface Tem- perature (SST) is used to find past time\nperiods with similar trajectories to the current pre-forecast period. The\nanalog model is developed on projection coefficients from a basis expansion of\nthe soil moisture and SST fields. Separate models are constructed for locations\nfalling in each Iowa Crop Reporting District (CRD) and the forecasting ability\nof the proposed model is compared against a variety of alternative methods and\nmetrics.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 22:16:06 GMT"}, {"version": "v2", "created": "Fri, 12 Feb 2016 23:04:13 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["McDermott", "Patrick L.", ""], ["Wikle", "Christopher K.", ""]]}, {"id": "1506.06190", "submitter": "Martin Felix-Medina", "authors": "Martin H. F\\'elix Medina", "title": "Combining cluster sampling and link-tracing sampling to estimate the\n  size of a hidden population: asymptotic properties of the estimators", "comments": "37 pages (including cover page)", "journal-ref": null, "doi": null, "report-no": "FCFM-UAS-2015-01", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  F\\'elix-Medina and Thompson (2004) proposed a variant of link-tracing\nsampling to estimate the size of a hidden population such as drug users, sexual\nworkers or homeless people. In their variant a sampling frame of sites where\nthe members of the population tend to gather is constructed. The frame is not\nassumed to cover the whole population, but only a portion of it. A simple\nrandom sample of sites is selected; the people in the sampled sites are\nidentified and are asked to name other members of the population which are\nadded to the sample. Those authors proposed maximum likelihood estimators of\nthe population size which derived from a multinomial model for the numbers of\npeople found in the sampled sites and a model that considers that the\nprobability that a person is named by any element in a particular sampled site\n(link-probability) does not depend on the named person, that is, that the\nprobabilities are homogeneous. Later, F\\'elix-Medina et al. (2015) proposed\nunconditional and conditional maximum likelihood estimators of the population\nsize which derived from a model that takes into account the heterogeneity of\nthe link-probabilities. In this work we consider this sampling design and set\nconditions for a general model for the link-probabilities that guarantee the\nconsistency and asymptotic normality of the estimators of the population size\nand of the estimators of the parameters of the model for the\nlink-probabilities. In particular we showed that both the unconditional and\nconditional maximum likelihood estimators of the population size are consistent\nand have asymptotic normal distributions which are different from each other.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 01:14:27 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Medina", "Martin H. F\u00e9lix", ""]]}, {"id": "1506.06199", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, Hamed Firouzi and Alfred O. Hero III", "title": "Non-parametric Quickest Change Detection for Large Scale Random Matrices", "comments": "Proc. of ISIT, Hong Kong, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of quickest detection of a change in the distribution of a\n$n\\times p$ random matrix based on a sequence of observations having a single\nunknown change point is considered. The forms of the pre- and post-change\ndistributions of the rows of the matrices are assumed to belong to the family\nof elliptically contoured densities with sparse dispersion matrices but are\notherwise unknown. We propose a non-parametric stopping rule that is based on a\nnovel summary statistic related to k-nearest neighbor correlation between\ncolumns of each observed random matrix. In the large scale regime of\n$p\\rightarrow \\infty$ and $n$ fixed we show that, among all functions of the\nproposed summary statistic, the proposed stopping rule is asymptotically\noptimal under a minimax quickest change detection (QCD) model.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 03:45:15 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Banerjee", "Taposh", ""], ["Firouzi", "Hamed", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1506.06201", "submitter": "Shravan Vasishth", "authors": "Tanner Sorensen and Shravan Vasishth", "title": "Bayesian linear mixed models using Stan: A tutorial for psychologists,\n  linguists, and cognitive scientists", "comments": "Submitted to Psychological Methods (Special Issue on Bayesian Data\n  Analysis); 30 pages; 6 figures", "journal-ref": "Quantitative Methods for Psychology 2016", "doi": "10.20982/tqmp.12.3.p175", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the arrival of the R packages nlme and lme4, linear mixed models (LMMs)\nhave come to be widely used in experimentally-driven areas like psychology,\nlinguistics, and cognitive science. This tutorial provides a practical\nintroduction to fitting LMMs in a Bayesian framework using the probabilistic\nprogramming language Stan. We choose Stan (rather than WinBUGS or JAGS) because\nit provides an elegant and scalable framework for fitting models in most of the\nstandard applications of LMMs. We ease the reader into fitting increasingly\ncomplex LMMs, first using a two-condition repeated measures self-paced reading\nstudy, followed by a more complex $2\\times 2$ repeated measures factorial\ndesign that can be generalized to much more complex designs.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 05:50:33 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Sorensen", "Tanner", ""], ["Vasishth", "Shravan", ""]]}, {"id": "1506.06268", "submitter": "Abhra Sarkar", "authors": "Abhra Sarkar and David B. Dunson", "title": "Bayesian Nonparametric Modeling of Higher Order Markov Chains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of flexible modeling of higher order Markov chains\nwhen an upper bound on the order of the chain is known but the true order and\nnature of the serial dependence are unknown. We propose Bayesian nonparametric\nmethodology based on conditional tensor factorizations, which can characterize\nany transition probability with a specified maximal order. The methodology\nselects the important lags and captures higher order interactions among the\nlags, while also facilitating calculation of Bayes factors for a variety of\nhypotheses of interest. We design efficient Markov chain Monte Carlo algorithms\nfor posterior computation, allowing for uncertainty in the set of important\nlags to be included and in the nature and order of the serial dependence. The\nmethods are illustrated using simulation experiments and real world\napplications.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jun 2015 16:54:16 GMT"}, {"version": "v2", "created": "Sat, 27 Jun 2015 14:51:26 GMT"}, {"version": "v3", "created": "Tue, 30 Jun 2015 18:55:26 GMT"}, {"version": "v4", "created": "Tue, 20 Oct 2015 15:09:43 GMT"}], "update_date": "2015-10-21", "authors_parsed": [["Sarkar", "Abhra", ""], ["Dunson", "David B.", ""]]}, {"id": "1506.06322", "submitter": "Mohammad Jafari Jozani", "authors": "Saeid Amiri, Mohammad Jafari Jozani, Reza Modarres", "title": "Exponentially Titled Empirical Distribution Function for Ranked Set\n  Samples", "comments": "18 pages, 3 Figuers, 6 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study nonparametric estimation of the distribution function (DF) of a\ncontinuous random variable based on a ranked set sampling design using the\nexponentially tilted (ET) empirical likelihood method. We propose ET estimators\nof the DF and use them to construct new resampling algorithms for unbalanced\nranked set samples. We explore the properties of the proposed algorithms. For a\nhypothesis testing problem about the underlying population mean, we show that\nthe bootstrap tests based on the ET estimators of the DF are asymptotically\nnormal and exhibit a small bias of order $O(n^{-1})$. We illustrate the methods\nand evaluate the finite sample performance of the algorithms under both perfect\nand imperfect ranking schemes using a real data set and several Monte Carlo\nsimulation studies. We compare the performance of the test statistics based on\nthe ET estimators with those based on the empirical likelihood estimators.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 05:11:06 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Amiri", "Saeid", ""], ["Jozani", "Mohammad Jafari", ""], ["Modarres", "Reza", ""]]}, {"id": "1506.06330", "submitter": "Zhong Guan", "authors": "Zhong Guan", "title": "Bernstein Polynomial Model for Grouped Continuous Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grouped data are commonly encountered in applications. The Bernstein\npolynomial model is proposed as an approximate model in this paper for\nestimating a univariate density function based on grouped data. The\ncoefficients of the Bernstein polynomial, as the mixture proportions of beta\ndistributions, can be estimated using an EM algorithm. The optimal degree of\nthe Bernstein polynomial can be determined using a change-point estimation\nmethod. The rate of convergence of the proposed density estimate to the true\ndensity is proved to be almost parametric by an acceptance-rejection arguments\nused in Monte Carlo method. The proposed method is compared with some existing\nmethods in a simulation study and is applied to a real dataset.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 08:22:49 GMT"}, {"version": "v2", "created": "Sat, 18 Jul 2015 22:20:23 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Guan", "Zhong", ""]]}, {"id": "1506.06405", "submitter": "Ville Satopaa", "authors": "Ville Satop\\\"a\\\"a and Lyle Ungar", "title": "Combining and Extremizing Real-Valued Forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The weighted average is by far the most popular approach to combining\nmultiple forecasts of some future outcome. This paper shows that both for\nprobability or real-valued forecasts, a non-trivial weighted average of\ndifferent forecasts is always sub-optimal. More specifically, it is not\nconsistent with any set of information about the future outcome even if the\nindividual forecasts are. Furthermore, weighted averaging does not behave as if\nit collects information from the forecasters and hence needs to be extremized,\nthat is, systematically transformed away from the marginal mean. This paper\nproposes a linear extremization technique for improving the weighted average of\nreal-valued forecasts. The resulting more extreme version of the weighted\naverage exhibits many properties of optimal aggregation. Both this and the\nsub-optimality of the weighted average are illustrated with simple examples\ninvolving synthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 19:45:10 GMT"}, {"version": "v2", "created": "Fri, 25 Sep 2015 15:44:03 GMT"}], "update_date": "2015-09-28", "authors_parsed": [["Satop\u00e4\u00e4", "Ville", ""], ["Ungar", "Lyle", ""]]}, {"id": "1506.06502", "submitter": "Ehsan Zamanzade", "authors": "Ehsan Zamanzade and Michael Vock", "title": "Variance Estimation in Ranked Set Sampling Using a Concomitant Variable", "comments": null, "journal-ref": "Statistics and probability letters, 105,1-5 (2015)", "doi": "10.1016/j.spl.2015.04.034", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a nonparametric variance estimator when ranked set sampling (RSS)\nand judgment post stratification (JPS) are applied by measuring a concomitant\nvariable. Our proposed estimator is obtained by conditioning on observed\nconcomitant values and using nonparametric kernel regression.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 08:45:06 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Zamanzade", "Ehsan", ""], ["Vock", "Michael", ""]]}, {"id": "1506.06696", "submitter": "Philip Leifeld", "authors": "Philip Leifeld and Skyler J. Cranmer", "title": "A Theoretical and Empirical Comparison of the Temporal Exponential\n  Random Graph Model and the Stochastic Actor-Oriented Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The temporal exponential random graph model (TERGM) and the stochastic\nactor-oriented model (SAOM, e.g., SIENA) are popular models for longitudinal\nnetwork analysis. We compare these models theoretically, via simulation, and\nthrough a real-data example in order to assess their relative strengths and\nweaknesses. Though we do not aim to make a general claim about either being\nsuperior to the other across all specifications, we highlight several\ntheoretical differences the analyst might consider and find that with some\nspecifications, the two models behave very similarly, while each model\nout-predicts the other one the more the specific assumptions of the respective\nmodel are met.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 17:34:53 GMT"}, {"version": "v2", "created": "Mon, 7 Sep 2015 14:43:04 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 21:37:58 GMT"}, {"version": "v4", "created": "Fri, 8 Jun 2018 04:59:13 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Leifeld", "Philip", ""], ["Cranmer", "Skyler J.", ""]]}, {"id": "1506.06707", "submitter": "Faicel Chamroukhi", "authors": "Faicel Chamroukhi", "title": "Non-Normal Mixtures of Experts", "comments": "61 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of Experts (MoE) is a popular framework for modeling heterogeneity in\ndata for regression, classification and clustering. For continuous data which\nwe consider here in the context of regression and cluster analysis, MoE usually\nuse normal experts, that is, expert components following the Gaussian\ndistribution. However, for a set of data containing a group or groups of\nobservations with asymmetric behavior, heavy tails or atypical observations,\nthe use of normal experts may be unsuitable and can unduly affect the fit of\nthe MoE model. In this paper, we introduce new non-normal mixture of experts\n(NNMoE) which can deal with these issues regarding possibly skewed,\nheavy-tailed data and with outliers. The proposed models are the skew-normal\nMoE and the robust $t$ MoE and skew $t$ MoE, respectively named SNMoE, TMoE and\nSTMoE. We develop dedicated expectation-maximization (EM) and expectation\nconditional maximization (ECM) algorithms to estimate the parameters of the\nproposed models by monotonically maximizing the observed data log-likelihood.\nWe describe how the presented models can be used in prediction and in\nmodel-based clustering of regression data. Numerical experiments carried out on\nsimulated data show the effectiveness and the robustness of the proposed models\nin terms modeling non-linear regression functions as well as in model-based\nclustering. Then, to show their usefulness for practical applications, the\nproposed models are applied to the real-world data of tone perception for\nmusical data analysis, and the one of temperature anomalies for the analysis of\nclimate change data.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 18:12:36 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 14:18:03 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Chamroukhi", "Faicel", ""]]}, {"id": "1506.06823", "submitter": "James McCracken", "authors": "James M. McCracken and Robert S. Weigel", "title": "Non-parametric causal inference for bivariate time series", "comments": "15 pages, 10 figures; added some notes to address various reviewer\n  comments", "journal-ref": "Phys. Rev. E 93, 022207 (2015)", "doi": "10.1103/PhysRevE.93.022207", "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new quantities for exploratory causal inference between\nbivariate time series. The quantities, called penchants and leanings, are\ncomputationally straightforward to apply, follow directly from assumptions of\nprobabilistic causality, do not depend on any assumed models for the time\nseries generating process, and do not rely on any embedding procedures; these\nfeatures may provide a clearer interpretation of the results than those from\nexisting time series causality tools. The penchant and leaning are computed\nbased on a structured method for computing probabilities.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 23:43:38 GMT"}, {"version": "v2", "created": "Wed, 27 Jan 2016 17:04:01 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["McCracken", "James M.", ""], ["Weigel", "Robert S.", ""]]}, {"id": "1506.06831", "submitter": "Paul Northrop", "authors": "Paul J. Northrop", "title": "An efficient semiparametric maxima estimator of the extremal index", "comments": "17 pages, 7 figures. Minor edits made to version 1 prior to journal\n  publication. The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s10687-015-0221-5", "journal-ref": "Extremes 18 (2015) 585-603", "doi": "10.1007/s10687-015-0221-5", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extremal index $\\theta$, a measure of the degree of local dependence in\nthe extremes of a stationary process, plays an important role in extreme value\nanalyses. We estimate $\\theta$ semiparametrically, using the relationship\nbetween the distribution of block maxima and the marginal distribution of a\nprocess to define a semiparametric model. We show that these semiparametric\nestimators are simpler and substantially more efficient than their parametric\ncounterparts. We seek to improve efficiency further using maxima over sliding\nblocks. A simulation study shows that the semiparametric estimators are\ncompetitive with the leading estimators. An application to sea-surge heights\ncombines inferences about $\\theta$ with a standard extreme value analysis of\nblock maxima to estimate marginal quantiles.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 00:27:44 GMT"}, {"version": "v2", "created": "Mon, 13 Jul 2015 14:29:17 GMT"}, {"version": "v3", "created": "Sun, 2 Aug 2015 22:23:39 GMT"}], "update_date": "2016-06-02", "authors_parsed": [["Northrop", "Paul J.", ""]]}, {"id": "1506.06998", "submitter": "Paul Jenkins", "authors": "Paul A. Jenkins and Dario Spano", "title": "Exact simulation of the Wright-Fisher diffusion", "comments": "36 pages, 2 figure, 2 tables. This version corrects an error in the\n  proof of Lemma 6.1", "journal-ref": "Annals of Applied Probability 27(3):1478-1509 (2017)", "doi": "10.1214/16-AAP1236", "report-no": "CRiSM Working Paper 14-27", "categories": "stat.ME math.PR q-bio.PE stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wright-Fisher family of diffusion processes is a widely used class of\nevolutionary models. However, simulation is difficult because there is no known\nclosed-form formula for its transition function. In this article we demonstrate\nthat it is in fact possible to simulate exactly from a broad class of\nWright-Fisher diffusion processes and their bridges. For those diffusions\ncorresponding to reversible, neutral evolution, our key idea is to exploit an\neigenfunction expansion of the transition function; this approach even applies\nto its infinite-dimensional analogue, the Fleming-Viot process. We then develop\nan exact rejection algorithm for processes with more general drift functions,\nincluding those modelling natural selection, using ideas from retrospective\nsimulation. Our approach also yields methods for exact simulation of the moment\ndual of the Wright-Fisher diffusion, the ancestral process of an infinite-leaf\nKingman coalescent tree. We believe our new perspective on diffusion simulation\nholds promise for other models admitting a transition eigenfunction expansion.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 13:47:42 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 15:04:56 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 13:32:15 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Jenkins", "Paul A.", ""], ["Spano", "Dario", ""]]}, {"id": "1506.07186", "submitter": "Haimeng Zhang", "authors": "Chunfeng Huang, Haimeng Zhang, and Scott M. Robeson", "title": "Intrinsic Random Functions and Universal Kriging on the Circle", "comments": "14 pages, initial version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsic random functions (IRF) provide a versatile approach when the\nassumption of second-order stationarity is not met. Here, we develop the IRF\ntheory on the circle with its universal kriging application. Unlike IRF in\nEuclidean spaces, where differential operations are used to achieve\nstationarity, our result shows that low-frequency truncation of the Fourier\nseries representation of the IRF is required for such processes on the circle.\nAll of these features and developments are presented through the theory of\nreproducing kernel Hilbert space. In addition, the connection between kriging\nand splines is also established, demonstrating their equivalence on the circle.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 20:02:43 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Huang", "Chunfeng", ""], ["Zhang", "Haimeng", ""], ["Robeson", "Scott M.", ""]]}, {"id": "1506.07412", "submitter": "Tristan Gray-Davies", "authors": "Tristan Gray-Davies, Chris Holmes and Francois Caron", "title": "Scalable Bayesian nonparametric regression via a Plackett-Luce model for\n  conditional ranks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Bayesian nonparametric regression model for covariates X\nand continuous, real response variable Y. The model is parametrized in terms of\nmarginal distributions for Y and X and a regression function which tunes the\nstochastic ordering of the conditional distributions F(y|x). By adopting an\napproximate composite likelihood approach, we show that the resulting posterior\ninference can be decoupled for the separate components of the model. This\nprocedure can scale to very large datasets and allows for the use of standard,\nexisting, software from Bayesian nonparametric density estimation and\nPlackett-Luce ranking estimation to be applied. As an illustration, we show an\napplication of our approach to a US Census dataset, with over 1,300,000 data\npoints and more than 100 covariates.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 15:09:25 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Gray-Davies", "Tristan", ""], ["Holmes", "Chris", ""], ["Caron", "Francois", ""]]}, {"id": "1506.07415", "submitter": "Ana\\\"is Rouanet", "authors": "Ana\\\"is Rouanet, Pierre Joly, Jean-Fran\\c{c}ois Dartigues, C\\'ecile\n  Proust-Lima and H\\'el\\`ene Jacqmin-Gadda", "title": "Joint latent class model for longitudinal data and interval-censored\n  semi-competing events: Application to dementia", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint models are used in ageing studies to investigate the association\nbetween longitudinal markers and a time-to-event, and have been extended to\nmultiple markers and/or competing risks. The competing risk of death must be\nconsidered in the elderly because death and dementia have common risk factors.\nMoreover, in cohort studies, time-to-dementia is interval-censored because\ndementia is only assessed intermittently. So subjects can become demented and\ndie between two follow-up visits without being diagnosed. To study pre-dementia\ncognitive decline, we propose a joint latent class model combining a (possibly\nmultivariate) mixed model and an illness-death model handling both interval\ncensoring (by accounting for a possible unobserved transition to dementia) and\nsemi-competing risks. Parameters are estimated by maximum likelihood handling\ninterval censoring. The correlation between the marker and the times-to-events\nis captured by latent classes, homogeneous groups with specific risks of death\nand dementia and profiles of cognitive decline. We propose markovian and\nsemi-markovian versions. Both approaches are compared to a joint latent class\nmodel for standard competing risks through a simulation study, and then applied\nin a prospective cohort study of cerebral and functional ageing to distinguish\ndifferent profiles of cognitive decline associated with risks of dementia and\ndeath. The comparison highlights that among demented subjects, mortality\ndepends more on age than duration of dementia. This model distinguishes the\nso-called terminal pre-death decline (among non-demented subjects) from the\npre-dementia decline.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 15:11:25 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Rouanet", "Ana\u00efs", ""], ["Joly", "Pierre", ""], ["Dartigues", "Jean-Fran\u00e7ois", ""], ["Proust-Lima", "C\u00e9cile", ""], ["Jacqmin-Gadda", "H\u00e9l\u00e8ne", ""]]}, {"id": "1506.07447", "submitter": "Hannes Matuschek", "authors": "Hannes Matuschek", "title": "Fraud detection with statistics: A comment on \"Evidential Value in\n  ANOVA-Regression Results in Scientific Integrity Studies\" (Klaassen, 2015)", "comments": "9 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Klaassen in (Klaassen 2015) proposed a method for the detection of data\nmanipulation given the means and standard deviations for the cells of a oneway\nANOVA design. This comment critically reviews this method. In addition,\ninspired by this analysis, an alternative approach to test sample correlations\nover several experiments is derived. The results are in close agreement with\nthe initial analysis reported by an anonymous whistlelblower. Importantly, the\nstatistic requires several similar experiments; a test for correlations between\n3 sample means based on a single experiment must be considered as unreliable.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 16:13:18 GMT"}, {"version": "v2", "created": "Wed, 15 Jul 2015 12:40:43 GMT"}], "update_date": "2015-07-16", "authors_parsed": [["Matuschek", "Hannes", ""]]}, {"id": "1506.07454", "submitter": "Marina Paez", "authors": "Marina S. Paez and Stephen G. Walker", "title": "Modeling with a Large Class of Unimodal Multivariate Distributions", "comments": "33 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new class of multivariate unimodal\ndistributions, motivated by Khintchine's representation. We start by proposing\na univariate model, whose support covers all the unimodal distributions on the\nreal line. The proposed class of unimodal distributions can be naturally\nextended to higher dimensions, by using the multivariate Gaussian copula. Under\nboth univariate and multivariate settings, we provide MCMC algorithms to\nperform inference about the model parameters and predictive densities. The\nmethodology is illustrated with univariate and bivariate examples, and with\nvariables taken from a real data-set.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 16:29:22 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Paez", "Marina S.", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1506.07456", "submitter": "Rand Wilcox", "authors": "Rand Wilcox", "title": "Comparisons of two quantile regression smoothers", "comments": "18 pp, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper compares the small-sample properties of two non-parametric quantile\nregression estimators. The first is based on constrained B-spline smoothing\n(COBS) and the other is based on a variation and slight extension of a running\ninterval smoother, which apparently has not been studied via simulations. The\nmotivation for this paper stems from the Well Elderly 2 study, a portion of\nwhich was aimed at understanding the association between the cortisol awakening\nresponse and two measures of stress.\n  COBS indicated what appeared be an usual form of curvature. The modified\nrunning interval smoother gave a strikingly different estimate, which raised\nthe issue of how it compares to COBS in terms of mean squared error and bias as\nwell as its ability to avoid a spurious indication of curvature. R functions\nfor applying the methods were used in conjunction with default settings for the\nvarious optional arguments. The results indicate that the modified running\ninterval smoother has practical value. Manipulation of the optional arguments\nmight impact the relative merits of the two methods, but the extent to which\nthis is the case remains unknown.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 16:31:04 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Wilcox", "Rand", ""]]}, {"id": "1506.07461", "submitter": "Rand Wilcox", "authors": "Rand Wilcox", "title": "Global comparisons of medians and other quantiles in a one-way design\n  when there are tied values", "comments": "18 pp. 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For $J \\ge 2$ independent groups, the paper deals with testing the global\nhypothesis that all $J$ groups have a common population median or identical\nquantiles, with an emphasis on the quartiles. Classic rank-based methods are\nsometimes suggested for comparing medians, but it is well known that under\ngeneral conditions they do not adequately address this goal. Extant methods\nbased on the usual sample median are unsatisfactory when there are tied values\nexcept for the special case $J=2$. A variation of the percentile bootstrap used\nin conjunction with the Harrell--Davis quantile estimator performs well in\nsimulations. The method is illustrated with data from the Well Elderly 2 study.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 16:41:14 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Wilcox", "Rand", ""]]}, {"id": "1506.07464", "submitter": "Catherine Matias", "authors": "Catherine Matias and Vincent Miele", "title": "Statistical clustering of temporal networks through a dynamic stochastic\n  block model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical node clustering in discrete time dynamic networks is an emerging\nfield that raises many challenges. Here, we explore statistical properties and\nfrequentist inference in a model that combines a stochastic block model (SBM)\nfor its static part with independent Markov chains for the evolution of the\nnodes groups through time. We model binary data as well as weighted dynamic\nrandom graphs (with discrete or continuous edges values). Our approach,\nmotivated by the importance of controlling for label switching issues across\nthe different time steps, focuses on detecting groups characterized by a stable\nwithin group connectivity behavior. We study identifiability of the model\nparameters, propose an inference procedure based on a variational expectation\nmaximization algorithm as well as a model selection criterion to select for the\nnumber of groups. We carefully discuss our initialization strategy which plays\nan important role in the method and compare our procedure with existing ones on\nsynthetic datasets. We also illustrate our approach on dynamic contact\nnetworks, one of encounters among high school students and two others on animal\ninteractions. An implementation of the method is available as a R package\ncalled dynsbm.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 16:49:45 GMT"}, {"version": "v2", "created": "Wed, 22 Jun 2016 05:53:45 GMT"}], "update_date": "2016-06-23", "authors_parsed": [["Matias", "Catherine", ""], ["Miele", "Vincent", ""]]}, {"id": "1506.07467", "submitter": "Rand Wilcox", "authors": "Rand Wilcox", "title": "ANCOVA: A global test based on a robust measure of location or quantiles\n  when there is curvature", "comments": "23 pp 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For two independent groups, let $M_j(x)$ be some conditional measure of\nlocation for the $j$th group associated with some random variable $Y$, given\nthat some covariate $X=x$. When $M_j(x)$ is a robust measure of location, or\neven some conditional quantile of $Y$, given $X$, methods have been proposed\nand studied that are aimed at testing $H_0$: $M_1(x)=M_2(x)$ that deal with\ncurvature in a flexible manner. In addition, methods have been studied where\nthe goal is to control the probability of one or more Type I errors when\ntesting $H_0$ for each $x \\in \\{x_1, \\ldots, x_p\\}$. This paper suggests a\nmethod for testing the global hypothesis $H_0$: $M_1(x)=M_2(x)$ for $\\forall x\n\\in \\{x_1, \\ldots, x_p\\}$ when using a robust or quantile location estimator.\nAn obvious advantage of testing $p$ hypotheses, rather than the global\nhypothesis, is that it can provide information about where regression lines\ndiffer and by how much. But the paper summarizes three general reasons to\nsuspect that testing the global hypothesis can have more power. 2 Data from the\nWell Elderly 2 study illustrate that testing the global hypothesis can make a\npractical difference.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 17:00:42 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Wilcox", "Rand", ""]]}, {"id": "1506.07669", "submitter": "Preetam Nandy", "authors": "Marloes H. Maathuis and Preetam Nandy", "title": "A review of some recent advances in causal inference", "comments": "23 pages, 4 figures, To appear in the \"Handbook of Big Data\", Chapman\n  and Hall", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a selective review of some recent developments in causal inference,\nintended for researchers who are not familiar with graphical models and\ncausality, and with a focus on methods that are applicable to large data sets.\nWe mainly address the problem of estimating causal effects from observational\ndata. For example, one can think of estimating the effect of single or multiple\ngene knockouts from wild-type gene expression data, that is, from gene\nexpression measurements that were obtained without doing any gene knockout\nexperiments.\n  We assume that the observational data are generated from a causal structure\nthat can be represented by a directed acyclic graph (DAG). First, we discuss\nestimation of causal effects when the underlying causal DAG is known. In\nlarge-scale networks, however, the causal DAG is often unknown. Next, we\ntherefore discuss causal structure learning, that is, learning information\nabout the causal structure from observational data. We then combine these two\nparts and discuss methods to estimate (bounds on) causal effects from\nobservational data when the causal structure is unknown. We also illustrate\nthis method on a yeast gene expression data set. We close by mentioning several\nextensions of the discussed work.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 08:59:23 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Maathuis", "Marloes H.", ""], ["Nandy", "Preetam", ""]]}, {"id": "1506.07800", "submitter": "Anna-Kaisa Ylitalo", "authors": "Antti Penttinen and Anna-Kaisa Ylitalo", "title": "Deducing self-interaction in eye movement data using sequential spatial\n  point processes", "comments": "27 pages, 13 figures and 2 tables", "journal-ref": "Spatial Statistics 17 (2016) 1-21", "doi": "10.1016/j.spasta.2016.03.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Eye movement data are outputs of an analyser tracking the gaze when a person\nis inspecting a scene. These kind of data are of increasing importance in\nscientific research as well as in applications, e.g. in marketing and\nman-machine interface planning. Thus the new areas of application call for\nadvanced analysis tools. Our research objective is to suggest statistical\nmodelling of eye movement sequences using sequential spatial point processes,\nwhich decomposes the variation in data into structural components having\ninterpretation.\n  We consider three elements of an eye movement sequence: heterogeneity of the\ntarget space, contextuality between subsequent movements, and time-dependent\nbehaviour describing self-interaction. We propose two model constructions. One\nis based on the history-dependent rejection of transitions in a random walk and\nthe other makes use of a history-adapted kernel function penalized by\nuser-defined geometric model characteristics. Both models are inhomogeneous\nself-interacting random walks. Statistical inference based on the likelihood is\nsuggested, some experiments are carried out, and the models are used for\ndetermining the uncertainty of important data summaries for eye movement data.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 15:56:53 GMT"}, {"version": "v2", "created": "Thu, 29 Oct 2015 16:30:04 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 15:02:19 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Penttinen", "Antti", ""], ["Ylitalo", "Anna-Kaisa", ""]]}, {"id": "1506.07806", "submitter": "Riccardo Rastelli", "authors": "Riccardo Rastelli, Nial Friel, Adrian E. Raftery", "title": "Properties of Latent Variable Network Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive properties of Latent Variable Models for networks, a broad class of\nmodels that includes the widely-used Latent Position Models. These include the\naverage degree distribution, clustering coefficient, average path length and\ndegree correlations. We introduce the Gaussian Latent Position Model, and\nderive analytic expressions and asymptotic approximations for its network\nproperties. We pay particular attention to one special case, the Gaussian\nLatent Position Models with Random Effects, and show that it can represent the\nheavy-tailed degree distributions, positive asymptotic clustering coefficients\nand small-world behaviours that are often observed in social networks. Several\nreal and simulated examples illustrate the ability of the models to capture\nimportant features of observed networks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 16:14:50 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Rastelli", "Riccardo", ""], ["Friel", "Nial", ""], ["Raftery", "Adrian E.", ""]]}, {"id": "1506.07825", "submitter": "Kody Law", "authors": "K.J.H. Law, A.M. Stuart, and K.C. Zygalakis", "title": "Data Assimilation: A Mathematical Introduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS math.OC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These notes provide a systematic mathematical treatment of the subject of\ndata assimilation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 17:25:34 GMT"}], "update_date": "2015-06-26", "authors_parsed": [["Law", "K. J. H.", ""], ["Stuart", "A. M.", ""], ["Zygalakis", "K. C.", ""]]}, {"id": "1506.07836", "submitter": "Emeric Thibaud", "authors": "Emeric Thibaud, Juha Aalto, Daniel S. Cooley, Anthony C. Davison, Juha\n  Heikkinen", "title": "Bayesian inference for the Brown-Resnick process, with an application to\n  extreme low temperatures", "comments": null, "journal-ref": "The Annals of Applied Statistics, 2016, Vol. 10, No. 4, 2303-2324", "doi": "10.1214/16-AOAS980", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Brown-Resnick max-stable process has proven to be well-suited for\nmodeling extremes of complex environmental processes, but in many applications\nits likelihood function is intractable and inference must be based on a\ncomposite likelihood, thereby preventing the use of classical Bayesian\ntechniques. In this paper we exploit a case in which the full likelihood of a\nBrown-Resnick process can be calculated, using componentwise maxima and their\npartitions in terms of individual events, and we propose two new approaches to\ninference. The first estimates the partitions using declustering, while the\nsecond uses random partitions in a Markov chain Monte Carlo algorithm. We use\nthese approaches to construct a Bayesian hierarchical model for extreme low\ntemperatures in northern Fennoscandia.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jun 2015 18:04:02 GMT"}, {"version": "v2", "created": "Mon, 17 Oct 2016 20:53:14 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Thibaud", "Emeric", ""], ["Aalto", "Juha", ""], ["Cooley", "Daniel S.", ""], ["Davison", "Anthony C.", ""], ["Heikkinen", "Juha", ""]]}, {"id": "1506.08149", "submitter": "Lan Liu", "authors": "Lan Liu, Wang Miao, Baoluo Sun, James Robins, and Eric Tchetgen\n  Tchetgen", "title": "Identification and Inference for Marginal Average Treatment Effect on\n  the Treated With an Instrumental Variable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In observational studies, treatments are typically not randomized and\ntherefore estimated treatment effects may be subject to confounding bias. The\ninstrumental variable (IV) design plays the role of a quasi-experimental handle\nsince the IV is associated with the treatment and only affects the outcome\nthrough the treatment. In this paper, we present a novel framework for\nidentification and inference using an IV for the marginal average treatment\neffect amongst the treated (ETT) in the presence of unmeasured confounding. For\ninference, we propose three different semiparametric approaches: (i) inverse\nprobability weighting (IPW), (ii) outcome regression (OR), and (iii) doubly\nrobust (DR) estimation, which is consistent if either (i) or (ii) is\nconsistent, but not necessarily both. A closed-form locally semiparametric\nefficient estimator is obtained in the simple case of binary IV and outcome and\nthe efficiency bound is derived for the more general case.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 16:37:37 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 20:30:28 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Liu", "Lan", ""], ["Miao", "Wang", ""], ["Sun", "Baoluo", ""], ["Robins", "James", ""], ["Tchetgen", "Eric Tchetgen", ""]]}, {"id": "1506.08180", "submitter": "Amar Shah", "authors": "Amar Shah and David A. Knowles and Zoubin Ghahramani", "title": "An Empirical Study of Stochastic Variational Algorithms for the Beta\n  Bernoulli Process", "comments": "ICML, 12 pages. Volume 37: Proceedings of The 32nd International\n  Conference on Machine Learning, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic variational inference (SVI) is emerging as the most promising\ncandidate for scaling inference in Bayesian probabilistic models to large\ndatasets. However, the performance of these methods has been assessed primarily\nin the context of Bayesian topic models, particularly latent Dirichlet\nallocation (LDA). Deriving several new algorithms, and using synthetic, image\nand genomic datasets, we investigate whether the understanding gleaned from LDA\napplies in the setting of sparse latent factor models, specifically beta\nprocess factor analysis (BPFA). We demonstrate that the big picture is\nconsistent: using Gibbs sampling within SVI to maintain certain posterior\ndependencies is extremely effective. However, we find that different posterior\ndependencies are important in BPFA relative to LDA. Particularly,\napproximations able to model intra-local variable dependence perform best.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 18:55:11 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Shah", "Amar", ""], ["Knowles", "David A.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1506.08237", "submitter": "Peter Hoff", "authors": "Peter D. Hoff", "title": "Dyadic data analysis with amen", "comments": "This is a vignette for the R package \"amen\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dyadic data on pairs of objects, such as relational or social network data,\noften exhibit strong statistical dependencies. Certain types of second-order\ndependencies, such as degree heterogeneity and reciprocity, can be\nwell-represented with additive random effects models. Higher-order\ndependencies, such as transitivity and stochastic equivalence, can often be\nrepresented with multiplicative effects. The \"amen\" package for the R\nstatistical computing environment provides estimation and inference for a class\nof additive and multiplicative random effects models for ordinal, continuous,\nbinary and other types of dyadic data. The package also provides methods for\nmissing, censored and fixed-rank nomination data, as well as longitudinal\ndyadic data. This tutorial illustrates the \"amen\" package via example\nstatistical analyses of several of these different data types.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 23:48:01 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Hoff", "Peter D.", ""]]}, {"id": "1506.08253", "submitter": "Yanxun Xu", "authors": "Yanxun Xu and Peter Mueller and Donatello Telesca", "title": "Bayesian Inference for Latent Biologic Structure with Determinantal\n  Point Processes (DPP)", "comments": null, "journal-ref": null, "doi": "10.1111/biom.12482", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the use of the determinantal point process (DPP) as a prior for\nlatent structure in biomedical applications, where inference often centers on\nthe interpretation of latent features as biologically or clinically meaningful\nstructure. Typical examples include mixture models, when the terms of the\nmixture are meant to represent clinically meaningful subpopulations (of\npatients, genes, etc.). Another class of examples are feature allocation\nmodels. We propose the DPP prior as a repulsive prior on latent mixture\ncomponents in the first example, and as prior on feature-specific parameters in\nthe second case. We argue that the DPP is in general an attractive prior model\nfor latent structure when biologically relevant interpretation of such\nstructure is desired. We illustrate the advantages of DPP prior in three case\nstudies, including inference in mixture models for magnetic resonance images\n(MRI) and for protein expression, and a feature allocation model for gene\nexpression using data from The Cancer Genome Atlas. An important part of our\nargument are efficient and straightforward posterior simulation methods. We\nimplement a variation of reversible jump Markov chain Monte Carlo simulation\nfor inference under the DPP prior, using a density with respect to the unit\nrate Poisson process.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 03:56:25 GMT"}, {"version": "v2", "created": "Mon, 16 Nov 2015 22:27:13 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Xu", "Yanxun", ""], ["Mueller", "Peter", ""], ["Telesca", "Donatello", ""]]}, {"id": "1506.08256", "submitter": "Daniel Cervone", "authors": "Daniel Cervone and Natesh S. Pillai", "title": "Gaussian Process Regression with Location Errors", "comments": "28 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate Gaussian process regression models where inputs\nare subject to measurement error. In spatial statistics, input measurement\nerrors occur when the geographical locations of observed data are not known\nexactly. Such sources of error are not special cases of \"nugget\" or microscale\nvariation, and require alternative methods for both interpolation and parameter\nestimation. Gaussian process models do not straightforwardly extend to\nincorporate input measurement error, and simply ignoring noise in the input\nspace can lead to poor performance for both prediction and parameter inference.\nWe review and extend existing theory on prediction and estimation in the\npresence of location errors, and show that ignoring location errors may lead to\nKriging that is not \"self-efficient\". We also introduce a Markov Chain Monte\nCarlo (MCMC) approach using the Hybrid Monte Carlo algorithm that obtains\noptimal (minimum MSE) predictions, and discuss situations that lead to\nmultimodality of the target distribution and/or poor chain mixing. Through\nsimulation study and analysis of global air temperature data, we show that\nappropriate methods for incorporating location measurement error are essential\nto valid inference in this regime.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 04:24:10 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Cervone", "Daniel", ""], ["Pillai", "Natesh S.", ""]]}, {"id": "1506.08278", "submitter": "Francesco Bartolucci", "authors": "Francesco Bartolucci, Francesca Chiaromonte, Prabhani Kuruppumullage\n  Don, Bruce George Lindsay", "title": "Composite likelihood inference in a discrete latent variable model for\n  two-way \"clustering-by-segmentation\" problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a discrete latent variable model for two-way data arrays, which\nallows one to simultaneously produce clusters along one of the data dimensions\n(e.g. exchangeable observational units or features) and contiguous groups, or\nsegments, along the other (e.g. consecutively ordered times or locations). The\nmodel relies on a hidden Markov structure but, given its complexity, cannot be\nestimated by full maximum likelihood. We therefore introduce composite\nlikelihood methodology based on considering different subsets of the data. The\nproposed approach is illustrated by simulation, and with an application to\ngenomic data.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 10:35:59 GMT"}], "update_date": "2016-10-31", "authors_parsed": [["Bartolucci", "Francesco", ""], ["Chiaromonte", "Francesca", ""], ["Don", "Prabhani Kuruppumullage", ""], ["Lindsay", "Bruce George", ""]]}, {"id": "1506.08292", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universit\\'e Paris-Dauphine, University of\n  Warwick, and CREST)", "title": "The expected demise of the Bayes factor", "comments": "10 pages, one figure, discussion of Ly, A., Verhagen, A. J., and\n  Wagenmakers, E.-J. (in press). Harold Jeffreys's default Bayes factor\n  hypothesis tests: Explanation, extension, and application in psychology.\n  Journal of Mathematical Psychology", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note is a discussion commenting on the paper by Ly et al. on \"Harold\nJeffreys's Default Bayes Factor Hypothesis Tests: Explanation, Extension, and\nApplication in Psychology\" and on the perceived shortcomings of the classical\nBayesian approach to testing, while reporting on an alternative approach\nadvanced by Kamary, Mengersen, Robert and Rousseau (2014. arxiv:1412.2044) as a\nsolution to this quintessential inference problem.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 13:41:16 GMT"}, {"version": "v2", "created": "Mon, 27 Jul 2015 07:08:35 GMT"}], "update_date": "2015-07-28", "authors_parsed": [["Robert", "Christian P.", "", "Universit\u00e9 Paris-Dauphine, University of\n  Warwick, and CREST"]]}, {"id": "1506.08312", "submitter": "Long Feng", "authors": "Long Feng and Fasheng Sun", "title": "Spatial-Sign based High-Dimensional Location Test", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we consider the problem of testing the mean vector in the high\ndimensional settings. We proposed a new robust scalar transform invariant test\nbased on spatial sign. The proposed test statistic is asymptotically normal\nunder elliptical distributions. Simulation studies show that our test is very\nrobust and efficient in a wide range of distributions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 17:02:04 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Feng", "Long", ""], ["Sun", "Fasheng", ""]]}, {"id": "1506.08315", "submitter": "Long Feng", "authors": "Long Feng", "title": "High Dimensional Spatial Rank Test for Two-Sample Location Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article concerns tests for the two-sample location problem when the\ndimension is larger than the sample size. The traditional\nmultivariate-rank-based procedures cannot be used in high dimensional settings\nbecause the sample scatter matrix is not available. We propose a novel\nhigh-dimensional spatial rank test in this article. The asymptotic normality is\nestablished. We can allow the dimension being almost the exponential rate of\nthe sample sizes. Simulations demonstrate that it is very robust and efficient\nin a wide range of distributions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jun 2015 17:20:31 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Feng", "Long", ""]]}, {"id": "1506.08339", "submitter": "Sen Zhao", "authors": "Sen Zhao and Ali Shojaie", "title": "A Significance Test for Graph-Constrained Estimation", "comments": "42 pages, 3 tables, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-constrained estimation methods encourage similarities among neighboring\ncovariates presented as nodes on a graph, which can result in more accurate\nestimations, especially in high dimensional settings. Variable selection\napproaches can then be utilized to select a subset of variables that are\nassociated with the response. However, existing procedures do not provide\nmeasures of uncertainty of the estimates. Moreover, the vast majority of\nexisting approaches assume that available graphs accurately capture the\nassociation among covariates; violating this assumption could severely hurt the\nreliability of the resulting estimates. In this paper, we present an inference\nframework, called the Grace test, which simultaneously produces coefficient\nestimates and corresponding $p$-values while incorporating the external graph\ninformation. We show, both theoretically and via numerical studies, that the\nproposed method asymptotically controls the type-I error rate regardless of the\nchoice of the graph. When the underlying graph is informative, the Grace test\nis asymptotically more powerful than similar tests that ignore external\ninformation. We further propose a more general Grace-ridge test that results in\na higher power than the Grace test when the choice of the graph is not fully\ninformative. Our numerical studies show that as long as the graph is reasonably\ninformative, the proposed testing methods deliver improved statistical power\nover existing inference procedures that ignore external information.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 00:03:58 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Zhao", "Sen", ""], ["Shojaie", "Ali", ""]]}, {"id": "1506.08467", "submitter": "Long Feng", "authors": "Long Feng", "title": "Optimal Sign Test for High Dimensional Location Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article concerns tests for location parameters in cases where the data\ndimension is larger than the sample size. We propose a family of tests based on\nthe optimality arguments in Le Cam (1986) under elliptical symmetric. The\nasymptotic normality of these tests are established. By maximizing the\nasymptotic power function, we propose an uniformly optimal test for all\nelliptical symmetric distributions. The optimality is also confirmed by a Monte\nCarlo investigation.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 22:53:18 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Feng", "Long", ""]]}, {"id": "1506.08640", "submitter": "James Ridgway", "authors": "Nicolas Chopin and James Ridgway", "title": "Leave Pima Indians alone: binary regression as a benchmark for Bayesian\n  computation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract. Whenever a new approach to perform Bayesian computation is\nintroduced, a common practice is to showcase this approach on a binary\nregression model and datasets of moderate size. This paper discusses to which\nextent this practice is sound. It also reviews the current state of the art of\nBayesian computation, using binary regression as a running example. Both\nsampling-based algorithms (importance sampling, MCMC and SMC) and fast\napproximations (Laplace and EP) are covered. Extensive numerical results are\nprovided, some of which might go against conventional wisdom regarding the\neffectiveness of certain algorithms. Implications for other problems (variable\nselection) and other models are also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 14:14:38 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Chopin", "Nicolas", ""], ["Ridgway", "James", ""]]}, {"id": "1506.08796", "submitter": "So Young Park", "authors": "So Young Park and Ana-Maria Staicu", "title": "Longitudinal Functional Data Analysis", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider analysis of dependent functional data that are correlated because\nof a longitudinal-based design: each subject is observed at repeated time\nvisits and for each visit we record a functional variable. We propose a novel\nparsimonious modeling framework for the repeatedly observed functional\nvariables that allows to extract low dimensional features. The proposed\nmethodology accounts for the longitudinal design, is designed for the study of\nthe dynamic behavior of the underlying process, and is computationally fast.\nTheoretical properties of this framework are studied and numerical\ninvestigation confirms excellent behavior in finite samples. The proposed\nmethod is motivated by and applied to a diffusion tensor imaging study of\nmultiple sclerosis. Using Shiny (Chang et al., 2015) we implement interactive\nplots to help visualize longitudinal functional data as well as the various\ncomponents and prediction obtained using the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 19:33:00 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Park", "So Young", ""], ["Staicu", "Ana-Maria", ""]]}, {"id": "1506.08826", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen, Christopher R. Genovese, Larry Wasserman", "title": "Statistical Inference using the Morse-Smale Complex", "comments": "45 pages, 13 figures. Accepted to Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Morse-Smale complex of a function $f$ decomposes the sample space into\ncells where $f$ is increasing or decreasing. When applied to nonparametric\ndensity estimation and regression, it provides a way to represent, visualize,\nand compare multivariate functions. In this paper, we present some statistical\nresults on estimating Morse-Smale complexes. This allows us to derive new\nresults for two existing methods: mode clustering and Morse-Smale regression.\nWe also develop two new methods based on the Morse-Smale complex: a\nvisualization technique for multivariate functions and a two-sample,\nmultivariate hypothesis test.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 20:00:40 GMT"}, {"version": "v2", "created": "Tue, 4 Apr 2017 02:18:58 GMT"}], "update_date": "2017-04-05", "authors_parsed": [["Chen", "Yen-Chi", ""], ["Genovese", "Christopher R.", ""], ["Wasserman", "Larry", ""]]}, {"id": "1506.08910", "submitter": "Ravi Ganti", "authors": "Ravi Ganti, Nikhil Rao, Rebecca M. Willett and Robert Nowak", "title": "Learning Single Index Models in High Dimensions", "comments": "16 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single Index Models (SIMs) are simple yet flexible semi-parametric models for\nclassification and regression. Response variables are modeled as a nonlinear,\nmonotonic function of a linear combination of features. Estimation in this\ncontext requires learning both the feature weights, and the nonlinear function.\nWhile methods have been described to learn SIMs in the low dimensional regime,\na method that can efficiently learn SIMs in high dimensions has not been\nforthcoming. We propose three variants of a computationally and statistically\nefficient algorithm for SIM inference in high dimensions. We establish excess\nrisk bounds for the proposed algorithms and experimentally validate the\nadvantages that our SIM learning methods provide relative to Generalized Linear\nModel (GLM) and low dimensional SIM based learning methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 00:45:25 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Ganti", "Ravi", ""], ["Rao", "Nikhil", ""], ["Willett", "Rebecca M.", ""], ["Nowak", "Robert", ""]]}, {"id": "1506.08915", "submitter": "Jue Wang", "authors": "Jue Wang", "title": "Optimal Sequential Multi-class Diagnosis", "comments": "68 pages, 22 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential multi-class diagnosis, also known as multi-hypothesis testing, is\na classical sequential decision problem with broad applications. However, the\noptimal solution remains, in general, unknown as the dynamic program suffers\nfrom the curse of dimensionality in the posterior belief space. We consider a\nclass of practical problems in which the observation distributions associated\nwith different classes are related through exponential tilting, and show that\nthe reachable beliefs could be restricted on, or near, a set of\nlow-dimensional, time-dependent manifolds with closed-form expressions. This\nsparsity is driven by the low dimensionality of the observation distributions\n(which is intuitive) as well as by specific structural interrelations among\nthem (which is less intuitive). We use a matrix factorization approach to\nuncover the potential low dimensionality hidden in high-dimensional beliefs and\nreconstruct the beliefs using a diagnostic statistic in lower dimension. For\ncommon univariate distributions, e.g., normal, binomial, and Poisson, the\nbelief reconstruction is exact, and the optimal policies can be efficiently\ncomputed for a large number of classes. We also characterize the structure of\nthe optimal policy in the reduced dimension. For multivariate distributions, we\npropose a low-rank matrix approximation scheme that works well when the beliefs\nare near the low-dimensional manifolds. The optimal policy significantly\noutperforms the state-of-the-art heuristic policy in quick diagnosis with noisy\ndata. (forthcoming in Operations Research)\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 01:55:30 GMT"}, {"version": "v2", "created": "Thu, 26 Jan 2017 16:10:14 GMT"}, {"version": "v3", "created": "Mon, 23 Dec 2019 15:10:18 GMT"}, {"version": "v4", "created": "Thu, 3 Dec 2020 19:13:35 GMT"}], "update_date": "2020-12-07", "authors_parsed": [["Wang", "Jue", ""]]}, {"id": "1506.09163", "submitter": "Gautier Marti", "authors": "Gautier Marti and Frank Nielsen and Philippe Very and Philippe Donnat", "title": "Comment partitionner automatiquement des marches al\\'eatoires ? Avec\n  application \\`a la finance quantitative", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a novel non-parametric approach useful for\nclustering Markov processes. We introduce a pre-processing step consisting in\nmapping multivariate independent and identically distributed samples from\nrandom variables to a generic non-parametric representation which factorizes\ndependency and marginal distribution apart without losing any. An associated\nmetric is defined where the balance between random variables dependency and\ndistribution information is controlled by a single parameter. This mixing\nparameter can be learned or played with by a practitioner, such use is\nillustrated on the case of clustering financial time series. Experiments,\nimplementation and results obtained on public financial time series are online\non a web portal \\url{http://www.datagrapple.com}.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 17:17:10 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Marti", "Gautier", ""], ["Nielsen", "Frank", ""], ["Very", "Philippe", ""], ["Donnat", "Philippe", ""]]}]