[{"id": "1103.0612", "submitter": "Kentaro Tanaka", "authors": "Kentaro Tanaka, Atsushi Yagishita and Masami Miyakawa", "title": "Application of Mathematical Optimization Procedures to Intervention\n  Effects in Structural Equation Models", "comments": "19 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a given statistical model, it often happens that it is necessary to\nintervene the model to reduce the variances of the output variables. In\nstructural equation models, this can be done by changing the values of the path\ncoefficients by intervention. First, we explain that the expectations and\nvariance matrix can be decomposed into several parts in terms of the total\neffects. Then, we show that an algorithm to obtain intervention method which\nminimizes the weighted sum of the variances can be formulated as a convex\nquadratic programming. This formulation allows us to impose boundary conditions\nfor the intervention, so that we can find the practical solutions. We also\ntreat a problem to adjust the expectations on targets.\n", "versions": [{"version": "v1", "created": "Thu, 3 Mar 2011 06:28:03 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2011 05:19:41 GMT"}, {"version": "v3", "created": "Mon, 15 Aug 2011 02:24:31 GMT"}], "update_date": "2011-08-16", "authors_parsed": [["Tanaka", "Kentaro", ""], ["Yagishita", "Atsushi", ""], ["Miyakawa", "Masami", ""]]}, {"id": "1103.0818", "submitter": "Jie Peng", "authors": "Ru Wang, Jie Peng, Pei Wang", "title": "A note on logistic regression and logistic kernel machine models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a note on logistic regression models and logistic kernel machine\nmodels. It contains derivations to some of the expressions in a paper -- SNP\nSet Analysis for Detecting Disease Association Using Exon Sequence Data --\nsubmitted to BMC proceedings by these authors.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2011 04:25:01 GMT"}], "update_date": "2011-03-07", "authors_parsed": [["Wang", "Ru", ""], ["Peng", "Jie", ""], ["Wang", "Pei", ""]]}, {"id": "1103.0949", "submitter": "Cosma Rohilla Shalizi", "authors": "Cosma Rohilla Shalizi, Abigail Z. Jacobs, Kristina Lisa Klinkner,\n  Aaron Clauset", "title": "Adapting to Non-stationarity with Growing Expert Ensembles", "comments": "9 pages, 1 figure; CMU Statistics Technical Report. v2: Added\n  empirical example, revised discussion of related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with time series with complex non-stationarities, low\nretrospective regret on individual realizations is a more appropriate goal than\nlow prospective risk in expectation. Online learning algorithms provide\npowerful guarantees of this form, and have often been proposed for use with\nnon-stationary processes because of their ability to switch between different\nforecasters or ``experts''. However, existing methods assume that the set of\nexperts whose forecasts are to be combined are all given at the start, which is\nnot plausible when dealing with a genuinely historical or evolutionary system.\nWe show how to modify the ``fixed shares'' algorithm for tracking the best\nexpert to cope with a steadily growing set of experts, obtained by fitting new\nmodels to new data as it becomes available, and obtain regret bounds for the\ngrowing ensemble.\n", "versions": [{"version": "v1", "created": "Fri, 4 Mar 2011 17:04:20 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2011 23:25:41 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Shalizi", "Cosma Rohilla", ""], ["Jacobs", "Abigail Z.", ""], ["Klinkner", "Kristina Lisa", ""], ["Clauset", "Aaron", ""]]}, {"id": "1103.1110", "submitter": "Ngoc Mai Tran", "authors": "Ngoc Mai Tran", "title": "Pairwise ranking: choice of method can produce arbitrarily different\n  rank order", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine three methods for ranking by pairwise comparison: Principal\nEigenvector, HodgeRank and Tropical Eigenvector. It is shown that the choice of\nmethod can produce arbitrarily different rank order.To be precise, for any two\nof the three methods, and for any pair of rankings of at least four items,\nthere exists a comparison matrix for the items such that the rankings found by\nthe two methods are the prescribed ones. We discuss the implications of this\nresult in practice, study the geometry of the methods, and state some open\nproblems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Mar 2011 09:00:57 GMT"}], "update_date": "2011-03-08", "authors_parsed": [["Tran", "Ngoc Mai", ""]]}, {"id": "1103.1458", "submitter": "Kengo Kato", "authors": "Kengo Kato", "title": "Group Lasso for high dimensional sparse quantile regression models", "comments": "37 pages. Some errors are corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the statistical properties of the group Lasso estimator\nfor high dimensional sparse quantile regression models where the number of\nexplanatory variables (or the number of groups of explanatory variables) is\npossibly much larger than the sample size while the number of variables in\n\"active\" groups is sufficiently small. We establish a non-asymptotic bound on\nthe $\\ell_{2}$-estimation error of the estimator. This bound explains\nsituations under which the group Lasso estimator is potentially\nsuperior/inferior to the $\\ell_{1}$-penalized quantile regression estimator in\nterms of the estimation error. We also propose a data-dependent choice of the\ntuning parameter to make the method more practical, by extending the original\nproposal of Belloni and Chernozhukov (2011) for the $\\ell_{1}$-penalized\nquantile regression estimator. As an application, we analyze high dimensional\nadditive quantile regression models. We show that under a set of suitable\nregularity conditions, the group Lasso estimator can attain the convergence\nrate arbitrarily close to the oracle rate. Finally, we conduct simulations\nexperiments to examine our theoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 8 Mar 2011 08:11:54 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2011 09:12:03 GMT"}], "update_date": "2011-03-28", "authors_parsed": [["Kato", "Kengo", ""]]}, {"id": "1103.1761", "submitter": "Ferenc Husz\\'ar", "authors": "Ferenc Husz\\'ar and Simon Lacoste-Julien", "title": "A Kernel Approach to Tractable Bayesian Nonparametrics", "comments": "acknowledgements added to previous version, content otherwise\n  unchanged", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in popular nonparametric Bayesian models typically relies on\nsampling or other approximations. This paper presents a general methodology for\nconstructing novel tractable nonparametric Bayesian methods by applying the\nkernel trick to inference in a parametric Bayesian model. For example, Gaussian\nprocess regression can be derived this way from Bayesian linear regression.\nDespite the success of the Gaussian process framework, the kernel trick is\nrarely explicitly considered in the Bayesian literature. In this paper, we aim\nto fill this gap and demonstrate the potential of applying the kernel trick to\ntractable Bayesian parametric models in a wider context than just regression.\nAs an example, we present an intuitive Bayesian kernel machine for density\nestimation that is obtained by applying the kernel trick to a Gaussian\ngenerative model in feature space.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2011 11:39:21 GMT"}, {"version": "v2", "created": "Thu, 24 Mar 2011 17:05:24 GMT"}, {"version": "v3", "created": "Fri, 12 Aug 2011 10:56:21 GMT"}], "update_date": "2011-08-15", "authors_parsed": [["Husz\u00e1r", "Ferenc", ""], ["Lacoste-Julien", "Simon", ""]]}, {"id": "1103.1787", "submitter": "Kaspar Rufibach", "authors": "Kaspar Rufibach", "title": "A smooth ROC curve estimator based on log-concave density estimates", "comments": "23 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new smooth estimator of the ROC curve based on log-concave\ndensity estimates of the constituent distributions. We show that our estimate\nis asymptotically equivalent to the empirical ROC curve if the underlying\ndensities are in fact log-concave. In addition, we empirically show that our\nproposed estimator exhibits an efficiency gain for finite sample sizes with\nrespect to the standard empirical estimate in various scenarios and that it is\nonly slightly less efficient, if at all, compared to the fully parametric\nbinormal estimate in case the underlying distributions are normal. The\nestimator is also quite robust against modest deviations from the log-concavity\nassumption. We show that bootstrap confidence intervals for the value of the\nROC curve at a fixed false positive fraction based on the new estimate are on\naverage shorter compared to the approach by Zhou & Qin (2005), while\nmaintaining coverage probability. Computation of our proposed estimate uses the\nR package logcondens that implements univariate log-concave density estimation\nand can be done very efficiently using only one line of code. These obtained\nresults lead us to advocate our estimate for a wide range of scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 9 Mar 2011 14:03:48 GMT"}, {"version": "v2", "created": "Sun, 18 Sep 2011 10:34:34 GMT"}, {"version": "v3", "created": "Mon, 19 Dec 2011 19:15:03 GMT"}], "update_date": "2011-12-20", "authors_parsed": [["Rufibach", "Kaspar", ""]]}, {"id": "1103.1963", "submitter": "Hung Hung", "authors": "Hung Hung and Chin-Tsang Chiang", "title": "Nonparametric Methodology for the Time-Dependent Partial Area under the\n  ROC Curve", "comments": "20 pages, 4 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To assess the classification accuracy of a continuous diagnostic result, the\nreceiver operating characteristic (ROC) curve is commonly used in applications.\nThe partial area under the ROC curve (pAUC) is one of widely accepted summary\nmeasures due to its generality and ease of probability interpretation. In the\nfield of life science, a direct extension of the pAUC into the time-to-event\nsetting can be used to measure the usefulness of a biomarker for disease\ndetection over time. Without using a trapezoidal rule, we propose nonparametric\nestimators, which are easily computed and have closed-form expressions, for the\ntime-dependent pAUC. The asymptotic Gaussian processes of the estimators are\nestablished and the estimated variance-covariance functions are provided, which\nare essential in the construction of confidence intervals. The finite sample\nperformance of the proposed inference procedures are investigated through a\nseries of simulations. Our method is further applied to evaluate the\nclassification ability of CD4 cell counts on patient's survival time in the\nAIDS Clinical Trials Group (ACTG) 175 study. In addition, the inferences can be\ngeneralized to compare the time-dependent pAUCs between patients received the\nprior antiretroviral therapy and those without it.\n", "versions": [{"version": "v1", "created": "Thu, 10 Mar 2011 08:46:16 GMT"}], "update_date": "2011-03-11", "authors_parsed": [["Hung", "Hung", ""], ["Chiang", "Chin-Tsang", ""]]}, {"id": "1103.2372", "submitter": "Raydonal Ospina", "authors": "Raydonal Ospina, Silvia L. P. Ferrari", "title": "A general class of zero-or-one inflated beta regression models", "comments": "21 pages, 3 figures, 5 tables. Computational Statistics and Data\n  Analysis, 17 October 2011, ISSN 0167-9473\n  (http://www.sciencedirect.com/science/article/pii/S0167947311003628)", "journal-ref": null, "doi": "10.1016/j.csda.2011.10.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general class of regression models for continuous\nproportions when the data contain zeros or ones. The proposed class of models\nassumes that the response variable has a mixed continuous-discrete distribution\nwith probability mass at zero or one. The beta distribution is used to describe\nthe continuous component of the model, since its density has a wide range of\ndifferent shapes depending on the values of the two parameters that index the\ndistribution. We use a suitable parameterization of the beta law in terms of\nits mean and a precision parameter. The parameters of the mixture distribution\nare modeled as functions of regression parameters. We provide inference,\ndiagnostic, and model selection tools for this class of models. A practical\napplication that employs real data is presented.\n", "versions": [{"version": "v1", "created": "Fri, 11 Mar 2011 21:05:02 GMT"}, {"version": "v2", "created": "Sun, 20 Mar 2011 00:05:39 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2011 21:34:15 GMT"}], "update_date": "2011-11-04", "authors_parsed": [["Ospina", "Raydonal", ""], ["Ferrari", "Silvia L. P.", ""]]}, {"id": "1103.2486", "submitter": "Daniel Gervini", "authors": "Daniel Gervini", "title": "Dynamic Retrospective Regression for Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Samples of curves, or functional data, usually present phase variability in\naddition to amplitude variability. Existing functional regression methods do\nnot handle phase variability in an efficient way. In this paper we propose a\nfunctional regression method that incorporates phase synchronization as an\nintrinsic part of the model, and then attains better predictive power than\nordinary linear regression in a simple and parsimonious way. The finite-sample\nproperties of the estimators are studied by simulation. As an example of\napplication, we analyze neuromotor data arising from a study of human lip\nmovement.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2011 00:23:37 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2012 18:07:21 GMT"}, {"version": "v3", "created": "Tue, 8 Oct 2013 19:37:43 GMT"}], "update_date": "2013-10-09", "authors_parsed": [["Gervini", "Daniel", ""]]}, {"id": "1103.2523", "submitter": "Nanny Wermuth", "authors": "Nanny Wermuth (Department of Mathematics, Chalmers Technical\n  University, University of Gothenburg, Sweden) Kayvan Sadeghi (Department of\n  Statistics, University of Oxford, UK)", "title": "Sequences of regressions and their independences", "comments": "43 pages with 17 figures The manuscript is to appear as an invited\n  discussion paper in the journal TEST", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ordered sequences of univariate or multivariate regressions provide\nstatistical models for analysing data from randomized, possibly sequential\ninterventions, from cohort or multi-wave panel studies, but also from\ncross-sectional or retrospective studies. Conditional independences are\ncaptured by what we name regression graphs, provided the generated distribution\nshares some properties with a joint Gaussian distribution. Regression graphs\nextend purely directed, acyclic graphs by two types of undirected graph, one\ntype for components of joint responses and the other for components of the\ncontext vector variable. We review the special features and the history of\nregression graphs, derive criteria to read all implied independences of a\nregression graph and prove criteria for Markov equivalence that is to judge\nwhether two different graphs imply the same set of independence statements.\nKnowledge of Markov equivalence provides alternative interpretations of a given\nsequence of regressions, is essential for machine learning strategies and\npermits to use the simple graphical criteria of regression graphs on graphs for\nwhich the corresponding criteria are in general more complex. Under the known\nconditions that a Markov equivalent directed acyclic graph exists for any given\nregression graph, we give a polynomial time algorithm to find one such graph.\n", "versions": [{"version": "v1", "created": "Sun, 13 Mar 2011 15:22:48 GMT"}, {"version": "v2", "created": "Wed, 12 Oct 2011 10:36:22 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2012 15:25:16 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Wermuth", "Nanny", "", "Department of Mathematics, Chalmers Technical\n  University, University of Gothenburg, Sweden"], ["Sadeghi", "Kayvan", "", "Department of\n  Statistics, University of Oxford, UK"]]}, {"id": "1103.2697", "submitter": "Julien Chiquet", "authors": "Julien Chiquet, Yves Grandvalet, Camille Charbonnier", "title": "Sparsity with sign-coherent groups of variables via the\n  cooperative-Lasso", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS520 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 2, 795-830", "doi": "10.1214/11-AOAS520", "report-no": "IMS-AOAS-AOAS520", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problems of estimation and selection of parameters endowed\nwith a known group structure, when the groups are assumed to be sign-coherent,\nthat is, gathering either nonnegative, nonpositive or null parameters. To\ntackle this problem, we propose the cooperative-Lasso penalty. We derive the\noptimality conditions defining the cooperative-Lasso estimate for generalized\nlinear models, and propose an efficient active set algorithm suited to\nhigh-dimensional problems. We study the asymptotic consistency of the estimator\nin the linear regression setup and derive its irrepresentable conditions, which\nare milder than the ones of the group-Lasso regarding the matching of groups\nwith the sparsity pattern of the true parameters. We also address the problem\nof model selection in linear regression by deriving an approximation of the\ndegrees of freedom of the cooperative-Lasso estimator. Simulations comparing\nthe proposed estimator to the group and sparse group-Lasso comply with our\ntheoretical results, showing consistent improvements in support recovery for\nsign-coherent groups. We finally propose two examples illustrating the wide\napplicability of the cooperative-Lasso: first to the processing of ordinal\nvariables, where the penalty acts as a monotonicity prior; second to the\nprocessing of genomic data, where the set of differentially expressed probes is\nenriched by incorporating all the probes of the microarray that are related to\nthe corresponding genes.\n", "versions": [{"version": "v1", "created": "Mon, 14 Mar 2011 15:54:20 GMT"}, {"version": "v2", "created": "Wed, 28 Sep 2011 04:49:49 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2012 09:13:29 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Chiquet", "Julien", ""], ["Grandvalet", "Yves", ""], ["Charbonnier", "Camille", ""]]}, {"id": "1103.2852", "submitter": "Roberto D. Pascual-Marqui", "authors": "Roberto D. Pascual-Marqui and Rolando J. Biscay-Lirio", "title": "Interaction patterns of brain activity across space, time and frequency.\n  Part I: methods", "comments": "Technical report 2011-March-15, The KEY Institute for Brain-Mind\n  Research Zurich, KMU Osaka", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.NC stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We consider exploratory methods for the discovery of cortical functional\nconnectivity. Typically, data for the i-th subject (i=1...NS) is represented as\nan NVxNT matrix Xi, corresponding to brain activity sampled at NT moments in\ntime from NV cortical voxels. A widely used method of analysis first\nconcatenates all subjects along the temporal dimension, and then performs an\nindependent component analysis (ICA) for estimating the common cortical\npatterns of functional connectivity. There exist many other interesting\nvariations of this technique, as reviewed in [Calhoun et al. 2009 Neuroimage\n45: S163-172]. We present methods for the more general problem of discovering\nfunctional connectivity occurring at all possible time lags. For this purpose,\nbrain activity is viewed as a function of space and time, which allows the use\nof the relatively new techniques of functional data analysis [Ramsay &\nSilverman 2005: Functional data analysis. New York: Springer]. In essence, our\nmethod first vectorizes the data from each subject, which constitutes the\nnatural discrete representation of a function of several variables, followed by\nconcatenation of all subjects. The singular value decomposition (SVD), as well\nas the ICA of this new matrix of dimension [rows=(NT*NV); columns=NS] will\nreveal spatio-temporal patterns of connectivity. As a further example, in the\ncase of EEG neuroimaging, Xi of size NVxNW may represent spectral density for\nelectric neuronal activity at NW discrete frequencies from NV cortical voxels,\nfrom the i-th EEG epoch. In this case our functional data analysis approach\nwould reveal coupling of brain regions at possibly different frequencies.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2011 06:36:17 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2011 01:02:17 GMT"}], "update_date": "2011-03-17", "authors_parsed": [["Pascual-Marqui", "Roberto D.", ""], ["Biscay-Lirio", "Rolando J.", ""]]}, {"id": "1103.2872", "submitter": "Holger Drees", "authors": "Holger Drees", "title": "Extreme value analysis of actuarial risks: estimation and model\n  validation", "comments": "to appear in Advances in Statistical Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an overview of several aspects arising in the statistical analysis of\nextreme risks with actuarial applications in view. In particular it is\ndemonstrated that empirical process theory is a very powerful tool, both for\nthe asymptotic analysis of extreme value estimators and to devise tools for the\nvalidation of the underlying model assumptions. While the focus of the paper is\non univariate tail risk analysis, the basic ideas of the analysis of the\nextremal dependence between different risks are also outlined. Here we\nemphasize some of the limitation of classical multivariate extreme value theory\nand sketch how a different model proposed by Ledford and Tawn can help to avoid\npitfalls. Finally, these theoretical results are used to analyze a data set of\nlarge claim sizes from health insurance.\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2011 10:03:53 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2011 09:19:38 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Drees", "Holger", ""]]}, {"id": "1103.2987", "submitter": "Christian R\\\"over", "authors": "Christian R\\\"over, Chris Messenger and Reinhard Prix", "title": "Bayesian versus frequentist upper limits", "comments": "http://cdsweb.cern.ch/record/1306523/files/CERN-2011-006.pdf,\n  http://indico.cern.ch/materialDisplay.py?contribId=52&materialId=paper&confId=107747", "journal-ref": "Proceedings of the PHYSTAT 2011 Workshop, CERN, Geneva,\n  Switzerland, 17-20 January 2011, pp. 158-163", "doi": null, "report-no": "AEI-2011-014", "categories": "physics.data-an gr-qc stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While gravitational waves have not yet been measured directly, data analysis\nfrom detection experiments commonly includes an upper limit statement. Such\nupper limits may be derived via a frequentist or Bayesian approach; the\ntheoretical implications are very different, and on the technical side, one\nnotable difference is that one case requires maximization of the likelihood\nfunction over parameter space, while the other requires integration. Using a\nsimple example (detection of a sinusoidal signal in white Gaussian noise), we\ninvestigate the differences in performance and interpretation, and the effect\nof the \"trials factor\", or \"look-elsewhere effect\".\n", "versions": [{"version": "v1", "created": "Tue, 15 Mar 2011 18:25:54 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2011 15:30:01 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2011 13:52:39 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["R\u00f6ver", "Christian", ""], ["Messenger", "Chris", ""], ["Prix", "Reinhard", ""]]}, {"id": "1103.3300", "submitter": "Georg M Goerg", "authors": "Georg M. Goerg", "title": "A Nonparametric Frequency Domain EM Algorithm for Time Series\n  Classification with Applications to Spike Sorting and Macro-Economics", "comments": "Winner of the JSM 2011 student paper competition in \"Statistical\n  Learning and Data Mining (SAM)\"; 34 pages. Accepted for publication in SAM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML physics.data-an stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I propose a frequency domain adaptation of the Expectation Maximization (EM)\nalgorithm to group a family of time series in classes of similar dynamic\nstructure. It does this by viewing the magnitude of the discrete Fourier\ntransform (DFT) of each signal (or power spectrum) as a probability\ndensity/mass function (pdf/pmf) on the unit circle: signals with similar\ndynamics have similar pdfs; distinct patterns have distinct pdfs. An advantage\nof this approach is that it does not rely on any parametric form of the dynamic\nstructure, but can be used for non-parametric, robust and model-free\nclassification. This new method works for non-stationary signals of similar\nshape as well as stationary signals with similar auto-correlation structure.\nApplications to neural spike sorting (non-stationary) and pattern-recognition\nin socio-economic time series (stationary) demonstrate the usefulness and wide\napplicability of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 16 Mar 2011 21:31:17 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2011 03:41:54 GMT"}, {"version": "v3", "created": "Sun, 2 Oct 2011 19:07:38 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Goerg", "Georg M.", ""]]}, {"id": "1103.3532", "submitter": "Lotfi Chaari", "authors": "Lotfi Chaari, S\\'ebastien M\\'eriaux, Solveig Badillo, Jean-Christophe\n  Pesquet and Philippe Ciuciu", "title": "4D Wavelet-Based Regularization for Parallel MRI Reconstruction: Impact\n  on Subject and Group-Levels Statistical Sensitivity in fMRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV physics.med-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel MRI is a fast imaging technique that enables the acquisition of\nhighly resolved images in space. It relies on $k$-space undersampling and\nmultiple receiver coils with complementary sensitivity profiles in order to\nreconstruct a full Field-Of-View (FOV) image. The performance of parallel\nimaging mainly depends on the reconstruction algorithm, which can proceed\neither in the original $k$-space (GRAPPA, SMASH) or in the image domain\n(SENSE-like methods). To improve the performance of the widely used SENSE\nalgorithm, 2D- or slice-specific regularization in the wavelet domain has been\nefficiently investigated. In this paper, we extend this approach using\n3D-wavelet representations in order to handle all slices together and address\nreconstruction artifacts which propagate across adjacent slices. The extension\nalso accounts for temporal correlations that exist between successive scans in\nfunctional MRI (fMRI). The proposed 4D reconstruction scheme is fully\n\\emph{unsupervised} in the sense that all regularization parameters are\nestimated in the maximum likelihood sense on a reference scan. The gain induced\nby such extensions is first illustrated on EPI image reconstruction but also\nmeasured in terms of statistical sensitivity during a fast event-related fMRI\nprotocol. The proposed 4D-UWR-SENSE algorithm outperforms the SENSE\nreconstruction at the subject and group-levels (15 subjects) for different\ncontrasts of interest and using different parallel acceleration factors on\n$2\\times2\\times3$mm$^3$ EPI images.\n", "versions": [{"version": "v1", "created": "Thu, 17 Mar 2011 23:11:58 GMT"}], "update_date": "2011-03-21", "authors_parsed": [["Chaari", "Lotfi", ""], ["M\u00e9riaux", "S\u00e9bastien", ""], ["Badillo", "Solveig", ""], ["Pesquet", "Jean-Christophe", ""], ["Ciuciu", "Philippe", ""]]}, {"id": "1103.3817", "submitter": "Anuj Srivastava", "authors": "Anuj Srivastava and Wei Wu and Sebastian Kurtek and Eric Klassen and\n  J. S. Marron", "title": "Registration of Functional Data Using Fisher-Rao Metric", "comments": "Revised paper. More focused on a subproblem and more theoretical\n  results", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel geometric framework for separating the phase and the\namplitude variability in functional data of the type frequently studied in\ngrowth curve analysis. This framework uses the Fisher-Rao Riemannian metric to\nderive a proper distance on the quotient space of functions modulo the\ntime-warping group. A convenient square-root velocity function (SRVF)\nrepresentation transforms the Fisher-Rao metric into the standard $\\ltwo$\nmetric, simplifying the computations. This distance is then used to define a\nKarcher mean template and warp the individual functions to align them with the\nKarcher mean template. The strength of this framework is demonstrated by\nderiving a consistent estimator of a signal observed under random warping,\nscaling, and vertical translation. These ideas are demonstrated using both\nsimulated and real data from different application domains: the Berkeley growth\nstudy, handwritten signature curves, neuroscience spike trains, and gene\nexpression signals. The proposed method is empirically shown to be be superior\nin performance to several recently published methods for functional alignment.\n", "versions": [{"version": "v1", "created": "Sat, 19 Mar 2011 23:00:07 GMT"}, {"version": "v2", "created": "Mon, 16 May 2011 18:32:25 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Srivastava", "Anuj", ""], ["Wu", "Wei", ""], ["Kurtek", "Sebastian", ""], ["Klassen", "Eric", ""], ["Marron", "J. S.", ""]]}, {"id": "1103.3932", "submitter": "Sofia Olhede Professor", "authors": "Sofia Olhede", "title": "Ambiguity Sparse Processes", "comments": "28 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the class of ambiguity sparse processes, containing\nsubsets of popular nonstationary time series such as locally stationary,\ncyclostationary and uniformly modulated processes. The class also contains\naggregations of the aforementioned processes. Ambiguity sparse processes are\ndefined for a fixed sampling regime, in terms of a given number of sample\npoints and a fixed sampling period. The framework naturally allows us to treat\nheterogeneously nonstationary processes, and to develop methodology for\nprocesses that have growing but controlled complexity with increasing sample\nsizes and shrinking sampling periods. Expressions for the moments of the sample\nambiguity function are derived for ambiguity sparse processes. These properties\ninspire an Empirical Bayes shrinkage estimation procedure. The representation\nof the covariance structure of the process in terms of a time-frequency\nrepresentation is separated from the estimation of these second order\nproperties. The estimated ambiguity function is converted into an estimate of\nthe time-varying moments of the process, and from these moments, any bilinear\nrepresentation can be calculated with reduced estimation risk. Any of these\nrepresentations can be used to understand the time-varying spectral content of\nthe signal. The choice of representation is discussed. Parameters of the\nshrinkage procedure quantify the performance of the proposed estimation.\n", "versions": [{"version": "v1", "created": "Mon, 21 Mar 2011 07:51:31 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2011 10:06:51 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Olhede", "Sofia", ""]]}, {"id": "1103.4615", "submitter": "Bruce Desmarais", "authors": "Bruce A. Desmarais, Skyler J. Cranmer", "title": "Statistical Inference for Valued-Edge Networks: Generalized Exponential\n  Random Graph Models", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0030136", "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Across the sciences, the statistical analysis of networks is central to the\nproduction of knowledge on relational phenomena. Because of their ability to\nmodel the structural generation of networks, exponential random graph models\nare a ubiquitous means of analysis. However, they are limited by an inability\nto model networks with valued edges. We solve this problem by introducing a\nclass of generalized exponential random graph models capable of modeling\nnetworks whose edges are valued, thus greatly expanding the scope of networks\napplied researchers can subject to statistical analysis.\n", "versions": [{"version": "v1", "created": "Wed, 23 Mar 2011 20:02:32 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Desmarais", "Bruce A.", ""], ["Cranmer", "Skyler J.", ""]]}, {"id": "1103.4767", "submitter": "Mojgan Mohajer", "authors": "Mojgan Mohajer, Karl-Hans Englmeier, Volker J. Schmid", "title": "A comparison of Gap statistic definitions with and without logarithm\n  function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gap statistic is a standard method for determining the number of clusters\nin a set of data. The Gap statistic standardizes the graph of $\\log(W_{k})$,\nwhere $W_{k}$ is the within-cluster dispersion, by comparing it to its\nexpectation under an appropriate null reference distribution of the data. We\nsuggest to use $W_{k}$ instead of $\\log(W_{k})$, and to compare it to the\nexpectation of $W_{k}$ under a null reference distribution. In fact, whenever a\nnumber fulfills the original Gap statistic inequality, this number also\nfulfills the inequality of a Gap statistic using $W_{k}$, but not \\textit{vice\nversa}. The two definitions of the Gap function are evaluated on several\nsimulated data sets and on a real data of DCE-MR images.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2011 13:51:46 GMT"}], "update_date": "2011-03-25", "authors_parsed": [["Mohajer", "Mojgan", ""], ["Englmeier", "Karl-Hans", ""], ["Schmid", "Volker J.", ""]]}, {"id": "1103.4801", "submitter": "Eno Vangjeli", "authors": "Eno Vangjeli", "title": "ASN-Minimax double sampling plans by variables for two-sided\n  specification limits when {\\sigma} is unknown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ASN-Minimax double sampling plans by variables for a normally distributed\nquality characteristic with unknown standard deviation and two-sided\nspecification limits are introduced. These plans base on the essentially\nMaximum-Likelihood (ML) estimator p* and the Minimum Variance Unbiased (MVU)\nestimator ^p of the fraction defective p. The operation characteristic (OC) of\nthe ASN-Minimax double sampling plans is determined by using the independent\nrandom variables p*_1, p*_2 and ^p_1, ^p_2, which relate to the first and\nsecond samples, respectively. The maximum of the average sample number (ASN) of\nthese plans is shown to be considerably smaller than the sample size of the\ncorresponding single sampling plans.\n", "versions": [{"version": "v1", "created": "Thu, 24 Mar 2011 16:25:09 GMT"}, {"version": "v2", "created": "Fri, 25 Mar 2011 09:26:38 GMT"}, {"version": "v3", "created": "Tue, 29 Mar 2011 12:28:18 GMT"}, {"version": "v4", "created": "Mon, 4 Apr 2011 09:35:29 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Vangjeli", "Eno", ""]]}, {"id": "1103.4890", "submitter": "Alexis Akira Toda", "authors": "Alexis Akira Toda", "title": "An Information-Theoretic Approach to Nonparametric Estimation, Model\n  Selection, and Goodness of Fit", "comments": "Submitted to Econometrica on March 24, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper applies the recently axiomatized Optimum Information Principle\n(minimize the Kullback-Leibler information subject to all relevant information)\nto nonparametric density estimation, which provides a theoretical foundation as\nwell as a computational algorithm for maximum entropy density estimation. The\nestimator, called optimum information estimator, approximates the true density\narbitrarily well. As a by-product I obtain a measure of goodness of fit of\nparametric models (both conditional and unconditional) and an absolute\ncriterion for model selection, as opposed to other conventional methods such as\nAIC and BIC which are relative measures.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2011 01:11:14 GMT"}], "update_date": "2011-03-28", "authors_parsed": [["Toda", "Alexis Akira", ""]]}, {"id": "1103.4891", "submitter": "Adrian Dobra", "authors": "Adrian Dobra", "title": "Dynamic Markov Bases", "comments": "26 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational approach for generating Markov bases for multi-way\ncontingency tables whose cells counts might be constrained by fixed marginals\nand by lower and upper bounds. Our framework includes tables with structural\nzeros as a particular case. In- stead of computing the entire Markov basis in\nan initial step, our framework finds sets of local moves that connect each\ntable in the reference set with a set of neighbor tables. We construct a Markov\nchain on the reference set of tables that requires only a set of local moves at\neach iteration. The union of these sets of local moves forms a dynamic Markov\nbasis. We illustrate the practicality of our algorithms in the estimation of\nexact p-values for a three-way table with structural zeros and a sparse\neight-way table. Computer code implementing the methods de- scribed in the\narticle as well as the two datasets used in the numerical examples are\navailable as supplemental material.\n", "versions": [{"version": "v1", "created": "Fri, 25 Mar 2011 01:16:08 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Dobra", "Adrian", ""]]}, {"id": "1103.5178", "submitter": "Zack Almquist", "authors": "Zack W. Almquist and Carter T. Butts", "title": "Logistic Network Regression for Scalable Analysis of Networks with Joint\n  Edge/Vertex Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": "IMBS Technical Report MBS 11-03, University of California, Irvine", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network dynamics may be viewed as a process of change in the edge structure\nof a network, in the vertex set on which edges are defined, or in both\nsimultaneously. Though early studies of such processes were primarily\ndescriptive, recent work on this topic has increasingly turned to formal\nstatistical models. While showing great promise, many of these modern dynamic\nmodels are computationally intensive and scale very poorly in the size of the\nnetwork under study and/or the number of time points considered. Likewise,\ncurrently employed models focus on edge dynamics, with little support for\nendogenously changing vertex sets. Here, we show how an existing approach based\non logistic network regression can be extended to serve as highly scalable\nframework for modeling large networks with dynamic vertex sets. We place this\napproach within a general dynamic exponential family (ERGM) context, clarifying\nthe assumptions underlying the framework (and providing a clear path for\nextensions), and show how model assessment methods for cross-sectional networks\ncan be extended to the dynamic case. Finally, we illustrate this approach on a\nclassic data set involving interactions among windsurfers on a California\nbeach.\n", "versions": [{"version": "v1", "created": "Sun, 27 Mar 2011 02:13:05 GMT"}], "update_date": "2011-03-29", "authors_parsed": [["Almquist", "Zack W.", ""], ["Butts", "Carter T.", ""]]}, {"id": "1103.5339", "submitter": "Marcela Svarc", "authors": "Ricardo Fraiman, Badih Ghattas and Marcela Svarc", "title": "Interpretable Clustering using Unsupervised Binary Trees", "comments": "25 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We herein introduce a new method of interpretable clustering that uses\nunsupervised binary trees. It is a three-stage procedure, the first stage of\nwhich entails a series of recursive binary splits to reduce the heterogeneity\nof the data within the new subsamples. During the second stage (pruning),\nconsideration is given to whether adjacent nodes can be aggregated. Finally,\nduring the third stage (joining), similar clusters are joined together, even if\nthey do not descend from the same node originally. Consistency results are\nobtained, and the procedure is used on simulated and real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2011 12:20:27 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2011 14:04:44 GMT"}], "update_date": "2011-10-28", "authors_parsed": [["Fraiman", "Ricardo", ""], ["Ghattas", "Badih", ""], ["Svarc", "Marcela", ""]]}, {"id": "1103.5399", "submitter": "Thomas Dean", "authors": "Thomas A. Dean and Sumeetpal S. Singh and Ajay Jasra and Gareth W.\n  Peters", "title": "Parameter Estimation for Hidden Markov Models with Intractable\n  Likelihoods", "comments": "First version: 1 October 2010", "journal-ref": null, "doi": null, "report-no": "Cambridge University Engineering Department Technical Report 660", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate Bayesian computation (ABC) is a popular technique for\napproximating likelihoods and is often used in parameter estimation when the\nlikelihood functions are analytically intractable. Although the use of ABC is\nwidespread in many fields, there has been little investigation of the\ntheoretical properties of the resulting estimators. In this paper we give a\ntheoretical analysis of the asymptotic properties of ABC based maximum\nlikelihood parameter estimation for hidden Markov models. In particular, we\nderive results analogous to those of consistency and asymptotic normality for\nstandard maximum likelihood estimation. We also discuss how Sequential Monte\nCarlo methods provide a natural method for implementing likelihood based ABC\nprocedures.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2011 16:10:24 GMT"}], "update_date": "2011-03-29", "authors_parsed": [["Dean", "Thomas A.", ""], ["Singh", "Sumeetpal S.", ""], ["Jasra", "Ajay", ""], ["Peters", "Gareth W.", ""]]}, {"id": "1103.5407", "submitter": "James Scott", "authors": "Nicholas G. Polson and James G. Scott", "title": "Data augmentation for non-Gaussian regression models using variance-mean\n  mixtures", "comments": "Added a discussion of quasi-Newton acceleration", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We use the theory of normal variance-mean mixtures to derive a\ndata-augmentation scheme for a class of common regularization problems. This\ngeneralizes existing theory on normal variance mixtures for priors in\nregression and classification. It also allows variants of the\nexpectation-maximization algorithm to be brought to bear on a wider range of\nmodels than previously appreciated. We demonstrate the method on several\nexamples, including sparse quantile regression and binary logistic regression.\nWe also show that quasi-Newton acceleration can substantially improve the speed\nof the algorithm without compromising its robustness.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2011 16:28:32 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2011 22:37:08 GMT"}, {"version": "v3", "created": "Sun, 26 Feb 2012 04:51:17 GMT"}, {"version": "v4", "created": "Sat, 22 Sep 2012 21:19:23 GMT"}], "update_date": "2012-09-25", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""]]}, {"id": "1103.5447", "submitter": "Nickos Papadatos D", "authors": "G. Afendras and N. Papadatos", "title": "On matrix variance inequalities", "comments": "7 pages, submitted for publication", "journal-ref": "Journal of Statistical Planning and Inference (2011), vol. 141,\n  pp. 3628-3631", "doi": "10.1016/j.jspi.2011.05.016", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Olkin and Shepp (2005, J. Statist. Plann. Inference, vol. 130, pp. 351--358)\npresented a matrix form of Chernoff's inequality for Normal and Gamma\n(univariate) distributions. We extend and generalize this result, proving\nPoincare-type and Bessel-type inequalities, for matrices of arbitrary order and\nfor a large class of distributions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Mar 2011 19:15:13 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Afendras", "G.", ""], ["Papadatos", "N.", ""]]}, {"id": "1103.5679", "submitter": "Kengo Kamatani", "authors": "Kengo Kamatani", "title": "Weak consistency of Markov chain Monte Carlo methods", "comments": "14 pages", "journal-ref": "Bulletin of Informatics and Cybernetics, 45 (2013) 103-123", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov chain Monte Calro methods (MCMC) are commonly used in Bayesian\nstatistics. In the last twenty years, many results have been established for\nthe calculation of the exact convergence rate of MCMC methods. We introduce\nanother rate of convergence for MCMC methods by approximation techniques. This\nrate can be obtained by the convergence of the Markov chain to a diffusion\nprocess. We apply it to a simple mixture model and obtain its convergence rate.\nNumerical simulations are performed to illustrate the effect of the rate.\n", "versions": [{"version": "v1", "created": "Tue, 29 Mar 2011 15:13:43 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2012 05:31:10 GMT"}, {"version": "v3", "created": "Wed, 25 Sep 2013 06:59:47 GMT"}], "update_date": "2014-02-17", "authors_parsed": [["Kamatani", "Kengo", ""]]}, {"id": "1103.5913", "submitter": "Stephane Girard", "authors": "St\\'ephane Girard and Anatoli Iouditski and Alexander Nazin", "title": "Linear programming problems for l_1- optimal frontier estimation", "comments": null, "journal-ref": "S. Girard, A. Iouditski & A. Nazin. \"L1-optimal frontier\n  estimation via linear programming\", Automation and Remote Control, 66(12),\n  2000-2018, 2005", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new optimal estimators for the Lipschitz frontier of a set of\npoints. They are defined as kernel estimators being sufficiently regular,\ncovering all the points and whose associated support is of smallest surface.\nThe estimators are written as linear combinations of kernel functions applied\nto the points of the sample. The coefficients of the linear combination are\nthen computed by solving related linear programming problem. The L_1 error\nbetween the estimated and the true frontier function with a known Lipschitz\nconstant is shown to be almost surely converging to zero, and the rate of\nconvergence is proved to be optimal.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 13:11:12 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Girard", "St\u00e9phane", ""], ["Iouditski", "Anatoli", ""], ["Nazin", "Alexander", ""]]}, {"id": "1103.5925", "submitter": "Stephane Girard", "authors": "Guillaume Bouchard and St\\'ephane Girard and Anatoli Iouditski and\n  Alexander Nazin", "title": "Linear programming problems for frontier estimation", "comments": null, "journal-ref": "Automation and Remote Control, 65(1), 58-64, 2004", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose new estimates for the frontier of a set of points. They are\ndefined as kernel estimates covering all the points and whose associated\nsupport is of smallest surface. The estimates are written as linear combinatio-\nns of kernel functions applied to the points of the sample. The coefficients of\nthe linear combination are then computed by solving a linear programming\nproblem. In the general case, the solution of the optimizat- ion problem is\nsparse, that is, only a few coefficients are non zero. The corresponding points\nplay the role of support vectors in the statistical learning theory. The L_1\nerror between the estimated and the true frontiers is shown to be almost surely\nconverging to zero, and the rate of convergence is provided. The behaviour of\nthe estimates on finite sample situations is illustrated on some simulations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 13:35:36 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Bouchard", "Guillaume", ""], ["Girard", "St\u00e9phane", ""], ["Iouditski", "Anatoli", ""], ["Nazin", "Alexander", ""]]}, {"id": "1103.5931", "submitter": "Stephane Girard", "authors": "St\\'ephane Girard and Pierre Jacob", "title": "Extreme values and kernel estimates of point processes boundaries", "comments": null, "journal-ref": "ESAIM: Probability and Statistics, 8, 150-168, 2004", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for estimating the edge of a two-dimensional bounded set,\ngiven a finite random set of points drawn from the interior. The estimator is\nbased both on a Parzen-Rosenblatt kernel and extreme values of point processes.\nWe give conditions for various kinds of convergence and asymptotic normality.\nWe propose a method of reducing the negative bias and edge effects, illustrated\nby a simulation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 13:57:03 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Girard", "St\u00e9phane", ""], ["Jacob", "Pierre", ""]]}, {"id": "1103.5938", "submitter": "Stephane Girard", "authors": "St\\'ephane Girard and Pierre Jacob", "title": "Projection estimates of point processes boundaries", "comments": null, "journal-ref": "Journal of Statistical Planning and Inference, 116(1):1-15, 2003", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for estimating the edge of a two-dimensional bounded set,\ngiven a finite random set of points drawn from the interior. The estimator is\nbased both on projections on C^1 bases and on extreme points of the point\nprocess. We give conditions on the Dirichlet's kernel associated to the C^1\nbases for various kinds of convergence and asymptotic normality. We propose a\nmethod for reducing the negative bias and illustrate it by a simulation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 14:31:52 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Girard", "St\u00e9phane", ""], ["Jacob", "Pierre", ""]]}, {"id": "1103.5947", "submitter": "Stephane Girard", "authors": "St\\'ephane Girard and Pierre Jacob", "title": "Extreme value and Haar series estimates of point process boundaries", "comments": null, "journal-ref": "Scandinavian Journal of Statistics, 30(2):369-384, 2003", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for estimating the edge of a two-dimensional bounded\nset, given a finite random set of points drawn from the interior. The estimator\nis based both on Haar series and extreme values of the point process. We give\nconditions for various kind of convergence and we obtain remarkably different\npossible limit distributions. We propose a method of reducing the negative\nbias, illustrated by a simulation.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 14:47:49 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Girard", "St\u00e9phane", ""], ["Jacob", "Pierre", ""]]}, {"id": "1103.5956", "submitter": "Stephane Girard", "authors": "St\\'ephane Girard and Pierre Jacob", "title": "Frontier estimation via kernel regression on high power-transformed data", "comments": null, "journal-ref": "Journal of Multivariate Analysis, 99, 403--420, 2008", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for estimating the frontier of a multidimensional\nsample. The estimator is based on a kernel regression on the power-transformed\ndata. We assume that the exponent of the transformation goes to infinity while\nthe bandwidth of the kernel goes to zero. We give conditions on these two\nparameters to obtain complete convergence and asymptotic normality. The good\nperformance of the estimator is illustrated on some finite sample situations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Mar 2011 15:18:22 GMT"}], "update_date": "2011-03-31", "authors_parsed": [["Girard", "St\u00e9phane", ""], ["Jacob", "Pierre", ""]]}, {"id": "1103.6112", "submitter": "Stephane Girard", "authors": "St\\'ephane Girard and Ludovic Menneteau", "title": "Smoothed extreme value estimators of non-uniform point processes\n  boundaries with application to star-shaped supports estimation", "comments": null, "journal-ref": "S. Girard & L. Menneteau. \"Smoothed extreme value estimators of\n  non-uniform point processes boundaries with application to star-shaped\n  supports estimation\", Communication in Statistics - Theory and Methods,\n  37(6), 881--897, 2008", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of estimating the edge of a bounded set in R^d given a\nrandom set of points drawn from the interior. Our method is based on a\ntransformation of estimators dedicated to uniform point processes and obtained\nby smoothing some of its bias corrected extreme points. An application to the\nestimation of star-shaped supports is presented.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2011 07:34:34 GMT"}], "update_date": "2011-04-01", "authors_parsed": [["Girard", "St\u00e9phane", ""], ["Menneteau", "Ludovic", ""]]}, {"id": "1103.6204", "submitter": "Stephane Girard", "authors": "Jean Diebolt and Laurent Gardes and St\\'ephane Girard and Armelle\n  Guillou", "title": "Bias-reduced extreme quantiles estimators of Weibull-tail distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the problem of estimating an extreme quantile of a\nWeibull tail-distribution. The new extreme quantile estimator has a reduced\nbias compared to the more classical ones proposed in the literature. It is\nbased on an exponential regression model that was introduced in Diebolt et al.\n(2008). The asymptotic normality of the extreme quantile estimator is\nestablished. We also introduce an adaptive selection procedure to determine the\nnumber of upper order statistics to be used. A simulation study as well as an\napplication to a real data set are provided in order to prove the efficiency of\nthe above mentioned methods.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2011 14:55:30 GMT"}], "update_date": "2011-04-01", "authors_parsed": [["Diebolt", "Jean", ""], ["Gardes", "Laurent", ""], ["Girard", "St\u00e9phane", ""], ["Guillou", "Armelle", ""]]}, {"id": "1103.6216", "submitter": "Stephane Girard", "authors": "Jean Diebolt and Mhamed El-Aroui and Myriam Garrido and St\\'ephane\n  Girard", "title": "Quasi-conjugate Bayes estimates for GPD parameters and application to\n  heavy tails modelling", "comments": null, "journal-ref": "J. Diebolt, M. El-Aroui, M. Garrido & S. Girard. \"Quasi-conjugate\n  bayes estimates for GPD parameters and application to heavy tails modelling\",\n  Extremes, 8, 57-78, 2005", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a quasi-conjugate Bayes approach for estimating Generalized Pareto\nDistribution (GPD) parameters, distribution tails and extreme quantiles within\nthe Peaks-Over-Threshold framework. Damsleth conjugate Bayes structure on Gamma\ndistributions is transfered to GPD. Posterior estimates are then computed by\nGibbs samplers with Hastings-Metropolis steps. Accurate Bayes credibility\nintervals are also defined, they provide assessment of the quality of the\nextreme events estimates. An empirical Bayesian method is used in this work,\nbut the suggested approach could incorporate prior information. It is shown\nthat the obtained quasi-conjugate Bayes estimators compare well with the GPD\nstandard estimators when simulated and real data sets are studied.\n", "versions": [{"version": "v1", "created": "Thu, 31 Mar 2011 15:36:20 GMT"}], "update_date": "2011-04-01", "authors_parsed": [["Diebolt", "Jean", ""], ["El-Aroui", "Mhamed", ""], ["Garrido", "Myriam", ""], ["Girard", "St\u00e9phane", ""]]}]