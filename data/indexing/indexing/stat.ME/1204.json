[{"id": "1204.0064", "submitter": "Hongtu Zhu", "authors": "Hongtu Zhu, Joseph G. Ibrahim, Hyunsoon Cho", "title": "Perturbation and scaled Cook's distance", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS978 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 2, 785-811", "doi": "10.1214/12-AOS978", "report-no": "IMS-AOS-AOS978", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cook's distance [Technometrics 19 (1977) 15-18] is one of the most important\ndiagnostic tools for detecting influential individual or subsets of\nobservations in linear regression for cross-sectional data. However, for many\ncomplex data structures (e.g., longitudinal data), no rigorous approach has\nbeen developed to address a fundamental issue: deleting subsets with different\nnumbers of observations introduces different degrees of perturbation to the\ncurrent model fitted to the data, and the magnitude of Cook's distance is\nassociated with the degree of the perturbation. The aim of this paper is to\naddress this issue in general parametric models with complex data structures.\nWe propose a new quantity for measuring the degree of the perturbation\nintroduced by deleting a subset. We use stochastic ordering to quantify the\nstochastic relationship between the degree of the perturbation and the\nmagnitude of Cook's distance. We develop several scaled Cook's distances to\nresolve the comparison of Cook's distance for different subset deletions.\nTheoretical and numerical examples are examined to highlight the broad spectrum\nof applications of these scaled Cook's distances in a formal influence\nanalysis.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2012 03:02:49 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2012 07:26:18 GMT"}], "update_date": "2012-06-08", "authors_parsed": [["Zhu", "Hongtu", ""], ["Ibrahim", "Joseph G.", ""], ["Cho", "Hyunsoon", ""]]}, {"id": "1204.0105", "submitter": "Ioannis Kosmidis", "authors": "Ioannis Kosmidis", "title": "Improved estimation in cumulative link models", "comments": null, "journal-ref": "J.R.Stat.Soc.B 76 (2014) 169-196", "doi": "10.1111/rssb.12025", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the estimation of cumulative link models for ordinal data, the\nbias-reducing adjusted score equations in \\citet{firth:93} are obtained, whose\nsolution ensures an estimator with smaller asymptotic bias than the maximum\nlikelihood estimator. Their form suggests a parameter-dependent adjustment of\nthe multinomial counts, which, in turn suggests the solution of the adjusted\nscore equations through iterated maximum likelihood fits on adjusted counts,\ngreatly facilitating implementation. Like the maximum likelihood estimator, the\nreduced-bias estimator is found to respect the invariance properties that make\ncumulative link models a good choice for the analysis of categorical data. Its\nadditional finiteness and optimal frequentist properties, along with the\nadequate behaviour of related asymptotic inferential procedures make the\nreduced-bias estimator attractive as a default choice for practical\napplications. Furthermore, the proposed estimator enjoys certain shrinkage\nproperties that are defensible from an experimental point of view relating to\nthe nature of ordinal data.\n", "versions": [{"version": "v1", "created": "Sat, 31 Mar 2012 15:07:44 GMT"}, {"version": "v2", "created": "Tue, 17 Apr 2012 19:26:17 GMT"}, {"version": "v3", "created": "Mon, 28 Jan 2013 23:39:33 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Kosmidis", "Ioannis", ""]]}, {"id": "1204.0285", "submitter": "Jun Yan", "authors": "Steven Chiou, Junghi Kim, and Jun Yan", "title": "Semiparametric Multivariate Accelerated Failure Time Model with\n  Generalized Estimating Equations", "comments": null, "journal-ref": "Lifetime Data Analysis, 2014, Volume 20, Issue 4, pp 599-618", "doi": "10.1007/s10985-014-9292-x", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The semiparametric accelerated failure time model is not as widely used as\nthe Cox relative risk model mainly due to computational difficulties. Recent\ndevelopments in least squares estimation and induced smoothing estimating\nequations provide promising tools to make the accelerate failure time models\nmore attractive in practice. For semiparametric multivariate accelerated\nfailure time models, we propose a generalized estimating equation approach to\naccount for the multivariate dependence through working correlation structures.\nThe marginal error distributions can be either identical as in sequential event\nsettings or different as in parallel event settings. Some regression\ncoefficients can be shared across margins as needed. The initial estimator is a\nrank-based estimator with Gehan's weight, but obtained from an induced\nsmoothing approach with computation ease. The resulting estimator is consistent\nand asymptotically normal, with a variance estimated through a multiplier\nresampling method. In a simulation study, our estimator was up to three times\nas efficient as the initial estimator, especially with stronger multivariate\ndependence and heavier censoring percentage. Two real examples demonstrate the\nutility of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2012 01:02:12 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Chiou", "Steven", ""], ["Kim", "Junghi", ""], ["Yan", "Jun", ""]]}, {"id": "1204.0286", "submitter": "Hongwei Shang", "authors": "Hongwei Shang, Jun Yan, Xuebin Zhang", "title": "A two-step approach to model precipitation extremes in California based\n  on max-stable and marginal point processes", "comments": "Published at http://dx.doi.org/10.1214/14-AOAS804 in the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2015, Vol. 9, No. 1, 452-473", "doi": "10.1214/14-AOAS804", "report-no": "IMS-AOAS-AOAS804", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modeling spatial extremes, the dependence structure is classically\ninferred by assuming that block maxima derive from max-stable processes.\nWeather stations provide daily records rather than just block maxima. The point\nprocess approach for univariate extreme value analysis, which uses more\nhistorical data and is preferred by some practitioners, does not adapt easily\nto the spatial setting. We propose a two-step approach with a composite\nlikelihood that utilizes site-wise daily records in addition to block maxima.\nThe procedure separates the estimation of marginal parameters and dependence\nparameters into two steps. The first step estimates the marginal parameters\nwith an independence likelihood from the point process approach using daily\nrecords. Given the marginal parameter estimates, the second step estimates the\ndependence parameters with a pairwise likelihood using block maxima. In a\nsimulation study, the two-step approach was found to be more efficient than the\npairwise likelihood approach using only block maxima. The method was applied to\nstudy the effect of El Ni\\~{n}o-Southern Oscillation on extreme precipitation\nin California with maximum daily winter precipitation from 35 sites over 55\nyears. Using site-specific generalized extreme value models, the two-step\napproach led to more sites detected with the El Ni\\~{n}o effect, narrower\nconfidence intervals for return levels and tighter confidence regions for risk\nmeasures of jointly defined events.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2012 01:13:28 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 05:12:57 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Shang", "Hongwei", ""], ["Yan", "Jun", ""], ["Zhang", "Xuebin", ""]]}, {"id": "1204.0316", "submitter": "Stefan Wager", "authors": "Stefan Wager", "title": "Subsampling Extremes: From Block Maxima to Smooth Tail Estimation", "comments": "Added references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a new estimator for the tail index of a distribution in the Frechet\ndomain of attraction that arises naturally by computing subsample maxima. This\nestimator is equivalent to taking a U-statistic over a Hill estimator with two\norder statistics. The estimator presents multiple advantages over the Hill\nestimator. In particular, it has asymptotically smooth sample paths as a\nfunction of the threshold k, making it considerably more stable than the Hill\nestimator. The estimator also admits a simple and intuitive threshold selection\nrule that does not require fitting a second-order model. Journal of\nMultivariate Analysis, 130, 2014\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2012 06:03:25 GMT"}, {"version": "v2", "created": "Sat, 13 Oct 2012 22:54:10 GMT"}, {"version": "v3", "created": "Thu, 17 Oct 2013 23:36:38 GMT"}, {"version": "v4", "created": "Sun, 27 Apr 2014 01:36:26 GMT"}, {"version": "v5", "created": "Sun, 19 Oct 2014 21:57:16 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Wager", "Stefan", ""]]}, {"id": "1204.0332", "submitter": "Johan Segers", "authors": "Johan Segers", "title": "Max-stable models for multivariate extremes", "comments": "Invited paper for RevStat Statistical Journal. 22 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate extreme-value analysis is concerned with the extremes in a\nmultivariate random sample, that is, points of which at least some components\nhave exceptionally large values. Mathematical theory suggests the use of\nmax-stable models for univariate and multivariate extremes. A comprehensive\naccount is given of the various ways in which max-stable models are described.\nFurthermore, a construction device is proposed for generating parametric\nfamilies of max-stable distributions. Although the device is not new, its role\nas a model generator seems not yet to have been fully exploited.\n", "versions": [{"version": "v1", "created": "Mon, 2 Apr 2012 07:28:09 GMT"}], "update_date": "2012-04-03", "authors_parsed": [["Segers", "Johan", ""]]}, {"id": "1204.0585", "submitter": "Theodoros Tsiligkaridis", "authors": "Theodoros Tsiligkaridis and Alfred O. Hero III and Shuheng Zhou", "title": "Convergence Properties of Kronecker Graphical Lasso Algorithms", "comments": "47 pages, accepted to IEEE Transactions on Signal Processing", "journal-ref": "IEEE Transactions on Signal Processing, Vol. 61, Issue 7, pp.\n  1743-1755, April 2013", "doi": "10.1109/TSP.2013.2240157", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies iteration convergence of Kronecker graphical lasso\n(KGLasso) algorithms for estimating the covariance of an i.i.d. Gaussian random\nsample under a sparse Kronecker-product covariance model and MSE convergence\nrates. The KGlasso model, originally called the transposable regularized\ncovariance model by Allen [\"Transposable regularized covariance models with an\napplication to missing data imputation,\" Ann. Appl. Statist., vol. 4, no. 2,\npp. 764-790, 2010], implements a pair of $\\ell_1$ penalties on each Kronecker\nfactor to enforce sparsity in the covariance estimator. The KGlasso algorithm\ngeneralizes Glasso, introduced by Yuan and Lin [\"Model selection and estimation\nin the Gaussian graphical model,\" Biometrika, vol. 94, pp. 19-35, 2007] and\nBanerjee [\"Model selection through sparse maximum likelihood estimation for\nmultivariate Gaussian or binary data,\" J. Mach. Learn. Res., vol. 9, pp.\n485-516, Mar. 2008], to estimate covariances having Kronecker product form. It\nalso generalizes the unpenalized ML flip-flop (FF) algorithm of Dutilleul [\"The\nMLE algorithm for the matrix normal distribution,\" J. Statist. Comput. Simul.,\nvol. 64, pp. 105-123, 1999] and Werner [\"On estimation of covariance matrices\nwith Kronecker product structure,\" IEEE Trans. Signal Process., vol. 56, no. 2,\npp. 478-491, Feb. 2008] to estimation of sparse Kronecker factors. We establish\nthat the KGlasso iterates converge pointwise to a local maximum of the\npenalized likelihood function. We derive high dimensional rates of convergence\nto the true covariance as both the number of samples and the number of\nvariables go to infinity. Our results establish that KGlasso has significantly\nfaster asymptotic convergence than Glasso and FF. Simulations are presented\nthat validate the results of our analysis.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 03:28:10 GMT"}, {"version": "v2", "created": "Sat, 28 Jul 2012 06:37:59 GMT"}, {"version": "v3", "created": "Fri, 31 May 2013 22:16:48 GMT"}, {"version": "v4", "created": "Fri, 1 Nov 2013 17:46:31 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Tsiligkaridis", "Theodoros", ""], ["Hero", "Alfred O.", "III"], ["Zhou", "Shuheng", ""]]}, {"id": "1204.0757", "submitter": "Hamdi Raissi", "authors": "Hamdi Ra\\\"Issi", "title": "Lag length identification for VAR models with non-constant variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The identification of the lag length for vector autoregressive models by mean\nof Akaike Information Criterion (AIC), Partial Autoregressive and Correlation\nMatrices (PAM and PCM hereafter) is studied in the framework of processes with\ntime varying variance. It is highlighted that the use of the standard tools are\nnot justified in such a case. As a consequence we propose an adaptive AIC which\nis robust to the presence of unconditional heteroscedasticity. Corrected\nconfidence bounds are proposed for the usual PAM and PCM obtained from the\nOrdinary Least Squares (OLS) estimation. The volatility structure of the\ninnovations is used to develop adaptive PAM and PCM. We underline that the\nadaptive PAM and PCM are more accurate than the OLS PAM and PCM for identifying\nthe lag length of the autoregressive models. Monte Carlo experiments show that\nthe adaptive $AIC$ have a greater ability to select the correct autoregressive\norder than the standard AIC. An illustrative application using US international\nfinance data is presented.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 18:30:51 GMT"}, {"version": "v2", "created": "Fri, 23 Aug 2013 21:25:04 GMT"}], "update_date": "2013-08-27", "authors_parsed": [["Ra\u00cfssi", "Hamdi", ""]]}, {"id": "1204.0798", "submitter": "Ralph Brinks", "authors": "Ralph Brinks", "title": "Fast Calculation of Calendar Time-, Age- and Duration Dependent Time at\n  Risk in the Lexis Space", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In epidemiology, the person-years method is broadly used to estimate the\nincidence rates of health related events. This needs determination of time at\nrisk stratified by period, age and sometimes by duration of disease or\nexposition. The article describes a fast method for calculating the time at\nrisk in two- or three-dimensional Lexis diagrams based on Siddon's algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 3 Apr 2012 20:21:40 GMT"}], "update_date": "2012-04-05", "authors_parsed": [["Brinks", "Ralph", ""]]}, {"id": "1204.1303", "submitter": "Rodrigo Silva", "authors": "Rodrigo B. Silva, Marcelo B. Pereira, C\\'icero R.B. Dias and Gauss M.\n  Cordeiro", "title": "The compound class of extended Weibull power series distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a new class of distributions which is obtained by\ncompounding the extended Weibull and power series distributions. The\ncompounding procedure follows the same set-up carried out by Adamidis and\nLoukas (1998) and defines at least new 68 sub-models. This class includes some\nwell-known mixing distributions, such as the Weibull power series (Morais and\nBarreto-Souza, 2010) and exponential power series (Chahkandi and Ganjali, 2009)\ndistributions. Some mathematical properties of the new class are studied\nincluding moments and generating function. We provide the density function of\nthe order statistics and obtain their moments. The method of maximum likelihood\nis used for estimating the model parameters and an EM algorithm is proposed for\ncomputing the estimates. Special distributions are investigated in some detail.\nAn application to a real data set is given to show the flexibility and\npotentiality of the new class of distributions.\n", "versions": [{"version": "v1", "created": "Thu, 5 Apr 2012 18:37:50 GMT"}], "update_date": "2012-04-06", "authors_parsed": [["Silva", "Rodrigo B.", ""], ["Pereira", "Marcelo B.", ""], ["Dias", "C\u00edcero R. B.", ""], ["Cordeiro", "Gauss M.", ""]]}, {"id": "1204.1389", "submitter": "Rodrigo Silva", "authors": "Marcelo B. Pereira, Rodrigo B. Silva, Luz M. Zea and Gauss M. Cordeiro", "title": "The Kumaraswamy Pareto distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The modeling and analysis of lifetimes is an important aspect of statistical\nwork in a wide variety of scientific and technological fields. For the first\ntime, the called Kumaraswamy Pareto distribution is introduced and studied. The\nnew distribution can have a decreasing and upside-down bathtub failure rate\nfunction depending on the values of its parameters. It includes as special\nsub-models the Pareto and exponentiated Pareto (Gupta et al. [12])\ndistributions. Some structural properties of the proposed distribution are\nstudied including explicit expressions for the moments and generating function.\nWe provide the density function of the order statistics and obtain their\nmoments. The method of maximum likelihood is used for estimating the model\nparameters and the observed information matrix is derived. A real data set is\nused to compare the new model with widely known distributions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 01:06:33 GMT"}, {"version": "v2", "created": "Wed, 2 May 2012 00:57:28 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2012 13:20:28 GMT"}], "update_date": "2012-12-05", "authors_parsed": [["Pereira", "Marcelo B.", ""], ["Silva", "Rodrigo B.", ""], ["Zea", "Luz M.", ""], ["Cordeiro", "Gauss M.", ""]]}, {"id": "1204.1567", "submitter": "Min Wang", "authors": "Min Wang and Xiaoqian Sun", "title": "Bayes Factor Consistency for Unbalanced ANOVA Models", "comments": "I would like to withdraw this paper due to copyright problem", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practical situations, most experimental designs often yield unbalanced\ndata which have different numbers of observations per unit because of cost\nconstraints, or missing data, etc. In this paper, we consider the Bayesian\napproach to hypothesis testing or model selection under the one-way unbalanced\nfixed-effects ANOVA model. We adopt Zellner's g-prior with the beta-prime\ndistribution for g, which results in an explicit closed-form expression of the\nBayes factor without integral representation. Furthermore, we investigate the\nmodel selection consistency of the Bayes factor under three different\nasymptotic scenarios: either the number of units goes to infinity, the number\nof observations per unit goes to infinity, or both go to infinity. The results\npresented extend some existing ones of the Bayes factor for the balanced ANOVA\nmodels in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 6 Apr 2012 21:29:14 GMT"}, {"version": "v2", "created": "Mon, 21 May 2012 19:40:51 GMT"}], "update_date": "2012-05-22", "authors_parsed": [["Wang", "Min", ""], ["Sun", "Xiaoqian", ""]]}, {"id": "1204.1632", "submitter": "Nickos Papadatos D", "authors": "Nickos Papadatos and Tatiana Xifara", "title": "A Simple Method for Obtaining the Maximal Correlation Coefficient and\n  Related Characterizations", "comments": "Journal of Multivariate Analysis (to appear, 19 pages)", "journal-ref": "Journal of Multivariate Analysis (2013), vol. 118, pp. 102-114", "doi": "10.1016/j.jmva.2013.03.017", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a method that enables the simple calculation of the maximal\ncorrelation coefficient of a bivariate distribution, under suitable conditions.\nIn particular, the method readily applies to known results on order statistics\nand records. As an application we provide a new characterization of the\nexponential distribution: Under a splitting model on independent identically\ndistributed observations, it is the (unique, up to a location-scale\ntransformation) parent distribution that maximizes the correlation coefficient\nbetween the records among two different branches of the splitting sequence.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 13:34:45 GMT"}, {"version": "v2", "created": "Wed, 9 May 2012 12:03:31 GMT"}, {"version": "v3", "created": "Mon, 8 Apr 2013 14:03:43 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Papadatos", "Nickos", ""], ["Xifara", "Tatiana", ""]]}, {"id": "1204.1633", "submitter": "Nickos Papadatos D", "authors": "Theophilos Cacoullos and Nickos Papadatos", "title": "Self-Inverse and Exchangeable Random Variables", "comments": "Statistics and Probability Letters (to appear, 6 pages)", "journal-ref": "Statistics and Probability Letters (2013), vol. 83, pp. 9-12", "doi": "10.1016/j.spl.2012.06.032", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A random variable Z will be called self-inverse if it has the same\ndistribution as its reciprocal 1/Z. It is shown that if Z is defined as a\nratio, X/Y, of two rv's X and Y (with Pr[X=0]=Pr[Y=0]=0), then Z is\nself-inverse if and only if X and Y are (or can be chosen to be) exchangeable.\nIn general, however, there may not exist iid X and Y in the ratio\nrepresentation of Z.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 13:42:52 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2012 14:38:31 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Cacoullos", "Theophilos", ""], ["Papadatos", "Nickos", ""]]}, {"id": "1204.1673", "submitter": "Igor Kheifets", "authors": "Igor Kheifets and Carlos Velasco", "title": "Model Adequacy Checks for Discrete Choice Dynamic Models", "comments": null, "journal-ref": null, "doi": "10.1007/978-1-4614-1653-1_14", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes new parametric model adequacy tests for possibly\nnonlinear and nonstationary time series models with noncontinuous data\ndistribution, which is often the case in applied work. In particular, we\nconsider the correct specification of parametric conditional distributions in\ndynamic discrete choice models, not only of some particular conditional\ncharacteristics such as moments or symmetry. Knowing the true distribution is\nimportant in many circumstances, in particular to apply efficient maximum\nlikelihood methods, obtain consistent estimates of partial effects and\nappropriate predictions of the probability of future events. We propose a\ntransformation of data which under the true conditional distribution leads to\ncontinuous uniform iid series. The uniformity and serial independence of the\nnew series is then examined simultaneously. The transformation can be\nconsidered as an extension of the integral transform tool for noncontinuous\ndata. We derive asymptotic properties of such tests taking into account the\nparameter estimation effect. Since transformed series are iid we do not require\nany mixing conditions and asymptotic results illustrate the double simultaneous\nchecking nature of our test. The test statistics converges under the null with\na parametric rate to the asymptotic distribution, which is case dependent,\nhence we justify a parametric bootstrap approximation. The test has power\nagainst local alternatives and is consistent. The performance of the new tests\nis compared with classical specification checks for discrete choice models.\n", "versions": [{"version": "v1", "created": "Sat, 7 Apr 2012 19:19:29 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Kheifets", "Igor", ""], ["Velasco", "Carlos", ""]]}, {"id": "1204.1937", "submitter": "Giovanni Montana", "authors": "Matt Silver, Eva Janousova, Xue Hua, Paul M. Thompson and Giovanni\n  Montana", "title": "Identification of gene pathways implicated in Alzheimer's disease using\n  longitudinal imaging phenotypes with sparse regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new method for the detection of gene pathways associated with a\nmultivariate quantitative trait, and use it to identify causal pathways\nassociated with an imaging endophenotype characteristic of longitudinal\nstructural change in the brains of patients with Alzheimer's disease (AD). Our\nmethod, known as pathways sparse reduced-rank regression (PsRRR), uses group\nlasso penalised regression to jointly model the effects of genome-wide single\nnucleotide polymorphisms (SNPs), grouped into functional pathways using prior\nknowledge of gene-gene interactions. Pathways are ranked in order of importance\nusing a resampling strategy that exploits finite sample variability. Our\napplication study uses whole genome scans and MR images from 464 subjects in\nthe Alzheimer's Disease Neuroimaging Initiative (ADNI) database. 66,182 SNPs\nare mapped to 185 gene pathways from the KEGG pathways database. Voxel-wise\nimaging signatures characteristic of AD are obtained by analysing 3D patterns\nof structural change at 6, 12 and 24 months relative to baseline. High-ranking,\nAD endophenotype-associated pathways in our study include those describing\nchemokine, Jak-stat and insulin signalling pathways, and tight junction\ninteractions. All of these have been previously implicated in AD biology. In a\nsecondary analysis, we investigate SNPs and genes that may be driving pathway\nselection, and identify a number of previously validated AD genes including\nCR1, APOE and TOMM40.\n", "versions": [{"version": "v1", "created": "Mon, 9 Apr 2012 17:43:12 GMT"}], "update_date": "2012-04-10", "authors_parsed": [["Silver", "Matt", ""], ["Janousova", "Eva", ""], ["Hua", "Xue", ""], ["Thompson", "Paul M.", ""], ["Montana", "Giovanni", ""]]}, {"id": "1204.2067", "submitter": "Charles Bouveyron", "authors": "Charles Bouveyron and Camille Brunet", "title": "Discriminative variable selection for clustering with the sparse\n  Fisher-EM algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interest in variable selection for clustering has increased recently due\nto the growing need in clustering high-dimensional data. Variable selection\nallows in particular to ease both the clustering and the interpretation of the\nresults. Existing approaches have demonstrated the efficiency of variable\nselection for clustering but turn out to be either very time consuming or not\nsparse enough in high-dimensional spaces. This work proposes to perform a\nselection of the discriminative variables by introducing sparsity in the\nloading matrix of the Fisher-EM algorithm. This clustering method has been\nrecently proposed for the simultaneous visualization and clustering of\nhigh-dimensional data. It is based on a latent mixture model which fits the\ndata into a low-dimensional discriminative subspace. Three different approaches\nare proposed in this work to introduce sparsity in the orientation matrix of\nthe discriminative subspace through $\\ell_{1}$-type penalizations. Experimental\ncomparisons with existing approaches on simulated and real-world data sets\ndemonstrate the interest of the proposed methodology. An application to the\nsegmentation of hyperspectral images of the planet Mars is also presented.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 07:34:07 GMT"}], "update_date": "2012-04-11", "authors_parsed": [["Bouveyron", "Charles", ""], ["Brunet", "Camille", ""]]}, {"id": "1204.2098", "submitter": "Matthias Katzfuss", "authors": "Matthias Katzfuss", "title": "Bayesian Nonstationary Spatial Modeling for Very Large Datasets", "comments": "16 pages, 2 color figures", "journal-ref": "Environmetrics 24 (2013) 189-200", "doi": "10.1002/env.2200", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of modern high-resolution measuring instruments\nmounted on satellites, planes, ground-based vehicles and monitoring stations, a\nneed has arisen for statistical methods suitable for the analysis of large\nspatial datasets observed on large spatial domains. Statistical analyses of\nsuch datasets provide two main challenges: First, traditional\nspatial-statistical techniques are often unable to handle large numbers of\nobservations in a computationally feasible way. Second, for large and\nheterogeneous spatial domains, it is often not appropriate to assume that a\nprocess of interest is stationary over the entire domain.\n  We address the first challenge by using a model combining a low-rank\ncomponent, which allows for flexible modeling of medium-to-long-range\ndependence via a set of spatial basis functions, with a tapered remainder\ncomponent, which allows for modeling of local dependence using a compactly\nsupported covariance function. Addressing the second challenge, we propose two\nextensions to this model that result in increased flexibility: First, the model\nis parameterized based on a nonstationary Matern covariance, where the\nparameters vary smoothly across space. Second, in our fully Bayesian model, all\ncomponents and parameters are considered random, including the number,\nlocations, and shapes of the basis functions used in the low-rank component.\n  Using simulated data and a real-world dataset of high-resolution soil\nmeasurements, we show that both extensions can result in substantial\nimprovements over the current state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 10:36:45 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2012 10:03:30 GMT"}, {"version": "v3", "created": "Thu, 12 Apr 2012 11:28:53 GMT"}, {"version": "v4", "created": "Sat, 22 Sep 2012 10:18:06 GMT"}, {"version": "v5", "created": "Fri, 21 Dec 2012 15:58:51 GMT"}], "update_date": "2015-11-26", "authors_parsed": [["Katzfuss", "Matthias", ""]]}, {"id": "1204.2108", "submitter": "Kengo Kato", "authors": "Kengo Kato", "title": "Quasi-Bayesian analysis of nonparametric instrumental variables models", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1150 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 5, 2359-2390", "doi": "10.1214/13-AOS1150", "report-no": "IMS-AOS-AOS1150", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims at developing a quasi-Bayesian analysis of the nonparametric\ninstrumental variables model, with a focus on the asymptotic properties of\nquasi-posterior distributions. In this paper, instead of assuming a\ndistributional assumption on the data generating process, we consider a\nquasi-likelihood induced from the conditional moment restriction, and put\npriors on the function-valued parameter. We call the resulting posterior\nquasi-posterior, which corresponds to ``Gibbs posterior'' in the literature.\nHere we focus on priors constructed on slowly growing finite-dimensional\nsieves. We derive rates of contraction and a nonparametric Bernstein-von Mises\ntype result for the quasi-posterior distribution, and rates of convergence for\nthe quasi-Bayes estimator defined by the posterior expectation. We show that,\nwith priors suitably chosen, the quasi-posterior distribution (the quasi-Bayes\nestimator) attains the minimax optimal rate of contraction (convergence,\nresp.). These results greatly sharpen the previous related work.\n", "versions": [{"version": "v1", "created": "Tue, 10 Apr 2012 11:28:45 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2012 00:19:18 GMT"}, {"version": "v3", "created": "Thu, 20 Dec 2012 09:45:20 GMT"}, {"version": "v4", "created": "Wed, 17 Jul 2013 13:06:42 GMT"}, {"version": "v5", "created": "Wed, 20 Nov 2013 06:59:17 GMT"}], "update_date": "2013-11-21", "authors_parsed": [["Kato", "Kengo", ""]]}, {"id": "1204.2353", "submitter": "Kun  Yang", "authors": "Kun Yang", "title": "Least Absolute Gradient Selector: Statistical Regression via Pseudo-Hard\n  Thresholding", "comments": "variable selection, pseudo-hard thresholding", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection in linear models plays a pivotal role in modern\nstatistics. Hard-thresholding methods such as $l_0$ regularization are\ntheoretically ideal but computationally infeasible. In this paper, we propose a\nnew approach, called the LAGS, short for \"least absulute gradient selector\", to\nthis challenging yet interesting problem by mimicking the discrete selection\nprocess of $l_0$ regularization. To estimate $\\beta$ under the influence of\nnoise, we consider, nevertheless, the following convex program [\\hat{\\beta} =\n\\textrm{arg min}\\frac{1}{n}\\|X^{T}(y - X\\beta)\\|_1 + \\lambda_n\\sum_{i =\n1}^pw_i(y;X;n)|\\beta_i|]\n  $\\lambda_n > 0$ controls the sparsity and $w_i > 0$ dependent on $y, X$ and\n$n$ is the weights on different $\\beta_i$; $n$ is the sample size.\nSurprisingly, we shall show in the paper, both geometrically and analytically,\nthat LAGS enjoys two attractive properties: (1) LAGS demonstrates discrete\nselection behavior and hard thresholding property as $l_0$ regularization by\nstrategically chosen $w_i$, we call this property \"pseudo-hard thresholding\";\n(2) Asymptotically, LAGS is consistent and capable of discovering the true\nmodel; nonasymptotically, LAGS is capable of identifying the sparsity in the\nmodel and the prediction error of the coefficients is bounded at the noise\nlevel up to a logarithmic factor---$\\log p$, where $p$ is the number of\npredictors.\n  Computationally, LAGS can be solved efficiently by convex program routines\nfor its convexity or by simplex algorithm after recasting it into a linear\nprogram. The numeric simulation shows that LAGS is superior compared to\nsoft-thresholding methods in terms of mean squared error and parsimony of the\nmodel.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 06:57:39 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2012 05:28:28 GMT"}, {"version": "v3", "created": "Sat, 14 Apr 2012 23:52:09 GMT"}, {"version": "v4", "created": "Fri, 19 Oct 2012 03:56:01 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Yang", "Kun", ""]]}, {"id": "1204.2477", "submitter": "Matthew Johnson", "authors": "Matthew James Johnson", "title": "A Simple Explanation of A Spectral Algorithm for Learning Hidden Markov\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple linear algebraic explanation of the algorithm in \"A Spectral\nAlgorithm for Learning Hidden Markov Models\" (COLT 2009). Most of the content\nis in Figure 2; the text just makes everything precise in four nearly-trivial\nclaims.\n", "versions": [{"version": "v1", "created": "Wed, 11 Apr 2012 15:35:43 GMT"}], "update_date": "2012-04-12", "authors_parsed": [["Johnson", "Matthew James", ""]]}, {"id": "1204.2664", "submitter": "Marie-Colette van Lieshout", "authors": "M. N. M. van Lieshout", "title": "Multi-colour random fields with polygonal realisations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of random fields that can be understood as discrete\nversions of multi-colour polygonal fields built on regular linear\ntessellations. We focus fir st on consistent polygonal fields, for which we\nshow Markovianity and solvability by means of a dynamic representation. This\nrepresentation forms the basis for new sampling techniques for Gibbsian\nmodifications of such fields, a class which cove rs lattice based random\nfields. A flux based modification is applied to the extracti on of the field\ntracks network from a SAR image of a rural area.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 09:18:20 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2012 09:26:26 GMT"}], "update_date": "2012-11-27", "authors_parsed": [["van Lieshout", "M. N. M.", ""]]}, {"id": "1204.2762", "submitter": "Joseph P. Romano", "authors": "Joseph P. Romano, Azeem M. Shaikh", "title": "On the uniform asymptotic validity of subsampling and the bootstrap", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1051 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 6, 2798-2822", "doi": "10.1214/12-AOS1051", "report-no": "IMS-AOS-AOS1051", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides conditions under which subsampling and the bootstrap can\nbe used to construct estimators of the quantiles of the distribution of a root\nthat behave well uniformly over a large class of distributions $\\mathbf{P}$.\nThese results are then applied (i) to construct confidence regions that behave\nwell uniformly over $\\mathbf{P}$ in the sense that the coverage probability\ntends to at least the nominal level uniformly over $\\mathbf{P}$ and (ii) to\nconstruct tests that behave well uniformly over $\\mathbf{P}$ in the sense that\nthe size tends to no greater than the nominal level uniformly over\n$\\mathbf{P}$. Without these stronger notions of convergence, the asymptotic\napproximations to the coverage probability or size may be poor, even in very\nlarge samples. Specific applications include the multivariate mean, testing\nmoment inequalities, multiple testing, the empirical process and U-statistics.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 15:59:18 GMT"}, {"version": "v2", "created": "Mon, 18 Feb 2013 14:25:45 GMT"}], "update_date": "2013-02-19", "authors_parsed": [["Romano", "Joseph P.", ""], ["Shaikh", "Azeem M.", ""]]}, {"id": "1204.2841", "submitter": "Jobst Heitzig", "authors": "Jobst Heitzig", "title": "Moving Taylor Bayesian Regression for nonparametric multidimensional\n  function estimation with possibly correlated errors", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a nonparametric method for estimating the value and several\nderivatives of an unknown, sufficiently smooth real-valued function of\nreal-valued arguments from a finite sample of points, where both the function\narguments and the corresponding values are known only up to measurement errors\nhaving some assumed distribution and correlation structure. The method, Moving\nTaylor Bayesian Regression (MOTABAR), uses Bayesian updating to find the\nposterior mean of the coefficients of a Taylor polynomial of the function at a\nmoving position of interest. When measurement errors are neglected, MOTABAR\nbecomes a multivariate interpolation method. It contains several well-known\nregression and interpolation methods as special or limit cases. We demonstrate\nthe performance of MOTABAR using the reconstruction of the Lorenz attractor\nfrom noisy observations as an example.\n", "versions": [{"version": "v1", "created": "Thu, 12 Apr 2012 21:07:18 GMT"}], "update_date": "2012-04-16", "authors_parsed": [["Heitzig", "Jobst", ""]]}, {"id": "1204.3130", "submitter": "Shuichi Kawano", "authors": "Shuichi Kawano", "title": "Adaptive bridge regression modeling with model selection criteria", "comments": "14 pages, 4 figures", "journal-ref": "Bulletin of Informatics and Cybernetics 44 (2012) 29-39", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing an adaptive bridge regression\nmodeling, which is a penalized procedure by imposing different weights to\ndifferent coefficients in the bridge penalty term. A crucial issue in the\nmodeling process is the choices of adjusted parameters included in the models.\nWe treat the selection of the adjusted parameters as model selection and\nevaluation problems. In order to select the parameters, model selection\ncriteria are derived from information-theoretic and Bayesian approach. We\nconduct some numerical studies to investigate the effectiveness of our proposed\nmodeling strategy.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2012 02:08:26 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2012 08:30:13 GMT"}], "update_date": "2013-02-15", "authors_parsed": [["Kawano", "Shuichi", ""]]}, {"id": "1204.3132", "submitter": "Paul von Hippel", "authors": "Paul T. von Hippel", "title": "The Bias and Efficiency of Incomplete-Data Estimators in Small\n  Univariate Normal Samples", "comments": "32 pages, 3 figures, 3 tables, 2 Appendices", "journal-ref": "Sociological Methods & Research, November 2013; vol. 42, 4: pp.\n  531-558", "doi": "10.1177/0049124113494582", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Widely used methods for analyzing missing data can be biased in small\nsamples. To understand these biases, we evaluate in detail the situation where\na small univariate normal sample, with values missing at random, is analyzed\nusing either observed-data maximum likelihood (ML) or multiple imputation (MI).\nWe evaluate two types of MI: the usual Bayesian approach, which we call\nposterior draw (PD) imputation, and a little-used alternative, which we call ML\nimputation, in which values are imputed conditionally on an ML estimate. We\nfind that observed-data ML is more efficient and has lower mean squared error\nthan either type of MI. Between the two types of MI, ML imputation is more\nefficient than PD imputation, and ML imputation also has less potential for\nbias in small samples. The bias and efficiency of PD imputation can be improved\nby a change of prior.\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2012 02:25:58 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2012 21:32:32 GMT"}, {"version": "v3", "created": "Tue, 2 Oct 2012 18:58:43 GMT"}, {"version": "v4", "created": "Thu, 20 Jun 2013 16:10:36 GMT"}], "update_date": "2017-03-27", "authors_parsed": [["von Hippel", "Paul T.", ""]]}, {"id": "1204.3183", "submitter": "Cedric Ginestet", "authors": "Cedric E. Ginestet", "title": "Strong Consistency of Frechet Sample Mean Sets for Graph-Valued Random\n  Variables", "comments": "21 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-bio.QM stat.ME stat.TH", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Frechet mean or barycenter generalizes the idea of averaging in spaces\nwhere pairwise addition is not well-defined. In general metric spaces, the\nFrechet sample mean is not a consistent estimator of the theoretical Frechet\nmean. For graph-valued random variables, for instance, the Frechet sample mean\nmay fail to converge to a unique value. Hence, it becomes necessary to consider\nthe convergence of sequences of sets of graphs. We show that a specific type of\nalmost sure convergence for the Frechet sample mean previously introduced by\nZiezold (1977) is, in fact, equivalent to the Kuratowski outer limit of a\nsequence of Frechet sample means. Equipped with this outer limit, we provide a\nnew proof of the strong consistency of the Frechet sample mean for graph-valued\nrandom variables in separable (pseudo-)metric space. Our proof strategy\nexploits the fact that the metric of interest is bounded, since we are\nconsidering graphs over a finite number of vertices. In this setting, we\ndescribe two strong laws of large numbers for both the restricted and\nunrestricted Frechet sample means of all orders, thereby generalizing a\nprevious result, due to Sverdrup-Thygeson (1981).\n", "versions": [{"version": "v1", "created": "Sat, 14 Apr 2012 15:37:04 GMT"}, {"version": "v2", "created": "Fri, 29 Jun 2012 08:27:45 GMT"}, {"version": "v3", "created": "Sat, 16 Mar 2013 20:19:37 GMT"}, {"version": "v4", "created": "Wed, 15 May 2013 13:42:43 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Ginestet", "Cedric E.", ""]]}, {"id": "1204.3235", "submitter": "Xiaogang (Steven) Wang", "authors": "Xiaogang Wang and Jianhong Wu", "title": "Convergent and Anti-diffusive Properties of Mean-Shift Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An analytic framework based on partial differential equations is derived for\ncertain dynamic clustering methods.\n  The proposed mathematical framework is based on the application of the\nconservation law in physics to characterize successive transformations of the\nunderlying probability density function. It is then applied to analyze the\nconvergence and stability of mean shift type of dynamic clustering algorithms.\nTheoretical analysis shows that un-supervised mean-shift type of algorithm is\nintrinsically unstable. It is proved that the only possibility of a correct\nconvergence for unsupervised mean shift type of algorithm is to transform the\noriginal probability density into a multivariate normal distribution with no\ndependence struture. Our analytical results suggest that a more stable and\nconvergent mean shift algorithm might be achieved by adopting a judiciously\nchosen supervision mechanism.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2012 03:35:52 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2013 13:50:39 GMT"}], "update_date": "2013-07-11", "authors_parsed": [["Wang", "Xiaogang", ""], ["Wu", "Jianhong", ""]]}, {"id": "1204.3251", "submitter": "Vladimir Vovk", "authors": "Valentina Fedorova, Alex Gammerman, Ilia Nouretdinov, and Vladimir\n  Vovk", "title": "Plug-in martingales for testing exchangeability on-line", "comments": "8 pages, 7 figures; ICML 2012 Conference Proceedings", "journal-ref": null, "doi": null, "report-no": "On-line Compression Modelling Project (New Series), Working Paper 04", "categories": "cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard assumption in machine learning is the exchangeability of data,\nwhich is equivalent to assuming that the examples are generated from the same\nprobability distribution independently. This paper is devoted to testing the\nassumption of exchangeability on-line: the examples arrive one by one, and\nafter receiving each example we would like to have a valid measure of the\ndegree to which the assumption of exchangeability has been falsified. Such\nmeasures are provided by exchangeability martingales. We extend known\ntechniques for constructing exchangeability martingales and show that our new\nmethod is competitive with the martingales introduced before. Finally we\ninvestigate the performance of our testing method on two benchmark datasets,\nUSPS and Statlog Satellite data; for the former, the known techniques give\nsatisfactory results, but for the latter our new more flexible method becomes\nnecessary.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2012 10:21:57 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2012 09:36:27 GMT"}], "update_date": "2012-06-29", "authors_parsed": [["Fedorova", "Valentina", ""], ["Gammerman", "Alex", ""], ["Nouretdinov", "Ilia", ""], ["Vovk", "Vladimir", ""]]}, {"id": "1204.3331", "submitter": "Hua Zhou", "authors": "Hua Zhou and Lexin Li", "title": "Regularized Matrix Regression", "comments": "27 pages, 5 figure", "journal-ref": null, "doi": "10.1111/rssb.12031", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Modern technologies are producing a wealth of data with complex structures.\nFor instance, in two-dimensional digital imaging, flow cytometry, and\nelectroencephalography, matrix type covariates frequently arise when\nmeasurements are obtained for each combination of two underlying variables. To\naddress scientific questions arising from those data, new regression methods\nthat take matrices as covariates are needed, and sparsity or other forms of\nregularization are crucial due to the ultrahigh dimensionality and complex\nstructure of the matrix data. The popular lasso and related regularization\nmethods hinge upon the sparsity of the true signal in terms of the number of\nits nonzero coefficients. However, for the matrix data, the true signal is\noften of, or can be well approximated by, a low rank structure. As such, the\nsparsity is frequently in the form of low rank of the matrix parameters, which\nmay seriously violate the assumption of the classical lasso. In this article,\nwe propose a class of regularized matrix regression methods based on spectral\nregularization. Highly efficient and scalable estimation algorithm is\ndeveloped, and a degrees of freedom formula is derived to facilitate model\nselection along the regularization path. Superior performance of the proposed\nmethod is demonstrated on both synthetic and real examples.\n", "versions": [{"version": "v1", "created": "Sun, 15 Apr 2012 22:58:36 GMT"}], "update_date": "2013-10-22", "authors_parsed": [["Zhou", "Hua", ""], ["Li", "Lexin", ""]]}, {"id": "1204.3524", "submitter": "Johan Segers", "authors": "Miguel de Carvalho, Boris Oumow, Johan Segers, Micha{\\l} Warcho{\\l}", "title": "A Euclidean likelihood estimator for bivariate tail dependence", "comments": "18 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The spectral measure plays a key role in the statistical modeling of\nmultivariate extremes. Estimation of the spectral measure is a complex issue,\ngiven the need to obey a certain moment condition. We propose a Euclidean\nlikelihood-based estimator for the spectral measure which is simple and\nexplicitly defined, with its expression being free of Lagrange multipliers. Our\nestimator is shown to have the same limit distribution as the maximum empirical\nlikelihood estimator of J. H. J. Einmahl and J. Segers, Annals of Statistics\n37(5B), 2953--2989 (2009). Numerical experiments suggest an overall good\nperformance and identical behavior to the maximum empirical likelihood\nestimator. We illustrate the method in an extreme temperature data analysis.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 15:26:46 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["de Carvalho", "Miguel", ""], ["Oumow", "Boris", ""], ["Segers", "Johan", ""], ["Warcho\u0142", "Micha\u0142", ""]]}, {"id": "1204.3547", "submitter": "Dave Higdon", "authors": "Dave Higdon, Matt Pratola, James Gattiker, Earl Lawrence, Salman\n  Habib, Katrin Heitmann, Steve Price, Charles Jackson, Michael Tobis", "title": "Computer Model Calibration using the Ensemble Kalman Filter", "comments": "20 pages; 11 figures", "journal-ref": null, "doi": null, "report-no": "LA-UR-12-20660", "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The ensemble Kalman filter (EnKF) (Evensen, 2009) has proven effective in\nquantifying uncertainty in a number of challenging dynamic, state estimation,\nor data assimilation, problems such as weather forecasting and ocean modeling.\nIn these problems a high-dimensional state parameter is successively updated\nbased on recurring physical observations, with the aid of a computationally\ndemanding forward model that prop- agates the state from one time step to the\nnext. More recently, the EnKF has proven effective in history matching in the\npetroleum engineering community (Evensen, 2009; Oliver and Chen, 2010). Such\napplications typically involve estimating large numbers of parameters,\ndescribing an oil reservoir, using data from production history that accumulate\nover time. Such history matching problems are especially challenging examples\nof computer model calibration since they involve a large number of model\nparameters as well as a computationally demanding forward model. More\ngenerally, computer model calibration combines physical observations with a\ncomputational model - a computer model - to estimate unknown parameters in the\ncomputer model. This paper explores how the EnKF can be used in computer model\ncalibration problems, comparing it to other more common approaches, considering\napplications in climate and cosmology.\n", "versions": [{"version": "v1", "created": "Mon, 16 Apr 2012 16:03:23 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2012 23:54:01 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Higdon", "Dave", ""], ["Pratola", "Matt", ""], ["Gattiker", "James", ""], ["Lawrence", "Earl", ""], ["Habib", "Salman", ""], ["Heitmann", "Katrin", ""], ["Price", "Steve", ""], ["Jackson", "Charles", ""], ["Tobis", "Michael", ""]]}, {"id": "1204.3687", "submitter": "Benjamin Shaby", "authors": "Benjamin A Shaby", "title": "The open-faced sandwich adjustment for MCMC using estimating functions", "comments": null, "journal-ref": null, "doi": "10.1080/10618600.2013.842174", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The situation frequently arises where working with the likelihood function is\nproblematic. This can happen for several reasons---perhaps the likelihood is\nprohibitively computationally expensive, perhaps it lacks some robustness\nproperty, or perhaps it is simply not known for the model under consideration.\nIn these cases, it is often possible to specify alternative functions of the\nparameters and the data that can be maximized to obtain asymptotically normal\nestimates. However, these scenarios present obvious problems if one is\ninterested in applying Bayesian techniques. Here we describe open-faced\nsandwich adjustment, a way to incorporate a wide class of non-likelihood\nobjective functions within Bayesian-like models to obtain asymptotically valid\nparameter estimates and inference via MCMC. Two simulation examples show that\nthe method provides accurate frequentist uncertainty estimates. The open-faced\nsandwich adjustment is applied to a Poisson spatio-temporal model to analyze an\nornithology dataset from the citizen science initiative eBird.\n", "versions": [{"version": "v1", "created": "Tue, 17 Apr 2012 02:15:15 GMT"}], "update_date": "2014-05-16", "authors_parsed": [["Shaby", "Benjamin A", ""]]}, {"id": "1204.3993", "submitter": "Maria Giovanna Ranalli", "authors": "Enrico Fabrizi, Giorgio E. Montanari, Maria Giovanna Ranalli", "title": "A hierarchical latent class model for predicting disability small area\n  counts from survey data", "comments": null, "journal-ref": null, "doi": "10.1111/rssa.12112", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article considers the estimation of the number of severely disabled\npeople using data from the Italian survey on Health Conditions and Appeal to\nMedicare. Disability is indirectly measured using a set of categorical items,\nwhich survey a set of functions concerning the ability of a person to\naccomplish everyday tasks. Latent Class Models can be employed to classify the\npopulation according to different levels of a latent variable connected with\ndisability. The survey, however, is designed to provide reliable estimates at\nthe level of Administrative Regions (NUTS2 level), while local authorities are\ninterested in quantifying the amount of population that belongs to each latent\nclass at a sub-regional level. Therefore, small area estimation techniques\nshould be used. The challenge of the present application is that the variable\nof interest is not directly observed. Adopting a full Bayesian approach, we\nbase small area estimation on a Latent Class model in which the probability of\nbelonging to each latent class changes with covariates and the influence of age\nis learnt from the data using penalized splines. Deimmler-Reisch bases are\nshown to improve speed and mixing of MCMC chains used to simulate posteriors.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 07:03:04 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Fabrizi", "Enrico", ""], ["Montanari", "Giorgio E.", ""], ["Ranalli", "Maria Giovanna", ""]]}, {"id": "1204.4021", "submitter": "Charles Bouveyron", "authors": "Charles Bouveyron and St\\'ephane Girard and Mathieu Fauvel", "title": "Kernel discriminant analysis and clustering with parsimonious Gaussian\n  process models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a family of parsimonious Gaussian process models which\nallow to build, from a finite sample, a model-based classifier in an infinite\ndimensional space. The proposed parsimonious models are obtained by\nconstraining the eigen-decomposition of the Gaussian processes modeling each\nclass. This allows in particular to use non-linear mapping functions which\nproject the observations into infinite dimensional spaces. It is also\ndemonstrated that the building of the classifier can be directly done from the\nobservation space through a kernel function. The proposed classification method\nis thus able to classify data of various types such as categorical data,\nfunctional data or networks. Furthermore, it is possible to classify mixed data\nby combining different kernels. The methodology is as well extended to the\nunsupervised classification case. Experimental results on various data sets\ndemonstrate the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 09:18:14 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2012 15:13:49 GMT"}], "update_date": "2012-06-18", "authors_parsed": [["Bouveyron", "Charles", ""], ["Girard", "St\u00e9phane", ""], ["Fauvel", "Mathieu", ""]]}, {"id": "1204.4140", "submitter": "Chul-Ho Lee", "authors": "Chul-Ho Lee, Xin Xu, and Do Young Eun", "title": "Beyond Random Walk and Metropolis-Hastings Samplers: Why You Should Not\n  Backtrack for Unbiased Graph Sampling", "comments": "A short (double-column, 12-page) version of this paper will appear in\n  ACM SIGMETRICS/Performance 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.DS cs.NI cs.SI physics.data-an physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph sampling via crawling has been actively considered as a generic and\nimportant tool for collecting uniform node samples so as to consistently\nestimate and uncover various characteristics of complex networks. The so-called\nsimple random walk with re-weighting (SRW-rw) and Metropolis-Hastings (MH)\nalgorithm have been popular in the literature for such unbiased graph sampling.\nHowever, an unavoidable downside of their core random walks -- slow diffusion\nover the space, can cause poor estimation accuracy. In this paper, we propose\nnon-backtracking random walk with re-weighting (NBRW-rw) and MH algorithm with\ndelayed acceptance (MHDA) which are theoretically guaranteed to achieve, at\nalmost no additional cost, not only unbiased graph sampling but also higher\nefficiency (smaller asymptotic variance of the resulting unbiased estimators)\nthan the SRW-rw and the MH algorithm, respectively. In particular, a remarkable\nfeature of the MHDA is its applicability for any non-uniform node sampling like\nthe MH algorithm, but ensuring better sampling efficiency than the MH\nalgorithm. We also provide simulation results to confirm our theoretical\nfindings.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 17:11:23 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Lee", "Chul-Ho", ""], ["Xu", "Xin", ""], ["Eun", "Do Young", ""]]}, {"id": "1204.4148", "submitter": "Vadim Asnin", "authors": "Vadim Asnin", "title": "Algorithm for multivariate data standardization up to third moment", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for transforming multivariate data to a form with normalized\nfirst, second and third moments is presented.\n", "versions": [{"version": "v1", "created": "Wed, 18 Apr 2012 17:46:32 GMT"}], "update_date": "2012-04-19", "authors_parsed": [["Asnin", "Vadim", ""]]}, {"id": "1204.4227", "submitter": "Miles Lopes", "authors": "Miles E. Lopes", "title": "Estimating Unknown Sparsity in Compressed Sensing", "comments": "This is version 2. Many aspects of the paper have been revised. The\n  restriction to non-negative signals has been removed. 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the theory of compressed sensing (CS), the sparsity ||x||_0 of the unknown\nsignal x\\in\\R^p is commonly assumed to be a known parameter. However, it is\ntypically unknown in practice. Due to the fact that many aspects of CS depend\non knowing ||x||_0, it is important to estimate this parameter in a data-driven\nway. A second practical concern is that ||x||_0 is a highly unstable function\nof x. In particular, for real signals with entries not exactly equal to 0, the\nvalue ||x||_0=p is not a useful description of the effective number of\ncoordinates. In this paper, we propose to estimate a stable measure of sparsity\ns(x):=||x||_1^2/||x||_2^2, which is a sharp lower bound on ||x||_0. Our\nestimation procedure uses only a small number of linear measurements, does not\nrely on any sparsity assumptions, and requires very little computation. A\nconfidence interval for s(x) is provided, and its width is shown to have no\ndependence on the signal dimension p. Moreover, this result extends naturally\nto the matrix recovery setting, where a soft version of matrix rank can be\nestimated with analogous guarantees. Finally, we show that the use of\nrandomized measurements is essential to estimating s(x). This is accomplished\nby proving that the minimax risk for estimating s(x) with deterministic\nmeasurements is large when n<<p.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 00:43:05 GMT"}, {"version": "v2", "created": "Mon, 25 Feb 2013 14:51:32 GMT"}], "update_date": "2013-02-26", "authors_parsed": [["Lopes", "Miles E.", ""]]}, {"id": "1204.4333", "submitter": "Fatih Lau Mr", "authors": "Axel Gandy, F. Din-Houn Lau", "title": "Non-Restarting CUSUM charts and Control of the False Discovery Rate", "comments": "10 pages, 2 figures", "journal-ref": "Biometrika (2013) 100 (1): 261-268", "doi": "10.1093/biomet/ass066", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cumulative sum (CUSUM) charts are typically used to detect changes in a\nstream of observations e.g. shifts in the mean. Usually, after signalling, the\nchart is restarted by setting it to some value below the signalling threshold.\nWe propose a non-restarting CUSUM chart which is able to detect periods during\nwhich the stream is out of control. Further, we advocate an upper boundary to\nprevent the CUSUM chart rising too high, which helps detecting a change back\ninto control. We present a novel algorithm to control the false discovery rate\n(FDR) pointwise in time when considering CUSUM charts based on multiple streams\nof data. We prove that the FDR is controlled under two definitions of a false\ndiscovery simultaneously. Simulations reveal the difference in FDR control when\nusing these two definitions and other desirable definitions of a false\ndiscovery.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 12:14:10 GMT"}], "update_date": "2013-07-30", "authors_parsed": [["Gandy", "Axel", ""], ["Lau", "F. Din-Houn", ""]]}, {"id": "1204.4460", "submitter": "Adina Soaita", "authors": "Robb J. Muirhead and Adina I. Soaita", "title": "On an Approach to Bayesian Sample Sizing in Clinical Trials", "comments": "To appear in \"Festschrift in honor of Professor Morris L. Eaton\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores an approach to Bayesian sample size determination in\nclinical trials. The approach falls into the category of what is often called\n\"proper Bayesian\", in that it does not mix frequentist concepts with Bayesian\nones. A criterion for a \"successful trial\" is defined in terms of a posterior\nprobability, its probability is assessed using the marginal distribution of the\ndata, and this probability forms the basis for choosing sample sizes. We\nillustrate with a standard problem in clinical trials, that of establishing\nsuperiority of a new drug over a control.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 20:13:03 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Muirhead", "Robb J.", ""], ["Soaita", "Adina I.", ""]]}, {"id": "1204.4494", "submitter": "David Degras", "authors": "David Degras", "title": "Rotation Sampling for Functional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the survey estimation of a population mean in continuous\ntime. For this purpose we extend the rotation sampling method to functional\ndata. In contrast to conventional rotation designs that select the sample\nbefore the survey, our approach randomizes each sample replacement and thus\nallows for adaptive sampling. Using Markov chain theory, we evaluate the\ncovariance structure and the integrated squared error [ISE] of the related\nHorvitz-Thompson estimator. Our sampling designs decrease the mean ISE by\nsuitably reallocating the sample across population strata during replacements.\nThey also reduce the variance of the ISE by increasing the frequency or the\nintensity of replacements. To investigate the benefits of using both current\nand past measurements in the estimation, we develop a new composite estimator.\nIn an application to electricity usage data, our rotation method outperforms\nfixed panels and conventional rotation samples. Because of the weak temporal\ndependence of the data, the composite estimator only slightly improves upon the\nHorvitz-Thompson estimator.\n", "versions": [{"version": "v1", "created": "Thu, 19 Apr 2012 23:04:14 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2012 02:14:31 GMT"}, {"version": "v3", "created": "Wed, 18 Jul 2012 13:57:26 GMT"}, {"version": "v4", "created": "Tue, 9 Apr 2013 17:15:01 GMT"}], "update_date": "2013-04-10", "authors_parsed": [["Degras", "David", ""]]}, {"id": "1204.4544", "submitter": "Silvia Bacci Dr", "authors": "Silvia Bacci and Francesco Bartolucci", "title": "Mixtures of equispaced normal distributions and their use for testing\n  symmetry in univariate data", "comments": "16 pages, 3 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a random sample of observations, mixtures of normal densities are often\nused to estimate the unknown continuous distribution from which the data come.\nHere we propose the use of this semiparametric framework for testing symmetry\nabout an unknown value. More precisely, we show how the null hypothesis of\nsymmetry may be formulated in terms of normal mixture model, with weights about\nthe centre of symmetry constrained to be equal one another. The resulting model\nis nested in a more general unconstrained one, with same number of mixture\ncomponents and free weights. Therefore, after having maximised the constrained\nand unconstrained log-likelihoods by means of a suitable algorithm, such as the\nExpectation-Maximisation, symmetry is tested against skewness through a\nlikelihood ratio statistic. The performance of the proposed mixture-based test\nis illustrated through a Monte Carlo simulation study, where we compare two\nversions of the test, based on different criteria to select the number of\nmixture components, with the traditional one based on the third standardised\nmoment. An illustrative example is also given that focuses on real data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 06:54:50 GMT"}], "update_date": "2012-04-23", "authors_parsed": [["Bacci", "Silvia", ""], ["Bartolucci", "Francesco", ""]]}, {"id": "1204.4699", "submitter": "Subhadeep Mukhopadhyay", "authors": "Emanuel Parzen and Subhadeep Mukhopadhyay (Deep)", "title": "Modeling, dependence, classification, united statistical science, many\n  cultures", "comments": "31 pages, 10 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breiman (2001) proposed to statisticians awareness of two cultures: 1.\nParametric modeling culture, pioneered by R.A.Fisher and Jerzy Neyman; 2.\nAlgorithmic predictive culture, pioneered by machine learning research.\n  Parzen (2001), as a part of discussing Breiman (2001), proposed that\nresearchers be aware of many cultures, including the focus of our research: 3.\nNonparametric, quantile based, information theoretic modeling. We provide a\nunification of many statistical methods for traditional small data sets and\nemerging big data sets in terms of comparison density, copula density, measure\nof dependence, correlation, information, new measures (called LP score\ncomoments) that apply to long tailed distributions with out finite second order\nmoments. A very important goal is to unify methods for discrete and continuous\nrandom variables. Our research extends these methods to modern high dimensional\ndata modeling.\n", "versions": [{"version": "v1", "created": "Fri, 20 Apr 2012 18:50:20 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2012 01:12:08 GMT"}, {"version": "v3", "created": "Tue, 24 Apr 2012 02:11:01 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Parzen", "Emanuel", "", "Deep"], ["Mukhopadhyay", "Subhadeep", "", "Deep"]]}, {"id": "1204.4763", "submitter": "Art Owen", "authors": "Art B. Owen", "title": "Better estimation of small Sobol' sensitivity indices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new method for estimating Sobol' indices is proposed. The new method makes\nuse of 3 independent input vectors rather than the usual 2. It attains much\ngreater accuracy on problems where the target Sobol' index is small, even\noutperforming some oracles which adjust using the true but unknown mean of the\nfunction. When the target Sobol' index is quite large, the oracles do better\nthan the new method.\n", "versions": [{"version": "v1", "created": "Sat, 21 Apr 2012 00:15:20 GMT"}], "update_date": "2012-04-24", "authors_parsed": [["Owen", "Art B.", ""]]}, {"id": "1204.5243", "submitter": "Francesca Petralia", "authors": "Francesca Petralia, Vinayak Rao, David B. Dunson", "title": "Repulsive Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete mixture models are routinely used for density estimation and\nclustering. While conducting inferences on the cluster-specific parameters,\ncurrent frequentist and Bayesian methods often encounter problems when clusters\nare placed too close together to be scientifically meaningful. Current Bayesian\npractice generates component-specific parameters independently from a common\nprior, which tends to favor similar components and often leads to substantial\nprobability assigned to redundant components that are not needed to fit the\ndata. As an alternative, we propose to generate components from a repulsive\nprocess, which leads to fewer, better separated and more interpretable\nclusters. We characterize this repulsive prior theoretically and propose a\nMarkov chain Monte Carlo sampling algorithm for posterior computation. The\nmethods are illustrated using simulated data as well as real datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 00:46:57 GMT"}, {"version": "v2", "created": "Thu, 20 Sep 2012 02:52:08 GMT"}], "update_date": "2012-09-21", "authors_parsed": [["Petralia", "Francesca", ""], ["Rao", "Vinayak", ""], ["Dunson", "David B.", ""]]}, {"id": "1204.5291", "submitter": "Georgios Fellouris Dr.", "authors": "Georgios Fellouris, Alexander G. Tartakovsky", "title": "Almost optimal sequential tests of discrete composite hypotheses", "comments": "31 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of sequentially testing a simple null hypothesis\nversus a composite alternative hypothesis that consists of a finite set of\ndensities. We study sequential tests that are based on thresholding of\nmixture-based likelihood ratio statistics and weighted generalized likelihood\nratio statistics. It is shown that both sequential tests have several\nasymptotic optimality properties as error probabilities go to zero. First, for\nany weights, they minimize the expected sample size within a constant term\nunder every scenario in the alternative hypothesis and at least to first order\nunder the null hypothesis. Second, for appropriate weights that are specified\nup to a prior distribution, they minimize within an asymptotically negligible\nterm a weighted expected sample size in the alternative hypothesis. Third, for\na particular prior distribution, they are almost minimax with respect to the\nexpected Kullback-Leibler divergence until stopping. Furthermore, based on\nhigh-order asymptotic expansions for the operating characteristics, we propose\nprior distributions that lead to a robust behavior. Finally, based on\nasymptotic analysis as well as on simulation experiments, we argue that both\ntests have the same performance when they are designed with the same weights.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 07:49:49 GMT"}, {"version": "v2", "created": "Tue, 22 Jan 2013 06:33:02 GMT"}], "update_date": "2013-01-23", "authors_parsed": [["Fellouris", "Georgios", ""], ["Tartakovsky", "Alexander G.", ""]]}, {"id": "1204.5459", "submitter": "Umberto Picchini", "authors": "Umberto Picchini", "title": "Inference for SDE models via Approximate Bayesian Computation", "comments": "Version accepted for publication in Journal of Computational &\n  Graphical Statistics", "journal-ref": null, "doi": "10.1080/10618600.2013.866048", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Models defined by stochastic differential equations (SDEs) allow for the\nrepresentation of random variability in dynamical systems. The relevance of\nthis class of models is growing in many applied research areas and is already a\nstandard tool to model e.g. financial, neuronal and population growth dynamics.\nHowever inference for multidimensional SDE models is still very challenging,\nboth computationally and theoretically. Approximate Bayesian computation (ABC)\nallow to perform Bayesian inference for models which are sufficiently complex\nthat the likelihood function is either analytically unavailable or\ncomputationally prohibitive to evaluate. A computationally efficient ABC-MCMC\nalgorithm is proposed, halving the running time in our simulations. Focus is on\nthe case where the SDE describes latent dynamics in state-space models; however\nthe methodology is not limited to the state-space framework. Simulation studies\nfor a pharmacokinetics/pharmacodynamics model and for stochastic chemical\nreactions are considered and a MATLAB package implementing our ABC-MCMC\nalgorithm is provided.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 18:58:34 GMT"}, {"version": "v2", "created": "Sat, 28 Apr 2012 12:19:49 GMT"}, {"version": "v3", "created": "Sun, 5 Aug 2012 08:18:26 GMT"}, {"version": "v4", "created": "Sun, 6 Jan 2013 11:31:32 GMT"}, {"version": "v5", "created": "Tue, 25 Jun 2013 11:27:55 GMT"}, {"version": "v6", "created": "Fri, 8 Nov 2013 08:13:14 GMT"}], "update_date": "2014-08-06", "authors_parsed": [["Picchini", "Umberto", ""]]}, {"id": "1204.5488", "submitter": "Rohit Patra", "authors": "Rohit Kumar Patra and Bodhisattva Sen", "title": "Estimation of a Two-component Mixture Model with Applications to\n  Multiple Testing", "comments": "42 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a two-component mixture model with one known component. We\ndevelop methods for estimating the mixing proportion and the unknown\ndistribution nonparametrically, given i.i.d.~data from the mixture model, using\nideas from shape restricted function estimation. We establish the consistency\nof our estimators. We find the rate of convergence and asymptotic limit of the\nestimator for the mixing proportion. Completely automated distribution-free\nhonest finite sample lower confidence bounds are developed for the mixing\nproportion. Connection to the problem of multiple testing is discussed. The\nidentifiability of the model, and the estimation of the density of the unknown\ndistribution are also addressed. We compare the proposed estimators, which are\neasily implementable, with some of the existing procedures through simulation\nstudies and analyse two data sets, one arising from an application in astronomy\nand the other from a microarray experiment.\n", "versions": [{"version": "v1", "created": "Tue, 24 Apr 2012 20:07:56 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 14:50:34 GMT"}, {"version": "v3", "created": "Wed, 16 Apr 2014 14:11:16 GMT"}, {"version": "v4", "created": "Tue, 15 Sep 2015 01:04:04 GMT"}, {"version": "v5", "created": "Mon, 9 Nov 2015 01:11:50 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Patra", "Rohit Kumar", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1204.5581", "submitter": "Christina  Steinkohl", "authors": "Richard A. Davis, Claudia Kl\\\"uppelberg and Christina Steinkohl", "title": "Statistical inference for max-stable processes in space and time", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-stable processes have proved to be useful for the statistical modelling\nof spatial extremes. Several representations of max-stable random fields have\nbeen proposed in the literature. One such representation is based on a limit of\nnormalized and scaled pointwise maxima of stationary Gaussian processes that\nwas first introduced by Kabluchko, Schlather and de Haan (2009).\n  This paper deals with statistical inference for max-stable space-time\nprocesses that are defined in an analogous fashion. We describe pairwise\nlikelihood estimation, where the pairwise density of the process is used to\nestimate the model parameters and prove strong consistency and asymptotic\nnormality of the parameter estimates for an increasing space-time dimension,\ni.e., as the joint number of spatial locations and time points tends to\ninfinity. A simulation study shows that the proposed method works well for\nthese models.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2012 08:04:09 GMT"}], "update_date": "2012-04-26", "authors_parsed": [["Davis", "Richard A.", ""], ["Kl\u00fcppelberg", "Claudia", ""], ["Steinkohl", "Christina", ""]]}, {"id": "1204.5724", "submitter": "Paul T Edlefsen", "authors": "Paul T. Edlefsen and Arthur P. Dempster", "title": "Nonparametric survival analysis and vaccine efficacy using\n  Dempster-Shafer analysis", "comments": "This is an incomplete draft (missing refs, results)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an extension of nonparametric DS inference for arbitrary\nunivariate CDFs to the case in which some failure times are (right)-censored,\nand then apply this to the problem of assessing evidence regarding assertions\nabout relative risks across two populations. The approach enables exploration\nof the sensitivity of survival analyses to assumed independence of the missing\ndata process and the failure proces. We present an application to the partially\nefficacious RV144 (HIV-1) vaccine trial, and show that the strength of\nconclusions of vaccine efficacy depend on assumptions about the maximum failure\nrates of the subjects lost-to-followup.\n", "versions": [{"version": "v1", "created": "Wed, 25 Apr 2012 18:12:50 GMT"}], "update_date": "2012-04-26", "authors_parsed": [["Edlefsen", "Paul T.", ""], ["Dempster", "Arthur P.", ""]]}, {"id": "1204.5894", "submitter": "M{\\aa}ns Thulin", "authors": "M{\\aa}ns Thulin", "title": "Coverage-adjusted confidence intervals for a binomial proportion", "comments": "20 pages, 10 figures, 2 tables", "journal-ref": "Scandinavian Journal of Statistics, 41, 291-300 (2014)", "doi": "10.1111/sjos.12021", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the classic problem of interval estimation of a proportion $p$\nbased on binomial sampling. The \"exact\" Clopper-Pearson confidence interval for\n$p$ is known to be unnecessarily conservative. We propose coverage-adjustments\nof the Clopper-Pearson interval using prior and posterior distributions of $p$.\nThe adjusted intervals have improved coverage and are often shorter than\ncompeting intervals found in the literature. Using new heatmap-type plots for\ncomparing confidence intervals, we find that the coverage-adjusted intervals\nare particularly suitable for $p$ close to 0 or 1.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 11:57:59 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2012 14:10:05 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Thulin", "M\u00e5ns", ""]]}, {"id": "1204.6087", "submitter": "Christopher Paciorek", "authors": "Christopher J. Paciorek", "title": "Spatial models for point and areal data using Markov random fields on a\n  fine grid", "comments": "26 pages, 10 figures", "journal-ref": "Electronic Journal of Statistics, Vol. 7 (2013) 946-972", "doi": "10.1214/13-EJS791", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I consider the use of Markov random fields (MRFs) on a fine grid to represent\nlatent spatial processes when modeling point-level and areal data, including\nsituations with spatial misalignment. Point observations are related to the\ngrid cell in which they reside, while areal observations are related to the\n(approximate) integral over the latent process within the area of interest. I\nreview several approaches to specifying the neighborhood structure for\nconstructing the MRF precision matrix, presenting results comparing these MRF\nrepresentations analytically, in simulations, and in two examples. The results\nprovide practical guidance for choosing a spatial process representation and\nhighlight the importance of this choice. In particular, the results demonstrate\nthat, and explain why, standard CAR models can behave strangely for point-level\ndata. They show that various neighborhood weighting approaches based on\nhigher-order neighbors that have been suggested for MRF models do not produce\nsmooth fields, which raises doubts about their utility. Finally, they indicate\nthat an MRF that approximates a thin plate spline compares favorably to\nstandard CAR models and to kriging under many circumstances.\n", "versions": [{"version": "v1", "created": "Thu, 26 Apr 2012 23:54:35 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2013 18:33:13 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Paciorek", "Christopher J.", ""]]}, {"id": "1204.6118", "submitter": "Fabio Sigrist", "authors": "Fabio Sigrist, Hans R. K\\\"unsch, Werner A. Stahel", "title": "Stochastic partial differential equation based modelling of large\n  space-time data sets", "comments": null, "journal-ref": "Journal of the Royal Statistical Society: Series B (Statistical\n  Methodology) 77-1 (2015) 3-33", "doi": "10.1111/rssb.12061", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increasingly larger data sets of processes in space and time ask for\nstatistical models and methods that can cope with such data. We show that the\nsolution of a stochastic advection-diffusion partial differential equation\nprovides a flexible model class for spatio-temporal processes which is\ncomputationally feasible also for large data sets. The Gaussian process defined\nthrough the stochastic partial differential equation has in general a\nnonseparable covariance structure. Furthermore, its parameters can be\nphysically interpreted as explicitly modeling phenomena such as transport and\ndiffusion that occur in many natural processes in diverse fields ranging from\nenvironmental sciences to ecology. In order to obtain computationally efficient\nstatistical algorithms we use spectral methods to solve the stochastic partial\ndifferential equation. This has the advantage that approximation errors do not\naccumulate over time, and that in the spectral space the computational cost\ngrows linearly with the dimension, the total computational costs of Bayesian or\nfrequentist inference being dominated by the fast Fourier transform. The\nproposed model is applied to postprocessing of precipitation forecasts from a\nnumerical weather prediction model for northern Switzerland. In contrast to the\nraw forecasts from the numerical model, the postprocessed forecasts are\ncalibrated and quantify prediction uncertainty. Moreover, they outperform the\nraw forecasts, in the sense that they have a lower mean absolute error.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2012 07:02:11 GMT"}, {"version": "v2", "created": "Wed, 30 May 2012 07:43:16 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2012 09:31:19 GMT"}, {"version": "v4", "created": "Tue, 20 Nov 2012 08:53:22 GMT"}, {"version": "v5", "created": "Wed, 26 Mar 2014 10:55:37 GMT"}, {"version": "v6", "created": "Thu, 11 Feb 2016 15:46:47 GMT"}], "update_date": "2016-02-18", "authors_parsed": [["Sigrist", "Fabio", ""], ["K\u00fcnsch", "Hans R.", ""], ["Stahel", "Werner A.", ""]]}, {"id": "1204.6160", "submitter": "Jos\\'e Enrique Chac\\'on", "authors": "Jos\\'e E. Chac\\'on and Tarn Duong", "title": "Data-driven density derivative estimation, with applications to\n  nonparametric clustering and bump hunting", "comments": "36 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Important information concerning a multivariate data set, such as clusters\nand modal regions, is contained in the derivatives of the probability density\nfunction. Despite this importance, nonparametric estimation of higher order\nderivatives of the density functions have received only relatively scant\nattention. Kernel estimators of density functions are widely used as they\nexhibit excellent theoretical and practical properties, though their\ngeneralization to density derivatives has progressed more slowly due to the\nmathematical intractabilities encountered in the crucial problem of bandwidth\n(or smoothing parameter) selection. This paper presents the first fully\nautomatic, data-based bandwidth selectors for multivariate kernel density\nderivative estimators. This is achieved by synthesizing recent advances in\nmatrix analytic theory which allow mathematically and computationally tractable\nrepresentations of higher order derivatives of multivariate vector valued\nfunctions. The theoretical asymptotic properties as well as the finite sample\nbehaviour of the proposed selectors are studied. {In addition, we explore in\ndetail the applications of the new data-driven methods for two other\nstatistical problems: clustering and bump hunting. The introduced techniques\nare combined with the mean shift algorithm to develop novel automatic,\nnonparametric clustering procedures which are shown to outperform mixture-model\ncluster analysis and other recent nonparametric approaches in practice.\nFurthermore, the advantage of the use of smoothing parameters designed for\ndensity derivative estimation for feature significance analysis for bump\nhunting is illustrated with a real data example.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2012 10:04:13 GMT"}, {"version": "v2", "created": "Mon, 21 May 2012 18:22:25 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2013 19:18:10 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Chac\u00f3n", "Jos\u00e9 E.", ""], ["Duong", "Tarn", ""]]}, {"id": "1204.6286", "submitter": "Artur Lemonte", "authors": "Artur J. Lemonte, Guillermo Mart\\'inez-Florez, Germ\\'an Moreno-Arenas", "title": "Skewed Multivariate Birnbaum-Saunders Distributions", "comments": "Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The univariate Birnbaum-Saunders distribution has been used quite effectively\nto model times to failure for materials subject to fatigue and for modeling\nlifetime data. In this article, we define a skewed version of the\nBirnbaum-Saunders distribution in the multivariate setting and derive several\nof its properties. The proposed skewed multivariate model is an absolutely\ncontinuous distribution whose marginals are univariate Birnbaum-Saunders\ndistributions. Estimation of the parameters by maximum likelihood is discussed\nand the Fisher's information matrix is determined. A skewed bivariate version\nfor the generalized Birnbaum-Saunders distribution is also introduced. We\nprovide an application to real data which illustrates the usefulness of the\nproposed multivariate model.\n", "versions": [{"version": "v1", "created": "Fri, 27 Apr 2012 18:19:13 GMT"}], "update_date": "2012-04-30", "authors_parsed": [["Lemonte", "Artur J.", ""], ["Mart\u00ednez-Florez", "Guillermo", ""], ["Moreno-Arenas", "Germ\u00e1n", ""]]}, {"id": "1204.6376", "submitter": "Yuan Yao", "authors": "E. Weinan, Jianfeng Lu, Yuan Yao", "title": "The Landscape of Complex Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological landscape is introduced for networks with functions defined on\nthe nodes. By extending the notion of gradient flows to the network setting,\ncritical nodes of different indices are defined. This leads to a concise and\nhierarchical representation of the network. Persistent homology from\ncomputational topology is used to design efficient algorithms for performing\nsuch analysis. Applications to some examples in social and biological networks\nare demonstrated, which show that critical nodes carry important information\nabout structures and dynamics of such networks.\n", "versions": [{"version": "v1", "created": "Sat, 28 Apr 2012 07:11:02 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Weinan", "E.", ""], ["Lu", "Jianfeng", ""], ["Yao", "Yuan", ""]]}, {"id": "1204.6452", "submitter": "Qi Zhang", "authors": "Jiashun Jin, Cun-Hui Zhang, Qi Zhang", "title": "Optimality of Graphlet Screening in High Dimensional Variable Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Consider a linear regression model where the design matrix X has n rows and p\ncolumns. We assume (a) p is much large than n, (b) the coefficient vector beta\nis sparse in the sense that only a small fraction of its coordinates is\nnonzero, and (c) the Gram matrix G = X'X is sparse in the sense that each row\nhas relatively few large coordinates (diagonals of G are normalized to 1).\n  The sparsity in G naturally induces the sparsity of the so-called graph of\nstrong dependence (GOSD). We find an interesting interplay between the signal\nsparsity and the graph sparsity, which ensures that in a broad context, the set\nof true signals decompose into many different small-size components of GOSD,\nwhere different components are disconnected.\n  We propose Graphlet Screening (GS) as a new approach to variable selection,\nwhich is a two-stage Screen and Clean method. The key methodological innovation\nof GS is to use GOSD to guide both the screening and cleaning. Compared to\nm-variate brute-forth screening that has a computational cost of p^m, the GS\nonly has a computational cost of p (up to some multi-log(p) factors) in\nscreening.\n  We measure the performance of any variable selection procedure by the minimax\nHamming distance. We show that in a very broad class of situations, GS achieves\nthe optimal rate of convergence in terms of the Hamming distance. Somewhat\nsurprisingly, the well-known procedures subset selection and the lasso are rate\nnon-optimal, even in very simple settings and even when their tuning parameters\nare ideally set.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2012 03:57:18 GMT"}, {"version": "v2", "created": "Fri, 13 Jun 2014 08:23:49 GMT"}], "update_date": "2014-06-16", "authors_parsed": [["Jin", "Jiashun", ""], ["Zhang", "Cun-Hui", ""], ["Zhang", "Qi", ""]]}, {"id": "1204.6491", "submitter": "Jian Huang", "authors": "Jian Huang, Patrick Breheny, Shuangge Ma", "title": "A Selective Review of Group Selection in High-Dimensional Models", "comments": "Published in at http://dx.doi.org/10.1214/12-STS392 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 4, 481-499", "doi": "10.1214/12-STS392", "report-no": "IMS-STS-STS392", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grouping structures arise naturally in many statistical modeling problems.\nSeveral methods have been proposed for variable selection that respect grouping\nstructure in variables. Examples include the group LASSO and several concave\ngroup selection methods. In this article, we give a selective review of group\nselection concerning methodological developments, theoretical properties and\ncomputational algorithms. We pay particular attention to group selection\nmethods involving concave penalties. We address both group selection and\nbi-level selection methods. We describe several applications of these methods\nin nonparametric additive models, semiparametric regression, seemingly\nunrelated regressions, genomic data analysis and genome wide association\nstudies. We also highlight some issues that require further study.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2012 16:15:48 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2013 14:17:33 GMT"}], "update_date": "2013-01-07", "authors_parsed": [["Huang", "Jian", ""], ["Breheny", "Patrick", ""], ["Ma", "Shuangge", ""]]}, {"id": "1204.6505", "submitter": "Peter Hoff", "authors": "David C. Kessler, Peter D. Hoff and David B. Dunson", "title": "Marginally Specified Priors for Nonparametric Bayesian Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior specification for nonparametric Bayesian inference involves the\ndifficult task of quantifying prior knowledge about a parameter of high, often\ninfinite, dimension. Realistically, a statistician is unlikely to have informed\nopinions about all aspects of such a parameter, but may have real information\nabout functionals of the parameter, such the population mean or variance. This\narticle proposes a new framework for nonparametric Bayes inference in which the\nprior distribution for a possibly infinite-dimensional parameter is decomposed\ninto two parts: an informative prior on a finite set of functionals, and a\nnonparametric conditional prior for the parameter given the functionals. Such\npriors can be easily constructed from standard nonparametric prior\ndistributions in common use, and inherit the large support of the standard\npriors upon which they are based. Additionally, posterior approximations under\nthese informative priors can generally be made via minor adjustments to\nexisting Markov chain approximation algorithms for standard nonparametric prior\ndistributions. We illustrate the use of such priors in the context of\nmultivariate density estimation using Dirichlet process mixture models, and in\nthe modeling of high-dimensional sparse contingency tables.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2012 19:12:26 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Kessler", "David C.", ""], ["Hoff", "Peter D.", ""], ["Dunson", "David B.", ""]]}, {"id": "1204.6516", "submitter": "Maria Eduarda Silva", "authors": "Maria Eduarda Silva, Isabel Pereira", "title": "Detection of additive outliers in Poisson INteger-valued AutoRegressive\n  time series", "comments": "14 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlying observations are commonly encountered in the analysis of time\nseries. In this paper the problem of detecting additive outliers in\ninteger-valued time series is considered. We show how Gibbs sampling can be\nused to detect outlying observations in INAR(1) processes. The methodology\nproposed is illustrated using examples as well as an observed data set.\n", "versions": [{"version": "v1", "created": "Sun, 29 Apr 2012 20:35:21 GMT"}], "update_date": "2012-05-01", "authors_parsed": [["Silva", "Maria Eduarda", ""], ["Pereira", "Isabel", ""]]}, {"id": "1204.6644", "submitter": "Elif Fidan  Acar PhD", "authors": "Elif F. Acar, Radu V. Craiu and Fang Yao", "title": "Statistical Testing for Conditional Copulas", "comments": null, "journal-ref": "The Electronic Journal of Statistics , 7, 2822--2850, 2014", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In conditional copula models, the copula parameter is deterministically\nlinked to a covariate via the calibration function. The latter is of central\ninterest for inference and is usually estimated nonparametrically. However,\nwhen a parametric model for the calibration function is appropriate, the\nresulting estimator exhibits significant gains in statistical efficiency and\nrequires smaller computational costs. We develop methodology for testing a\nparametric formulation of the calibration function against a general\nalternative and propose a generalized likelihood ratio-type test that enables\nconditional copula model diagnostics. We derive the asymptotic null\ndistribution of the proposed test and study its finite sample performance using\nsimulations. The method is applied to two data examples.\n", "versions": [{"version": "v1", "created": "Mon, 30 Apr 2012 14:16:21 GMT"}], "update_date": "2014-03-19", "authors_parsed": [["Acar", "Elif F.", ""], ["Craiu", "Radu V.", ""], ["Yao", "Fang", ""]]}]