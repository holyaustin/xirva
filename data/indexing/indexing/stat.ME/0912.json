[{"id": "0912.0902", "submitter": "Tilmann Gneiting", "authors": "Tilmann Gneiting", "title": "Making and Evaluating Point Forecasts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typically, point forecasting methods are compared and assessed by means of an\nerror measure or scoring function, such as the absolute error or the squared\nerror. The individual scores are then averaged over forecast cases, to result\nin a summary measure of the predictive performance, such as the mean absolute\nerror or the (root) mean squared error. I demonstrate that this common practice\ncan lead to grossly misguided inferences, unless the scoring function and the\nforecasting task are carefully matched.\n  Effective point forecasting requires that the scoring function be specified\nex ante, or that the forecaster receives a directive in the form of a\nstatistical functional, such as the mean or a quantile of the predictive\ndistribution. If the scoring function is specified ex ante, the forecaster can\nissue the optimal point forecast, namely, the Bayes rule. If the forecaster\nreceives a directive in the form of a functional, it is critical that the\nscoring function be consistent for it, in the sense that the expected score is\nminimized when following the directive.\n  A functional is elicitable if there exists a scoring function that is\nstrictly consistent for it. Expectations, ratios of expectations and quantiles\nare elicitable. For example, a scoring function is consistent for the mean\nfunctional if and only if it is a Bregman function. It is consistent for a\nquantile if and only if it is generalized piecewise linear. Similar\ncharacterizations apply to ratios of expectations and to expectiles. Weighted\nscoring functions are consistent for functionals that adapt to the weighting in\npeculiar ways. Not all functionals are elicitable; for instance, conditional\nvalue-at-risk is not, despite its popularity in quantitative finance.\n", "versions": [{"version": "v1", "created": "Fri, 4 Dec 2009 17:56:26 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2010 16:31:27 GMT"}], "update_date": "2010-03-09", "authors_parsed": [["Gneiting", "Tilmann", ""]]}, {"id": "0912.1064", "submitter": "Wolfgang Konen K", "authors": "Wolfgang Konen", "title": "On the numeric stability of the SFA implementation sfa-tk", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Slow feature analysis (SFA) is a method for extracting slowly varying\nfeatures from a quickly varying multidimensional signal. An open source\nMatlab-implementation sfa-tk makes SFA easily useable. We show here that under\ncertain circumstances, namely when the covariance matrix of the nonlinearly\nexpanded data does not have full rank, this implementation runs into numerical\ninstabilities. We propse a modified algorithm based on singular value\ndecomposition (SVD) which is free of those instabilities even in the case where\nthe rank of the matrix is only less than 10% of its size. Furthermore we show\nthat an alternative way of handling the numerical problems is to inject a small\namount of noise into the multidimensional input signal which can restore a\nrank-deficient covariance matrix to full rank, however at the price of\nmodifying the original data and the need for noise parameter tuning.\n", "versions": [{"version": "v1", "created": "Sun, 6 Dec 2009 00:14:28 GMT"}], "update_date": "2009-12-08", "authors_parsed": [["Konen", "Wolfgang", ""]]}, {"id": "0912.1586", "submitter": "Robert B. Gramacy", "authors": "Matthew A. Taddy, Robert B. Gramacy, and Nicholas G. Polson", "title": "Dynamic Trees for Learning and Design", "comments": "37 pages, 8 figures, 3 tables; accepted at JASA", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic regression trees are an attractive option for automatic regression\nand classification with complicated response surfaces in on-line application\nsettings. We create a sequential tree model whose state changes in time with\nthe accumulation of new data, and provide particle learning algorithms that\nallow for the efficient on-line posterior filtering of tree-states. A major\nadvantage of tree regression is that it allows for the use of very simple\nmodels within each partition. The model also facilitates a natural division of\nlabor in our sequential particle-based inference: tree dynamics are defined\nthrough a few potential changes that are local to each newly arrived\nobservation, while global uncertainty is captured by the ensemble of particles.\nWe consider both constant and linear mean functions at the tree leaves, along\nwith multinomial leaves for classification problems, and propose default prior\nspecifications that allow for prediction to be integrated over all model\nparameters conditional on a given tree. Inference is illustrated in some\nstandard nonparametric regression examples, as well as in the setting of\nsequential experiment design, including both active learning and optimization\napplications, and in on-line classification. We detail implementation\nguidelines and problem specific methodology for each of these motivating\napplications. Throughout, it is demonstrated that our practical approach is\nable to provide better results compared to commonly used methods at a fraction\nof the cost.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2009 23:49:08 GMT"}, {"version": "v2", "created": "Fri, 11 Dec 2009 00:04:08 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2010 20:11:53 GMT"}, {"version": "v4", "created": "Sun, 21 Nov 2010 05:31:05 GMT"}], "update_date": "2010-11-23", "authors_parsed": [["Taddy", "Matthew A.", ""], ["Gramacy", "Robert B.", ""], ["Polson", "Nicholas G.", ""]]}, {"id": "0912.1628", "submitter": "Namrata Vaswani", "authors": "Namrata Vaswani", "title": "KF-CS: Compressive Sensing on Kalman Filtered Residual", "comments": "7 pages, 2 figures, submitted to the IEEE for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recursively reconstructing time sequences of\nsparse signals (with unknown and time-varying sparsity patterns) from a limited\nnumber of linear incoherent measurements with additive noise. The idea of our\nproposed solution, KF CS-residual (KF-CS) is to replace compressed sensing (CS)\non the observation by CS on the Kalman filtered (KF) observation residual\ncomputed using the previous estimate of the support. KF-CS error stability over\ntime is studied. Simulation comparisons with CS and LS-CS are shown.\n", "versions": [{"version": "v1", "created": "Tue, 8 Dec 2009 22:40:30 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2009 13:22:18 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2010 03:43:54 GMT"}], "update_date": "2010-03-25", "authors_parsed": [["Vaswani", "Namrata", ""]]}, {"id": "0912.1647", "submitter": "Jieqi Yu", "authors": "Jieqi Yu, Sanjeev R. Kulkarni, H. Vincent Poor", "title": "Robust Fitting of Ellipses and Spheroids", "comments": "in proceeding of 43rd Asilomar Conference on Signals, Systems and\n  Computers, Pacific Grove, California, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ellipse and ellipsoid fitting has been extensively researched and widely\napplied. Although traditional fitting methods provide accurate estimation of\nellipse parameters in the low-noise case, their performance is compromised when\nthe noise level or the ellipse eccentricity are high. A series of robust\nfitting algorithms are proposed that perform well in high-noise,\nhigh-eccentricity ellipse/spheroid (a special class of ellipsoid) cases. The\nnew algorithms are based on the geometric definition of an ellipse/spheroid,\nand improved using global statistical properties of the data. The efficacy of\nthe new algorithms is demonstrated through simulations.\n", "versions": [{"version": "v1", "created": "Wed, 9 Dec 2009 02:01:00 GMT"}], "update_date": "2009-12-10", "authors_parsed": [["Yu", "Jieqi", ""], ["Kulkarni", "Sanjeev R.", ""], ["Poor", "H. Vincent", ""]]}, {"id": "0912.2412", "submitter": "Stefan Haufe", "authors": "Stefan Haufe, Ryota Tomioka, Guido Nolte, Klaus-Robert Mueller and\n  Motoaki Kawanabe", "title": "Modeling sparse connectivity between underlying brain sources for\n  EEG/MEG", "comments": "9 pages, 6 figures", "journal-ref": "IEEE Trans. Biomed. Eng. 57(8) (2010) 1954 - 1963;", "doi": "10.1109/TBME.2010.2046325", "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel technique to assess functional brain connectivity in\nEEG/MEG signals. Our method, called Sparsely-Connected Sources Analysis (SCSA),\ncan overcome the problem of volume conduction by modeling neural data\ninnovatively with the following ingredients: (a) the EEG is assumed to be a\nlinear mixture of correlated sources following a multivariate autoregressive\n(MVAR) model, (b) the demixing is estimated jointly with the source MVAR\nparameters, (c) overfitting is avoided by using the Group Lasso penalty. This\napproach allows to extract the appropriate level cross-talk between the\nextracted sources and in this manner we obtain a sparse data-driven model of\nfunctional connectivity. We demonstrate the usefulness of SCSA with simulated\ndata, and compare to a number of existing algorithms with excellent results.\n", "versions": [{"version": "v1", "created": "Sat, 12 Dec 2009 11:03:23 GMT"}], "update_date": "2010-08-05", "authors_parsed": [["Haufe", "Stefan", ""], ["Tomioka", "Ryota", ""], ["Nolte", "Guido", ""], ["Mueller", "Klaus-Robert", ""], ["Kawanabe", "Motoaki", ""]]}, {"id": "0912.2492", "submitter": "Hannes Nickisch", "authors": "Hannes Nickisch, Pushmeet Kohli and Carsten Rother", "title": "Learning an Interactive Segmentation System", "comments": "11 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CV stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many successful applications of computer vision to image or video\nmanipulation are interactive by nature. However, parameters of such systems are\noften trained neglecting the user. Traditionally, interactive systems have been\ntreated in the same manner as their fully automatic counterparts. Their\nperformance is evaluated by computing the accuracy of their solutions under\nsome fixed set of user interactions. This paper proposes a new evaluation and\nlearning method which brings the user in the loop. It is based on the use of an\nactive robot user - a simulated model of a human user. We show how this\napproach can be used to evaluate and learn parameters of state-of-the-art\ninteractive segmentation systems. We also show how simulated user models can be\nintegrated into the popular max-margin method for parameter learning and\npropose an algorithm to solve the resulting optimisation problem.\n", "versions": [{"version": "v1", "created": "Sun, 13 Dec 2009 12:27:37 GMT"}], "update_date": "2009-12-15", "authors_parsed": [["Nickisch", "Hannes", ""], ["Kohli", "Pushmeet", ""], ["Rother", "Carsten", ""]]}, {"id": "0912.2695", "submitter": "Yang Feng", "authors": "Jianqing Fan, Yang Feng and Rui Song", "title": "Nonparametric Independence Screening in Sparse Ultra-High Dimensional\n  Additive Models", "comments": "48 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A variable screening procedure via correlation learning was proposed Fan and\nLv (2008) to reduce dimensionality in sparse ultra-high dimensional models.\nEven when the true model is linear, the marginal regression can be highly\nnonlinear. To address this issue, we further extend the correlation learning to\nmarginal nonparametric learning. Our nonparametric independence screening is\ncalled NIS, a specific member of the sure independence screening. Several\nclosely related variable screening procedures are proposed. Under the\nnonparametric additive models, it is shown that under some mild technical\nconditions, the proposed independence screening methods enjoy a sure screening\nproperty. The extent to which the dimensionality can be reduced by independence\nscreening is also explicitly quantified. As a methodological extension, an\niterative nonparametric independence screening (INIS) is also proposed to\nenhance the finite sample performance for fitting sparse additive models. The\nsimulation results and a real data analysis demonstrate that the proposed\nprocedure works well with moderate sample size and large dimension and performs\nbetter than competing methods.\n", "versions": [{"version": "v1", "created": "Mon, 14 Dec 2009 17:35:50 GMT"}, {"version": "v2", "created": "Tue, 18 Jan 2011 18:17:27 GMT"}], "update_date": "2011-01-19", "authors_parsed": [["Fan", "Jianqing", ""], ["Feng", "Yang", ""], ["Song", "Rui", ""]]}, {"id": "0912.2873", "submitter": "Pierre Latouche", "authors": "Pierre Latouche, Etienne Birmele, Christophe Ambroise", "title": "Variational Bayesian Inference and Complexity Control for Stochastic\n  Block Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now widely accepted that knowledge can be acquired from networks by\nclustering their vertices according to connection profiles. Many methods have\nbeen proposed and in this paper we concentrate on the Stochastic Block Model\n(SBM). The clustering of vertices and the estimation of SBM model parameters\nhave been subject to previous work and numerous inference strategies such as\nvariational Expectation Maximization (EM) and classification EM have been\nproposed. However, SBM still suffers from a lack of criteria to estimate the\nnumber of components in the mixture. To our knowledge, only one model based\ncriterion, ICL, has been derived for SBM in the literature. It relies on an\nasymptotic approximation of the Integrated Complete-data Likelihood and recent\nstudies have shown that it tends to be too conservative in the case of small\nnetworks. To tackle this issue, we propose a new criterion that we call ILvb,\nbased on a non asymptotic approximation of the marginal likelihood. We describe\nhow the criterion can be computed through a variational Bayes EM algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2009 13:06:23 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2010 15:00:24 GMT"}], "update_date": "2010-07-27", "authors_parsed": [["Latouche", "Pierre", ""], ["Birmele", "Etienne", ""], ["Ambroise", "Christophe", ""]]}, {"id": "0912.2883", "submitter": "Jacques Touboul", "authors": "Jacques Touboul (LSTA)", "title": "Projection Pursuit through $\\Phi$-Divergence Minimisation", "comments": "32 pages, 4 figures, 5 tableaux, elsarticle class", "journal-ref": null, "doi": "10.3390/e12061581", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a defined density on a set of very large dimension. It is quite\ndifficult to find an estimate of this density from a data set. However, it is\npossible through a projection pursuit methodology to solve this problem.\nTouboul's article \"Projection Pursuit Through Relative Entropy Minimization\",\n2009, demonstrates the interest of the author's method in a very simple given\ncase. He considers the factorization of a density through an Elliptical\ncomponent and some residual density. The above Touboul's work is based on\nminimizing relative entropy. In the present article, our proposal will aim at\nextending this very methodology to the $\\Phi-$divergence. Furthermore, we will\nalso consider the case when the density to be factorized is estimated from an\ni.i.d. sample. We will then propose a test for the factorization of the\nestimated density. Applications include a new test of fit pertaining to the\nElliptical copulas.\n", "versions": [{"version": "v1", "created": "Tue, 15 Dec 2009 13:31:42 GMT"}, {"version": "v2", "created": "Mon, 21 Dec 2009 16:59:08 GMT"}, {"version": "v3", "created": "Mon, 31 May 2010 08:45:09 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Touboul", "Jacques", "", "LSTA"]]}, {"id": "0912.3182", "submitter": "Oliver Ratmann", "authors": "Oliver Ratmann, Christophe Andrieu, Carsten Wiuf and Sylvia Richardson", "title": "Notes to Robert et al.: Model criticism informs model choice and model\n  comparison", "comments": "Reply to [arXiv:0909.5673v2]", "journal-ref": null, "doi": "10.1073/pnas.0912887107", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In their letter to PNAS and a comprehensive set of notes on arXiv\n[arXiv:0909.5673v2], Christian Robert, Kerrie Mengersen and Carla Chen (RMC)\nrepresent our approach to model criticism in situations when the likelihood\ncannot be computed as a way to \"contrast several models with each other\". In\naddition, RMC argue that model assessment with Approximate Bayesian Computation\nunder model uncertainty (ABCmu) is unduly challenging and question its Bayesian\nfoundations. We disagree, and clarify that ABCmu is a probabilistically sound\nand powerful too for criticizing a model against aspects of the observed data,\nand discuss further the utility of ABCmu.\n", "versions": [{"version": "v1", "created": "Wed, 16 Dec 2009 16:31:51 GMT"}], "update_date": "2015-05-14", "authors_parsed": [["Ratmann", "Oliver", ""], ["Andrieu", "Christophe", ""], ["Wiuf", "Carsten", ""], ["Richardson", "Sylvia", ""]]}, {"id": "0912.3861", "submitter": "Min Yang", "authors": "Min Yang", "title": "On the de la Garza Phenomenon", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deriving optimal designs for nonlinear models is in general challenging. One\ncrucial step is to determine the number of support points needed. Current tools\nhandle this on a case-by-case basis. Each combination of model, optimality\ncriterion and objective requires its own proof. The celebrated de la Garza\nPhenomenon states that under a (p-1)th-degree polynomial regression model, any\noptimal design can be based on at most p design points, the minimum number of\nsupport points such that all parameters are estimable. Does this conclusion\nalso hold for nonlinear models? If the answer is yes, it would be relatively\neasy to derive any optimal design, analytically or numerically. In this paper,\na novel approach is developed to address this question. Using this new\napproach, it can be easily shown that the de la Garza phenomenon exists for\nmany commonly studied nonlinear models, such as the Emax model, exponential\nmodel, three- and four-parameter log-linear models, Emax-PK1 model, as well as\nmany classical polynomial regression models. The proposed approach unifies and\nextends many well-known results in the optimal design literature. It has four\nadvantages over current tools: (i) it can be applied to many forms of nonlinear\nmodels; to continuous or discrete data; to data with homogeneous or\nnon-homogeneous errors; (ii) it can be applied to any design region; (iii) it\ncan be applied to multiple-stage optimal design; and (iv) it can be easily\nimplemented.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2009 05:15:42 GMT"}], "update_date": "2009-12-22", "authors_parsed": [["Yang", "Min", ""]]}, {"id": "0912.3878", "submitter": "Michael Wood", "authors": "Michael Wood", "title": "P values, confidence intervals, or confidence levels for hypotheses?", "comments": "The essential argument is unchanged from previous versions, but the\n  paper has been largely rewritten, the argument extended, and more examples\n  and background context included. 21 pages, 3 diagrams, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Null hypothesis significance tests and p values are widely used despite very\nstrong arguments against their use in many contexts. Confidence intervals are\noften recommended as an alternative, but these do not achieve the objective of\nassessing the credibility of a hypothesis, and the distinction between\nconfidence and probability is an unnecessary confusion. This paper proposes a\nmore straightforward (probabilistic) definition of confidence, and suggests how\nthe idea can be applied to whatever hypotheses are of interest to researchers.\nThe relative merits of the different approaches are discussed using a series of\nillustrative examples: usually confidence based approaches seem more\ntransparent and useful, but there are some contexts in which p values may be\nappropriate. I also suggest some methods for converting results from one format\nto another. (The attractiveness of the idea of confidence is demonstrated by\nthe widespread persistence of the completely incorrect idea that p=5% is\nequivalent to 95% confidence in the alternative hypothesis. In this paper I\nshow how p values can be used to derive meaningful confidence statements, and\nthe assumptions underlying the derivation.) Key words: Confidence interval,\nConfidence level, Hypothesis testing, Null hypothesis significance tests, P\nvalue, User friendliness.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2009 09:33:09 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2012 10:36:51 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2012 08:44:45 GMT"}, {"version": "v4", "created": "Fri, 6 Jul 2012 12:39:50 GMT"}, {"version": "v5", "created": "Tue, 11 Feb 2014 11:01:10 GMT"}], "update_date": "2014-02-12", "authors_parsed": [["Wood", "Michael", ""]]}, {"id": "0912.3880", "submitter": "Michael Wood", "authors": "Michael Wood", "title": "Bootstrapping Confidence Levels for Hypotheses about Quadratic\n  (U-Shaped) Regression Models", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bootstrapping can produce confidence levels for hypotheses about quadratic\nregression models - such as whether the U-shape is inverted, and the location\nof optima. The method has several advantages over conventional methods: it\nprovides more, and clearer, information, and is flexible - it could easily be\napplied to a wide variety of different types of models. The utility of the\nmethod can be enhanced by formulating models with interpretable coefficients,\nsuch as the location and value of the optimum. Keywords: Bootstrap resampling;\nConfidence level; Quadratic model; Regression, U-shape.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2009 09:48:17 GMT"}, {"version": "v2", "created": "Sat, 18 Feb 2012 20:36:37 GMT"}, {"version": "v3", "created": "Thu, 15 Mar 2012 08:38:44 GMT"}, {"version": "v4", "created": "Fri, 6 Jul 2012 12:35:12 GMT"}], "update_date": "2012-07-09", "authors_parsed": [["Wood", "Michael", ""]]}, {"id": "0912.3891", "submitter": "Herv\\'e Cardot", "authors": "Herv\\'e Cardot, Etienne Josserand", "title": "Horvitz-Thompson estimators for functional data: asymptotic confidence\n  bands and optimal allocation for stratified sampling", "comments": "Accepted for publication in Biometrika", "journal-ref": "Biometrika, 2011, vol. 98, pages 107-118", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When dealing with very large datasets of functional data, survey sampling\napproaches are useful in order to obtain estimators of simple functional\nquantities, without being obliged to store all the data. We propose here a\nHorvitz--Thompson estimator of the mean trajectory. In the context of a\nsuperpopulation framework, we prove under mild regularity conditions that we\nobtain uniformly consistent estimators of the mean function and of its variance\nfunction. With additional assumptions on the sampling design we state a\nfunctional Central Limit Theorem and deduce asymptotic confidence bands.\nStratified sampling is studied in detail, and we also obtain a functional\nversion of the usual optimal allocation rule considering a mean variance\ncriterion. These techniques are illustrated by means of a test population of\nN=18902 electricity meters for which we have individual electricity consumption\nmeasures every 30 minutes over one week. We show that stratification can\nsubstantially improve both the accuracy of the estimators and reduce the width\nof the global confidence bands compared to simple random sampling without\nreplacement.\n", "versions": [{"version": "v1", "created": "Sat, 19 Dec 2009 12:44:47 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2010 15:54:41 GMT"}, {"version": "v3", "created": "Mon, 7 Jun 2010 09:53:37 GMT"}, {"version": "v4", "created": "Mon, 20 Sep 2010 08:57:18 GMT"}, {"version": "v5", "created": "Wed, 29 Sep 2010 15:02:01 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Cardot", "Herv\u00e9", ""], ["Josserand", "Etienne", ""]]}, {"id": "0912.4169", "submitter": "Matthias Mielke", "authors": "M. Mielke, A. Munk", "title": "The assessment and planning of non-inferiority trials for retention of\n  effect hypotheses - towards a general approach", "comments": "23 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of this paper is to develop statistical methodology for\nplanning and evaluating three-armed non-inferiority trials for general\nretention of effect hypotheses, where the endpoint of interest may follow any\n(regular) parametric distribution family. This generalizes and unifies specific\nresults for binary, normally and exponentially distributed endpoints. We\npropose a Wald-type test procedure for the retention of effect hypothesis\n(RET), which assures that the test treatment maintains at least a proportion\n$\\Delta$ of reference treatment effect compared to placebo. At this, we\ndistinguish the cases where the variance of the test statistic is estimated\nunrestrictedly and restrictedly to the null hypothesis, to improve accuracy of\nthe nominal level. We present a general valid sample size allocation rule to\nachieve optimal power and sample size formulas, which significantly improve\nexisting ones. Moreover, we propose a general applicable rule of thumb for\nsample allocation and give conditions where this rule is theoretically\njustified. The presented methodologies are discussed in detail for binary and\nfor Poisson distributed endpoints by means of two clinical trials in the\ntreatment of depression and in the treatment of epilepsy, respectively.\n$R$-software for implementation of the proposed tests and for sample size\nplanning accompanies this paper.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2009 14:12:07 GMT"}], "update_date": "2009-12-22", "authors_parsed": [["Mielke", "M.", ""], ["Munk", "A.", ""]]}, {"id": "0912.4269", "submitter": "Glenn Shafer", "authors": "Glenn Shafer, Alexander Shen, Nikolai Vereshchagin, Vladimir Vovk", "title": "Test Martingales, Bayes Factors and $p$-Values", "comments": "Published in at http://dx.doi.org/10.1214/10-STS347 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 1, 84-101", "doi": "10.1214/10-STS347", "report-no": "IMS-STS-STS347", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A nonnegative martingale with initial value equal to one measures evidence\nagainst a probabilistic hypothesis. The inverse of its value at some stopping\ntime can be interpreted as a Bayes factor. If we exaggerate the evidence by\nconsidering the largest value attained so far by such a martingale, the\nexaggeration will be limited, and there are systematic ways to eliminate it.\nThe inverse of the exaggerated value at some stopping time can be interpreted\nas a $p$-value. We give a simple characterization of all increasing functions\nthat eliminate the exaggeration.\n", "versions": [{"version": "v1", "created": "Mon, 21 Dec 2009 21:19:17 GMT"}, {"version": "v2", "created": "Tue, 11 May 2010 11:18:18 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2011 06:55:42 GMT"}], "update_date": "2011-06-17", "authors_parsed": [["Shafer", "Glenn", ""], ["Shen", "Alexander", ""], ["Vereshchagin", "Nikolai", ""], ["Vovk", "Vladimir", ""]]}, {"id": "0912.4434", "submitter": "Julien  Chiquet Dr.", "authors": "Julien Chiquet, Yves Grandvalet, Christophe Ambroise", "title": "Inferring Multiple Graphical Structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian Graphical Models provide a convenient framework for representing\ndependencies between variables. Recently, this tool has received a high\ninterest for the discovery of biological networks. The literature focuses on\nthe case where a single network is inferred from a set of measurements, but, as\nwetlab data is typically scarce, several assays, where the experimental\nconditions affect interactions, are usually merged to infer a single network.\nIn this paper, we propose two approaches for estimating multiple related\ngraphs, by rendering the closeness assumption into an empirical prior or group\npenalties. We provide quantitative results demonstrating the benefits of the\nproposed approaches. The methods presented in this paper are embeded in the R\npackage 'simone' from version 1.0-0 and later.\n", "versions": [{"version": "v1", "created": "Tue, 22 Dec 2009 16:05:09 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2010 08:08:56 GMT"}, {"version": "v3", "created": "Wed, 12 May 2010 13:18:21 GMT"}], "update_date": "2010-05-13", "authors_parsed": [["Chiquet", "Julien", ""], ["Grandvalet", "Yves", ""], ["Ambroise", "Christophe", ""]]}, {"id": "0912.4554", "submitter": "Georg M. Goerg", "authors": "Georg M. Goerg", "title": "Lambert W random variables - a new family of generalized skewed\n  distributions with applications to risk estimation", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS457 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 3, 2197-2230", "doi": "10.1214/11-AOAS457", "report-no": "IMS-AOAS-AOAS457", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Originating from a system theory and an input/output point of view, I\nintroduce a new class of generalized distributions. A parametric nonlinear\ntransformation converts a random variable $X$ into a so-called Lambert $W$\nrandom variable $Y$, which allows a very flexible approach to model skewed\ndata. Its shape depends on the shape of $X$ and a skewness parameter $\\gamma$.\nIn particular, for symmetric $X$ and nonzero $\\gamma$ the output $Y$ is skewed.\nIts distribution and density function are particular variants of their input\ncounterparts. Maximum likelihood and method of moments estimators are\npresented, and simulations show that in the symmetric case additional\nestimation of $\\gamma$ does not affect the quality of other parameter\nestimates. Applications in finance and biomedicine show the relevance of this\nclass of distributions, which is particularly useful for slightly skewed data.\nA practical by-result of the Lambert $W$ framework: data can be \"unskewed.\" The\n$R$ package http://cran.r-project.org/web/packages/LambertWLambertW developed\nby the author is publicly available (http://cran.r-project.orgCRAN).\n", "versions": [{"version": "v1", "created": "Wed, 23 Dec 2009 02:44:44 GMT"}, {"version": "v2", "created": "Fri, 15 Jan 2010 23:39:51 GMT"}, {"version": "v3", "created": "Fri, 20 Aug 2010 15:58:20 GMT"}, {"version": "v4", "created": "Wed, 12 Jan 2011 01:44:09 GMT"}, {"version": "v5", "created": "Fri, 25 Nov 2011 07:20:57 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Goerg", "Georg M.", ""]]}, {"id": "0912.5013", "submitter": "Ivan Fernandez-Val", "authors": "Victor Chernozhukov, Ivan Fernandez-Val", "title": "Inference for Extremal Conditional Quantile Models, with an Application\n  to Market and Birthweight Risks", "comments": "41 pages, 9 figures", "journal-ref": "Review of Economic Studies (2011) 78 (2): 559-589", "doi": "10.1093/restud/rdq020", "report-no": null, "categories": "stat.ME econ.EM math.ST q-fin.RM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression is an increasingly important empirical tool in economics\nand other sciences for analyzing the impact of a set of regressors on the\nconditional distribution of an outcome. Extremal quantile regression, or\nquantile regression applied to the tails, is of interest in many economic and\nfinancial applications, such as conditional value-at-risk, production\nefficiency, and adjustment bands in (S,s) models. In this paper we provide\nfeasible inference tools for extremal conditional quantile models that rely\nupon extreme value approximations to the distribution of self-normalized\nquantile regression statistics. The methods are simple to implement and can be\nof independent interest even in the non-regression case. We illustrate the\nresults with two empirical examples analyzing extreme fluctuations of a stock\nreturn and extremely low percentiles of live infants' birthweights in the range\nbetween 250 and 1500 grams.\n", "versions": [{"version": "v1", "created": "Sat, 26 Dec 2009 12:57:29 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Chernozhukov", "Victor", ""], ["Fernandez-Val", "Ivan", ""]]}, {"id": "0912.5193", "submitter": "Ricardo Silva", "authors": "Ricardo Silva, Katherine Heller, Zoubin Ghahramani, Edoardo M. Airoldi", "title": "Ranking relations using analogies in biological and information networks", "comments": "Published in at http://dx.doi.org/10.1214/09-AOAS321 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2010, Vol. 4, No. 2, 615-644", "doi": "10.1214/09-AOAS321", "report-no": "IMS-AOAS-AOAS321", "categories": "stat.ME cs.LG physics.soc-ph q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analogical reasoning depends fundamentally on the ability to learn and\ngeneralize about relations between objects. We develop an approach to\nrelational learning which, given a set of pairs of objects\n$\\mathbf{S}=\\{A^{(1)}:B^{(1)},A^{(2)}:B^{(2)},\\ldots,A^{(N)}:B ^{(N)}\\}$,\nmeasures how well other pairs A:B fit in with the set $\\mathbf{S}$. Our work\naddresses the following question: is the relation between objects A and B\nanalogous to those relations found in $\\mathbf{S}$? Such questions are\nparticularly relevant in information retrieval, where an investigator might\nwant to search for analogous pairs of objects that match the query set of\ninterest. There are many ways in which objects can be related, making the task\nof measuring analogies very challenging. Our approach combines a similarity\nmeasure on function spaces with Bayesian analysis to produce a ranking. It\nrequires data containing features of the objects of interest and a link matrix\nspecifying which relationships exist; no further attributes of such\nrelationships are necessary. We illustrate the potential of our method on text\nanalysis and information networks. An application on discovering functional\ninteractions between pairs of proteins is discussed in detail, where we show\nthat our approach can work in practice even if a small set of protein pairs is\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2009 17:56:50 GMT"}, {"version": "v2", "created": "Mon, 8 Nov 2010 11:52:09 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2013 06:50:07 GMT"}], "update_date": "2013-08-30", "authors_parsed": [["Silva", "Ricardo", ""], ["Heller", "Katherine", ""], ["Ghahramani", "Zoubin", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "0912.5200", "submitter": "Jelena Bradic", "authors": "Jelena Bradic, Jianqing Fan, Weiwei Wang", "title": "Penalized Composite Quasi-Likelihood for Ultrahigh-Dimensional Variable\n  Selection", "comments": null, "journal-ref": "Journal of Royal Statistical Society: Series B (2011), 73(3), p.\n  325-349", "doi": "10.1111/j.1467-9868.2010.00764.x", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In high-dimensional model selection problems, penalized simple least-square\napproaches have been extensively used. This paper addresses the question of\nboth robustness and efficiency of penalized model selection methods, and\nproposes a data-driven weighted linear combination of convex loss functions,\ntogether with weighted $L_1$-penalty. It is completely data-adaptive and does\nnot require prior knowledge of the error distribution. The weighted\n$L_1$-penalty is used both to ensure the convexity of the penalty term and to\nameliorate the bias caused by the $L_1$-penalty. In the setting with\ndimensionality much larger than the sample size, we establish a strong oracle\nproperty of the proposed method that possesses both the model selection\nconsistency and estimation efficiency for the true non-zero coefficients. As\nspecific examples, we introduce a robust method of composite L1-L2, and optimal\ncomposite quantile method and evaluate their performance in both simulated and\nreal data examples.\n", "versions": [{"version": "v1", "created": "Mon, 28 Dec 2009 18:27:09 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2010 15:23:56 GMT"}], "update_date": "2011-07-06", "authors_parsed": [["Bradic", "Jelena", ""], ["Fan", "Jianqing", ""], ["Wang", "Weiwei", ""]]}, {"id": "0912.5303", "submitter": "Adelchi Azzalini", "authors": "Adelchi Azzalini", "title": "Selection models under generalized symmetry settings", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An active stream of literature has followed up the idea of skew-elliptical\ndensities initiated by Azzalini and Capitanio (1999). Their original\nformulation was based on a general lemma which is however of broader\napplicability than usually perceived. This note examines new directions of its\nuse, and illustrates them with the construction of some probability\ndistributions falling outside the family of the so-called skew-symmetric\ndensities.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2009 13:33:32 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2010 15:07:06 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2010 10:09:52 GMT"}], "update_date": "2010-04-06", "authors_parsed": [["Azzalini", "Adelchi", ""]]}, {"id": "0912.5410", "submitter": "Edoardo Airoldi", "authors": "Anna Goldenberg, Alice X Zheng, Stephen E Fienberg, Edoardo M Airoldi", "title": "A survey of statistical network models", "comments": "96 pages, 14 figures, 333 references", "journal-ref": "Foundations and Trends in Machine Learning, 2(2):1-117, 2009", "doi": null, "report-no": null, "categories": "stat.ME cs.LG physics.soc-ph q-bio.MN stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are ubiquitous in science and have become a focal point for\ndiscussion in everyday life. Formal statistical models for the analysis of\nnetwork data have emerged as a major topic of interest in diverse areas of\nstudy, and most of these involve a form of graphical representation.\nProbability models on graphs date back to 1959. Along with empirical studies in\nsocial psychology and sociology from the 1960s, these early works generated an\nactive network community and a substantial literature in the 1970s. This effort\nmoved into the statistical literature in the late 1970s and 1980s, and the past\ndecade has seen a burgeoning network literature in statistical physics and\ncomputer science. The growth of the World Wide Web and the emergence of online\nnetworking communities such as Facebook, MySpace, and LinkedIn, and a host of\nmore specialized professional network communities has intensified interest in\nthe study of networks and network data. Our goal in this review is to provide\nthe reader with an entry point to this burgeoning literature. We begin with an\noverview of the historical development of statistical network modeling and then\nwe introduce a number of examples that have been studied in the network\nliterature. Our subsequent discussion focuses on a number of prominent static\nand dynamic network models and their interconnections. We emphasize formal\nmodel descriptions, and pay special attention to the interpretation of\nparameters and their estimation. We end with a description of some open\nproblems and challenges for machine learning and statistics.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2009 17:53:13 GMT"}], "update_date": "2009-12-31", "authors_parsed": [["Goldenberg", "Anna", ""], ["Zheng", "Alice X", ""], ["Fienberg", "Stephen E", ""], ["Airoldi", "Edoardo M", ""]]}, {"id": "0912.5467", "submitter": "Guillaume Sagnol", "authors": "Guillaume Sagnol", "title": "Computing Optimal Designs of multiresponse Experiments reduces to\n  Second-Order Cone Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Elfving's Theorem is a major result in the theory of optimal experimental\ndesign, which gives a geometrical characterization of $c-$optimality. In this\npaper, we extend this theorem to the case of multiresponse experiments, and we\nshow that when the number of experiments is finite, $c-,A-,T-$ and $D-$optimal\ndesign of multiresponse experiments can be computed by Second-Order Cone\nProgramming (SOCP). Moreover, our SOCP approach can deal with design problems\nin which the variable is subject to several linear constraints.\n  We give two proofs of this generalization of Elfving's theorem. One is based\non Lagrangian dualization techniques and relies on the fact that the\nsemidefinite programming (SDP) formulation of the multiresponse $c-$optimal\ndesign always has a solution which is a matrix of rank $1$. Therefore, the\ncomplexity of this problem fades.\n  We also investigate a \\emph{model robust} generalization of $c-$optimality,\nfor which an Elfving-type theorem was established by Dette (1993). We show with\nthe same Lagrangian approach that these model robust designs can be computed\nefficiently by minimizing a geometric mean under some norm constraints.\nMoreover, we show that the optimality conditions of this geometric programming\nproblem yield an extension of Dette's theorem to the case of multiresponse\nexperiments.\n  When the number of unknown parameters is small, or when the number of linear\nfunctions of the parameters to be estimated is small, we show by numerical\nexamples that our approach can be between 10 and 1000 times faster than the\nclassic, state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2009 15:38:58 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2010 01:25:47 GMT"}, {"version": "v3", "created": "Thu, 25 Nov 2010 14:00:27 GMT"}], "update_date": "2010-11-29", "authors_parsed": [["Sagnol", "Guillaume", ""]]}, {"id": "0912.5489", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni, Robert L. Winkler", "title": "On multivariate quantiles under partial orders", "comments": "Published in at http://dx.doi.org/10.1214/10-AOS863 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 2, 1125-1179", "doi": "10.1214/10-AOS863", "report-no": "IMS-AOS-AOS863", "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on generalizing quantiles from the ordering point of view.\nWe propose the concept of partial quantiles, which are based on a given partial\norder. We establish that partial quantiles are equivariant under\norder-preserving transformations of the data, robust to outliers, characterize\nthe probability distribution if the partial order is sufficiently rich,\ngeneralize the concept of efficient frontier, and can measure dispersion from\nthe partial order perspective. We also study several statistical aspects of\npartial quantiles. We provide estimators, associated rates of convergence, and\nasymptotic distributions that hold uniformly over a continuum of quantile\nindices. Furthermore, we provide procedures that can restore monotonicity\nproperties that might have been disturbed by estimation error, establish\ncomputational complexity bounds, and point out a concentration of measure\nphenomenon (the latter under independence and the componentwise natural order).\nFinally, we illustrate the concepts by discussing several theoretical examples\nand simulations. Empirical applications to compare intake nutrients within\ndiets, to evaluate the performance of investment funds, and to study the impact\nof policies on tobacco awareness are also presented to illustrate the concepts\nand their use.\n", "versions": [{"version": "v1", "created": "Tue, 29 Dec 2009 22:18:12 GMT"}, {"version": "v2", "created": "Wed, 27 Oct 2010 23:17:37 GMT"}, {"version": "v3", "created": "Mon, 30 May 2011 11:52:55 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Belloni", "Alexandre", ""], ["Winkler", "Robert L.", ""]]}, {"id": "0912.5507", "submitter": "Jun Zhu", "authors": "Jun Zhu, Amr Ahmed, Eric P. Xing", "title": "MedLDA: A General Framework of Maximum Margin Supervised Topic Models", "comments": "27 Pages", "journal-ref": "Journal of Machine Learning Research, 13(Aug): 2237--2278, 2012", "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supervised topic models utilize document's side information for discovering\npredictive low dimensional representations of documents. Existing models apply\nthe likelihood-based estimation. In this paper, we present a general framework\nof max-margin supervised topic models for both continuous and categorical\nresponse variables. Our approach, the maximum entropy discrimination latent\nDirichlet allocation (MedLDA), utilizes the max-margin principle to train\nsupervised topic models and estimate predictive topic representations that are\narguably more suitable for prediction tasks. The general principle of MedLDA\ncan be applied to perform joint max-margin learning and maximum likelihood\nestimation for arbitrary topic models, directed or undirected, and supervised\nor unsupervised, when the supervised side information is available. We develop\nefficient variational methods for posterior inference and parameter estimation,\nand demonstrate qualitatively and quantitatively the advantages of MedLDA over\nlikelihood-based topic models on movie review and 20 Newsgroups data sets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Dec 2009 18:32:21 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Zhu", "Jun", ""], ["Ahmed", "Amr", ""], ["Xing", "Eric P.", ""]]}]