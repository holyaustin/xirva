[{"id": "1110.0169", "submitter": "Gleb Beliakov", "authors": "Gleb Beliakov, Andrei Kelarev, John Yearwood", "title": "Robust artificial neural networks and outlier detection. Technical\n  report", "comments": null, "journal-ref": null, "doi": "10.1080/02331934.2012.674946", "report-no": null, "categories": "math.OC cs.CV cs.NA cs.NE math.NA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large outliers break down linear and nonlinear regression models. Robust\nregression methods allow one to filter out the outliers when building a model.\nBy replacing the traditional least squares criterion with the least trimmed\nsquares criterion, in which half of data is treated as potential outliers, one\ncan fit accurate regression models to strongly contaminated data.\nHigh-breakdown methods have become very well established in linear regression,\nbut have started being applied for non-linear regression only recently. In this\nwork, we examine the problem of fitting artificial neural networks to\ncontaminated data using least trimmed squares criterion. We introduce a\npenalized least trimmed squares criterion which prevents unnecessary removal of\nvalid data. Training of ANNs leads to a challenging non-smooth global\noptimization problem. We compare the efficiency of several derivative-free\noptimization methods in solving it, and show that our approach identifies the\noutliers correctly when ANNs are used for nonlinear regression.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2011 10:56:07 GMT"}], "update_date": "2012-06-07", "authors_parsed": [["Beliakov", "Gleb", ""], ["Kelarev", "Andrei", ""], ["Yearwood", "John", ""]]}, {"id": "1110.0219", "submitter": "Robert B. Gramacy", "authors": "Yuao Hua, Robert B. Gramacy and Heng Lian", "title": "Bayesian Quantile Regression for Single-Index Models", "comments": "26 pages, 8 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using an asymmetric Laplace distribution, which provides a mechanism for\nBayesian inference of quantile regression models, we develop a fully Bayesian\napproach to fitting single-index models in conditional quantile regression. In\nthis work, we use a Gaussian process prior for the unknown nonparametric link\nfunction and a Laplace distribution on the index vector, with the latter\nmotivated by the recent popularity of the Bayesian lasso idea. We design a\nMarkov chain Monte Carlo algorithm for posterior inference. Careful\nconsideration of the singularity of the kernel matrix, and tractability of some\nof the full conditional distributions leads to a partially collapsed approach\nwhere the nonparametric link function is integrated out in some of the sampling\nsteps. Our simulations demonstrate the superior performance of the Bayesian\nmethod versus the frequentist approach. The method is further illustrated by an\napplication to the hurricane data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Oct 2011 19:40:38 GMT"}, {"version": "v2", "created": "Fri, 30 Dec 2011 02:24:12 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Hua", "Yuao", ""], ["Gramacy", "Robert B.", ""], ["Lian", "Heng", ""]]}, {"id": "1110.0721", "submitter": "Tomonari Sei", "authors": "Tomonari Sei, Hiroki Shibata, Akimichi Takemura, Katsuyoshi Ohara,\n  Nobuki Takayama", "title": "Properties and applications of Fisher distribution on the rotation group", "comments": "25 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study properties of Fisher distribution (von Mises-Fisher distribution,\nmatrix Langevin distribution) on the rotation group SO(3). In particular we\napply the holonomic gradient descent, introduced by Nakayama et al. (2011), and\na method of series expansion for evaluating the normalizing constant of the\ndistribution and for computing the maximum likelihood estimate. The rotation\ngroup can be identified with the Stiefel manifold of two orthonormal vectors.\nTherefore from the viewpoint of statistical modeling, it is of interest to\ncompare Fisher distributions on these manifolds. We illustrate the difference\nwith an example of near-earth objects data.\n", "versions": [{"version": "v1", "created": "Tue, 4 Oct 2011 15:21:36 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2013 18:27:50 GMT"}], "update_date": "2013-02-05", "authors_parsed": [["Sei", "Tomonari", ""], ["Shibata", "Hiroki", ""], ["Takemura", "Akimichi", ""], ["Ohara", "Katsuyoshi", ""], ["Takayama", "Nobuki", ""]]}, {"id": "1110.0902", "submitter": "Georgios Fellouris Dr.", "authors": "Georgios Fellouris and Alexander G. Tartakovsky", "title": "Nearly Minimax One-Sided Mixture-Based Sequential Tests", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on one-sided, mixture-based stopping rules for the problem of\nsequential testing a simple null hypothesis against a composite alternative.\nFor the latter, we consider two cases---either a discrete alternative or a\ncontinuous alternative that can be embedded into an exponential family. For\neach case, we find a mixture-based stopping rule that is nearly minimax in the\nsense of minimizing the maximal Kullback-Leibler information. The proof of this\nresult is based on finding an almost Bayes rule for an appropriate sequential\ndecision problem and on high-order asymptotic approximations for the\nperformance characteristics of arbitrary mixture-based stopping times. We also\nevaluate the asymptotic performance loss of certain intuitive mixture rules and\nverify the accuracy of our asymptotic approximations with simulation\nexperiments.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 06:14:13 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2012 07:31:13 GMT"}], "update_date": "2012-04-25", "authors_parsed": [["Fellouris", "Georgios", ""], ["Tartakovsky", "Alexander G.", ""]]}, {"id": "1110.1012", "submitter": "Sylvain Sardy", "authors": "Sylvain Sardy", "title": "Smooth blockwise iterative thresholding: a smooth fixed point estimator\n  based on the likelihood's block gradient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proposed smooth blockwise iterative thresholding estimator (SBITE) is a\nmodel selection technique defined as a fixed point reached by iterating a\nlikelihood gradient-based thresholding function. The smooth James-Stein\nthresholding function has two regularization parameters $\\lambda$ and $\\nu$,\nand a smoothness parameter $s$. It enjoys smoothness like ridge regression and\nselects variables like lasso. Focusing on Gaussian regression, we show that\nSBITE is uniquely defined, and that its Stein unbiased risk estimate is a\nsmooth function of $\\lambda$ and $\\nu$, for better selection of the two\nregularization parameters. We perform a Monte-Carlo simulation to investigate\nthe predictive and oracle properties of this smooth version of adaptive lasso.\n  The motivation is a gravitational wave burst detection problem from several\nconcomitant time series. A nonparametric wavelet-based estimator is developed\nto combine information from all captors by block-thresholding multiresolution\ncoefficients. We study how the smoothness parameter $s$ tempers the erraticity\nof the risk estimate, and derive a universal threshold, an information\ncriterion and an oracle inequality in this canonical setting.\n", "versions": [{"version": "v1", "created": "Wed, 5 Oct 2011 14:59:55 GMT"}], "update_date": "2011-10-06", "authors_parsed": [["Sardy", "Sylvain", ""]]}, {"id": "1110.1462", "submitter": "Antonio Irpino PhD", "authors": "Antonio Irpino and Rosanna Verde and Francisco de AT De Carvalho", "title": "Dynamic Clustering of Histogram Data Based on Adaptive Squared\n  Wasserstein Distances", "comments": null, "journal-ref": "Expert Systems with Applications, vol. 41, p. 3351-3366, 2014", "doi": "10.1016/j.eswa.2013.12.001", "report-no": null, "categories": "math.ST cs.DS math.PR stat.ME stat.OT stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with clustering methods based on adaptive distances for\nhistogram data using a dynamic clustering algorithm. Histogram data describes\nindividuals in terms of empirical distributions. These kind of data can be\nconsidered as complex descriptions of phenomena observed on complex objects:\nimages, groups of individuals, spatial or temporal variant data, results of\nqueries, environmental data, and so on. The Wasserstein distance is used to\ncompare two histograms. The Wasserstein distance between histograms is\nconstituted by two components: the first based on the means, and the second, to\ninternal dispersions (standard deviation, skewness, kurtosis, and so on) of the\nhistograms. To cluster sets of histogram data, we propose to use Dynamic\nClustering Algorithm, (based on adaptive squared Wasserstein distances) that is\na k-means-like algorithm for clustering a set of individuals into $K$ classes\nthat are apriori fixed.\n  The main aim of this research is to provide a tool for clustering histograms,\nemphasizing the different contributions of the histogram variables, and their\ncomponents, to the definition of the clusters. We demonstrate that this can be\nachieved using adaptive distances. Two kind of adaptive distances are\nconsidered: the first takes into account the variability of each component of\neach descriptor for the whole set of individuals; the second takes into account\nthe variability of each component of each descriptor in each cluster. We\nfurnish interpretative tools of the obtained partition based on an extension of\nthe classical measures (indexes) to the use of adaptive distances in the\nclustering criterion function. Applications on synthetic and real-world data\ncorroborate the proposed procedure.\n", "versions": [{"version": "v1", "created": "Fri, 7 Oct 2011 09:11:35 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Irpino", "Antonio", ""], ["Verde", "Rosanna", ""], ["De Carvalho", "Francisco de AT", ""]]}, {"id": "1110.1986", "submitter": "Nanny Wermuth", "authors": "Nanny Wermuth", "title": "Sequences of regressions and their dependences", "comments": "25 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we define and study the concept of traceable regressions.\nThese are sequences of regressions in joint or single responses for which a\ncorresponding regression graph captures not only an independence structure but\nrepresents, in addition, conditional dependences that permit the tracing of\npathways of dependence. We give the properties needed for transforming these\ngraphs and graphical criteria to decide whether a path in the graph induces a\ndependence. The much stronger constraints on distributions that are faithful to\na graph are compared to those needed for traceable regressions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 10:15:52 GMT"}, {"version": "v2", "created": "Tue, 8 May 2012 18:46:00 GMT"}], "update_date": "2012-05-09", "authors_parsed": [["Wermuth", "Nanny", ""]]}, {"id": "1110.2058", "submitter": "Eduardo Mendes", "authors": "Eduardo F. Mendes and Wenxin Jiang", "title": "Convergence Rates for Mixture-of-Experts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In mixtures-of-experts (ME) model, where a number of submodels (experts) are\ncombined, there have been two longstanding problems: (i) how many experts\nshould be chosen, given the size of the training data? (ii) given the total\nnumber of parameters, is it better to use a few very complex experts, or is it\nbetter to combine many simple experts? In this paper, we try to provide some\ninsights to these problems through a theoretic study on a ME structure where\n$m$ experts are mixed, with each expert being related to a polynomial\nregression model of order $k$. We study the convergence rate of the maximum\nlikelihood estimator (MLE), in terms of how fast the Kullback-Leibler\ndivergence of the estimated density converges to the true density, when the\nsample size $n$ increases. The convergence rate is found to be dependent on\nboth $m$ and $k$, and certain choices of $m$ and $k$ are found to produce\noptimal convergence rates. Therefore, these results shed light on the two\naforementioned important problems: on how to choose $m$, and on how $m$ and $k$\nshould be compromised, for achieving good convergence rates.\n", "versions": [{"version": "v1", "created": "Mon, 10 Oct 2011 14:43:02 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2011 11:39:30 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Mendes", "Eduardo F.", ""], ["Jiang", "Wenxin", ""]]}, {"id": "1110.2295", "submitter": "Antonio Irpino PhD", "authors": "Antonio Irpino and Rosanna Verde", "title": "Basic statistics for probabilistic symbolic variables: a novel\n  metric-based approach", "comments": "19 pages, 3 figures", "journal-ref": null, "doi": "10.1007/s11634-014-0176-4", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In data mining, it is usually to describe a set of individuals using some\nsummaries (means, standard deviations, histograms, confidence intervals) that\ngeneralize individual descriptions into a typology description. In this case,\ndata can be described by several values. In this paper, we propose an approach\nfor computing basic statics for such data, and, in particular, for data\ndescribed by numerical multi-valued variables (interval, histograms, discrete\nmulti-valued descriptions). We propose to treat all numerical multi-valued\nvariables as distributional data, i.e. as individuals described by\ndistributions. To obtain new basic statistics for measuring the variability and\nthe association between such variables, we extend the classic measure of\ninertia, calculated with the Euclidean distance, using the squared Wasserstein\ndistance defined between probability measures. The distance is a generalization\nof the Wasserstein distance, that is a distance between quantile functions of\ntwo distributions. Some properties of such a distance are shown. Among them, we\nprove the Huygens theorem of decomposition of the inertia. We show the use of\nthe Wasserstein distance and of the basic statistics presenting a k-means like\nclustering algorithm, for the clustering of a set of data described by modal\nnumerical variables (distributional variables), on a real data set. Keywords:\nWasserstein distance, inertia, dependence, distributional data, modal\nvariables.\n", "versions": [{"version": "v1", "created": "Tue, 11 Oct 2011 08:16:15 GMT"}, {"version": "v2", "created": "Tue, 10 Dec 2013 19:13:55 GMT"}], "update_date": "2016-05-03", "authors_parsed": [["Irpino", "Antonio", ""], ["Verde", "Rosanna", ""]]}, {"id": "1110.2563", "submitter": "Stephanie S Zhang", "authors": "Cun-Hui Zhang and Stephanie S. Zhang", "title": "Confidence Intervals for Low-Dimensional Parameters in High-Dimensional\n  Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to propose methodologies for statistical\ninference of low-dimensional parameters with high-dimensional data. We focus on\nconstructing confidence intervals for individual coefficients and linear\ncombinations of several of them in a linear regression model, although our\nideas are applicable in a much broad context. The theoretical results presented\nhere provide sufficient conditions for the asymptotic normality of the proposed\nestimators along with a consistent estimator for their finite-dimensional\ncovariance matrices. These sufficient conditions allow the number of variables\nto far exceed the sample size. The simulation results presented here\ndemonstrate the accuracy of the coverage probability of the proposed confidence\nintervals, strongly supporting the theoretical results.\n", "versions": [{"version": "v1", "created": "Wed, 12 Oct 2011 03:50:03 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2012 15:54:15 GMT"}], "update_date": "2012-11-05", "authors_parsed": [["Zhang", "Cun-Hui", ""], ["Zhang", "Stephanie S.", ""]]}, {"id": "1110.2868", "submitter": "Joanna Janczura", "authors": "Joanna Janczura and Agnieszka Wy{\\l}oma\\'nska", "title": "Anomalous diffusion models: different types of subordinator distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math-ph math.MP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subordinated processes play an important role in modeling anomalous\ndiffusion-type behavior. In such models the observed constant time periods are\ndescribed by the subordinator distribution. Therefore, on the basis of the\nobserved time series, it is possible to conclude on the main properties of the\nsubordinator. In this paper we analyze the anomalous diffusion models with\nthree types of subordinator distribution: \\alpha-stable, tempered stable and\ngamma. We present similarities and differences between the analyzed processes\nand point at their main properties (like the behavior of moments or the mean\nsquared displacement).\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 08:49:14 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Janczura", "Joanna", ""], ["Wy\u0142oma\u0144ska", "Agnieszka", ""]]}, {"id": "1110.2894", "submitter": "Robin Evans", "authors": "Robin J. Evans and Antonio Forcina", "title": "Two algorithms for fitting constrained marginal models", "comments": "12 pages", "journal-ref": "Computational Statistics and Data Analysis, Volume 66, pages 1-7,\n  2013", "doi": "10.1016/j.csda.2013.02.001", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study in detail the two main algorithms which have been considered for\nfitting constrained marginal models to discrete data, one based on Lagrange\nmultipliers and the other on a regression model. We show that the updates\nproduced by the two methods are identical, but that the Lagrangian method is\nmore efficient in the case of identically distributed observations. We provide\na generalization of the regression algorithm for modelling the effect of\nexogenous individual-level covariates, a context in which the use of the\nLagrangian algorithm would be infeasible for even moderate sample sizes. An\nextension of the method to likelihood-based estimation under $L_1$-penalties is\nalso considered.\n", "versions": [{"version": "v1", "created": "Thu, 13 Oct 2011 11:18:20 GMT"}, {"version": "v2", "created": "Wed, 2 May 2012 13:06:38 GMT"}, {"version": "v3", "created": "Mon, 24 Dec 2012 16:11:44 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Evans", "Robin J.", ""], ["Forcina", "Antonio", ""]]}, {"id": "1110.3151", "submitter": "Ngom Papa", "authors": "Papa Ngom and Bertrand Ntep", "title": "Minimum penalized Hellinger distance for model selection in small\n  samples", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In statistical modeling area, the Akaike information criterion AIC, is a\nwidely known and extensively used tool for model choice. The {\\phi}-divergence\ntest statistic is a recently developed tool for statistical model selection.\nThe popularity of the divergence criterion is however tempered by their known\nlack of robustness in small sample. In this paper the penalized minimum\nHellinger distance type statistics are considered and some properties are\nestablished. The limit laws of the estimates and test statistics are given\nunder both the null and the alternative hypotheses, and approximations of the\npower functions are deduced. A model selection criterion relative to these\ndivergence measures are developed for parametric inference. Our interest is in\nthe problem to testing for choosing between two models using some informational\ntype statistics, when independent sample are drawn from a discrete population.\nHere, we discuss the asymptotic properties and the performance of new procedure\ntests and investigate their small sample behavior.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2011 08:56:59 GMT"}, {"version": "v2", "created": "Thu, 27 Oct 2011 13:41:10 GMT"}], "update_date": "2011-10-28", "authors_parsed": [["Ngom", "Papa", ""], ["Ntep", "Bertrand", ""]]}, {"id": "1110.3238", "submitter": "Maikol Sol\\'is", "authors": "S\\'ebastien Da Veiga, Jean-Michel Loubes and Maikol Sol\\'is", "title": "Efficient estimation of conditional covariance matrices for dimension\n  reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $\\boldsymbol{X}\\in \\mathbb{R}^p$ and $Y\\in \\mathbb{R}$. In this paper we\npropose an estimator of the conditional covariance matrix,\n$\\mathrm{Cov}(\\mathbb{E}[\\boldsymbol{X}\\vert Y])$, in an inverse regression\nsetting. Based on the estimation of a quadratic functional, this methodology\nprovides an efficient estimator from a semi parametric point of view. We\nconsider a functional Taylor expansion of\n$\\mathrm{Cov}(\\mathbb{E}[\\boldsymbol{X}\\vert Y])$ under some mild conditions\nand the effect of using an estimate of the unknown joint distribution. The\nasymptotic properties of this estimator are also provided.\n", "versions": [{"version": "v1", "created": "Fri, 14 Oct 2011 15:12:15 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2012 06:59:45 GMT"}, {"version": "v3", "created": "Thu, 22 Mar 2012 05:15:53 GMT"}, {"version": "v4", "created": "Tue, 19 Aug 2014 20:16:13 GMT"}], "update_date": "2014-08-21", "authors_parsed": [["Da Veiga", "S\u00e9bastien", ""], ["Loubes", "Jean-Michel", ""], ["Sol\u00eds", "Maikol", ""]]}, {"id": "1110.3392", "submitter": "Qing Zhou", "authors": "Qing Zhou", "title": "Multi-Domain Sampling With Applications to Structural Inference of\n  Bayesian Networks", "comments": "39 pages (double space), 3 figures. To appear in JASA", "journal-ref": "Journal of the American Statistical Association, 106: 1317-1330\n  (2011)", "doi": "10.1198/jasa.2011.ap10346.", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a posterior distribution has multiple modes, unconditional expectations,\nsuch as the posterior mean, may not offer informative summaries of the\ndistribution. Motivated by this problem, we propose to decompose the sample\nspace of a multimodal distribution into domains of attraction of local modes.\nDomain-based representations are defined to summarize the probability masses of\nand conditional expectations on domains of attraction, which are much more\ninformative than the mean and other unconditional expectations. A computational\nmethod, the multi-domain sampler, is developed to construct domain-based\nrepresentations for an arbitrary multimodal distribution. The multi-domain\nsampler is applied to structural learning of protein-signaling networks from\nhigh-throughput single-cell data, where a signaling network is modeled as a\ncausal Bayesian network. Not only does our method provide a detailed landscape\nof the posterior distribution but also improves the accuracy and the predictive\npower of estimated networks.\n", "versions": [{"version": "v1", "created": "Sat, 15 Oct 2011 07:16:52 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Zhou", "Qing", ""]]}, {"id": "1110.3556", "submitter": "Florentina Bunea", "authors": "Florentina Bunea, Yiyuan She, Marten H. Wegkamp", "title": "Joint variable and rank selection for parsimonious estimation of\n  high-dimensional matrices", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1039 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2012, Vol. 40, No. 5, 2359-2388", "doi": "10.1214/12-AOS1039", "report-no": "IMS-AOS-AOS1039", "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose dimension reduction methods for sparse, high-dimensional\nmultivariate response regression models. Both the number of responses and that\nof the predictors may exceed the sample size. Sometimes viewed as\ncomplementary, predictor selection and rank reduction are the most popular\nstrategies for obtaining lower-dimensional approximations of the parameter\nmatrix in such models. We show in this article that important gains in\nprediction accuracy can be obtained by considering them jointly. We motivate a\nnew class of sparse multivariate regression models, in which the coefficient\nmatrix has low rank and zero rows or can be well approximated by such a matrix.\nNext, we introduce estimators that are based on penalized least squares, with\nnovel penalties that impose simultaneous row and rank restrictions on the\ncoefficient matrix. We prove that these estimators indeed adapt to the unknown\nmatrix sparsity and have fast rates of convergence. We support our theoretical\nresults with an extensive simulation study and two data analyses.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 02:11:00 GMT"}, {"version": "v2", "created": "Mon, 30 Apr 2012 17:40:09 GMT"}, {"version": "v3", "created": "Mon, 20 Aug 2012 23:10:29 GMT"}, {"version": "v4", "created": "Wed, 13 Feb 2013 08:42:52 GMT"}], "update_date": "2013-02-14", "authors_parsed": [["Bunea", "Florentina", ""], ["She", "Yiyuan", ""], ["Wegkamp", "Marten H.", ""]]}, {"id": "1110.3599", "submitter": "Etienne Roquain", "authors": "Gilles Blanchard, Sylvain Delattre (LPMA), Etienne Roquain (LPMA)", "title": "Testing over a continuum of null hypotheses with False Discovery Rate\n  control", "comments": null, "journal-ref": "Bernoulli (2014) 304-333", "doi": "10.3150/12-BEJ488", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider statistical hypothesis testing simultaneously over a fairly\ngeneral, possibly uncountably infinite, set of null hypotheses, under the\nassumption that a suitable single test (and corresponding $p$-value) is known\nfor each individual hypothesis. We extend to this setting the notion of false\ndiscovery rate (FDR) as a measure of type I error. Our main result studies\nspecific procedures based on the observation of the $p$-value process. Control\nof the FDR at a nominal level is ensured either under arbitrary dependence of\n$p$-values, or under the assumption that the finite dimensional distributions\nof the $p$-value process have positive correlations of a specific type (weak\nPRDS). Both cases generalize existing results established in the finite\nsetting. Its interest is demonstrated in several non-parametric examples:\ntesting the mean/signal in a Gaussian white noise model, testing the intensity\nof a Poisson process and testing the c.d.f. of i.i.d. random variables.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 08:28:24 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2012 11:54:48 GMT"}, {"version": "v3", "created": "Thu, 6 Feb 2014 21:25:17 GMT"}], "update_date": "2014-02-10", "authors_parsed": [["Blanchard", "Gilles", "", "LPMA"], ["Delattre", "Sylvain", "", "LPMA"], ["Roquain", "Etienne", "", "LPMA"]]}, {"id": "1110.3689", "submitter": "Feng Li", "authors": "Feng Li and Mattias Villani", "title": "Efficient Bayesian Multivariate Surface Regression", "comments": null, "journal-ref": "Scandinavian Journal of Statistics, (2013), 40(4)", "doi": "10.1111/sjos.12022", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for choosing a fixed set of knot locations in additive spline models\nare fairly well established in the statistical literature. While most of these\nmethods are in principle directly extendable to non-additive surface models,\nthey are less likely to be successful in that setting because of the curse of\ndimensionality, especially when there are more than a couple of covariates. We\npropose a regression model for a multivariate Gaussian response that combines\nboth additive splines and interactive splines, and a highly efficient MCMC\nalgorithm that updates all the knot locations jointly. We use shrinkage priors\nto avoid overfitting with different estimated shrinkage factors for the\nadditive and surface part of the model, and also different shrinkage parameters\nfor the different response variables. This makes it possible for the model to\nadapt to varying degrees of nonlinearity in different parts of the data in a\nparsimonious way. Simulated data and an application to firm leverage data show\nthat the approach is computationally efficient, and that allowing for freely\nestimated knot locations can offer a substantial improvement in out-of-sample\npredictive performance.\n", "versions": [{"version": "v1", "created": "Mon, 17 Oct 2011 15:00:42 GMT"}, {"version": "v2", "created": "Thu, 4 Oct 2012 16:56:39 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Li", "Feng", ""], ["Villani", "Mattias", ""]]}, {"id": "1110.4025", "submitter": "Pierre E. Jacob", "authors": "Pierre E. Jacob, Robin J. Ryder", "title": "The Wang-Landau algorithm reaches the flat histogram criterion in finite\n  time", "comments": "Published in at http://dx.doi.org/10.1214/12-AAP913 the Annals of\n  Applied Probability (http://www.imstat.org/aap/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Probability 2014, Vol. 24, No. 1, 34-53", "doi": "10.1214/12-AAP913", "report-no": "IMS-AAP-AAP913", "categories": "math.ST math.PR stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Wang-Landau algorithm aims at sampling from a probability distribution,\nwhile penalizing some regions of the state space and favoring others. It is\nwidely used, but its convergence properties are still unknown. We show that for\nsome variations of the algorithm, the Wang-Landau algorithm reaches the\nso-called flat histogram criterion in finite time, and that this criterion can\nbe never reached for other variations. The arguments are shown in a simple\ncontext - compact spaces, density functions bounded from both sides - for the\nsake of clarity, and could be extended to more general contexts.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 15:38:57 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2012 14:18:56 GMT"}, {"version": "v3", "created": "Sat, 15 Dec 2012 09:01:17 GMT"}, {"version": "v4", "created": "Wed, 15 Jan 2014 09:29:23 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Jacob", "Pierre E.", ""], ["Ryder", "Robin J.", ""]]}, {"id": "1110.4102", "submitter": "David Albers", "authors": "D. J. Albers and George Hripcsak", "title": "Using time-delayed mutual information to discover and interpret temporal\n  correlation structure in complex populations", "comments": null, "journal-ref": null, "doi": "10.1063/1.3675621", "report-no": null, "categories": "nlin.CD cs.IT math.DS math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses how to calculate and interpret the time-delayed mutual\ninformation for a complex, diversely and sparsely measured, possibly\nnon-stationary population of time-series of unknown composition and origin. The\nprimary vehicle used for this analysis is a comparison between the time-delayed\nmutual information averaged over the population and the time-delayed mutual\ninformation of an aggregated population (here aggregation implies the\npopulation is conjoined before any statistical estimates are implemented).\nThrough the use of information theoretic tools, a sequence of practically\nimplementable calculations are detailed that allow for the average and\naggregate time-delayed mutual information to be interpreted. Moreover, these\ncalculations can be also be used to understand the degree of homo- or\nheterogeneity present in the population. To demonstrate that the proposed\nmethods can be used in nearly any situation, the methods are applied and\ndemonstrated on the time series of glucose measurements from two different\nsubpopulations of individuals from the Columbia University Medical Center\nelectronic health record repository, revealing a picture of the composition of\nthe population as well as physiological features.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 19:57:55 GMT"}], "update_date": "2015-05-30", "authors_parsed": [["Albers", "D. J.", ""], ["Hripcsak", "George", ""]]}, {"id": "1110.4128", "submitter": "Pablo Tamayo", "authors": "Pablo Tamayo, George Steinhardt, Arthur Liberzon, and Jill P. Mesirov", "title": "The Limitations of Simple Gene Set Enrichment Analysis Assuming Gene\n  Independence", "comments": "Submitted to Statistical Methods in Medical Research", "journal-ref": null, "doi": "10.1016/j.jbi.2011.12.002", "report-no": null, "categories": "stat.ME q-bio.GN stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its first publication in 2003, the Gene Set Enrichment Analysis (GSEA)\nmethod, based on the Kolmogorov-Smirnov statistic, has been heavily used,\nmodified, and also questioned. Recently a simplified approach, using a one\nsample t test score to assess enrichment and ignoring gene-gene correlations\nwas proposed by Irizarry et al. 2009 as a serious contender. The argument\ncriticizes GSEA's nonparametric nature and its use of an empirical null\ndistribution as unnecessary and hard to compute. We refute these claims by\ncareful consideration of the assumptions of the simplified method and its\nresults, including a comparison with GSEA's on a large benchmark set of 50\ndatasets. Our results provide strong empirical evidence that gene-gene\ncorrelations cannot be ignored due to the significant variance inflation they\nproduced on the enrichment scores and should be taken into account when\nestimating gene set enrichment significance. In addition, we discuss the\nchallenges that the complex correlation structure and multi-modality of gene\nsets pose more generally for gene set enrichment methods.\n", "versions": [{"version": "v1", "created": "Tue, 18 Oct 2011 21:24:51 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2012 16:28:04 GMT"}], "update_date": "2012-10-12", "authors_parsed": [["Tamayo", "Pablo", ""], ["Steinhardt", "George", ""], ["Liberzon", "Arthur", ""], ["Mesirov", "Jill P.", ""]]}, {"id": "1110.4304", "submitter": "J\\'an Dolinsk\\'y", "authors": "J\\'an Dolinsk\\'y and Kei Hirose and Sadanori Konishi", "title": "Readouts for Echo-state Networks Built using Locally Regularized\n  Orthogonal Forward Regression", "comments": "12 pages, pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo state network (ESN) is viewed as a temporal non-orthogonal expansion\nwith pseudo-random parameters. Such expansions naturally give rise to\nregressors of various relevance to a teacher output. We illustrate that often\nonly a certain amount of the generated echo-regressors effectively explain the\nvariance of the teacher output and also that sole local regularization is not\nable to provide in-depth information concerning the importance of the generated\nregressors. The importance is therefore determined by a joint calculation of\nthe individual variance contributions and Bayesian relevance using locally\nregularized orthogonal forward regression (LROFR) algorithm. This information\ncan be advantageously used in a variety of ways for an in-depth analysis of an\nESN structure and its state-space parameters in relation to the unknown\ndynamics of the underlying problem. We present locally regularized linear\nreadout built using LROFR. The readout may have a different dimensionality than\nan ESN model itself, and besides improving robustness and accuracy of an ESN it\nrelates the echo-regressors to different features of the training data and may\ndetermine what type of an additional readout is suitable for a task at hand.\nMoreover, as flexibility of the linear readout has limitations and might\nsometimes be insufficient for certain tasks, we also present a radial basis\nfunction (RBF) readout built using LROFR. It is a flexible and parsimonious\nreadout with excellent generalization abilities and is a viable alternative to\nreadouts based on a feed-forward neural network (FFNN) or an RBF net built\nusing relevance vector machine (RVM).\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 14:59:01 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2012 14:27:53 GMT"}, {"version": "v3", "created": "Mon, 2 Jul 2012 10:06:26 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Dolinsk\u00fd", "J\u00e1n", ""], ["Hirose", "Kei", ""], ["Konishi", "Sadanori", ""]]}, {"id": "1110.4400", "submitter": "Bjoern Bornkamp", "authors": "Bj\\\"orn Bornkamp", "title": "Functional Uniform Priors for Nonlinear Modelling", "comments": "submitted for publication", "journal-ref": "updated version published in Biometrics (2012) 68, 893-901,", "doi": "10.1111/j.1541-0420.2012.01747.x", "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the topic of finding prior distributions when a major\ncomponent of the statistical model depends on a nonlinear function. Using\nresults on how to construct uniform distributions in general metric spaces, we\npropose a prior distribution that is uniform in the space of functional shapes\nof the underlying nonlinear function and then back-transform to obtain a prior\ndistribution for the original model parameters. The primary application\nconsidered in this article is nonlinear regression, but the idea might be of\ninterest beyond this case. For nonlinear regression the so constructed priors\nhave the advantage that they are parametrization invariant and do not violate\nthe likelihood principle, as opposed to uniform distributions on the parameters\nor the Jeffrey's prior, respectively. The utility of the proposed priors is\ndemonstrated in the context of nonlinear regression modelling in clinical\ndose-finding trials, through a real data example and simulation. In addition\nthe proposed priors are used for calculation of an optimal Bayesian design.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 21:03:02 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Bornkamp", "Bj\u00f6rn", ""]]}, {"id": "1110.4411", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, David A. Knowles, Zoubin Ghahramani", "title": "Gaussian Process Regression Networks", "comments": "17 pages, 3 figures, 1 table. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new regression framework, Gaussian process regression networks\n(GPRN), which combines the structural properties of Bayesian neural networks\nwith the non-parametric flexibility of Gaussian processes. This model\naccommodates input dependent signal and noise correlations between multiple\nresponse variables, input dependent length-scales and amplitudes, and\nheavy-tailed predictive distributions. We derive both efficient Markov chain\nMonte Carlo and variational Bayes inference procedures for this model. We apply\nGPRN as a multiple output regression and multivariate volatility model,\ndemonstrating substantially improved performance over eight popular multiple\noutput (multi-task) Gaussian process models and three multivariate volatility\nmodels on benchmark datasets, including a 1000 dimensional gene expression\ndataset.\n", "versions": [{"version": "v1", "created": "Wed, 19 Oct 2011 22:18:03 GMT"}], "update_date": "2011-10-21", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Knowles", "David A.", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1110.4648", "submitter": "Martin Goldberg PhD", "authors": "Martin Goldberg", "title": "Anti-Robust and Tonsured Statistics", "comments": "18 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST q-fin.RM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This describes a statistical technique called \"tonsuring\" for exploratory\ndata analysis in finance. Instead of rejecting \"outlier\" data that conflicts\nwith the model, this strips out \"inlier\" data to get a clearer picture of how\nthe market changes for larger moves.\n", "versions": [{"version": "v1", "created": "Thu, 20 Oct 2011 20:59:31 GMT"}], "update_date": "2011-10-24", "authors_parsed": [["Goldberg", "Martin", ""]]}, {"id": "1110.4700", "submitter": "Christian P. Robert", "authors": "J.-M. Marin (Universite Montpellier), N. Pillai (Harvard University),\n  C.P. Robert (Universite Paris Dauphine, University of Warwick, and CREST),\n  and J. Rousseau (ENSAE, Universite Paris Dauphine and CREST)", "title": "Relevant statistics for Bayesian model choice", "comments": "30 pages, 8 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-bio.PE stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The choice of the summary statistics used in Bayesian inference and in\nparticular in ABC algorithms has bearings on the validation of the resulting\ninference. Those statistics are nonetheless customarily used in ABC algorithms\nwithout consistency checks. We derive necessary and sufficient conditions on\nsummary statistics for the corresponding Bayes factor to be convergent, namely\nto asymptotically select the true model. Those conditions, which amount to the\nexpectations of the summary statistics to asymptotically differ under both\nmodels, are quite natural and can be exploited in ABC settings to infer whether\nor not a choice of summary statistics is appropriate, via a Monte Carlo\nvalidation.\n", "versions": [{"version": "v1", "created": "Fri, 21 Oct 2011 05:07:49 GMT"}, {"version": "v2", "created": "Fri, 4 May 2012 18:50:31 GMT"}, {"version": "v3", "created": "Fri, 15 Mar 2013 18:57:13 GMT"}, {"version": "v4", "created": "Thu, 22 Aug 2013 15:55:26 GMT"}], "update_date": "2013-08-23", "authors_parsed": [["Marin", "J. -M.", "", "Universite Montpellier"], ["Pillai", "N.", "", "Harvard University"], ["Robert", "C. P.", "", "Universite Paris Dauphine, University of Warwick, and CREST"], ["Rousseau", "J.", "", "ENSAE, Universite Paris Dauphine and CREST"]]}, {"id": "1110.5415", "submitter": "Jeremie Bigot", "authors": "J\\'er\\'emie Bigot (DMIA), Benjamin Charlier (IMT)", "title": "Consistent estimation of a mean planar curve modulo similarities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating a mean planar curve from a set of $J$\nrandom planar curves observed on a $k$-points deterministic design. We study\nthe consistency of a smoothed Procrustean mean curve when the observations obey\na deformable model including some nuisance parameters such as random\ntranslations, rotations and scaling. The main contribution of the paper is to\nanalyze the influence of the dimension $k$ of the data and of the number $J$ of\nobserved configurations on the convergence of the smoothed Procrustean\nestimator to the mean curve of the model. Some numerical experiments illustrate\nthese results.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2011 06:08:48 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2012 13:20:53 GMT"}], "update_date": "2012-11-01", "authors_parsed": [["Bigot", "J\u00e9r\u00e9mie", "", "DMIA"], ["Charlier", "Benjamin", "", "IMT"]]}, {"id": "1110.5429", "submitter": "Egil Ferkingstad", "authors": "Egil Ferkingstad, Anders L{\\o}land, Mathilde Wilhelmsen", "title": "Causal modeling and inference for electricity markets", "comments": null, "journal-ref": "Energy Economics (2011), 33(3):404-412", "doi": "10.1016/j.eneco.2010.10.006", "report-no": null, "categories": "stat.AP q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How does dynamic price information flow among Northern European electricity\nspot prices and prices of major electricity generation fuel sources? We use\ntime series models combined with new advances in causal inference to answer\nthese questions. Applying our methods to weekly Nordic and German electricity\nprices, and oil, gas and coal prices, with German wind power and Nordic water\nreservoir levels as exogenous variables, we estimate a causal model for the\nprice dynamics, both for contemporaneous and lagged relationships. In\ncontemporaneous time, Nordic and German electricity prices are interlinked\nthrough gas prices. In the long run, electricity prices and British gas prices\nadjust themselves to establish the equlibrium price level, since oil, coal,\ncontinental gas and EUR/USD are found to be weakly exogenous.\n", "versions": [{"version": "v1", "created": "Tue, 25 Oct 2011 07:59:28 GMT"}], "update_date": "2011-10-26", "authors_parsed": [["Ferkingstad", "Egil", ""], ["L\u00f8land", "Anders", ""], ["Wilhelmsen", "Mathilde", ""]]}, {"id": "1110.6451", "submitter": "Roman Jandarov", "authors": "Roman Jandarov, Murali Haran, Ottar Bj{\\o}rnstad and Bryan Grenfell", "title": "Emulating a gravity model to infer the spatiotemporal dynamics of an\n  infectious disease", "comments": "31 pages, 8 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic models for infectious disease dynamics are useful for\nunderstanding the mechanism underlying the spread of infection. When the\nlikelihood function for these models is expensive to evaluate, traditional\nlikelihood-based inference may be computationally intractable. Furthermore,\ntraditional inference may lead to poor parameter estimates and the fitted model\nmay not capture important biological characteristics of the observed data. We\npropose a novel approach for resolving these issues that is inspired by recent\nwork in emulation and calibration for complex computer models. Our motivating\nexample is the gravity time series susceptible-infected-recovered (TSIR) model.\nOur approach focuses on the characteristics of the process that are of\nscientific interest. We find a Gaussian process approximation to the gravity\nmodel using key summary statistics obtained from model simulations. We\ndemonstrate via simulated examples that the new approach is computationally\nexpedient, provides accurate parameter inference, and results in a good model\nfit. We apply our method to analyze measles outbreaks in England and Wales in\ntwo periods, the pre-vaccination period from 1944-1965 and the vaccination\nperiod from 1966-1994. Based on our results, we are able to obtain important\nscientific insights about the transmission of measles. In general, our method\nis applicable to problems where traditional likelihood-based inference is\ncomputationally intractable or produces a poor model fit. It is also an\nalternative to approximate Bayesian computation (ABC) when simulations from the\nmodel are expensive.\n", "versions": [{"version": "v1", "created": "Fri, 28 Oct 2011 20:05:44 GMT"}, {"version": "v2", "created": "Fri, 6 Jan 2012 00:35:39 GMT"}, {"version": "v3", "created": "Thu, 14 Feb 2013 22:52:49 GMT"}], "update_date": "2013-02-18", "authors_parsed": [["Jandarov", "Roman", ""], ["Haran", "Murali", ""], ["Bj\u00f8rnstad", "Ottar", ""], ["Grenfell", "Bryan", ""]]}, {"id": "1110.6560", "submitter": "Xi Luo", "authors": "Xi Luo, Dylan S. Small, Chiang-shan R. Li, Paul R. Rosenbaum", "title": "Inference with interference between units in an fMRI experiment of motor\n  inhibition", "comments": "Published by Journal of the American Statistical Association at\n  http://www.tandfonline.com/doi/full/10.1080/01621459.2012.655954 . R package\n  cin (Causal Inference for Neuroscience) implementing the proposed method is\n  freely available on CRAN at https://CRAN.R-project.org/package=cin", "journal-ref": "Journal of the American Statistical Association 107, no. 498\n  (2012): 530-541", "doi": "10.1080/01621459.2012.655954", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An experimental unit is an opportunity to randomly apply or withhold a\ntreatment. There is interference between units if the application of the\ntreatment to one unit may also affect other units. In cognitive neuroscience, a\ncommon form of experiment presents a sequence of stimuli or requests for\ncognitive activity at random to each experimental subject and measures\nbiological aspects of brain activity that follow these requests. Each subject\nis then many experimental units, and interference between units within an\nexperimental subject is likely, in part because the stimuli follow one another\nquickly and in part because human subjects learn or become experienced or\nprimed or bored as the experiment proceeds. We use a recent fMRI experiment\nconcerned with the inhibition of motor activity to illustrate and further\ndevelop recently proposed methodology for inference in the presence of\ninterference. A simulation evaluates the power of competing procedures.\n", "versions": [{"version": "v1", "created": "Sat, 29 Oct 2011 22:11:41 GMT"}, {"version": "v2", "created": "Tue, 10 Jan 2017 16:05:21 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Luo", "Xi", ""], ["Small", "Dylan S.", ""], ["Li", "Chiang-shan R.", ""], ["Rosenbaum", "Paul R.", ""]]}, {"id": "1110.6623", "submitter": "Jay Bartroff", "authors": "Jay Bartroff", "title": "A New Characterization of Elfving's Method for High Dimensional\n  Computation", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a new characterization of Elfving's (1952) method for computing\nc-optimal designs in k dimensions which gives explicit formulae for the k\nunknown optimal weights and k unknown signs in Elfving's characterization. This\neliminates the need to search over these parameters to compute c-optimal\ndesigns, and thus reduces the computational burden from solving a family of\noptimization problems to solving a single optimization problem for the optimal\nfinite support set. We give two illustrative examples: a high dimensional\npolynomial regression model and a logistic regression model, the latter showing\nthat the method can be used for locally optimal designs in nonlinear models as\nwell.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2011 16:59:26 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Bartroff", "Jay", ""]]}, {"id": "1110.6817", "submitter": "Sihai Zhao", "authors": "Sihai D. Zhao and Yi Li", "title": "Sure screening for estimating equations in ultra-high dimensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the number of possible predictors generated by high-throughput experiments\ncontinues to increase, methods are needed to quickly screen out unimportant\ncovariates. Model-based screening methods have been proposed and theoretically\njustified, but only for a few specific models. Model-free screening methods\nhave also recently been studied, but can have lower power to detect important\ncovariates. In this paper we propose EEScreen, a screening procedure that can\nbe used with any model that can be fit using estimating equations, and provide\nunified results on its finite-sample screening performance. EEScreen thus\ngeneralizes many recently proposed model-based and model-free screening\nprocedures. We also propose iEEScreen, an iterative version of EEScreen, and\nshow that it is closely related to a recently studied boosting method for\nestimating equations. We show via simulations for two different estimating\nequations that EEScreen and iEEScreen are useful and flexible screening\nprocedures, and demonstrate our methods on data from a multiple myeloma study.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 15:03:11 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2012 03:31:19 GMT"}, {"version": "v3", "created": "Wed, 30 May 2012 01:16:21 GMT"}], "update_date": "2012-05-31", "authors_parsed": [["Zhao", "Sihai D.", ""], ["Li", "Yi", ""]]}, {"id": "1110.6896", "submitter": "Mohamed Boutahar", "authors": "Mohamed Boutahar (IML), Denys Pommeret (IML)", "title": "Testing for equality between two transformations of random variables", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider two random variables contaminated by two unknown transformations.\nThe aim of this paper is to test the equality of those transformations. Two\ncases are distinguished: first, the two random variables have known\ndistributions. Second, they are unknown but observed before contaminations. We\npropose a nonparametric test statistic based on empirical cumulative\ndistribution functions. Monte Carlo studies are performed to analyze the level\nand the power of the test. An illustration is presented through a real data\nset.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 18:45:14 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Boutahar", "Mohamed", "", "IML"], ["Pommeret", "Denys", "", "IML"]]}]