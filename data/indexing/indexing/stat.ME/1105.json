[{"id": "1105.0562", "submitter": "Vincent Dubourg", "authors": "V. Dubourg and F. Deheeger and B. Sudret", "title": "Metamodel-based importance sampling for structural reliability analysis", "comments": "20 pages, 7 figures, 2 tables. Preprint submitted to Probabilistic\n  Engineering Mechanics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural reliability methods aim at computing the probability of failure of\nsystems with respect to some prescribed performance functions. In modern\nengineering such functions usually resort to running an expensive-to-evaluate\ncomputational model (e.g. a finite element model). In this respect simulation\nmethods, which may require $10^{3-6}$ runs cannot be used directly. Surrogate\nmodels such as quadratic response surfaces, polynomial chaos expansions or\nkriging (which are built from a limited number of runs of the original model)\nare then introduced as a substitute of the original model to cope with the\ncomputational cost. In practice it is almost impossible to quantify the error\nmade by this substitution though. In this paper we propose to use a kriging\nsurrogate of the performance function as a means to build a quasi-optimal\nimportance sampling density. The probability of failure is eventually obtained\nas the product of an augmented probability computed by substituting the\nmeta-model for the original performance function and a correction term which\nensures that there is no bias in the estimation even if the meta-model is not\nfully accurate. The approach is applied to analytical and finite element\nreliability problems and proves efficient up to 100 random variables.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2011 12:16:58 GMT"}, {"version": "v2", "created": "Sat, 7 May 2011 09:54:02 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Dubourg", "V.", ""], ["Deheeger", "F.", ""], ["Sudret", "B.", ""]]}, {"id": "1105.0760", "submitter": "Stevenn Volant", "authors": "Stevenn Volant, Marie-Laure Martin Magniette and St\\'ephane Robin", "title": "Variational Bayes approach for model aggregation in unsupervised\n  classification with Markovian dependency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a binary unsupervised classification problem where each\nobservation is associated with an unobserved label that we want to retrieve.\nMore precisely, we assume that there are two groups of observation: normal and\nabnormal. The `normal' observations are coming from a known distribution\nwhereas the distribution of the `abnormal' observations is unknown. Several\nmodels have been developed to fit this unknown distribution. In this paper, we\npropose an alternative based on a mixture of Gaussian distributions. The\ninference is done within a variational Bayesian framework and our aim is to\ninfer the posterior probability of belonging to the class of interest. To this\nend, it makes no sense to estimate the mixture component number since each\nmixture model provides more or less relevant information to the posterior\nprobability estimation. By computing a weighted average (named aggregated\nestimator) over the model collection, Bayesian Model Averaging (BMA) is one way\nof combining models in order to account for information provided by each model.\nThe aim is then the estimation of the weights and the posterior probability for\none specific model. In this work, we derive optimal approximations of these\nquantities from the variational theory and propose other approximations of the\nweights. To perform our method, we consider that the data are dependent\n(Markovian dependency) and hence we consider a Hidden Markov Model. A\nsimulation study is carried out to evaluate the accuracy of the estimates in\nterms of classification. We also present an application to the analysis of\npublic health surveillance systems.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 08:30:58 GMT"}], "update_date": "2011-05-05", "authors_parsed": [["Volant", "Stevenn", ""], ["Magniette", "Marie-Laure Martin", ""], ["Robin", "St\u00e9phane", ""]]}, {"id": "1105.0902", "submitter": "Drew Conway", "authors": "Drew Conway", "title": "Modeling Network Evolution Using Graph Motifs", "comments": "33 pages, 7 pages, GMM code available at:\n  https://github.com/drewconway/GMM", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph stat.CO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Network structures are extremely important to the study of political science.\nMuch of the data in its subfields are naturally represented as networks. This\nincludes trade, diplomatic and conflict relationships. The social structure of\nseveral organization is also of interest to many researchers, such as the\naffiliations of legislators or the relationships among terrorist. A key aspect\nof studying social networks is understanding the evolutionary dynamics and the\nmechanism by which these structures grow and change over time. While current\nmethods are well suited to describe static features of networks, they are less\ncapable of specifying models of change and simulating network evolution. In the\nfollowing paper I present a new method for modeling network growth and\nevolution. This method relies on graph motifs to generate simulated network\ndata with particular structural characteristic. This technique departs notably\nfrom current methods both in form and function. Rather than a closed-form\nmodel, or stochastic implementation from a single class of graphs, the proposed\n\"graph motif model\" provides a framework for building flexible and complex\nmodels of network evolution. The paper proceeds as follows: first a brief\nreview of the current literature on network modeling is provided to place the\ngraph motif model in context. Next, the graph motif model is introduced, and a\nsimple example is provided. As a proof of concept, three classic random graph\nmodels are recovered using the graph motif modeling method: the Erdos-Renyi\nbinomial random graph, the Watts-Strogatz \"small world\" model, and the\nBarabasi-Albert preferential attachment model. In the final section I discuss\nthe results of these simulations and subsequent advantage and disadvantages\npresented by using this technique to model social networks.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 19:23:48 GMT"}], "update_date": "2011-05-05", "authors_parsed": [["Conway", "Drew", ""]]}, {"id": "1105.1475", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni, Victor Chernozhukov, Lie Wang", "title": "Pivotal estimation via square-root Lasso in nonparametric regression", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1204 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 2, 757-788", "doi": "10.1214/14-AOS1204", "report-no": "IMS-AOS-AOS1204", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-tuning $\\sqrt{\\mathrm {Lasso}}$ method that simultaneously\nresolves three important practical problems in high-dimensional regression\nanalysis, namely it handles the unknown scale, heteroscedasticity and (drastic)\nnon-Gaussianity of the noise. In addition, our analysis allows for badly\nbehaved designs, for example, perfectly collinear regressors, and generates\nsharp bounds even in extreme cases, such as the infinite variance case and the\nnoiseless case, in contrast to Lasso. We establish various nonasymptotic bounds\nfor $\\sqrt{\\mathrm {Lasso}}$ including prediction norm rate and sparsity. Our\nanalysis is based on new impact factors that are tailored for bounding\nprediction norm. In order to cover heteroscedastic non-Gaussian noise, we rely\non moderate deviation theory for self-normalized sums to achieve Gaussian-like\nresults under weak conditions. Moreover, we derive bounds on the performance of\nordinary least square (ols) applied to the model selected by $\\sqrt{\\mathrm\n{Lasso}}$ accounting for possible misspecification of the selected model. Under\nmild conditions, the rate of convergence of ols post $\\sqrt{\\mathrm {Lasso}}$\nis as good as $\\sqrt{\\mathrm {Lasso}}$'s rate. As an application, we consider\nthe use of $\\sqrt{\\mathrm {Lasso}}$ and ols post $\\sqrt{\\mathrm {Lasso}}$ as\nestimators of nuisance parameters in a generic semiparametric problem\n(nonlinear moment condition or $Z$-problem), resulting in a construction of\n$\\sqrt{n}$-consistent and asymptotically normal estimators of the main\nparameters.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2011 21:26:59 GMT"}, {"version": "v2", "created": "Sun, 15 May 2011 16:57:53 GMT"}, {"version": "v3", "created": "Thu, 31 May 2012 03:38:58 GMT"}, {"version": "v4", "created": "Sat, 8 Sep 2012 18:43:36 GMT"}, {"version": "v5", "created": "Sun, 8 Dec 2013 21:56:59 GMT"}, {"version": "v6", "created": "Mon, 16 Dec 2013 20:18:40 GMT"}, {"version": "v7", "created": "Thu, 6 Feb 2014 03:48:21 GMT"}, {"version": "v8", "created": "Mon, 26 May 2014 11:07:50 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Wang", "Lie", ""]]}, {"id": "1105.1575", "submitter": "Yuan-chin Chang yc.ivan.chang", "authors": "Zhanfeng Wang and Yuan-chin Ivan Chang", "title": "Evaluating the diagnostic powers of variables and their linear\n  combinations when the gold standard is continuous", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The receiver operating characteristic (ROC) curve is a very useful tool for\nanalyzing the diagnostic/classification power of instruments/classification\nschemes as long as a binary-scale gold standard is available. When the gold\nstandard is continuous and there is no confirmative threshold, ROC curve\nbecomes less useful. Hence, there are several extensions proposed for\nevaluating the diagnostic potential of variables of interest. However, due to\nthe computational difficulties of these nonparametric based extensions, they\nare not easy to be used for finding the optimal combination of variables to\nimprove the individual diagnostic power. Therefore, we propose a new measure,\nwhich extends the AUC index for identifying variables with good potential to be\nused in a diagnostic scheme. In addition, we propose a threshold gradient\ndescent based algorithm for finding the best linear combination of variables\nthat maximizes this new measure, which is applicable even when the number of\nvariables is huge. The estimate of the proposed index and its asymptotic\nproperty are studied. The performance of the proposed method is illustrated\nusing both synthesized and real data sets.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2011 03:54:34 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Wang", "Zhanfeng", ""], ["Chang", "Yuan-chin Ivan", ""]]}, {"id": "1105.1684", "submitter": "Silvia Bianconcini", "authors": "Silvia Bianconcini and Silvia Cagnone", "title": "Estimation of latent variable models for ordinal data via fully\n  exponential Laplace approximation", "comments": null, "journal-ref": null, "doi": "10.1016/j.jmva.2012.06.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models for ordinal data represent a useful tool in different\nfields of research in which the constructs of interest are not directly\nobservable. In such models, problems related to the integration of the\nlikelihood function can arise since analytical solutions do not exist.\nNumerical approximations, like the widely used Gauss Hermite (GH) quadrature,\nare generally applied to solve these problems. However, GH becomes unfeasible\nas the number of latent variables increases. Thus, alternative solutions have\nto be found. In this paper, we propose an extended version of the Laplace\nmethod for approximating the integrals, known as fully exponential Laplace\napproximation. It is computational feasible also in presence of many latent\nvariables, and it is more accurate than the classical Laplace method.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2011 14:28:14 GMT"}], "update_date": "2012-06-26", "authors_parsed": [["Bianconcini", "Silvia", ""], ["Cagnone", "Silvia", ""]]}, {"id": "1105.1924", "submitter": "Lauren Hannah", "authors": "Lauren A. Hannah, David B. Dunson", "title": "Multivariate convex regression with adaptive partitioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new, nonparametric method for multivariate regression subject to\nconvexity or concavity constraints on the response function. Convexity\nconstraints are common in economics, statistics, operations research, financial\nengineering and optimization, but there is currently no multivariate method\nthat is computationally feasible for more than a few hundred observations. We\nintroduce Convex Adaptive Partitioning (CAP), which creates a globally convex\nregression model from locally linear estimates fit on adaptively selected\ncovariate partitions. CAP is computationally efficient, in stark contrast to\ncurrent methods. The most popular method, the least squares estimator, has a\ncomputational complexity of $\\mathcal{O}(n^3)$. We show that CAP has a\ncomputational complexity of $\\mathcal{O}(n \\log(n)\\log(\\log(n)))$ and also give\nconsistency results. CAP is applied to value function approximation for pricing\nAmerican basket options with a large number of underlying assets.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2011 12:09:22 GMT"}, {"version": "v2", "created": "Sun, 13 Nov 2011 03:37:42 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Hannah", "Lauren A.", ""], ["Dunson", "David B.", ""]]}, {"id": "1105.2072", "submitter": "Lizandra Castilho Fabio", "authors": "Lizandra C. Fabio, Gilberto A. Paula and Mario de Castro", "title": "A Poisson Mixed Model with Nonnormal Random Effect Distribution", "comments": "Submitted in the Computational Statistics & Data Analysis journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose in this paper a random intercept Poisson model in which the random\neffect distribution is assumed to follow a generalized log-gamma (GLG)\ndistribution. We derive the first two moments for the marginal distribution as\nwell as the intraclass correlation. Even though numerical integration methods\nare in general required for deriving the marginal models, we obtain the\nmultivariate negative binomial model for a particular parameter setting of the\nhierarchical model. An iterative process is derived for obtaining the maximum\nlikelihood estimates for the parameters in the multivariate negative binomial\nmodel. Residual analysis are proposed and two applications with real data are\ngiven for illustration.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2011 23:15:19 GMT"}], "update_date": "2011-05-12", "authors_parsed": [["Fabio", "Lizandra C.", ""], ["Paula", "Gilberto A.", ""], ["de Castro", "Mario", ""]]}, {"id": "1105.2135", "submitter": "Herv\\'{e} Cardot", "authors": "Herv\\'e Cardot, David Degras, Etienne Josserand", "title": "Confidence bands for Horvitz-Thompson estimators using sampled noisy\n  functional data", "comments": "Published in at http://dx.doi.org/10.3150/12-BEJ443 the Bernoulli\n  (http://isi.cbs.nl/bernoulli/) by the International Statistical\n  Institute/Bernoulli Society (http://isi.cbs.nl/BS/bshome.htm)", "journal-ref": "Bernoulli 2013, Vol. 19, No. 5A, 2067-2097", "doi": "10.3150/12-BEJ443", "report-no": "IMS-BEJ-BEJ443", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When collections of functional data are too large to be exhaustively\nobserved, survey sampling techniques provide an effective way to estimate\nglobal quantities such as the population mean function. Assuming functional\ndata are collected from a finite population according to a probabilistic\nsampling scheme, with the measurements being discrete in time and noisy, we\npropose to first smooth the sampled trajectories with local polynomials and\nthen estimate the mean function with a Horvitz-Thompson estimator. Under mild\nconditions on the population size, observation times, regularity of the\ntrajectories, sampling scheme, and smoothing bandwidth, we prove a Central\nLimit theorem in the space of continuous functions. We also establish the\nuniform consistency of a covariance function estimator and apply the former\nresults to build confidence bands for the mean function. The bands attain\nnominal coverage and are obtained through Gaussian process simulations\nconditional on the estimated covariance function. To select the bandwidth, we\npropose a cross-validation method that accounts for the sampling weights. A\nsimulation study assesses the performance of our approach and highlights the\ninfluence of the sampling scheme and bandwidth choice.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2011 09:20:57 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2012 08:41:01 GMT"}, {"version": "v3", "created": "Tue, 10 Apr 2012 08:40:01 GMT"}, {"version": "v4", "created": "Wed, 11 Dec 2013 10:30:02 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Cardot", "Herv\u00e9", ""], ["Degras", "David", ""], ["Josserand", "Etienne", ""]]}, {"id": "1105.2150", "submitter": "Hung Hung", "authors": "Hung Hung, Chen-Chien Wang", "title": "Matrix Variate Logistic Regression Model with Application to EEG Data", "comments": "19 pages, 1 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logistic regression has been widely applied in the field of biomedical\nresearch for a long time. In some applications, covariates of interest have a\nnatural structure, such as being a matrix, at the time of collection. The rows\nand columns of the covariate matrix then have certain physical meanings, and\nthey must contain useful information regarding the response. If we simply stack\nthe covariate matrix as a vector and fit the conventional logistic regression\nmodel, relevant information can be lost, and the problem of inefficiency will\narise. Motivated from these reasons, we propose in this paper the matrix\nvariate logistic (MV-logistic) regression model. Advantages of MV-logistic\nregression model include the preservation of the inherent matrix structure of\ncovariates and the parsimony of parameters needed. In the EEG Database Data\nSet, we successfully extract the structural effects of covariate matrix, and a\nhigh classification accuracy is achieved.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2011 10:44:13 GMT"}, {"version": "v2", "created": "Thu, 1 Dec 2011 04:09:39 GMT"}], "update_date": "2011-12-02", "authors_parsed": [["Hung", "Hung", ""], ["Wang", "Chen-Chien", ""]]}, {"id": "1105.2204", "submitter": "William Astle", "authors": "William Astle, Maria De Iorio, Sylvia Richardson, David Stephens and\n  Timothy Ebbels", "title": "A Bayesian Model of NMR Spectra for the Deconvolution and Quantification\n  of Metabolites in Complex Biological Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear Magnetic Resonance (NMR) spectra are widely used in metabolomics to\nobtain profiles of metabolites dissolved in biofluids such as cell\nsupernatants. Methods for estimating metabolite concentrations from these\nspectra are presently confined to manual peak fitting and to binning procedures\nfor integrating resonance peaks. Extensive information on the patterns of\nspectral resonance generated by human metabolites is now available in online\ndatabases. By incorporating this information into a Bayesian model we can\ndeconvolve resonance peaks from a spectrum and obtain explicit concentration\nestimates for the corresponding metabolites. Spectral resonances that cannot be\ndeconvolved in this way may also be of scientific interest so we model them\njointly using wavelets.\n  We describe a Markov chain Monte Carlo algorithm which allows us to sample\nfrom the joint posterior distribution of the model parameters, using\nspecifically designed block updates to improve mixing. The strong prior on\nresonance patterns allows the algorithm to identify peaks corresponding to\nparticular metabolites automatically, eliminating the need for manual peak\nassignment.\n  We assess our method for peak alignment and concentration estimation. Except\nin cases when the target resonance signal is very weak, alignment is unbiased\nand precise. We compare the Bayesian concentration estimates to those obtained\nfrom a conventional numerical integration method and find that our point\nestimates have sixfold lower mean squared error.\n  Finally, we apply our method to a spectral dataset taken from an\ninvestigation of the metabolic response of yeast to recombinant protein\nexpression. We estimate the concentrations of 26 metabolites and compare to\nmanual quantification by five expert spectroscopists. We discuss the reason for\ndiscrepancies and the robustness of our methods concentration estimates.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2011 14:24:04 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2012 23:48:33 GMT"}, {"version": "v3", "created": "Tue, 15 May 2012 18:36:56 GMT"}], "update_date": "2012-05-16", "authors_parsed": [["Astle", "William", ""], ["De Iorio", "Maria", ""], ["Richardson", "Sylvia", ""], ["Stephens", "David", ""], ["Ebbels", "Timothy", ""]]}, {"id": "1105.2220", "submitter": "Johan Segers", "authors": "Ivan Kojadinovic, Johan Segers, Jun Yan", "title": "Large-sample tests of extreme-value dependence for multivariate copulas", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from the characterization of extreme-value copulas based on\nmax-stability, large-sample tests of extreme-value dependence for multivariate\ncopulas are studied. The two key ingredients of the proposed tests are the\nempirical copula of the data and a multiplier technique for obtaining\napproximate p-values for the derived statistics. The asymptotic validity of the\nmultiplier approach is established, and the finite-sample performance of a\nlarge number of candidate test statistics is studied through extensive Monte\nCarlo experiments for data sets of dimension two to five. In the bivariate\ncase, the rejection rates of the best versions of the tests are compared with\nthose of the test of Ghoudi, Khoudraji and Rivest (1998) recently revisited by\nBen Ghorbal, Genest and Neslehova (2009). The proposed procedures are\nillustrated on bivariate financial data and trivariate geological data.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2011 15:25:32 GMT"}], "update_date": "2011-05-12", "authors_parsed": [["Kojadinovic", "Ivan", ""], ["Segers", "Johan", ""], ["Yan", "Jun", ""]]}, {"id": "1105.2266", "submitter": "Albert Mao", "authors": "Albert H. Mao, Rohit V. Pappu", "title": "Exact recording of Metropolis-Hastings-class Monte Carlo simulations\n  using one bit per sample", "comments": "5 pages, 2 tables, 1 executable Java Archive (JAR) file", "journal-ref": "Computer Physics Communications, Volume 182, Issue 7, July 2011,\n  Pages 1452-1454", "doi": "10.1016/j.cpc.2011.03.013", "report-no": null, "categories": "physics.comp-ph physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Metropolis-Hastings (MH) algorithm is the prototype for a class of Markov\nchain Monte Carlo methods that propose transitions between states and then\naccept or reject the proposal. These methods generate a correlated sequence of\nrandom samples that convey information about the desired probability\ndistribution. Deciding how this information gets recorded is an important step\nin the practical design of MH-class algorithm implementations. Many\nimplementations discard most of this information in order to reduce demands on\nstorage capacity and disk writing throughput. Here, we describe how recording a\nbit string containing 1's for acceptance and 0's for rejection allows the full\nsample sequence to be recorded with no information loss, facilitating\ndecoupling of simulation design from the constraints of data analysis. The\nrecording uses only one bit per sample, which is an upper bound on the rate at\nwhich information about the desired distribution is acquired. We also\ndemonstrate the method and quantify its benefits on a nontrivial colloidal\nsystem of charged particles in the canonical ensemble. The method imposes no\nrestrictions on the system or simulation design and is compatible with\ndescendants of the MH algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 11 May 2011 17:58:49 GMT"}], "update_date": "2011-05-12", "authors_parsed": [["Mao", "Albert H.", ""], ["Pappu", "Rohit V.", ""]]}, {"id": "1105.2526", "submitter": "Alexander Blocker", "authors": "Alexander W. Blocker and Edoardo M. Airoldi", "title": "Deconvolution of mixing time series on a graph", "comments": "10 pages, 11 page supplement; updated with minor edits; accepted into\n  UAI 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications we are interested in making inference on latent time\nseries from indirect measurements, which are often low-dimensional projections\nresulting from mixing or aggregation. Positron emission tomography,\nsuper-resolution, and network traffic monitoring are some examples. Inference\nin such settings requires solving a sequence of ill-posed inverse problems,\ny_t= A x_t, where the projection mechanism provides information on A. We\nconsider problems in which A specifies mixing on a graph of times series that\nare bursty and sparse. We develop a multilevel state-space model for mixing\ntimes series and an efficient approach to inference. A simple model is used to\ncalibrate regularization parameters that lead to efficient inference in the\nmultilevel state-space model. We apply this method to the problem of estimating\npoint-to-point traffic flows on a network from aggregate measurements. Our\nsolution outperforms existing methods for this problem, and our two-stage\napproach suggests an efficient inference strategy for multilevel models of\ndependent time series.\n", "versions": [{"version": "v1", "created": "Thu, 12 May 2011 16:38:17 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2011 16:51:32 GMT"}], "update_date": "2011-06-13", "authors_parsed": [["Blocker", "Alexander W.", ""], ["Airoldi", "Edoardo M.", ""]]}, {"id": "1105.2917", "submitter": "Liang Li", "authors": "Liang Li", "title": "Propensity Score Analysis with Matching Weights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The propensity score analysis is one of the most widely used methods for\nstudying the causal treatment effect in observational studies. This paper\nstudies treatment effect estimation with the method of matching weights. This\nmethod resembles propensity score matching but offers a number of new features\nincluding efficient estimation, rigorous variance calculation, simple\nasymptotics, statistical tests of balance, clearly identified target population\nwith optimal sampling property, and no need for choosing matching algorithm and\ncaliper size. In addition, we propose the mirror histogram as a useful tool for\ngraphically displaying balance. The method also shares some features of the\ninverse probability weighting methods, but the computation remains stable when\nthe propensity scores approach 0 or 1. An augmented version of the matching\nweight estimator is developed that has the double robust property, i.e., the\nestimator is consistent if either the outcome model or the propensity score\nmodel is correct. In the numerical studies, the proposed methods demonstrated\nbetter performance than many widely used propensity score analysis methods such\nas stratification by quintiles, matching with propensity scores, and inverse\nprobability weighting.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2011 01:12:33 GMT"}], "update_date": "2011-05-17", "authors_parsed": [["Li", "Liang", ""]]}, {"id": "1105.2965", "submitter": "Dalton Lunga", "authors": "Dalton Lunga, Sergey Kirshner", "title": "Generating Similar Graphs From Spherical Features", "comments": "29 pages, 14 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI physics.soc-ph stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel model for generating graphs similar to a given example\ngraph. Unlike standard approaches that compute features of graphs in Euclidean\nspace, our approach obtains features on a surface of a hypersphere. We then\nutilize a von Mises-Fisher distribution, an exponential family distribution on\nthe surface of a hypersphere, to define a model over possible feature values.\nWhile our approach bears similarity to a popular exponential random graph model\n(ERGM), unlike ERGMs, it does not suffer from degeneracy, a situation when a\nsignificant probability mass is placed on unrealistic graphs. We propose a\nparameter estimation approach for our model, and a procedure for drawing\nsamples from the distribution. We evaluate the performance of our approach both\non the small domain of all 8-node graphs as well as larger real-world social\nnetworks.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2011 20:23:45 GMT"}, {"version": "v2", "created": "Thu, 19 May 2011 03:26:10 GMT"}], "update_date": "2011-05-20", "authors_parsed": [["Lunga", "Dalton", ""], ["Kirshner", "Sergey", ""]]}, {"id": "1105.3007", "submitter": "Sokbae Lee", "authors": "Xiaohong Chen, Victor Chernozhukov, Sokbae Lee, Whitney K. Newey", "title": "Local Identification of Nonparametric and Semiparametric Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In parametric, nonlinear structural models a classical sufficient condition\nfor local identification, like Fisher (1966) and Rothenberg (1971), is that the\nvector of moment conditions is differentiable at the true parameter with full\nrank derivative matrix. We derive an analogous result for the nonparametric,\nnonlinear structural models, establishing conditions under which an\ninfinite-dimensional analog of the full rank condition is sufficient for local\nidentification. Importantly, we show that additional conditions are often\nneeded in nonlinear, nonparametric models to avoid nonlinearities overwhelming\nlinear effects. We give restrictions on a neighborhood of the true value that\nare sufficient for local identification. We apply these results to obtain new,\nprimitive identification conditions in several important models, including\nnonseparable quantile instrumental variable (IV) models, single-index IV\nmodels, and semiparametric consumption-based asset pricing models.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2011 06:18:00 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2012 16:36:41 GMT"}, {"version": "v3", "created": "Fri, 22 Mar 2013 04:39:42 GMT"}, {"version": "v4", "created": "Wed, 8 May 2013 15:37:49 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Chen", "Xiaohong", ""], ["Chernozhukov", "Victor", ""], ["Lee", "Sokbae", ""], ["Newey", "Whitney K.", ""]]}, {"id": "1105.3169", "submitter": "Ali Arab", "authors": "Ali Arab, Scott H. Holan, Christopher K. Wikle, Mark L. Wildhaber", "title": "Semiparametric Bivariate Zero-Inflated Poisson Models with Application\n  to Studies of Abundance for Multiple Species", "comments": "25 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ecological studies involving counts of abundance, presence-absence or\noccupancy rates often produce data having a substantial proportion of zeros.\nFurthermore, these types of processes are typically multivariate and only\nadequately described by complex nonlinear relationships involving externally\nmeasured covariates. Ignoring these aspects of the data and implementing\nstandard approaches can lead to models that fail to provide adequate scientific\nunderstanding of the underlying ecological processes, possibly resulting in a\nloss of inferential power. One method of dealing with data having excess zeros\nis to consider the class of univariate zero-inflated generalized linear models.\nHowever, this class of models fails to address the multivariate and nonlinear\naspects associated with the data usually encountered in practice. Therefore, we\npropose a semiparametric bivariate zero-inflated Poisson model that takes into\naccount both of these data attributes. The general modeling framework is\nhierarchical Bayes and is suitable for a broad range of applications. We\ndemonstrate the effectiveness of our model through a motivating example on\nmodeling catch per unit area for multiple species using data from the Missouri\nRiver benthic fish study, implemented by the United States Geological Survey.\n", "versions": [{"version": "v1", "created": "Mon, 16 May 2011 18:30:13 GMT"}], "update_date": "2011-05-17", "authors_parsed": [["Arab", "Ali", ""], ["Holan", "Scott H.", ""], ["Wikle", "Christopher K.", ""], ["Wildhaber", "Mark L.", ""]]}, {"id": "1105.3280", "submitter": "Jay Bartroff", "authors": "Jay Bartroff and Tze Leung Lai", "title": "Efficient adaptive designs with mid-course sample size adjustment in\n  clinical trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive designs have been proposed for clinical trials in which the nuisance\nparameters or alternative of interest are unknown or likely to be misspecified\nbefore the trial. Whereas most previous works on adaptive designs and\nmid-course sample size re-estimation have focused on two-stage or group\nsequential designs in the normal case, we consider here a new approach that\ninvolves at most three stages and is developed in the general framework of\nmultiparameter exponential families. Not only does this approach maintain the\nprescribed type I error probability, but it also provides a simple but\nasymptotically efficient sequential test whose finite-sample performance,\nmeasured in terms of the expected sample size and power functions, is shown to\nbe comparable to the optimal sequential design, determined by dynamic\nprogramming, in the simplified normal mean case with known variance and\nprespecified alternative, and superior to the existing two-stage designs and\nalso to adaptive group sequential designs when the alternative or nuisance\nparameters are unknown or misspecified.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2011 05:28:39 GMT"}], "update_date": "2011-05-18", "authors_parsed": [["Bartroff", "Jay", ""], ["Lai", "Tze Leung", ""]]}, {"id": "1105.3430", "submitter": "Pierre-Andr\\'e Cornillon", "authors": "P. A. Cornillon, N. Hengartner, E. Matzner-L{\\o}ber", "title": "Recursive bias estimation for multivariate regression smoothers", "comments": "26 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a practical and simple fully nonparametric multivariate\nsmoothing procedure that adapts to the underlying smoothness of the true\nregression function. Our estimator is easily computed by successive application\nof existing base smoothers (without the need of selecting an optimal smoothing\nparameter), such as thin-plate spline or kernel smoothers. The resulting\nsmoother has better out of sample predictive capabilities than the underlying\nbase smoother, or competing structurally constrained models (GAM) for small\ndimension (3 < d < 8) and moderate sample size (n < 800). Moreover our\nestimator is still useful when (d> 10) and to our knowledge, no other adaptive\nfully nonparametric regression estimator is available without constrained\nassumption such as additivity for example. On a real example, the Boston\nHousing Data, our method reduces the out of sample prediction error by 20 %. An\nR package ibr, available at CRAN, implements the proposed multivariate\nnonparametric method in R.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2011 16:50:49 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2011 09:13:26 GMT"}], "update_date": "2011-06-08", "authors_parsed": [["Cornillon", "P. A.", ""], ["Hengartner", "N.", ""], ["Matzner-L\u00f8ber", "E.", ""]]}, {"id": "1105.3638", "submitter": "Hamdi Raissi", "authors": "Valentin Patilea, Hamdi Ra\\\"issi", "title": "Corrected portmanteau tests for VAR models with time-varying variance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The problem of test of fit for Vector AutoRegressive (VAR) processes with\nunconditionally heteroscedastic errors is studied. The volatility structure is\ndeterministic but time-varying and allows for changes that are commonly\nobserved in economic or financial multivariate series. Our analysis is based on\nthe residual autocovariances and autocorrelations obtained from Ordinary Least\nSquares (OLS), Generalized Least Squares (GLS)and Adaptive Least Squares (ALS)\nestimation of the autoregressive parameters. The ALS approach is the GLS\napproach adapted to the unknown time-varying volatility that is then estimated\nby kernel smoothing. The properties of the three types of residual\nautocovariances and autocorrelations are derived. In particular it is shown\nthat the ALS and GLS residual autocorrelations are asymptotically equivalent.\nIt is also found that the asymptotic distribution of the OLS residual\nautocorrelations can be quite different from the standard chi-square asymptotic\ndistribution obtained in a correctly specified VAR model with iid innovations.\nAs a consequence the standard portmanteau tests are unreliable in our\nframework. The correct critical values of the standard portmanteau tests based\non the OLS residuals are derived. Moreover, modified portmanteau statistics\nbased on ALS residual autocorrelations are introduced. Portmanteau tests with\nmodified statistics based on OLS and ALS residuals and standard chi-square\nasymptotic distributions under the null hypothesis are also proposed. An\nextension of our portmanteau approaches to testing the lag length in a vector\nerror correction type model for co-integrating relations is briefly\ninvestigated. The finite sample properties of the goodness-of-fit tests we\nconsider are investigated by Monte Carlo experiments. The theoretical results\nare also illustrated using two U.S. economic data sets.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2011 14:09:25 GMT"}, {"version": "v2", "created": "Tue, 31 May 2011 10:17:37 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Patilea", "Valentin", ""], ["Ra\u00efssi", "Hamdi", ""]]}, {"id": "1105.4292", "submitter": "Jianqing Fan", "authors": "Jianqing Fan, Yuan Liao, Martina Mincheva", "title": "High-dimensional covariance matrix estimation in approximate factor\n  models", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS944 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 6, 3320-3356", "doi": "10.1214/11-AOS944", "report-no": "IMS-AOS-AOS944", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variance--covariance matrix plays a central role in the inferential\ntheories of high-dimensional factor models in finance and economics. Popular\nregularization methods of directly exploiting sparsity are not directly\napplicable to many financial problems. Classical methods of estimating the\ncovariance matrices are based on the strict factor models, assuming independent\nidiosyncratic components. This assumption, however, is restrictive in practical\napplications. By assuming sparse error covariance matrix, we allow the presence\nof the cross-sectional correlation even after taking out common factors, and it\nenables us to combine the merits of both methods. We estimate the sparse\ncovariance using the adaptive thresholding technique as in Cai and Liu [J.\nAmer. Statist. Assoc. 106 (2011) 672--684], taking into account the fact that\ndirect observations of the idiosyncratic components are unavailable. The impact\nof high dimensionality on the covariance matrix estimation based on the factor\nstructure is then studied.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2011 21:28:11 GMT"}, {"version": "v2", "created": "Wed, 14 Sep 2011 14:34:12 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2012 14:59:28 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Fan", "Jianqing", ""], ["Liao", "Yuan", ""], ["Mincheva", "Martina", ""]]}, {"id": "1105.4519", "submitter": "Laurent Calvet", "authors": "Laurent E. Calvet and Veronika Czellar", "title": "State-Observation Sampling and the Econometrics of Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nonlinear state-space models, sequential learning about the hidden state\ncan proceed by particle filtering when the density of the observation\nconditional on the state is available analytically (e.g. Gordon et al., 1993).\nThis condition need not hold in complex environments, such as the\nincomplete-information equilibrium models considered in financial economics. In\nthis paper, we make two contributions to the learning literature. First, we\nintroduce a new filtering method, the state-observation sampling (SOS) filter,\nfor general state-space models with intractable observation densities. Second,\nwe develop an indirect inference-based estimator for a large class of\nincomplete-information economies. We demonstrate the good performance of these\ntechniques on an asset pricing model with investor learning applied to over 80\nyears of daily equity returns.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2011 14:49:52 GMT"}], "update_date": "2011-05-24", "authors_parsed": [["Calvet", "Laurent E.", ""], ["Czellar", "Veronika", ""]]}, {"id": "1105.4667", "submitter": "Jay Bartroff", "authors": "Jay Bartroff and Tze Leung Lai", "title": "Generalized Likelihood Ratio Statistics and Uncertainty Adjustments in\n  Efficient Adaptive Design of Clinical Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new approach to adaptive design of clinical trials is proposed in a general\nmultiparameter exponential family setting, based on generalized likelihood\nratio statistics and optimal sequential testing theory. These designs are easy\nto implement, maintain the prescribed Type I error probability, and are\nasymptotically efficient. Practical issues involved in clinical trials allowing\nmid-course adaptation and the large literature on this subject are discussed,\nand comparisons between the proposed and existing designs are presented in\nextensive simulation studies of their finite-sample performance, measured in\nterms of the expected sample size and power functions.\n", "versions": [{"version": "v1", "created": "Tue, 24 May 2011 01:58:41 GMT"}], "update_date": "2011-05-25", "authors_parsed": [["Bartroff", "Jay", ""], ["Lai", "Tze Leung", ""]]}, {"id": "1105.5004", "submitter": "Cedric Ginestet", "authors": "Cedric E. Ginestet", "title": "Bayesian Decision-theoretic Methods for Parameter Ensembles with\n  Application to Epidemiology", "comments": "Imperial College London PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST q-bio.QM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter ensembles or sets of random effects constitute one of the\ncornerstones of modern statistical practice. This is especially the case in\nBayesian hierarchical models, where several decision theoretic frameworks can\nbe deployed. The estimation of these parameter ensembles may substantially vary\ndepending on which inferential goals are prioritised by the modeller. Since one\nmay wish to satisfy a range of desiderata, it is therefore of interest to\ninvestigate whether some sets of point estimates can simultaneously meet\nseveral inferential objectives. In this thesis, we will be especially concerned\nwith identifying ensembles of point estimates that produce good approximations\nof (i) the true empirical quantiles and empirical quartile ratio (QR) and (ii)\nprovide an accurate classification of the ensemble's elements above and below a\ngiven threshold. For this purpose, we review various decision-theoretic\nframeworks, which have been proposed in the literature in relation to the\noptimisation of different aspects of the empirical distribution of a parameter\nensemble. This includes the constrained Bayes (CB), weighted-rank squared error\nloss (WRSEL), and triple-goal (GR) ensembles of point estimates. In addition,\nwe also consider the set of maximum likelihood estimates (MLEs) and the\nensemble of posterior means --the latter being optimal under the summed squared\nerror loss (SSEL). Firstly, we test the performance of these different sets of\npoint estimates as plug-in estimators for the empirical quantiles and empirical\nQR under a range of synthetic scenarios encompassing both spatial and\nnon-spatial simulated data sets. Performance evaluation is here conducted using\nthe posterior regret. Secondly, two threshold classification losses (TCLs)\n--weighted and unweighted-- are formulated and formally optimised. The\nperformance of these decision-theoretic tools is also evaluated on real data\nsets.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2011 12:37:41 GMT"}, {"version": "v2", "created": "Fri, 27 May 2011 12:37:10 GMT"}, {"version": "v3", "created": "Tue, 31 May 2011 15:56:21 GMT"}, {"version": "v4", "created": "Thu, 9 Jun 2011 14:12:32 GMT"}, {"version": "v5", "created": "Fri, 1 Jul 2011 15:01:58 GMT"}, {"version": "v6", "created": "Tue, 18 Mar 2014 15:28:53 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Ginestet", "Cedric E.", ""]]}, {"id": "1105.5065", "submitter": "Enrique Alvarez", "authors": "Enrique E. \\'Alvarez and V\\'ictor J. Yohai", "title": "M-estimators for Isotonic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a family of robust estimates for isotonic\nregression: isotonic M-estimators. We show that their asymptotic distribution\nis, up to an scalar factor, the same as that of Brunk's classical isotonic\nestimator. We also derive the influence function and the breakdown point of\nthese estimates. Finally we perform a Monte Carlo study that shows that the\nproposed family includes estimators that are simultaneously highly efficient\nunder gaussian errors and highly robust when the error distribution has heavy\ntails.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2011 15:36:42 GMT"}], "update_date": "2011-05-26", "authors_parsed": [["\u00c1lvarez", "Enrique E.", ""], ["Yohai", "V\u00edctor J.", ""]]}, {"id": "1105.5250", "submitter": "Fabian Scheipl", "authors": "Fabian Scheipl, Ludwig Fahrmeir, Thomas Kneib", "title": "Spike-and-Slab Priors for Function Selection in Structured Additive\n  Regression Models", "comments": null, "journal-ref": "Journal of the American Statistical Association (2012), 107:500,\n  pages 1518--1532", "doi": "10.1080/01621459.2012.737742", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured additive regression provides a general framework for complex\nGaussian and non-Gaussian regression models, with predictors comprising\narbitrary combinations of nonlinear functions and surfaces, spatial effects,\nvarying coefficients, random effects and further regression terms. The large\nflexibility of structured additive regression makes function selection a\nchallenging and important task, aiming at (1) selecting the relevant\ncovariates, (2) choosing an appropriate and parsimonious representation of the\nimpact of covariates on the predictor and (3) determining the required\ninteractions. We propose a spike-and-slab prior structure for function\nselection that allows to include or exclude single coefficients as well as\nblocks of coefficients representing specific model terms. A novel\nmultiplicative parameter expansion is required to obtain good mixing and\nconvergence properties in a Markov chain Monte Carlo simulation approach and is\nshown to induce desirable shrinkage properties. In simulation studies and with\n(real) benchmark classification data, we investigate sensitivity to\nhyperparameter settings and compare performance to competitors. The flexibility\nand applicability of our approach are demonstrated in an additive piecewise\nexponential model with time-varying effects for right-censored survival times\nof intensive care patients with sepsis. Geoadditive and additive mixed logit\nmodel applications are discussed in an extensive appendix.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2011 10:34:22 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2011 16:54:59 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Scheipl", "Fabian", ""], ["Fahrmeir", "Ludwig", ""], ["Kneib", "Thomas", ""]]}, {"id": "1105.5253", "submitter": "Fabian Scheipl", "authors": "Fabian Scheipl", "title": "spikeSlabGAM: Bayesian Variable Selection, Model Choice and\n  Regularization for Generalized Additive Mixed Models in R", "comments": null, "journal-ref": "Journal of Statistical Software 2011 43(14) 1--24", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The R package spikeSlabGAM implements Bayesian variable selection, model\nchoice, and regularized estimation in (geo-)additive mixed models for Gaussian,\nbinomial, and Poisson responses. Its purpose is to (1) choose an appropriate\nsubset of potential covariates and their interactions, (2) to determine whether\nlinear or more flexible functional forms are required to model the effects of\nthe respective covariates, and (3) to estimate their shapes. Selection and\nregularization of the model terms is based on a novel spike-and-slab-type prior\non coefficient groups associated with parametric and semi-parametric effects.\n", "versions": [{"version": "v1", "created": "Thu, 26 May 2011 10:46:36 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Scheipl", "Fabian", ""]]}, {"id": "1105.5565", "submitter": "Elie Maza", "authors": "Chlo\\'e Dimeglio (IMT), Jean-Michel Loubes (IMT), Elie Maza (GBF)", "title": "Manifold embedding for curve registration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the problem of finding a good representative of a sample of\nrandom curves warped from a common pattern f. We first prove that such a\nproblem can be moved onto a manifold framework. Then, we propose an estimation\nof the common pattern f based on an approximated geodesic distance on a\nsuitable manifold. We then compare the proposed method to more classical\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 14:13:43 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Dimeglio", "Chlo\u00e9", "", "IMT"], ["Loubes", "Jean-Michel", "", "IMT"], ["Maza", "Elie", "", "GBF"]]}, {"id": "1105.5737", "submitter": "Garritt Page", "authors": "Abhishek Bhattacharya, Garritt Page, David Dunson", "title": "Density Estimation and Classification via Bayesian Nonparametric\n  Learning of Affine Subspaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is now practically the norm for data to be very high dimensional in areas\nsuch as genetics, machine vision, image analysis and many others. When\nanalyzing such data, parametric models are often too inflexible while\nnonparametric procedures tend to be non-robust because of insufficient data on\nthese high dimensional spaces. It is often the case with high-dimensional data\nthat most of the variability tends to be along a few directions, or more\ngenerally along a much smaller dimensional submanifold of the data space. In\nthis article, we propose a class of models that flexibly learn about this\nsubmanifold and its dimension which simultaneously performs dimension\nreduction. As a result, density estimation is carried out efficiently. When\nperforming classification with a large predictor space, our approach allows the\ncategory probabilities to vary nonparametrically with a few features expressed\nas linear combinations of the predictors. As opposed to many black-box methods\nfor dimensionality reduction, the proposed model is appealing in having clearly\ninterpretable and identifiable parameters. Gibbs sampling methods are developed\nfor posterior computation, and the methods are illustrated in simulated and\nreal data applications.\n", "versions": [{"version": "v1", "created": "Sat, 28 May 2011 20:11:25 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Bhattacharya", "Abhishek", ""], ["Page", "Garritt", ""], ["Dunson", "David", ""]]}, {"id": "1105.5850", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Mark Briers, Pavel V. Shevchenko and Arnaud Doucet", "title": "Calibration and filtering for multi factor commodity models with\n  seasonality: incorporating panel data from futures contracts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.ST stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a general multi-factor model for commodity spot prices and futures\nvaluation. We extend the multi-factor long-short model in Schwartz and Smith\n(2000) and Yan (2002) in two important aspects: firstly we allow for both the\nlong and short term dynamic factors to be mean reverting incorporating\nstochastic volatility factors and secondly we develop an additive structural\nseasonality model. Then a Milstein discretized non-linear stochastic volatility\nstate space representation for the model is developed which allows for futures\nand options contracts in the observation equation. We then develop numerical\nmethodology based on an advanced Sequential Monte Carlo algorithm utilising\nParticle Markov chain Monte Carlo to perform calibration of the model jointly\nwith the filtering of the latent processes for the long-short dynamics and\nvolatility factors. In this regard we explore and develop a novel methodology\nbased on an adaptive Rao-Blackwellised version of the Particle Markov chain\nMonte Carlo methodology. In doing this we deal accurately with the\nnon-linearities in the state-space model which are therefore introduced into\nthe filtering framework. We perform analysis on synthetic and real data for oil\ncommodities.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 02:55:30 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Peters", "Gareth W.", ""], ["Briers", "Mark", ""], ["Shevchenko", "Pavel V.", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1105.5945", "submitter": "Beatriz Pateiro-L\\'opez", "authors": "Beatriz Pateiro-L\\'opez and Alberto Rodr\\'iguez Casal", "title": "Recovering the shape of a point cloud in the plane", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we deal with the problem of support estimation under shape\nrestrictions. The shape restriction we deal with is an extension of the notion\nof convexity named alpha-convexity. Instead of assuming, as in the convex case,\nthe existence of a separating hyperplane for each exterior point we assume the\nexistence of a separating open ball with radius alpha. Given an alpha-convex\nset S, the alpha-convex hull of independent random points in S is the natural\nestimator of the set. If alpha is unknown the r_n-convex hull of the sample can\nbe considered. We analyze the asymptotic properties of the r_n-convex hull\nestimator in the bidimensional case and obtain the convergence rate for the\nexpected distance in measure between the set and the estimator. The geometrical\ncomplexity of the estimator and its dependence on r_n is also obtained via the\nanalysis of the expected number of vertices of the r_n-convex hull.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 11:19:20 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Pateiro-L\u00f3pez", "Beatriz", ""], ["Casal", "Alberto Rodr\u00edguez", ""]]}, {"id": "1105.6075", "submitter": "Robin Evans", "authors": "Robin J. Evans and Thomas S. Richardson", "title": "Marginal log-linear parameters for graphical Markov models", "comments": "36 pages", "journal-ref": "Journal of the Royal Statistical Society, Series B. 75 (4)\n  743-768, 2013", "doi": "10.1111/rssb.12020", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal log-linear (MLL) models provide a flexible approach to multivariate\ndiscrete data. MLL parametrizations under linear constraints induce a wide\nvariety of models, including models defined by conditional independences. We\nintroduce a sub-class of MLL models which correspond to Acyclic Directed Mixed\nGraphs (ADMGs) under the usual global Markov property. We characterize for\nprecisely which graphs the resulting parametrization is variation independent.\nThe MLL approach provides the first description of ADMG models in terms of a\nminimal list of constraints. The parametrization is also easily adapted to\nsparse modelling techniques, which we illustrate using several examples of real\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 19:15:21 GMT"}, {"version": "v2", "created": "Tue, 8 May 2012 15:16:33 GMT"}, {"version": "v3", "created": "Wed, 31 Oct 2012 16:04:45 GMT"}], "update_date": "2013-08-16", "authors_parsed": [["Evans", "Robin J.", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1105.6077", "submitter": "Brahimi Brahim", "authors": "Brahim Brahimi and Abdelhakim Necir", "title": "A semiparametric estimation of copula models based on the method of\n  moments", "comments": "Accepted paper in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the classical estimation method of moments, we propose a new\nsemiparametric estimation procedure for multi-parameter copula models.\nConsistency and asymptotic normality of the obtained estimators are\nestablished. By considering an Archimedean copula model, an extensive\nsimulation study, comparing these estimators with the pseudo maximum\nlikelihood, rho-inversion and tau-inversion ones, is carried out. We show that,\nwith regards to the other methods, the moment based estimation is quick and\nsimple to use with reasonable bias and root mean squared error.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 19:23:54 GMT"}, {"version": "v2", "created": "Sat, 7 Jan 2012 17:52:16 GMT"}], "update_date": "2012-01-10", "authors_parsed": [["Brahimi", "Brahim", ""], ["Necir", "Abdelhakim", ""]]}, {"id": "1105.6154", "submitter": "Ivan Fernandez-Val", "authors": "Alexandre Belloni, Victor Chernozhukov, Denis Chetverikov and Iv\\'an\n  Fern\\'andez-Val", "title": "Conditional Quantile Processes based on Series or Many Regressors", "comments": "131 pages, 2 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantile regression (QR) is a principal regression method for analyzing the\nimpact of covariates on outcomes. The impact is described by the conditional\nquantile function and its functionals. In this paper we develop the\nnonparametric QR-series framework, covering many regressors as a special case,\nfor performing inference on the entire conditional quantile function and its\nlinear functionals. In this framework, we approximate the entire conditional\nquantile function by a linear combination of series terms with\nquantile-specific coefficients and estimate the function-valued coefficients\nfrom the data. We develop large sample theory for the QR-series coefficient\nprocess, namely we obtain uniform strong approximations to the QR-series\ncoefficient process by conditionally pivotal and Gaussian processes. Based on\nthese strong approximations, or couplings, we develop four resampling methods\n(pivotal, gradient bootstrap, Gaussian, and weighted bootstrap) that can be\nused for inference on the entire QR-series coefficient function.\n  We apply these results to obtain estimation and inference methods for linear\nfunctionals of the conditional quantile function, such as the conditional\nquantile function itself, its partial derivatives, average partial derivatives,\nand conditional average partial derivatives. Specifically, we obtain uniform\nrates of convergence and show how to use the four resampling methods mentioned\nabove for inference on the functionals. All of the above results are for\nfunction-valued parameters, holding uniformly in both the quantile index and\nthe covariate value, and covering the pointwise case as a by-product. We\ndemonstrate the practical utility of these results with an example, where we\nestimate the price elasticity function and test the Slutsky condition of the\nindividual demand for gasoline, as indexed by the individual unobserved\npropensity for gasoline consumption.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2011 03:15:37 GMT"}, {"version": "v2", "created": "Sun, 28 Aug 2016 21:08:18 GMT"}, {"version": "v3", "created": "Fri, 28 Jul 2017 21:00:12 GMT"}, {"version": "v4", "created": "Thu, 9 Aug 2018 18:26:55 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Chetverikov", "Denis", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""]]}, {"id": "1105.6245", "submitter": "Edoardo Airoldi", "authors": "Edoardo M. Airoldi, David S. Choi, Patrick J. Wolfe", "title": "Confidence sets for network structure", "comments": "17 pages, 3 figures, 3 tables", "journal-ref": "Statistical Analysis and Data Mining, vol. 4, pp. 461-469, 2011", "doi": "10.1002/sam.10136", "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent variable models are frequently used to identify structure in\ndichotomous network data, in part because they give rise to a Bernoulli product\nlikelihood that is both well understood and consistent with the notion of\nexchangeable random graphs. In this article we propose conservative confidence\nsets that hold with respect to these underlying Bernoulli parameters as a\nfunction of any given partition of network nodes, enabling us to assess\nestimates of 'residual' network structure, that is, structure that cannot be\nexplained by known covariates and thus cannot be easily verified by manual\ninspection. We demonstrate the proposed methodology by analyzing student\nfriendship networks from the National Longitudinal Survey of Adolescent Health\nthat include race, gender, and school year as covariates. We employ a\nstochastic expectation-maximization algorithm to fit a logistic regression\nmodel that includes these explanatory variables as well as a latent stochastic\nblockmodel component and additional node-specific effects. Although\nmaximum-likelihood estimates do not appear consistent in this context, we are\nable to evaluate confidence sets as a function of different blockmodel\npartitions, which enables us to qualitatively assess the significance of\nestimated residual network structure relative to a baseline, which models\ncovariates but lacks block structure.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2011 11:49:04 GMT"}], "update_date": "2012-02-13", "authors_parsed": [["Airoldi", "Edoardo M.", ""], ["Choi", "David S.", ""], ["Wolfe", "Patrick J.", ""]]}, {"id": "1105.6322", "submitter": "Cedric Ginestet", "authors": "Cedric E. Ginestet and Nicky G. Best and Sylvia Richardson", "title": "Classification Loss Function for Parameter Ensembles in Bayesian\n  Hierarchical Models", "comments": "Submitted to Probability and Statistics Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter ensembles or sets of point estimates constitute one of the\ncornerstones of modern statistical practice. This is especially the case in\nBayesian hierarchical models, where different decision-theoretic frameworks can\nbe deployed to summarize such parameter ensembles. The estimation of these\nparameter ensembles may thus substantially vary depending on which inferential\ngoals are prioritised by the modeller. In this note, we consider the problem of\nclassifying the elements of a parameter ensemble above or below a given\nthreshold. Two threshold classification losses (TCLs) --weighted and\nunweighted-- are formulated. The weighted TCL can be used to emphasize the\nestimation of false positives over false negatives or the converse. We prove\nthat the weighted and unweighted TCLs are optimized by the ensembles of\nunit-specific posterior quantiles and posterior medians, respectively. In\naddition, we relate these classification loss functions on parameter ensembles\nto the concepts of posterior sensitivity and specificity. Finally, we find some\nrelationships between the unweighted TCL and the absolute value loss, which\nexplain why both functions are minimized by posterior medians.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2011 15:54:02 GMT"}, {"version": "v2", "created": "Thu, 9 Jun 2011 14:15:23 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Ginestet", "Cedric E.", ""], ["Best", "Nicky G.", ""], ["Richardson", "Sylvia", ""]]}]