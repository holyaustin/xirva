[{"id": "1807.00135", "submitter": "Ioannis Kalogridis Mr", "authors": "Ioannis Kalogridis and Stefan Van Aelst", "title": "Robust functional regression based on principal components", "comments": "33 pages, including the appendix and references", "journal-ref": "Journal of Multivariate Analysis, 173, 2019, 393-415", "doi": "10.1016/j.jmva.2019.04.003", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis is a fast evolving branch of modern statistics and\nthe functional linear model has become popular in recent years. However, most\nestimation methods for this model rely on generalized least squares procedures\nand therefore are sensitive to atypical observations. To remedy this, we\npropose a two-step estimation procedure that combines robust functional\nprincipal components and robust linear regression. Moreover, we propose a\ntransformation that reduces the curvature of the estimators and can be\nadvantageous in many settings. For these estimators we prove Fisher-consistency\nat elliptical distributions and consistency under mild regularity conditions.\nThe influence function of the estimators is investigated as well. Simulation\nexperiments show that the proposed estimators have reasonable efficiency,\nprotect against outlying observations, produce smooth estimates and perform\nwell in comparison to existing approaches.\n", "versions": [{"version": "v1", "created": "Sat, 30 Jun 2018 07:51:30 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 11:29:14 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2018 14:03:00 GMT"}], "update_date": "2020-06-24", "authors_parsed": [["Kalogridis", "Ioannis", ""], ["Van Aelst", "Stefan", ""]]}, {"id": "1807.00282", "submitter": "Axel B\\\"ucher", "authors": "Axel B\\\"ucher and Chen Zhou", "title": "A horse racing between the block maxima method and the\n  peak-over-threshold approach", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical extreme value statistics consists of two fundamental approaches:\nthe block maxima (BM) method and the peak-over-threshold (POT) approach. It\nseems to be general consensus among researchers in the field that the POT\nmethod makes use of extreme observations more efficiently than the BM method.\nWe shed light on this discussion from three different perspectives. First,\nbased on recent theoretical results for the BM approach, we provide a\ntheoretical comparison in i.i.d.\\ scenarios. We argue that the data generating\nprocess may favour either one or the other approach. Second, if the underlying\ndata possesses serial dependence, we argue that the choice of a method should\nbe primarily guided by the ultimate statistical interest: for instance, POT is\npreferable for quantile estimation, while BM is preferable for return level\nestimation. Finally, we discuss the two approaches for multivariate\nobservations and identify various open ends for future research.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 07:05:23 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["B\u00fccher", "Axel", ""], ["Zhou", "Chen", ""]]}, {"id": "1807.00305", "submitter": "Olivier Binette", "authors": "Olivier Binette and Simon Guillotte", "title": "Bayesian Nonparametrics for Directional Statistics", "comments": "29 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a density basis of the trigonometric polynomials that is\nsuitable to mixture modelling. Statistical and geometric properties are\nderived, suggesting it as a circular analogue to the Bernstein polynomial\ndensities. Nonparametric priors are constructed using this basis and a\nsimulation study shows that the use of the resulting Bayes estimator may\nprovide gains over comparable circular density estimators previously suggested\nin the literature. From a theoretical point of view, we propose a general prior\nspecification framework for density estimation on compact metric space using\nsieve priors. This is tailored to density bases such as the one considered\nherein and may also be used to exploit their particular shape-preserving\nproperties. Furthermore, strong posterior consistency is shown to hold under\nnotably weak regularity assumptions and adaptative convergence rates are\nobtained in terms of the approximation properties of positive linear operators\ngenerating our models.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 10:12:38 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 19:09:19 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 16:48:54 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Binette", "Olivier", ""], ["Guillotte", "Simon", ""]]}, {"id": "1807.00347", "submitter": "Edgar Dobriban", "authors": "Edgar Dobriban, Weijie J. Su", "title": "Robust Inference Under Heteroskedasticity via the Hadamard Estimator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing statistical inferences from large datasets in a model-robust way is\nan important problem in statistics and data science. In this paper, we propose\nmethods that are robust to large and unequal noise in different observational\nunits (i.e., heteroskedasticity) for statistical inference in linear\nregression. We leverage the Hadamard estimator, which is unbiased for the\nvariances of ordinary least-squares regression. This is in contrast to the\npopular White's sandwich estimator, which can be substantially biased in high\ndimensions. We propose to estimate the signal strength, noise level,\nsignal-to-noise ratio, and mean squared error via the Hadamard estimator. We\ndevelop a new degrees of freedom adjustment that gives more accurate confidence\nintervals than variants of White's sandwich estimator. Moreover, we provide\nconditions ensuring the estimator is well-defined, by studying a new random\nmatrix ensemble in which the entries of a random orthogonal projection matrix\nare squared. We also show approximate normality, using the second-order\nPoincare inequality. Our work provides improved statistical theory and methods\nfor linear regression in high dimensions.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 15:55:25 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Dobriban", "Edgar", ""], ["Su", "Weijie J.", ""]]}, {"id": "1807.00359", "submitter": "Mats Julius Stensrud", "authors": "Mats Julius Stensrud, Kjetil R{\\o}ysland and P{\\aa}l Christie Ryalen", "title": "On null hypotheses in survival analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conventional nonparametric tests in survival analysis, such as the\nlog-rank test, assess the null hypothesis that the hazards are equal at all\ntimes. However, hazards are hard to interpret causally, and other null\nhypotheses are more relevant in many scenarios with survival outcomes. To allow\nfor a wider range of null hypotheses, we present a generic approach to define\ntest statistics. This approach utilizes the fact that a wide range of common\nparameters in survival analysis can be expressed as solutions of differential\nequations. Thereby we can test hypotheses based on survival parameters that\nsolve differential equations driven by cumulative hazards, and it is easy to\nimplement the tests on a computer. We present simulations, suggesting that our\ntests perform well for several hypotheses in a range of scenarios. Finally, we\nuse our tests to evaluate the effect of adjuvant chemotherapies in patients\nwith colon cancer, using data from a randomised controlled trial.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 17:18:27 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 02:30:24 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Stensrud", "Mats Julius", ""], ["R\u00f8ysland", "Kjetil", ""], ["Ryalen", "P\u00e5l Christie", ""]]}, {"id": "1807.00365", "submitter": "Ilya Novikov D.", "authors": "Ilya Novikov", "title": "Calculation of sample size guaranteeing the required width of the\n  empirical confidence interval with predefined probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of any estimation study is an interval estimation of a the\nparameter(s) of interest. These estimations are mostly expressed using\nempirical confidence intervals that are based on sample point estimates of the\ncorresponding parameter(s). In contrast, calculations of the necessary sample\nsize usually use expected confidence intervals that are based on the expected\nvalue of the parameter(s). The approach that guarantees the required\nprobability of the required width of empirical confidence interval is known at\nleast since 1989. However, till now, this approach is not implemented for most\nsoftware and is not even described in many modern papers and textbooks. Here we\npresent the concise description of the approach to sample size calculation for\nobtaining empirical confidence interval of the required width with the\npredefined probability and give a framework of its general implementation. We\nillustrate the approach in Normal, Poisson, and Binomial distributions. The\nnumeric results showed that the sample size necessary to obtain the required\nwidth of empirical confidence interval with the standard probability of $0.8$\nor $0.9$ may be more than 20\\% larger than the sample size calculated for the\nexpected values of the parameters.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 18:19:42 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Novikov", "Ilya", ""]]}, {"id": "1807.00615", "submitter": "Debasis Kundu Professor", "authors": "Deepak Prajapati and Sharmistha Mitra and Debasis Kundu", "title": "A new decision theoretic sampling plan for type-I and type-I hybrid\n  censored samples from the exponential distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study proposes a new decision theoretic sampling plan (DSP) for Type-I\nand Type-I hybrid censored samples when the lifetimes of individual items are\nexponentially distributed with a scale parameter. The DSP is based on an\nestimator of the scale parameter which always exists, unlike the MLE which may\nnot always exist. Using a quadratic loss function and a decision function based\non the proposed estimator, a DSP is derived. To obtain the optimum DSP, a\nfinite algorithm is used. Numerical results demonstrate that in terms of the\nBayes risk, the optimum DSP is as good as the Bayesian sampling plan (BSP)\nproposed by \\cite{lin2002bayesian} and \\cite{liang2013optimal}. The proposed\nDSP performs better than the sampling plan of \\cite{Lam1994bayesian} and\n\\cite{lin2008-10exact} in terms of Bayes risks. The main advantage of the\nproposed DSP is that for higher degree polynomial and non-polynomial loss\nfunctions, it can be easily obtained as compared to the BSP.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 12:07:47 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Prajapati", "Deepak", ""], ["Mitra", "Sharmistha", ""], ["Kundu", "Debasis", ""]]}, {"id": "1807.00810", "submitter": "Ingo Hoffmann", "authors": "Ingo Hoffmann and Christoph J. B\\\"orner", "title": "The risk function of the goodness-of-fit tests for tail models", "comments": "1 Figure. arXiv admin note: text overlap with arXiv:1805.10040", "journal-ref": "Statistical Papers 2020", "doi": "10.1007/s00362-020-01159-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper contributes to answering a question that is of crucial importance\nin risk management and extreme value theory: How to select the threshold above\nwhich one assumes that the tail of a distribution follows a generalized Pareto\ndistribution. This question has gained increasing attention, particularly in\nfinance institutions, as the recent regulative norms require the assessment of\nrisk at high quantiles. Recent methods answer this question by multiple uses of\nthe standard goodness-of-fit tests. These tests are based on a particular\nchoice of symmetric weighting of the mean square error between the empirical\nand the fitted tail distributions. Assuming an asymmetric weighting, which\nrates high quantiles more than small ones, we propose new goodness-of-fit tests\nand automated threshold selection procedures. We consider a parameterized\nfamily of asymmetric weight functions and calculate the corresponding mean\nsquare error as a loss function. We then explicitly determine the risk function\nas the finite sample expected value of the loss function. Finally, the risk\nfunction can be used to discuss the question of which symmetric or asymmetric\nweight function and, thus, which goodness-of-fit test should be used in a new\nmethod for determining the threshold value.\n", "versions": [{"version": "v1", "created": "Sun, 1 Jul 2018 04:53:59 GMT"}], "update_date": "2020-01-27", "authors_parsed": [["Hoffmann", "Ingo", ""], ["B\u00f6rner", "Christoph J.", ""]]}, {"id": "1807.00931", "submitter": "Yuxiang Xie", "authors": "Yuxiang Xie and Kwun Chuen Gary Chan", "title": "Controlling the False Discovery Rate for Binary Feature Selection via\n  Knockoff", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection has been widely used in data analysis for the past\ndecades, and it becomes increasingly important in the Big Data era as there are\nusually hundreds of variables available in a dataset. To enhance\ninterpretability of a model, identifying potentially relevant features is often\na step before fitting all the features into a regression model. A good variable\nselection method should effectively control the fraction of false discoveries\nand ensure large enough power of its selection set. In a lot of contemporary\ndata applications, a great portion of features are coded as binary variables.\nBinary features are widespread in many fields, from online controlled\nexperiments to genome science to physical statistics. Although there has\nrecently been a handful of literature for provable false discovery rate (FDR)\ncontrol in variable selection, most of the theoretical analyses were based on\nsome strong dependency assumption or Gaussian assumption among features. In\nthis paper we propose a variable selection method in regression framework for\nselecting binary features. Under mild conditions, we show that FDR is\ncontrolled exactly under a target level in a finite sample if the underlying\ndistribution of the binary features is known. We show in simulations that FDR\ncontrol is still attained when feature distribution is estimated from data. We\nalso provide theoretical results on the power of our variables selection method\nin a linear regression model or a logistic regression model. In the restricted\nsettings where competitors exist, we show in simulations and real data\napplication on a HIV antiretroviral therapy dataset that our method has higher\npower than the competitor.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 00:07:03 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 14:14:50 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2020 18:44:47 GMT"}], "update_date": "2020-08-17", "authors_parsed": [["Xie", "Yuxiang", ""], ["Chan", "Kwun Chuen Gary", ""]]}, {"id": "1807.00943", "submitter": "Qunhua Li", "authors": "Feipeng Zhang, Frank Shen, Tao Yang and Qunhua Li", "title": "Segmented correspondence curve regression model for quantifying\n  reproducibility of high-throughput experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reliability of a high-throughput biological experiment relies highly on\nthe settings of the operational factors in its experimental and data-analytic\nprocedures. Understanding how operational factors influence the reproducibility\nof the experimental outcome is critical for constructing robust workflows and\nobtaining reliable results. One challenge in this area is that candidates at\ndifferent levels of significance may respond to the operational factors\ndifferently. To model this heterogeneity, we develop a novel segmented\nregression model, based on the rank concordance between candidates from\ndifferent replicate samples, to characterize the varying effects of operational\nfactors for candidates at different levels of significance. A grid search\nmethod is developed to identify the change point in response to the operational\nfactors and estimate the covariate effects accounting for the change. A\nsup-likelihood-ratio-type test is proposed to test the existence of a change\npoint. Simulation studies show that our method yields a well-calibrated type I\nerror, is powerful in detecting the difference in reproducibility, and achieves\na better model fitting than the existing method. An application on a ChIP-seq\ndataset reveals interesting insights on how sequencing depth affects the\nreproducibility of experimental results, demonstrating the usefulness of our\nmethod in designing cost-effective and reliable high-throughput workflows.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 01:16:42 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Zhang", "Feipeng", ""], ["Shen", "Frank", ""], ["Yang", "Tao", ""], ["Li", "Qunhua", ""]]}, {"id": "1807.01111", "submitter": "Mahendra Saha", "authors": "Abhimanyu Singh Yadav, Sudhansu S. Maiti, Mahendra Saha and Arvind\n  Pandey", "title": "The inverse xgamma distribution: statistical properties and different\n  methods of estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  This paper proposed a new probability distribution named as inverse xgamma\ndistribution (IXGD). Different mathematical and statistical properties,viz.,\nreliability characteristics, moments, inverse moments, stochastic ordering and\norder statistics of the proposed distribution have been derived and discussed.\nThe estimation of the parameter of IXGD has been approached by different\nmethods of estimation, namely, maximum likelihood method of estimation (MLE),\nLeast square method of estimation (LSE), Weighted least square method of\nestimation (WLSE), Cram'er-von-Mises method of estimation (CME) and maximum\nproduct spacing method of estimation (MPSE). Asymptotic confidence interval\n(ACI) of the parameter is also obtained. A simulation study has been carried\nout to compare the performance of the obtained estimators and corresponding ACI\nin terms of average widths and corresponding coverage probabilities. Finally,\ntwo real data sets have been used to demonstrate the applicability of IXGD in\nreal life situations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 12:19:34 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Yadav", "Abhimanyu Singh", ""], ["Maiti", "Sudhansu S.", ""], ["Saha", "Mahendra", ""], ["Pandey", "Arvind", ""]]}, {"id": "1807.01133", "submitter": "Jonas Krampe", "authors": "Jonas Krampe", "title": "Time Series Modeling on Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on modeling the dynamic attributes of a dynamic network\nwith a fixed number of vertices. These attributes are considered as time series\nwhich dependency structure is influenced by the underlying network. They are\nmodeled by a multivariate doubly stochastic time series framework, that is we\nassume linear processes for which the coefficient matrices are stochastic\nprocesses themselves. We explicitly allow for dependence in the dynamics of the\ncoefficient matrices as well as between these two stochastic processes. This\nframework allows for a separate modeling of the attributes and the underlying\nnetwork. In this setting, we define network autoregressive models and discuss\ntheir stationarity conditions. Furthermore, an estimation approach is discussed\nin a low- and high-dimensional setting and how this can be applied to\nforecasting. The finite sample behavior of the forecast approach is\ninvestigated. This approach is applied to real data whereby the goal is to\nforecast the GDP of 33 economies.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 12:57:56 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 08:18:11 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Krampe", "Jonas", ""]]}, {"id": "1807.01152", "submitter": "Claudia Tarantola Dr", "authors": "Ioannis Ntzoufras, Claudia Tarantola and Monia Lupparelli", "title": "Probability Based Independence Sampler for Bayesian Quantitative\n  Learning in Graphical Log-Linear Marginal Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods for graphical log-linear marginal models have not been\ndeveloped in the same extent as traditional frequentist approaches. In this\nwork, we introduce a novel Bayesian approach for quantitative learning for such\nmodels. These models belong to curved exponential families that are difficult\nto handle from a Bayesian perspective. Furthermore, the likelihood cannot be\nanalytically expressed as a function of the marginal log-linear interactions,\nbut only in terms of cell counts or probabilities.\n  Posterior distributions cannot be directly obtained, and MCMC methods are\nneeded. Finally, a well-defined model requires parameter values that lead to\ncompatible marginal probabilities. Hence, any MCMC should account for this\nimportant restriction. We construct a fully automatic and efficient MCMC\nstrategy for quantitative learning for graphical log-linear marginal models\nthat handles these problems. While the prior is expressed in terms of the\nmarginal log-linear interactions, we build an MCMC algorithm that employs a\nproposal on the probability parameter space. The corresponding proposal on the\nmarginal log-linear interactions is obtained via parameter transformation.\n  By this strategy, we achieve to move within the desired target space. At each\nstep, we directly work with well-defined probability distributions.\n  Moreover, we can exploit a conditional conjugate setup to build an efficient\nproposal on probability parameters. The proposed methodology is illustrated by\na simulation study and a real dataset.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 13:21:05 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Ntzoufras", "Ioannis", ""], ["Tarantola", "Claudia", ""], ["Lupparelli", "Monia", ""]]}, {"id": "1807.01239", "submitter": "Reihaneh Entezari", "authors": "Reihaneh Entezari, Patrick E. Brown, and Jeffrey S. Rosenthal", "title": "Bayesian Spatial Analysis of Hardwood Tree Counts in Forests via MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we perform Bayesian Inference to analyze spatial tree count\ndata from the Timiskaming and Abitibi River forests in Ontario, Canada. We\nconsider a Bayesian Generalized Linear Geostatistical Model and implement a\nMarkov Chain Monte Carlo algorithm to sample from its posterior distribution.\nHow spatial predictions for new sites in the forests change as the amount of\ntraining data is reduced is studied and compared with a Logistic Regression\nmodel without a spatial effect. Finally, we discuss a stratified sampling\napproach for selecting subsets of data that allows for potential better\npredictions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 15:35:52 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Entezari", "Reihaneh", ""], ["Brown", "Patrick E.", ""], ["Rosenthal", "Jeffrey S.", ""]]}, {"id": "1807.01346", "submitter": "Joseph Marion", "authors": "Joseph Marion and Scott C. Schmidler", "title": "Finite Sample $L_2$ Bounds for Sequential Monte Carlo and Adaptive Path\n  Selection", "comments": "Correcting errors in the proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove a bound on the finite sample error of sequential Monte Carlo (SMC)\non static spaces using the $L_2$ distance between interpolating distributions\nand the mixing times of Markov kernels. This result is unique in that it is the\nfirst finite sample convergence result for SMC that does not require an upper\nbound on the importance weights. Using this bound we show that careful\nselection of the interpolating distributions can lead to substantial\nimprovements in the computational complexity of the algorithm. This result also\njustifies the adaptive selection of SMC distributions using the relative\neffective sample size commonly used in the literature and we establish\nconditions guaranteeing the approximation accuracy of the adaptive SMC\napproach. We then demonstrate empirically that this procedure provides\nnearly-optimal sequences of distributions in an automatic fashion for realistic\nexamples.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 19:02:32 GMT"}, {"version": "v2", "created": "Tue, 20 Nov 2018 12:35:23 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Marion", "Joseph", ""], ["Schmidler", "Scott C.", ""]]}, {"id": "1807.01585", "submitter": "Joram Soch", "authors": "Joram Soch", "title": "cvBMS and cvBMA: filling in the gaps", "comments": "14 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  With this technical report, we provide mathematical and implementational\ndetails of cross-validated Bayesian model selection (cvBMS) and averaging\n(cvBMA) that could not be communicated in the corresponding peer-reviewed\njournal articles. This will allow statisticians and developers to comprehend\ninternal functionalities of cvBMS and cvBMA for further development of these\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 3 Jul 2018 07:19:40 GMT"}], "update_date": "2018-07-05", "authors_parsed": [["Soch", "Joram", ""]]}, {"id": "1807.01635", "submitter": "Xinran Li", "authors": "Xinran Li, Peng Ding, Qian Lin, Dawei Yang, Jun S. Liu", "title": "Randomization Inference for Peer Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many previous causal inference studies require no interference, that is, the\npotential outcomes of a unit do not depend on the treatments of other units.\nHowever, this no-interference assumption becomes unreasonable when a unit\ninteracts with other units in the same group or cluster. In a motivating\napplication, a university in China admits students through two channels: the\ncollege entrance exam (also known as Gaokao) and recommendation (often based on\nOlympiads in various subjects). The university randomly assigns students to\ndorms, each of which hosts four students. Students within the same dorm live\ntogether and have extensive interactions. Therefore, it is likely that peer\neffects exist and the no-interference assumption does not hold. It is important\nto understand peer effects, because they give useful guidance for future\nroommate assignment to improve the performance of students. We define peer\neffects using potential outcomes. We then propose a randomization-based\ninference framework to study peer effects with arbitrary numbers of peers and\npeer types. Our inferential procedure does not assume any parametric model on\nthe outcome distribution. Our analysis gives useful practical guidance for\npolicy makers of the university in China.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 15:19:19 GMT"}, {"version": "v2", "created": "Mon, 30 Jul 2018 21:33:03 GMT"}, {"version": "v3", "created": "Fri, 21 Dec 2018 02:11:56 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Li", "Xinran", ""], ["Ding", "Peng", ""], ["Lin", "Qian", ""], ["Yang", "Dawei", ""], ["Liu", "Jun S.", ""]]}, {"id": "1807.01987", "submitter": "Bettina Gr\\\"un", "authors": "Bettina Gr\\\"un", "title": "Model-based Clustering", "comments": "This is a preprint of a chapter forthcoming in Handbook of Mixture\n  Analysis, edited by Gilles Celeux, Sylvia Fr\\\"uhwirth-Schnatter, and\n  Christian P. Robert", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models extend the toolbox of clustering methods available to the data\nanalyst. They allow for an explicit definition of the cluster shapes and\nstructure within a probabilistic framework and exploit estimation and inference\ntechniques available for statistical models in general. In this chapter an\nintroduction to cluster analysis is provided, model-based clustering is related\nto standard heuristic clustering methods and an overview on different ways to\nspecify the cluster model is given. Post-processing methods to determine a\nsuitable clustering, infer cluster distribution characteristics and validate\nthe cluster solution are discussed. The versatility of the model-based\nclustering approach is illustrated by giving an overview on the different areas\nof applications.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 13:26:28 GMT"}], "update_date": "2018-07-06", "authors_parsed": [["Gr\u00fcn", "Bettina", ""]]}, {"id": "1807.02099", "submitter": "Dmitry Arkhangelsky", "authors": "Dmitry Arkhangelsky, Guido Imbens", "title": "The Role of the Propensity Score in Fixed Effect Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach for estimating average treatment effects in the\nobservational studies with unobserved group-level heterogeneity. A common\napproach in such settings is to use linear fixed effect specifications\nestimated by least squares regression. Such methods severely limit the extent\nof the heterogeneity between groups by making the restrictive assumption that\nlinearly adjusting for differences between groups in average covariate values\naddresses all concerns with cross-group comparisons. We start by making two\nobservations. First we note that the fixed effect method in effect adjusts only\nfor differences between groups by adjusting for the average of covariate values\nand average treatment. Second, we note that weighting by the inverse of the\npropensity score would remove biases for comparisons between treated and\ncontrol units under the fixed effect set up. We then develop three\ngeneralizations of the fixed effect approach based on these two observations.\nFirst, we suggest more general, nonlinear, adjustments for the average\ncovariate values. Second, we suggest robustifying the estimators by using\npropensity score weighting. Third, we motivate and develop implementations for\nadjustments that also adjust for group characteristics beyond the average\ncovariate values.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 17:40:31 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 21:45:23 GMT"}, {"version": "v3", "created": "Fri, 13 Jul 2018 04:53:25 GMT"}, {"version": "v4", "created": "Sun, 22 Jul 2018 13:47:22 GMT"}, {"version": "v5", "created": "Wed, 19 Sep 2018 07:16:04 GMT"}, {"version": "v6", "created": "Sun, 14 Apr 2019 02:18:38 GMT"}], "update_date": "2019-04-16", "authors_parsed": [["Arkhangelsky", "Dmitry", ""], ["Imbens", "Guido", ""]]}, {"id": "1807.02161", "submitter": "Martin Weidner", "authors": "St\\'ephane Bonhomme, Martin Weidner", "title": "Minimizing Sensitivity to Model Misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for estimation and inference when the model may be\nmisspecified. We rely on a local asymptotic approach where the degree of\nmisspecification is indexed by the sample size. We construct estimators whose\nmean squared error is minimax in a neighborhood of the reference model, based\non one-step adjustments. In addition, we provide confidence intervals that\ncontain the true parameter under local misspecification. As a tool to interpret\nthe degree of misspecification, we map it to the local power of a specification\ntest of the reference model. Our approach allows for systematic sensitivity\nanalysis when the parameter of interest may be partially or irregularly\nidentified. As illustrations, we study three applications: an empirical\nanalysis of the impact of conditional cash transfers in Mexico where\nmisspecification stems from the presence of stigma effects of the program, a\ncross-sectional binary choice model where the error distribution is\nmisspecified, and a dynamic panel data binary choice model where the number of\ntime periods is small and the distribution of individual effects is\nmisspecified.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 19:33:23 GMT"}, {"version": "v2", "created": "Tue, 9 Oct 2018 21:55:03 GMT"}, {"version": "v3", "created": "Wed, 8 Jul 2020 19:14:59 GMT"}, {"version": "v4", "created": "Wed, 9 Dec 2020 19:27:53 GMT"}, {"version": "v5", "created": "Tue, 1 Jun 2021 10:09:58 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Bonhomme", "St\u00e9phane", ""], ["Weidner", "Martin", ""]]}, {"id": "1807.02191", "submitter": "Hani Doss", "authors": "Hani Doss and Yeonhee Park", "title": "An MCMC Approach to Empirical Bayes Inference and Bayesian Sensitivity\n  Analysis via Empirical Processes", "comments": "44 pages, 3 figures. To appear in The Annals of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a Bayesian situation in which we observe $Y \\sim p_{\\theta}$, where\n$\\theta \\in \\Theta$, and we have a family $\\{ \\nu_h, \\, h \\in \\mathcal{H} \\}$\nof potential prior distributions on $\\Theta$. Let $g$ be a real-valued function\nof $\\theta$, and let $I_g(h)$ be the posterior expectation of $g(\\theta)$ when\nthe prior is $\\nu_h$. We are interested in two problems: (i) selecting a\nparticular value of $h$, and (ii) estimating the family of posterior\nexpectations $\\{ I_g(h), \\, h \\in \\mathcal{H} \\}$. Let $m_y(h)$ be the marginal\nlikelihood of the hyperparameter $h$: $m_y(h) = \\int p_{\\theta}(y) \\,\n\\nu_h(d\\theta)$. The empirical Bayes estimate of $h$ is, by definition, the\nvalue of $h$ that maximizes $m_y(h)$. It turns out that it is typically\npossible to use Markov chain Monte Carlo to form point estimates for $m_y(h)$\nand $I_g(h)$ for each individual $h$ in a continuum, and also confidence\nintervals for $m_y(h)$ and $I_g(h)$ that are valid pointwise. However, we are\ninterested in forming estimates, with confidence statements, of the entire\nfamilies of integrals $\\{ m_y(h), \\, h \\in \\mathcal{H} \\}$ and $\\{ I_g(h), \\, h\n\\in \\mathcal{H} \\}$: we need estimates of the first family in order to carry\nout empirical Bayes inference, and we need estimates of the second family in\norder to do Bayesian sensitivity analysis. We establish strong consistency and\nfunctional central limit theorems for estimates of these families by using\ntools from empirical process theory. We give two applications, one to Latent\nDirichlet Allocation, which is used in topic modelling, and the other is to a\nmodel for Bayesian variable selection in linear regression.\n", "versions": [{"version": "v1", "created": "Thu, 5 Jul 2018 22:14:59 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Doss", "Hani", ""], ["Park", "Yeonhee", ""]]}, {"id": "1807.02244", "submitter": "Sepehr Akhavan Masouleh", "authors": "Sepehr Akhavan Masouleh, Babak Shahbaba, Daniel L. Gillen", "title": "A Bayesian Framework for Non-Collapsible Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we discuss the non-collapsibility concept and propose a new\napproach based on Dirichlet process mixtures to estimate the conditional effect\nof covariates in non-collapsible models. Using synthetic data, we evaluate the\nperformance of our proposed method and examine its sensitivity under different\nsettings. We also apply our method to real data on access failure among\nhemodialysis patients.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 03:41:56 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["Masouleh", "Sepehr Akhavan", ""], ["Shahbaba", "Babak", ""], ["Gillen", "Daniel L.", ""]]}, {"id": "1807.02289", "submitter": "Xu He", "authors": "Xu He", "title": "Interleaved lattice-based maximin distance designs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to construct maximin distance designs with arbitrary\nnumber of dimensions and points. The proposed designs hold interleaved-layer\nstructures and are by far the best maximin distance designs in four or more\ndimensions. Applicable to distance measures with equal or unequal weights, our\nmethod is useful for emulating computer experiments when a relatively accurate\npriori guess on the variable importance is available.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 07:25:32 GMT"}], "update_date": "2018-07-09", "authors_parsed": [["He", "Xu", ""]]}, {"id": "1807.02357", "submitter": "Stephan Smeekes", "authors": "Marina Friedrich, Stephan Smeekes, Jean-Pierre Urbain", "title": "Autoregressive Wild Bootstrap Inference for Nonparametric Trends", "comments": null, "journal-ref": "Journal of Econometrics 214 (2020) 81-109", "doi": "10.1016/j.jeconom.2019.05.006", "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an autoregressive wild bootstrap method to construct\nconfidence bands around a smooth deterministic trend. The bootstrap method is\neasy to implement and does not require any adjustments in the presence of\nmissing data, which makes it particularly suitable for climatological\napplications. We establish the asymptotic validity of the bootstrap method for\nboth pointwise and simultaneous confidence bands under general conditions,\nallowing for general patterns of missing data, serial dependence and\nheteroskedasticity. The finite sample properties of the method are studied in a\nsimulation study. We use the method to study the evolution of trends in daily\nmeasurements of atmospheric ethane obtained from a weather station in the Swiss\nAlps, where the method can easily deal with the many missing observations due\nto adverse weather conditions.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 11:19:19 GMT"}, {"version": "v2", "created": "Mon, 25 Nov 2019 14:02:33 GMT"}], "update_date": "2019-12-12", "authors_parsed": [["Friedrich", "Marina", ""], ["Smeekes", "Stephan", ""], ["Urbain", "Jean-Pierre", ""]]}, {"id": "1807.02421", "submitter": "Ray Bai", "authors": "Ray Bai, Malay Ghosh", "title": "Large-Scale Multiple Hypothesis Testing with the Normal-Beta Prime Prior", "comments": "39 pages, 6 figures, 1 table. New Section (Section 5.3) was added\n  with new figures for mean squared error and false discovery rate (FDR). arXiv\n  admin note: text overlap with arXiv:1710.04369", "journal-ref": null, "doi": "10.1080/02331888.2019.1662017", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of simultaneously testing the means of $n$ independent\nnormal observations under sparsity. We take a Bayesian approach to this problem\nby introducing a scale-mixture prior known as the normal-beta prime (NBP)\nprior. We first derive new concentration properties when the beta prime density\nis employed for a scale parameter in Bayesian hierarchical models. To detect\nsignals in our data, we then propose a hypothesis test based on thresholding\nthe posterior shrinkage weight under the NBP prior. Taking the loss function to\nbe the expected number of misclassified tests, we show that our test procedure\nasymptotically attains the optimal Bayes risk when the signal proportion $p$ is\nknown. When $p$ is unknown, we introduce an empirical Bayes variant of our test\nwhich also asymptotically attains the Bayes Oracle risk in the entire range of\nsparsity parameters $p \\propto n^{-\\epsilon}, \\epsilon \\in (0, 1)$. Finally, we\nalso consider restricted marginal maximum likelihood (REML) and hierarchical\nBayes approaches for estimating a key hyperparameter in the NBP prior and\nexamine multiple testing under these frameworks.\n", "versions": [{"version": "v1", "created": "Wed, 4 Jul 2018 23:23:05 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 23:08:22 GMT"}, {"version": "v3", "created": "Thu, 10 Jan 2019 15:09:47 GMT"}, {"version": "v4", "created": "Tue, 30 Apr 2019 19:15:16 GMT"}], "update_date": "2020-07-29", "authors_parsed": [["Bai", "Ray", ""], ["Ghosh", "Malay", ""]]}, {"id": "1807.02535", "submitter": "Soumyasundar Pal", "authors": "Yunpeng Li, Soumyasundar Pal and Mark Coates", "title": "Invertible Particle Flow-based Sequential MCMC with extension to\n  Gaussian Mixture noise models", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2019.2905816", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential state estimation in non-linear and non-Gaussian state spaces has a\nwide range of applications in statistics and signal processing. One of the most\neffective non-linear filtering approaches, particle filtering, suffers from\nweight degeneracy in high-dimensional filtering scenarios. Several avenues have\nbeen pursued to address high-dimensionality. Among these, particle flow\nparticle filters construct effective proposal distributions by using invertible\nflow to migrate particles continuously from the prior distribution to the\nposterior, and sequential Markov chain Monte Carlo (SMCMC) methods use a\nMetropolis-Hastings (MH) accept-reject approach to improve filtering\nperformance. In this paper, we propose to combine the strengths of invertible\nparticle flow and SMCMC by constructing a composite Metropolis-Hastings (MH)\nkernel within the SMCMC framework using invertible particle flow. In addition,\nwe propose a Gaussian mixture model (GMM)-based particle flow algorithm to\nconstruct effective MH kernels for multi-modal distributions. Simulation\nresults show that for high-dimensional state estimation example problems the\nproposed kernels significantly increase the acceptance rate with minimal\nadditional computational overhead and improve estimation accuracy compared with\nstate-of-the-art filtering algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 18:08:46 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 19:18:55 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 19:00:54 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Li", "Yunpeng", ""], ["Pal", "Soumyasundar", ""], ["Coates", "Mark", ""]]}, {"id": "1807.02590", "submitter": "Ottmar Cronie", "authors": "M. Mehdi Moradi, Ottmar Cronie, Ege Rubak, Raphael Lachieze-Rey, Jorge\n  Mateu, Adrian Baddeley", "title": "Resample-smoothing of Voronoi intensity estimators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voronoi intensity estimators, which are non-parametric estimators for\nintensity functions of point processes, are both parameter-free and adaptive;\nthe intensity estimate at a given location is given by the reciprocal size of\nthe Voronoi/Dirichlet cell containing that location. Their major drawback,\nhowever, is that they tend to under-smooth the data in regions where the point\ndensity of the observed point pattern is high and over-smooth in regions where\nthe point density is low. To remedy this problem, i.e. to find some\nmiddle-ground between over- and under-smoothing, we propose an additional\nsmoothing technique for Voronoi intensity estimators for point processes in\narbitrary metric spaces, which is based on repeated independent thinnings of\nthe point process/pattern. Through a simulation study we show that our\nresample-smoothing technique improves the estimation significantly. In\naddition, we study statistical properties such as unbiasedness and variance,\nand propose a rule-of-thumb and a data-driven cross-validation approach to\nchoose the amount of thinning/smoothing to apply. We finally apply our proposed\nintensity estimation scheme to two datasets: locations of pine saplings (planar\npoint pattern) and motor vehicle traffic accidents (linear network point\npattern).\n", "versions": [{"version": "v1", "created": "Fri, 6 Jul 2018 23:49:26 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Moradi", "M. Mehdi", ""], ["Cronie", "Ottmar", ""], ["Rubak", "Ege", ""], ["Lachieze-Rey", "Raphael", ""], ["Mateu", "Jorge", ""], ["Baddeley", "Adrian", ""]]}, {"id": "1807.02602", "submitter": "Grisel Maribel Britos", "authors": "Grisel Maribel Britos and Silvia Mar\\'ia Ojeda", "title": "Robust Estimation for Two-Dimensional Autoregressive Processes Based on\n  Bounded Innovation Propagation Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust methods have been a successful approach to deal with contaminations\nand noises in image processing. In this paper, we introduce a new robust method\nfor two-dimensional autoregressive models. Our method, called BMM-2D, relies on\nrepresenting a two-dimensional autoregressive process with an auxiliary model\nto attenuate the effect of contamination (outliers). We compare the performance\nof our method with existing robust estimators and the least squares estimator\nvia a comprehensive Monte Carlo simulation study which considers different\nlevels of replacement contamination and window sizes. The results show that the\nnew estimator is superior to the other estimators, both in accuracy and\nprecision. An application to image filtering highlights the findings and\nillustrates how the estimator works in practical applications.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 02:05:30 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Britos", "Grisel Maribel", ""], ["Ojeda", "Silvia Mar\u00eda", ""]]}, {"id": "1807.02603", "submitter": "Helio M. de Oliveira", "authors": "H. M. de Oliveira and Raydonal Ospina", "title": "A Note on the Shannon Entropy of Short Sequences", "comments": "3 figures", "journal-ref": null, "doi": "10.14209/SBRT.2018.8", "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For source sequences of length L symbols we proposed to use a more realistic\nvalue to the usual benchmark of number of code letters by source letters. Our\nidea is based on a quantifier of information fluctuation of a source, F(U),\nwhich corresponds to the second central moment of the random variable that\nmeasures the information content of a source symbol. An alternative\ninterpretation of typical sequences is additionally provided through this\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 7 Jul 2018 02:13:18 GMT"}], "update_date": "2019-09-27", "authors_parsed": [["de Oliveira", "H. M.", ""], ["Ospina", "Raydonal", ""]]}, {"id": "1807.02737", "submitter": "Konrad Menzel", "authors": "Guido Imbens and Konrad Menzel", "title": "A Causal Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bootstrap, introduced by Efron (1982), has become a very popular method\nfor estimating variances and constructing confidence intervals. A key insight\nis that one can approximate the properties of estimators by using the empirical\ndistribution function of the sample as an approximation for the true\ndistribution function. This approach views the uncertainty in the estimator as\ncoming exclusively from sampling uncertainty. We argue that for causal\nestimands the uncertainty arises entirely, or partially, from a different\nsource, corresponding to the stochastic nature of the treatment received. We\ndevelop a bootstrap procedure that accounts for this uncertainty, and compare\nits properties to that of the classical bootstrap.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 01:49:32 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 02:06:23 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Imbens", "Guido", ""], ["Menzel", "Konrad", ""]]}, {"id": "1807.02789", "submitter": "Jos\\'e Enrique Chac\\'on", "authors": "Jos\\'e E. Chac\\'on", "title": "The modal age of Statistics", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a number of statistical problems have found an unexpected solution\nby inspecting them through a \"modal point of view\". These include classical\ntasks such as clustering or regression. This has led to a renewed interest in\nestimation and inference for the mode. This paper offers an extensive survey of\nthe traditional approaches to mode estimation and explores the consequences of\napplying this modern modal methodology to other, seemingly unrelated, fields.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 09:26:54 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Chac\u00f3n", "Jos\u00e9 E.", ""]]}, {"id": "1807.02814", "submitter": "Eric Blankmeyer", "authors": "Eric Blankmeyer", "title": "Measurement Errors as Bad Leverage Points", "comments": "20 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Errors-in-variables is a long-standing, difficult issue in linear regression;\nand progress depends in part on new identifying assumptions. I characterize\nmeasurement error as bad-leverage points and assume that fewer than half the\nsample observations are heavily contaminated, in which case a high-breakdown\nrobust estimator may be able to isolate and down weight or discard the\nproblematic data. In simulations of simple and multiple regression where eiv\naffects 25% of the data and R-squared is mediocre, certain high-breakdown\nestimators have small bias and reliable confidence intervals.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 13:25:45 GMT"}, {"version": "v2", "created": "Mon, 16 Mar 2020 11:15:57 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Blankmeyer", "Eric", ""]]}, {"id": "1807.02817", "submitter": "Shu Yang", "authors": "Shu Yang and Jae Kwang Kim", "title": "Integration of survey data and big observational data for finite\n  population inference using mass imputation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple data sources are becoming increasingly available for statistical\nanalyses in the era of big data. As an important example in finite-population\ninference, we consider an imputation approach to combining a probability sample\nwith big observational data. Unlike the usual imputation for missing data\nanalysis, we create imputed values for the whole elements in the probability\nsample. Such mass imputation is attractive in the context of survey data\nintegration (Kim and Rao, 2012). We extend mass imputation as a tool for data\nintegration of survey data and big non-survey data. The mass imputation methods\nand their statistical properties are presented. The matching estimator of\nRivers (2007) is also covered as a special case. Variance estimation with\nmass-imputed data is discussed. The simulation results demonstrate the proposed\nestimators outperform existing competitors in terms of robustness and\nefficiency.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 13:40:05 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Yang", "Shu", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1807.02877", "submitter": "Jonas Haslbeck", "authors": "Jonas Haslbeck, Denny Borsboom, Lourens Waldorp", "title": "Moderated Network Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise network models such as the Gaussian Graphical Model (GGM) are a\npowerful and intuitive way to analyze dependencies in multivariate data. A key\nassumption of the GGM is that each pairwise interaction is independent of the\nvalues of all other variables. However, in psychological research this is often\nimplausible. In this paper, we extend the GGM by allowing each pairwise\ninteraction between two variables to be moderated by (a subset of) all other\nvariables in the model, and thereby introduce a Moderated Network Model (MNM).\nWe show how to construct the MNM and propose an L1-regularized nodewise\nregression approach to estimate it. We provide performance results in a\nsimulation study and show that MNMs outperform the split-sample based methods\nNetwork Comparison Test (NCT) and Fused Graphical Lasso (FGL) in detecting\nmoderation effects. Finally, we provide a fully reproducible tutorial on how to\nestimate MNMs with the R-package mgm and discuss possible issues with model\nmisspecification.\n", "versions": [{"version": "v1", "created": "Sun, 8 Jul 2018 20:22:39 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2019 09:32:58 GMT"}, {"version": "v3", "created": "Thu, 5 Sep 2019 09:34:34 GMT"}, {"version": "v4", "created": "Tue, 1 Oct 2019 12:28:11 GMT"}, {"version": "v5", "created": "Thu, 13 Feb 2020 09:53:57 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["Haslbeck", "Jonas", ""], ["Borsboom", "Denny", ""], ["Waldorp", "Lourens", ""]]}, {"id": "1807.02930", "submitter": "John Palowitch", "authors": "John Palowitch", "title": "Computing the statistical significance of optimized communities in\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often of interest to find communities in network data for unsupervised\nlearning, feature discovery, anomaly detection, or scientific study. The vast\nmajority of community detection methods proceed via optimization of a quality\nfunction, which is possible even on random networks without communities.\nTherefore there is usually not an easy way to tell if a community is\n\"significant\", in this context meaning more internally connected than would be\nexpected under a random graph model without communities. This paper generalizes\nexisting null models for this purpose to bipartite graphs, and introduces a new\nsignificance scoring algorithm called Fast Optimized Community Significance\n(FOCS) that is highly scalable and agnostic to the type of graph. Furthermore,\ncompared with existing methods on unipartite graphs, FOCS is more numerically\nstable and better balances the trade-off between detection power and false\npositives.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 03:38:52 GMT"}, {"version": "v2", "created": "Sat, 17 Nov 2018 00:48:19 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Palowitch", "John", ""]]}, {"id": "1807.03090", "submitter": "Irene C\\'ordoba", "authors": "Irene C\\'ordoba, Gherardo Varando, Concha Bielza, Pedro Larra\\~naga", "title": "A partial orthogonalization method for simulating covariance and\n  concentration graph matrices", "comments": "12 pages, 5 figures, conference", "journal-ref": "Proceedings of Machine Learning Research (PGM 2018), 72:61-72,\n  2018", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structure learning methods for covariance and concentration graphs are often\nvalidated on synthetic models, usually obtained by randomly generating: (i) an\nundirected graph, and (ii) a compatible symmetric positive definite (SPD)\nmatrix. In order to ensure positive definiteness in (ii), a dominant diagonal\nis usually imposed. However, the link strengths in the resulting graphical\nmodel, determined by off-diagonal entries in the SPD matrix, are in many\nscenarios extremely weak. Recovering the structure of the undirected graph thus\nbecomes a challenge, and algorithm validation is notably affected. In this\npaper, we propose an alternative method which overcomes such problem yet\nyielding a compatible SPD matrix. We generate a partially row-wise-orthogonal\nmatrix factor, where pairwise orthogonal rows correspond to missing edges in\nthe undirected graph. In numerical experiments ranging from moderately dense to\nsparse scenarios, we obtain that, as the dimension increases, the link strength\nwe simulate is stable with respect to the structure sparsity. Importantly, we\nshow in a real validation setting how structure recovery is greatly improved\nfor all learning algorithms when using our proposed method, thereby producing a\nmore realistic comparison framework.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 13:06:00 GMT"}], "update_date": "2019-10-02", "authors_parsed": [["C\u00f3rdoba", "Irene", ""], ["Varando", "Gherardo", ""], ["Bielza", "Concha", ""], ["Larra\u00f1aga", "Pedro", ""]]}, {"id": "1807.03113", "submitter": "Benjamin Bloem-Reddy", "authors": "Benjamin Bloem-Reddy and Adam Foster and Emile Mathieu and Yee Whye\n  Teh", "title": "Sampling and Inference for Beta Neutral-to-the-Left Models of Sparse\n  Networks", "comments": "Accepted for publication in the proceedings of Conference on\n  Uncertainty in Artificial Intelligence (UAI) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical evidence suggests that heavy-tailed degree distributions occurring\nin many real networks are well-approximated by power laws with exponents $\\eta$\nthat may take values either less than and greater than two. Models based on\nvarious forms of exchangeability are able to capture power laws with $\\eta <\n2$, and admit tractable inference algorithms; we draw on previous results to\nshow that $\\eta > 2$ cannot be generated by the forms of exchangeability used\nin existing random graph models. Preferential attachment models generate power\nlaw exponents greater than two, but have been of limited use as statistical\nmodels due to the inherent difficulty of performing inference in\nnon-exchangeable models. Motivated by this gap, we design and implement\ninference algorithms for a recently proposed class of models that generates\n$\\eta$ of all possible values. We show that although they are not exchangeable,\nthese models have probabilistic structure amenable to inference. Our methods\nmake a large class of previously intractable models useful for statistical\ninference.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 13:28:15 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Bloem-Reddy", "Benjamin", ""], ["Foster", "Adam", ""], ["Mathieu", "Emile", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1807.03184", "submitter": "Emeline Perthame", "authors": "Emilie Devijver and Emeline Perthame", "title": "Prediction regions through Inverse Regression", "comments": "25 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predict a new response from a covariate is a challenging task in regression,\nwhich raises new question since the era of high-dimensional data. In this\npaper, we are interested in the inverse regression method from a theoretical\nviewpoint. Theoretical results have already been derived for the well-known\nlinear model, but recently, the curse of dimensionality has increased the\ninterest of practitioners and theoreticians into generalization of those\nresults for various estimators, calibrated for the high-dimension context. To\ndeal with high-dimensional data, inverse regression is used in this paper. It\nis known to be a reliable and efficient approach when the number of features\nexceeds the number of observations. Indeed, under some conditions, dealing with\nthe inverse regression problem associated to a forward regression problem\ndrastically reduces the number of parameters to estimate and make the problem\ntractable. When both the responses and the covariates are multivariate,\nestimators constructed by the inverse regression are studied in this paper, the\nmain result being explicit asymptotic prediction regions for the response. The\nperformances of the proposed estimators and prediction regions are also\nanalyzed through a simulation study and compared with usual estimators.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 14:15:11 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Devijver", "Emilie", ""], ["Perthame", "Emeline", ""]]}, {"id": "1807.03265", "submitter": "Gerton Lunter", "authors": "Donna Henderson and Gerton Lunter", "title": "Efficient convergence through adaptive learning in sequential Monte\n  Carlo Expectation Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Expectation maximization (EM) is a technique for estimating\nmaximum-likelihood parameters of a latent variable model given observed data by\nalternating between taking expectations of sufficient statistics, and\nmaximizing the expected log likelihood. For situations where sufficient\nstatistics are intractable, stochastic approximation EM (SAEM) is often used,\nwhich uses Monte Carlo techniques to approximate the expected log likelihood.\nTwo common implementations of SAEM, Batch EM (BEM) and online EM (OEM), are\nparameterized by a \"learning rate\", and their efficiency depend strongly on\nthis parameter. We propose an extension to the OEM algorithm, termed\nIntrospective Online Expectation Maximization (IOEM), which removes the need\nfor specifying this parameter by adapting the learning rate according to trends\nin the parameter updates. We show that our algorithm matches the efficiency of\nthe optimal BEM and OEM algorithms in multiple models, and that the efficiency\nof IOEM can exceed that of BEM/OEM methods with optimal learning rates when the\nmodel has many parameters. A Python implementation is available at\nhttps://github.com/luntergroup/IOEM.git.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 16:34:39 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Henderson", "Donna", ""], ["Lunter", "Gerton", ""]]}, {"id": "1807.03299", "submitter": "Aurelien Garivier", "authors": "Gr\\'egoire Jauvion, Nicolas Grislain, Pascal Sielenou Dkengne (IMT),\n  Aur\\'elien Garivier (IMT), S\\'ebastien Gerchinovitz (IMT)", "title": "Optimization of a SSP's Header Bidding Strategy using Thompson Sampling", "comments": null, "journal-ref": "The 24th ACM SIGKDD International Conference on Knowledge\n  Discovery & Data Mining, Aug 2018, London, United Kingdom", "doi": "10.1145/3219819.3219917", "report-no": null, "categories": "cs.LG cs.GT stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, digital media (web or app publishers) generalized the\nuse of real time ad auctions to sell their ad spaces. Multiple auction\nplatforms, also called Supply-Side Platforms (SSP), were created. Because of\nthis multiplicity, publishers started to create competition between SSPs. In\nthis setting, there are two successive auctions: a second price auction in each\nSSP and a secondary, first price auction, called header bidding auction,\nbetween SSPs.In this paper, we consider an SSP competing with other SSPs for ad\nspaces. The SSP acts as an intermediary between an advertiser wanting to buy ad\nspaces and a web publisher wanting to sell its ad spaces, and needs to define a\nbidding strategy to be able to deliver to the advertisers as many ads as\npossible while spending as little as possible. The revenue optimization of this\nSSP can be written as a contextual bandit problem, where the context consists\nof the information available about the ad opportunity, such as properties of\nthe internet user or of the ad placement.Using classical multi-armed bandit\nstrategies (such as the original versions of UCB and EXP3) is inefficient in\nthis setting and yields a low convergence speed, as the arms are very\ncorrelated. In this paper we design and experiment a version of the Thompson\nSampling algorithm that easily takes this correlation into account. We combine\nthis bayesian algorithm with a particle filter, which permits to handle\nnon-stationarity by sequentially estimating the distribution of the highest bid\nto beat in order to win an auction. We apply this methodology on two real\nauction datasets, and show that it significantly outperforms more classical\napproaches.The strategy defined in this paper is being developed to be deployed\non thousands of publishers worldwide.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 08:47:19 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Jauvion", "Gr\u00e9goire", "", "IMT"], ["Grislain", "Nicolas", "", "IMT"], ["Dkengne", "Pascal Sielenou", "", "IMT"], ["Garivier", "Aur\u00e9lien", "", "IMT"], ["Gerchinovitz", "S\u00e9bastien", "", "IMT"]]}, {"id": "1807.03375", "submitter": "Debashis Ghosh", "authors": "Debashis Ghosh and Youngjoo Cho", "title": "Predictive Directions for Individualized Treatment Selection in Clinical\n  Trials", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many clinical trials, individuals in different subgroups have experience\ndifferential treatment effects. This leads to individualized differences in\ntreatment benefit. In this article, we introduce the general concept of\npredictive directions, which are risk scores motivated by potential outcomes\nconsiderations. These techniques borrow heavily from sufficient dimension\nreduction (SDR) and causal inference methodology. Under some conditions, one\ncan use existing methods from the SDR literature to estimate the directions\nassuming an idealized complete data structure, which subsequently yields an\nobvious extension to clinical trial datasets. In addition, we generalize the\ndirection idea to a nonlinear setting that exploits support vector machines.\nThe methodology is illustrated with application to a series of colorectal\ncancer clinical trials.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 20:30:12 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Ghosh", "Debashis", ""], ["Cho", "Youngjoo", ""]]}, {"id": "1807.03386", "submitter": "Abdolrahman Khoshrou", "authors": "Abdolrahman Khoshrou and Eric J. Pauwels", "title": "Data-driven pattern identification and outlier detection in time series", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-01174-1_35", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of data-driven pattern identification and outlier\ndetection in time series. To this end, we use singular value decomposition\n(SVD) which is a well-known technique to compute a low-rank approximation for\nan arbitrary matrix. By recasting the time series as a matrix it becomes\npossible to use SVD to highlight the underlying patterns and periodicities.\nThis is done without the need for specifying user-defined parameters. From a\ndata mining perspective, this opens up new ways of analyzing time series in a\ndata-driven, bottom-up fashion. However, in order to get correct results, it is\nimportant to understand how the SVD-spectrum of a time series is influenced by\nvarious characteristics of the underlying signal and noise. In this paper, we\nhave extended the work in earlier papers by initiating a more systematic\nanalysis of these effects. We then illustrate our findings on some real-life\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 07:31:55 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Khoshrou", "Abdolrahman", ""], ["Pauwels", "Eric J.", ""]]}, {"id": "1807.03413", "submitter": "Harlan Campbell", "authors": "Harlan Campbell and Paul Gustafson", "title": "What to make of non-inferiority and equivalence testing with a\n  post-specified margin?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to determine whether or not an effect is absent based on a\nstatistical test, the recommended frequentist tool is the equivalence test.\nTypically, it is expected that an appropriate equivalence margin has been\nspecified before any data are observed. Unfortunately, this can be a difficult\ntask. If the margin is too small, then the test's power will be substantially\nreduced. If the margin is too large, any claims of equivalence will be\nmeaningless. Moreover, it remains unclear how defining the margin afterwards\nwill bias one's results. In this short article, we consider a series of\nhypothetical scenarios in which the margin is defined post-hoc or is otherwise\nconsidered controversial. We also review a number of relevant, potentially\nproblematic actual studies from clinical trials research, with the aim of\nmotivating a critical discussion as to what is acceptable and desirable in the\nreporting and interpretation of equivalence tests.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 22:45:50 GMT"}, {"version": "v2", "created": "Thu, 6 Feb 2020 05:56:13 GMT"}, {"version": "v3", "created": "Mon, 24 Feb 2020 16:36:59 GMT"}, {"version": "v4", "created": "Thu, 24 Sep 2020 00:57:35 GMT"}, {"version": "v5", "created": "Mon, 22 Feb 2021 23:14:34 GMT"}], "update_date": "2021-02-24", "authors_parsed": [["Campbell", "Harlan", ""], ["Gustafson", "Paul", ""]]}, {"id": "1807.03419", "submitter": "Yu-hsuan Wang", "authors": "Wenyu Chen, Mathias Drton, and Y. Samuel Wang", "title": "On Causal Discovery with Equal Variance Assumption", "comments": null, "journal-ref": null, "doi": "10.1093/biomet/asz049", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior work has shown that causal structure can be uniquely identified from\nobservational data when these follow a structural equation model whose error\nterms have equal variances. We show that this fact is implied by an ordering\namong (conditional) variances. We demonstrate that ordering estimates of these\nvariances yields a simple yet state-of-the-art method for causal structure\nlearning that is readily extendable to high-dimensional problems.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 23:11:17 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 15:47:27 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Chen", "Wenyu", ""], ["Drton", "Mathias", ""], ["Wang", "Y. Samuel", ""]]}, {"id": "1807.03462", "submitter": "Donald Lee", "authors": "Peter M. Aronow, Donald K. K. Lee", "title": "A note on breaking ties among sample medians", "comments": "5 pages, no tables or figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given samples $x_1,\\cdots,x_n$, it is well known that any sample median value\n(not necessarily unique) minimizes the absolute loss $\\sum_{i=1}^n |q-x_i|$.\nInterestingly, we show that the minimizer of the loss\n$\\sum_{i=1}^n|q-x_i|^{1+\\epsilon}$ exhibits a singular perturbation behaviour\nthat provides a unique definition for the sample median as $\\epsilon\n\\rightarrow 0$. This definition is the unique point among all candidate median\nvalues that balances the $logarithmic$ moment of the empirical distribution.\nThe result generalizes directly to breaking ties among sample quantiles when\nthe quantile regression loss is modified in the same way.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 03:23:06 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 01:52:16 GMT"}], "update_date": "2019-09-04", "authors_parsed": [["Aronow", "Peter M.", ""], ["Lee", "Donald K. K.", ""]]}, {"id": "1807.03469", "submitter": "Yang Feng", "authors": "Sihan Huang and Yang Feng", "title": "Pairwise Covariates-adjusted Block Model for Community Detection", "comments": "41 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most fundamental problems in network study is community detection.\nThe stochastic block model (SBM) is one widely used model for network data with\ndifferent estimation methods developed with their community detection\nconsistency results unveiled. However, the SBM is restricted by the strong\nassumption that all nodes in the same community are stochastically equivalent,\nwhich may not be suitable for practical applications. We introduce a pairwise\ncovariates-adjusted stochastic block model (PCABM), a generalization of SBM\nthat incorporates pairwise covariate information. We study the maximum\nlikelihood estimates of the coefficients for the covariates as well as the\ncommunity assignments. It is shown that both the coefficient estimates of the\ncovariates and the community assignments are consistent under suitable sparsity\nconditions. Spectral clustering with adjustment (SCWA) is introduced to\nefficiently solve PCABM. Under certain conditions, we derive the error bound of\ncommunity estimation under SCWA and show that it is community detection\nconsistent. PCABM compares favorably with the SBM or degree-corrected\nstochastic block model (DCBM) under a wide range of simulated and real networks\nwhen covariate information is accessible.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 03:37:55 GMT"}, {"version": "v2", "created": "Sat, 20 Jun 2020 18:01:26 GMT"}], "update_date": "2020-06-23", "authors_parsed": [["Huang", "Sihan", ""], ["Feng", "Yang", ""]]}, {"id": "1807.03582", "submitter": "Christoph Dalitz", "authors": "Christoph Dalitz", "title": "Construction of Confidence Intervals", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": "Technical Report No. 2017-01, pp. 15-28, Hochschule Niederrhein,\n  Fachbereich Elektrotechnik & Informatik (2017)", "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Introductory texts on statistics typically only cover the classical \"two\nsigma\" confidence interval for the mean value and do not describe methods to\nobtain confidence intervals for other estimators. The present technical report\nfills this gap by first defining different methods for the construction of\nconfidence intervals, and then by their application to a binomial proportion,\nthe mean value, and to arbitrary estimators. Beside the frequentist approach,\nthe likelihood ratio and the highest posterior density approach are explained.\nTwo methods to estimate the variance of general maximum likelihood estimators\nare described (Hessian, Jackknife), and for arbitrary estimators the bootstrap\nis suggested. For three examples, the different methods are evaluated by means\nof Monte Carlo simulations with respect to their coverage probability and\ninterval length. R code is given for all methods, and the practitioner obtains\na guideline which method should be used in which cases.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 12:03:16 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Dalitz", "Christoph", ""]]}, {"id": "1807.03693", "submitter": "Rachel Lynne Wilkerson", "authors": "Rachel L. Wilkerson, Jim Q. Smith", "title": "Customised Structural Elicitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Established methods for structural elicitation typically rely on code\nmodelling standard graphical models classes, most often Bayesian networks.\nHowever, more appropriate models may arise from asking the expert questions in\ncommon language about what might relate to what and exploring the logical\nimplications of the statements. Only after identifying the best matching\nstructure should this be embellished into a fully quantified probability model.\nExamples of the efficacy and potential of this more flexible approach are shown\nbelow for four classes of graphical models: Bayesian networks, Chain Event\nGraphs, Multi-regression Dynamic Models, and Flow Graphs. We argue that to be\nfully effective any structural elicitation phase must first be customised to an\napplication and if necessary new types of structure with their own bespoke\nsemantics elicited.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 15:05:01 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Wilkerson", "Rachel L.", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1807.03705", "submitter": "Matthias Troffaes", "authors": "Matthias C. M. Troffaes", "title": "Decision making under uncertainty using imprecise probabilities", "comments": "11 pages", "journal-ref": "International Journal of Approximate Reasoning 45 (2007) 17-29", "doi": "10.1016/j.ijar.2006.06.001", "report-no": null, "categories": "math.PR stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various ways for decision making with imprecise probabilities (admissibility,\nmaximal expected utility, maximality, E-admissibility, $\\Gamma$-maximax,\n$\\Gamma$-maximin, all of which are well-known from the literature) are\ndiscussed and compared. We generalize a well-known sufficient condition for\nexistence of optimal decisions. A simple numerical example shows how these\ncriteria can work in practice, and demonstrates their differences. Finally, we\nsuggest an efficient approach to calculate optimal decisions under these\ndecision criteria.\n", "versions": [{"version": "v1", "created": "Mon, 9 Jul 2018 07:40:58 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Troffaes", "Matthias C. M.", ""]]}, {"id": "1807.03712", "submitter": "Olivier Zahm", "authors": "Olivier Zahm and Tiangang Cui and Kody Law and Alessio Spantini and\n  Youssef Marzouk", "title": "Certified dimension reduction in nonlinear Bayesian inverse problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.NA math.NA stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a dimension reduction technique for Bayesian inverse problems with\nnonlinear forward operators, non-Gaussian priors, and non-Gaussian observation\nnoise. The likelihood function is approximated by a ridge function, i.e., a map\nwhich depends non-trivially only on a few linear combinations of the\nparameters. We build this ridge approximation by minimizing an upper bound on\nthe Kullback--Leibler divergence between the posterior distribution and its\napproximation. This bound, obtained via logarithmic Sobolev inequalities,\nallows one to certify the error of the posterior approximation. Computing the\nbound requires computing the second moment matrix of the gradient of the\nlog-likelihood function. In practice, a sample-based approximation of the upper\nbound is then required. We provide an analysis that enables control of the\nposterior approximation error due to this sampling. Numerical and theoretical\ncomparisons with existing methods illustrate the benefits of the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 07:49:11 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 08:38:57 GMT"}, {"version": "v3", "created": "Tue, 9 Mar 2021 12:38:20 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zahm", "Olivier", ""], ["Cui", "Tiangang", ""], ["Law", "Kody", ""], ["Spantini", "Alessio", ""], ["Marzouk", "Youssef", ""]]}, {"id": "1807.03874", "submitter": "Silvia D'Angelo", "authors": "Silvia D'Angelo, Marco Alf\\`o and Thomas Brendan Murphy", "title": "Modelling heterogeneity in Latent Space Models for Multidimensional\n  Networks", "comments": null, "journal-ref": "Stat. Neerl. 74(3): 324-341 (August 2020)", "doi": "10.1111/stan.12209", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional network data can have different levels of complexity, as\nnodes may be characterized by heterogeneous individual-specific features, which\nmay vary across the networks. This paper introduces a class of models for\nmultidimensional network data, where different levels of heterogeneity within\nand between networks can be considered. The proposed framework is developed in\nthe family of latent space models, and it aims to distinguish symmetric\nrelations between the nodes and node-specific features. Model parameters are\nestimated via a Markov Chain Monte Carlo algorithm. Simulated data and an\napplication to a real example, on fruits import/export data, are used to\nillustrate and comment on the performance of the proposed models.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 21:39:11 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 16:10:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["D'Angelo", "Silvia", ""], ["Alf\u00f2", "Marco", ""], ["Murphy", "Thomas Brendan", ""]]}, {"id": "1807.03889", "submitter": "Xiongzhi Chen", "authors": "Xiongzhi Chen", "title": "Uniformly consistently estimating the proportion of false null\n  hypotheses via Lebesgue-Stieltjes integral equations", "comments": "44 pages in single spacing; 5 figures; proofs moved to supplementary\n  materials; extended Introduction and Discussion; added a Simulation Study;\n  accepted by Journal of Multivariate Analysis", "journal-ref": "Journal of Multivariate Analysis, 2019", "doi": "10.1016/j.jmva.2019.06.003", "report-no": null, "categories": "math.PR math.CA stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The proportion of false null hypotheses is a very important quantity in\nstatistical modelling and inference based on the two-component mixture model\nand its extensions, and in control and estimation of the false discovery rate\nand false non-discovery rate. Most existing estimators of this proportion\nthreshold p-values, deconvolve the mixture model under constraints on its\ncomponents, or depend heavily on the location-shift property of distributions.\nHence, they usually are not consistent, applicable to non-location-shift\ndistributions, or applicable to discrete statistics or p-values. To eliminate\nthese shortcomings, we construct uniformly consistent estimators of the\nproportion as solutions to Lebesgue-Stieltjes integral equations. In\nparticular, we provide such estimators respectively for random variables whose\ndistributions have Riemann-Lebesgue type characteristic functions, form\ndiscrete natural exponential families with infinite supports, and form natural\nexponential families with separable moment sequences. We provide the speed of\nconvergence and uniform consistency class for each such estimator under\nindependence. In addition, we provide example distribution families for which a\nconsistent estimator of the proportion cannot be constructed using our\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 10 Jul 2018 22:19:10 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 19:32:52 GMT"}], "update_date": "2020-03-09", "authors_parsed": [["Chen", "Xiongzhi", ""]]}, {"id": "1807.04003", "submitter": "Peida Zhan", "authors": "Peida Zhan, Hong Jiao, Wen-Chung Wang, Kaiwen Man", "title": "A Multidimensional Hierarchical Framework for Modeling Speed and Ability\n  in Computer-based Multidimensional Tests", "comments": "27 pages, 3 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In psychological and educational computer-based multidimensional tests,\nlatent speed, a rate of the amount of labor performed on the items with respect\nto time, may also be multidimensional. To capture the multidimensionality of\nlatent speed, this study firstly proposed a multidimensional log-normal\nresponse time (RT) model to consider the potential multidimensional latent\nspeed. Further, to simultaneously take into account the response accuracy (RA)\nand RTs in multidimensional tests, a multidimensional hierarchical modeling\nframework was proposed. The framework is an extension of the van der Linden\n(2007; doi:10.1007/s11336-006-1478-z) and allows a \"plug-and-play approach\"\nwith alternative choices of multidimensional models for RA and RT. The model\nparameters within the framework were estimated using the Bayesian Markov chain\nMonte Carlo method. The 2012 Program for International Student Assessment\ncomputer-based mathematics data were analyzed first to illustrate the\nimplications and applications of the proposed models. The results indicated\nthat it is appropriate to simultaneously consider the multidimensionality of\nlatent speed and latent ability for multidimensional tests. A brief simulation\nstudy was conducted to evaluate the parameter recovery of the proposed model\nand the consequences of ignoring the multidimensionality of latent speed.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 08:49:55 GMT"}, {"version": "v2", "created": "Sat, 14 Jul 2018 04:20:11 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Zhan", "Peida", ""], ["Jiao", "Hong", ""], ["Wang", "Wen-Chung", ""], ["Man", "Kaiwen", ""]]}, {"id": "1807.04133", "submitter": "Kei Hirose", "authors": "Kei Hirose and Hiroki Masuda", "title": "Robust relative error estimation", "comments": "34 pages, 6 figures", "journal-ref": null, "doi": "10.3390/e20090632", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relative error estimation has been recently used in regression analysis. A\ncrucial issue of the existing relative error estimation procedures is that they\nare sensitive to outliers. To address this issue, we employ the\n$\\gamma$-likelihood function, which is constructed through $\\gamma$-cross\nentropy with keeping the original statistical model in use. The estimating\nequation has a redescending property, a desirable property in robust\nstatistics, for a broad class of noise distributions. To find a minimizer of\nthe negative $\\gamma$-likelihood function, a majorize-minimization (MM)\nalgorithm is constructed. The proposed algorithm is guaranteed to decrease the\nnegative $\\gamma$-likelihood function at each iteration. We also derive\nasymptotic normality of the corresponding estimator together with a simple\nconsistent estimator of the asymptotic covariance matrix, so that we can\nreadily construct approximate confidence sets. Monte Carlo simulation is\nconducted to investigate the effectiveness of the proposed procedure. Real data\nanalysis illustrates the usefulness of our proposed procedure.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 13:49:49 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Hirose", "Kei", ""], ["Masuda", "Hiroki", ""]]}, {"id": "1807.04164", "submitter": "Richard Berk", "authors": "Richard Berk, Matthew Olson, Andreas Buja, and Aurelie Ouss", "title": "Using Recursive Partitioning to Find and Estimate Heterogenous Treatment\n  Effects In Randomized Clinical Trials", "comments": "21 pages, 1 figure, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous treatment effects can be very important in the analysis of\nrandomized clinical trials. Heightened risks or enhanced benefits may exist for\nparticular subsets of study subjects. When the heterogeneous treatment effects\nare specified as the research is being designed, there are proper and readily\navailable analysis techniques. When the heterogeneous treatment effects are\ninductively obtained as an experiment's data are analyzed, significant\ncomplications are introduced. There can be a need for special loss functions\ndesigned to find local average treatment effects and for techniques that\nproperly address post selection statistical inference. In this paper, we tackle\nboth while undertaking a recursive partitioning analysis of a randomized\nclinical trial testing whether individuals on probation, who are low risk, can\nbe minimally supervised with no increase in recidivism.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 14:40:31 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Berk", "Richard", ""], ["Olson", "Matthew", ""], ["Buja", "Andreas", ""], ["Ouss", "Aurelie", ""]]}, {"id": "1807.04272", "submitter": "Philip White", "authors": "Philip White, Emilio Porcu", "title": "Towards a Complete Picture of Stationary Covariance Functions on Spheres\n  Cross Time", "comments": null, "journal-ref": "The Electronic Journal of Statistics, 13(2), 2566-2594, (2019)", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of wide-spread global and continental-scale spatiotemporal\ndatasets, increased attention has been given to covariance functions on spheres\nover time. This paper provides results for stationary covariance functions of\nrandom fields defined over $d$-dimensional spheres cross time. Specifically, we\nprovide a bridge between the characterization in \\cite{berg-porcu} for\ncovariance functions on spheres cross time and Gneiting's lemma\n\\citep{gneiting2002} that deals with planar surfaces.\n  We then prove that there is a valid class of covariance functions similar in\nform to the Gneiting class of space-time covariance functions\n\\citep{gneiting2002} that replaces the squared Euclidean distance with the\ngreat circle distance. Notably, the provided class is shown to be positive\ndefinite on every $d$-dimensional sphere cross time, while the Gneiting class\nis positive definite over $\\R^d \\times \\R$ for fixed $d$ only.\n  In this context, we illustrate the value of our adapted Gneiting class by\ncomparing examples from this class to currently established nonseparable\ncovariance classes using out-of-sample predictive criteria. These comparisons\nare carried out on two climate reanalysis datasets from the National Centers\nfor Environmental Prediction and National Center for Atmospheric Research. For\nthese datasets, we show that examples from our covariance class have better\npredictive performance than competing models.\n", "versions": [{"version": "v1", "created": "Wed, 11 Jul 2018 13:26:02 GMT"}, {"version": "v2", "created": "Thu, 13 Jun 2019 17:17:59 GMT"}], "update_date": "2021-01-06", "authors_parsed": [["White", "Philip", ""], ["Porcu", "Emilio", ""]]}, {"id": "1807.04426", "submitter": "Mingao Yuan", "authors": "Mingao Yuan, Yang Feng and Zuofeng Shang", "title": "A likelihood-ratio type test for stochastic block models with bounded\n  degrees", "comments": "In this new submission, we add a comment in introduction stating that\n  > the classic test based on counting the $k_n$-cycles with >\n  $k_n=\\log^{1/4}{n}$ is unrealistic in practice, which is also the >\n  motivation of our regularized LR test", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in network data analysis is to test Erd\\\"{o}s-R\\'{e}nyi\nmodel $\\mathcal{G}\\left(n,\\frac{a+b}{2n}\\right)$ versus a bisection stochastic\nblock model $\\mathcal{G}\\left(n,\\frac{a}{n},\\frac{b}{n}\\right)$, where $a,b>0$\nare constants that represent the expected degrees of the graphs and $n$ denotes\nthe number of nodes. This problem serves as the foundation of many other\nproblems such as testing-based methods for determining the number of\ncommunities (\\cite{BS16,L16}) and community detection (\\cite{MS16}). Existing\nwork has been focusing on growing-degree regime $a,b\\to\\infty$\n(\\cite{BS16,L16,MS16,BM17,B18,GL17a,GL17b}) while leaving the bounded-degree\nregime untreated. In this paper, we propose a likelihood-ratio (LR) type\nprocedure based on regularization to test stochastic block models with bounded\ndegrees. We derive the limit distributions as power Poisson laws under both\nnull and alternative hypotheses, based on which the limit power of the test is\ncarefully analyzed. We also examine a Monte-Carlo method that partly resolves\nthe computational cost issue. The proposed procedures are examined by both\nsimulated and real-world data. The proof depends on a contiguity theory\ndeveloped by Janson \\cite{J95}.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 05:05:09 GMT"}, {"version": "v2", "created": "Fri, 23 Nov 2018 02:21:09 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Yuan", "Mingao", ""], ["Feng", "Yang", ""], ["Shang", "Zuofeng", ""]]}, {"id": "1807.04429", "submitter": "Miles Lopes", "authors": "Miles E. Lopes, Zhenhua Lin, Hans-Georg Mueller", "title": "Bootstrapping Max Statistics in High Dimensions: Near-Parametric Rates\n  Under Weak Variance Decay and Application to Functional and Multinomial Data", "comments": "64 pages; to appear in The Annals of Statistics, 2019+", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, bootstrap methods have drawn attention for their ability to\napproximate the laws of \"max statistics\" in high-dimensional problems. A\nleading example of such a statistic is the coordinate-wise maximum of a sample\naverage of $n$ random vectors in $\\mathbb{R}^p$. Existing results for this\nstatistic show that the bootstrap can work when $n\\ll p$, and rates of\napproximation (in Kolmogorov distance) have been obtained with only logarithmic\ndependence in $p$. Nevertheless, one of the challenging aspects of this setting\nis that established rates tend to scale like $n^{-1/6}$ as a function of $n$.\n  The main purpose of this paper is to demonstrate that improvement in rate is\npossible when extra model structure is available. Specifically, we show that if\nthe coordinate-wise variances of the observations exhibit decay, then a nearly\n$n^{-1/2}$ rate can be achieved, independent of $p$. Furthermore, a surprising\naspect of this dimension-free rate is that it holds even when the decay is very\nweak. Lastly, we provide examples showing how these ideas can be applied to\ninference problems dealing with functional and multinomial data.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 05:25:41 GMT"}, {"version": "v2", "created": "Sat, 20 Jul 2019 02:16:33 GMT"}], "update_date": "2019-07-23", "authors_parsed": [["Lopes", "Miles E.", ""], ["Lin", "Zhenhua", ""], ["Mueller", "Hans-Georg", ""]]}, {"id": "1807.04431", "submitter": "Yen-Chi Chen", "authors": "Yen-Chi Chen", "title": "Statistical Inference with Local Optima", "comments": "66 page, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the statistical properties of an estimator derived by applying a\ngradient ascent method with multiple initializations to a multi-modal\nlikelihood function. We derive the population quantity that is the target of\nthis estimator and study the properties of confidence intervals (CIs)\nconstructed from asymptotic normality and the bootstrap approach. In\nparticular, we analyze the coverage deficiency due to finite number of random\ninitializations. We also investigate the CIs by inverting the likelihood ratio\ntest, the score test, and the Wald test, and we show that the resulting CIs may\nbe very different. We provide a summary of the uncertainties that we need to\nconsider while making inference about the population. Note that we do not\nprovide a solution to the problem of multiple local maxima; instead, our goal\nis to investigate the effect from local maxima on the behavior of our\nestimator. In addition, we analyze the performance of the EM algorithm under\nrandom initializations and derive the coverage of a CI with a finite number of\ninitializations. Finally, we extend our analysis to a nonparametric mode\nhunting problem.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 06:08:33 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Chen", "Yen-Chi", ""]]}, {"id": "1807.04450", "submitter": "Kattumannil Sudheesh Dr", "authors": "Deepesh Bhati, Sudheesh K Kattumannil and N Sreelakshmi", "title": "Jackknife empirical likelihood based inference for Probability weighted\n  moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the present article, we discuss jackknife empirical likelihood (JEL) and\nadjusted jackknife empirical likelihood (AJEL) based inference for finding\nconfidence intervals for probability weighted moment (PWM). We obtain the\nasymptotic distribution of the JEL ratio and AJEL ratio statistics. We compare\nthe performance of the proposed confidence intervals with recently developed\nmethods in terms of coverage probability and average length. We also develop\nJEL and AJEL based test for PWM and study it properties. Finally we illustrate\nour method using rainfall data of Indian states.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 07:43:18 GMT"}], "update_date": "2018-07-13", "authors_parsed": [["Bhati", "Deepesh", ""], ["Kattumannil", "Sudheesh K", ""], ["Sreelakshmi", "N", ""]]}, {"id": "1807.04680", "submitter": "Yuan Zhang", "authors": "Yuan Zhang", "title": "Unseeded low-rank graph matching by transform-based unsupervised point\n  registration", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of learning a correspondence relationship between nodes of two\nnetworks has drawn much attention of the computer science community and\nrecently that of statisticians. The unseeded version of this problem, in which\nwe do not know any part of the true correspondence, is a long-standing\nchallenge. For low-rank networks, the problem can be translated into an\nunsupervised point registration problem, in which two point sets generated from\nthe same distribution are matchable by an unknown orthonormal transformation.\nConventional methods generally lack consistency guarantee and are usually\ncomputationally costly.\n  In this paper, we propose a novel approach to this problem. Instead of\nsimultaneously estimating the unknown correspondence and orthonormal\ntransformation to match up the two point sets, we match their distributions via\nminimizing our designed loss function capturing the discrepancy between their\nLaplace transforms, thus avoiding the optimization over all possible\ncorrespondences. This dramatically reduces the dimension of the optimization\nproblem from $\\Omega(n^2)$ parameters to $O(d^2)$ parameters, where $d$ is the\nfixed rank, and enables convenient theoretical analysis. In this paper, we\nprovide arguably the first consistency guarantee and explicit error rate for\ngeneral low-rank models. Our method provides control over the computational\ncomplexity ranging from $\\omega(n)$ (any growth rate faster than $n$) to\n$O(n^2)$ while pertaining consistency. We demonstrate the effectiveness of our\nmethod through several numerical examples.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 15:37:32 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Zhang", "Yuan", ""]]}, {"id": "1807.04982", "submitter": "Yipeng Song", "authors": "Yipeng Song, Johan A. Westerhuis, Nanne Aben, Lodewyk F.A. Wessels,\n  Patrick J.F. Groenen, Age K. Smilde", "title": "Generalized simultaneous component analysis of binary and quantitative\n  data", "comments": "19 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current era of systems biological research there is a need for the\nintegrative analysis of binary and quantitative genomics data sets measured on\nthe same objects. One standard tool of exploring the underlying dependence\nstructure present in multiple quantitative data sets is simultaneous component\nanalysis (SCA) model. However, it does not have any provisions when a part of\nthe data are binary. To this end, we propose the generalized SCA (GSCA) model,\nwhich takes into account the distinct mathematical properties of binary and\nquantitative measurements in the maximum likelihood framework. Like in the SCA\nmodel, a common low dimensional subspace is assumed to represent the shared\ninformation between these two distinct types of measurements. However, the GSCA\nmodel can easily be overfitted when a rank larger than one is used, leading to\nsome of the estimated parameters to become very large. To achieve a low rank\nsolution and combat overfitting, we propose to use a concave variant of the\nnuclear norm penalty. An efficient majorization algorithm is developed to fit\nthis model with different concave penalties. Realistic simulations (low\nsignal-to-noise ratio and highly imbalanced binary data) are used to evaluate\nthe performance of the proposed model in recovering the underlying structure.\nAlso, a missing value based cross validation procedure is implemented for model\nselection. We illustrate the usefulness of the GSCA model for exploratory data\nanalysis of quantitative gene expression and binary copy number aberration\n(CNA) measurements obtained from the GDSC1000 data sets.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 09:31:46 GMT"}, {"version": "v2", "created": "Fri, 24 Aug 2018 16:44:13 GMT"}, {"version": "v3", "created": "Mon, 3 Jun 2019 09:04:35 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Song", "Yipeng", ""], ["Westerhuis", "Johan A.", ""], ["Aben", "Nanne", ""], ["Wessels", "Lodewyk F. A.", ""], ["Groenen", "Patrick J. F.", ""], ["Smilde", "Age K.", ""]]}, {"id": "1807.05035", "submitter": "Debolina Ghatak", "authors": "Debolina Ghatak and Bimak K Roy", "title": "Conditional Masking to Numerical Data", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Protecting the privacy of data-sets has become hugely important these days.\nMany real-life data-sets like income data, medical data need to be secured\nbefore making it public. However, security comes at the cost of losing some\nuseful statistical information about the data-set. Data obfuscation deals with\nthis problem of masking a data-set in such a way that the utility of the data\nis maximized while minimizing the risk of the disclosure of sensitive\ninformation. Two popular approaches to data obfuscation for numerical data\ninvolves (i) data swapping and (ii) adding noise to data. While the former\nmasks well sacrificing the whole of correlation information, the latter gives\nestimates for most of the popular statistics like mean, variance, quantiles,\ncorrelation but fails to give an unbiased estimate of the distribution curve of\nthe original data. In this paper, we propose a mixed method of obfuscation\ncombining the above two approaches and discuss how the proposed method succeeds\nin giving an unbiased estimation of the distribution curve while giving\nreliable estimates of the other well-known statistics like moments,\ncorrelation.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 12:29:06 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Ghatak", "Debolina", ""], ["Roy", "Bimak K", ""]]}, {"id": "1807.05048", "submitter": "Rand Wilcox", "authors": "Rand Wilcox, Guillaume Rousselet, Cyril Pernet", "title": "Improved Methods for Making Inferences About Multiple Skipped\n  Correlations", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A skipped correlation has the advantage of dealing with outliers in a manner\nthat takes into account the overall structure of the data cloud. For p-variate\ndata, $p \\ge 2$, there is an extant method for testing the hypothesis of a zero\ncorrelation for each pair of variables that is designed to control the\nprobability of one or more Type I errors. And there are methods for the related\nsituation where the focus is on the association between a dependent variable\nand $p$ explanatory variables. However, there are limitations and several\nconcerns with extant techniques. The paper describes alternative approaches\nthat deal with these issues.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 13:09:28 GMT"}], "update_date": "2018-07-16", "authors_parsed": [["Wilcox", "Rand", ""], ["Rousselet", "Guillaume", ""], ["Pernet", "Cyril", ""]]}, {"id": "1807.05066", "submitter": "Matthew Williams", "authors": "Matthew R. Williams and Terrance D. Savitsky", "title": "Bayesian Estimation Under Informative Sampling with Unattenuated\n  Dependence", "comments": "35 pages, 5 figures. arXiv admin note: text overlap with\n  arXiv:1710.10102", "journal-ref": "Bayesian Anal., advance publication, 4 January 2019", "doi": "10.1214/18-BA1143", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An informative sampling design leads to unit inclusion probabilities that are\ncorrelated with the response variable of interest. However, multistage sampling\ndesigns may also induce higher order dependencies, which are typically ignored\nin the literature when establishing consistency of estimators for survey data\nunder a condition requiring asymptotic independence among the unit inclusion\nprobabilities. We refine and relax this condition of asymptotic independence or\nasymptotic factorization and demonstrate that consistency is still achieved in\nthe presence of residual sampling dependence. A popular approach for conducting\ninference on a population based on a survey sample is the use of a\npseudo-posterior, which uses sampling weights based on first order inclusion\nprobabilities to exponentiate the likelihood. We show that the pseudo-posterior\nis consistent not only for survey designs which have asymptotic factorization,\nbut also for designs with residual or unattenuated dependence. Using the\ncomplex sampling design of the National Survey on Drug Use and Health, we\nexplore the impact of multistage designs and order based sampling. The use of\nthe survey-weighted pseudo-posterior together with our relaxed requirements for\nthe survey design establish a broad class of analysis models that can be\napplied to a wide variety of survey data sets.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 15:39:21 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Williams", "Matthew R.", ""], ["Savitsky", "Terrance D.", ""]]}, {"id": "1807.05234", "submitter": "Holger Dette", "authors": "Kira Alhorn, Kirsten Schorning, Holger Dette", "title": "Optimal designs for frequentist model averaging", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing experiments for the estimation of a\ntarget in regression analysis if there is uncertainty about the parametric form\nof the regression function. A new optimality criterion is proposed, which\nminimizes the asymptotic mean squared error of the frequentist model averaging\nestimate by the choice of an experimental design. Necessary conditions for the\noptimal solution of a locally and Bayesian optimal design problem are\nestablished. The results are illustrated in several examples and it is\ndemonstrated that Bayesian optimal designs can yield a reduction of the mean\nsquared error of the model averaging estimator up to $45\\%$.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 18:00:17 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Alhorn", "Kira", ""], ["Schorning", "Kirsten", ""], ["Dette", "Holger", ""]]}, {"id": "1807.05274", "submitter": "Grace Yoon", "authors": "Grace Yoon, Raymond J. Carroll, Irina Gaynanova", "title": "Sparse semiparametric canonical correlation analysis for data of mixed\n  types", "comments": "Accepted to Biometrika. Main text: 19 pages and 3 figures.\n  Supplementary material: 28 pages and 9 figures", "journal-ref": "Biometrika 2020, Vol. 107, No. 3, 609-625", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Canonical correlation analysis investigates linear relationships between two\nsets of variables, but often works poorly on modern data sets due to\nhigh-dimensionality and mixed data types such as continuous, binary and\nzero-inflated. To overcome these challenges, we propose a semiparametric\napproach for sparse canonical correlation analysis based on Gaussian copula.\nOur main contribution is a truncated latent Gaussian copula model for data with\nexcess zeros, which allows us to derive a rank-based estimator of the latent\ncorrelation matrix for mixed variable types without the estimation of marginal\ntransformation functions. The resulting canonical correlation analysis method\nworks well in high-dimensional settings as demonstrated via numerical studies,\nas well as in application to the analysis of association between gene\nexpression and micro RNA data of breast cancer patients.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 20:18:32 GMT"}, {"version": "v2", "created": "Thu, 10 Oct 2019 04:59:20 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yoon", "Grace", ""], ["Carroll", "Raymond J.", ""], ["Gaynanova", "Irina", ""]]}, {"id": "1807.05313", "submitter": "Linbo Wang", "authors": "Linbo Wang, Eric Tchetgen Tchetgen, Torben Martinussen and Stijn\n  Vansteelandt", "title": "Learning Causal Hazard Ratio with Endogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cox's proportional hazards model is one of the most popular statistical\nmodels to evaluate associations of a binary exposure with a censored failure\ntime outcome. When confounding factors are not fully observed, the exposure\nhazard ratio estimated using a Cox model is not causally interpretable. To\naddress this, we propose novel approaches for identification and estimation of\nthe causal hazard ratio in the presence of unmeasured confounding factors. Our\napproaches are based on a binary instrumental variable and an additional\nno-interaction assumption. We derive, to the best of our knowledge, the first\nconsistent estimator of the population marginal causal hazard ratio within an\ninstrumental variable framework. Our estimator admits a closed-form\nrepresentation, and hence avoids the drawbacks of estimating equation based\nestimators. Our approach is illustrated via simulation studies and a data\nanalysis.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 00:20:37 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Wang", "Linbo", ""], ["Tchetgen", "Eric Tchetgen", ""], ["Martinussen", "Torben", ""], ["Vansteelandt", "Stijn", ""]]}, {"id": "1807.05405", "submitter": "Thomas Berrett", "authors": "Thomas B. Berrett, Yi Wang, Rina Foygel Barber and Richard J. Samworth", "title": "The conditional permutation test for independence while controlling for\n  confounders", "comments": "31 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general new method, the conditional permutation test, for\ntesting the conditional independence of variables $X$ and $Y$ given a\npotentially high-dimensional random vector $Z$ that may contain confounding\nfactors. The proposed test permutes entries of $X$ non-uniformly, so as to\nrespect the existing dependence between $X$ and $Z$ and thus account for the\npresence of these confounders. Like the conditional randomization test of\nCand\\`es et al. (2018), our test relies on the availability of an approximation\nto the distribution of $X \\mid Z$. While Cand\\`es et al. (2018)'s test uses\nthis estimate to draw new $X$ values, for our test we use this approximation to\ndesign an appropriate non-uniform distribution on permutations of the $X$\nvalues already seen in the true data. We provide an efficient Markov Chain\nMonte Carlo sampler for the implementation of our method, and establish bounds\non the Type I error in terms of the error in the approximation of the\nconditional distribution of $X\\mid Z$, finding that, for the worst case test\nstatistic, the inflation in Type I error of the conditional permutation test is\nno larger than that of the conditional randomization test. We validate these\ntheoretical results with experiments on simulated data and on the Capital\nBikeshare data set.\n", "versions": [{"version": "v1", "created": "Sat, 14 Jul 2018 15:03:24 GMT"}, {"version": "v2", "created": "Tue, 7 May 2019 15:25:23 GMT"}], "update_date": "2019-05-08", "authors_parsed": [["Berrett", "Thomas B.", ""], ["Wang", "Yi", ""], ["Barber", "Rina Foygel", ""], ["Samworth", "Richard J.", ""]]}, {"id": "1807.05600", "submitter": "Philip White", "authors": "Philip A. White, Emilio Porcu", "title": "Modeling Daily Seasonality of Mexico City Ozone using Nonseparable\n  Covariance Models on Circles Cross Time", "comments": null, "journal-ref": "Environmetrics. 2019;e2558", "doi": "10.1002/env.2558", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mexico City tracks ground-level ozone levels to assess compliance with\nnational ambient air quality standards and to prevent environmental health\nemergencies. Ozone levels show distinct daily patterns, within the city, and\nover the course of the year. To model these data, we use covariance models over\nspace, circular time, and linear time. We review existing models and develop\nnew classes of nonseparable covariance models of this type, models appropriate\nfor quasi-periodic data collected at many locations. With these covariance\nmodels, we use nearest-neighbor Gaussian processes to predict hourly ozone\nlevels at unobserved locations in April and May, the peak ozone season, to\ninfer compliance to Mexican air quality standards and to estimate respiratory\nhealth risk associated with ozone. Predicted compliance with air quality\nstandards and estimated respiratory health risk vary greatly over space and\ntime. In some regions, we predict exceedance of national standards for more\nthan a third of the hours in April and May. On many days, we predict that\nnearly all of Mexico City exceeds nationally legislated ozone thresholds at\nleast once. In peak regions, we estimate respiratory risk for ozone to be 55%\nhigher on average than the annual average risk and as much at 170% higher on\nsome days.\n", "versions": [{"version": "v1", "created": "Sun, 15 Jul 2018 19:35:46 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["White", "Philip A.", ""], ["Porcu", "Emilio", ""]]}, {"id": "1807.05662", "submitter": "Atanu Ghosh Kumar", "authors": "Atanu Kumar Ghosh, Arnab Chakraborty", "title": "Data Reduction in Markov model using EM algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a data reduction technique in case of a markov chain of\nspecified order. Instead of observing all the transitions in a markov chain we\nrecord only a few of them and treat the remaining part as missing. The decision\nabout which transitions to be filtered is taken before the observation process\nstarts. Based on the filtered chain we try to estimate the parameters of the\nmarkov model using EM algorithm. In the first half of the paper we characterize\na class of filtering mechanism for which all the parameters remain\nidentifiable. In the later half we explain methods of estimation and testing\nabout the transition probabilities of the markov chain based on the filtered\ndata. The methods are first developed assuming a simple markov model with each\nprobability of transition positive, but then generalized for models with\nstructural zeroes in the transition probability matrix. Further extension is\nalso done for multiple markov chains. The performance of the developed method\nof estimation is studied using simulated data along with a real life data.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 02:48:16 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Ghosh", "Atanu Kumar", ""], ["Chakraborty", "Arnab", ""]]}, {"id": "1807.05675", "submitter": "Jingyi Kenneth Tay", "authors": "J. Kenneth Tay, Robert Tibshirani", "title": "A latent factor approach for prediction from multiple assays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many domains such as healthcare or finance, data often come in different\nassays or measurement modalities, with features in each assay having a common\ntheme. Simply concatenating these assays together and performing prediction can\nbe effective but ignores this structure. In this setting, we propose a model\nwhich contains latent factors specific to each assay, as well as a common\nlatent factor across assays. We frame our model-fitting procedure, which we\ncall the \"Sparse Factor Method\" (SFM), as an optimization problem and present\nan iterative algorithm to solve it.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 04:30:58 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Tay", "J. Kenneth", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1807.05781", "submitter": "Pavel Mozgunov", "authors": "Pavel Mozgunov and Thomas Jaki", "title": "Improving Safety of the Continual Reassessment Method via a Modified\n  Allocation Rule", "comments": "26 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel criterion for the allocation of patients in\nPhase~I dose-escalation clinical trials aiming to find the maximum tolerated\ndose (MTD). Conventionally, using a model-based approach the next patient is\nallocated to the dose with the toxicity estimate closest (in terms of the\nabsolute or squared distance) to the maximum acceptable toxicity. This\napproach, however, ignores the uncertainty in point estimates and ethical\nconcerns of assigning a lot of patients to overly toxic doses. Motivated by\nrecent discussions in the theory of estimation in restricted parameter spaces,\nwe propose a criterion which accounts for both of these issues. The criterion\nrequires a specification of one additional parameter only which has a simple\nand intuitive interpretation. We incorporate the proposed criterion into the\none-parameter Bayesian continual reassessment method (CRM) and show, using\nsimulations, that it results in the same proportion of correct selections on\naverage as the original design, but in fewer mean number of toxic responses. A\ncomparison to other model-based dose-escalation designs demonstrates that the\nproposed design can result in either the same mean accuracy as alternatives but\nfewer number of toxic responses, or in a higher mean accuracy but the same\nnumber of toxic responses. We conclude that the new criterion makes the\nexisting model-based designs more ethical without losing efficiency in the\ncontext of Phase I clinical trials.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 10:43:30 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Mozgunov", "Pavel", ""], ["Jaki", "Thomas", ""]]}, {"id": "1807.05819", "submitter": "Joris Mulder", "authors": "Joris Mulder, John P.T.M. Gelissen", "title": "Bayes factor testing of equality and order constraints on measures of\n  association in social research", "comments": "53 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measures of association play a central role in the social sciences to\nquantify the strength of a linear relationship between the variables of\ninterest. In many applications researchers can translate scientific\nexpectations to hypotheses with equality and/or order constraints on these\nmeasures of association. In this paper a Bayes factor test is proposed for\ntesting multiple hypotheses with constraints on the measures of association\nbetween ordinal and/or continuous variables, possibly after correcting for\ncertain covariates. This test can be used to obtain a direct answer to the\nresearch question how much evidence there is in the data for a social science\ntheory relative to competing theories. The accompanying software package `BCT'\nallows users to apply the methodology in an easy manner. An empirical\napplication from leisure studies about the associations between life, leisure\nand relationship satisfaction and an application about the differences about\negalitarian justice beliefs across countries are used to illustrate the\nmethodology.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 12:33:58 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 08:40:36 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Mulder", "Joris", ""], ["Gelissen", "John P. T. M.", ""]]}, {"id": "1807.05846", "submitter": "Zheng Chen", "authors": "Jinbao Chen, Yawen Hou and Zheng Chen", "title": "Statistical inference methods for cumulative incidence function curves\n  at a fixed point in time", "comments": null, "journal-ref": "Communications in Statistics - Simulation and Computation, 2018", "doi": "10.1080/03610918.2018.1476697", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competing risks data arise frequently in clinical trials. When the\nproportional subdistribution hazard assumption is violated or two cumulative\nincidence function (CIF) curves cross, rather than comparing the overall\ntreatment effects, researchers may be interested in focusing on a comparison of\nclinical utility at some fixed time points. This paper extend a series of tests\nthat are constructed based on a pseudo-value regression technique or different\ntransformation functions for CIFs and their variances based on Gaynor's or\nAalen's work, and the differences among CIFs at a given time point are\ncompared.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 13:27:35 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 10:11:46 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Chen", "Jinbao", ""], ["Hou", "Yawen", ""], ["Chen", "Zheng", ""]]}, {"id": "1807.05920", "submitter": "Siamak Zamani Dadaneh", "authors": "Ariana Broumand, Siamak Zamani Dadaneh", "title": "Sequential Sampling for Optimal Bayesian Classification of Sequencing\n  Count Data", "comments": "6 pages, 4 figures, accepted in Asilomar Conference on Signals,\n  Systems, and Computers 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High throughput technologies have become the practice of choice for\ncomparative studies in biomedical applications. Limited number of sample points\ndue to sequencing cost or access to organisms of interest necessitates the\ndevelopment of efficient sample collections to maximize the power of downstream\nstatistical analyses. We propose a method for sequentially choosing training\nsamples under the Optimal Bayesian Classification framework. Specifically\ndesigned for RNA sequencing count data, the proposed method takes advantage of\nefficient Gibbs sampling procedure with closed-form updates. Our results shows\nenhanced classification accuracy, when compared to random sampling.\n", "versions": [{"version": "v1", "created": "Thu, 12 Jul 2018 19:00:31 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Broumand", "Ariana", ""], ["Dadaneh", "Siamak Zamani", ""]]}, {"id": "1807.06111", "submitter": "Weidong Li", "authors": "Weidong Li and Chuanrong Zhang", "title": "Markov chain random fields, spatial Bayesian networks, and optimal\n  neighborhoods for simulation of categorical fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Markov chain random field (MCRF) model/theory provides a non-linear\nspatial Bayesian updating solution at the neighborhood nearest data level for\nsimulating categorical spatial variables. In the MCRF solution, the spatial\ndependencies among nearest data and the central random variable is a\nprobabilistic directed acyclic graph that conforms to a neighborhood-based\nBayesian network on spatial data. By selecting different neighborhood sizes and\nstructures, applying the spatial conditional independence assumption to nearest\nneighbors, or incorporating ancillary information, one may construct specific\nMCRF models based on the MCRF general solution for various application\npurposes. Simplified MCRF models based on assuming the spatial conditional\nindependence of nearest data involve only spatial transition probabilities, and\none can implement them easily in sequential simulations. In this article, we\nprove the spatial Bayesian network characteristic of MCRFs, and test the\noptimal neighborhoods under the spatial conditional independence assumption.\nThe testing results indicate that the quadrantal (i.e., one nearest datum per\nquadrant) neighborhood is generally the best choice for the simplified MCRF\nsolution, performing better than other sectored neighborhoods and non-sectored\nneighborhoods with regard to simulation accuracy and pattern rationality.\n", "versions": [{"version": "v1", "created": "Mon, 16 Jul 2018 21:04:55 GMT"}, {"version": "v2", "created": "Thu, 8 Nov 2018 17:22:02 GMT"}], "update_date": "2018-11-09", "authors_parsed": [["Li", "Weidong", ""], ["Zhang", "Chuanrong", ""]]}, {"id": "1807.06217", "submitter": "Jonathan P Williams", "authors": "Iain Carmichael and Jonathan P Williams", "title": "An exposition of the false confidence theorem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent paper presents the \"false confidence theorem\" (FCT) which has\npotentially broad implications for statistical inference using Bayesian\nposterior uncertainty. This theorem says that with arbitrarily large\n(sampling/frequentist) probability, there exists a set which does \\textit{not}\ncontain the true parameter value, but which has arbitrarily large posterior\nprobability. Since the use of Bayesian methods has become increasingly popular\nin applications of science, engineering, and business, it is critically\nimportant to understand when Bayesian procedures lead to problematic\nstatistical inferences or interpretations. In this paper, we consider a number\nof examples demonstrating the paradoxical nature of false confidence to begin\nto understand the contexts in which the FCT does (and does not) play a\nmeaningful role in statistical inference. Our examples illustrate that models\ninvolving marginalization to non-linear, not one-to-one functions of multiple\nparameters play a key role in more extreme manifestations of false confidence.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 04:27:40 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Carmichael", "Iain", ""], ["Williams", "Jonathan P", ""]]}, {"id": "1807.06367", "submitter": "Norbert Henze", "authors": "Norbert Henze, Celeste Mayer", "title": "More good news on the (only) affine invariant test for multivariate\n  reflected symmetry about an unknown center", "comments": "20 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We revisit the problem of testing for multivariate reflected symmetry about\nan unspecified point. Although this testing problem is invariant with respect\nto full-rank affine transformations, among the hitherto few proposed tests only\nthe test studied in [12] respects this property. We identify a measure of\ndeviation $\\Delta$ (say) from symmetry associated with the test statistic $T_n$\n(say), and we obtain the limit normal distribution of $T_n$ as $n \\to \\infty$\nunder a fixed alternative to symmetry. Since a consistent estimator of the\nvariance of this limit normal distribution is available, we obtain an\nasymptotic confidence interval for $\\Delta$. The test, when applied to a\nclassical data set, strongly rejects the hypothesis of reflected symmetry,\nalthough other tests even do not object against the much stronger hypothesis of\nelliptical symmetry.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 11:56:28 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Henze", "Norbert", ""], ["Mayer", "Celeste", ""]]}, {"id": "1807.06480", "submitter": "Lingji Chen", "authors": "Lingji Chen", "title": "Roos' Matrix Permanent Approximation Bounds for Data Association\n  Probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix permanent plays a key role in data association probability\ncalculations. Exact algorithms (such as Ryser's) scale exponentially with\nmatrix size. Fully polynomial time randomized approximation schemes exist but\nare quite complex. This letter introduces to the tracking community a simple\napproximation algorithm with error bounds, recently developed by Bero Roos, and\nillustrates its potential use for estimating probabilities of data association\nhypotheses.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 14:50:43 GMT"}], "update_date": "2018-07-18", "authors_parsed": [["Chen", "Lingji", ""]]}, {"id": "1807.06539", "submitter": "Ray Bai", "authors": "Ray Bai, Malay Ghosh", "title": "On the Beta Prime Prior for Scale Parameters in High-Dimensional\n  Bayesian Regression Models", "comments": "37 pages, 4 figures, 3 tables. We have added a section on posterior\n  computation and corrected the theoretical results. Sections on normal means\n  estimation were removed in this updated technical report", "journal-ref": null, "doi": "10.5705/ss.202019.0037", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study high-dimensional Bayesian linear regression with a general beta\nprime distribution for the scale parameter. Under the assumption of sparsity,\nwe show that appropriate selection of the hyperparameters in the beta prime\nprior leads to the (near) minimax posterior contraction rate when $p \\gg n$.\nFor finite samples, we propose a data-adaptive method for estimating the\nhyperparameters based on marginal maximum likelihood (MML). This enables our\nprior to adapt to both sparse and dense settings, and under our proposed\nempirical Bayes procedure, the MML estimates are never at risk of collapsing to\nzero. We derive efficient Monte Carlo EM and variational EM algorithms for\nimplementing our model, which are available in the R package NormalBetaPrime.\nSimulations and analysis of a gene expression data set illustrate our model's\nself-adaptivity to varying levels of sparsity and signal strengths.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 16:35:12 GMT"}, {"version": "v2", "created": "Thu, 16 Aug 2018 23:54:48 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 22:03:55 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Bai", "Ray", ""], ["Ghosh", "Malay", ""]]}, {"id": "1807.06661", "submitter": "Madhurima Nath", "authors": "Madhurima Nath and Stephen Eubank", "title": "Model selection for sequential designs in discrete finite systems using\n  Bernstein kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We view sequential design as a model selection problem to determine which new\nobservation is expected to be the most informative, given the existing set of\nobservations. For estimating a probability distribution on a bounded interval,\nwe use bounds constructed from kernel density estimators along with the\nestimated density itself to estimate the information gain expected from each\nobservation. We choose Bernstein polynomials for the kernel functions because\nthey provide a complete set of basis functions for polynomials of finite degree\nand thus have useful convergence properties. We illustrate the method with\napplications to estimating network reliability polynomials, which give the\nprobability of certain sets of configurations in finite, discrete stochastic\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 20:33:34 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Nath", "Madhurima", ""], ["Eubank", "Stephen", ""]]}, {"id": "1807.06690", "submitter": "Luca Scrucca", "authors": "Luca Scrucca", "title": "A transformation-based approach to Gaussian mixture density estimation\n  for bounded data", "comments": null, "journal-ref": "Biometrical Journal, 2019, 61:4, 873--888", "doi": "10.1002/bimj.201800174", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finite mixture of Gaussian distributions provide a flexible semi-parametric\nmethodology for density estimation when the variables under investigation have\nno boundaries. However, in practical applications variables may be partially\nbounded (e.g. taking non-negative values) or completely bounded (e.g. taking\nvalues in the unit interval). In this case the standard Gaussian finite mixture\nmodel assigns non-zero densities to any possible values, even to those outside\nthe ranges where the variables are defined, hence resulting in severe bias. In\nthis paper we propose a transformation-based approach for Gaussian mixture\nmodelling in case of bounded variables. The basic idea is to carry out density\nestimation not on the original data but on appropriately transformed data.\nThen, the density for the original data can be obtained by a change of\nvariables. Both the transformation parameters and the parameters of the\nGaussian mixture are jointly estimated by the Expectation-Maximisation (EM)\nalgorithm. The methodology for partially and completely bounded data is\nillustrated using both simulated data and real data applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 22:18:25 GMT"}, {"version": "v2", "created": "Thu, 13 Dec 2018 10:02:28 GMT"}], "update_date": "2019-12-30", "authors_parsed": [["Scrucca", "Luca", ""]]}, {"id": "1807.06693", "submitter": "Krishnakumar Balasubramanian", "authors": "Krishnakumar Balasubramanian, Jianqing Fan, Zhuoran Yang", "title": "Tensor Methods for Additive Index Models under Discordance and\n  Heterogeneity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the sampling problems and heterogeneity issues common in high-\ndimensional big datasets, we consider a class of discordant additive index\nmodels. We propose method of moments based procedures for estimating the\nindices of such discordant additive index models in both low and\nhigh-dimensional settings. Our estimators are based on factorizing certain\nmoment tensors and are also applicable in the overcomplete setting, where the\nnumber of indices is more than the dimensionality of the datasets. Furthermore,\nwe provide rates of convergence of our estimator in both high and\nlow-dimensional setting. Establishing such results requires deriving tensor\noperator norm concentration inequalities that might be of independent interest.\nFinally, we provide simulation results supporting our theory. Our contributions\nextend the applicability of tensor methods for novel models in addition to\nmaking progress on understanding theoretical properties of such tensor methods.\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 22:28:43 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Balasubramanian", "Krishnakumar", ""], ["Fan", "Jianqing", ""], ["Yang", "Zhuoran", ""]]}, {"id": "1807.06776", "submitter": "Leying Guan", "authors": "Leying Guan, Xi Chen, Wing Hung Wong", "title": "Detecting strong signals in gene perturbation experiments: An adaptive\n  approach with power guarantee and FDR control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The perturbation of a transcription factor should affect the expression\nlevels of its direct targets. However, not all genes showing changes in\nexpression are direct targets. To increase the chance of detecting direct\ntargets, we propose a modified two-group model where the null group corresponds\nto genes which are not direct targets, but can have small non-zero effects. We\nmodel the behaviour of genes from the null set by a Gaussian distribution with\nunknown variance $\\tau^2$, and we discuss and compare three methods which\nadaptively estimate $\\tau^2$ from the data: the iterated empirical Bayes\nestimator, the truncated MLE and the central moment matching estimator. We\nconduct a detailed analysis of the properties of the iterated EB estimate which\nhas the best performance in the simulations. In particular, we provide\ntheoretical guarantee of its good performance under mild conditions.\n  We provide simulations comparing the new modeling approach with existing\nmethods, and the new approach shows more stable and better performance under\ndifferent situations. We also apply it to a real data set from gene knock-down\nexperiments and obtained better results compared with the original two-group\nmodel testing for non-zero effects.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 05:21:47 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Guan", "Leying", ""], ["Chen", "Xi", ""], ["Wong", "Wing Hung", ""]]}, {"id": "1807.06945", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, Gene Whipps, Prudhvi Gurram and Vahid Tarokh", "title": "Cyclostationary Statistical Models and Algorithms for Anomaly Detection\n  Using Multi-Modal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework is proposed to detect anomalies in multi-modal data. A deep\nneural network-based object detector is employed to extract counts of objects\nand sub-events from the data. A cyclostationary model is proposed to model\nregular patterns of behavior in the count sequences. The anomaly detection\nproblem is formulated as a problem of detecting deviations from learned\ncyclostationary behavior. Sequential algorithms are proposed to detect\nanomalies using the proposed model. The proposed algorithms are shown to be\nasymptotically efficient in a well-defined sense. The developed algorithms are\napplied to a multi-modal data consisting of CCTV imagery and social media posts\nto detect a 5K run in New York City.\n", "versions": [{"version": "v1", "created": "Mon, 2 Jul 2018 14:56:37 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Banerjee", "Taposh", ""], ["Whipps", "Gene", ""], ["Gurram", "Prudhvi", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1807.06993", "submitter": "Hajime Shimao", "authors": "Junpei Komiyama and Hajime Shimao", "title": "Cross Validation Based Model Selection via Generalized Method of Moments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural estimation is an important methodology in empirical economics, and\na large class of structural models are estimated through the generalized method\nof moments (GMM). Traditionally, selection of structural models has been\nperformed based on model fit upon estimation, which take the entire observed\nsamples. In this paper, we propose a model selection procedure based on\ncross-validation (CV), which utilizes sample-splitting technique to avoid\nissues such as over-fitting. While CV is widely used in machine learning\ncommunities, we are the first to prove its consistency in model selection in\nGMM framework. Its empirical property is compared to existing methods by\nsimulations of IV regressions and oligopoly market model. In addition, we\npropose the way to apply our method to Mathematical Programming of Equilibrium\nConstraint (MPEC) approach. Finally, we perform our method to online-retail\nsales data to compare dynamic market model to static model.\n", "versions": [{"version": "v1", "created": "Wed, 18 Jul 2018 15:27:00 GMT"}], "update_date": "2018-07-19", "authors_parsed": [["Komiyama", "Junpei", ""], ["Shimao", "Hajime", ""]]}, {"id": "1807.07697", "submitter": "Adam Maidman", "authors": "Lan Wang, Ingrid Van Keilegrom, Adam Maidman", "title": "Wild Residual Bootstrap Inference for Penalized Quantile Regression with\n  Heteroscedastic Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a heteroscedastic regression model in which some of the\nregression coefficients are zero but it is not known which ones. Penalized\nquantile regression is a useful approach for analyzing such data. By allowing\ndifferent covariates to be relevant for modeling conditional quantile functions\nat different quantile levels, it provides a more complete picture of the\nconditional distribution of a response variable than mean regression. Existing\nwork on penalized quantile regression has been mostly focused on point\nestimation. Although bootstrap procedures have recently been shown to be\neffective for inference for penalized mean regression, they are not directly\napplicable to penalized quantile regression with heteroscedastic errors. We\nprove that a wild residual bootstrap procedure for unpenalized quantile\nregression is asymptotically valid for approximating the distribution of a\npenalized quantile regression estimator with an adaptive $L_1$ penalty and that\na modified version can be used to approximate the distribution of\n$L_1$-penalized quantile regression estimator. The new methods do not need to\nestimate the unknown error density function. We establish consistency,\ndemonstrate finite sample performance, and illustrate the applications on a\nreal data example.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 02:23:49 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Wang", "Lan", ""], ["Van Keilegrom", "Ingrid", ""], ["Maidman", "Adam", ""]]}, {"id": "1807.07797", "submitter": "Lee Richardson", "authors": "Lee F. Richardson, William F. Eddy", "title": "The Sliding Window Discrete Fourier Transform", "comments": "27 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new tool for time-series analysis: the Sliding Window\nDiscrete Fourier Transform (SWDFT). The SWDFT is especially useful for\ntime-series with local- in-time periodic components. We define a 5-parameter\nmodel for noiseless local periodic signals, then study the SWDFT of this model.\nOur study illustrates several key concepts crucial to analyzing time-series\nwith the SWDFT, in particular Aliasing, Leakage, and Ringing. We also show how\nthese ideas extend to R > 1 local periodic components, using the linearity\nproperty of the Fourier transform. Next, we propose a simple procedure for\nestimating the 5 parameters of our local periodic signal model using the SWDFT.\nOur estimation procedure speeds up computation by using a trigonometric\nidentity that linearizes estimation of 2 of the 5 parameters. We conclude with\na very small Monte Carlo simulation study of our estimation procedure under\ndifferent levels of noise.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 11:35:35 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Richardson", "Lee F.", ""], ["Eddy", "William F.", ""]]}, {"id": "1807.07812", "submitter": "Tchilabalo Abozou Kpanzou", "authors": "Pape Djiby Mergane, Tchilabalo Abozou Kpanzou, Diam Ba and Gane Samb\n  Lo", "title": "A Theil-like Class of Inequality Measures, its Asymptotic Normality\n  Theory and Applications", "comments": "19 pages, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a coherent theory about the asymptotic\nrepresentations for a family of inequality indices called Theil-Like Inequality\nMeasures (TLIM), within a Gaussian field. The theory uses the functional\nempirical process approach. We provide the finite-distribution and uniform\nasymptotic normality of the elements of the TLIM class in a unified approach\nrather than in a case by case one. The results are then applied to some UEMOA\ncountries databases.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 12:38:06 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Mergane", "Pape Djiby", ""], ["Kpanzou", "Tchilabalo Abozou", ""], ["Ba", "Diam", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1807.07874", "submitter": "Clara Grazian", "authors": "Clara Grazian, Cristiano Villa and Brunero Liseo", "title": "On a Loss-based prior for the number of components in mixture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a prior distribution for the number of components of a finite\nmixture model. The novelty is that the prior distribution is obtained by\nconsidering the loss one would incur if the true value representing the number\nof components were not considered. The prior has an elegant and easy to\nimplement structure, which allows to naturally include any prior information\none may have as well as to opt for a default solution in cases where this\ninformation is not available. The performance of the prior, and comparison with\nexisting alternatives, is studied through the analysis of both real and\nsimulated data.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 14:41:26 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 13:12:17 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Grazian", "Clara", ""], ["Villa", "Cristiano", ""], ["Liseo", "Brunero", ""]]}, {"id": "1807.07996", "submitter": "David Miller", "authors": "Mark V Bravington, David L Miller, Sharon L Hedley", "title": "Variance propagation for density surface models", "comments": "38 pages (incl. supp. mat.), 5 figures", "journal-ref": null, "doi": "10.1007/s13253-021-00438-2", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatially-explicit estimates of population density, together with appropriate\nestimates of uncertainty, are required in many management contexts. Density\nSurface Models (DSMs) are a two-stage approach for estimating spatially-varying\ndensity from distance-sampling data. First, detection probabilities -- perhaps\ndepending on covariates -- are estimated based on details of individual\nencounters; next, local densities are estimated using a GAM, by fitting local\nencounter rates to location and/or spatially-varying covariates while allowing\nfor the estimated detectabilities. One criticism of DSMs has been that\nuncertainty from the two stages is not usually propagated correctly into the\nfinal variance estimates. We show how to reformulate a DSM so that the\nuncertainty in detection probability from the distance sampling stage\n(regardless of its complexity) is captured as an extra random effect in the GAM\nstage. In effect, we refit an approximation to the detection function model at\nthe same time as fitting the spatial model. This allows straightforward\ncomputation of the overall variance via exactly the same software already\nneeded to fit the GAM. A further extension allows for spatial variation in\ngroup size, which can be an important covariate for detectability as well as\ndirectly affecting abundance. We illustrate these models using point transect\nsurvey data of Island Scrub-Jays on Santa Cruz Island, CA and harbour porpoise\nfrom the SCANS-II line transect survey of European waters.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 18:34:18 GMT"}, {"version": "v2", "created": "Thu, 23 Aug 2018 15:15:34 GMT"}, {"version": "v3", "created": "Tue, 25 Sep 2018 12:32:33 GMT"}, {"version": "v4", "created": "Sun, 9 Jun 2019 14:41:06 GMT"}, {"version": "v5", "created": "Sat, 26 Dec 2020 16:02:03 GMT"}], "update_date": "2021-02-25", "authors_parsed": [["Bravington", "Mark V", ""], ["Miller", "David L", ""], ["Hedley", "Sharon L", ""]]}, {"id": "1807.08018", "submitter": "Houman Safaai", "authors": "Houman Safaai, Arno Onken, Christopher D. Harvey, Stefano Panzeri", "title": "Information estimation using nonparametric copulas", "comments": "17 pages, 13 figures", "journal-ref": "Phys. Rev. E 98, 053302 (2018)", "doi": "10.1103/PhysRevE.98.053302", "report-no": null, "categories": "stat.ME cs.IT math.IT q-bio.NC q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of mutual information between random variables has become crucial\nin a range of fields, from physics to neuroscience to finance. Estimating\ninformation accurately over a wide range of conditions relies on the\ndevelopment of flexible methods to describe statistical dependencies among\nvariables, without imposing potentially invalid assumptions on the data. Such\nmethods are needed in cases that lack prior knowledge of their statistical\nproperties and that have limited sample numbers. Here we propose a powerful and\ngenerally applicable information estimator based on non-parametric copulas.\nThis estimator, called the non-parametric copula-based estimator (NPC), is\ntailored to take into account detailed stochastic relationships in the data\nindependently of the data's marginal distributions. The NPC estimator can be\nused both for continuous and discrete numerical variables and thus provides a\nsingle framework for the mutual information estimation of both continuous and\ndiscrete data. By extensive validation on artificial samples drawn from various\nstatistical distributions, we found that the NPC estimator compares well\nagainst commonly used alternatives. Unlike methods not based on copulas, it\nallows an estimation of information that is robust to changes of the details of\nthe marginal distributions. Unlike parametric copula methods, it remains\naccurate regardless of the precise form of the interactions between the\nvariables. In addition, the NPC estimator had accurate information estimates\neven at low sample numbers, in comparison to alternative estimators. The NPC\nestimator therefore provides a good balance between general applicability to\narbitrarily shaped statistical dependencies in the data and shows accurate and\nrobust performance when working with small sample sizes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 20:17:59 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 20:56:28 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Safaai", "Houman", ""], ["Onken", "Arno", ""], ["Harvey", "Christopher D.", ""], ["Panzeri", "Stefano", ""]]}, {"id": "1807.08038", "submitter": "Peter Hoff", "authors": "Peter D. Hoff", "title": "Additive and multiplicative effects network models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network datasets typically exhibit certain types of statistical dependencies,\nsuch as within-dyad correlation, row and column heterogeneity, and third-order\ndependence patterns such as transitivity and clustering. The first two of these\ncan be well-represented statistically with a social relations model, a type of\nadditive random effects model originally developed for continuous dyadic data.\nThird-order patterns can be represented with multiplicative random effects\nmodels, which are related to matrix decompositions commonly used for\nmatrix-variate data analysis. Additionally, these multiplicative random effects\nmodels generalize other popular latent variable network models, such as the\nstochastic blockmodel and the latent space model. In this article we review a\ngeneral regression framework for the analysis of network data that combines\nthese two types of random effects and accommodates a variety of network data\ntypes, including continuous, binary and ordinal network relations.\n", "versions": [{"version": "v1", "created": "Fri, 20 Jul 2018 21:32:33 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Hoff", "Peter D.", ""]]}, {"id": "1807.08213", "submitter": "Yuguang Yue", "authors": "Yuguang Yue, Lieven Vandenberghe, Weng Kee Wong", "title": "T-optimal designs for multi-factor polynomial regression models via a\n  semidefinite relaxation method", "comments": "22 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider T-optimal experiment design problems for discriminating\nmulti-factor polynomial regression models where the design space is defined by\npolynomial inequalities and the regression parameters are constrained to given\nconvex sets. Our proposed optimality criterion is formulated as a convex\noptimization problem with a moment cone constraint. When the regression models\nhave one factor, an exact semidefinite representation of the moment cone\nconstraint can be applied to obtain an equivalent semidefinite program. When\nthere are two or more factors in the models, we apply a moment relaxation\ntechnique and approximate the moment cone constraint by a hierarchy of\nsemidefinite-representable outer approximations. When the relaxation hierarchy\nconverges, an optimal discrimination design can be recovered from the optimal\nmoment matrix, and its optimality is confirmed by an equivalence theorem. The\nmethodology is illustrated with several examples.\n", "versions": [{"version": "v1", "created": "Sat, 21 Jul 2018 23:51:07 GMT"}, {"version": "v2", "created": "Sun, 2 Feb 2020 16:08:29 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Yue", "Yuguang", ""], ["Vandenberghe", "Lieven", ""], ["Wong", "Weng Kee", ""]]}, {"id": "1807.08216", "submitter": "Bal\\'azs Csan\\'ad Cs\\'aji", "authors": "Bal\\'azs Cs. Cs\\'aji, Marco C. Campi, Erik Weyer", "title": "Sign-Perturbed Sums: A New System Identification Approach for\n  Constructing Exact Non-Asymptotic Confidence Regions in Linear Regression\n  Models", "comments": "12 pages, 7 figures, 8 tables, 32 references", "journal-ref": "IEEE Transactions on Signal Processing, Volume 63, Issue 1, 2015,\n  pp. 169-181", "doi": "10.1109/TSP.2014.2369000", "report-no": null, "categories": "eess.SP cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new system identification method, called Sign-Perturbed Sums\n(SPS), for constructing non-asymptotic confidence regions under mild\nstatistical assumptions. SPS is introduced for linear regression models,\nincluding but not limited to FIR systems, and we show that the SPS confidence\nregions have exact confidence probabilities, i.e., they contain the true\nparameter with a user-chosen exact probability for any finite data set.\nMoreover, we also prove that the SPS regions are star convex with the\nLeast-Squares (LS) estimate as a star center. The main assumptions of SPS are\nthat the noise terms are independent and symmetrically distributed about zero,\nbut they can be nonstationary, and their distributions need not be known. The\npaper also proposes a computationally efficient ellipsoidal outer approximation\nalgorithm for SPS. Finally, SPS is demonstrated through a number of simulation\nexperiments.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 00:43:35 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Cs\u00e1ji", "Bal\u00e1zs Cs.", ""], ["Campi", "Marco C.", ""], ["Weyer", "Erik", ""]]}, {"id": "1807.08225", "submitter": "Bomin Kim", "authors": "Bomin Kim, Aaron Schein, Bruce A. Desmarais, Hanna Wallach", "title": "The Hyperedge Event Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the hyperedge event model (HEM)---a generative model for events\nthat can be represented as directed edges with one sender and one or more\nreceivers or one receiver and one or more senders. We integrate a dynamic\nversion of the exponential random graph model (ERGM) of edge structure with a\nsurvival model for event timing to jointly understand who interacts with whom,\nand when. The HEM offers three innovations with respect to the\nliterature---first, it extends a growing class of dynamic network models to\nmodel hyperedges. The current state-of-the-art approach to dealing with\nhyperedges is to inappropriately break them into separate edges/events. Second,\nour model involves a novel receiver selection distribution that is based on\nestablished edge formation models, but assures non-empty receiver lists. Third,\nthe HEM integrates separate, but interacting, equations governing edge\nformation and event timing. We use the HEM to analyze emails sent among\ndepartment managers in Montgomery County government in North Carolina. Our\napplication demonstrates that the model is effective at predicting and\nexplaining time-stamped network data involving edges with multiple receivers.\nWe present an out-of-sample prediction experiment to illustrate how researchers\ncan select between different specifications of the model.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 02:06:28 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Kim", "Bomin", ""], ["Schein", "Aaron", ""], ["Desmarais", "Bruce A.", ""], ["Wallach", "Hanna", ""]]}, {"id": "1807.08256", "submitter": "Diam Ba", "authors": "Tchilabalo Abozou Kpanzou, Diam Ba, Pape Djiby Mergane and Gane Samb\n  Lo", "title": "On the influence function for the Theil-like class of inequality\n  measures", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On one hand, a large class of inequality measures, which includes the\ngeneralized entropy, the Atkinson, the Gini, etc., for example, has been\nintroduced in Mergane and Lo (2013). On the other hand, the influence function\nof statistics is an important tool in the asymptotics of a nonparametric\nstatistic. This function has been and is being determined and analysed in\nvarious aspects for a large number of statistics. We proceed to a unifying\nstudy of the IF of all the members of the so-called Theil-like family and\nregroup those IF's in one formula. Comparative studies become easier.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 08:17:40 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Kpanzou", "Tchilabalo Abozou", ""], ["Ba", "Diam", ""], ["Mergane", "Pape Djiby", ""], ["Lo", "Gane Samb", ""]]}, {"id": "1807.08380", "submitter": "Anjali Silva", "authors": "Anjali Silva, Steven J. Rothstein, Paul D. McNicholas, Sanjeena Subedi", "title": "Finite mixtures of matrix-variate Poisson-log normal distributions for\n  three-way count data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Three-way data structures, characterized by three entities, the units, the\nvariables and the occasions, are frequent in biological studies. In RNA\nsequencing, three-way data structures are obtained when high-throughput\ntranscriptome sequencing data are collected for n genes across p conditions at\nr occasions. Matrix-variate distributions offer a natural way to model\nthree-way data and mixtures of matrix-variate distributions can be used to\ncluster three-way data. Clustering of gene expression data is carried out as\nmeans to discovering gene co-expression networks. In this work, a mixture of\nmatrix-variate Poisson-log normal distributions is proposed for clustering read\ncounts from RNA sequencing. By considering the matrix-variate structure, full\ninformation on the conditions and occasions of the RNA sequencing dataset is\nsimultaneously considered, and the number of covariance parameters to be\nestimated is reduced. A Markov chain Monte Carlo expectation-maximization\nalgorithm is used for parameter estimation and information criteria are used\nfor model selection. The models are applied to both real and simulated data,\ngiving favourable clustering results.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 23:05:46 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Silva", "Anjali", ""], ["Rothstein", "Steven J.", ""], ["McNicholas", "Paul D.", ""], ["Subedi", "Sanjeena", ""]]}, {"id": "1807.08383", "submitter": "Yubin Park", "authors": "Yubin Park and Joyce C. Ho", "title": "PaloBoost: An Overfitting-robust TreeBoost with Out-of-Bag Sample\n  Regularization Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient TreeBoost is often found in many winning solutions in\npublic data science challenges. Unfortunately, the best performance requires\nextensive parameter tuning and can be prone to overfitting. We propose\nPaloBoost, a Stochastic Gradient TreeBoost model that uses novel regularization\ntechniques to guard against overfitting and is robust to parameter settings.\nPaloBoost uses the under-utilized out-of-bag samples to perform gradient-aware\npruning and estimate adaptive learning rates. Unlike other Stochastic Gradient\nTreeBoost models that use the out-of-bag samples to estimate test errors,\nPaloBoost treats the samples as a second batch of training samples to prune the\ntrees and adjust the learning rates. As a result, PaloBoost can dynamically\nadjust tree depths and learning rates to achieve faster learning at the start\nand slower learning as the algorithm converges. We illustrate how these\nregularization techniques can be efficiently implemented and propose a new\nformula for calculating feature importance to reflect the node coverages and\nlearning rates. Extensive experimental results on seven datasets demonstrate\nthat PaloBoost is robust to overfitting, is less sensitivity to the parameters,\nand can also effectively identify meaningful features.\n", "versions": [{"version": "v1", "created": "Sun, 22 Jul 2018 23:29:28 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Park", "Yubin", ""], ["Ho", "Joyce C.", ""]]}, {"id": "1807.08390", "submitter": "Bal\\'azs Csan\\'ad Cs\\'aji", "authors": "Bal\\'azs Csan\\'ad Cs\\'aji", "title": "Score Permutation Based Finite Sample Inference for Generalized\n  AutoRegressive Conditional Heteroskedasticity (GARCH) Models", "comments": "19th International Conference on Artificial Intelligence and\n  Statistics (AISTATS)", "journal-ref": "Proceedings of Machine Learning Research, Volume 51, 2016, pp.\n  296-304", "doi": null, "report-no": null, "categories": "stat.ME cs.LG econ.EM math.DS q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard model of (conditional) heteroscedasticity, i.e., the phenomenon\nthat the variance of a process changes over time, is the Generalized\nAutoRegressive Conditional Heteroskedasticity (GARCH) model, which is\nespecially important for economics and finance. GARCH models are typically\nestimated by the Quasi-Maximum Likelihood (QML) method, which works under mild\nstatistical assumptions. Here, we suggest a finite sample approach, called\nScoPe, to construct distribution-free confidence regions around the QML\nestimate, which have exact coverage probabilities, despite no additional\nassumptions about moments are made. ScoPe is inspired by the recently developed\nSign-Perturbed Sums (SPS) method, which however cannot be applied in the GARCH\ncase. ScoPe works by perturbing the score function using randomly permuted\nresiduals. This produces alternative samples which lead to exact confidence\nregions. Experiments on simulated and stock market data are also presented, and\nScoPe is compared with the asymptotic theory and bootstrap approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 00:02:36 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Cs\u00e1ji", "Bal\u00e1zs Csan\u00e1d", ""]]}, {"id": "1807.08393", "submitter": "Ruitao Lin", "authors": "Ruitao Lin and Ying Yuan", "title": "Time-to-Event Model-Assisted Designs to Accelerate Phase I Clinical\n  Trials", "comments": "31 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two useful strategies to speed up drug development are to increase the\npatient accrual rate and use novel adaptive designs. Unfortunately, these two\nstrategies often conflict when the evaluation of the outcome cannot keep pace\nwith the patient accrual rate and thus the interim data cannot be observed in\ntime to make adaptive decisions. A similar logistic difficulty arises when the\noutcome is of late onset. Based on a novel formulation and approximation of the\nlikelihood of the observed data, we propose a general methodology for\nmodel-assisted designs to handle toxicity data that are pending due to fast\naccrual or late-onset toxicity, and facilitate seamless decision making in\nphase I dose-finding trials. The dose escalation/de-escalation rules of the\nproposed time-to-event model-assisted designs can be tabulated before the trial\nbegins, which greatly simplifies trial conduct in practice compared to that\nunder existing methods. We show that the proposed designs have desirable finite\nand large-sample properties and yield performance that is superior to that of\nmore complicated model-based designs. We provide user-friendly software for\nimplementing the designs.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 01:13:37 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Lin", "Ruitao", ""], ["Yuan", "Ying", ""]]}, {"id": "1807.08409", "submitter": "Matias Quiroz", "authors": "Matias Quiroz, Mattias Villani, Robert Kohn, Minh-Ngoc Tran, Khue-Dung\n  Dang", "title": "Subsampling MCMC - An introduction for the survey statistician", "comments": "Accepted for publication in Sankhya A. Previous uploaded version\n  contained a bug in generating the figures and references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of computing power and efficient Markov Chain Monte\nCarlo (MCMC) simulation algorithms have revolutionized Bayesian statistics,\nmaking it a highly practical inference method in applied work. However, MCMC\nalgorithms tend to be computationally demanding, and are particularly slow for\nlarge datasets. Data subsampling has recently been suggested as a way to make\nMCMC methods scalable on massively large data, utilizing efficient sampling\nschemes and estimators from the survey sampling literature. These developments\ntend to be unknown by many survey statisticians who traditionally work with\nnon-Bayesian methods, and rarely use MCMC. Our article explains the idea of\ndata subsampling in MCMC by reviewing one strand of work, Subsampling MCMC, a\nso called pseudo-marginal MCMC approach to speeding up MCMC through data\nsubsampling. The review is written for a survey statistician without previous\nknowledge of MCMC methods since our aim is to motivate survey sampling experts\nto contribute to the growing Subsampling MCMC literature.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 02:49:41 GMT"}, {"version": "v2", "created": "Tue, 24 Jul 2018 01:59:17 GMT"}, {"version": "v3", "created": "Wed, 19 Sep 2018 09:50:24 GMT"}, {"version": "v4", "created": "Thu, 20 Sep 2018 16:39:04 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Quiroz", "Matias", ""], ["Villani", "Mattias", ""], ["Kohn", "Robert", ""], ["Tran", "Minh-Ngoc", ""], ["Dang", "Khue-Dung", ""]]}, {"id": "1807.08429", "submitter": "Bo Chang", "authors": "Bo Chang, Harry Joe", "title": "Prediction based on conditional distributions of vine copulas", "comments": null, "journal-ref": null, "doi": "10.1016/j.csda.2019.04.015", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vine copulas are a flexible tool for multivariate non-Gaussian distributions.\nFor data from an observational study where the explanatory variables and\nresponse variables are measured together, a proposed vine copula regression\nmethod uses regular vines and handles mixed continuous and discrete variables.\nThis method can efficiently compute the conditional distribution of the\nresponse variable given the explanatory variables. The performance of the\nproposed method is evaluated on simulated data sets and a real data set. The\nexperiments demonstrate that the vine copula regression method is superior to\nlinear regression in making inferences with conditional heteroscedasticity.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 05:10:30 GMT"}, {"version": "v2", "created": "Mon, 29 Apr 2019 13:23:17 GMT"}], "update_date": "2019-10-30", "authors_parsed": [["Chang", "Bo", ""], ["Joe", "Harry", ""]]}, {"id": "1807.08440", "submitter": "Shengming Luo", "authors": "Jiashun Jin, Zheng Tracy Ke and Shengming Luo", "title": "Network Global Testing by Counting Graphlets", "comments": null, "journal-ref": "Proceedings of the 35th International Conference on Machine\n  Learning (ICML 2018), Stockholm, Sweden, PMLR Vol. 80, Pages 2338-2346, 2018", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a large social network with possibly severe degree heterogeneity and\nmixed-memberships. We are interested in testing whether the network has only\none community or there are more than one communities. The problem is known to\nbe non-trivial, partially due to the presence of severe degree heterogeneity.\nWe construct a class of test statistics using the numbers of short paths and\nshort cycles, and the key to our approach is a general framework for canceling\nthe effects of degree heterogeneity. The tests compare favorably with existing\nmethods. We support our methods with careful analysis and numerical study with\nsimulated data and a real data example.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 06:17:12 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Jin", "Jiashun", ""], ["Ke", "Zheng Tracy", ""], ["Luo", "Shengming", ""]]}, {"id": "1807.08448", "submitter": "Shinsuke Koyama", "authors": "Shinsuke Koyama and Yoshi Fujiwara", "title": "Modeling event cascades using networks of additive count sequences", "comments": "15 pages, 5 figures", "journal-ref": "Journal of Statistical Mechanics 2019", "doi": "10.1088/1742-5468/aafa7c", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a statistical model for networks of event count sequences built on\na cascade structure. We assume that each event triggers successor events, whose\ncounts follow additive probability distributions; the ensemble of counts is\ngiven by their superposition. These assumptions allow the marginal distribution\nof count sequences and the conditional distribution of event cascades to take\nanalytic forms. We present our model framework using Poisson and negative\nbinomial distributions as the building blocks. Based on this formulation, we\ndescribe a statistical method for estimating the model parameters and event\ncascades from the observed count sequences.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 06:51:40 GMT"}, {"version": "v2", "created": "Fri, 2 Nov 2018 05:14:43 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 08:18:54 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Koyama", "Shinsuke", ""], ["Fujiwara", "Yoshi", ""]]}, {"id": "1807.08660", "submitter": "Corwin Zigler", "authors": "Corwin M. Zigler and Georgia Papadogeorgou", "title": "Bipartite Causal Inference with Interference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical methods to evaluate the effectiveness of interventions are\nincreasingly challenged by the inherent interconnectedness of units.\nSpecifically, a recent flurry of methods research has addressed the problem of\ninterference between observations, which arises when one observational unit's\noutcome depends not only on its treatment but also the treatment assigned to\nother units. We introduce the setting of bipartite causal inference with\ninterference, which arises when 1) treatments are defined on observational\nunits that are distinct from those at which outcomes are measured and 2) there\nis interference between units in the sense that outcomes for some units depend\non the treatments assigned to many other units. Basic definitions and\nformulations are provided for this setting, highlighting similarities and\ndifferences with more commonly considered settings of causal inference with\ninterference. Several types of causal estimands are discussed, and a simple\ninverse probability of treatment weighted estimator is developed for a subset\nof simplified estimands. The estimators are deployed to evaluate how\ninterventions to reduce air pollution from 473 power plants in the U.S.\ncausally affect cardiovascular hospitalization among Medicare beneficiaries\nresiding at 23,458 zip code locations.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 15:03:23 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Zigler", "Corwin M.", ""], ["Papadogeorgou", "Georgia", ""]]}, {"id": "1807.08673", "submitter": "Iker Perez", "authors": "Iker Perez, Giuliano Casale", "title": "Variational inequalities and mean-field approximations for partially\n  observed systems of queueing networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.PF stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Queueing networks are systems of theoretical interest that find widespread\nuse in the performance evaluation of interconnected resources. In comparison to\ncounterpart models in genetics or mathematical biology, the stochastic (jump)\nprocesses induced by queueing networks have distinctive coupling and\nsynchronization properties. This has prevented the derivation of variational\napproximations for conditional representations of transient dynamics, which\nrely on simplifying independence assumptions. Here, we present a model\naugmentation to a multivariate counting process for interactions across service\nstations, and we enable the variational evaluation of mean-field measures for\npartially-observed multi-class networks. We also show that our framework offers\nan efficient and improved alternative for inference tasks, where existing\nvariational or numerically intensive solutions do not work.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 15:22:21 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 17:08:22 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Perez", "Iker", ""], ["Casale", "Giuliano", ""]]}, {"id": "1807.08702", "submitter": "Juan Carlos Pardo-Fernandez", "authors": "Juan Carlos Pardo-Fernandez and M. Dolores Jimenez-Gamero", "title": "A model specification test for the variance function in nonparametric\n  regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of testing for the parametric form of the conditional variance is\nconsidered in a fully nonparametric regression model. A test statistic based on\na weighted $L_2$-distance between the empirical characteristic functions of\nresiduals constructed under the null hypothesis and under the alternative is\nproposed and studied theoretically. The null asymptotic distribution of the\ntest statistic is obtained and employed to approximate the critical values.\nFinite sample properties of the proposed test are numerically investigated in\nseveral Monte Carlo experiments. The developed results assume independent data.\nTheir extension to dependent observations is also discussed.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 16:25:27 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Pardo-Fernandez", "Juan Carlos", ""], ["Jimenez-Gamero", "M. Dolores", ""]]}, {"id": "1807.08834", "submitter": "Zack Almquist", "authors": "Abhirup Mallik and Zack W. Almquist", "title": "Stable Multiple Time Step Simulation/Prediction from Lagged Dynamic\n  Network Regression Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in computers and automated data collection strategies\nhave greatly increased the interest in statistical modeling of dynamic\nnetworks. Many of the statistical models employed for inference on large-scale\ndynamic networks suffer from limited forward simulation/prediction ability. A\nmajor problem with many of the forward simulation procedures is the tendency\nfor the model to become degenerate in only a few time steps, i.e., the\nsimulation/prediction procedure results in either null graphs or complete\ngraphs. Here, we describe an algorithm for simulating a sequence of networks\ngenerated from lagged dynamic network regression models DNR(V), a sub-family of\nTERGMs. We introduce a smoothed estimator for forward prediction based on\nsmoothing of the change statistics obtained for a dynamic network regression\nmodel. We focus on the implementation of the algorithm, providing a series of\nmotivating examples with comparisons to dynamic network models from the\nliterature. We find that our algorithm significantly improves multi-step\nprediction/simulation over standard DNR(V) forecasting. Furthermore, we show\nthat our method performs comparably to existing more complex dynamic network\nanalysis frameworks (SAOM and STERGMs) for small networks over short time\nperiods, and significantly outperforms these approaches over long time time\nintervals and/or large networks.\n", "versions": [{"version": "v1", "created": "Mon, 23 Jul 2018 21:20:06 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Mallik", "Abhirup", ""], ["Almquist", "Zack W.", ""]]}, {"id": "1807.08925", "submitter": "Srijan Sengupta", "authors": "Srijan Sengupta", "title": "Anomaly detection in static networks using egonets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data has rapidly emerged as an important and active area of\nstatistical methodology. In this paper we consider the problem of anomaly\ndetection in networks. Given a large background network, we seek to detect\nwhether there is a small anomalous subgraph present in the network, and if such\na subgraph is present, which nodes constitute the subgraph. We propose an\ninferential tool based on egonets to answer this question. The proposed method\nis computationally efficient and naturally amenable to parallel computing, and\neasily extends to a wide variety of network models. We demonstrate through\nsimulation studies that the egonet method works well under a wide variety of\nnetwork models. We obtain some fascinating empirical results by applying the\negonet method on several well-studied benchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 06:34:16 GMT"}, {"version": "v2", "created": "Fri, 27 Jul 2018 22:37:38 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Sengupta", "Srijan", ""]]}, {"id": "1807.08928", "submitter": "Sylwia Bujkiewicz", "authors": "Sylwia Bujkiewicz, Dan Jackson, John R Thompson, Rebecca Turner, Keith\n  R Abrams and Ian R White", "title": "Bivariate network meta-analysis for surrogate endpoint evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surrogate endpoints are very important in regulatory decision-making in\nhealthcare, in particular if they can be measured early compared to the\nlong-term final clinical outcome and act as good predictors of clinical\nbenefit. Bivariate meta-analysis methods can be used to evaluate surrogate\nendpoints and to predict the treatment effect on the final outcome from the\ntreatment effect measured on a surrogate endpoint. However, candidate surrogate\nendpoints are often imperfect, and the level of association between the\ntreatment effects on the surrogate and final outcomes may vary between\ntreatments. This imposes a limitation on the pairwise methods which do not\ndifferentiate between the treatments. We develop bivariate network\nmeta-analysis (bvNMA) methods which combine data on treatment effects on the\nsurrogate and final outcomes, from trials investigating heterogeneous treatment\ncontrasts. The bvNMA methods estimate the effects on both outcomes for all\ntreatment contrasts individually in a single analysis. At the same time, they\nallow us to model the surrogacy patterns across multiple trials (different\npopulations) within a treatment contrast and across treatment contrasts, thus\nenabling predictions of the treatment effect on the final outcome for a new\nstudy in a new population or investigating a new treatment. Modelling\nassumptions about the between-studies heterogeneity and the network\nconsistency, and their impact on predictions, are investigated using simulated\ndata and an illustrative example in advanced colorectal cancer. When the\nstrength of the surrogate relationships varies across treatment contrasts,\nbvNMA has the advantage of identifying treatments for which surrogacy holds,\nthus leading to better predictions.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 06:58:36 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Bujkiewicz", "Sylwia", ""], ["Jackson", "Dan", ""], ["Thompson", "John R", ""], ["Turner", "Rebecca", ""], ["Abrams", "Keith R", ""], ["White", "Ian R", ""]]}, {"id": "1807.09037", "submitter": "Christian R\\\"over", "authors": "Svenja E. Seide, Christian R\\\"over, Tim Friede", "title": "Likelihood-based meta-analysis with few studies: Empirical and\n  simulation studies", "comments": "22 pages", "journal-ref": "BMC Medical Research Methodology 19:16, 2019", "doi": "10.1186/s12874-018-0618-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard random-effects meta-analysis methods perform poorly when applied to\nfew studies only. Such settings however are commonly encountered in practice.\nIt is unclear, whether or to what extent small-sample-size behaviour can be\nimproved by more sophisticated modeling.\n  We consider several likelihood-based inference methods. Confidence intervals\nare based on normal or Student-t approximations. We extract an empirical data\nset of 40 meta-analyses from recent reviews published by the German Institute\nfor Quality and Efficiency in Health Care (IQWiG). Methods are then compared\nempirically as well as in a simulation study, considering odds-ratio and risk\nratio effect sizes.\n  Empirically, a majority of the identified meta-analyses include only 2\nstudies. In the simulation study, coverage probability is, in the presence of\nheterogeneity and few studies, below the nominal level for all frequentist\nmethods based on normal approximation, in particular when sizes in\nmeta-analyses are not balanced, but improve when confidence intervals are\nadjusted. Bayesian methods result in better coverage than the frequentist\nmethods with normal approximation in all scenarios. Credible intervals are\nempirically and in the simulation study wider than unadjusted confidence\nintervals, but considerably narrower than adjusted ones. Confidence intervals\nbased on the generalized linear mixed models are in general, slightly narrower\nthan those from other frequentist methods. Certain methods turned out\nimpractical due to frequent numerical problems.\n  In the presence of between-study heterogeneity, especially with unbalanced\nstudy sizes, caution is needed in applying meta-analytical methods to few\nstudies, as either coverage probabilities might be compromised, or intervals\nare inconclusively wide. Bayesian estimation with a sensibly chosen prior for\nbetween-trial heterogeneity may offer a promising compromise.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 11:17:28 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 14:34:51 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Seide", "Svenja E.", ""], ["R\u00f6ver", "Christian", ""], ["Friede", "Tim", ""]]}, {"id": "1807.09148", "submitter": "Iv\\'an D\\'iaz", "authors": "Iv\\'an D\\'iaz", "title": "Doubly robust estimators for the average treatment effect under\n  positivity violations: introducing the $e$-score", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimation of causal parameters from observational data requires complete\nconfounder adjustment, as well as positivity of the propensity score for each\ntreatment arm. There is often a trade-off between these two assumptions:\nconfounding bias may be reduced through adjustment for a large number of\npre-treatment covariates, but positivity is less likely in analyses with\nirrelevant predictors of treatment such as instrumental variables. Under\nempirical positivity violations, propensity score weights are highly variable,\nand doubly robust estimators suffer from high variance and large finite sample\nbias. To solve this problem, we introduce the $e$-score, which is defined\nthrough a dimension reduction for the propensity score. This dimension\nreduction is based on a result known as collaborative double robustness, which\nroughly states that a propensity score conditioning only on the bias of the\noutcome regression estimator is sufficient to attain double robustness. We\npropose methods to construct doubly robust estimators based on the $e$-score,\nand discuss their properties such as consistency, efficiency, and asymptotic\ndistribution. This allows the construction of asymptotically valid Wald-type\nconfidence intervals and hypothesis tests. We present an illustrative\napplication on estimating the effect of smoking on bone mineral content in\nadolescent girls well as a synthetic data simulation illustrating the bias and\nvariance reduction and asymptotic normality achieved by our proposed\nestimators.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 14:32:43 GMT"}, {"version": "v2", "created": "Wed, 27 Feb 2019 20:08:48 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["D\u00edaz", "Iv\u00e1n", ""]]}, {"id": "1807.09267", "submitter": "Debasis Kundu Professor", "authors": "Rhythm Grover and Debasis Kundu and Amit Mitra", "title": "Asymptotic of Approximate Least Squares Estimators of Parameters\n  Two-Dimensional Chirp Signal", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of parameter estimation of a 2-D chirp\nmodel under the assumption that the errors are stationary. We extend the 2-D\nperiodogram method for the sinusoidal model, to find initial values to use in\nany iterative procedure to compute the least squares estimators (LSEs) of the\nunknown parameters, to the 2-D chirp model. Next we propose an estimator, known\nas the approximate least squares estimator (ALSE), that is obtained by\nmaximising a periodogram-type function and is observed to be asymptotically\nequivalent to the LSE. Moreover the asymptotic properties of these estimators\nare obtained under slightly mild conditions than those required for the LSEs.\nFor the multiple component 2-D chirp model, we propose a sequential method of\nestimation of the ALSEs, that significantly reduces the computational\ndifficulty involved in reckoning the LSEs and the ALSEs. We perform some\nsimulation studies to see how the proposed method works and a data set has been\nanalysed for illustrative purposes.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 06:43:11 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Grover", "Rhythm", ""], ["Kundu", "Debasis", ""], ["Mitra", "Amit", ""]]}, {"id": "1807.09616", "submitter": "Louis Aslett", "authors": "Xianzhen Huang and Louis J. M. Aslett and Frank P. A. Coolen", "title": "Reliability analysis of general phased mission systems with a new\n  survival signature", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often difficult for a phased mission system (PMS) to be highly\nreliable, because this entails achieving high reliability in every phase of\noperation. Consequently, reliability analysis of such systems is of critical\nimportance. However, efficient and interpretable analysis of PMSs enabling\ngeneral component lifetime distributions, arbitrary structures, and the\npossibility that components skip phases has been an open problem.\n  In this paper, we show that the survival signature can be used for\nreliability analysis of PMSs with similar types of component in each phase,\nproviding an alternative to the existing limited approaches in the literature.\nWe then develop new methodology addressing the full range of challenges above.\nThe new method retains the attractive survival signature property of separating\nthe system structure from the component lifetime distributions, simplifying\ncomputation, insight into, and inference for system reliability.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 13:05:08 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Huang", "Xianzhen", ""], ["Aslett", "Louis J. M.", ""], ["Coolen", "Frank P. A.", ""]]}, {"id": "1807.09621", "submitter": "Sahani Pathiraja Dr.", "authors": "Sahani Pathiraja and Peter Jan van Leeuwen", "title": "Multiplicative non-Gaussian model error estimation in data assimilation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model uncertainty quantification is an essential component of effective data\nassimilation. Model errors associated with sub-grid scale processes are often\nrepresented through stochastic parameterizations of the unresolved process.\nMany existing Stochastic Parameterization schemes are only applicable when\nknowledge of the true sub-grid scale process or full observations of the coarse\nscale process are available, which is typically not the case in real\napplications. We present a methodology for estimating the statistics of\nsub-grid scale processes for the more realistic case that only partial\nobservations of the coarse scale process are available. Model error\nrealizations are estimated over a training period by minimizing their\nconditional sum of squared deviations given some informative covariates (e.g.\nstate of the system), constrained by available observations and assuming that\nthe observation errors are smaller than the model errors. From these\nrealizations a conditional probability distribution of additive model errors\ngiven these covariates is obtained, allowing for complex non-Gaussian error\nstructures. Random draws from this density are then used in actual ensemble\ndata assimilation experiments. We demonstrate the efficacy of the approach\nthrough numerical experiments with the multi-scale Lorenz 96 system using both\nsmall and large time scale separations between slow (coarse scale) and fast\n(fine scale) variables. The resulting error estimates and forecasts obtained\nwith this new method are superior to those from two existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jul 2018 15:14:22 GMT"}, {"version": "v2", "created": "Sat, 10 Apr 2021 16:10:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Pathiraja", "Sahani", ""], ["van Leeuwen", "Peter Jan", ""]]}, {"id": "1807.09656", "submitter": "Debolina Ghatak", "authors": "Debolina Ghatak and Bimal K Roy", "title": "An Improved Bound for Security in an Identity Disclosure Problem", "comments": "14pages, 2tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identity disclosure of an individual from a released data is a matter of\nconcern especially if it belongs to a category with low frequency in the\ndata-set. Nayak et al. (2016) discussed this problem vividly in a census report\nand suggested a method of obfuscation, which would ensure that the probability\nof correctly identifying a unit from released data, would not exceed t for some\n1/3 < t < 1. However, we observe that for the above method the level of\nsecurity could be extended under certain conditions. In this paper, we discuss\nsome conditions under which one can achieve a security for any 0 < t < 1.\n", "versions": [{"version": "v1", "created": "Fri, 13 Jul 2018 12:20:47 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Ghatak", "Debolina", ""], ["Roy", "Bimal K", ""]]}, {"id": "1807.09681", "submitter": "Daisuke Murakami", "authors": "Daisuke Murakami, Daniel A. Griffith", "title": "Spatially varying coefficient modeling for large datasets: Eliminating N\n  from spatial regressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While spatially varying coefficient (SVC) modeling is popular in applied\nscience, its computational burden is substantial. This is especially true if a\nmultiscale property of SVC is considered. Given this background, this study\ndevelops a Moran's eigenvector-based spatially varying coefficients (M-SVC)\nmodeling approach that estimates multiscale SVCs computationally efficiently.\nThis estimation is accelerated through a (i) rank reduction, (ii)\npre-compression, and (iii) sequential likelihood maximization. Steps (i) and\n(ii) eliminate the sample size N from the likelihood function; after these\nsteps, the likelihood maximization cost is independent of N. Step (iii) further\naccelerates the likelihood maximization so that multiscale SVCs can be\nestimated even if the number of SVCs, K, is large. The M-SVC approach is\ncompared with geographically weighted regression (GWR) through Monte Carlo\nsimulation experiments. These simulation results show that our approach is far\nfaster than GWR when N is large, despite numerically estimating 2K parameters\nwhile GWR numerically estimates only 1 parameter. Then, the proposed approach\nis applied to a land price analysis as an illustration. The developed SVC\nestimation approach is implemented in the R package \"spmoran.\"\n", "versions": [{"version": "v1", "created": "Tue, 17 Jul 2018 16:20:01 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Murakami", "Daisuke", ""], ["Griffith", "Daniel A.", ""]]}, {"id": "1807.09969", "submitter": "Sepehr Akhavan Masouleh", "authors": "Sepehr Akhavan-Masouleh, Alexander Vandenberg-Rodes, Babak Shahbaba,\n  Daniel L. Gillen", "title": "A Flexible Joint Longitudinal-Survival Modeling Framework for\n  Incorporating Multiple Longitudinal Biomarkers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are interested in survival analysis of hemodialysis patients for whom\nseveral biomarkers are recorded over time. Motivated by this challenging\nproblem, we propose a general framework for multivariate joint\nlongitudinal-survival modeling that can be used to examine the association\nbetween several longitudinally recorded covariates and a time-to-event\nendpoint. Our method allows for simultaneous modeling of longitudinal\ncovariates by taking their correlation into account. This leads to a more\nefficient method for modeling their trajectories over time, and hence, it can\nbetter capture their relationship to the survival outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 06:22:39 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Akhavan-Masouleh", "Sepehr", ""], ["Vandenberg-Rodes", "Alexander", ""], ["Shahbaba", "Babak", ""], ["Gillen", "Daniel L.", ""]]}, {"id": "1807.10100", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo, Michael Jansson, Xinwei Ma", "title": "Two-Step Estimation and Inference with Possibly Many Included Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the implications of including many covariates in a first-step\nestimate entering a two-step estimation procedure. We find that a first order\nbias emerges when the number of \\textit{included} covariates is \"large\"\nrelative to the square-root of sample size, rendering standard inference\nprocedures invalid. We show that the jackknife is able to estimate this \"many\ncovariates\" bias consistently, thereby delivering a new automatic\nbias-corrected two-step point estimator. The jackknife also consistently\nestimates the standard error of the original two-step point estimator. For\ninference, we develop a valid post-bias-correction bootstrap approximation that\naccounts for the additional variability introduced by the jackknife\nbias-correction. We find that the jackknife bias-corrected point estimator and\nthe bootstrap post-bias-correction inference perform excellent in simulations,\noffering important improvements over conventional two-step point estimators and\ninference procedures, which are not robust to including many covariates. We\napply our results to an array of distinct treatment effect, policy evaluation,\nand other applied microeconomics settings. In particular, we discuss production\nfunction and marginal treatment effect estimation in detail.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 12:53:58 GMT"}], "update_date": "2018-07-27", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Jansson", "Michael", ""], ["Ma", "Xinwei", ""]]}, {"id": "1807.10138", "submitter": "Sophie Donnet Dr", "authors": "Avner Bar-Hen, Pierre Barbillon and Sophie Donnet", "title": "Block models for multipartite networks.Applications in ecology and\n  ethnobiology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling relations between individuals is a classical question in social\nsciences, ecology, etc. In order to uncover a latent structure in the data, a\npopular approach consists in clustering individuals according to the observed\npatterns of interactions. To do so, Stochastic block models (SBM) and Latent\nBlock models (LBM) are standard tools for clustering the individuals with\nrespect to their comportment in a unique network. However, when adopting an\nintegrative point of view, individuals are not involved in a unique network but\nare part of several networks, resulting into a potentially complex multipartite\nnetwork. In this contribution, we propose a stochastic block model able to\nhandle multipartite networks, thus supplying a clustering of the individuals\nbased on their connection behavior in more than one network. Our model is an\nextension of the latent block models (LBM) and stochastic block model (SBM).\nThe parameters -- such as the marginal probabilities of assignment to blocks\nand the matrix of probabilities of connections between blocks -- are estimated\nthrough a variational Expectation-Maximization procedure. The numbers of blocks\nare chosen with the Integrated Completed Likelihood criterion, a penalized\nlikelihood criterion. The pertinence of our methodology is illustrated on two\ndatasets issued from ecology and ethnobiology.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 13:51:12 GMT"}, {"version": "v2", "created": "Thu, 7 Mar 2019 10:40:28 GMT"}, {"version": "v3", "created": "Thu, 27 Feb 2020 15:05:31 GMT"}], "update_date": "2020-02-28", "authors_parsed": [["Bar-Hen", "Avner", ""], ["Barbillon", "Pierre", ""], ["Donnet", "Sophie", ""]]}, {"id": "1807.10173", "submitter": "Min Ren", "authors": "Min Ren and Dabao Zhang", "title": "Differential Analysis of Directed Networks", "comments": "23 pages, Proceedings of the 34th Conference on Uncertainty in\n  Artificial Intelligence (UAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We developed a novel statistical method to identify structural differences\nbetween networks characterized by structural equation models. We propose to\nreparameterize the model to separate the differential structures from common\nstructures, and then design an algorithm with calibration and construction\nstages to identify these differential structures. The calibration stage serves\nto obtain consistent prediction by building the L2 regularized regression of\neach endogenous variables against pre-screened exogenous variables, correcting\nfor potential endogeneity issue. The construction stage consistently selects\nand estimates both common and differential effects by undertaking L1\nregularized regression of each endogenous variable against the predicts of\nother endogenous variables as well as its anchoring exogenous variables. Our\nmethod allows easy parallel computation at each stage. Theoretical results are\nobtained to establish nonasymptotic error bounds of predictions and estimates\nat both stages, as well as the consistency of identified common and\ndifferential effects. Our studies on synthetic data demonstrated that our\nproposed method performed much better than independently constructing the\nnetworks. A real data set is analyzed to illustrate the applicability of our\nmethod.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 14:47:52 GMT"}, {"version": "v2", "created": "Mon, 27 Aug 2018 21:15:33 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 02:20:03 GMT"}], "update_date": "2018-10-10", "authors_parsed": [["Ren", "Min", ""], ["Zhang", "Dabao", ""]]}, {"id": "1807.10259", "submitter": "Jordan Franks", "authors": "Neil K. Chada, Jordan Franks, Ajay Jasra, Kody J. H. Law and Matti\n  Vihola", "title": "Unbiased inference for discretely observed hidden Markov model\n  diffusions", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a Bayesian inference method for diffusions observed discretely and\nwith noise, which is free of discretisation bias. Unlike existing unbiased\ninference methods, our method does not rely on exact simulation techniques.\nInstead, our method uses standard time-discretised approximations of\ndiffusions, such as the Euler--Maruyama scheme. Our approach is based on\nparticle marginal Metropolis--Hastings, a particle filter, randomised\nmultilevel Monte Carlo, and importance sampling type correction of approximate\nMarkov chain Monte Carlo. The resulting estimator leads to inference without a\nbias from the time-discretisation as the number of Markov chain iterations\nincreases. We give convergence results and recommend allocations for algorithm\ninputs. Our method admits a straightforward parallelisation, and can be\ncomputationally efficient. The user-friendly approach is illustrated on three\nexamples, where the underlying diffusion is an Ornstein--Uhlenbeck process, a\ngeometric Brownian motion, and a 2d non-reversible Langevin equation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 17:40:18 GMT"}, {"version": "v2", "created": "Mon, 15 Oct 2018 13:47:58 GMT"}, {"version": "v3", "created": "Mon, 4 Mar 2019 12:12:50 GMT"}, {"version": "v4", "created": "Fri, 5 Apr 2019 10:11:52 GMT"}, {"version": "v5", "created": "Thu, 5 Dec 2019 15:11:25 GMT"}, {"version": "v6", "created": "Thu, 21 Jan 2021 17:35:11 GMT"}, {"version": "v7", "created": "Sun, 7 Mar 2021 11:47:28 GMT"}, {"version": "v8", "created": "Tue, 9 Mar 2021 12:13:33 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Chada", "Neil K.", ""], ["Franks", "Jordan", ""], ["Jasra", "Ajay", ""], ["Law", "Kody J. H.", ""], ["Vihola", "Matti", ""]]}, {"id": "1807.10320", "submitter": "Caleb Bastian", "authors": "Caleb Deen Bastian and Herschel Rabitz", "title": "High Dimensional Model Representation as a Glass Box in Supervised\n  Machine Learning", "comments": "54 pages, 23 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction and explanation are key objects in supervised machine learning,\nwhere predictive models are known as black boxes and explanatory models are\nknown as glass boxes. Explanation provides the necessary and sufficient\ninformation to interpret the model output in terms of the model input. It\nincludes assessments of model output dependence on important input variables\nand measures of input variable importance to model output. High dimensional\nmodel representation (HDMR), also known as the generalized functional ANOVA\nexpansion, provides useful insight into the input-output behavior of supervised\nmachine learning models. This article gives applications of HDMR in supervised\nmachine learning. The first application is characterizing information leakage\nin ``big-data'' settings. The second application is reduced-order\nrepresentation of elementary symmetric polynomials. The third application is\nanalysis of variance with correlated variables. The last application is\nestimation of HDMR from kernel machine and decision tree black box\nrepresentations. These results suggest HDMR to have broad utility within\nmachine learning as a glass box representation.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 18:51:42 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Bastian", "Caleb Deen", ""], ["Rabitz", "Herschel", ""]]}, {"id": "1807.10338", "submitter": "Guilherme Pumi", "authors": "Guilherme Pumi, Marcio Valk, Cleber Bisognin, F\\'abio Mariano Bayer,\n  Taiane Schaedler Prass", "title": "Beta Autoregressive Fractionally Integrated Moving Average Models", "comments": null, "journal-ref": null, "doi": "10.1016/j.jspi.2018.10.001", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce the class of beta autoregressive fractionally\nintegrated moving average models for continuous random variables taking values\nin the continuous unit interval $(0,1)$. The proposed model accommodates a set\nof regressors and a long-range dependent time series structure. We derive the\npartial likelihood estimator for the parameters of the proposed model, obtain\nthe associated score vector and Fisher information matrix. We also prove the\nconsistency and asymptotic normality of the estimator under mild conditions.\nHypotheses testing, diagnostic tools and forecasting are also proposed. A Monte\nCarlo simulation is considered to evaluate the finite sample performance of the\npartial likelihood estimators and to study some of the proposed tests. An\nempirical application is also presented and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 19:42:08 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Pumi", "Guilherme", ""], ["Valk", "Marcio", ""], ["Bisognin", "Cleber", ""], ["Bayer", "F\u00e1bio Mariano", ""], ["Prass", "Taiane Schaedler", ""]]}, {"id": "1807.10375", "submitter": "Kun Chen", "authors": "Gen Li, Xiaokang Liu and Kun Chen", "title": "Integrative Multi-View Reduced-Rank Regression: Bridging Group-Sparse\n  and Low-Rank Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view data have been routinely collected in various fields of science\nand engineering. A general problem is to study the predictive association\nbetween multivariate responses and multi-view predictor sets, all of which can\nbe of high dimensionality. It is likely that only a few views are relevant to\nprediction, and the predictors within each relevant view contribute to the\nprediction collectively rather than sparsely. We cast this new problem under\nthe familiar multivariate regression framework and propose an integrative\nreduced-rank regression (iRRR), where each view has its own low-rank\ncoefficient matrix. As such, latent features are extracted from each view in a\nsupervised fashion. For model estimation, we develop a convex composite nuclear\nnorm penalization approach, which admits an efficient algorithm via alternating\ndirection method of multipliers. Extensions to non-Gaussian and incomplete data\nare discussed. Theoretically, we derive non-asymptotic oracle bounds of iRRR\nunder a restricted eigenvalue condition. Our results recover oracle bounds of\nseveral special cases of iRRR including Lasso, group Lasso and nuclear norm\npenalized regression. Therefore, iRRR seamlessly bridges group-sparse and\nlow-rank methods and can achieve substantially faster convergence rate under\nrealistic settings of multi-view learning. Simulation studies and an\napplication in the Longitudinal Studies of Aging further showcase the efficacy\nof the proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 21:42:17 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Li", "Gen", ""], ["Liu", "Xiaokang", ""], ["Chen", "Kun", ""]]}, {"id": "1807.10451", "submitter": "Daniel J. Schad", "authors": "Daniel J. Schad, Shravan Vasishth, Sven Hohenstein, Reinhold Kliegl", "title": "How to capitalize on a priori contrasts in linear (mixed) models: A\n  tutorial", "comments": "120 pages, 6 figures in main text, 1 figure in appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Factorial experiments in research on memory, language, and in other areas are\noften analyzed using analysis of variance (ANOVA). However, for effects with\nmore than one numerator degrees of freedom, e.g., for experimental factors with\nmore than two levels, the ANOVA omnibus F-test is not informative about the\nsource of a main effect or interaction. Because researchers typically have\nspecific hypotheses about which condition means differ from each other, a\npriori contrasts (i.e., comparisons planned before the sample means are known)\nbetween specific conditions or combinations of conditions are the appropriate\nway to represent such hypotheses in the statistical model. Many researchers\nhave pointed out that contrasts should be \"tested instead of, rather than as a\nsupplement to, the ordinary `omnibus' F test\" (Hays, 1973, p. 601). In this\ntutorial, we explain the mathematics underlying different kinds of contrasts\n(i.e., treatment, sum, repeated, polynomial, custom, nested, interaction\ncontrasts), discuss their properties, and demonstrate how they are applied in\nthe R System for Statistical Computing (R Core Team, 2018). In this context, we\nexplain the generalized inverse which is needed to compute the coefficients for\ncontrasts that test hypotheses that are not covered by the default set of\ncontrasts. A detailed understanding of contrast coding is crucial for\nsuccessful and correct specification in linear models (including linear mixed\nmodels). Contrasts defined a priori yield far more useful confirmatory tests of\nexperimental hypotheses than standard omnibus F-test. Reproducible code is\navailable from https://osf.io/7ukf6/.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 06:34:15 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 15:25:48 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 12:55:11 GMT"}, {"version": "v4", "created": "Wed, 17 Jul 2019 19:41:59 GMT"}], "update_date": "2019-07-19", "authors_parsed": [["Schad", "Daniel J.", ""], ["Vasishth", "Shravan", ""], ["Hohenstein", "Sven", ""], ["Kliegl", "Reinhold", ""]]}, {"id": "1807.10467", "submitter": "Xingjie Shi Mr", "authors": "Xingjie Shi, Yuling Jiao, Yi Yang, Ching-Yu Cheng, Can Yang, Xinyi Lin\n  and Jin Liu", "title": "VIMCO: Variational Inference for Multiple Correlated Outcomes in\n  Genome-wide Association Studies", "comments": "14 pages, 4 figures, 1 R package on GitHub. Supplementary available\n  upon request", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Genome-Wide Association Studies (GWAS) where multiple correlated traits\nhave been measured on participants, a joint analysis strategy, whereby the\ntraits are analyzed jointly, can improve statistical power over a single-trait\nanalysis strategy. There are two questions of interest to be addressed when\nconducting a joint GWAS analysis with multiple traits. The first question\nexamines whether a genetic loci is significantly associated with any of the\ntraits being tested. The second question focuses on identifying the specific\ntrait(s) that is associated with the genetic loci. Since existing methods\nprimarily focus on the first question, this paper seeks to provide a\ncomplementary method that addresses the second question. We propose a novel\nmethod, Variational Inference for Multiple Correlated Outcomes (VIMCO), that\nfocuses on identifying the specific trait that is associated with the genetic\nloci, when performing a joint GWAS analysis of multiple traits, while\naccounting for correlation among the multiple traits. We performed extensive\nnumerical studies and also applied VIMCO to analyze two datasets. The numerical\nstudies and real data analysis demonstrate that VIMCO improves statistical\npower over single-trait analysis strategies when the multiple traits are\ncorrelated and has comparable performance when the traits are not correlated.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 07:33:10 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Shi", "Xingjie", ""], ["Jiao", "Yuling", ""], ["Yang", "Yi", ""], ["Cheng", "Ching-Yu", ""], ["Yang", "Can", ""], ["Lin", "Xinyi", ""], ["Liu", "Jin", ""]]}, {"id": "1807.10618", "submitter": "Joris Mulder", "authors": "Joris Mulder and Anton Olsson-Collentine", "title": "Simple Bayesian testing of scientific expectations in linear regression\n  models", "comments": "33 pages, 1 figure, 2 appendices", "journal-ref": null, "doi": "10.3758/s13428-018-01196-9", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific theories can often be formulated using equality and order\nconstraints on the relative effects in a linear regression model. For example,\nit may be expected that the effect of the first predictor is larger than the\neffect of the second predictor, and the second predictor is expected to be\nlarger than the third predictor. The goal is then to test such expectations\nagainst competing scientific expectations or theories. In this paper a simple\ndefault Bayes factor test is proposed for testing multiple hypotheses with\nequality and order constraints on the effects of interest. The proposed testing\ncriterion can be computed without requiring external prior information about\nthe expected effects before observing the data. The method is implemented in\nR-package called `{\\tt lmhyp}' which is freely downloadable and ready to use.\nThe usability of the method and software is illustrated using empirical\napplications from the social and behavioral sciences.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 13:39:28 GMT"}, {"version": "v2", "created": "Tue, 29 Jan 2019 08:46:17 GMT"}], "update_date": "2019-05-01", "authors_parsed": [["Mulder", "Joris", ""], ["Olsson-Collentine", "Anton", ""]]}, {"id": "1807.10628", "submitter": "Manuele Leonelli", "authors": "Manuele Leonelli, Martine J. Barons and Jim Q. Smith", "title": "A conditional independence framework for coherent modularized inference", "comments": "arXiv admin note: text overlap with arXiv:1507.07394", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in current domains of application are often complex and require us\nto integrate the expertise of a variety of disparate panels of experts and\nmodels coherently. In this paper we develop a formal statistical methodology to\nguide the networking together of a diverse collection of probabilistic models.\nIn particular, we derive sufficient conditions that ensure inference remains\ncoherent across the composite before and after accommodating relevant evidence.\n", "versions": [{"version": "v1", "created": "Thu, 26 Jul 2018 11:21:14 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Leonelli", "Manuele", ""], ["Barons", "Martine J.", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1807.10678", "submitter": "Dennis Dobler", "authors": "Dennis Dobler and Markus Pauly", "title": "Survival of the Fittest Group: Factorial Analyses of Treatment Effects\n  under Independent Right-Censoring", "comments": "16 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces new effect parameters for factorial survival designs\nwith possibly right-censored time-to-event data. In the special case of a\ntwo-sample design it coincides with the concordance or Wilcoxon parameter in\nsurvival analysis. More generally, the new parameters describe treatment or\ninteraction effects and we develop estimates and tests to infer their presence.\nWe rigorously study the asymptotic properties by means of empirical process\ntechniques and additionally suggest wild bootstrapping for a consistent and\ndistribution-free application of the inference procedures. The small sample\nperformance is discussed based on simulation results. The practical usefulness\nof the developed methodology is exemplified on a data example about patients\nwith colon cancer by conducting one- and two-factorial analyses.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 15:19:03 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Dobler", "Dennis", ""], ["Pauly", "Markus", ""]]}, {"id": "1807.10797", "submitter": "Holger Dette", "authors": "H. Dette, G. M. Pan, Q. Yang", "title": "Estimating a change point in a sequence of very high-dimensional\n  covariance matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating a change point in the\ncovariance matrix in a sequence of high-dimensional vectors, where the\ndimension is substantially larger than the sample size. A two-stage approach is\nproposed to efficiently estimate the location of the change point. The first\nstep consists of a reduction of the dimension to identify elements of the\ncovariance matrices corresponding to significant changes. In a second step we\nuse the components after dimension reduction to determine the position of the\nchange point. Theoretical properties are developed for both steps and numerical\nstudies are conducted to support the new methodology.\n", "versions": [{"version": "v1", "created": "Fri, 27 Jul 2018 18:56:10 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Dette", "H.", ""], ["Pan", "G. M.", ""], ["Yang", "Q.", ""]]}, {"id": "1807.10873", "submitter": "Hejian Sang", "authors": "Hejian Sang, Gyuhyeong Goh and Jae Kwang Kim", "title": "Bayesian Sparse Propensity Score Estimation for Unit Nonresponse", "comments": "38 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nonresponse weighting adjustment using propensity score is a popular method\nfor handling unit nonresponse. However, including all available auxiliary\nvariables into the propensity model can lead to inefficient and inconsistent\nestimation, especially with high-dimensional covariates. In this paper, a new\nBayesian method using the Spike-and-Slab prior is proposed for sparse\npropensity score estimation. The proposed method is not based on any model\nassumption on the outcome variable and is computationally efficient. Instead of\ndoing model selection and parameter estimation separately as in many\nfrequentist methods, the proposed method simultaneously selects the sparse\nresponse probability model and provides consistent parameter estimation. Some\nasymptotic properties of the proposed method are presented. The efficiency of\nthis sparse propensity score estimator is further improved by incorporating\nrelated auxiliary variables from the full sample. The finite-sample performance\nof the proposed method is investigated in two limited simulation studies,\nincluding a partially simulated real data example from the Korean Labor and\nIncome Panel Survey.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 02:04:59 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Sang", "Hejian", ""], ["Goh", "Gyuhyeong", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1807.10981", "submitter": "Mevin Hooten", "authors": "Mevin B. Hooten and Devin S. Johnson and Brian M. Brost", "title": "Making Recursive Bayesian Inference Accessible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian models provide recursive inference naturally because they can\nformally reconcile new data and existing scientific information. However,\npopular use of Bayesian methods often avoids priors that are based on exact\nposterior distributions resulting from former studies. Two existing Recursive\nBayesian methods are: Prior- and Proposal-Recursive Bayes. Prior-Recursive\nBayes uses Bayesian updating, fitting models to partitions of data\nsequentially, and provides a way to accommodate new data as they become\navailable using the posterior from the previous stage as the prior in the new\nstage based on the latest data. Proposal-Recursive Bayes is intended for use\nwith hierarchical Bayesian models and uses a set of transient priors in first\nstage independent analyses of the data partitions. The second stage of\nProposal-Recursive Bayes uses the posteriors from the first stage as proposals\nin an MCMC algorithm to fit the full model. We combine Prior- and\nProposal-Recursive concepts to fit any Bayesian model, and often with\ncomputational improvements. We demonstrate our method with two case studies.\nOur approach has implications for big data, streaming data, and optimal\nadaptive design situations.\n", "versions": [{"version": "v1", "created": "Sat, 28 Jul 2018 22:41:20 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 01:36:08 GMT"}, {"version": "v3", "created": "Fri, 26 Apr 2019 16:24:39 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Hooten", "Mevin B.", ""], ["Johnson", "Devin S.", ""], ["Brost", "Brian M.", ""]]}, {"id": "1807.10987", "submitter": "Manoel Santos Neto", "authors": "M\\'ario F. Desousa, Helton Saulo, Manoel Santos-Neto and V\\'ictor\n  Leiva", "title": "A new mixture-based fixed-effect model for a biometrical case-study\n  related to immunogenecity with highly censored data", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": "10.1080/00949655.2020.1790560", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new continuous-discrete mixture regression model which is useful\nfor describing highly censored data. We motivate our investigation based on a\ncase-study in biometry related to measles vaccines in Haiti. In this\ncase-study, the neutralization antibody level is explained by the type of\nvaccine used, level of the dosage and gender of the patient. This mixture model\nallows us to account for excess of censored observations and consists of the\nBirnbaum-Saunders and Bernoulli distributions. These distributions describe the\nantibody level and the point mass of the censoring observations. We estimate\nthe model parameters with the maximum likelihood method. Numerical evaluation\nof the model is performed by Monte Carlo simulations and by an illustration\nwith biometrical data, both of which show its good performance and its\npotential applications.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 01:09:26 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Desousa", "M\u00e1rio F.", ""], ["Saulo", "Helton", ""], ["Santos-Neto", "Manoel", ""], ["Leiva", "V\u00edctor", ""]]}, {"id": "1807.11027", "submitter": "Yuan Zhang", "authors": "Yuan Zhang", "title": "Consistent polynomial-time unseeded graph matching for Lipschitz\n  graphons", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.CC cs.LG math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a consistent polynomial-time method for the unseeded node matching\nproblem for networks with smooth underlying structures. Despite widely\nconjectured by the research community that the structured graph matching\nproblem to be significantly easier than its worst case counterpart, well-known\nto be NP-hard, the statistical version of the problem has stood a challenge\nthat resisted any solution both provable and polynomial-time. The closest\nexisting work requires quasi-polynomial time. Our method is based on the latest\nadvances in graphon estimation techniques and analysis on the concentration of\nempirical Wasserstein distances. Its core is a simple yet unconventional\nsampling-and-matching scheme that reduces the problem from unseeded to seeded.\nOur method allows flexible efficiencies, is convenient to analyze and\npotentially can be extended to more general settings. Our work enables a rich\nvariety of subsequent estimations and inferences.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 09:25:37 GMT"}], "update_date": "2018-08-24", "authors_parsed": [["Zhang", "Yuan", ""]]}, {"id": "1807.11038", "submitter": "Laura Forastiere", "authors": "Laura Forastiere, Fabrizia Mealli, Albert Wu, Edoardo Airoldi", "title": "Estimating Causal Effects Under Interference Using Bayesian Generalized\n  Propensity Scores", "comments": "arXiv admin note: text overlap with arXiv:1610.06511 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most real-world systems units are interconnected and can be represented as\nnetworks consisting of nodes and edges. For instance, in social systems\nindividuals can have social ties, family or financial relationships. In\nsettings where some units are exposed to a treatment and its effect spills over\nconnected units, estimating both the direct effect of the treatment and\nspillover effects presents several challenges. First, assumptions on the way\nand the extent to which spillover effects occur along the observed network are\nrequired. Second, in observational studies, where the treatment assignment is\nnot under the control of the investigator, confounding and homophily are\npotential threats to the identification and estimation of causal effects on\nnetworks. Here, we make two structural assumptions: i) neighborhood\ninterference, which assumes interference operates only through a function of\nthe immediate neighbors' treatments ii) unconfoundedness of the individual and\nneighborhood treatment, which rules out the presence of unmeasured confounding\nvariables, including those driving homophily. Under these assumptions we\ndevelop a new covariate-adjustment estimator for treatment and spillover\neffects in observational studies on networks. Estimation is based on a\ngeneralized propensity score that balances individual and neighborhood\ncovariates across units under different levels of individual treatment and of\nexposure to neighbors' treatment. Adjustment for propensity score is performed\nusing a penalized spline regression. Inference capitalizes on a three-step\nBayesian procedure which allows to take into account the uncertainty in the\npropensity score estimation and avoiding model feedback. Finally, correlation\nof interacting units is taken into account using a community detection\nalgorithm and incorporating random effects in the outcome model.\n", "versions": [{"version": "v1", "created": "Sun, 29 Jul 2018 10:58:59 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Forastiere", "Laura", ""], ["Mealli", "Fabrizia", ""], ["Wu", "Albert", ""], ["Airoldi", "Edoardo", ""]]}, {"id": "1807.11143", "submitter": "Mingyuan Zhou", "authors": "Mingzhang Yin, Mingyuan Zhou", "title": "ARM: Augment-REINFORCE-Merge Gradient for Stochastic Binary Networks", "comments": "ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To backpropagate the gradients through stochastic binary layers, we propose\nthe augment-REINFORCE-merge (ARM) estimator that is unbiased, exhibits low\nvariance, and has low computational complexity. Exploiting variable\naugmentation, REINFORCE, and reparameterization, the ARM estimator achieves\nadaptive variance reduction for Monte Carlo integration by merging two\nexpectations via common random numbers. The variance-reduction mechanism of the\nARM estimator can also be attributed to either antithetic sampling in an\naugmented space, or the use of an optimal anti-symmetric \"self-control\"\nbaseline function together with the REINFORCE estimator in that augmented\nspace. Experimental results show the ARM estimator provides state-of-the-art\nperformance in auto-encoding variational inference and maximum likelihood\nestimation, for discrete latent variable models with one or multiple stochastic\nbinary layers. Python code for reproducible research is publicly available.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 02:21:07 GMT"}, {"version": "v2", "created": "Mon, 9 Sep 2019 22:34:47 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Yin", "Mingzhang", ""], ["Zhou", "Mingyuan", ""]]}, {"id": "1807.11221", "submitter": "Pierre Tandeo", "authors": "Pierre Tandeo, Pierre Ailliot, Marc Bocquet, Alberto Carrassi,\n  Takemasa Miyoshi, Manuel Pulido, Yicun Zhen", "title": "A Review of Innovation-Based Methods to Jointly Estimate Model and\n  Observation Error Covariance Matrices in Ensemble Data Assimilation", "comments": "The manuscript is being considered for publication at Monthly Weather\n  Review", "journal-ref": null, "doi": "10.1175/MWR-D-19-0240.1", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data assimilation combines forecasts from a numerical model with\nobservations. Most of the current data assimilation algorithms consider the\nmodel and observation error terms as additive Gaussian noise, specified by\ntheir covariance matrices Q and R, respectively. These error covariances, and\nspecifically their respective amplitudes, determine the weights given to the\nbackground (i.e., the model forecasts) and to the observations in the solution\nof data assimilation algorithms (i.e., the analysis). Consequently, Q and R\nmatrices significantly impact the accuracy of the analysis. This review aims to\npresent and to discuss, with a unified framework, different methods to jointly\nestimate the Q and R matrices using ensemble-based data assimilation\ntechniques. Most of the methodologies developed to date use the innovations,\ndefined as differences between the observations and the projection of the\nforecasts onto the observation space. These methodologies are based on two main\nstatistical criteria: (i) the method of moments, in which the theoretical and\nempirical moments of the innovations are assumed to be equal, and (ii) methods\nthat use the likelihood of the observations, themselves contained in the\ninnovations. The reviewed methods assume that innovations are Gaussian random\nvariables, although extension to other distributions is possible for\nlikelihood-based methods. The methods also show some differences in terms of\nlevels of complexity and applicability to high-dimensional systems. The\nconclusion of the review discusses the key challenges to further develop\nestimation methods for Q and R. These challenges include taking into account\ntime-varying error covariances, using limited observational coverage,\nestimating additional deterministic error terms, or accounting for correlated\nnoises.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 08:17:54 GMT"}, {"version": "v2", "created": "Wed, 31 Oct 2018 09:11:45 GMT"}, {"version": "v3", "created": "Tue, 16 Jul 2019 08:39:16 GMT"}, {"version": "v4", "created": "Wed, 15 Jan 2020 21:28:43 GMT"}, {"version": "v5", "created": "Tue, 19 May 2020 08:19:18 GMT"}], "update_date": "2020-09-02", "authors_parsed": [["Tandeo", "Pierre", ""], ["Ailliot", "Pierre", ""], ["Bocquet", "Marc", ""], ["Carrassi", "Alberto", ""], ["Miyoshi", "Takemasa", ""], ["Pulido", "Manuel", ""], ["Zhen", "Yicun", ""]]}, {"id": "1807.11554", "submitter": "Adrien Hardy", "authors": "R\\'emi Bardenet and Adrien Hardy", "title": "Time-frequency transforms of white noises and Gaussian analytic\n  functions", "comments": "to appear in Applied and Computational Harmonic Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.CA stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of Gaussian analytic functions (GAFs) has recently been linked to\nthe Gabor transform of white Gaussian noise [Bardenet et al., 2017]. This\nanswered pioneering work by Flandrin [2015], who observed that the zeros of the\nGabor transform of white noise had a very regular distribution and proposed\nfiltering algorithms based on the zeros of a spectrogram. The mathematical link\nwith GAFs provides a wealth of probabilistic results to inform the design of\nsuch signal processing procedures. In this paper, we study in a systematic way\nthe link between GAFs and a class of time-frequency transforms of Gaussian\nwhite noises on Hilbert spaces of signals. Our main observation is a conceptual\ncorrespondence between pairs (transform, GAF) and generating functions for\nclassical orthogonal polynomials. This correspondence covers some classical\ntime-frequency transforms, such as the Gabor transform and the Daubechies-Paul\nanalytic wavelet transform. It also unveils new windowed discrete Fourier\ntransforms, which map white noises to fundamental GAFs. All these transforms\nmay thus be of interest to the research program `filtering with zeros'. We also\nidentify the GAF whose zeros are the extrema of the Gabor transform of the\nwhite noise and derive their first intensity. Moreover, we discuss important\nsubtleties in defining a white noise and its transform on infinite dimensional\nHilbert spaces. Finally, we provide quantitative estimates concerning the\nfinite-dimensional approximations of these white noises, which is of practical\ninterest when it comes to implementing signal processing algorithms based on\nGAFs.\n", "versions": [{"version": "v1", "created": "Mon, 30 Jul 2018 20:14:07 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2019 14:09:08 GMT"}], "update_date": "2019-07-24", "authors_parsed": [["Bardenet", "R\u00e9mi", ""], ["Hardy", "Adrien", ""]]}, {"id": "1807.11641", "submitter": "Oscar Hernan Madrid Padilla", "authors": "Oscar Hernan Madrid Padilla, James Sharpnack, Yanzhen Chen, and\n  Daniela M. Witten", "title": "Adaptive Non-Parametric Regression With the $K$-NN Fused Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fused lasso, also known as total-variation denoising, is a\nlocally-adaptive function estimator over a regular grid of design points. In\nthis paper, we extend the fused lasso to settings in which the points do not\noccur on a regular grid, leading to an approach for non-parametric regression.\nThis approach, which we call the $K$-nearest neighbors ($K$-NN) fused lasso,\ninvolves (i) computing the $K$-NN graph of the design points; and (ii)\nperforming the fused lasso over this $K$-NN graph. We show that this procedure\nhas a number of theoretical advantages over competing approaches: specifically,\nit inherits local adaptivity from its connection to the fused lasso, and it\ninherits manifold adaptivity from its connection to the $K$-NN approach. We\nshow that excellent results are obtained in a simulation study and on an\napplication to flu data. For completeness, we also study an estimator that\nmakes use of an $\\epsilon$-graph rather than a $K$-NN graph, and contrast this\nwith the $K$-NN fused lasso.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 02:50:47 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 15:31:01 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 03:31:58 GMT"}, {"version": "v4", "created": "Mon, 8 Jul 2019 05:18:40 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Padilla", "Oscar Hernan Madrid", ""], ["Sharpnack", "James", ""], ["Chen", "Yanzhen", ""], ["Witten", "Daniela M.", ""]]}, {"id": "1807.11796", "submitter": "Matthew Williams", "authors": "Matthew R. Williams and Terrance D. Savitsky", "title": "Bayesian Uncertainty Estimation Under Complex Sampling", "comments": "45 pages, 4 figures, 1 table", "journal-ref": "International Statistical Review 2020", "doi": "10.1111/insr.12376", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social and economic studies are often implemented as complex survey designs.\nFor example, multistage, unequal probability sampling designs utilized by\nfederal statistical agencies are typically constructed to maximize the\nefficiency of the target domain level estimator (e.g., indexed by geographic\narea) within cost constraints for survey administration. Such designs may\ninduce dependence between the sampled units; for example, with employment of a\nsampling step that selects geographically-indexed clusters of units. A\nsampling-weighted pseudo-posterior distribution may be used to estimate the\npopulation model on the observed sample. The dependence induced between\nco-clustered units inflates the scale of the resulting pseudo-posterior\ncovariance matrix that has been shown to induce under coverage of the\ncredibility sets. By bridging results across Bayesian model mispecification and\nsurvey sampling, we demonstrate that the scale and shape of the asymptotic\ndistributions are different between each of the pseudo-MLE, the\npseudo-posterior and the MLE under simple random sampling. Through insights\nfrom survey sampling variance estimation and recent advances in computational\nmethods, we devise a correction applied as a simple and fast post-processing\nstep to MCMC draws of the pseudo-posterior distribution. This adjustment\nprojects the pseudo-posterior covariance matrix such that the nominal coverage\nis approximately achieved. We make an application to the National Survey on\nDrug Use and Health as a motivating example and we demonstrate the efficacy of\nour scale and shape projection procedure on synthetic data on several common\narchetypes of survey designs.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 13:01:25 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2019 12:43:08 GMT"}], "update_date": "2020-06-09", "authors_parsed": [["Williams", "Matthew R.", ""], ["Savitsky", "Terrance D.", ""]]}, {"id": "1807.11907", "submitter": "Paul Blackwell", "authors": "Paul G Blackwell", "title": "Integrated Continuous-time Hidden Markov Models", "comments": "Fuller explanation of algorithm; corrected and expanded numerical\n  results; addition of backward sampling of states", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in movement ecology, in this paper I propose a new\nclass of integrated continuous-time hidden Markov models in which each\nobservation depends on the underlying state of the process over the whole\ninterval since the previous observation, not only on its current state. This\nclass gives a new representation of a range of existing models, including some\nwidely applied switching diffusion models. I show that under appropriate\nconditioning, a model in this class can be regarded as a conventional hidden\nMarkov model, enabling use of the Forward Algorithm for efficient evaluation of\nits likelihood without sampling of its state sequence. This leads to an\nalgorithm for inference which is more efficient, and scales better with the\namount of data, than existing methods. This is demonstrated and quantified in\nsome applications to animal movement data and some related simulation\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jul 2018 16:33:08 GMT"}, {"version": "v2", "created": "Wed, 27 Mar 2019 09:06:42 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 10:55:05 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Blackwell", "Paul G", ""]]}]