[{"id": "1809.00083", "submitter": "Haicang Zhang", "authors": "Haicang Zhang, Qi Zhang, Fusong Ju, Jianwei Zhu, Shiwei Sun, Yujuan\n  Gao, Ziwei Xie, Minghua Deng, Shiwei Sun, Wei-Mou Zheng, Dongbo Bu", "title": "Predicting protein inter-residue contacts using composite likelihood\n  maximization and deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.BM cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate prediction of inter-residue contacts of a protein is important to\ncalcu- lating its tertiary structure. Analysis of co-evolutionary events among\nresidues has been proved effective to inferring inter-residue contacts. The\nMarkov ran- dom field (MRF) technique, although being widely used for contact\nprediction, suffers from the following dilemma: the actual likelihood function\nof MRF is accurate but time-consuming to calculate, in contrast, approximations\nto the actual likelihood, say pseudo-likelihood, are efficient to calculate but\ninaccu- rate. Thus, how to achieve both accuracy and efficiency simultaneously\nremains a challenge. In this study, we present such an approach (called clmDCA)\nfor contact prediction. Unlike plmDCA using pseudo-likelihood, i.e., the\nproduct of conditional probability of individual residues, our approach uses\ncomposite- likelihood, i.e., the product of conditional probability of all\nresidue pairs. Com- posite likelihood has been theoretically proved as a better\napproximation to the actual likelihood function than pseudo-likelihood.\nMeanwhile, composite likelihood is still efficient to maximize, thus ensuring\nthe efficiency of clmDCA. We present comprehensive experiments on popular\nbenchmark datasets, includ- ing PSICOV dataset and CASP-11 dataset, to show\nthat: i) clmDCA alone outperforms the existing MRF-based approaches in\nprediction accuracy. ii) When equipped with deep learning technique for\nrefinement, the prediction ac- curacy of clmDCA was further significantly\nimproved, suggesting the suitability of clmDCA for subsequent refinement\nprocedure. We further present successful application of the predicted contacts\nto accurately build tertiary structures for proteins in the PSICOV dataset.\n  Accessibility: The software clmDCA and a server are publicly accessible\nthrough http://protein.ict.ac.cn/clmDCA/.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2018 23:38:41 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Zhang", "Haicang", ""], ["Zhang", "Qi", ""], ["Ju", "Fusong", ""], ["Zhu", "Jianwei", ""], ["Sun", "Shiwei", ""], ["Gao", "Yujuan", ""], ["Xie", "Ziwei", ""], ["Deng", "Minghua", ""], ["Sun", "Shiwei", ""], ["Zheng", "Wei-Mou", ""], ["Bu", "Dongbo", ""]]}, {"id": "1809.00100", "submitter": "Xiaoqi Zhang", "authors": "Yanqiao Zheng, Xiaobing Zhao, Xiaoqi Zhang", "title": "A simulation-based approach to estimate joint model of longitudinal and\n  event-time data with many missing longitudinal observations", "comments": "43 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint models of longitudinal and event-time data have been extensively\nstudied and applied in many different fields. Estimation of joint models is\nchallenging, most present procedures are computational expensive and have a\nstrict requirement on data quality. In this study, a novel simulation-based\nprocedure is proposed to estimate a general family of joint models, which\ninclude many widely-applied joint models as special cases. Our procedure can\neasily handle low-quality data where longitudinal observations are\nsystematically missed for some of the covariate dimensions. In addition, our\nestimation procedure is compatible with parallel computing framework when\ncombining with stochastic descending algorithm, it is perfectly applicable to\nmassive data and therefore suitable for many financial applications.\nConsistency and asymptotic normality of our estimator are proved, a simulation\nstudy is conducted to illustrate its effectiveness. Finally, as an application,\nthe procedure is applied to estimate pre-payment probability of a massive\nconsumer-loan dataset drawn from one biggest P2P loan platform of China.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 02:18:46 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Zheng", "Yanqiao", ""], ["Zhao", "Xiaobing", ""], ["Zhang", "Xiaoqi", ""]]}, {"id": "1809.00236", "submitter": "Max Farrell", "authors": "Sebastian Calonico and Matias D. Cattaneo and Max H. Farrell", "title": "Optimal Bandwidth Choice for Robust Bias Corrected Inference in\n  Regression Discontinuity Designs", "comments": null, "journal-ref": "Econometrics Journal, 23(2), 192--210, 2020", "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern empirical work in Regression Discontinuity (RD) designs often employs\nlocal polynomial estimation and inference with a mean square error (MSE)\noptimal bandwidth choice. This bandwidth yields an MSE-optimal RD treatment\neffect estimator, but is by construction invalid for inference. Robust bias\ncorrected (RBC) inference methods are valid when using the MSE-optimal\nbandwidth, but we show they yield suboptimal confidence intervals in terms of\ncoverage error. We establish valid coverage error expansions for RBC confidence\ninterval estimators and use these results to propose new inference-optimal\nbandwidth choices for forming these intervals. We find that the standard\nMSE-optimal bandwidth for the RD point estimator is too large when the goal is\nto construct RBC confidence intervals with the smallest coverage error. We\nfurther optimize the constant terms behind the coverage error to derive new\noptimal choices for the auxiliary bandwidth required for RBC inference. Our\nexpansions also establish that RBC inference yields higher-order refinements\n(relative to traditional undersmoothing) in the context of RD designs. Our main\nresults cover sharp and sharp kink RD designs under conditional\nheteroskedasticity, and we discuss extensions to fuzzy and other RD designs,\nclustered sampling, and pre-intervention covariates adjustments. The\ntheoretical findings are illustrated with a Monte Carlo experiment and an\nempirical application, and the main methodological results are available in\n\\texttt{R} and \\texttt{Stata} packages.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 18:48:54 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 12:02:19 GMT"}, {"version": "v3", "created": "Tue, 29 Oct 2019 13:40:33 GMT"}, {"version": "v4", "created": "Fri, 3 Jan 2020 02:08:26 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Calonico", "Sebastian", ""], ["Cattaneo", "Matias D.", ""], ["Farrell", "Max H.", ""]]}, {"id": "1809.00266", "submitter": "Yusha Liu", "authors": "Yusha Liu, Meng Li, Jeffrey S. Morris", "title": "Function-on-Scalar Quantile Regression with Application to Mass\n  Spectrometry Proteomics Data", "comments": null, "journal-ref": "The Annals of Applied Statistics, 14(2020) 521-541", "doi": "10.1214/19-AOAS1319", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mass spectrometry proteomics, characterized by spiky, spatially heterogeneous\nfunctional data, can be used to identify potential cancer biomarkers. Existing\nmass spectrometry analyses utilize mean regression to detect spectral regions\nthat are differentially expressed across groups. However, given the\ninter-patient heterogeneity that is a key hallmark of cancer, many biomarkers\nare only present at aberrant levels for a subset of, not all, cancer samples.\nDifferences in these biomarkers can easily be missed by mean regression, but\nmight be more easily detected by quantile-based approaches. Thus, we propose a\nunified Bayesian framework to perform quantile regression on functional\nresponses. Our approach utilizes an asymmetric Laplace working likelihood,\nrepresents the functional coefficients with basis representations which enable\nborrowing of strength from nearby locations, and places a global-local\nshrinkage prior on the basis coefficients to achieve adaptive regularization.\nDifferent types of basis transform and continuous shrinkage priors can be used\nin our framework. A scalable Gibbs sampler is developed to generate posterior\nsamples that can be used to perform Bayesian estimation and inference while\naccounting for multiple testing. Our framework performs quantile regression and\ncoefficient regularization in a unified manner, allowing them to inform each\nother and leading to improvement in performance over competing methods as\ndemonstrated by simulation studies. We also introduce an adjustment procedure\nto the model to improve its frequentist properties of posterior inference. We\napply our model to identify proteomic biomarkers of pancreatic cancer that are\ndifferentially expressed for a subset of cancer patients compared to the normal\ncontrols, which were missed by previous mean-regression based approaches.\nSupplementary materials for this article are available online.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 23:39:07 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 12:09:04 GMT"}], "update_date": "2020-07-03", "authors_parsed": [["Liu", "Yusha", ""], ["Li", "Meng", ""], ["Morris", "Jeffrey S.", ""]]}, {"id": "1809.00268", "submitter": "Anthony Scotina", "authors": "Anthony D. Scotina, Francesca L. Beaudoin, Roee Gutman", "title": "Matching Estimators for Causal Effects of Multiple Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching estimators for average treatment effects are widely used in the\nbinary treatment setting, in which missing potential outcomes are imputed as\nthe average of observed outcomes of all matches for each unit. With more than\ntwo treatment groups, however, estimation using matching requires additional\ntechniques. In this paper, we propose a nearest-neighbors matching estimator\nfor use with multiple, nominal treatments, and use simulations to show that\nthis method is precise and has coverage levels that are close to nominal. In\naddition, we implement the proposed inference methods to examine the effects of\ndifferent medication regimens on long-term pain for patients experiencing motor\nvehicle collision.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 23:56:23 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 19:48:23 GMT"}], "update_date": "2019-04-29", "authors_parsed": [["Scotina", "Anthony D.", ""], ["Beaudoin", "Francesca L.", ""], ["Gutman", "Roee", ""]]}, {"id": "1809.00269", "submitter": "Anthony Scotina", "authors": "Anthony D. Scotina, Roee Gutman", "title": "Matching Algorithms for Causal Inference with Multiple Treatments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized clinical trials (RCTs) are ideal for estimating causal effects,\nbecause the distributions of background covariates are similar in expectation\nacross treatment groups. When estimating causal effects using observational\ndata, matching is a commonly used method to replicate the covariate balance\nachieved in a RCT. Matching algorithms have a rich history dating back to the\nmid-1900s, but have been used mostly to estimate causal effects between two\ntreatment groups. When there are more than two treatments, estimating causal\neffects requires additional assumptions and techniques. We propose matching\nalgorithms that address the drawbacks of the current methods, and we use\nsimulations to compare current and new methods. All of the methods display\nimproved covariate balance in the matched sets relative to the pre-matched\ncohorts. In addition, we provide advice to investigators on which matching\nalgorithms are preferred for different covariate distributions.\n", "versions": [{"version": "v1", "created": "Sat, 1 Sep 2018 23:57:12 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 18:47:21 GMT"}], "update_date": "2019-02-27", "authors_parsed": [["Scotina", "Anthony D.", ""], ["Gutman", "Roee", ""]]}, {"id": "1809.00358", "submitter": "Taposh Banerjee", "authors": "Taposh Banerjee, Stephen Allsop, Kay M. Tye, Demba Ba and Vahid Tarokh", "title": "Sequential Detection of Regime Changes in Neural Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP q-bio.NC stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of detecting changes in firing patterns in neural data is\nstudied. The problem is formulated as a quickest change detection problem.\nImportant algorithms from the literature are reviewed. A new algorithmic\ntechnique is discussed to detect deviations from learned baseline behavior. The\nalgorithms studied can be applied to both spike and local field potential data.\nThe algorithms are applied to mice spike data to verify the presence of\nbehavioral learning.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 15:31:14 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Banerjee", "Taposh", ""], ["Allsop", "Stephen", ""], ["Tye", "Kay M.", ""], ["Ba", "Demba", ""], ["Tarokh", "Vahid", ""]]}, {"id": "1809.00399", "submitter": "Alexander Franks", "authors": "Alexander Franks, Alexander D'Amour and Avi Feller", "title": "Flexible sensitivity analysis for observational studies without\n  observable implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental challenge in observational causal inference is that assumptions\nabout unconfoundedness are not testable from data. Assessing sensitivity to\nsuch assumptions is therefore important in practice. Unfortunately, some\nexisting sensitivity analysis approaches inadvertently impose restrictions that\nare at odds with modern causal inference methods, which emphasize flexible\nmodels for observed data. To address this issue, we propose a framework that\nallows (1) flexible models for the observed data and (2) clean separation of\nthe identified and unidentified parts of the sensitivity model. Our framework\nextends an approach from the missing data literature, known as Tukey's\nfactorization, to the causal inference setting. Under this factorization, we\ncan represent the distributions of unobserved potential outcomes in terms of\nunidentified selection functions that posit an unidentified relationship\nbetween the treatment assignment indicator and the observed potential outcomes.\nThe sensitivity parameters in this framework are easily interpreted, and we\nprovide heuristics for calibrating these parameters against observable\nquantities. We demonstrate the flexibility of this approach in two examples,\nwhere we estimate both average treatment effects and quantile treatment effects\nusing Bayesian nonparametric models for the observed data.\n", "versions": [{"version": "v1", "created": "Sun, 2 Sep 2018 21:48:13 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 18:22:50 GMT"}, {"version": "v3", "created": "Mon, 14 Jan 2019 00:09:47 GMT"}], "update_date": "2019-01-15", "authors_parsed": [["Franks", "Alexander", ""], ["D'Amour", "Alexander", ""], ["Feller", "Avi", ""]]}, {"id": "1809.00420", "submitter": "Yi Su", "authors": "Yi Su, Raymond K. W. Wong and Thomas C. M. Lee", "title": "Network estimation via graphon with node features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the probabilities of linkages in a network has gained increasing\ninterest in recent years. One popular model for network analysis is the\nexchangeable graph model (ExGM) characterized by a two-dimensional function\nknown as a graphon. Estimating an underlying graphon becomes the key of such\nanalysis. Several nonparametric estimation methods have been proposed, and some\nare provably consistent. However, if certain useful features of the nodes\n(e.g., age and schools in social network context) are available, none of these\nmethods was designed to incorporate this source of information to help with the\nestimation. This paper develops a consistent graphon estimation method that\nintegrates the information from both the adjacency matrix itself and node\nfeatures. We show that properly leveraging the features can improve the\nestimation. A cross-validation method is proposed to automatically select the\ntuning parameter of the method.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 01:29:21 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Su", "Yi", ""], ["Wong", "Raymond K. W.", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "1809.00463", "submitter": "Ansgar Steland", "authors": "Ansgar Steland", "title": "Shrinkage for Covariance Estimation: Asymptotics, Confidence Intervals,\n  Bounds and Applications in Sensor Monitoring and Finance", "comments": null, "journal-ref": "Statistical Papers, 2018, Vol. 59, 1441-1462", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When shrinking a covariance matrix towards (a multiple) of the identity\nmatrix, the trace of the covariance matrix arises naturally as the optimal\nscaling factor for the identity target. The trace also appears in other\ncontext, for example when measuring the size of a matrix or the amount of\nuncertainty.\n  Of particular interest is the case when the dimension of the covariance\nmatrix is large. Then the problem arises that the sample covariance matrix is\nsingular if the dimension is larger than the sample size. Another issue is that\nusually the estimation has to based on correlated time series data. We study\nthe estimation of the trace functional allowing for a high-dimensional time\nseries model, where the dimension is allowed to grow with the sample size -\nwithout any constraint. Based on a recent result, we investigate a confidence\ninterval for the trace, which also allows us to propose lower and upper bounds\nfor the shrinkage covariance estimator as well as bounds for the variance of\nprojections. In addition, we provide a novel result dealing with shrinkage\ntowards a diagonal target.\n  We investigate the accuracy of the confidence interval by a simulation study,\nwhich indicates good performance, and analyze three stock market data sets to\nillustrate the proposed bounds, where the dimension (number of stocks) ranges\nbetween $32$ and $475$. Especially, we apply the results to portfolio\noptimization and determine bounds for the risk associated to the\nvariance-minimizing portfolio.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 06:45:33 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Steland", "Ansgar", ""]]}, {"id": "1809.00652", "submitter": "Ryan John Abat Cubero", "authors": "Ryan John Cubero, Matteo Marsili and Yasser Roudi", "title": "Minimum Description Length codes are critical", "comments": "23 pages, 5 figures; Corrected the author name, revised Section 2.2\n  (Large Deviations of the Universal Codes Exhibit Phase Transitions),\n  corrected Eq. (89)", "journal-ref": "Entropy 2018, 20(10)", "doi": "10.3390/e20100755", "report-no": null, "categories": "stat.ME physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the Minimum Description Length (MDL) principle, learning from the data is\nequivalent to an optimal coding problem. We show that the codes that achieve\noptimal compression in MDL are critical in a very precise sense. First, when\nthey are taken as generative models of samples, they generate samples with\nbroad empirical distributions and with a high value of the relevance, defined\nas the entropy of the empirical frequencies. These results are derived for\ndifferent statistical models (Dirichlet model, independent and pairwise\ndependent spin models, and restricted Boltzmann machines). Second, MDL codes\nsit precisely at a second order phase transition point where the symmetry\nbetween the sampled outcomes is spontaneously broken. The order parameter\ncontrolling the phase transition is the coding cost of the samples. The phase\ntransition is a manifestation of the optimality of MDL codes, and it arises\nbecause codes that achieve a higher compression do not exist. These results\nsuggest a clear interpretation of the widespread occurrence of statistical\ncriticality as a characterization of samples which are maximally informative on\nthe underlying generative process.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 16:44:37 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 09:07:15 GMT"}], "update_date": "2018-10-03", "authors_parsed": [["Cubero", "Ryan John", ""], ["Marsili", "Matteo", ""], ["Roudi", "Yasser", ""]]}, {"id": "1809.00694", "submitter": "Lidia Sacchetto", "authors": "Lidia Sacchetto and Mauro Gasparini", "title": "Proper likelihood ratio based ROC curves for general binary\n  classification problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Everybody writes that ROC curves, a very common tool in binary classification\nproblems, should be optimal, and in particular concave, non-decreasing and\nabove the 45-degree line. Everybody uses ROC curves, theoretical and especially\nempirical, which are not so. This work is an attempt to correct this\nschizophrenic behavior. Optimality stems from the Neyman-Pearson lemma, which\nprescribes using likelihood-ratio based ROC curves. Starting from there, we\ngive the most general definition of a likelihood-ratio based classification\nprocedure, which encompasses finite, continuous and even more complex data\ntypes. We point out a strict relationship with a general notion of\nconcentration of two probability measures. We give some nontrivial examples of\nsituations with non-monotone and non-continuous likelihood ratios. Finally, we\npropose the ROC curve of a likelihood ratio based Gaussian kernel flexible\nBayes classifier as a proper default alternative to the usual empirical ROC\ncurve.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 19:23:11 GMT"}, {"version": "v2", "created": "Wed, 31 Jul 2019 13:28:01 GMT"}], "update_date": "2019-08-01", "authors_parsed": [["Sacchetto", "Lidia", ""], ["Gasparini", "Mauro", ""]]}, {"id": "1809.00734", "submitter": "Ivana Malenica", "authors": "Mark J. van der Laan and Ivana Malenica", "title": "Robust Estimation of Data-Dependent Causal Effects based on Observing a\n  Single Time-Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.LG stat.AP stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the case that one observes a single time-series, where at each time\nt one observes a data record O(t) involving treatment nodes A(t), possible\ncovariates L(t) and an outcome node Y(t). The data record at time t carries\ninformation for an (potentially causal) effect of the treatment A(t) on the\noutcome Y(t), in the context defined by a fixed dimensional summary measure\nCo(t). We are concerned with defining causal effects that can be consistently\nestimated, with valid inference, for sequentially randomized experiments\nwithout further assumptions. More generally, we consider the case when the\n(possibly causal) effects can be estimated in a double robust manner, analogue\nto double robust estimation of effects in the i.i.d. causal inference\nliterature. We propose a general class of averages of conditional\n(context-specific) causal parameters that can be estimated in a double robust\nmanner, therefore fully utilizing the sequential randomization. We propose a\ntargeted maximum likelihood estimator (TMLE) of these causal parameters, and\npresent a general theorem establishing the asymptotic consistency and normality\nof the TMLE. We extend our general framework to a number of typically studied\ncausal target parameters, including a sequentially adaptive design within a\nsingle unit that learns the optimal treatment rule for the unit over time. Our\nwork opens up robust statistical inference for causal questions based on\nobserving a single time-series on a particular unit.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 22:02:11 GMT"}], "update_date": "2021-02-04", "authors_parsed": [["van der Laan", "Mark J.", ""], ["Malenica", "Ivana", ""]]}, {"id": "1809.00737", "submitter": "Rodney Vasconcelos Fonseca", "authors": "Rodney V. Fonseca and Alu\\'isio Pinheiro", "title": "Wavelet estimation of the dimensionality of curve time series", "comments": "29 pages, 7 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional data analysis is ubiquitous in most areas of sciences and\nengineering. Several paradigms are proposed to deal with the dimensionality\nproblem which is inherent to this type of data. Sparseness, penalization,\nthresholding, among other principles, have been used to tackle this issue. We\ndiscuss here a solution based on a finite-dimensional functional space. We\nemploy wavelet representation of the functionals to estimate this finite\ndimension, and successfully model a time series of curves. The proposed method\nis shown to have nice asymptotic properties. Moreover, the wavelet\nrepresentation permits the use of several bootstrap procedures, and it results\nin faster computing algorithms. Besides the theoretical and computational\nproperties, some simulation studies and an application to real data are\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 3 Sep 2018 22:23:22 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Fonseca", "Rodney V.", ""], ["Pinheiro", "Alu\u00edsio", ""]]}, {"id": "1809.01028", "submitter": "Yichong Zhang", "authors": "Shujie Ma, Liangjun Su and Yichong Zhang", "title": "Determining the Number of Communities in Degree-corrected Stochastic\n  Block Models", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to estimate the number of communities in degree-corrected\nstochastic block models based on a pseudo likelihood ratio statistic. To this\nend, we introduce a method that combines spectral clustering with binary\nsegmentation. This approach guarantees an upper bound for the pseudo likelihood\nratio statistic when the model is over-fitted. We also derive its limiting\ndistribution when the model is under-fitted. Based on these properties, we\nestablish the consistency of our estimator for the true number of communities.\nDeveloping these theoretical properties require a mild condition on the average\ndegrees -- growing at a rate no slower than log(n), where n is the number of\nnodes. Our proposed method is further illustrated by simulation studies and\nanalysis of real-world networks. The numerical results show that our approach\nhas satisfactory performance when the network is semi-dense.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 14:49:36 GMT"}, {"version": "v2", "created": "Tue, 30 Jul 2019 04:20:26 GMT"}], "update_date": "2019-07-31", "authors_parsed": [["Ma", "Shujie", ""], ["Su", "Liangjun", ""], ["Zhang", "Yichong", ""]]}, {"id": "1809.01038", "submitter": "Ivan Fernandez-Val", "authors": "Xi Chen, Victor Chernozhukov, Iv\\'an Fern\\'andez-Val, Scott Kostyshak\n  and Ye Luo", "title": "Shape-Enforcing Operators for Point and Interval Estimators", "comments": "42 pages, 5 figures, 3 tables, v5 includes changes in the main text", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in econometrics, statistics, and machine learning is to\nestimate and make inference on functions that satisfy shape restrictions. For\nexample, distribution functions are nondecreasing and range between zero and\none, height growth charts are nondecreasing in age, and production functions\nare nondecreasing and quasi-concave in input quantities. We propose a method to\nenforce these restrictions ex post on point and interval estimates of the\ntarget function by applying functional operators. If an operator satisfies\ncertain properties that we make precise, the shape-enforced point estimates are\ncloser to the target function than the original point estimates and the\nshape-enforced interval estimates have greater coverage and shorter length than\nthe original interval estimates. We show that these properties hold for six\ndifferent operators that cover commonly used shape restrictions in practice:\nrange, convexity, monotonicity, monotone convexity, quasi-convexity, and\nmonotone quasi-convexity. We illustrate the results with two empirical\napplications to the estimation of a height growth chart for infants in India\nand a production function for chemical firms in China.\n", "versions": [{"version": "v1", "created": "Tue, 4 Sep 2018 15:21:08 GMT"}, {"version": "v2", "created": "Mon, 8 Jul 2019 09:43:47 GMT"}, {"version": "v3", "created": "Mon, 28 Oct 2019 21:15:20 GMT"}, {"version": "v4", "created": "Thu, 21 May 2020 02:15:16 GMT"}, {"version": "v5", "created": "Fri, 12 Feb 2021 21:08:04 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Chen", "Xi", ""], ["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Iv\u00e1n", ""], ["Kostyshak", "Scott", ""], ["Luo", "Ye", ""]]}, {"id": "1809.01278", "submitter": "Sean McGrath", "authors": "Sean McGrath, Hojoon Sohn, Russell Steele, Andrea Benedetti", "title": "Two-sample aggregate data meta-analysis of medians", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of meta-analyzing two-group studies that report the\nmedian of the outcome. Often, these studies are excluded from meta-analysis\nbecause there are no well-established statistical methods to pool the\ndifference of medians. To include these studies in meta-analysis, several\nauthors have recently proposed methods to estimate the sample mean and standard\ndeviation from the median, sample size, and several commonly reported measures\nof spread. Researchers frequently apply these methods to estimate the\ndifference of means and its variance for each primary study and pool the\ndifference of means using inverse variance weighting. In this work, we develop\nseveral methods to directly meta-analyze the difference of medians. We conduct\na simulation study evaluating the performance of the proposed median-based\nmethods and the competing transformation-based methods. The simulation results\nshow that the median-based methods outperform the transformation-based methods\nwhen meta-analyzing studies that report the median of the outcome, especially\nwhen the outcome is skewed. Moreover, we illustrate the various methods on a\nreal-life data set.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 00:35:03 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["McGrath", "Sean", ""], ["Sohn", "Hojoon", ""], ["Steele", "Russell", ""], ["Benedetti", "Andrea", ""]]}, {"id": "1809.01291", "submitter": "Yishu Xue", "authors": "Yishu Xue, HaiYing Wang, Jun Yan, Elizabeth D. Schifano", "title": "An Online Updating Approach for Testing the Proportional Hazards\n  Assumption with Streams of Survival Data", "comments": null, "journal-ref": null, "doi": "10.1111/biom.13137", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The Cox model, which remains as the first choice in analyzing time-to-event\ndata even for large datasets, relies on the proportional hazards (PH)\nassumption. When survival data arrive sequentially in chunks, a fast and\nminimally storage intensive approach to test the PH assumption is desirable. We\npropose an online updating approach that updates the standard test statistic as\neach new block of data becomes available, and greatly lightens the\ncomputational burden. Under the null hypothesis of PH, the proposed statistic\nis shown to have the same asymptotic distribution as the standard version\ncomputed on the entire data stream with the data blocks pooled into one\ndataset. In simulation studies, the test and its variant based on most recent\ndata blocks maintain their sizes when the PH assumption holds and have\nsubstantial power to detect different violations of the PH assumption. We also\nshow in simulation that our approach can be used successfully with \"big data\"\nthat exceed a single computer's computational resources. The approach is\nillustrated with the survival analysis of patients with lymphoma cancer from\nthe Surveillance, Epidemiology, and End Results Program. The proposed test\npromptly identified deviation from the PH assumption that was not captured by\nthe test based on the entire data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 01:45:44 GMT"}, {"version": "v2", "created": "Sat, 21 Nov 2020 18:49:10 GMT"}], "update_date": "2020-11-24", "authors_parsed": [["Xue", "Yishu", ""], ["Wang", "HaiYing", ""], ["Yan", "Jun", ""], ["Schifano", "Elizabeth D.", ""]]}, {"id": "1809.01319", "submitter": "Ingrid Baade", "authors": "Ingrid Annette Baade", "title": "Cross validation residuals for generalised least squares and other\n  correlated data models", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross validation residuals are well known for the ordinary least squares\nmodel. Here leave-M-out cross validation is extended to generalised least\nsquares. The relationship between cross validation residuals and Cook's\ndistance is demonstrated, in terms of an approximation to the difference in the\ngeneralised residual sum of squares for a model fit to all the data (training\nand test) and a model fit to a reduced dataset (training data only). For\ngeneralised least squares, as for ordinary least squares, there is no need to\nrefit the model to reduced size datasets as all the values for K fold cross\nvalidation are available after fitting the model to all the data.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 04:44:55 GMT"}], "update_date": "2018-09-06", "authors_parsed": [["Baade", "Ingrid Annette", ""]]}, {"id": "1809.01322", "submitter": "Shinichiro Shirota Dr", "authors": "Alan. E. Gelfand and Shinichiro Shirota", "title": "Preferential sampling for presence/absence data and for fusion of\n  presence/absence data with presence-only data", "comments": "Ecological Monographs, in press", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presence/absence data and presence-only data are the two customary sources\nfor learning about species distributions over a region. We illuminate the\nfundamental modeling differences between the two types of data. Most simply,\nlocations are considered as fixed under presence/absence data; locations are\nrandom under presence-only data. The definition of \"probability of presence\" is\nincompatible between the two. So, we take issue with modeling strategies in the\nliterature which ignore this incompatibility, which assume that\npresence/absence modeling can be induced from presence-only specifications and\ntherefore, that fusion of presence-only and presence/absence data sources is\nroutine. We argue that presence/absence data should be modeled at point level.\nThat is, we need to specify a surface which provides the probability of\npresence at any location in the region. A realization from this surface is a\nbinary map yielding the results of Bernoulli trials across all locations.\nPresence-only data should be modeled as a point pattern driven by specification\nof an intensity function. We further argue that, with just presence/absence\ndata, preferential sampling, using a shared process perspective, can improve\nour estimated presence/absence surface and prediction of presence. We also\nargue that preferential sampling can enable a probabilistically coherent fusion\nof the two data types. We illustrate with two real datasets, one\npresence/absence, one presence-only for invasive species presence in New\nEngland in the United States. We demonstrate that potential bias in sampling\nlocations can affect inference with regard to presence/absence and show that\ninference can be improved with preferential sampling ideas. We also provide a\nprobabilistically coherent fusion of the two datasets to again improve\ninference with regard to presence/absence.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 04:55:24 GMT"}, {"version": "v2", "created": "Tue, 2 Apr 2019 21:26:50 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Gelfand", "Alan. E.", ""], ["Shirota", "Shinichiro", ""]]}, {"id": "1809.01606", "submitter": "Emma Simpson", "authors": "Emma S. Simpson, Jennifer L. Wadsworth and Jonathan A. Tawn", "title": "Determining the Dependence Structure of Multivariate Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multivariate extreme value analysis, the nature of the extremal dependence\nbetween variables should be considered when selecting appropriate statistical\nmodels. Interest often lies with determining which subsets of variables can\ntake their largest values simultaneously, while the others are of smaller\norder. Our approach to this problem exploits hidden regular variation\nproperties on a collection of non-standard cones and provides a new set of\nindices that reveal aspects of the extremal dependence structure not available\nthrough existing measures of dependence. We derive theoretical properties of\nthese indices, demonstrate their value through a series of examples, and\ndevelop methods of inference that also estimate the proportion of extremal mass\nassociated with each cone. We apply the methods to UK river flows, estimating\nthe probabilities of different subsets of sites being large simultaneously.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 16:22:18 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 14:21:03 GMT"}, {"version": "v3", "created": "Fri, 11 Oct 2019 14:51:54 GMT"}], "update_date": "2019-10-14", "authors_parsed": [["Simpson", "Emma S.", ""], ["Wadsworth", "Jennifer L.", ""], ["Tawn", "Jonathan A.", ""]]}, {"id": "1809.01792", "submitter": "Eugene Katsevich", "authors": "Eugene Katsevich, Chiara Sabatti, Marina Bogomolov", "title": "Filtering the rejection set while preserving false discovery rate\n  control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific hypotheses in a variety of applications have domain-specific\nstructures, such as the tree structure of the International Classification of\nDiseases (ICD), the directed acyclic graph structure of the Gene Ontology (GO),\nor the spatial structure in genome-wide association studies. In the context of\nmultiple testing, the resulting relationships among hypotheses can create\nredundancies among rejections that hinder interpretability. This leads to the\npractice of filtering rejection sets obtained from multiple testing procedures,\nwhich may in turn invalidate their inferential guarantees. We propose Focused\nBH, a simple, flexible, and principled methodology to adjust for the\napplication of any pre-specified filter. We prove that Focused BH controls the\nfalse discovery rate under various conditions, including when the filter\nsatisfies an intuitive monotonicity property and the p-values are positively\ndependent. We demonstrate in simulations that Focused BH performs well across a\nvariety of settings, and illustrate this method's practical utility via\nanalyses of real datasets based on ICD and GO.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 02:30:00 GMT"}, {"version": "v2", "created": "Tue, 6 Nov 2018 05:42:20 GMT"}, {"version": "v3", "created": "Sat, 11 Apr 2020 00:53:56 GMT"}], "update_date": "2020-04-14", "authors_parsed": [["Katsevich", "Eugene", ""], ["Sabatti", "Chiara", ""], ["Bogomolov", "Marina", ""]]}, {"id": "1809.01796", "submitter": "Anru Zhang", "authors": "Anru Zhang and Rungang Han", "title": "Optimal Sparse Singular Value Decomposition for High-dimensional\n  High-order Data", "comments": "73 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this article, we consider the sparse tensor singular value decomposition,\nwhich aims for dimension reduction on high-dimensional high-order data with\ncertain sparsity structure. A method named Sparse Tensor Alternating\nThresholding for Singular Value Decomposition (STAT-SVD) is proposed. The\nproposed procedure features a novel double projection \\& thresholding scheme,\nwhich provides a sharp criterion for thresholding in each iteration. Compared\nwith regular tensor SVD model, STAT-SVD permits more robust estimation under\nweaker assumptions. Both the upper and lower bounds for estimation accuracy are\ndeveloped. The proposed procedure is shown to be minimax rate-optimal in a\ngeneral class of situations. Simulation studies show that STAT-SVD performs\nwell under a variety of configurations. We also illustrate the merits of the\nproposed procedure on a longitudinal tensor dataset on European country\nmortality rates.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 02:55:47 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Zhang", "Anru", ""], ["Han", "Rungang", ""]]}, {"id": "1809.01812", "submitter": "Zhuang Ma", "authors": "Zhuang Ma, Michael Collins", "title": "Noise Contrastive Estimation and Negative Sampling for Conditional\n  Models: Consistency and Statistical Efficiency", "comments": "To appear in EMNLP2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise Contrastive Estimation (NCE) is a powerful parameter estimation method\nfor log-linear models, which avoids calculation of the partition function or\nits derivatives at each training step, a computationally demanding step in many\ncases. It is closely related to negative sampling methods, now widely used in\nNLP. This paper considers NCE-based estimation of conditional models.\nConditional models are frequently encountered in practice; however there has\nnot been a rigorous theoretical analysis of NCE in this setting, and we will\nargue there are subtle but important questions when generalizing NCE to the\nconditional case. In particular, we analyze two variants of NCE for conditional\nmodels: one based on a classification objective, the other based on a ranking\nobjective. We show that the ranking-based variant of NCE gives consistent\nparameter estimates under weaker assumptions than the classification-based\nmethod; we analyze the statistical efficiency of the ranking-based and\nclassification-based variants of NCE; finally we describe experiments on\nsynthetic data and language modeling showing the effectiveness and trade-offs\nof both methods.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 04:11:46 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Ma", "Zhuang", ""], ["Collins", "Michael", ""]]}, {"id": "1809.01832", "submitter": "Pratheepa Jeganathan", "authors": "Pratheepa Jeganathan, Benjamin J. Callahan, Diana M. Proctor, David A.\n  Relman, Susan P. Holmes", "title": "The Block Bootstrap Method for Longitudinal Microbiome Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Microbial ecology serves as a foundation for a wide range of scientific and\nbiomedical studies. Rapidly-evolving high-throughput sequencing technology\nenables the comprehensive search for microbial biomarkers using longitudinal\nexperiments. Such experiments consist of repeated biological observations from\neach subject over time and are essential in accounting for the high\nbetween-subject and within-subject variability.\n  Unfortunately, many of the statistical tests based on parametric models rely\non correctly specifying temporal dependence structure which is unavailable in\nmost microbiome data.\n  In this paper, we propose an extension of the nonparametric bootstrap method\nthat enables inference on these types longitudinal data. The proposed moving\nblock bootstrap (MBB) method accounts for within-subject dependency by using\noverlapping blocks of repeated observations within each subject to draw valid\ninferences based on approximately pivotal statistics. Our simulation studies\nshow an increase in power compared to merge-by-subject (MBS) strategies. We\nalso show that compared to tests that presume independent samples (PIS), our\nproposed method reduces false microbial biomarker discovery rates.\n  In this paper, we illustrated the MBB method using three different pregnancy\ndata and an oral microbiome data. We provide an open-source R package\nhttps://github.com/PratheepaJ/bootLong to make our method accessible and the\nstudy in this paper reproducible.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 05:42:21 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2018 18:51:50 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Jeganathan", "Pratheepa", ""], ["Callahan", "Benjamin J.", ""], ["Proctor", "Diana M.", ""], ["Relman", "David A.", ""], ["Holmes", "Susan P.", ""]]}, {"id": "1809.02023", "submitter": "Michelle Norris", "authors": "Michelle Norris", "title": "Sample Design for Medicaid and Healthcare Audits", "comments": "35 pages, 6 figures. arXiv admin note: substantial text overlap with\n  arXiv:1802.03778", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop several tools for the determination of sample size and design for\nMedicaid and healthcare audits. The goal of these audits is to examine a\npopulation of claims submitted by a healthcare provider for reimbursement by a\nthird party payer to determine the total amount of money which is erroneously\nclaimed. For large audit populations, conclusions about the total amount of\nreimbursement claimed erroneously are often based on sample data. Often, sample\nsize determination must be made in the absence of pilot study data and existing\nmethods for doing so typically rely on restrictive assumptions. This includes\nthe `all-or-nothing errors' assumption which assumes the error in a claim is\neither the entire claim amount or none of it. Under the all-or-nothing errors\nassumption, Roberts (1978) has derived estimates of the variances needed for\nsample size calculations under simple expansion and ratio estimation. Some\naudit populations, however, will contain claims which are partially in error.\nWe broaden existing methodology to handle this scenario by proposing an error\nmodel which allows for partial errors by modeling the line-item error\nmechanism. We use this model to derive estimates of the variances needed for\nsample size determination under simple expansion and ratio estimation in the\npresence of partial errors. In the absence of certain error-rate parameter\nestimates needed to implement our method, we show that conservative sample\nsizes can be determined using the claim data alone. We further show that, under\nall-or-nothing errors, ratio estimation will tend to outperform simple\nexpansion and that optimal stratification is independent of the population\nerror rate under ratio estimation. The proposed sample design methods are\nillustrated on three simulated audit populations.\n", "versions": [{"version": "v1", "created": "Wed, 5 Sep 2018 00:56:00 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Norris", "Michelle", ""]]}, {"id": "1809.02089", "submitter": "Russell Bowater", "authors": "Russell J. Bowater and Ludmila E. Guzm\\'an-Pantoja", "title": "Bayesian, classical and hybrid methods of inference when one parameter\n  value is special", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of making statistical inferences about a\nparameter when a narrow interval centred at a given value of the parameter is\nconsidered special, which is interpreted as meaning that there is a substantial\ndegree of prior belief that the true value of the parameter lies in this\ninterval. A clear justification of the practical importance of this problem is\nprovided. The main difficulty with the standard Bayesian solution to this\nproblem is discussed and, as a result, a pseudo-Bayesian solution is put\nforward based on determining lower limits for the posterior probability of the\nparameter lying in the special interval by means of a sensitivity analysis.\nSince it is not assumed that prior beliefs necessarily need to be expressed in\nterms of prior probabilities, nor that post-data probabilities must be Bayesian\nposterior probabilities, hybrid methods of inference are also proposed that are\nbased on specific ways of measuring and interpreting the classical concept of\nsignificance. The various methods that are outlined are compared and contrasted\nat both a foundational level, and from a practical viewpoint by applying them\nto real data from meta-analyses that appeared in a well-known medical article.\n", "versions": [{"version": "v1", "created": "Thu, 6 Sep 2018 16:57:28 GMT"}], "update_date": "2018-09-07", "authors_parsed": [["Bowater", "Russell J.", ""], ["Guzm\u00e1n-Pantoja", "Ludmila E.", ""]]}, {"id": "1809.02262", "submitter": "Yunpeng Zhao", "authors": "Yunpeng Zhao, Qing Pan and Chengan Du", "title": "Logistic Regression Augmented Community Detection for Network Data with\n  Application in Identifying Autism-Related Gene Pathways", "comments": null, "journal-ref": "Biometrics (2019)", "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When searching for gene pathways leading to specific disease outcomes,\nadditional information on gene characteristics is often available that may\nfacilitate to differentiate genes related to the disease from irrelevant\nbackground when connections involving both types of genes are observed and\ntheir relationships to the disease are unknown. We propose method to single out\nirrelevant background genes with the help of auxiliary information through a\nlogistic regression, and cluster relevant genes into cohesive groups using the\nadjacency matrix. Expectation-maximization algorithm is modified to maximize a\njoint pseudo-likelihood assuming latent indicators for relevance to the disease\nand latent group memberships as well as Poisson or multinomial distributed link\nnumbers within and between groups. A robust version allowing arbitrary linkage\npatterns within the background is further derived. Asymptotic consistency of\nlabel assignments under the stochastic blockmodel is proven. Superior\nperformance and robustness in finite samples are observed in simulation\nstudies. The proposed robust method identifies previously missed gene sets\nunderlying autism related neurological diseases using diverse data sources\nincluding de novo mutations, gene expressions and protein-protein interactions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 00:38:20 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Zhao", "Yunpeng", ""], ["Pan", "Qing", ""], ["Du", "Chengan", ""]]}, {"id": "1809.02303", "submitter": "Lin Fan", "authors": "Lin Fan, Peter W. Glynn, Markus Pelger", "title": "Change-Point Testing and Estimation for Risk Measures in Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate methods of change-point testing and confidence interval\nconstruction for nonparametric estimators of expected shortfall and related\nrisk measures in weakly dependent time series. A key aspect of our work is the\nability to detect general multiple structural changes in the tails of time\nseries marginal distributions. Unlike extant approaches for detecting tail\nstructural changes using quantities such as tail index, our approach does not\nrequire parametric modeling of the tail and detects more general changes in the\ntail. Additionally, our methods are based on the recently introduced\nself-normalization technique for time series, allowing for statistical analysis\nwithout the issues of consistent standard error estimation. The theoretical\nfoundation for our methods are functional central limit theorems, which we\ndevelop under weak assumptions. An empirical study of S&P 500 returns and US\n30-Year Treasury bonds illustrates the practical use of our methods in\ndetecting and quantifying market instability via the tails of financial time\nseries during times of financial crisis.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 04:14:03 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Fan", "Lin", ""], ["Glynn", "Peter W.", ""], ["Pelger", "Markus", ""]]}, {"id": "1809.02385", "submitter": "Paul McNicholas", "authors": "Michael P.B. Gallaugher and Paul D. McNicholas", "title": "Mixtures of Skewed Matrix Variate Bilinear Factor Analyzers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, data have become increasingly higher dimensional and,\ntherefore, an increased need has arisen for dimension reduction techniques for\nclustering. Although such techniques are firmly established in the literature\nfor multivariate data, there is a relative paucity in the area of matrix\nvariate, or three-way, data. Furthermore, the few methods that are available\nall assume matrix variate normality, which is not always sensible if cluster\nskewness or excess kurtosis is present. Mixtures of bilinear factor analyzers\nusing skewed matrix variate distributions are proposed. In all, four such\nmixture models are presented, based on matrix variate skew-t, generalized\nhyperbolic, variance-gamma, and normal inverse Gaussian distributions,\nrespectively.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 10:04:39 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 23:50:33 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 17:05:05 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Gallaugher", "Michael P. B.", ""], ["McNicholas", "Paul D.", ""]]}, {"id": "1809.02408", "submitter": "Laura Balzer PhD", "authors": "Hachem Saddiki and Laura B. Balzer", "title": "A Primer on Causality in Data Science", "comments": "26 pages (with references); 4 figures", "journal-ref": "Journal de la Societe Francaise de Statistique, 161, 2020, 67-90", "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many questions in Data Science are fundamentally causal in that our objective\nis to learn the effect of some exposure, randomized or not, on an outcome\ninterest. Even studies that are seemingly non-causal, such as those with the\ngoal of prediction or prevalence estimation, have causal elements, including\ndifferential censoring or measurement. As a result, we, as Data Scientists,\nneed to consider the underlying causal mechanisms that gave rise to the data,\nrather than simply the pattern or association observed in those data. In this\nwork, we review the 'Causal Roadmap' of Petersen and van der Laan (2014) to\nprovide an introduction to some key concepts in causal inference. Similar to\nother causal frameworks, the steps of the Roadmap include clearly stating the\nscientific question, defining of the causal model, translating the scientific\nquestion into a causal parameter, assessing the assumptions needed to express\nthe causal parameter as a statistical estimand, implementation of statistical\nestimators including parametric and semi-parametric methods, and interpretation\nof our findings. We believe that using such a framework in Data Science will\nhelp to ensure that our statistical analyses are guided by the scientific\nquestion driving our research, while avoiding over-interpreting our results. We\nfocus on the effect of an exposure occurring at a single time point and\nhighlight the use of targeted maximum likelihood estimation (TMLE) with Super\nLearner.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 11:26:51 GMT"}, {"version": "v2", "created": "Tue, 5 Mar 2019 04:38:35 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Saddiki", "Hachem", ""], ["Balzer", "Laura B.", ""]]}, {"id": "1809.02432", "submitter": "Jarno Vanhatalo", "authors": "Jarno Vanhatalo and Marcelo Hartmann and Lari Veneranta", "title": "Additive multivariate Gaussian processes for joint species distribution\n  modeling with heterogeneous data", "comments": "Accepted for publication in Bayesian Analysis. First available in\n  Project Euclid: 3 June 2019. Permanent link to the BA article:\n  https://projecteuclid.org/euclid.ba/1559548823 Digital Object Identifier:\n  doi:10.1214/19-BA1158", "journal-ref": null, "doi": "10.1214/19-BA1158", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Species distribution models (SDM) are a key tool in ecology, conservation and\nmanagement of natural resources. Two key components of the state-of-the-art\nSDMs are the description for species distribution response along environmental\ncovariates and the spatial random effect. Joint species distribution models\n(JSDMs) additionally include interspecific correlations which have been shown\nto improve their descriptive and predictive performance compared to single\nspecies models. Current JSDMs are restricted to hierarchical generalized linear\nmodeling framework. These parametric models have trouble in explaining changes\nin abundance due, e.g., highly non-linear physical tolerance limits which is\nparticularly important when predicting species distribution in new areas or\nunder scenarios of environmental change. On the other hand, semi-parametric\nresponse functions have been shown to improve the predictive performance of\nSDMs in these tasks in single species models. Here, we propose JSDMs where the\nresponses to environmental covariates are modeled with additive multivariate\nGaussian processes coded as linear models of coregionalization. These allow\ninference for wide range of functional forms and interspecific correlations\nbetween the responses. We propose also an efficient approach for inference with\nLaplace approximation and parameterization of the interspecific covariance\nmatrices on the euclidean space. We demonstrate the benefits of our model with\ntwo small scale examples and one real world case study. We use cross-validation\nto compare the proposed model to analogous semi-parametric single species\nmodels and parametric single and joint species models in interpolation and\nextrapolation tasks. The proposed model outperforms the alternative models in\nall cases. We also show that the proposed model can be seen as an extension of\nthe current state-of-the-art JSDMs to semi-parametric models.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 12:20:01 GMT"}, {"version": "v2", "created": "Thu, 8 Aug 2019 04:52:13 GMT"}], "update_date": "2019-08-09", "authors_parsed": [["Vanhatalo", "Jarno", ""], ["Hartmann", "Marcelo", ""], ["Veneranta", "Lari", ""]]}, {"id": "1809.02463", "submitter": "Bernardo Nipoti", "authors": "Julyan Arbel, Riccardo Corradin, Bernardo Nipoti", "title": "Dirichlet process mixtures under affine transformations of the data", "comments": "36 pages, 7 Figures", "journal-ref": "Computational Statistics (2020)", "doi": "10.1007/s00180-020-01013-y", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Location-scale Dirichlet process mixtures of Gaussians (DPM-G) have proved\nextremely useful in dealing with density estimation and clustering problems in\na wide range of domains. Motivated by an astronomical application, in this work\nwe address the robustness of DPM-G models to affine transformations of the\ndata, a natural requirement for any sensible statistical method for density\nestimation and clustering. First, we devise a coherent prior specification of\nthe model which makes posterior inference invariant with respect to affine\ntransformations of the data. Second, we formalise the notion of asymptotic\nrobustness under data transformation and show that mild assumptions on the true\ndata generating process are sufficient to ensure that DPM-G models feature such\na property. Our investigation is supported by an extensive simulation study and\nillustrated by the analysis of an astronomical dataset consisting of physical\nmeasurements of stars in the field of the globular cluster NGC 2419.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 13:30:06 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 16:14:50 GMT"}, {"version": "v3", "created": "Thu, 2 Jul 2020 13:09:31 GMT"}], "update_date": "2020-07-14", "authors_parsed": [["Arbel", "Julyan", ""], ["Corradin", "Riccardo", ""], ["Nipoti", "Bernardo", ""]]}, {"id": "1809.02527", "submitter": "Christophe Andrieu", "authors": "Sinan Y{\\i}ld{\\i}r{\\i}m, Christophe Andrieu and Arnaud Doucet", "title": "Scalable Monte Carlo inference for state-space models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an original simulation-based method to estimate likelihood ratios\nefficiently for general state-space models. Our method relies on a novel use of\nthe conditional Sequential Monte Carlo (cSMC) algorithm introduced in\n\\citet{Andrieu_et_al_2010} and presents several practical advantages over\nstandard approaches. The ratio is estimated using a unique source of randomness\ninstead of estimating separately the two likelihood terms involved. Beyond the\nbenefits in terms of variance reduction one may expect in general from this\ntype of approach, an important point here is that the variance of this\nestimator decreases as the distance between the likelihood parameters\ndecreases. We show how this can be exploited in the context of Monte Carlo\nMarkov chain (MCMC) algorithms, leading to the development of a new class of\nexact-approximate MCMC methods to perform Bayesian static parameter inference\nin state-space models. We show through simulations that, in contrast to the\nParticle Marginal Metropolis-Hastings (PMMH) algorithm of Andrieu_et_al_2010,\nthe computational effort required by this novel MCMC scheme scales very\nfavourably for large data sets.\n", "versions": [{"version": "v1", "created": "Fri, 7 Sep 2018 15:13:14 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Y\u0131ld\u0131r\u0131m", "Sinan", ""], ["Andrieu", "Christophe", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1809.03006", "submitter": "Alexei Botchkarev", "authors": "Alexei Botchkarev", "title": "Performance Metrics (Error Measures) in Machine Learning Regression,\n  Forecasting and Prognostics: Properties and Typology", "comments": null, "journal-ref": "Interdisciplinary Journal of Information, Knowledge, and\n  Management, 2019, 14, 45-79", "doi": "10.28945/4184", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performance metrics (error measures) are vital components of the evaluation\nframeworks in various fields. The intention of this study was to overview of a\nvariety of performance metrics and approaches to their classification. The main\ngoal of the study was to develop a typology that will help to improve our\nknowledge and understanding of metrics and facilitate their selection in\nmachine learning regression, forecasting and prognostics. Based on the analysis\nof the structure of numerous performance metrics, we propose a framework of\nmetrics which includes four (4) categories: primary metrics, extended metrics,\ncomposite metrics, and hybrid sets of metrics. The paper identified three (3)\nkey components (dimensions) that determine the structure and properties of\nprimary metrics: method of determining point distance, method of normalization,\nmethod of aggregation of point distances over a data set.\n", "versions": [{"version": "v1", "created": "Sun, 9 Sep 2018 16:58:33 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Botchkarev", "Alexei", ""]]}, {"id": "1809.03084", "submitter": "Kohei Yata", "authors": "Yusuke Narita, Shota Yasui, Kohei Yata", "title": "Efficient Counterfactual Learning from Bandit Feedback", "comments": "accepted at AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is the most statistically efficient way to do off-policy evaluation and\noptimization with batch data from bandit feedback? For log data generated by\ncontextual bandit algorithms, we consider offline estimators for the expected\nreward from a counterfactual policy. Our estimators are shown to have lowest\nvariance in a wide class of estimators, achieving variance reduction relative\nto standard estimators. We then apply our estimators to improve advertisement\ndesign by a major advertisement company. Consistent with the theoretical\nresult, our estimators allow us to improve on the existing bandit algorithm\nwith more statistical confidence compared to a state-of-the-art benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 02:08:14 GMT"}, {"version": "v2", "created": "Sun, 2 Dec 2018 21:07:33 GMT"}, {"version": "v3", "created": "Wed, 5 Dec 2018 23:41:04 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Narita", "Yusuke", ""], ["Yasui", "Shota", ""], ["Yata", "Kohei", ""]]}, {"id": "1809.03105", "submitter": "Kyoungjae Lee", "authors": "Kyoungjae Lee, Lizhen Lin and David Dunson", "title": "Maximum Pairwise Bayes Factors for Covariance Structure Testing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hypothesis testing of structure in covariance matrices is of significant\nimportance, but faces great challenges in high-dimensional settings. Although\nconsistent frequentist one-sample covariance tests have been proposed, there is\na lack of simple, computationally scalable, and theoretically sound Bayesian\ntesting methods for large covariance matrices. Motivated by this gap and by the\nneed for tests that are powerful against sparse alternatives, we propose a\nnovel testing framework based on the maximum pairwise Bayes factor. Our initial\nfocus is on one-sample covariance testing; the proposed test can {\\it\noptimally} distinguish null and alternative hypotheses in a frequentist\nasymptotic sense. We then propose diagonal tests and a scalable covariance\ngraph selection procedure that are shown to be consistent. A simulation study\nevaluates the proposed approach relative to competitors. We illustrate\nadvantages of our graph selection method on a gene expression data set.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 02:34:21 GMT"}, {"version": "v2", "created": "Tue, 11 Sep 2018 00:47:46 GMT"}, {"version": "v3", "created": "Tue, 21 Jul 2020 08:07:09 GMT"}], "update_date": "2020-07-22", "authors_parsed": [["Lee", "Kyoungjae", ""], ["Lin", "Lizhen", ""], ["Dunson", "David", ""]]}, {"id": "1809.03127", "submitter": "Philipp Wittenberg", "authors": "Tahir Mahmood, Philipp Wittenberg, Inez Maria Zwetsloot, Hailiang\n  Wang, Kwok Leung Tsui", "title": "Monitoring data quality for telehealth systems in the presence of\n  missing data", "comments": "13 pages, 5 figures", "journal-ref": "Int. J. Med. Inform. 126 (2019) 156-163", "doi": "10.1016/j.ijmedinf.2019.03.011", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: All-in-one station-based health monitoring devices are\nimplemented in elder homes in Hong Kong to support the monitoring of vital\nsigns of the elderly. During a pilot study, it was discovered that the systolic\nblood pressure was incorrectly measured during multiple weeks. A real-time\nsolution was needed to identify future data quality issues as soon as possible.\n  Methods: Control charts are an effective tool for real-time monitoring and\nsignaling issues (changes) in data. In this study, as in other healthcare\napplications, many observations are missing. Few methods are available for\nmonitoring data with missing observations. A data quality monitoring method is\ndeveloped to signal issues with the accuracy of the collected data quickly.\nThis method has the ability to deal with missing observations. A Hotelling's\nT-squared control chart is selected as the basis for our proposed method.\n  Findings: The proposed method is retrospectively validated on a case study\nwith a known measurement error in the systolic blood pressure measurements. The\nmethod is able to adequately detect this data quality problem. The proposed\nmethod was integrated into a personalized telehealth monitoring system and\nprospectively implemented in a second case study. It was found that the\nproposed scheme supports the control of data quality.\n  Conclusions: Data quality is an important issue and control charts are useful\nfor real-time monitoring of data quality. However, these charts must be\nadjusted to account for missing data that often occur in healthcare context.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 04:28:42 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2019 08:14:43 GMT"}], "update_date": "2019-06-12", "authors_parsed": [["Mahmood", "Tahir", ""], ["Wittenberg", "Philipp", ""], ["Zwetsloot", "Inez Maria", ""], ["Wang", "Hailiang", ""], ["Tsui", "Kwok Leung", ""]]}, {"id": "1809.03439", "submitter": "Frank Marrs", "authors": "Frank W. Marrs, Benjamin W. Campbell, Bailey K. Fosdick, Skyler J.\n  Cranmer, and Tobias B\\\"ohmelt", "title": "Inferring Influence Networks from Longitudinal Bipartite Relational Data", "comments": null, "journal-ref": "Journal of Computational and Graphical Statistics, 1-13 (2019)", "doi": "10.1080/10618600.2019.1694523", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Longitudinal bipartite relational data characterize the evolution of\nrelations between pairs of actors, where actors are of two distinct types and\nrelations exist only between disparate types. A common goal is to understand\nthe temporal dependencies, specifically which actor relations incite later\nactor relations. There are two existing approaches to this problem. The first\napproach projects the bipartite data in each time period to a unipartite\nnetwork and uses existing unipartite network models. Unfortunately, information\nis lost in calculating the projection and generative models for networks\nobtained through this process are scarce. The second approach represents\ndependencies using two unipartite \\emph{influence networks}, corresponding to\nthe two actor types. Existing models taking this approach are bilinear in the\ninfluence networks, creating challenges in computation and interpretation. We\npropose a novel generative model that permits estimation of weighted, directed\ninfluence networks and does not suffer from these shortcomings. The proposed\nmodel is linear in the influence networks, permitting inference using\noff-the-shelf software tools. We prove our estimator is consistent under cases\nof model misspecification and nearly asymptotically equivalent to the bilinear\nestimator. We demonstrate the performance of the proposed model in simulation\nstudies and an analysis of weekly international state interactions.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 16:27:08 GMT"}, {"version": "v2", "created": "Tue, 12 Nov 2019 16:31:00 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Marrs", "Frank W.", ""], ["Campbell", "Benjamin W.", ""], ["Fosdick", "Bailey K.", ""], ["Cranmer", "Skyler J.", ""], ["B\u00f6hmelt", "Tobias", ""]]}, {"id": "1809.03498", "submitter": "Yaqing Chen", "authors": "Yaqing Chen and Hans-Georg M\\\"uller", "title": "Wasserstein Gradients for the Temporal Evolution of Probability\n  Distributions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many studies have been conducted on flows of probability measures, often in\nterms of gradient flows. We utilize a generalized notion of derivatives with\nrespect to time to model the instantaneous evolution of empirically observed\none-dimensional distributions that vary over time and develop consistent\nestimates for these derivatives. Employing local Fr\\'echet regression and\nworking in local tangent spaces with regard to the Wasserstein metric, we\nderive the rate of convergence of the proposed estimators. The resulting time\ndynamics are illustrated with time-varying distribution data that include\nyearly income distributions and the evolution of mortality over calendar years.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 17:24:24 GMT"}, {"version": "v2", "created": "Fri, 6 Dec 2019 05:10:16 GMT"}, {"version": "v3", "created": "Tue, 12 May 2020 15:50:50 GMT"}, {"version": "v4", "created": "Fri, 15 May 2020 01:57:47 GMT"}, {"version": "v5", "created": "Sat, 23 May 2020 22:09:26 GMT"}, {"version": "v6", "created": "Wed, 10 Jun 2020 15:53:07 GMT"}, {"version": "v7", "created": "Tue, 6 Jul 2021 01:51:02 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Chen", "Yaqing", ""], ["M\u00fcller", "Hans-Georg", ""]]}, {"id": "1809.03561", "submitter": "Florian Ziel", "authors": "Florian Ziel", "title": "Quantile Regression for Qualifying Match of GEFCom2017 Probabilistic\n  Load Forecasting", "comments": "accepted for International Journal of Forecasting", "journal-ref": "International Journal of Forecasting, 35.4 (2019) 1400-1408", "doi": "10.1016/j.ijforecast.2018.07.004", "report-no": null, "categories": "stat.AP stat.CO stat.ME stat.ML stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple quantile regression-based forecasting method that was\napplied in a probabilistic load forecasting framework of the Global Energy\nForecasting Competition 2017 (GEFCom2017). The hourly load data is log\ntransformed and split into a long-term trend component and a remainder term.\nThe key forecasting element is the quantile regression approach for the\nremainder term that takes into account weekly and annual seasonalities such as\ntheir interactions. Temperature information is only used to stabilize the\nforecast of the long-term trend component. Public holidays information is\nignored. Still, the forecasting method placed second in the open data track and\nfourth in the definite data track with our forecasting method, which is\nremarkable given simplicity of the model. The method also outperforms the\nVanilla benchmark consistently.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 19:31:15 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Ziel", "Florian", ""]]}, {"id": "1809.03584", "submitter": "Matias Cattaneo", "authors": "Matias D. Cattaneo and Richard K. Crump and Max H. Farrell and Ernst\n  Schaumburg", "title": "Characteristic-Sorted Portfolios: Estimation and Inference", "comments": null, "journal-ref": "Review of Economics and Statistics, 102(3), 531--551, 2020", "doi": null, "report-no": null, "categories": "econ.EM econ.GN q-fin.EC stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio sorting is ubiquitous in the empirical finance literature, where it\nhas been widely used to identify pricing anomalies. Despite its popularity,\nlittle attention has been paid to the statistical properties of the procedure.\nWe develop a general framework for portfolio sorting by casting it as a\nnonparametric estimator. We present valid asymptotic inference methods and a\nvalid mean square error expansion of the estimator leading to an optimal choice\nfor the number of portfolios. In practical settings, the optimal choice may be\nmuch larger than the standard choices of 5 or 10. To illustrate the relevance\nof our results, we revisit the size and momentum anomalies.\n", "versions": [{"version": "v1", "created": "Mon, 10 Sep 2018 20:31:28 GMT"}, {"version": "v2", "created": "Mon, 28 Jan 2019 17:13:30 GMT"}, {"version": "v3", "created": "Sat, 5 Oct 2019 15:52:49 GMT"}], "update_date": "2020-07-21", "authors_parsed": [["Cattaneo", "Matias D.", ""], ["Crump", "Richard K.", ""], ["Farrell", "Max H.", ""], ["Schaumburg", "Ernst", ""]]}, {"id": "1809.03643", "submitter": "Xialu Liu", "authors": "Xialu Liu and Rong Chen", "title": "Threshold factor models for high-dimensional time series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a threshold factor model for high-dimensional time series in\nwhich the dynamics of the time series is assumed to switch between different\nregimes according to the value of a threshold variable. This is an extension of\nthreshold modeling to a high-dimensional time series setting under a factor\nstructure. Specifically, within each threshold regime, the time series is\nassumed to follow a factor model. The regime switching mechanism creates\nstructural change in the factor loading matrices. It provides flexibility in\ndealing with situations that the underlying states may be changing over time,\nas often observed in economic time series and other applications. We develop\nthe procedures for the estimation of the loading spaces, the number of factors\nand the threshold value, as well as the identification of the threshold\nvariable, which governs the regime change mechanism. The theoretical properties\nare investigated. Simulated and real data examples are presented to illustrate\nthe performance of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 00:49:30 GMT"}, {"version": "v2", "created": "Tue, 4 Jun 2019 22:11:09 GMT"}], "update_date": "2019-06-06", "authors_parsed": [["Liu", "Xialu", ""], ["Chen", "Rong", ""]]}, {"id": "1809.03645", "submitter": "Hejian Sang", "authors": "Hejian Sang, Kosuke Morikawa", "title": "A Profile Likelihood Approach to Semiparametric Estimation with\n  Nonignorable Nonresponse", "comments": "40 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Statistical inference with nonresponse is quite challenging, especially when\nthe response mechanism is nonignorable. The existing methods often require\ncorrect model specifications for both outcome and response models. However, due\nto nonresponse, both models cannot be verified from data directly and model\nmisspecification can lead to a seriously biased inference. To overcome this\nlimitation, we develop a robust and efficient semiparametric method based on\nthe profile likelihood. The proposed method uses the robust semiparametric\nresponse model, in which fully unspecified function of study variable is\nassumed. An efficient computation algorithm using fractional imputation is\ndeveloped. A quasi-likelihood approach for testing ignorability is also\ndeveloped. The consistency and asymptotic normality of the proposed method are\nestablished. The finite-sample performance is examined in the extensive\nsimulation studies and an application to the Korean Labor and Income Panel\nStudy dataset is also presented.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 00:59:53 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Sang", "Hejian", ""], ["Morikawa", "Kosuke", ""]]}, {"id": "1809.03659", "submitter": "Boris Beranger", "authors": "Boris Beranger, Huan Lin, and Scott A. Sisson", "title": "New models for symbolic data analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic data analysis (SDA) is an emerging area of statistics concerned with\nunderstanding and modelling data that takes distributional form (i.e. symbols),\nsuch as random lists, intervals and histograms. It was developed under the\npremise that the statistical unit of interest is the symbol, and that inference\nis required at this level. Here we consider a different perspective, which\nopens a new research direction in the field of SDA. We assume that, as with a\nstandard statistical analysis, inference is required at the level of\nindividual-level data. However, the individual-level data are aggregated into\nsymbols - group-based distributional-valued summaries - prior to the analysis.\nIn this way, large and complex datasets can be reduced to a smaller number of\ndistributional summaries, that may be analysed more efficiently than the\noriginal dataset. As such, we develop SDA techniques as a new approach for the\nanalysis of big data. In particular we introduce a new general method for\nconstructing likelihood functions for symbolic data based on a desired\nprobability model for the underlying measurement-level data, while only\nobserving the distributional summaries. This approach opens the door for new\nclasses of symbol design and construction, in addition to developing SDA as a\nviable tool to enable and improve upon classical data analyses, particularly\nfor very large and complex datasets. We illustrate this new direction for SDA\nresearch through several real and simulated data analyses.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 02:36:10 GMT"}, {"version": "v2", "created": "Wed, 8 Apr 2020 03:13:04 GMT"}], "update_date": "2020-04-09", "authors_parsed": [["Beranger", "Boris", ""], ["Lin", "Huan", ""], ["Sisson", "Scott A.", ""]]}, {"id": "1809.03735", "submitter": "Sebastian Meyer", "authors": "Leonhard Held and Sebastian Meyer", "title": "Forecasting Based on Surveillance Data", "comments": "This is an author-created preprint of a book chapter to appear in the\n  Handbook of Infectious Disease Data Analysis edited by Leonhard Held, Niel\n  Hens, Philip D O'Neill and Jacco Wallinga, Chapman and Hall/CRC, 2019. 19\n  pages, including 9 figures and 4 tables; supplementary R package\n  'HIDDA.forecasting' available https://HIDDA.github.io/forecasting/", "journal-ref": "Handbook of Infectious Disease Data Analysis; Chapman & Hall/CRC,\n  2019; Chapter 25", "doi": "10.1201/9781315222912-25", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting the future course of epidemics has always been one of the main\ngoals of epidemic modelling. This chapter reviews statistical methods to\nquantify the accuracy of epidemic forecasts. We distinguish point and\nprobabilistic forecasts and describe different methods to evaluate and compare\nthe predictive performance across models. Two case studies demonstrate how to\napply the different techniques to uni- and multivariate forecasts. We focus on\nforecasting count time series from routine public health surveillance: weekly\ncounts of influenza-like illness in Switzerland, and age-stratified counts of\nnorovirus gastroenteritis in Berlin, Germany. Data and code for all analyses\nare available in a supplementary R package.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 08:27:42 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Held", "Leonhard", ""], ["Meyer", "Sebastian", ""]]}, {"id": "1809.03759", "submitter": "Fabio Rapallo", "authors": "Roberto Fontana and Fabio Rapallo", "title": "On the aberrations of mixed level Orthogonal Arrays with removed runs", "comments": "13 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an Orthogonal Array we analyze the aberrations of the sub-fractions\nwhich are obtained by the deletion of some of its points. We provide formulae\nto compute the Generalized Word-Length Pattern of any sub-fraction. In the case\nof the deletion of one single point, we provide a simple methodology to find\nwhich the best sub-fractions are according to the Generalized Minimum\nAberration criterion. We also study the effect of the deletion of 1, 2 or 3\npoints on some examples. The methodology does not put any restriction on the\nnumber of levels of each factor. It follows that any mixed level Orthogonal\nArray can be considered.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 09:15:37 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Fontana", "Roberto", ""], ["Rapallo", "Fabio", ""]]}, {"id": "1809.03904", "submitter": "Max Farrell", "authors": "Sebastian Calonico and Matias D. Cattaneo and Max H. Farrell and Rocio\n  Titiunik", "title": "Regression Discontinuity Designs Using Covariates", "comments": null, "journal-ref": "Review of Economics and Statistics, 101(3), 442--451, 2019", "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study regression discontinuity designs when covariates are included in the\nestimation. We examine local polynomial estimators that include discrete or\ncontinuous covariates in an additive separable way, but without imposing any\nparametric restrictions on the underlying population regression functions. We\nrecommend a covariate-adjustment approach that retains consistency under\nintuitive conditions, and characterize the potential for estimation and\ninference improvements. We also present new covariate-adjusted mean squared\nerror expansions and robust bias-corrected inference procedures, with\nheteroskedasticity-consistent and cluster-robust standard errors. An empirical\nillustration and an extensive simulation study is presented. All methods are\nimplemented in \\texttt{R} and \\texttt{Stata} software packages.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 13:58:17 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Calonico", "Sebastian", ""], ["Cattaneo", "Matias D.", ""], ["Farrell", "Max H.", ""], ["Titiunik", "Rocio", ""]]}, {"id": "1809.03905", "submitter": "Benjamin Taylor", "authors": "Erick Chacon, Luke Parry, Emanuele Giorgi, Patricia Torres, Jesem\n  Orellana, Benjamin M. Taylor", "title": "Spatial Item Factor Analysis With Application to Mapping Food Insecurity", "comments": "44 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item factor analysis is widely used for studying the relationship between a\nlatent construct and a set of observed variables. One of the main assumptions\nof this method is that the latent construct or factor is independent between\nsubjects, which might not be adequate in certain contexts. In the study of food\ninsecurity, for example, this is likely not true due to a close relationship\nwith socio-economic characteristics, that are spatially structured. In order to\ncapture these effects, we propose an extension of item factor analysis to the\nspatial domain that is able to predict the latent factors at unobserved spatial\nlocations. We develop a Bayesian sampling scheme for providing inference and\nillustrate the explanatory strength of our model by application to a study of\nthe latent construct `food insecurity' in a remote urban centre in the\nBrazilian Amazon. We use our method to map the dimensions of food insecurity in\nthis area and identify the most severely affected areas. Our methods are\nimplemented in an R package, spifa, available from Github.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 13:59:13 GMT"}], "update_date": "2018-09-12", "authors_parsed": [["Chacon", "Erick", ""], ["Parry", "Luke", ""], ["Giorgi", "Emanuele", ""], ["Torres", "Patricia", ""], ["Orellana", "Jesem", ""], ["Taylor", "Benjamin M.", ""]]}, {"id": "1809.03935", "submitter": "Hisashi Noma", "authors": "Hisashi Noma, Kengo Nagashima and Toshi A. Furukawa", "title": "Permutation inference methods for multivariate meta-analysis", "comments": "20 pages, 2 figures, 2 table", "journal-ref": "Biometrics 2020;76(1):337-347", "doi": "10.1111/biom.13134", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate meta-analysis is gaining prominence in evidence synthesis\nresearch because it enables simultaneous synthesis of multiple correlated\noutcome data, and random-effects models have generally been used for addressing\nbetween-studies heterogeneities. However, coverage probabilities of confidence\nregions or intervals for standard inference methods for random-effects models\n(e.g., restricted maximum likelihood estimation) cannot retain their nominal\nconfidence levels in general, especially when the number of synthesized studies\nis small because their validities depend on large sample approximations. In\nthis article, we provide permutation-based inference methods that enable exact\njoint inferences for average outcome measures without large sample\napproximations. We also provide accurate marginal inference methods under\ngeneral settings of multivariate meta-analyses. We propose effective approaches\nfor permutation inferences using optimal weighting based on the efficient score\nstatistic. The effectiveness of the proposed methods is illustrated via\napplications to bivariate meta-analyses of diagnostic accuracy studies for\nairway eosinophilia in asthma and a network meta-analysis for antihypertensive\ndrugs on incident diabetes, as well as through simulation experiments. In\nnumerical evaluations performed via simulations, our methods generally provided\naccurate confidence regions or intervals under a broad range of settings,\nwhereas the current standard inference methods exhibited serious undercoverage\nproperties.\n", "versions": [{"version": "v1", "created": "Tue, 11 Sep 2018 14:43:49 GMT"}, {"version": "v2", "created": "Sat, 9 Feb 2019 13:29:55 GMT"}, {"version": "v3", "created": "Wed, 26 Jun 2019 07:59:27 GMT"}, {"version": "v4", "created": "Tue, 30 Jul 2019 15:56:59 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Noma", "Hisashi", ""], ["Nagashima", "Kengo", ""], ["Furukawa", "Toshi A.", ""]]}, {"id": "1809.04235", "submitter": "Kellie Ottoboni", "authors": "Kellie Ottoboni, Philip B. Stark, Mark Lindeman and Neal McBurnett", "title": "Risk-Limiting Audits by Stratified Union-Intersection Tests of Elections\n  (SUITE)", "comments": "This article draws heavily from arXiv:1803.00698", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Risk-limiting audits (RLAs) offer a statistical guarantee: if a full manual\ntally of the paper ballots would show that the reported election outcome is\nwrong, an RLA has a known minimum chance of leading to a full manual tally.\nRLAs generally rely on random samples. Stratified sampling--partitioning the\npopulation of ballots into disjoint strata and sampling independently from the\nstrata--may simplify logistics or increase efficiency compared to simpler\nsampling designs, but makes risk calculations harder. We present SUITE, a new\nmethod for conducting RLAs using stratified samples. SUITE considers all\npossible partitions of outcome-changing error across strata. For each\npartition, it combines P-values from stratum-level tests into a combined\nP-value; there is no restriction on the tests used in different strata. SUITE\nmaximizes the combined P-value over all partitions of outcome-changing error.\nThe audit can stop if that maximum is less than the risk limit. Voting systems\nin some Colorado counties (comprising 98.2% of voters) allow auditors to check\nhow the system interpreted each ballot, which allows ballot-level comparison\nRLAs. Other counties use ballot polling, which is less efficient. Extant\napproaches to conducting an RLA of a statewide contest would require major\nchanges to Colorado's procedures and software, or would sacrifice the\nefficiency of ballot-level comparison. SUITE does not. It divides ballots into\ntwo strata: those cast in counties that can conduct ballot-level comparisons,\nand the rest. Stratum-level P-values are found by methods derived here. The\nresulting audit is substantially more efficient than statewide ballot polling.\nSUITE is useful in any state with a mix of voting systems or that uses\nstratified sampling for other reasons. We provide an open-source reference\nimplementation and exemplar calculations in Jupyter notebooks.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 02:56:22 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Ottoboni", "Kellie", ""], ["Stark", "Philip B.", ""], ["Lindeman", "Mark", ""], ["McBurnett", "Neal", ""]]}, {"id": "1809.04347", "submitter": "Silvia Montagna", "authors": "Silvia Montagna, Irina Irincheeva and Surya T. Tokdar", "title": "High-dimensional Bayesian Fourier Analysis For Detecting Circadian Gene\n  Expressions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genomic applications, there is often interest in identifying genes whose\ntime-course expression trajectories exhibit periodic oscillations with a period\nof approximately 24 hours. Such genes are usually referred to as circadian, and\ntheir identification is a crucial step toward discovering physiological\nprocesses that are clock-controlled. It is natural to expect that the\nexpression of gene i at time j might depend to some degree on the expression of\nthe other genes measured at the same time. However, widely-used rhythmicity\ndetection techniques do not accommodate for the potential dependence across\ngenes. We develop a Bayesian approach for periodicity identification that\nexplicitly takes into account the complex dependence structure across\ntime-course trajectories in gene expressions. We employ a latent factor\nrepresentation to accommodate dependence, while representing the true\ntrajectories in the Fourier domain allows for inference on period, phase, and\namplitude of the signal. Identification of circadian genes is allowed through a\ncarefully chosen variable selection prior on the Fourier basis coefficients.\nThe methodology is applied to a novel mouse liver circadian dataset. Although\nmotivated by time-course gene expression array data, the proposed approach is\napplicable to the analysis of dependent functional data at broad.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 10:23:46 GMT"}], "update_date": "2018-09-13", "authors_parsed": [["Montagna", "Silvia", ""], ["Irincheeva", "Irina", ""], ["Tokdar", "Surya T.", ""]]}, {"id": "1809.04348", "submitter": "Jose Jimenez", "authors": "Jos\\'e L. Jim\\'enez, Sungjin Kim, Mourad Tighiouart", "title": "A Bayesian seamless phase I-II trial design with two stages for cancer\n  clinical trials with drug combinations", "comments": null, "journal-ref": null, "doi": "10.1002/bimj.201900095", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of drug combinations in clinical trials is increasingly common during\nthe last years since a more favorable therapeutic response may be obtained by\ncombining drugs. In phase I clinical trials, most of the existing methodology\nrecommends a one unique dose combination as \"optimal\", which may result in a\nsubsequent failed phase II clinical trial since other dose combinations may\npresent higher treatment efficacy for the same level of toxicity. We are\nparticularly interested in the setting where it is necessary to wait a few\ncycles of therapy to observe an efficacy outcome and the phase I and II\npopulation of patients are different with respect to treatment efficacy. Under\nthese circumstances, it is common practice to implement two-stage designs where\na set of maximum tolerated dose combinations is selected in a first stage, and\nthen studied in a second stage for treatment efficacy. In this article we\npresent a new two-stage design for early phase clinical trials with drug\ncombinations. In the first stage, binary toxicity data is used to guide the\ndose escalation and set the maximum tolerated dose combinations. In the second\nstage, we take the set of maximum tolerated dose combinations recommended from\nthe first stage, which remains fixed along the entire second stage, and through\nadaptive randomization, we allocate subsequent cohorts of patients in dose\ncombinations that are likely to have high posterior median time to progression.\nThe methodology is assessed with extensive simulations and exemplified with a\nreal trial.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 10:26:45 GMT"}, {"version": "v2", "created": "Sun, 9 Dec 2018 12:20:38 GMT"}, {"version": "v3", "created": "Fri, 14 Feb 2020 10:46:53 GMT"}], "update_date": "2020-02-17", "authors_parsed": [["Jim\u00e9nez", "Jos\u00e9 L.", ""], ["Kim", "Sungjin", ""], ["Tighiouart", "Mourad", ""]]}, {"id": "1809.04389", "submitter": "Pulong Ma", "authors": "Pulong Ma, Emily L. Kang", "title": "Spatio-Temporal Data Fusion for Massive Sea Surface Temperature Data\n  from MODIS and AMSR-E Instruments", "comments": "Accepted in Environmetrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Remote sensing data have been widely used to study various geophysical\nprocesses. With the advances in remote-sensing technology, massive amount of\nremote sensing data are collected in space over time. Different satellite\ninstruments typically have different footprints, measurement-error\ncharacteristics, and data coverages. To combine datasets from different\nsatellite instruments, we propose a dynamic fused Gaussian process (DFGP) model\nthat enables fast statistical inference such as filtering and smoothing for\nmassive spatio-temporal datasets in a data-fusion context. Based upon a\nspatio-temporal-random-effects model, the DFGP methodology represents the\nunderlying true process with two components: a linear combination of a small\nnumber of basis functions and random coefficients with a general covariance\nmatrix, together with a linear combination of a large number of basis functions\nand Markov random coefficients. To model the underlying geophysical process at\ndifferent spatial resolutions, we rely on the change-of-support property, which\nalso allows efficient computations in the DFGP model. To estimate model\nparameters, we devise a computationally efficient stochastic\nexpectation-maximization (SEM) algorithm to ensure its scalability for massive\ndatasets. The DFGP model is applied to a total of 3.7 million sea surface\ntemperature datasets in the tropical Pacific Ocean for a one-week time period\nin 2010 from MODIS and AMSR-E instruments.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 12:54:06 GMT"}, {"version": "v2", "created": "Fri, 7 Jun 2019 17:01:34 GMT"}], "update_date": "2019-06-10", "authors_parsed": [["Ma", "Pulong", ""], ["Kang", "Emily L.", ""]]}, {"id": "1809.04541", "submitter": "Dootika Vats", "authors": "Dootika Vats and James M. Flegal", "title": "Lugsail lag windows for estimating time-average covariance matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lag windows are commonly used in time series, econometrics, steady-state\nsimulation, and Markov chain Monte Carlo to estimate time-average covariance\nmatrices. In the presence of positive correlation of the underlying process,\nestimators of this matrix almost always exhibit significant negative bias,\nleading to undesirable finite-sample properties. We propose a new family of lag\nwindows specifically designed to improve finite-sample performance by\noffsetting this negative bias. Any existing lag window can be adapted into a\nlugsail equivalent with no additional assumptions. We use these lag windows\nwithin spectral variance estimators and demonstrate its advantages in a linear\nregression model with autocorrelated and heteroskedastic residuals. We further\nemploy the lugsail lag windows in weighted batch means estimators due to their\ncomputational efficiency on large simulation output. We obtain bias and\nvariance results for these multivariate estimators and significantly weaken the\nmixing condition on the process. Superior finite-sample properties are\nillustrated in a vector autoregressive process and a Bayesian logistic\nregression model.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 16:19:01 GMT"}, {"version": "v2", "created": "Wed, 27 May 2020 17:52:15 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 03:56:54 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Vats", "Dootika", ""], ["Flegal", "James M.", ""]]}, {"id": "1809.04587", "submitter": "Anshuka Rangi", "authors": "Anshuka Rangi, Massimo Franceschetti and Stefano Marano", "title": "Distributed Chernoff Test: Optimal decision systems over networks", "comments": "A part of this work has been accepted in ISIT 2018 and CDC 2018;\n  Submitted to IEEE Transactions on Information Theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.IT cs.MA math.IT stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study \"active\" decision making over sensor networks where the sensors'\nsequential probing actions are actively chosen by continuously learning from\npast observations. We consider two network settings: with and without central\ncoordination. In the first case, the network nodes interact with each other\nthrough a central entity, which plays the role of a fusion center. In the\nsecond case, the network nodes interact in a fully distributed fashion. In both\nof these scenarios, we propose sequential and adaptive hypothesis tests\nextending the classic Chernoff test. We compare the performance of the proposed\ntests to the optimal sequential test. In the presence of a fusion center, our\ntest achieves the same asymptotic optimality of the Chernoff test, minimizing\nthe risk, expressed by the expected cost required to reach a decision plus the\nexpected cost of making a wrong decision, when the observation cost per unit\ntime tends to zero. The test is also asymptotically optimal in the higher\nmoments of the time required to reach a decision. Additionally, the test is\nparsimonious in terms of communications, and the expected number of channel\nuses per network node tends to a small constant. In the distributed setup, our\ntest achieves the same asymptotic optimality of Chernoff's test, up to a\nmultiplicative constant in terms of both risk and the higher moments of the\ndecision time. Additionally, the test is parsimonious in terms of\ncommunications in comparison to state-of-the-art schemes proposed in the\nliterature. The analysis of these tests is also extended to account for message\nquantization and communication over channels with random erasures.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 17:51:30 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2020 14:31:24 GMT"}], "update_date": "2020-11-30", "authors_parsed": [["Rangi", "Anshuka", ""], ["Franceschetti", "Massimo", ""], ["Marano", "Stefano", ""]]}, {"id": "1809.04598", "submitter": "Edward Higson", "authors": "Edward Higson, Will Handley, Michael Hobson, Anthony Lasenby", "title": "Bayesian sparse reconstruction: a brute-force approach to astronomical\n  imaging and machine learning", "comments": "18 pages + appendix, 19 figures, minor updates to text and layout", "journal-ref": "Mon. Notices Royal Astron. Soc. 483, 4 (2019) p4828-4846", "doi": "10.1093/mnras/sty3307", "report-no": null, "categories": "astro-ph.IM stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a principled Bayesian framework for signal reconstruction, in\nwhich the signal is modelled by basis functions whose number (and form, if\nrequired) is determined by the data themselves. This approach is based on a\nBayesian interpretation of conventional sparse reconstruction and\nregularisation techniques, in which sparsity is imposed through priors via\nBayesian model selection. We demonstrate our method for noisy 1- and\n2-dimensional signals, including astronomical images. Furthermore, by using a\nproduct-space approach, the number and type of basis functions can be treated\nas integer parameters and their posterior distributions sampled directly. We\nshow that order-of-magnitude increases in computational efficiency are possible\nfrom this technique compared to calculating the Bayesian evidences separately,\nand that further computational gains are possible using it in combination with\ndynamic nested sampling. Our approach can also be readily applied to neural\nnetworks, where it allows the network architecture to be determined by the data\nin a principled Bayesian manner by treating the number of nodes and hidden\nlayers as parameters.\n", "versions": [{"version": "v1", "created": "Wed, 12 Sep 2018 18:00:01 GMT"}, {"version": "v2", "created": "Sun, 25 Nov 2018 17:59:37 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Higson", "Edward", ""], ["Handley", "Will", ""], ["Hobson", "Michael", ""], ["Lasenby", "Anthony", ""]]}, {"id": "1809.04808", "submitter": "Peter Vogel", "authors": "Tilmann Gneiting and Peter Vogel", "title": "Receiver Operating Characteristic (ROC) Curves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Receiver operating characteristic (ROC) curves are used ubiquitously to\nevaluate covariates, markers, or features as potential predictors in binary\nproblems. We distinguish raw ROC diagnostics and ROC curves, elucidate the\nspecial role of concavity in interpreting and modelling ROC curves, and\nestablish an equivalence between ROC curves and cumulative distribution\nfunctions (CDFs). These results support a subtle shift of paradigms in the\nstatistical modelling of ROC curves, which we view as curve fitting. We\nintroduce the flexible two-parameter beta family for fitting CDFs to empirical\nROC curves, derive the large sample distribution of the minimum distance\nestimator and provide software in R for estimation and testing, including both\nasymptotic and Monte Carlo based inference. In a range of empirical examples\nthe beta family and its three- and four-parameter ramifications that allow for\nstraight edges fit better than the classical binormal model, particularly under\nthe vital constraint of the fitted curve being concave.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 07:26:26 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Gneiting", "Tilmann", ""], ["Vogel", "Peter", ""]]}, {"id": "1809.04885", "submitter": "Anneke Weide", "authors": "Anneke Cleopatra Weide and Andr\\'e Beauducel", "title": "Varimax rotation based on gradient projection needs between 10 and more\n  than 500 random start loading matrices for optimal performance", "comments": "19 pages, 8 figures, 2 tables, 4 figures in the Supplement", "journal-ref": null, "doi": "10.3389/fpsyg.2019.00645", "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient projection rotation (GPR) is a promising method to rotate factor or\ncomponent loadings by different criteria. Since the conditions for optimal\nperformance of GPR-Varimax are widely unknown, this simulation study\ninvestigates GPR towards the Varimax criterion in principal component analysis.\nThe conditions of the simulation study comprise two sample sizes (n = 100, n =\n300), with orthogonal simple structure population models based on four numbers\nof components (3, 6, 9, 12), with- and without Kaiser-normalization, and six\nnumbers of random start loading matrices for GPR-Varimax rotation (1, 10, 50,\n100, 500, 1,000). GPR-Varimax rotation always performed better when at least 10\nrandom matrices were used for start loadings instead of the identity matrix.\nGPR-Varimax worked better for a small number of components, larger (n = 300) as\ncompared to smaller (n = 100) samples, and when loadings were Kaiser-normalized\nbefore rotation. To ensure optimal (stationary) performance of GPR-Varimax in\nrecovering orthogonal simple structure, we recommend using at least 10\niterations of start loading matrices for the rotation of up to three components\nand 50 iterations for up to six components. For up to nine components, rotation\nshould be based on a sample size of at least 300 cases, Kaiser-normalization,\nand more than 50 different start loading matrices. For more than nine\ncomponents, GPR-Varimax rotation should be based on at least 300 cases,\nKaiser-normalization, and at least 500 different start loading matrices.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 10:59:10 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Weide", "Anneke Cleopatra", ""], ["Beauducel", "Andr\u00e9", ""]]}, {"id": "1809.05038", "submitter": "Shirley Liao", "authors": "Shirley Liao and Corwin Zigler", "title": "Uncertainty in the Design Stage of Two-Stage Bayesian Propensity Score\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The two-stage process of propensity score analysis (PSA) includes a design\nstage where propensity scores are estimated and implemented to approximate a\nrandomized experiment and an analysis stage where treatment effects are\nestimated conditional upon the design. This paper considers how uncertainty\nassociated with the design stage impacts estimation of causal effects in the\nanalysis stage. Such design uncertainty can derive from the fact that the\npropensity score itself is an estimated quantity, but also from other features\nof the design stage tied to choice of propensity score implementation. This\npaper offers a procedure for obtaining the posterior distribution of causal\neffects after marginalizing over a distribution of design-stage outputs,\nlending a degree of formality to Bayesian methods for PSA (BPSA) that have\ngained attention in recent literature. Formulation of a probability\ndistribution for the design-stage output depends on how the propensity score is\nimplemented in the design stage, and propagation of uncertainty into causal\nestimates depends on how the treatment effect is estimated in the analysis\nstage. We explore these differences within a sample of commonly-used propensity\nscore implementations (quantile stratification, nearest-neighbor matching,\ncaliper matching, inverse probability of treatment weighting, and doubly robust\nestimation) and investigate in a simulation study the impact of statistician\nchoice in PS model and implementation on the degree of between- and\nwithin-design variability in the estimated treatment effect. The methods are\nthen deployed in an investigation of the association between levels of fine\nparticulate air pollution and elevated exposure to emissions from coal-fired\npower plants.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 16:15:39 GMT"}, {"version": "v2", "created": "Mon, 15 Jul 2019 15:09:45 GMT"}], "update_date": "2019-07-16", "authors_parsed": [["Liao", "Shirley", ""], ["Zigler", "Corwin", ""]]}, {"id": "1809.05052", "submitter": "Joe Watson", "authors": "Joe Watson and James V. Zidek and Gavin Shaddick", "title": "A general theory for preferential sampling in environmental networks", "comments": "33 pages of main text, 48 including the supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a general model framework for detecting the preferential\nsampling of environmental monitors recording an environmental process across\nspace and/or time. This is achieved by considering the joint distribution of an\nenvironmental process with a site--selection process that considers where and\nwhen sites are placed to measure the process. The environmental process may be\nspatial, temporal or spatio--temporal in nature. By sharing random effects\nbetween the two processes, the joint model is able to establish whether site\nplacement was stochastically dependent of the environmental process under\nstudy. The embedding into a spatio--temporal framework also allows for the\nmodelling of the dynamic site---selection process itself. Real--world factors\naffecting both the size and location of the network can be easily modelled and\nquantified. Depending upon the choice of population of locations to consider\nfor selection across space and time under the site--selection process,\ndifferent insights about the precise nature of preferential sampling can be\nobtained. The general framework developed in the paper is designed to be easily\nand quickly fit using the R-INLA package. We apply this framework to a case\nstudy involving particulate air pollution over the UK where a major reduction\nin the size of a monitoring network through time occurred. It is demonstrated\nthat a significant response--biased reduction in the air quality monitoring\nnetwork occurred. We also show that the network was consistently\nunrepresentative of the levels of particulate matter seen across much of GB\nthroughout the operating life of the network. Finally we show that this may\nhave led to a severe over-reporting of the population--average exposure levels\nexperienced across GB. This could have great impacts on estimates of the health\neffects of black smoke levels.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 16:35:30 GMT"}, {"version": "v2", "created": "Sat, 23 Mar 2019 01:20:43 GMT"}, {"version": "v3", "created": "Sun, 7 Apr 2019 00:10:46 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Watson", "Joe", ""], ["Zidek", "James V.", ""], ["Shaddick", "Gavin", ""]]}, {"id": "1809.05091", "submitter": "Jiguo Cao", "authors": "Tianyu Guan, Zhenhua Lin and Jiguo Cao", "title": "Estimating Historical Functional Linear Models with a Nested Group\n  Bridge Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a scalar-on-function historical linear regression model which\nassumes that the functional predictor does not influence the response when the\ntime passes a certain cutoff point. We approach this problem from the\nperspective of locally sparse modeling, where a function is locally sparse if\nit is zero on a substantial portion of its defining domain. In the historical\nlinear model, the slope function is exactly a locally sparse function that is\nzero beyond the cutoff time. A locally sparse estimate then gives rise to an\nestimate of the cutoff time. We propose a nested group bridge penalty that is\nable to specifically shrink the tail of a function. Combined with the B-spline\nbasis expansion and penalized least squares, the nested group bridge approach\ncan identify the cutoff time and produce a smooth estimate of the slope\nfunction simultaneously. The proposed locally sparse estimator is shown to be\nconsistent, while its numerical performance is illustrated by simulation\nstudies. The proposed method is demonstrated with an application of determining\nthe effect of the past engine acceleration on the current particulate matter\nemission.\n", "versions": [{"version": "v1", "created": "Thu, 13 Sep 2018 17:54:50 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Guan", "Tianyu", ""], ["Lin", "Zhenhua", ""], ["Cao", "Jiguo", ""]]}, {"id": "1809.05301", "submitter": "Markus Hainy", "authors": "Markus Hainy, David J. Price, Olivier Restif, Christopher Drovandi", "title": "Optimal Bayesian design for model discrimination via classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Performing optimal Bayesian design for discriminating between competing\nmodels is computationally intensive as it involves estimating posterior model\nprobabilities for thousands of simulated datasets. This issue is compounded\nfurther when the likelihood functions for the rival models are computationally\nexpensive. A new approach using supervised classification methods is developed\nto perform Bayesian optimal model discrimination design. This approach requires\nconsiderably fewer simulations from the candidate models than previous\napproaches using approximate Bayesian computation. Further, it is easy to\nassess the performance of the optimal design through the misclassification\nerror rate. The approach is particularly useful in the presence of models with\nintractable likelihoods but can also provide computational advantages when the\nlikelihoods are manageable.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 08:28:49 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2019 08:38:00 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Hainy", "Markus", ""], ["Price", "David J.", ""], ["Restif", "Olivier", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1809.05338", "submitter": "Jan-Frederik Mai", "authors": "Jan-Frederik Mai", "title": "Canonical spectral representation for exchangeable max-stable sequences", "comments": null, "journal-ref": "Extremes 23 (2020) 151-169", "doi": "10.1007/s10687-019-00361-3", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The set of infinite-dimensional, symmetric stable tail dependence functions\nassociated with exchangeable max-stable sequences of random variables with unit\nFr\\'echet margins is shown to be a simplex. Except for a single element, the\nextremal boundary is in one-to-one correspondence with the set of distribution\nfunctions of non-negative random variables with unit mean. Consequently, each\nelement is uniquely represented by a pair of a constant and a probability\nmeasure on the space of distribution functions of non-negative random variables\nwith unit mean. A canonical stochastic construction for arbitrary exchangeable\nmax-stable sequences and a stochastic representation for the Pickands\ndependence measure of finite-dimensional margins are immediate corollaries. As\nby-products, a canonical analytical description and an associated canonical Le\nPage series representation for non-decreasing stochastic processes that are\nstrongly infinitely divisible with respect to time are obtained.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 10:16:45 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 12:02:23 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 16:56:47 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2019 16:34:57 GMT"}, {"version": "v5", "created": "Mon, 28 Oct 2019 10:52:50 GMT"}], "update_date": "2020-11-06", "authors_parsed": [["Mai", "Jan-Frederik", ""]]}, {"id": "1809.05422", "submitter": "Eric Tchetgen Tchetgen", "authors": "Eric J Tchetgen Tchetgen, Haben Michael, Yifan Cui", "title": "Marginal Structural Models for Time-varying Endogenous Treatments: A\n  Time-Varying Instrumental Variable Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robins (1998) introduced marginal structural models (MSMs), a general class\nof counterfactual models for the joint effects of time-varying treatment\nregimes in complex longitudinal studies subject to time-varying confounding. He\nestablished identification of MSM parameters under a sequential randomization\nassumption (SRA), which essentially rules out unmeasured confounding of\ntreatment assignment over time. In this technical report, we consider\nsufficient conditions for identification of MSM parameters with the aid of a\ntime-varying instrumental variable, when sequential randomization fails to hold\ndue to unmeasured confounding. Our identification conditions essentially\nrequire that no unobserved confounder predicts compliance type for the\ntime-varying treatment, the longitudinal generalization of the identifying\ncondition of Wang and Tchetgen Tchetgen (2018). Under this assumption, We\nderive a large class of semiparametric estimators that extends standard\ninverse-probability weighting (IPW), the most popular approach for estimating\nMSMs under SRA, by incorporating the time-varying IV through a modified set of\nweights. The set of influence functions for MSM parameters is derived under a\nsemiparametric model with sole restriction on observed data distribution given\nby the MSM, and is shown to provide a rich class of multiply robust estimators,\nincluding a local semiparametric efficient estimator.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 13:47:47 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Tchetgen", "Eric J Tchetgen", ""], ["Michael", "Haben", ""], ["Cui", "Yifan", ""]]}, {"id": "1809.05497", "submitter": "Patrick Breheny", "authors": "Ryan Miller and Patrick Breheny", "title": "Feature-specific inference for penalized regression using local false\n  discovery rates", "comments": "22 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penalized regression methods, most notably the lasso, are a popular approach\nto analyzing high-dimensional data. An attractive property of the lasso is that\nit naturally performs variable selection. An important area of concern,\nhowever, is the reliability of these variable selections. Motivated by local\nfalse discovery rate methodology from the large-scale hypothesis testing\nliterature, we propose a method for calculating a local false discovery rate\nfor each variable under consideration by the lasso model. These rates can be\nused to assess the reliability of an individual feature, or to estimate the\nmodel's overall false discovery rate. The method can be used for all values of\n$\\lambda$. This is particularly useful for models with a few highly significant\nfeatures but a high overall Fdr, which are a relatively common occurrence when\nusing cross validation to select $\\lambda$. It is also flexible enough to be\napplied to many varieties of penalized likelihoods including GLM and Cox\nmodels, and a variety of penalties, including MCP and SCAD. We demonstrate the\nvalidity of this approach and contrast it with other inferential methods for\npenalized regression as well as with local false discovery rates for univariate\nhypothesis tests. Finally, we show the practical utility of our method by\napplying it to two case studies involving high dimensional genetic data.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 16:37:59 GMT"}, {"version": "v2", "created": "Thu, 19 Sep 2019 20:46:45 GMT"}], "update_date": "2019-09-23", "authors_parsed": [["Miller", "Ryan", ""], ["Breheny", "Patrick", ""]]}, {"id": "1809.05580", "submitter": "Christopher Franck", "authors": "Christopher T. Franck and Robert B. Gramacy", "title": "Assessing Bayes factor surfaces using interactive visualization and\n  computer surrogate modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian model selection provides a natural alternative to classical\nhypothesis testing based on p-values. While many papers mention that Bayesian\nmodel selection is frequently sensitive to prior specification on the\nparameters, there are few practical strategies to assess and report this\nsensitivity. This article has two goals. First, we aim educate the broader\nstatistical community about the extent of potential sensitivity through\nvisualization of the Bayes factor surface. The Bayes factor surface shows the\nvalue a Bayes factor takes (usually on the log scale) as a function of\nuser-specified hyperparameters. We provide interactive visualization through an\nR shiny application that allows the user to explore sensitivity in Bayes factor\nover a range of hyperparameter settings in a familiar regression setting. We\ncompare the surface with three automatic procedures. Second, we suggest\nsurrogate modeling via Gaussian processes (GPs) to visualize the Bayes factor\nsurface in situations where computation of Bayes factors is expensive. That is,\nwe treat Bayes factor calculation as a computer simulation experiment. In this\ncontext, we provide a fully reproducible example using accessible GP libraries\nto augment an important study of the influence of outliers in empirical\nfinance. We suggest Bayes factor surfaces are valuable for scientific reporting\nsince they (i) increase transparency, making potential instability in Bayes\nfactors easy to visualize, (ii) generalize to simple and more complicated\nexamples, and (iii) provide a path for researchers to assess the impact of\nprior choice on modeling decisions in a wide variety research areas.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 20:39:42 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 14:59:38 GMT"}, {"version": "v3", "created": "Thu, 30 Jan 2020 15:00:23 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Franck", "Christopher T.", ""], ["Gramacy", "Robert B.", ""]]}, {"id": "1809.05596", "submitter": "Preetum Nakkiran", "authors": "Preetum Nakkiran, Jaros{\\l}aw B{\\l}asiok", "title": "The Generic Holdout: Preventing False-Discoveries in Adaptive Data\n  Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive data analysis has posed a challenge to science due to its ability to\ngenerate false hypotheses on moderately large data sets. In general, with\nnon-adaptive data analyses (where queries to the data are generated without\nbeing influenced by answers to previous queries) a data set containing $n$\nsamples may support exponentially many queries in $n$. This number reduces to\nlinearly many under naive adaptive data analysis, and even sophisticated\nremedies such as the Reusable Holdout (Dwork et. al 2015) only allow\nquadratically many queries in $n$.\n  In this work, we propose a new framework for adaptive science which\nexponentially improves on this number of queries under a restricted yet\nscientifically relevant setting, where the goal of the scientist is to find a\nsingle (or a few) true hypotheses about the universe based on the samples. Such\na setting may describe the search for predictive factors of some disease based\non medical data, where the analyst may wish to try a number of predictive\nmodels until a satisfactory one is found.\n  Our solution, the Generic Holdout, involves two simple ingredients: (1) a\npartitioning of the data into a exploration set and a holdout set and (2) a\nlimited exposure strategy for the holdout set. An analyst is free to use the\nexploration set arbitrarily, but when testing hypotheses against the holdout\nset, the analyst only learns the answer to the question: \"Is the given\nhypothesis true (empirically) on the holdout set?\" -- and no more information,\nsuch as \"how well\" the hypothesis fits the holdout set. The resulting scheme is\nimmediate to analyze, but despite its simplicity we do not believe our method\nis obvious, as evidenced by the many violations in practice.\n  Our proposal can be seen as an alternative to pre-registration, and allows\nresearchers to get the benefits of adaptive data analysis without the problems\nof adaptivity.\n", "versions": [{"version": "v1", "created": "Fri, 14 Sep 2018 21:28:21 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Nakkiran", "Preetum", ""], ["B\u0142asiok", "Jaros\u0142aw", ""]]}, {"id": "1809.05627", "submitter": "Yifei Sun", "authors": "Yifei Sun, Sy Han Chiou, Mei-Cheng Wang", "title": "ROC-Guided Survival Trees and Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tree-based methods are popular nonparametric tools in studying time-to-event\noutcomes. In this article, we introduce a novel framework for survival trees\nand ensembles, where the trees partition the dynamic survivor population and\ncan handle time-dependent covariates. Using the idea of randomized tests, we\ndevelop generalized time-dependent Receiver Operating Characteristic (ROC)\ncurves for evaluating the performance of survival trees. The tree-building\nalgorithm is guided by decision-theoretic criteria based on ROC, targeting\nspecifically for prediction accuracy. To address the instability issue of a\nsingle tree, we propose a novel ensemble procedure based on averaging\nmartingale estimating equations, which is different from existing methods that\naverage the predicted survival or cumulative hazard functions from individual\ntrees. Extensive simulation studies are conducted to examine the performance of\nthe proposed methods. We apply the methods to a study on AIDS for illustration.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 01:09:42 GMT"}, {"version": "v2", "created": "Sun, 10 Feb 2019 02:08:59 GMT"}, {"version": "v3", "created": "Mon, 13 Jan 2020 18:50:06 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Sun", "Yifei", ""], ["Chiou", "Sy Han", ""], ["Wang", "Mei-Cheng", ""]]}, {"id": "1809.05667", "submitter": "Toni Karvonen", "authors": "Toni Karvonen, Silv\\`ere Bonnabel, Eric Moulines, Simo S\\\"arkk\\\"a", "title": "On stability of a class of filters for non-linear stochastic systems", "comments": "Accepted for publication in SIAM Journal on Control and Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article develops a comprehensive framework for stability analysis of a\nbroad class of commonly used continuous and discrete time-filters for\nstochastic dynamic systems with non-linear state dynamics and linear\nmeasurements under certain strong assumptions. The class of filters encompasses\nthe extended and unscented Kalman filters and most other Gaussian assumed\ndensity filters and their numerical integration approximations. The stability\nresults are in the form of time-uniform mean square bounds and exponential\nconcentration inequalities for the filtering error. In contrast to existing\nresults, it is not always necessary for the model to be exponentially stable or\nfully observed. We review three classes of models that can be rigorously shown\nto satisfy the stringent assumptions of the stability theorems. Numerical\nexperiments using synthetic data validate the derived error bounds.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 08:03:26 GMT"}, {"version": "v2", "created": "Wed, 10 Jun 2020 12:46:17 GMT"}], "update_date": "2020-06-11", "authors_parsed": [["Karvonen", "Toni", ""], ["Bonnabel", "Silv\u00e8re", ""], ["Moulines", "Eric", ""], ["S\u00e4rkk\u00e4", "Simo", ""]]}, {"id": "1809.05706", "submitter": "Sami Stouli", "authors": "Whitney Newey and Sami Stouli", "title": "Control Variables, Discrete Instruments, and Identification of\n  Structural Functions", "comments": "37 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Control variables provide an important means of controlling for endogeneity\nin econometric models with nonseparable and/or multidimensional heterogeneity.\nWe allow for discrete instruments, giving identification results under a\nvariety of restrictions on the way the endogenous variable and the control\nvariables affect the outcome. We consider many structural objects of interest,\nsuch as average or quantile treatment effects. We illustrate our results with\nan empirical application to Engel curve estimation.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 12:05:07 GMT"}, {"version": "v2", "created": "Thu, 5 Dec 2019 23:57:03 GMT"}], "update_date": "2019-12-09", "authors_parsed": [["Newey", "Whitney", ""], ["Stouli", "Sami", ""]]}, {"id": "1809.05722", "submitter": "Mahdi Teimouri Yanesari", "authors": "Mahdi Teimouri", "title": "Statistical Inference for Mixture of Cauchy Distributions", "comments": "17 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The class of $\\alpha$-stable distributions received much interest for\nmodelling impulsive phenomena occur in engineering, economics, insurance, and\nphysics. The lack of non-analytical form for probability density function is\nconsidered as the main obstacle to modelling data via the class of\n$\\alpha$-stable distributions. As the central member of this class, the Cauchy\ndistribution has received many applications in economics, seismology,\ntheoretical and applied physics. We derive estimators for the parameters of the\nCauchy and mixture of Cauchy distributions through the EM algorithm.\nPerformance of the EM algorithm is demonstrated through simulations and real\nsets of data.\n", "versions": [{"version": "v1", "created": "Sat, 15 Sep 2018 14:29:55 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Teimouri", "Mahdi", ""]]}, {"id": "1809.05800", "submitter": "Ziwen An", "authors": "Ziwen An, David J. Nott and Christopher Drovandi", "title": "Robust Bayesian Synthetic Likelihood via a Semi-Parametric Approach", "comments": "37 pages Latex; the paper has been re-organised, moved section 4 and\n  5 to appendices, moved less important example figures to appendices, added\n  \"sensitivity to n\" section to appendices, added a shrinkage example to\n  appendices, typos and references corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian synthetic likelihood (BSL) is now a well established method for\nperforming approximate Bayesian parameter estimation for simulation-based\nmodels that do not possess a tractable likelihood function. BSL approximates an\nintractable likelihood function of a carefully chosen summary statistic at a\nparameter value with a multivariate normal distribution. The mean and\ncovariance matrix of this normal distribution are estimated from independent\nsimulations of the model. Due to the parametric assumption implicit in BSL, it\ncan be preferred to its non-parametric competitor, approximate Bayesian\ncomputation, in certain applications where a high-dimensional summary statistic\nis of interest. However, despite several successful applications of BSL, its\nwidespread use in scientific fields may be hindered by the strong normality\nassumption. In this paper, we develop a semi-parametric approach to relax this\nassumption to an extent and maintain the computational advantages of BSL\nwithout any additional tuning. We test our new method, semiBSL, on several\nchallenging examples involving simulated and real data and demonstrate that\nsemiBSL can be significantly more robust than BSL and another approach in the\nliterature.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 03:28:16 GMT"}, {"version": "v2", "created": "Thu, 3 Oct 2019 07:15:12 GMT"}], "update_date": "2019-10-04", "authors_parsed": [["An", "Ziwen", ""], ["Nott", "David J.", ""], ["Drovandi", "Christopher", ""]]}, {"id": "1809.05935", "submitter": "Michele Peruzzi", "authors": "Michele Peruzzi, David B. Dunson", "title": "Bayesian Modular and Multiscale Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of multiscale regression for predictors that are\nspatially or temporally indexed, or with a pre-specified multiscale structure,\nwith a Bayesian modular approach. The regression function at the finest scale\nis expressed as an additive expansion of coarse to fine step functions. Our\nModular and Multiscale (M&M) methodology provides multiscale decomposition of\nhigh-dimensional data arising from very fine measurements. Unlike more complex\nmethods for functional predictors, our approach provides easy interpretation of\nthe results. Additionally, it provides a quantification of uncertainty on the\ndata resolution, solving a common problem researchers encounter with simple\nmodels on down-sampled data. We show that our modular and multiscale posterior\nhas an empirical Bayes interpretation, with a simple limiting distribution in\nlarge samples. An efficient sampling algorithm is developed for posterior\ncomputation, and the methods are illustrated through simulation studies and an\napplication to brain image classification. Source code is available as an R\npackage at https://github.com/mkln/bmms.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 19:18:28 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Peruzzi", "Michele", ""], ["Dunson", "David B.", ""]]}, {"id": "1809.05976", "submitter": "Hejian Sang", "authors": "Hejian Sang, Jae Kwang Kim", "title": "Semiparametric fractional imputation using Gaussian mixture models for\n  handling multivariate missing data", "comments": "23 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Item nonresponse is frequently encountered in practice. Ignoring missing data\ncan lose efficiency and lead to misleading inference. Fractional imputation is\na frequentist approach of imputation for handling missing data. However, the\nparametric fractional imputation of \\cite{kim2011parametric} may be subject to\nbias under model misspecification. In this paper, we propose a novel\nsemiparametric fractional imputation method using Gaussian mixture models. The\nproposed method is computationally efficient and leads to robust estimation.\nThe proposed method is further extended to incorporate the categorical\nauxiliary information. The asymptotic model consistency and $\\sqrt{n}$-\nconsistency of the semiparametric fractional imputation estimator are also\nestablished. Some simulation studies are presented to check the finite sample\nperformance of the proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 16 Sep 2018 23:01:35 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Sang", "Hejian", ""], ["Kim", "Jae Kwang", ""]]}, {"id": "1809.06052", "submitter": "Arabin Kumar Dey", "authors": "Biplab Paul, Arabin Kumar Dey, Arjun K Gupta and Debasis Kundu", "title": "Parameter Estimation of absolute continuous four parameter Geometric\n  Marshall-Olkin bivariate Pareto Distribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we formulate a four parameter absolute continuous Geometric\nMarshall-Olkin bivariate Pareto distribution and study its parameter estimation\nthrough EM algorithm and also explore the bayesian analysis through slice cum\nGibbs sampler approach. Numerical results are shown to verify the performance\nof the algorithms. We illustrate the procedures through a real life data\nanalysis.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 07:42:21 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Paul", "Biplab", ""], ["Dey", "Arabin Kumar", ""], ["Gupta", "Arjun K", ""], ["Kundu", "Debasis", ""]]}, {"id": "1809.06092", "submitter": "Kevin Kokot", "authors": "Holger Dette, Kevin Kokot, Stanislav Volgushev", "title": "Testing relevant hypotheses in functional time series via\n  self-normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop methodology for testing relevant hypotheses about\nfunctional time series in a tuning-free way. Instead of testing for exact\nequality, for example for the equality of two mean functions from two\nindependent time series, we propose to test the null hypothesis of no relevant\ndeviation. In the two sample problem this means that an $L^2$-distance between\nthe two mean functions is smaller than a pre-specified threshold. For such\nhypotheses self-normalization, which was introduced by Shao (2010) and Shao and\nZhang (2010) and is commonly used to avoid the estimation of nuisance\nparameters, is not directly applicable. We develop new self-normalized\nprocedures for testing relevant hypotheses in the one sample, two sample and\nchange point problem and investigate their asymptotic properties. Finite sample\nproperties of the proposed tests are illustrated by means of a simulation study\nand data examples. Our main focus is on functional time series, but extensions\nto other settings are also briefly discussed.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 09:27:04 GMT"}, {"version": "v2", "created": "Tue, 3 Sep 2019 08:18:11 GMT"}, {"version": "v3", "created": "Thu, 20 Feb 2020 07:52:08 GMT"}], "update_date": "2020-02-21", "authors_parsed": [["Dette", "Holger", ""], ["Kokot", "Kevin", ""], ["Volgushev", "Stanislav", ""]]}, {"id": "1809.06143", "submitter": "Christian R\\\"over", "authors": "Christian R\\\"over, Tim Friede", "title": "Contribution to the discussion of \"When should meta-analysis avoid\n  making hidden normality assumptions?\": A Bayesian perspective", "comments": "3 pages, 1 figure", "journal-ref": "Biometrical Journal, 60(6):1068-1070, 2018", "doi": "10.1002/bimj.201800179", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contribution to the discussion of \"When should meta-analysis avoid making\nhidden normality assumptions?\" by Dan Jackson and Ian R. White (2018;\nhttps://doi.org/10.1002/bimj.201800071).\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 11:50:20 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["R\u00f6ver", "Christian", ""], ["Friede", "Tim", ""]]}, {"id": "1809.06255", "submitter": "Xiaoyun Quan", "authors": "Xiaoyun Quan, James G. Booth and Martin T. Wells", "title": "Rank-based approach for estimating correlations in mixed ordinal data", "comments": "arXiv admin note: text overlap with arXiv:1703.04957 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High-dimensional mixed data as a combination of both continuous and ordinal\nvariables are widely seen in many research areas such as genomic studies and\nsurvey data analysis. Estimating the underlying correlation among mixed data is\nhence crucial for further inferring dependence structure. We propose a\nsemiparametric latent Gaussian copula model for this problem. We start with\nestimating the association among ternary-continuous mixed data via a rank-based\napproach and generalize the methodology to p-level-ordinal and continuous mixed\ndata. Concentration rate of the estimator is also provided and proved. At last,\nwe demonstrate the performance of the proposed estimator by extensive\nsimulations and two case studies of real data examples of algorithmic risk\nscore evaluation and cancer patients survival data.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 14:59:08 GMT"}], "update_date": "2018-09-18", "authors_parsed": [["Quan", "Xiaoyun", ""], ["Booth", "James G.", ""], ["Wells", "Martin T.", ""]]}, {"id": "1809.06405", "submitter": "Arabin Kumar Dey", "authors": "Biplab Paul, Arabin Kumar Dey and Sanku Dey", "title": "Bayesian analysis of absolute continuous Marshall-Olkin bivariate Pareto\n  distribution with location and scale parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides two different novel approaches of slice sampling to\nestimate the parameters of absolute continuous Marshall-Olkin bivariate Pareto\ndistribution with location and scale parameters. We carry out the bayesian\nanalysis taking gamma prior for shape and scale parameters and truncated normal\nfor location parameters. Credible intervals and coverage probabilities are also\nprovided for all methods. A real-life data analysis is shown for illustrative\npurpose.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 18:48:30 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Paul", "Biplab", ""], ["Dey", "Arabin Kumar", ""], ["Dey", "Sanku", ""]]}, {"id": "1809.06447", "submitter": "Pengfei Li", "authors": "Jiahua Chen, Pengfei Li and Guanfu Liu", "title": "Homogeneity testing under finite location-scale mixtures", "comments": "29 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The testing problem for the order of finite mixture models has a long history\nand remains an active research topic. Since Ghosh and Sen (1985) revealed the\nhard-to-manage asymptotic properties of the likelihood ratio test, there has\nbeen marked progress. The most successful attempts include the modified\nlikelihood ratio test and the EM-test, which lead to neat solutions for finite\nmixtures of univariate normal distributions, finite mixtures of\nsingle-parameter distributions, and several mixture-like models. The problem\nremains challenging, and there is still no generic solution for location-scale\nmixtures. In this paper, we provide an EM-test solution for homogeneity for\nfinite mixtures of location-scale family distributions. This EM-test has\nnonstandard limiting distributions, but we are able to find the critical values\nnumerically. We use computer experiments to obtain appropriate values for the\ntuning parameters. A simulation study shows that the fine-tuned EM-test has\nclose to nominal type I errors and very good power properties. Two application\nexamples are included to demonstrate the performance of the EM-test.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 21:12:48 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Chen", "Jiahua", ""], ["Li", "Pengfei", ""], ["Liu", "Guanfu", ""]]}, {"id": "1809.06462", "submitter": "Jiguo Cao", "authors": "Yunlong Nie and Yuping Yang and JIguo Cao", "title": "Recovering the Underlying Trajectory from Sparse and Irregular\n  Longitudinal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we consider the problem of recovering the underlying\ntrajectory when the longitudinal data are sparsely and irregularly observed and\nnoise-contaminated. Such data are popularly analyzed with functional principal\ncomponent analysis via the Principal Analysis by Conditional Estimation (PACE)\nmethod. The PACE method may sometimes be numerically unstable because it\ninvolves the inverse of the covariance matrix. We propose a sparse orthonormal\napproximation (SOAP) method as an alternative. It estimates the optimal\nempirical basis functions in the best approximation framework rather than\neigen-decomposing the covariance function. The SOAP method avoids estimating\nthe mean and covariance function, which is challenging when the assembled time\npoints with observations for all subjects are not sufficiently dense. The SOAP\nmethod avoids the inverse of the covariance matrix, hence the computation is\nmore stable. It does not require the functional principal component scores to\nfollow the Gaussian distribution. We show that the SOAP estimate for the\noptimal empirical basis function is asymptotically consistent. The finite\nsample performance of the SOAP method is investigated in simulation studies in\ncomparison with the PACE method. Our method is demonstrated by recovering the\nCD4 percentage curves from sparse and irregular data in the Multi-center AIDS\nCohort Study.\n", "versions": [{"version": "v1", "created": "Mon, 17 Sep 2018 22:26:20 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Nie", "Yunlong", ""], ["Yang", "Yuping", ""], ["Cao", "JIguo", ""]]}, {"id": "1809.06581", "submitter": "Mario Teixeira Parente", "authors": "Mario Teixeira Parente", "title": "A probabilistic framework for approximating functions in active\n  subspaces", "comments": "21 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.NA math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a comprehensive probabilistic setup to compute\napproximating functions in active subspaces. Constantine et al. proposed the\nactive subspace method in (Constantine et al., 2014) to reduce the dimension of\ncomputational problems. It can be seen as an attempt to approximate a\nhigh-dimensional function of interest $f$ by a low-dimensional one. To do this,\na common approach is to integrate $f$ over the inactive, i.e. non-dominant,\ndirections with a suitable conditional density function. In practice, this can\nbe done with a finite Monte Carlo sum, making not only the resulting\napproximation random in the inactive variable for each fixed input from the\nactive subspace, but also its expectation, i.e. the integral of the\nlow-dimensional function weighted with a probability measure on the active\nvariable. In this regard we develop a fully probabilistic framework extending\nresults from (Constantine et al., 2014, 2016). The results are supported by a\nsimple numerical example.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 08:21:22 GMT"}, {"version": "v2", "created": "Sun, 7 Apr 2019 07:43:21 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Parente", "Mario Teixeira", ""]]}, {"id": "1809.06594", "submitter": "Patrick J. Laub", "authors": "Thomas Taimre and Patrick J. Laub", "title": "Rare tail approximation using asymptotics and $L^1$ polar coordinates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a class of importance sampling (IS) estimators for\nestimating the right tail probability of a sum of continuous random variables\nbased on a change of variables to $L^1$ polar coordinates in which the radial\nand angular components of the IS distribution are considered separately.\n  When the asymptotic behaviour of the sum is known we exploit it for the\nradial change of measure, and the resulting estimator has the appealing form of\nthe (known) asymptotic multiplied by a random multiplicative correction factor.\nGiven we assume knowledge of the asymptotic behaviour of the sum in this\nframework, traditional notions of efficiency that appear in the rare-event\nliterature hold little practical meaning here. Instead, we focus on the\npractical behaviour of the proposed estimator in the pre-asymptotic regime for\nright tail probabilities between roughly $10^{-3}$ and $10^{-7}$.\n  The proposed estimator and procedure are applicable in both the heavy- and\nlight-tailed settings, as well as for independent and dependent summands. In\nthe case of independent summands, we find that our estimator compares\nfavourably with exponential tilting (iid light-tailed summands) and the\nAsmussen--Kroese method (independent subexponential summands).\n  However, for dependent subexponential summands using the same simple angular\ndistribution as for the independent case, the performance of our estimator\nrapidly degenerates with increasing dimension, suggesting an open avenue for\nfurther research.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 08:47:29 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Taimre", "Thomas", ""], ["Laub", "Patrick J.", ""]]}, {"id": "1809.06636", "submitter": "Gilles Kratzer", "authors": "Gilles Kratzer and Reinhard Furrer and Marta Pittavino", "title": "Comparison between Suitable Priors for Additive Bayesian Networks", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additive Bayesian networks are types of graphical models that extend the\nusual Bayesian generalized linear model to multiple dependent variables through\nthe factorisation of the joint probability distribution of the underlying\nvariables. When fitting an ABN model, the choice of the prior of the parameters\nis of crucial importance. If an inadequate prior - like a too weakly\ninformative one - is used, data separation and data sparsity lead to issues in\nthe model selection process. In this work a simulation study between two weakly\nand a strongly informative priors is presented. As weakly informative prior we\nuse a zero mean Gaussian prior with a large variance, currently implemented in\nthe R-package abn. The second prior belongs to the Student's t-distribution,\nspecifically designed for logistic regressions and, finally, the strongly\ninformative prior is again Gaussian with mean equal to true parameter value and\na small variance. We compare the impact of these priors on the accuracy of the\nlearned additive Bayesian network in function of different parameters. We\ncreate a simulation study to illustrate Lindley's paradox based on the prior\nchoice. We then conclude by highlighting the good performance of the\ninformative Student's t-prior and the limited impact of the Lindley's paradox.\nFinally, suggestions for further developments are provided.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 10:53:51 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Kratzer", "Gilles", ""], ["Furrer", "Reinhard", ""], ["Pittavino", "Marta", ""]]}, {"id": "1809.06679", "submitter": "Thomas Klausch", "authors": "Thomas Klausch, Peter van de Ven, Tim van de Brug, Mark A. van de\n  Wiel, Johannes Berkhof", "title": "Estimating Bayesian Optimal Treatment Regimes for Dichotomous Outcomes\n  using Observational Data", "comments": "30 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal treatment regimes (OTR) are individualised treatment assignment\nstrategies that identify a medical treatment as optimal given all background\ninformation available on the individual. We discuss Bayes optimal treatment\nregimes estimated using a loss function defined on the bivariate distribution\nof dichotomous potential outcomes. The proposed approach allows considering\nmore general objectives for the OTR than maximization of an expected outcome\n(e.g., survival probability) by taking into account, for example, unnecessary\ntreatment burden. As a motivating example we consider the case of oropharynx\ncancer treatment where unnecessary burden due to chemotherapy is to be avoided\nwhile maximizing survival chances. Assuming ignorable treatment assignment we\ndescribe Bayesian inference about the OTR including a sensitivity analysis on\nthe unobserved partial association of the potential outcomes. We evaluate the\nmethodology by simulations that apply Bayesian parametric and more flexible\nnon-parametric outcome models. The proposed OTR for oropharynx cancer reduces\nthe frequency of the more burdensome chemotherapy assignment by approximately\n75% without reducing the average survival probability. This regime thus offers\na strong increase in expected quality of life of patients.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 13:03:02 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 20:18:18 GMT"}], "update_date": "2018-10-02", "authors_parsed": [["Klausch", "Thomas", ""], ["van de Ven", "Peter", ""], ["van de Brug", "Tim", ""], ["van de Wiel", "Mark A.", ""], ["Berkhof", "Johannes", ""]]}, {"id": "1809.06758", "submitter": "James Scott Mr", "authors": "James Scott, Axel Gandy", "title": "State-Dependent Kernel Selection for Conditional Sampling of Graphs", "comments": "Package implementing the samplers can be found at\n  https://github.com/jscott6/cgsampr", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces new efficient algorithms for two problems: sampling\nconditional on vertex degrees in unweighted graphs, and sampling conditional on\nvertex strengths in weighted graphs. The algorithms can sample conditional on\nthe presence or absence of an arbitrary number of edges. The resulting\nconditional distributions provide the basis for exact tests. Existing samplers\nbased on MCMC or sequential importance sampling are generally not scalable;\ntheir efficiency degrades in sparse graphs. MCMC methods usually require\nexplicit computation of a Markov basis to navigate the complex state space;\nthis is computationally intensive even for small graphs. We use state-dependent\nkernel selection to develop new MCMC samplers. These do not require a Markov\nbasis, and are efficient both in sparse and dense graphs. The key idea is to\nintelligently select a Markov kernel on the basis of the current state of the\nchain. We apply our methods to testing hypotheses on a real network and\ncontingency table. The algorithms appear orders of magnitude more efficient\nthan existing methods in the test cases considered.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 14:17:30 GMT"}], "update_date": "2018-09-19", "authors_parsed": [["Scott", "James", ""], ["Gandy", "Axel", ""]]}, {"id": "1809.06959", "submitter": "Mark Tygert", "authors": "Mark Tygert, Rachel Ward, and Jure Zbontar", "title": "Compressed sensing with a jackknife and a bootstrap", "comments": "67 pages, 83 figures: the images in the appendix are low-quality;\n  high-quality images are available at http://tygert.com/comps.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV eess.SP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Compressed sensing proposes to reconstruct more degrees of freedom in a\nsignal than the number of values actually measured. Compressed sensing\ntherefore risks introducing errors -- inserting spurious artifacts or masking\nthe abnormalities that medical imaging seeks to discover. The present case\nstudy of estimating errors using the standard statistical tools of a jackknife\nand a bootstrap yields error \"bars\" in the form of full images that are\nremarkably representative of the actual errors (at least when evaluated and\nvalidated on data sets for which the ground truth and hence the actual error is\navailable). These images show the structure of possible errors -- without\nrecourse to measuring the entire ground truth directly -- and build confidence\nin regions of the images where the estimated errors are small.\n", "versions": [{"version": "v1", "created": "Tue, 18 Sep 2018 23:08:48 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Tygert", "Mark", ""], ["Ward", "Rachel", ""], ["Zbontar", "Jure", ""]]}, {"id": "1809.06996", "submitter": "Andr\\'es Ram\\'irez Hassan Pr.", "authors": "Andres Ramirez-Hassan and Manuel Correa-Giraldo", "title": "Focused econometric estimation for noisy and small datasets: A Bayesian\n  Minimum Expected Loss estimator approach", "comments": "46 pages, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Central to many inferential situations is the estimation of rational\nfunctions of parameters. The mainstream in statistics and econometrics\nestimates these quantities based on the plug-in approach without consideration\nof the main objective of the inferential situation. We propose the Bayesian\nMinimum Expected Loss (MELO) approach focusing explicitly on the function of\ninterest, and calculating its frequentist variability. Asymptotic properties of\nthe MELO estimator are similar to the plug-in approach. Nevertheless,\nsimulation exercises show that our proposal is better in situations\ncharacterized by small sample sizes and noisy models. In addition, we observe\nin the applications that our approach gives lower standard errors than\nfrequently used alternatives when datasets are not very informative.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 03:25:17 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Ramirez-Hassan", "Andres", ""], ["Correa-Giraldo", "Manuel", ""]]}, {"id": "1809.07068", "submitter": "Linda Nab", "authors": "Linda Nab, Rolf H.H. Groenwold, Paco M.J. Welsing and Maarten van\n  Smeden", "title": "Measurement error in continuous endpoints in randomised trials: problems\n  and solutions", "comments": "37 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In randomised trials, continuous endpoints are often measured with some\ndegree of error. This study explores the impact of ignoring measurement error,\nand proposes methods to improve statistical inference in the presence of\nmeasurement error. Three main types of measurement error in continuous\nendpoints are considered: classical, systematic and differential. For each\nmeasurement error type, a corrected effect estimator is proposed. The corrected\nestimators and several methods for confidence interval estimation are tested in\na simulation study. These methods combine information about error-prone and\nerror-free measurements of the endpoint in individuals not included in the\ntrial (external calibration sample). We show that if measurement error in\ncontinuous endpoints is ignored, the treatment effect estimator is unbiased\nwhen measurement error is classical, while Type-II error is increased at a\ngiven sample size. Conversely, the estimator can be substantially biased when\nmeasurement error is systematic or differential. In those cases, bias can\nlargely be prevented and inferences improved upon using information from an\nexternal calibration sample, of which the required sample size increases as the\nstrength of the association between the error-prone and error-free endpoint\ndecreases. Measurement error correction using already a small (external)\ncalibration sample is shown to improve inferences and should be considered in\ntrials with error-prone endpoints. Implementation of the proposed correction\nmethods is accommodated by a new software package for R.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 08:53:23 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 11:20:59 GMT"}, {"version": "v3", "created": "Thu, 29 Aug 2019 08:39:57 GMT"}], "update_date": "2019-08-30", "authors_parsed": [["Nab", "Linda", ""], ["Groenwold", "Rolf H. H.", ""], ["Welsing", "Paco M. J.", ""], ["van Smeden", "Maarten", ""]]}, {"id": "1809.07110", "submitter": "Chris Sherlock Dr.", "authors": "Chris Sherlock", "title": "Direct statistical inference for finite Markov jump processes via the\n  matrix exponential", "comments": "Focus much more on statistical inference, filtering and prediction", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given noisy, partial observations of a time-homogeneous, finite-statespace\nMarkov chain, conceptually simple, direct statistical inference is available,\nin theory, via its rate matrix, or infinitesimal generator, $\\mathsf{Q}$, since\n$\\exp (\\mathsf{Q}t)$ is the transition matrix over time $t$. However, perhaps\nbecause of inadequate tools for matrix exponentiation in programming languages\ncommonly used amongst statisticians or a belief that the necessary calculations\nare prohibitively expensive, statistical inference for continuous-time Markov\nchains with a large but finite state space is typically conducted via particle\nMCMC or other relatively complex inference schemes.\n  When, as in many applications $\\mathsf{Q}$ arises from a reaction network, it\nis usually sparse. We describe variations on known algorithms which allow fast,\nrobust and accurate evaluation of the product of a non-negative vector with the\nexponential of a large, sparse rate matrix. Our implementation uses relatively\nrecently developed, efficient, linear algebra tools that take advantage of such\nsparsity. We demonstrate the straightforward statistical application of the key\nalgorithm on a model for the mixing of two alleles in a population and on the\nSusceptible-Infectious-Removed epidemic model.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 10:16:32 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2020 08:48:56 GMT"}], "update_date": "2020-03-23", "authors_parsed": [["Sherlock", "Chris", ""]]}, {"id": "1809.07111", "submitter": "Miguel Angel Luque-Fernandez", "authors": "Miguel Angel Luque-Fernandez, Michael Schomaker, Daniel\n  Redondo-Sanchez, Maria Jose Sanchez Perez, Anand Vaidya, Mireille E.\n  Schnitzer", "title": "Educational Note: Paradoxical Collider Effect in the Analysis of\n  Non-Communicable Disease Epidemiological Data: a reproducible illustration\n  and web application", "comments": null, "journal-ref": null, "doi": "10.1093/ije/dyy275", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical epidemiology has focused on the control of confounding but it is\nonly recently that epidemiologists have started to focus on the bias produced\nby colliders. A collider for a certain pair of variables (e.g., an outcome Y\nand an exposure A) is a third variable (C) that is caused by both. In a\ndirected acyclic graph (DAG), a collider is the variable in the middle of an\ninverted fork (i.e., the variable C in A -> C <- Y). Controlling for, or\nconditioning an analysis on a collider (i.e., through stratification or\nregression) can introduce a spurious association between its causes. This\npotentially explains many paradoxical findings in the medical literature, where\nestablished risk factors for a particular outcome appear protective. We use an\nexample from non-communicable disease epidemiology to contextualize and explain\nthe effect of conditioning on a collider. We generate a dataset with 1,000\nobservations and run Monte-Carlo simulations to estimate the effect of 24-hour\ndietary sodium intake on systolic blood pressure, controlling for age, which\nacts as a confounder, and 24-hour urinary protein excretion, which acts as a\ncollider. We illustrate how adding a collider to a regression model introduces\nbias. Thus, to prevent paradoxical associations, epidemiologists estimating\ncausal effects should be wary of conditioning on colliders. We provide R-code\nin easy-to-read boxes throughout the manuscript and a GitHub repository\n(https://github.com/migariane/ColliderApp) for the reader to reproduce our\nexample. We also provide an educational web application allowing real-time\ninteraction to visualize the paradoxical effect of conditioning on a collider\nhttp://watzilei.com/shiny/collider/.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 10:19:06 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 09:21:37 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 10:12:22 GMT"}, {"version": "v4", "created": "Sun, 10 Nov 2019 12:44:15 GMT"}], "update_date": "2019-11-12", "authors_parsed": [["Luque-Fernandez", "Miguel Angel", ""], ["Schomaker", "Michael", ""], ["Redondo-Sanchez", "Daniel", ""], ["Perez", "Maria Jose Sanchez", ""], ["Vaidya", "Anand", ""], ["Schnitzer", "Mireille E.", ""]]}, {"id": "1809.07292", "submitter": "David Robertson", "authors": "David S. Robertson and James M.S. Wason", "title": "Online control of the false discovery rate in biomedical research", "comments": "48 pages, 18 figures; Updated references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern biomedical research frequently involves testing multiple related\nhypotheses, while maintaining control over a suitable error rate. In many\napplications the false discovery rate (FDR), which is the expected proportion\nof false positives among the rejected hypotheses, has become the standard error\ncriterion. Procedures that control the FDR, such as the well-known\nBenjamini-Hochberg procedure, assume that all p-values are available to be\ntested at a single time point. However, this ignores the sequential nature of\nmany biomedical experiments, where a sequence of hypotheses is tested without\nhaving access to future p-values or even the number of hypotheses. Recently,\nthe first procedures that control the FDR in this online manner have been\nproposed by Javanmard and Montanari (Ann. Stat. 2018), and built upon by Ramdas\net al. (NIPS 2017, ICML 2018). In this paper, we compare and contrast these\nproposed procedures, with a particular focus on the setting where the p-values\nare dependent. We also propose a simple modification of the procedures for when\nthere is an upper bound on the number of hypotheses to be tested. Using\ncomprehensive simulation scenarios and case studies, we provide recommendations\nfor which procedures to use in practice for online FDR control.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 16:37:46 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 17:08:01 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Robertson", "David S.", ""], ["Wason", "James M. S.", ""]]}, {"id": "1809.07415", "submitter": "Gustavo Lana", "authors": "Gustavo C. Lana, Glaura C. Franco, Sokol Ndreca", "title": "Bias corrected minimum distance estimator for short and long memory\n  processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new minimum distance estimator (MDE) for the parameters\nof short and long memory models. This bias corrected minimum distance estimator\n(BCMDE) considers a correction in the usual MDE to account for the bias of the\nsample autocorrelation function when the mean is unknown. We prove the weak\nconsistency of the BCMDE for the general fractional autoregressive moving\naverage (ARFIMA(p, d, q)) model and derive its asymptotic distribution for some\nparticular cases. Simulation studies show that the BCMDE presents a good\nperformance compared to other procedures frequently used in the literature,\nsuch as the maximum likelihood estimator, the Whittle estimator and the MDE.\nThe results also show that the BCMDE presents, in general, the smallest mean\nsquared error and is less biased than the MDE when the mean is a non-trivial\nfunction of time.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 21:59:20 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Lana", "Gustavo C.", ""], ["Franco", "Glaura C.", ""], ["Ndreca", "Sokol", ""]]}, {"id": "1809.07419", "submitter": "Peng Ding", "authors": "Jason Wu and Peng Ding", "title": "Randomization Tests for Weak Null Hypotheses in Randomized Experiments", "comments": null, "journal-ref": "Journal of the American Statistical Association 2020", "doi": "10.1080/01621459.2020.1750415", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Fisher randomization test (FRT) is appropriate for any test statistic,\nunder a sharp null hypothesis that can recover all missing potential outcomes.\nHowever, it is often sought after to test a weak null hypothesis that the\ntreatment does not affect the units on average. To use the FRT for a weak null\nhypothesis, we must address two issues. First, we need to impute the missing\npotential outcomes although the weak null hypothesis cannot determine all of\nthem. Second, we need to choose a proper test statistic. For a general weak\nnull hypothesis, we propose an approach to imputing missing potential outcomes\nunder a compatible sharp null hypothesis. Building on this imputation scheme,\nwe advocate a studentized statistic. The resulting FRT has multiple desirable\nfeatures. First, it is model-free. Second, it is finite-sample exact under the\nsharp null hypothesis that we use to impute the potential outcomes. Third, it\nconservatively controls large-sample type I errors under the weak null\nhypothesis of interest. Therefore, our FRT is agnostic to the treatment effect\nheterogeneity. We establish a unified theory for general factorial experiments.\nWe also extend it to stratified and clustered experiments.\n", "versions": [{"version": "v1", "created": "Wed, 19 Sep 2018 22:18:32 GMT"}, {"version": "v2", "created": "Wed, 24 Oct 2018 02:18:34 GMT"}, {"version": "v3", "created": "Sun, 17 Nov 2019 08:01:18 GMT"}, {"version": "v4", "created": "Tue, 17 Mar 2020 21:25:52 GMT"}, {"version": "v5", "created": "Thu, 5 Nov 2020 21:14:06 GMT"}], "update_date": "2020-11-09", "authors_parsed": [["Wu", "Jason", ""], ["Ding", "Peng", ""]]}, {"id": "1809.07441", "submitter": "Robin Dunn", "authors": "Robin Dunn, Larry Wasserman, Aaditya Ramdas", "title": "Distribution-Free Prediction Sets with Random Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of constructing distribution-free prediction sets\nwhen there are random effects. For iid data, prediction sets can be constructed\nusing the method of conformal prediction. The validity of this prediction set\nhinges on the assumption that the data are exchangeable, which is not true when\nthere are random effects. We extend the conformal method so that it is valid\nwith random effects. We develop a CDF pooling approach, a single subsampling\napproach, and a repeated subsampling approach to construct conformal prediction\nsets in unsupervised and supervised settings. We compare these approaches in\nterms of coverage and average set size. We recommend the repeated subsampling\napproach that constructs a conformal set by sampling one observation from each\ndistribution multiple times. Simulations show that this approach has the best\nbalance between coverage and average conformal set size.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 01:00:00 GMT"}, {"version": "v2", "created": "Tue, 15 Sep 2020 19:48:06 GMT"}], "update_date": "2020-09-17", "authors_parsed": [["Dunn", "Robin", ""], ["Wasserman", "Larry", ""], ["Ramdas", "Aaditya", ""]]}, {"id": "1809.07585", "submitter": "Marija Cupari\\'c", "authors": "Marija Cupari\\'c, Bojana Milo\\v{s}evi\\'c, Marko Obradovi\\'c", "title": "New $L^2$-type exponentiality tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce new consistent and scale-free goodness-of-fit tests for the\nexponential distribution based on Puri-Rubin characterization. For the\nconstruction of test statistics we employ weighted $L^2$ distance between\n$V$-empirical Laplace transforms of random variables that appear in the\ncharacterization. The resulting test statistics are degenerate V-statistics\nwith estimated parameters. We compare our tests, in terms of the Bahadur\nefficiency, to the likelihood ratio test, as well as some recent\ncharacterization based goodness-of-fit tests for the exponential distribution.\nWe also compare the powers of our tests to the powers of some recent and\nclassical exponentiality tests. In both criteria, our tests are shown to be\nstrong and outperform most of their competitors.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 12:09:18 GMT"}], "update_date": "2018-09-21", "authors_parsed": [["Cupari\u0107", "Marija", ""], ["Milo\u0161evi\u0107", "Bojana", ""], ["Obradovi\u0107", "Marko", ""]]}, {"id": "1809.07850", "submitter": "Michael Porter", "authors": "Ketong Wang and Michael D. Porter", "title": "Optimal Bayesian clustering using non-negative matrix factorization", "comments": null, "journal-ref": "Computational Statistics & Data Analysis Volume 128, December\n  2018, Pages 395-411", "doi": "10.1016/j.csda.2018.08.002", "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Bayesian model-based clustering is a widely applied procedure for discovering\ngroups of related observations in a dataset. These approaches use Bayesian\nmixture models, estimated with MCMC, which provide posterior samples of the\nmodel parameters and clustering partition. While inference on model parameters\nis well established, inference on the clustering partition is less developed. A\nnew method is developed for estimating the optimal partition from the pairwise\nposterior similarity matrix generated by a Bayesian cluster model. This\napproach uses non-negative matrix factorization (NMF) to provide a low-rank\napproximation to the similarity matrix. The factorization permits hard or soft\npartitions and is shown to perform better than several popular alternatives\nunder a variety of penalty functions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 20:52:21 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Wang", "Ketong", ""], ["Porter", "Michael D.", ""]]}, {"id": "1809.08018", "submitter": "Allan J\\'erolon", "authors": "Allan Jerolon, Laura Baglietto, Etienne Birmele, Vittorio Perduca and\n  Flora Alarcon", "title": "Causal mediation analysis in presence of multiple mediators uncausally\n  related", "comments": "47 pages, 7 Tables, 9 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mediation analysis aims at disentangling the effects of a treatment on an\noutcome through alternative causal mechanisms and has become a popular practice\nin biomedical and social science applications. The causal framework based on\ncounterfactuals is currently the standard approach to mediation, with important\nmethodological advances introduced in the literature in the last decade,\nespecially for simple mediation, that is with one mediator at the time. Among a\nvariety of alternative approaches, K. Imai et al. showed theoretical results\nand developed an R package to deal with simple mediation as well as with\nmultiple mediation involving multiple mediators conditionally independent given\nthe treatment and baseline covariates. This approach does not allow to consider\nthe often encountered situation in which an unobserved common cause induces a\nspurious correlation between the mediators. In this context, which we refer to\nas mediation with uncausally related mediators, we show that, under appropriate\nhypothesis, the natural direct and joint indirect effects are\nnon-parametrically identifiable. Moreover, we adopt the quasi-Bayesian\nalgorithm developed by Imai et al. and propose a procedure based on the\nsimulation of counterfactual distributions to estimate not only the direct and\njoint indirect effects but also the indirect effects through individual\nmediators. We study the properties of the proposed estimators through\nsimulations. As an illustration, we apply our method on a real data set from a\nlarge cohort to assess the effect of hormone replacement treatment on breast\ncancer risk through three mediators, namely dense mammographic area, nondense\narea and body mass index.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 10:17:59 GMT"}, {"version": "v2", "created": "Mon, 19 Aug 2019 12:52:09 GMT"}], "update_date": "2019-08-20", "authors_parsed": [["Jerolon", "Allan", ""], ["Baglietto", "Laura", ""], ["Birmele", "Etienne", ""], ["Perduca", "Vittorio", ""], ["Alarcon", "Flora", ""]]}, {"id": "1809.08024", "submitter": "Harry Gray", "authors": "Harry Gray, Gwena\\\"el G.R. Leday, Catalina A. Vallejos, and Sylvia\n  Richardson", "title": "Shrinkage estimation of large covariance matrices using multiple\n  shrinkage targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear shrinkage estimators of a covariance matrix --- defined by a weighted\naverage of the sample covariance matrix and a pre-specified shrinkage target\nmatrix --- are popular when analysing high-throughput molecular data. However,\ntheir performance strongly relies on an appropriate choice of target matrix.\nThis paper introduces a more flexible class of linear shrinkage estimators that\ncan accommodate multiple shrinkage target matrices, directly accounting for the\nuncertainty regarding the target choice. This is done within a conjugate\nBayesian framework, which is computationally efficient. Using both simulated\nand real data, we show that the proposed estimator is less sensitive to target\nmisspecification and can outperform state-of-the-art (nonparametric)\nsingle-target linear shrinkage estimators. Using protein expression data from\nThe Cancer Proteome Atlas we illustrate how multiple sources of prior\ninformation (obtained from more than 30 different cancer types) can be\nincorporated into the proposed multi-target linear shrinkage estimator. In\nparticular, it is shown that the target-specific weights can provide insights\ninto the differences and similarities between cancer types. Software for the\nmethod is freely available as an R-package at http://github.com/HGray384/TAS.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 10:34:31 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Gray", "Harry", ""], ["Leday", "Gwena\u00ebl G. R.", ""], ["Vallejos", "Catalina A.", ""], ["Richardson", "Sylvia", ""]]}, {"id": "1809.08035", "submitter": "Pier Luigi Conti", "authors": "Pier Luigi Conti and Alberto Di Iorio", "title": "Analytic inference in finite population framework via resampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to provide a resampling technique that allows us to\nmake inference on superpopulation parameters in finite population setting.\nUnder complex sampling designs, it is often difficult to obtain explicit\nresults about superpopulation parameters of interest, especially in terms of\nconfidence intervals and test-statistics. Computer intensive procedures, such\nas resampling, allow us to avoid this problem. To reach the above goal,\nasymptotic results about empirical processes in finite population framework are\nfirst obtained. Then, a resampling procedure is proposed, and justified via\nasymptotic considerations. Finally, the results obtained are applied to\ndifferent inferential problems and a simulation study is performed to test the\ngoodness of our proposal.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 10:56:27 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Conti", "Pier Luigi", ""], ["Di Iorio", "Alberto", ""]]}, {"id": "1809.08121", "submitter": "Martin Radloff", "authors": "Martin Radloff and Rainer Schwabe", "title": "Locally $D$-optimal Designs for a Wider Class of Non-linear Models on\n  the $k$-dimensional Ball with applications to logit and probit models", "comments": "11 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1806.00275", "journal-ref": "Statistical Papers, volume 60, pages 515-527, 2019", "doi": "10.1007/s00362-018-01078-4", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extend the results of Radloff and Schwabe (2018), which\ncould be applied for example to Poisson regression, negative binomial\nregression and proportional hazard models with censoring, to a wider class of\nnon-linear multiple regression models. This includes the binary response models\nwith logit and probit link besides other. For this class of models we derive\n(locally) $D$-optimal designs when the design region is a $k$-dimensional ball.\nFor the corresponding construction we make use of the concept of invariance and\nequivariance in the context of optimal designs as in our previous paper. In\ncontrast to the former results the designs will not necessarily be exact\ndesigns in all cases. Instead approximate designs can appear. These results can\nbe generalized to arbitrary ellipsoidal design regions.\n", "versions": [{"version": "v1", "created": "Thu, 20 Sep 2018 15:39:05 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Radloff", "Martin", ""], ["Schwabe", "Rainer", ""]]}, {"id": "1809.08246", "submitter": "Niall Jeffrey", "authors": "Niall Jeffrey and Filipe B. Abdalla", "title": "Parameter inference and model comparison using theoretical predictions\n  from noisy simulations", "comments": "9 pages, 6 figures, published by MNRAS. Changes: matches published\n  version, added Bayesian hierarchical interpretation and probabilistic\n  graphical model", "journal-ref": "Monthly Notices of the Royal Astronomical Society, Volume 490,\n  Issue 4, December 2019, Pages 5749-5756", "doi": "10.1093/mnras/stz2930", "report-no": null, "categories": "astro-ph.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When inferring unknown parameters or comparing different models, data must be\ncompared to underlying theory. Even if a model has no closed-form solution to\nderive summary statistics, it is often still possible to simulate mock data in\norder to generate theoretical predictions. For realistic simulations of noisy\ndata, this is identical to drawing realizations of the data from a likelihood\ndistribution. Though the estimated summary statistic from simulated data\nvectors may be unbiased, the estimator has variance which should be accounted\nfor. We show how to correct the likelihood in the presence of an estimated\nsummary statistic by marginalizing over the true summary statistic in the\nframework of a Bayesian hierarchical model. For Gaussian likelihoods where the\ncovariance must also be estimated from simulations, we present an alteration to\nthe Sellentin-Heavens corrected likelihood. We show that excluding the proposed\ncorrection leads to an incorrect estimate of the Bayesian evidence with JLA\ndata. The correction is highly relevant for cosmological inference that relies\non simulated data for theory (e.g. weak lensing peak statistics and simulated\npower spectra) and can reduce the number of simulations required.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 18:00:04 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 19:00:04 GMT"}], "update_date": "2019-12-20", "authors_parsed": [["Jeffrey", "Niall", ""], ["Abdalla", "Filipe B.", ""]]}, {"id": "1809.08330", "submitter": "Nicolas Verzelen", "authors": "Alexandra Carpentier and Sylvain Delattre and Etienne Roquain and\n  Nicolas Verzelen", "title": "Estimating minimum effect with outlier selection", "comments": "70 pages; 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce one-sided versions of Huber's contamination model, in which\ncorrupted samples tend to take larger values than uncorrupted ones. Two\nintertwined problems are addressed: estimation of the mean of uncorrupted\nsamples (minimum effect) and selection of corrupted samples (outliers).\nRegarding the minimum effect estimation, we derive the minimax risks and\nintroduce adaptive estimators to the unknown number of contaminations.\nInterestingly, the optimal convergence rate highly differs from that in\nclassical Huber's contamination model. Also, our analysis uncovers the effect\nof particular structural assumptions on the distribution of the contaminated\nsamples. As for the problem of selecting the outliers, we formulate the problem\nin a multiple testing framework for which the location/scaling of the null\nhypotheses are unknown. We rigorously prove how estimating the null hypothesis\nis possible while maintaining a theoretical guarantee on the amount of the\nfalsely selected outliers, both through false discovery rate (FDR) or post hoc\nbounds. As a by-product, we address a long-standing open issue on FDR control\nunder equi-correlation, which reinforces the interest of removing dependency\nwhen making multiple testing.\n", "versions": [{"version": "v1", "created": "Fri, 21 Sep 2018 22:01:45 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Carpentier", "Alexandra", ""], ["Delattre", "Sylvain", ""], ["Roquain", "Etienne", ""], ["Verzelen", "Nicolas", ""]]}, {"id": "1809.08449", "submitter": "Erik van Zwet", "authors": "Erik van Zwet", "title": "A default prior for regression coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When the sample size is not too small, M-estimators of regression\ncoefficients are approximately normal and unbiased. This leads to the familiar\nfrequentist inference in terms of normality-based confidence intervals and\np-values. From a Bayesian perspective, use of the (improper) uniform prior\nyields matching results in the sense that posterior quantiles agree with\none-sided confidence bounds. For this, and various other reasons, the uniform\nprior is often considered objective or non-informative. In spite of this, we\nargue that the uniform prior is not suitable as a default prior for inference\nabout a regression coefficient in the context of the bio-medical and social\nsciences. We propose that a more suitable default choice is the normal\ndistribution with mean zero and standard deviation equal to the standard error\nof the M-estimator. We base this recommendation on two arguments. First, we\nshow that this prior is non-informative for inference about the sign of the\nregression coefficient. Secondly, we show that this prior agrees well with a\nmeta-analysis of 50 articles from the MEDLINE database.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 15:54:44 GMT"}, {"version": "v2", "created": "Thu, 18 Oct 2018 13:07:36 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["van Zwet", "Erik", ""]]}, {"id": "1809.08503", "submitter": "Guosheng Yin", "authors": "Haolun Shi, Guosheng Yin", "title": "P-value: A Bless or A Curse for Evidence-Based Studies?", "comments": null, "journal-ref": "Reconnecting p-value and posterior probability under one- and\n  two-sided tests. The American Statistician 2020", "doi": "10.1080/00031305.2020.1717621", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a convention, p-value is often computed in frequentist hypothesis testing\nand compared with the nominal significance level of 0.05 to determine whether\nor not to reject the null hypothesis. The smaller the p-value, the more\nsignificant the statistical test. We consider both one-sided and two-sided\nhypotheses in the composite hypothesis setting. For one-sided hypothesis tests,\nwe establish the equivalence of p-value and the Bayesian posterior probability\nof the null hypothesis, which renders p-value an explicit interpretation of how\nstrong the data support the null. For two-sided hypothesis tests of a point\nnull, we recast the problem as a combination of two one-sided hypotheses alone\nthe opposite directions and put forward the notion of a two-sided posterior\nprobability, which also has an equivalent relationship with the (two-sided)\np-value. Extensive simulation studies are conducted to demonstrate the Bayesian\nposterior probability interpretation for the p-value. Contrary to common\ncriticisms of the use of p-value in evidence-based studies, we justify its\nutility and reclaim its importance from the Bayesian perspective, and recommend\nthe continual use of p-value in hypothesis testing. After all, p-value is not\nall that bad.\n", "versions": [{"version": "v1", "created": "Sat, 22 Sep 2018 23:45:57 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Shi", "Haolun", ""], ["Yin", "Guosheng", ""]]}, {"id": "1809.08521", "submitter": "Antonio Linero", "authors": "Antonio R. Linero, Debajyoti Sinha, and Stuart R. Lipsitz", "title": "Semiparametric Mixed-Scale Models Using Shared Bayesian Forests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates the advantages of sharing information about unknown\nfeatures of covariates across multiple model components in various\nnonparametric regression problems including multivariate, heteroscedastic, and\nsemi-continuous responses. In this paper, we present methodology which allows\nfor information to be shared nonparametrically across various model components\nusing Bayesian sum-of-tree models. Our simulation results demonstrate that\nsharing of information across related model components is often very\nbeneficial, particularly in sparse high-dimensional problems in which variable\nselection must be conducted. We illustrate our methodology by analyzing medical\nexpenditure data from the Medical Expenditure Panel Survey (MEPS). To\nfacilitate the Bayesian nonparametric regression analysis, we develop two novel\nmodels for analyzing the MEPS data using Bayesian additive regression trees - a\nheteroskedastic log-normal hurdle model with a\n\"shrink-towards-homoskedasticity\" prior, and a gamma hurdle model.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 03:13:55 GMT"}, {"version": "v2", "created": "Thu, 25 Apr 2019 17:50:39 GMT"}, {"version": "v3", "created": "Fri, 24 May 2019 21:50:43 GMT"}, {"version": "v4", "created": "Sun, 9 Jun 2019 23:26:53 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Linero", "Antonio R.", ""], ["Sinha", "Debajyoti", ""], ["Lipsitz", "Stuart R.", ""]]}, {"id": "1809.08524", "submitter": "Antonio Linero", "authors": "Junliang Du, Antonio R. Linero", "title": "Interaction Detection with Bayesian Decision Tree Ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods based on Bayesian decision tree ensembles have proven valuable in\nconstructing high-quality predictions, and are particularly attractive in\ncertain settings because they encourage low-order interaction effects. Despite\nadapting to the presence of low-order interactions for prediction purpose, we\nshow that Bayesian decision tree ensembles are generally anti-conservative for\nthe purpose of conducting interaction detection. We address this problem by\nintroducing Dirichlet process forests (DP-Forests), which leverage the presence\nof low-order interactions by clustering the trees so that trees within the same\ncluster focus on detecting a specific interaction. We show on both simulated\nand benchmark data that DP-Forests perform well relative to existing\ninteraction detection techniques for detecting low-order interactions,\nattaining very low false-positive and false-negative rates while maintaining\nthe same performance for prediction using a comparable computational budget.\n", "versions": [{"version": "v1", "created": "Sun, 23 Sep 2018 03:45:13 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Du", "Junliang", ""], ["Linero", "Antonio R.", ""]]}, {"id": "1809.08746", "submitter": "Weining Shen", "authors": "Wei Hu, Weining Shen, Hua Zhou, and Dehan Kong", "title": "Matrix Linear Discriminant Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel linear discriminant analysis approach for the\nclassification of high-dimensional matrix-valued data that commonly arises from\nimaging studies. Motivated by the equivalence of the conventional linear\ndiscriminant analysis and the ordinary least squares, we consider an efficient\nnuclear norm penalized regression that encourages a low-rank structure.\nTheoretical properties including a non-asymptotic risk bound and a rank\nconsistency result are established. Simulation studies and an application to\nelectroencephalography data show the superior performance of the proposed\nmethod over the existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 04:07:06 GMT"}, {"version": "v2", "created": "Fri, 3 May 2019 16:50:25 GMT"}], "update_date": "2019-05-06", "authors_parsed": [["Hu", "Wei", ""], ["Shen", "Weining", ""], ["Zhou", "Hua", ""], ["Kong", "Dehan", ""]]}, {"id": "1809.08771", "submitter": "{\\L}ukasz Kidzi\\'nski", "authors": "{\\L}ukasz Kidzi\\'nski, Trevor Hastie", "title": "Longitudinal data analysis using matrix completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In clinical practice and biomedical research, measurements are often\ncollected sparsely and irregularly in time while the data acquisition is\nexpensive and inconvenient. Examples include measurements of spine bone mineral\ndensity, cancer growth through mammography or biopsy, a progression of defect\nof vision, or assessment of gait in patients with neurological disorders. Since\nthe data collection is often costly and inconvenient, estimation of progression\nfrom sparse observations is of great interest for practitioners.\n  From the statistical standpoint, such data is often analyzed in the context\nof a mixed-effect model where time is treated as both random and fixed effect.\nAlternatively, researchers analyze Gaussian processes or functional data where\nobservations are assumed to be drawn from a certain distribution of processes.\nThese models are flexible but rely on probabilistic assumptions and require\nvery careful implementation.\n  In this study, we propose an alternative elementary framework for analyzing\nlongitudinal data, relying on matrix completion. Our method yields point\nestimates of progression curves by iterative application of the SVD. Our\nframework covers multivariate longitudinal data, regression and can be easily\nextended to other settings.\n  We apply our methods to understand trends of progression of motor impairment\nin children with Cerebral Palsy. Our model approximates individual progression\ncurves and explains 30% of the variability. Low-rank representation of\nprogression trends enables discovering that subtypes of Cerebral Palsy exhibit\ndifferent progression trends.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 06:18:13 GMT"}], "update_date": "2018-09-25", "authors_parsed": [["Kidzi\u0144ski", "\u0141ukasz", ""], ["Hastie", "Trevor", ""]]}, {"id": "1809.08872", "submitter": "Guillaume Chauvet", "authors": "Brigitte Gelein (ENSAI, IRMAR), Guillaume Chauvet (IRMAR)", "title": "Preserving the distribution function in surveys in case of imputation\n  for zero inflated data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item non-response in surveys is usually handled by single imputation, whose\nmain objective is to reduce the non-response bias. Imputation methods need to\nbe adapted to the study variable. For instance, in business surveys, the\ninterest variables often contain a large number of zeros. Motivated by a\nmixture regression model, we propose two imputation procedures for such data\nand study their statistical properties. We show that these procedures preserve\nthe distribution function if the imputation model is well specified. The\nresults of a simulation study illustrate the good performance of the proposed\nmethods in terms of bias and mean square error.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 12:28:49 GMT"}, {"version": "v2", "created": "Wed, 16 Oct 2019 08:47:41 GMT"}], "update_date": "2019-10-17", "authors_parsed": [["Gelein", "Brigitte", "", "ENSAI, IRMAR"], ["Chauvet", "Guillaume", "", "IRMAR"]]}, {"id": "1809.08889", "submitter": "Etienne Wijler", "authors": "Stephan Smeekes and Etienne Wijler", "title": "An Automated Approach Towards Sparse Single-Equation Cointegration\n  Modelling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the Single-equation Penalized Error Correction\nSelector (SPECS) as an automated estimation procedure for dynamic\nsingle-equation models with a large number of potentially (co)integrated\nvariables. By extending the classical single-equation error correction model,\nSPECS enables the researcher to model large cointegrated datasets without\nnecessitating any form of pre-testing for the order of integration or\ncointegrating rank. Under an asymptotic regime in which both the number of\nparameters and time series observations jointly diverge to infinity, we show\nthat SPECS is able to consistently estimate an appropriate linear combination\nof the cointegrating vectors that may occur in the underlying DGP. In addition,\nSPECS is shown to enable the correct recovery of sparsity patterns in the\nparameter space and to posses the same limiting distribution as the OLS oracle\nprocedure. A simulation study shows strong selective capabilities, as well as\nsuperior predictive performance in the context of nowcasting compared to\nhigh-dimensional models that ignore cointegration. An empirical application to\nnowcasting Dutch unemployment rates using Google Trends confirms the strong\npractical performance of our procedure.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 13:05:11 GMT"}, {"version": "v2", "created": "Sat, 25 Jan 2020 16:36:08 GMT"}, {"version": "v3", "created": "Wed, 22 Jul 2020 12:45:44 GMT"}], "update_date": "2020-07-23", "authors_parsed": [["Smeekes", "Stephan", ""], ["Wijler", "Etienne", ""]]}, {"id": "1809.09042", "submitter": "Marco Oesting", "authors": "Marco Oesting and Kirstin Strokorb", "title": "A comparative tour through the simulation algorithms for max-stable\n  processes", "comments": "28 pages, 3 tables, 9 figures; to appear in Statistical Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being the max-analogue of $\\alpha$-stable stochastic processes, max-stable\nprocesses form one of the fundamental classes of stochastic processes. With the\narrival of sufficient computational capabilities, they have become a benchmark\nin the analysis of spatio-temporal extreme events. Simulation is often a\nnecessary part of inference of certain characteristics, in particular for\nfuture spatial risk assessment. In this article we give an overview over\nexisting procedures for this task, put them into perspective of one another and\nuse some new theoretical results to make comparisons with respect to their\nproperties.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 16:41:41 GMT"}, {"version": "v2", "created": "Tue, 26 Feb 2019 09:19:57 GMT"}, {"version": "v3", "created": "Fri, 15 Jan 2021 08:57:05 GMT"}], "update_date": "2021-01-18", "authors_parsed": [["Oesting", "Marco", ""], ["Strokorb", "Kirstin", ""]]}, {"id": "1809.09159", "submitter": "Kyle Burris", "authors": "Kyle Burris and Peter Hoff", "title": "Exact adaptive confidence intervals for small areas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the analysis of survey data it is of interest to estimate and quantify\nuncertainty about means or totals for each of several non-overlapping\nsubpopulations, or areas. When the sample size for a given area is small,\nstandard confidence intervals based on data only from that area can be\nunacceptably wide. In order to reduce interval width, practitioners often\nutilize multilevel models in order to borrow information across areas,\nresulting in intervals centered around shrinkage estimators. However, such\nintervals only have the nominal coverage rate on average across areas under the\nassumed model for across-area heterogeneity. The coverage rate for a given area\ndepends on the actual value of the area mean, and can be nearly zero for areas\nwith means that are far from the across-group average. As such, the use of\nuncertainty intervals centered around shrinkage estimators are inappropriate\nwhen area-specific coverage rates are desired. In this article, we propose an\nalternative confidence interval procedure for area means and totals under\nnormally distributed sampling errors. This procedure not only has constant\n$1-\\alpha$ frequentist coverage for all values of the target quantity, but also\nuses auxiliary information to borrow information across areas. Because of this,\nthe corresponding intervals have shorter expected lengths than standard\nconfidence intervals centered on the unbiased direct estimator. Importantly,\nthe coverage of the procedure does not depend on the assumed model for\nacross-area heterogeneity. Rather, improvements to the model for across-area\nheterogeneity result in reduced expected interval width.\n", "versions": [{"version": "v1", "created": "Mon, 24 Sep 2018 18:45:28 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Burris", "Kyle", ""], ["Hoff", "Peter", ""]]}, {"id": "1809.09337", "submitter": "Ruocheng Guo", "authors": "Ruocheng Guo, Lu Cheng, Jundong Li, P. Richard Hahn, Huan Liu", "title": "A Survey of Learning Causality with Data: Problems and Methods", "comments": "35 pages, accepted by ACM CSUR", "journal-ref": null, "doi": "10.1145/3397269", "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the question of how convenient access to copious data\nimpacts our ability to learn causal effects and relations. In what ways is\nlearning causality in the era of big data different from -- or the same as --\nthe traditional one? To answer this question, this survey provides a\ncomprehensive and structured review of both traditional and frontier methods in\nlearning causality and relations along with the connections between causality\nand machine learning. This work points out on a case-by-case basis how big data\nfacilitates, complicates, or motivates each approach.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 06:33:18 GMT"}, {"version": "v2", "created": "Wed, 26 Sep 2018 01:43:19 GMT"}, {"version": "v3", "created": "Fri, 19 Apr 2019 16:38:36 GMT"}, {"version": "v4", "created": "Tue, 5 May 2020 04:06:27 GMT"}], "update_date": "2020-05-06", "authors_parsed": [["Guo", "Ruocheng", ""], ["Cheng", "Lu", ""], ["Li", "Jundong", ""], ["Hahn", "P. Richard", ""], ["Liu", "Huan", ""]]}, {"id": "1809.09445", "submitter": "Yousra El-Bachir", "authors": "Yousra El-Bachir and Anthony C. Davison", "title": "Fast Automatic Smoothing for Generalized Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple generalized additive models (GAMs) are a type of distributional\nregression wherein parameters of probability distributions depend on predictors\nthrough smooth functions, with selection of the degree of smoothness via $L_2$\nregularization. Multiple GAMs allow finer statistical inference by\nincorporating explanatory information in any or all of the parameters of the\ndistribution. Owing to their nonlinearity, flexibility and interpretability,\nGAMs are widely used, but reliable and fast methods for automatic smoothing in\nlarge datasets are still lacking, despite recent advances. We develop a general\nmethodology for automatically learning the optimal degree of $L_2$\nregularization for multiple GAMs using an empirical Bayes approach. The smooth\nfunctions are penalized by different amounts, which are learned simultaneously\nby maximization of a marginal likelihood through an approximate\nexpectation-maximization algorithm that involves a double Laplace approximation\nat the E-step, and leads to an efficient M-step. Empirical analysis shows that\nthe resulting algorithm is numerically stable, faster than all existing methods\nand achieves state-of-the-art accuracy. For illustration, we apply it to an\nimportant and challenging problem in the analysis of extremal data.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 12:59:01 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["El-Bachir", "Yousra", ""], ["Davison", "Anthony C.", ""]]}, {"id": "1809.09448", "submitter": "Charles Fontaine PhD", "authors": "Charles Fontaine and Ron D. Frostig and Hernando Ombao", "title": "Modeling Dependence via Copula of Functionals of Fourier Coefficients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to develop a measure for characterizing complex\ndependence between stationary time series that cannot be captured by\ntraditional measures such as correlation and coherence. Our approach is to use\ncopula models of functionals of the Fourier coefficients which is a\ngeneralization of coherence. Here, we use standard parametric copula models\nwith a single parameter both from elliptical and Archimedean families. Our\napproach is to analyze changes in local field potentials in the rat cortex\nprior to and immediately following the onset of stroke. We present the\nnecessary theoretical background, the multivariate models and an illustration\nof our methodology on these local field potential data. Simulations with\nnon-linear dependent data show information that were missed by not taking into\naccount dependence on specific frequencies. Moreover, these simulations\ndemonstrate how our proposed method captures more complex non-linear dependence\nbetween time series. Finally, we illustrate our copula-based approach in the\nanalysis of local field potentials of rats.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 12:59:32 GMT"}], "update_date": "2018-09-26", "authors_parsed": [["Fontaine", "Charles", ""], ["Frostig", "Ron D.", ""], ["Ombao", "Hernando", ""]]}, {"id": "1809.09527", "submitter": "Tymon Sloczynski", "authors": "Arun Advani, Toru Kitagawa and Tymon S{\\l}oczy\\'nski", "title": "Mostly Harmless Simulations? Using Monte Carlo Studies for Estimator\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM stat.ME stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider two recent suggestions for how to perform an empirically\nmotivated Monte Carlo study to help select a treatment effect estimator under\nunconfoundedness. We show theoretically that neither is likely to be\ninformative except under restrictive conditions that are unlikely to be\nsatisfied in many contexts. To test empirical relevance, we also apply the\napproaches to a real-world setting where estimator performance is known. Both\napproaches are worse than random at selecting estimators which minimise\nabsolute bias. They are better when selecting estimators that minimise mean\nsquared error. However, using a simple bootstrap is at least as good and often\nbetter. For now researchers would be best advised to use a range of estimators\nand compare estimates for robustness.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 14:54:11 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 15:32:29 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Advani", "Arun", ""], ["Kitagawa", "Toru", ""], ["S\u0142oczy\u0144ski", "Tymon", ""]]}, {"id": "1809.09561", "submitter": "Johan Ugander", "authors": "Alex Chin, Dean Eckles, Johan Ugander", "title": "Evaluating stochastic seeding strategies in networks", "comments": "63 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When trying to maximize the adoption of a behavior in a population connected\nby a social network, it is common to strategize about where in the network to\nseed the behavior, often with an element of randomness. Selecting seeds\nuniformly at random is a basic but compelling strategy in that it distributes\nseeds broadly throughout the network. A more sophisticated stochastic strategy,\none-hop targeting, is to select random network neighbors of random individuals;\nthis exploits a version of the friendship paradox, whereby the friend of a\nrandom individual is expected to have more friends than a random individual,\nwith the hope that seeding a behavior at more connected individuals leads to\nmore adoption. Many seeding strategies have been proposed, but empirical\nevaluations have demanded large field experiments designed specifically for\nthis purpose and have yielded relatively imprecise comparisons of strategies.\nHere we show how stochastic seeding strategies can be evaluated more\nefficiently in such experiments, how they can be evaluated \"off-policy\" using\nexisting data arising from experiments designed for other purposes, and how to\ndesign more efficient experiments. In particular, we consider contrasts between\nstochastic seeding strategies and analyze nonparametric estimators adapted from\npolicy evaluation and importance sampling. We use simulations on real networks\nto show that the proposed estimators and designs can increase precision while\nyielding valid inference. We then apply our proposed estimators to two field\nexperiments, one that assigned households to an intensive marketing\nintervention and one that assigned students to an anti-bullying intervention.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 15:48:42 GMT"}, {"version": "v2", "created": "Thu, 10 Jan 2019 21:20:56 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2020 22:43:47 GMT"}], "update_date": "2020-06-22", "authors_parsed": [["Chin", "Alex", ""], ["Eckles", "Dean", ""], ["Ugander", "Johan", ""]]}, {"id": "1809.09602", "submitter": "Daren Wang", "authors": "Daren Wang, Yi Yu, Alessandro Rinaldo", "title": "Optimal Change Point Detection and Localization in Sparse Dynamic\n  Networks", "comments": "49 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of change point localization in dynamic networks models.\nWe assume that we observe a sequence of independent adjacency matrices of the\nsame size, each corresponding to a realization of an unknown inhomogeneous\nBernoulli model. The underlying distribution of the adjacency matrices are\npiecewise constant, and may change over a subset of the time points, called\nchange points. We are concerned with recovering the unknown number and\npositions of the change points. In our model setting we allow for all the model\nparameters to change with the total number of time points, including the\nnetwork size, the minimal spacing between consecutive change points, the\nmagnitude of the smallest change and the degree of sparsity of the networks. We\nfirst identify a region of impossibility in the space of the model parameters\nsuch that no change point estimator is provably consistent if the data are\ngenerated according to parameters falling in that region. We propose a\ncomputationally-simple algorithm for network change point localization, called\nNetwork Binary Segmentation, that relies on weighted averages of the adjacency\nmatrices. We show that Network Binary Segmentation is consistent over a range\nof the model parameters that nearly cover the complement of the impossibility\nregion, thus demonstrating the existence of a phase transition for the problem\nat hand. Next, we devise a more sophisticated algorithm based on singular value\nthresholding, called Local Refinement, that delivers more accurate estimates of\nthe change point locations. Under appropriate conditions, Local Refinement\nguarantees a minimax optimal rate for network change point localization while\nremaining computationally feasible.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 17:32:36 GMT"}, {"version": "v2", "created": "Tue, 20 Oct 2020 18:40:42 GMT"}], "update_date": "2020-10-22", "authors_parsed": [["Wang", "Daren", ""], ["Yu", "Yi", ""], ["Rinaldo", "Alessandro", ""]]}, {"id": "1809.09740", "submitter": "Wei Wang", "authors": "Wei Wang, Nan Lin, Jordan D. Oberhaus, Michael S. Avidan", "title": "Assessing method agreement for paired repeated binary measurements\n  administered by multiple raters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Method comparison studies are essential for development in medical and\nclinical fields. These studies often compare a cheaper, faster, or less\ninvasive measuring method with a widely used one to see if they have sufficient\nagreement for interchangeable use. In the clinical and medical context, the\nresponse measurement is usually impacted not only by the measuring method but\nby the rater as well. This paper proposes a model-based approach to assess\nagreement of two measuring methods for paired repeated binary measurements\nunder the scenario when the agreement between two measuring methods and the\nagreement among raters are required to be studied in a unified framework. Based\nupon the generalized linear mixed models (GLMM), the decision on the adequacy\nof interchangeable use is made by testing the equality of fixed effects of\nmethods. Approaches for assessing method agreement, such as the Bland-Altman\ndiagram and Cohen's kappa, are also developed for repeated binary measurements\nbased upon the latent variables in GLMMs. We assess our novel model-based\napproach by simulation studies and a real clinical research application, in\nwhich patients are evaluated repeatedly for delirium with two validated\nscreening methods: the Confusion Assessment Method and the 3-Minute Diagnostic\nInterview for Confusion Assessment Method. Both the simulation studies and the\nreal data analyses demonstrate that our new approach can effectively assess\nmethod agreement.\n", "versions": [{"version": "v1", "created": "Tue, 25 Sep 2018 21:58:55 GMT"}, {"version": "v2", "created": "Tue, 25 Jun 2019 18:22:56 GMT"}], "update_date": "2019-06-27", "authors_parsed": [["Wang", "Wei", ""], ["Lin", "Nan", ""], ["Oberhaus", "Jordan D.", ""], ["Avidan", "Michael S.", ""]]}, {"id": "1809.09793", "submitter": "Xin Dang", "authors": "Xin Dang, Dao Nguyen, Yixin Chen and Junying Zhang", "title": "A new Gini correlation between quantitative and qualitative variables", "comments": "24 + 3 Pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new Gini correlation to measure dependence between a categorical\nand numerical variables. Analogous to Pearson $R^2$ in ANOVA model, the Gini\ncorrelation is interpreted as the ratio of the between-group variation and the\ntotal variation, but it characterizes independence (zero Gini correlation\nmutually implies independence). Closely related to the distance correlation,\nthe Gini correlation is of simple formulation by considering the nature of\ncategorical variable. As a result, the proposed Gini correlation has a lower\ncomputational cost than the distance correlation and is more straightforward to\nperform inference. Simulation and real applications are conducted to\ndemonstrate the advantages.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 03:44:23 GMT"}, {"version": "v2", "created": "Tue, 9 Jul 2019 12:19:57 GMT"}], "update_date": "2019-07-10", "authors_parsed": [["Dang", "Xin", ""], ["Nguyen", "Dao", ""], ["Chen", "Yixin", ""], ["Zhang", "Junying", ""]]}, {"id": "1809.09881", "submitter": "Almond St\\\"ocker", "authors": "Almond St\\\"ocker and Sarah Brockhaus and Sophia Schaffer and Benedikt\n  von Bronk and Madeleine Opitz and Sonja Greven", "title": "Boosting Functional Response Models for Location, Scale and Shape with\n  an Application to Bacterial Competition", "comments": "bootstrap confidence interval type uncertainty bounds added; minor\n  changes in formulations", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Generalized Additive Models for Location, Scale, and Shape (GAMLSS)\nto regression with functional response. This allows us to simultaneously model\npoint-wise mean curves, variances and other distributional parameters of the\nresponse in dependence of various scalar and functional covariate effects. In\naddition, the scope of distributions is extended beyond exponential families.\nThe model is fitted via gradient boosting, which offers inherent model\nselection and is shown to be suitable for both complex model structures and\nhighly auto-correlated response curves. This enables us to analyze bacterial\ngrowth in \\textit{Escherichia coli} in a complex interaction scenario,\nfruitfully extending usual growth models.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 09:56:22 GMT"}, {"version": "v2", "created": "Thu, 24 Jan 2019 16:54:35 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["St\u00f6cker", "Almond", ""], ["Brockhaus", "Sarah", ""], ["Schaffer", "Sophia", ""], ["von Bronk", "Benedikt", ""], ["Opitz", "Madeleine", ""], ["Greven", "Sonja", ""]]}, {"id": "1809.10139", "submitter": "Soudabeh Barghi", "authors": "Soudabeh Barghi, Lalet Scaria, Ali Salari, Tristan Glatard", "title": "Predicting computational reproducibility of data analysis pipelines in\n  large population studies using collaborative filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating the computational reproducibility of data analysis pipelines has\nbecome a critical issue. It is, however, a cumbersome process for analyses that\ninvolve data from large populations of subjects, due to their computational and\nstorage requirements. We present a method to predict the computational\nreproducibility of data analysis pipelines in large population studies. We\nformulate the problem as a collaborative filtering process, with constraints on\nthe construction of the training set. We propose 6 different strategies to\nbuild the training set, which we evaluate on 2 datasets, a synthetic one\nmodeling a population with a growing number of subject types, and a real one\nobtained with neuroinformatics pipelines. Results show that one sampling\nmethod, \"Random File Numbers (Uniform)\" is able to predict computational\nreproducibility with a good accuracy. We also analyze the relevance of\nincluding file and subject biases in the collaborative filtering model. We\nconclude that the proposed method is able to speedup reproducibility\nevaluations substantially, with a reduced accuracy loss.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 20:24:46 GMT"}], "update_date": "2018-09-28", "authors_parsed": [["Barghi", "Soudabeh", ""], ["Scaria", "Lalet", ""], ["Salari", "Ali", ""], ["Glatard", "Tristan", ""]]}, {"id": "1809.10227", "submitter": "Toni Karvonen", "authors": "Toni Karvonen, Simo S\\\"arkk\\\"a, Chris. J. Oates", "title": "Symmetry Exploits for Bayesian Cubature Methods", "comments": "Accepted for publication in Statistics and Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.NA stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian cubature provides a flexible framework for numerical integration, in\nwhich a priori knowledge on the integrand can be encoded and exploited. This\nadditional flexibility, compared to many classical cubature methods, comes at a\ncomputational cost which is cubic in the number of evaluations of the\nintegrand. It has been recently observed that fully symmetric point sets can be\nexploited in order to reduce - in some cases substantially - the computational\ncost of the standard Bayesian cubature method. This work identifies several\nadditional symmetry exploits within the Bayesian cubature framework. In\nparticular, we go beyond earlier work in considering non-symmetric measures\nand, in addition to the standard Bayesian cubature method, present exploits for\nthe Bayes-Sard cubature method and the multi-output Bayesian cubature method.\n", "versions": [{"version": "v1", "created": "Wed, 26 Sep 2018 20:47:40 GMT"}, {"version": "v2", "created": "Sat, 26 Jan 2019 21:27:28 GMT"}], "update_date": "2019-01-29", "authors_parsed": [["Karvonen", "Toni", ""], ["S\u00e4rkk\u00e4", "Simo", ""], ["Oates", "Chris. J.", ""]]}, {"id": "1809.10493", "submitter": "Oscar Claveria", "authors": "Oscar Claveria", "title": "A new metric of consensus for Likert scales", "comments": "11 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": "IREA Working Papers 2018/21", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we present a metric of consensus for Likert scales. The measure\ngives the level of agreement as the percentage of consensus among respondents.\nThe proposed framework allows to design a positional indicator that gives the\ndegree of agreement for each item independently of the number of reply options.\nIn order to assess the performance of the proposed metric of consensus, in an\niterated one-period ahead forecasting experiment we test whether the inclusion\nof the degree of agreement in expectations regarding the evolution of\nunemployment improves out-of-sample forecast accuracy in eight European\ncountries. We find evidence that the degree of agreement among consumers\ncontains useful information to predict unemployment rates. These results show\nthe usefulness of consensus-based metrics to track the evolution of economic\nvariables.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 12:37:08 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 14:27:50 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Claveria", "Oscar", ""]]}, {"id": "1809.10632", "submitter": "Matteo Fasiolo", "authors": "Matteo Fasiolo, Rapha\\\"el Nedellec, Yannig Goude, Simon N. Wood", "title": "Scalable visualisation methods for modern Generalized Additive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last two decades the growth of computational resources has made it\npossible to handle Generalized Additive Models (GAMs) that formerly were too\ncostly for serious applications. However, the growth in model complexity has\nnot been matched by improved visualisations for model development and results\npresentation. Motivated by an industrial application in electricity load\nforecasting, we identify the areas where the lack of modern visualisation tools\nfor GAMs is particularly severe, and we address the shortcomings of existing\nmethods by proposing a set of visual tools that a) are fast enough for\ninteractive use, b) exploit the additive structure of GAMs, c) scale to large\ndata sets and d) can be used in conjunction with a wide range of response\ndistributions. All the new visual methods proposed in this work are implemented\nby the mgcViz R package, which can be found on the Comprehensive R Archive\nNetwork.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 16:51:33 GMT"}, {"version": "v2", "created": "Thu, 9 May 2019 13:57:27 GMT"}], "update_date": "2019-05-10", "authors_parsed": [["Fasiolo", "Matteo", ""], ["Nedellec", "Rapha\u00ebl", ""], ["Goude", "Yannig", ""], ["Wood", "Simon N.", ""]]}, {"id": "1809.10652", "submitter": "Abhishek Chakrabortty", "authors": "Abhishek Chakrabortty, Preetam Nandy and Hongzhe Li", "title": "Inference for Individual Mediation Effects and Interventional Effects in\n  Sparse High-Dimensional Causal Graphical Models", "comments": "Revised version; 50 pages, 6 tables, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying intermediate variables (or mediators)\nthat regulate the effect of a treatment on a response variable. While there has\nbeen significant research on this classical topic, little work has been done\nwhen the set of potential mediators is high-dimensional (HD). A further\ncomplication arises when these mediators are interrelated (with unknown\ndependencies). In particular, we assume that the causal structure of the\ntreatment, the confounders, the potential mediators and the response is a\n(possibly unknown) directed acyclic graph (DAG). HD DAG models have previously\nbeen used for the estimation of causal effects from observational data. In\nparticular, methods called IDA and joint-IDA have been developed for estimating\nthe effects of single and multiple simultaneous interventions, respectively. In\nthis paper, we propose an IDA-type method called MIDA for estimating so-called\nindividual mediation effects from HD observational data. Although IDA and\njoint-IDA estimators have been shown to be consistent in certain sparse HD\nsettings, their asymptotic properties such as convergence in distribution and\ninferential tools in such settings have remained unknown. In this paper, we\nprove HD consistency of MIDA for linear structural equation models with\nsub-Gaussian errors. More importantly, we derive distributional convergence\nresults for MIDA in similar HD settings, which are applicable to IDA and\njoint-IDA estimators as well. To our knowledge, these are the first such\ndistributional convergence results facilitating inference for IDA-type\nestimators. These are built on our novel theoretical results regarding uniform\nbounds for linear regression estimators over varying subsets of HD covariates\nwhich may be of independent interest. Finally, we empirically validate our\nasymptotic theory for MIDA and demonstrate its usefulness via simulations and a\nreal data application.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 17:28:07 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 07:21:30 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 07:27:21 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Chakrabortty", "Abhishek", ""], ["Nandy", "Preetam", ""], ["Li", "Hongzhe", ""]]}, {"id": "1809.10765", "submitter": "Ying Liu", "authors": "Ying Liu and Cheng Zheng", "title": "Auto-Encoding Knockoff Generator for FDR Controlled Variable Selection", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new statistical procedure (Model-X \\cite{candes2018}) has provided a way to\nidentify important factors using any supervised learning method controlling for\nFDR. This line of research has shown great potential to expand the horizon of\nmachine learning methods beyond the task of prediction, to serve the broader\nneeds in scientific researches for interpretable findings. However, the lack of\na practical and flexible method to generate knockoffs remains the major\nobstacle for wide application of Model-X procedure. This paper fills in the gap\nby proposing a model-free knockoff generator which approximates the correlation\nstructure between features through latent variable representation. We\ndemonstrate our proposed method can achieve FDR control and better power than\ntwo existing methods in various simulated settings and a real data example for\nfinding mutations associated with drug resistance in HIV-1 patients.\n", "versions": [{"version": "v1", "created": "Thu, 27 Sep 2018 21:05:19 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Liu", "Ying", ""], ["Zheng", "Cheng", ""]]}, {"id": "1809.10925", "submitter": "Stanislav Nagy", "authors": "Stanislav Nagy, Carsten Schuett, Elisabeth M. Werner", "title": "Data depth and floating body", "comments": null, "journal-ref": null, "doi": "10.1214/19-SS123", "report-no": null, "categories": "math.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Little known relations of the renown concept of the halfspace depth for\nmultivariate data with notions from convex and affine geometry are discussed.\nHalfspace depth may be regarded as a measure of symmetry for random vectors. As\nsuch, the depth stands as a generalization of a measure of symmetry for convex\nsets, well studied in geometry. Under a mild assumption, the upper level sets\nof the halfspace depth coincide with the convex floating bodies used in the\ndefinition of the affine surface area for convex bodies in Euclidean spaces.\nThese connections enable us to partially resolve some persistent open problems\nregarding theoretical properties of the depth.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 09:11:55 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["Nagy", "Stanislav", ""], ["Schuett", "Carsten", ""], ["Werner", "Elisabeth M.", ""]]}, {"id": "1809.11052", "submitter": "Aleksejus Kononovicius dr.", "authors": "Mark Levene, Aleksejus Kononovicius", "title": "Empirical Survival Jensen-Shannon Divergence as a Goodness-of-Fit\n  Measure for Maximum Likelihood Estimation and Curve Fitting", "comments": "20 pages, 1 figure, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME physics.data-an q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The coefficient of determination, known as $R^2$, is commonly used as a\ngoodness-of-fit criterion for fitting linear models. $R^2$ is somewhat\ncontroversial when fitting nonlinear models, although it may be generalised on\na case-by-case basis to deal with specific models such as the logistic model.\nAssume we are fitting a parametric distribution to a data set using, say, the\nmaximum likelihood estimation method. A general approach to measure the\ngoodness-of-fit of the fitted parameters, which is advocated herein, is to use\na nonparametric measure for comparison between the empirical distribution,\ncomprising the raw data, and the fitted model. In particular, for this purpose\nwe put forward the Survival Jensen-Shannon divergence ($SJS$) and its empirical\ncounterpart (${\\cal E}SJS$) as a metric which is bounded, and is a natural\ngeneralisation of the Jensen-Shannon divergence. We demonstrate, via a\nstraightforward procedure making use of the ${\\cal E}SJS$, that it can be used\nas part of maximum likelihood estimation or curve fitting as a measure of\ngoodness-of-fit, including the construction of a confidence interval for the\nfitted parametric distribution. Furthermore, we show the validity of the\nproposed method with simulated data, and three empirical data sets.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:19:07 GMT"}, {"version": "v2", "created": "Fri, 5 Oct 2018 16:29:40 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2018 13:57:14 GMT"}, {"version": "v4", "created": "Wed, 5 Dec 2018 11:01:21 GMT"}, {"version": "v5", "created": "Tue, 4 Jun 2019 08:56:43 GMT"}], "update_date": "2019-06-05", "authors_parsed": [["Levene", "Mark", ""], ["Kononovicius", "Aleksejus", ""]]}, {"id": "1809.11064", "submitter": "Eufrasio Lima Neto Mr.", "authors": "Eufr\\'asio de A. Lima Neto, Alu\\'isio de S. Pinheiro and Adenice G. O.\n  Ferreira", "title": "On wavelets to select the parametric form of a regression model", "comments": null, "journal-ref": "Communications in Statistics - Simulation and Computation, 2019", "doi": "10.1080/03610918.2019.1610441", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let Y be a response variable related with a set of explanatory variables and\nlet f1, f2, ..., fk be a set of the parametric forms representing a set of\ncandidate's model. Let f* be the true model among the set of k plausible\nmodels. We discuss in this paper the use of wavelet regression method as\nauxiliary for the choice of the \"true\" parametric form of a regression model,\nparticularly, for the cases of nonlinear regression and generalized linear\nmodels. The use of a non-parametric method for the choice of the more\nappropriate parametric equation in regression problems would be interesting in\npractice due to the simplicity and because the probabilistic assumptions are\nnot required. We evaluate the performance of the proposed wavelet procedure\nbased on the true classification rate of the correct parametric form among a\nrange of k candidate models, taking into account a wide ranges of scenarios and\nconfigurations as well as in real data set applications.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 14:39:39 GMT"}], "update_date": "2019-04-23", "authors_parsed": [["Neto", "Eufr\u00e1sio de A. Lima", ""], ["Pinheiro", "Alu\u00edsio de S.", ""], ["Ferreira", "Adenice G. O.", ""]]}, {"id": "1809.11076", "submitter": "Somayeh Zarezadeh", "authors": "Majid Asadi and Somayeh Zarezadeh", "title": "A Unified Approach to Construct Correlation Coefficient Between Random\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the correlation (association) between two random variables is one\nof the important goals in statistical applications. In the literature, the\ncovariance between two random variables is a widely used criterion in measuring\nthe linear association between two random variables. In this paper, first we\npropose a covariance based unified measure of variability for a continuous\nrandom variable X and we show that several measures of variability and\nuncertainty, such as variance, Gini mean difference, cumulative residual\nentropy, etc., can be considered as special cases. Then, we propose a unified\nmeasure of correlation between two continuous random variables X and Y, with\ndistribution functions (DFs) F and G, based on the covariance between X and\nH^{-1}G(Y) (known as the Q-transformation of H on G) where H is a continuous\nDF. We show that our proposed measure of association subsumes some of the\nexisting measures of correlation. Under some mild condition on H, it is shown\nthe suggested index ranges between [-1,1] where the extremes of the range,\ni.e., -1 and 1, are attainable by the Frechet bivariate minimal and maximal\nDFs, respectively. A special case of the proposed correlation measure leads to\na variant of Pearson correlation coefficient which, as a measure of strength\nand direction of the linear relationship between X and Y, has absolute values\ngreater than or equal to the Pearson correlation. The results are examined\nnumerically for some well known bivariate DFs.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:06:35 GMT"}, {"version": "v2", "created": "Sun, 28 Oct 2018 09:56:49 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Asadi", "Majid", ""], ["Zarezadeh", "Somayeh", ""]]}, {"id": "1809.11108", "submitter": "Mathieu Gerber", "authors": "Mathieu Gerber and Kari Heine", "title": "Online Inference with Multi-modal Likelihood Functions", "comments": "64 pages (29 pages for the paper and 35 pages for the supplementary\n  material), 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $(Y_t)_{t\\geq 1}$ be a sequence of i.i.d.\\ observations and\n$\\{f_\\theta,\\theta\\in \\mathbb{R}^d\\}$ be a parametric model. We introduce a new\nonline algorithm for computing a sequence $(\\hat{\\theta}_t)_{t\\geq 1}$ which is\nshown to converge almost surely to $\\text{argmax}_{\\theta\\in\n\\mathbb{R}^d}\\mathbb{E}[\\log f_\\theta(Y_1)]$ at rate $ \\mathcal{O}(\\log\n(t)^{(1+\\varepsilon)/2}t^{-1/2})$, with $\\varepsilon>0$ a user specified\nparameter. This convergence result is obtained under standard conditions on the\nstatistical model and, most notably, we allow the mapping $\\theta\\mapsto\n\\mathbb{E}[\\log f_\\theta(Y_1)]$ to be multi-modal. However, the computational\ncost to process each observation grows exponentially with the dimension of\n$\\theta$, which makes the proposed approach applicable to low or moderate\ndimensional problems only. We also derive a version of the estimator\n$\\hat{\\theta}_t$ which is well suited to Student-t linear regression models.\nThe corresponding estimator of the regression coefficients is robust to the\npresence of outliers, as shown by experiments on simulated and real data, and\nthus, as a by-product of this work, we obtain a new online and adaptive robust\nestimation method for linear regression models.\n", "versions": [{"version": "v1", "created": "Fri, 28 Sep 2018 15:59:41 GMT"}, {"version": "v2", "created": "Fri, 12 Oct 2018 10:25:53 GMT"}, {"version": "v3", "created": "Fri, 2 Nov 2018 09:54:37 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2019 08:47:45 GMT"}, {"version": "v5", "created": "Mon, 19 Oct 2020 12:10:02 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Gerber", "Mathieu", ""], ["Heine", "Kari", ""]]}]