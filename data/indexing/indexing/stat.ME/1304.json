[{"id": "1304.0150", "submitter": "Antonio Punzo", "authors": "Salvatore Ingrassia and Antonio Punzo", "title": "Fitting Bivariate Mixed-Type Data via the Generalized Linear Exponential\n  Cluster-Weighted Model", "comments": "the paper has been withdrawn by the authors because they are working\n  on a quite extended version of the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cluster-weighted model (CWM) is a mixture model with random covariates\nwhich allows for flexible clustering and density estimation of a random vector\ncomposed by a response variable and by a set of covariates. In this class of\nmodels, the generalized linear exponential CWM is here introduced especially\nfor modeling bivariate data of mixed-type. Its natural counterpart, in the\nfamily of latent class models, is also defined. Maximum likelihood parameter\nestimates are derived using the EM algorithm and model selection is carried out\nusing the Bayesian information criterion (BIC). Artificial and real data are\nfinally considered to exemplify and appreciate the proposed model.\n", "versions": [{"version": "v1", "created": "Sun, 31 Mar 2013 01:41:26 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2013 08:07:22 GMT"}], "update_date": "2013-08-06", "authors_parsed": [["Ingrassia", "Salvatore", ""], ["Punzo", "Antonio", ""]]}, {"id": "1304.0282", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Victor Chernozhukov and Kengo Kato", "title": "Uniform Post Selection Inference for LAD Regression and Other\n  Z-estimation problems", "comments": "includes supplementary material; 2 figures", "journal-ref": null, "doi": "10.1093/biomet/asu056", "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop uniformly valid confidence regions for regression coefficients in\na high-dimensional sparse median regression model with homoscedastic errors.\nOur methods are based on a moment equation that is immunized against\nnon-regular estimation of the nuisance part of the median regression function\nby using Neyman's orthogonalization. We establish that the resulting\ninstrumental median regression estimator of a target regression coefficient is\nasymptotically normally distributed uniformly with respect to the underlying\nsparse model and is semi-parametrically efficient. We also generalize our\nmethod to a general non-smooth Z-estimation framework with the number of target\nparameters $p_1$ being possibly much larger than the sample size $n$. We extend\nHuber's results on asymptotic normality to this setting, demonstrating uniform\nasymptotic normality of the proposed estimators over $p_1$-dimensional\nrectangles, constructing simultaneous confidence bands on all of the $p_1$\ntarget parameters, and establishing asymptotic validity of the bands uniformly\nover underlying approximately sparse models.\n  Keywords: Instrument; Post-selection inference; Sparsity; Neyman's Orthogonal\nScore test; Uniformly valid inference; Z-estimation.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2013 02:29:25 GMT"}, {"version": "v2", "created": "Thu, 2 May 2013 00:33:23 GMT"}, {"version": "v3", "created": "Mon, 30 Dec 2013 16:30:10 GMT"}, {"version": "v4", "created": "Tue, 30 Dec 2014 04:46:58 GMT"}, {"version": "v5", "created": "Mon, 22 Jan 2018 16:50:02 GMT"}, {"version": "v6", "created": "Sun, 18 Oct 2020 19:20:19 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Kato", "Kengo", ""]]}, {"id": "1304.0353", "submitter": "Pedro Vitoria", "authors": "Galen Sher, Pedro Vitoria", "title": "An Information-Theoretic Test for Dependence with an Application to the\n  Temporal Structure of Stock Returns", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information theory provides ideas for conceptualising information and\nmeasuring relationships between objects. It has found wide application in the\nsciences, but economics and finance have made surprisingly little use of it. We\nshow that time series data can usefully be studied as information -- by noting\nthe relationship between statistical redundancy and dependence, we are able to\nuse the results of information theory to construct a test for joint dependence\nof random variables. The test is in the same spirit of those developed by\nRyabko and Astola (2005, 2006b,a), but differs from these in that we add extra\nrandomness to the original stochatic process. It uses data compression to\nestimate the entropy rate of a stochastic process, which allows it to measure\ndependence among sets of random variables, as opposed to the existing\neconometric literature that uses entropy and finds itself restricted to\npairwise tests of dependence. We show how serial dependence may be detected in\nS&P500 and PSI20 stock returns over different sample periods and frequencies.\nWe apply the test to synthetic data to judge its ability to recover known\ntemporal dependence structures.\n", "versions": [{"version": "v1", "created": "Mon, 1 Apr 2013 13:44:47 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 11:20:36 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Sher", "Galen", ""], ["Vitoria", "Pedro", ""]]}, {"id": "1304.0537", "submitter": "Rajesh  Singh", "authors": "Rajesh Singh, Mukesh Kumar, Manoj K. Chaudhary, Cem Kadilar", "title": "Dual To Ratio Cum Product Estimator In Stratified Random Sampling", "comments": "13 pages, presented in International conference on Bayesian theory\n  and applications (IWCBTA) Jan. 6-10, 2013, BHU, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracy et al.[8] have introduced a family of estimators using Srivenkataramana\nand Tracy ([6],[7]) transformation in simple random sampling. In this article,\nwe have proposed a dual to ratio-cum-product estimator in stratified random\nsampling. The expressions of the mean square error of the proposed estimators\nare derived. Also, the theoretical findings are supported by a numerical\nexample.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 05:45:58 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Singh", "Rajesh", ""], ["Kumar", "Mukesh", ""], ["Chaudhary", "Manoj K.", ""], ["Kadilar", "Cem", ""]]}, {"id": "1304.0564", "submitter": "Tyler J. VanderWeele", "authors": "Tyler J. VanderWeele, Ilya Shpitser", "title": "On the definition of a confounder", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1058 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 1, 196-220", "doi": "10.1214/12-AOS1058", "report-no": "IMS-AOS-AOS1058", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal inference literature has provided a clear formal definition of\nconfounding expressed in terms of counterfactual independence. The literature\nhas not, however, come to any consensus on a formal definition of a confounder,\nas it has given priority to the concept of confounding over that of a\nconfounder. We consider a number of candidate definitions arising from various\nmore informal statements made in the literature. We consider the properties\nsatisfied by each candidate definition, principally focusing on (i) whether\nunder the candidate definition control for all \"confounders\" suffices to\ncontrol for \"confounding\" and (ii) whether each confounder in some context\nhelps eliminate or reduce confounding bias. Several of the candidate\ndefinitions do not have these two properties. Only one candidate definition of\nthose considered satisfies both properties. We propose that a \"confounder\" be\ndefined as a pre-exposure covariate C for which there exists a set of other\ncovariates X such that effect of the exposure on the outcome is unconfounded\nconditional on (X,C) but such that for no proper subset of (X,C) is the effect\nof the exposure on the outcome unconfounded given the subset. We also provide a\nconditional analogue of the above definition; and we propose a variable that\nhelps reduce bias but not eliminate bias be referred to as a \"surrogate\nconfounder.\" These definitions are closely related to those given by Robins and\nMorgenstern [Comput. Math. Appl. 14 (1987) 869-916]. The implications that hold\namong the various candidate definitions are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 08:54:00 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["VanderWeele", "Tyler J.", ""], ["Shpitser", "Ilya", ""]]}, {"id": "1304.0596", "submitter": "Jairo Fuquene", "authors": "Jairo Fuquene", "title": "A Semiparametric Bayesian Extreme Value Model Using a Dirichlet Process\n  Mixture of Gamma Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a model with a Dirichlet process mixture of gamma\ndensities in the bulk part below threshold and a generalized Pareto density in\nthe tail for extreme value estimation. The proposed model is simple and\nflexible allowing us posterior density estimation and posterior inference for\nhigh quantiles. The model works well even for small sample sizes and in the\nabsence of prior information. We evaluate the performance of the proposed model\nthrough a simulation study. Finally, the proposed model is applied to a real\nenvironmental data.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 11:49:11 GMT"}], "update_date": "2013-04-03", "authors_parsed": [["Fuquene", "Jairo", ""]]}, {"id": "1304.0796", "submitter": "Susan Wei", "authors": "Susan Wei, Chihoon Lee, Lindsay Wichers, Gen Li, J.S. Marron", "title": "Direction-Projection-Permutation for High Dimensional Hypothesis Tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the prevalence of high dimensional low sample size datasets in\nmodern statistical applications, we propose a general nonparametric framework,\nDirection-Projection-Permutation (DiProPerm), for testing high dimensional\nhypotheses. The method is aimed at rigorous testing of whether lower\ndimensional visual differences are statistically significant. Theoretical\nanalysis under the non-classical asymptotic regime of dimension going to\ninfinity for fixed sample size reveals that certain natural variations of\nDiProPerm can have very different behaviors. An empirical power study both\nconfirms the theoretical results and suggests DiProPerm is a powerful test in\nmany settings. Finally DiProPerm is applied to a high dimensional gene\nexpression dataset.\n", "versions": [{"version": "v1", "created": "Tue, 2 Apr 2013 20:44:48 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Wei", "Susan", ""], ["Lee", "Chihoon", ""], ["Wichers", "Lindsay", ""], ["Li", "Gen", ""], ["Marron", "J. S.", ""]]}, {"id": "1304.0905", "submitter": "Aristidis K. Nikoloulopoulos", "authors": "Aristidis K. Nikoloulopoulos", "title": "On the estimation of normal copula discrete regression models using the\n  continuous extension and simulated likelihood", "comments": null, "journal-ref": "Journal of Statistical Planning and Inference, 2013,\n  143:1923--1937", "doi": "10.1016/j.jspi.2013.06.015", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuous extension of a discrete random variable is amongst the\ncomputational methods used for estimation of multivariate normal copula-based\nmodels with discrete margins. Its advantage is that the likelihood can be\nderived conveniently under the theory for copula models with continuous\nmargins, but there has not been a clear analysis of the adequacy of this\nmethod. We investigate the asymptotic and small-sample efficiency of two\nvariants of the method for estimating the multivariate normal copula with\nunivariate binary, Poisson, and negative binomial regressions, and show that\nthey lead to biased estimates for the latent correlations, and the univariate\nmarginal parameters that are not regression coefficients. We implement a\nmaximum simulated likelihood method, which is based on evaluating the\nmultidimensional integrals of the likelihood with randomized quasi Monte Carlo\nmethods. Asymptotic and small-sample efficiency calculations show that our\nmethod is nearly as efficient as maximum likelihood for fully specified\nmultivariate normal copula-based models. An illustrative example is given to\nshow the use of our simulated likelihood method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 10:38:59 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Nikoloulopoulos", "Aristidis K.", ""]]}, {"id": "1304.0925", "submitter": "Julie Forman", "authors": "Julie Forman and Michael S{\\o}rensen", "title": "A new approach to multi-modal diffusions with applications to protein\n  folding", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article demonstrates that flexible and statistically tractable\nmulti-modal diffusion models can be attained by transformation of simple\nwell-known diffusion models such as the Ornstein-Uhlenbeck model, or more\ngenerally a Pearson diffusion. The transformed diffusion inherits many\nproperties of the underlying simple diffusion including its mixing rates and\ndistributions of first passage times. Likelihood inference and martingale\nestimating functions are considered in the case of a discretely observed\nbimodal diffusion. It is further demonstrated that model parameters can be\nidentified and estimated when the diffusion is observed with additional\nmeasurement error. The new approach is applied to molecular dynamics data in\nform of a reaction coordinate of the small Trp-zipper protein, for which the\nfolding and unfolding rates are estimated. The new models provide a better fit\nto this type of protein folding data than previous models because the diffusion\ncoefficient is state-dependent.\n", "versions": [{"version": "v1", "created": "Wed, 3 Apr 2013 11:49:20 GMT"}], "update_date": "2013-04-04", "authors_parsed": [["Forman", "Julie", ""], ["S\u00f8rensen", "Michael", ""]]}, {"id": "1304.1350", "submitter": "Alex Lenkoski", "authors": "Alex Lenkoski", "title": "A Direct Sampler for G-Wishart Variates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The G-Wishart distribution is the conjugate prior for precision matrices that\nencode the conditional independencies of a Gaussian graphical model. While the\ndistribution has received considerable attention, posterior inference has\nproven computationally challenging, in part due to the lack of a direct\nsampler. In this note, we rectify this situation. The existence of a direct\nsampler offers a host of new possibilities for the use of G-Wishart variates.\nWe discuss one such development by outlining a new transdimensional model\nsearch algorithm--which we term double reversible jump--that leverages this\nsampler to avoid normalizing constant calculation when comparing graphical\nmodels. We conclude with two short studies meant to investigate our algorithm's\nvalidity.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 12:36:16 GMT"}], "update_date": "2013-04-05", "authors_parsed": [["Lenkoski", "Alex", ""]]}, {"id": "1304.1383", "submitter": "Benedikt M. P\\\"otscher", "authors": "David Preinerstorfer and Benedikt M. P\\\"otscher", "title": "On Size and Power of Heteroskedasticity and Autocorrelation Robust Tests", "comments": "Some errors corrected, some material added", "journal-ref": "Econometric Theory 32, 2016, 261-358", "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Testing restrictions on regression coefficients in linear models often\nrequires correcting the conventional F-test for potential heteroskedasticity or\nautocorrelation amongst the disturbances, leading to so-called\nheteroskedasticity and autocorrelation robust test procedures. These procedures\nhave been developed with the purpose of attenuating size distortions and power\ndeficiencies present for the uncorrected F-test. We develop a general theory to\nestablish positive as well as negative finite-sample results concerning the\nsize and power properties of a large class of heteroskedasticity and\nautocorrelation robust tests. Using these results we show that\nnonparametrically as well as parametrically corrected F-type tests in time\nseries regression models with stationary disturbances have either size equal to\none or nuisance-infimal power equal to zero under very weak assumptions on the\ncovariance model and under generic conditions on the design matrix. In addition\nwe suggest an adjustment procedure based on artificial regressors. This\nadjustment resolves the problem in many cases in that the so-adjusted tests do\nnot suffer from size distortions. At the same time their power function is\nbounded away from zero. As a second application we discuss the case of\nheteroskedastic disturbances.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 14:45:05 GMT"}, {"version": "v2", "created": "Tue, 8 Jul 2014 14:42:46 GMT"}], "update_date": "2016-12-21", "authors_parsed": [["Preinerstorfer", "David", ""], ["P\u00f6tscher", "Benedikt M.", ""]]}, {"id": "1304.1384", "submitter": "Nathan Uyttendaele", "authors": "Johan Segers and Nathan Uyttendaele", "title": "Nonparametric estimation of the tree structure of a nested Archimedean\n  copula", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the features inherent in nested Archimedean copulas, also called\nhierarchical Archimedean copulas, is their rooted tree structure. A\nnonparametric, rank-based method to estimate this structure is presented. The\nidea is to represent the target structure as a set of trivariate structures,\neach of which can be estimated individually with ease. Indeed, for any three\nvariables there are only four possible rooted tree structures and, based on a\nsample, a choice can be made by performing comparisons between the three\nbivariate margins of the empirical distribution of the three variables. The set\nof estimated trivariate structures can then be used to build an estimate of the\ntarget structure. The advantage of this estimation method is that it does not\nrequire any parametric assumptions concerning the generator functions at the\nnodes of the tree.\n", "versions": [{"version": "v1", "created": "Thu, 4 Apr 2013 14:48:36 GMT"}, {"version": "v2", "created": "Tue, 17 Dec 2013 13:38:46 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Segers", "Johan", ""], ["Uyttendaele", "Nathan", ""]]}, {"id": "1304.1600", "submitter": "Rebecca Steorts", "authors": "Rebecca C. Steorts and Malay Ghosh", "title": "On estimation of mean squared errors of benchmarked empirical Bayes\n  estimators", "comments": null, "journal-ref": "Statistica Sinica 23 (2013), 749-767", "doi": "10.5705/ss.2012.053", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider benchmarked empirical Bayes (EB) estimators under the basic\narea-level model of Fay and Herriot while requiring the standard benchmarking\nconstraint. In this paper we determine the excess mean squared error (MSE) from\nconstraining the estimates through benchmarking. We show that the increase due\nto benchmarking is O(m^{-1}), where m is the number of small areas.\nFurthermore, we find an asymptotically unbiased estimator of this MSE and\ncompare it to the second-order approximation of the MSE of the EB estimator or,\nequivalently, of the MSE of the empirical best linear unbiased predictor\n(EBLUP), that was derived by Prasad and Rao (1990). Morever, using methods\nsimilar to those of Butar and Lahiri (2003), we compute a parametric bootstrap\nestimator of the MSE of the benchmarked EB estimator under the Fay-Herriot\nmodel and compare it to the MSE of the benchmarked EB estimator found by a\nsecond-order approximation. Finally, we illustrate our methods using SAIPE data\nfrom the U.S. Census Bureau, and in a simulation study.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2013 02:23:29 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Steorts", "Rebecca C.", ""], ["Ghosh", "Malay", ""]]}, {"id": "1304.1720", "submitter": "Paul Marriott", "authors": "Karim Anaya-Izquierdo and Frank Critchley and Paul Marriott", "title": "Logistic regression geometry", "comments": "9 pages, 5 fugures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper looks at effects, due to the boundary, on inference in logistic\nregression. It shows that first -- and, indeed, higher -- order asymptotic\nresults are not uniform across the model. Near the boundary, effects such as\nhigh skewness, discreteness and collinearity dominate, any of which could\nrender inference based on asymptotic normality suspect. A highly interpretable\ndiagnostic tool is proposed allowing the analyst to check if the boundary is\ngoing to have an appreciable effect on standard inferential techniques.\n", "versions": [{"version": "v1", "created": "Fri, 5 Apr 2013 14:11:33 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Anaya-Izquierdo", "Karim", ""], ["Critchley", "Frank", ""], ["Marriott", "Paul", ""]]}, {"id": "1304.1837", "submitter": "Art Owen", "authors": "Aiyou Chen, Art B. Owen, Minghui Shi", "title": "Data enriched linear regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a linear regression method for predictions on a small data set\nmaking use of a second possibly biased data set that may be much larger. Our\nmethod fits linear regressions to the two data sets while penalizing the\ndifference between predictions made by those two models. The resulting\nalgorithm is a shrinkage method similar to those used in small area estimation.\nWe find a Stein-type finding for Gaussian responses: when the model has 5 or\nmore coefficients and 10 or more error degrees of freedom, it becomes\ninadmissible to use only the small data set, no matter how large the bias is.\nWe also present both plug-in and AICc-based methods to tune our penalty\nparameter. Most of our results use an $L_2$ penalty, but we obtain formulas for\n$L_1$ penalized estimates when the model is specialized to the location\nsetting. Ordinary Stein shrinkage provides an inadmissibility result for only 3\nor more coefficients, but we find that our shrinkage method typically produces\nmuch lower squared errors in as few as 5 or 10 dimensions when the bias is\nsmall and essentially equivalent squared errors when the bias is large.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 00:20:53 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 22:12:22 GMT"}, {"version": "v3", "created": "Wed, 17 Dec 2014 21:58:53 GMT"}], "update_date": "2014-12-19", "authors_parsed": [["Chen", "Aiyou", ""], ["Owen", "Art B.", ""], ["Shi", "Minghui", ""]]}, {"id": "1304.1875", "submitter": "Nicolas Dobigeon", "authors": "Nicolas Dobigeon and Jean-Yves Tourneret and C\\'edric Richard and\n  Jos\\'e C. M. Bermudez and Stephen McLaughlin and Alfred O. Hero", "title": "Nonlinear unmixing of hyperspectral images: models and algorithms", "comments": null, "journal-ref": null, "doi": "10.1109/MSP.2013.2279274", "report-no": null, "categories": "physics.data-an stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When considering the problem of unmixing hyperspectral images, most of the\nliterature in the geoscience and image processing areas relies on the widely\nused linear mixing model (LMM). However, the LMM may be not valid and other\nnonlinear models need to be considered, for instance, when there are\nmulti-scattering effects or intimate interactions. Consequently, over the last\nfew years, several significant contributions have been proposed to overcome the\nlimitations inherent in the LMM. In this paper, we present an overview of\nrecent advances in nonlinear unmixing modeling.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 10:21:56 GMT"}, {"version": "v2", "created": "Thu, 18 Jul 2013 14:01:41 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""], ["Richard", "C\u00e9dric", ""], ["Bermudez", "Jos\u00e9 C. M.", ""], ["McLaughlin", "Stephen", ""], ["Hero", "Alfred O.", ""]]}, {"id": "1304.1920", "submitter": "Michael Betancourt", "authors": "M. J. Betancourt", "title": "Generalizing the No-U-Turn Sampler to Riemannian Manifolds", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hamiltonian Monte Carlo provides efficient Markov transitions at the expense\nof introducing two free parameters: a step size and total integration time.\nBecause the step size controls discretization error it can be readily tuned to\nachieve certain accuracy criteria, but the total integration time is left\nunconstrained. Recently Hoffman and Gelman proposed a criterion for tuning the\nintegration time in certain systems with their No U-Turn Sampler, or NUTS. In\nthis paper I investigate the dynamical basis for the success of NUTS and\ngeneralize it to Riemannian Manifold Hamiltonian Monte Carlo.\n", "versions": [{"version": "v1", "created": "Sat, 6 Apr 2013 18:02:36 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Betancourt", "M. J.", ""]]}, {"id": "1304.2048", "submitter": "Christian P. Robert", "authors": "Christian P. Robert (Universite Paris-Dauphine)", "title": "Bayesian Computational Tools", "comments": "26 pages, 10 figures, revision of a paper written as a chapter for\n  the Annual Review of Statistics and Its Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter surveys advances in the field of Bayesian computation over the\npast twenty years, with missing data. It also contains some novel computational\nentries on the double-exponential model that may be of interest per se.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 19:27:56 GMT"}, {"version": "v2", "created": "Tue, 11 Jun 2013 05:34:51 GMT"}], "update_date": "2013-06-12", "authors_parsed": [["Robert", "Christian P.", "", "Universite Paris-Dauphine"]]}, {"id": "1304.2069", "submitter": "Peter Ruckdeschel", "authors": "Christina Erlwein and Peter Ruckdeschel", "title": "Robustification of Elliott's on-line EM algorithm for HMMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.ST stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish a robustification of an on-line algorithm for\nmodelling asset prices within a hidden Markov model (HMM). In this HMM\nframework, parameters of the model are guided by a Markov chain in discrete\ntime, parameters of the asset returns are therefore able to switch between\ndifferent regimes. The parameters are estimated through an on-line algorithm,\nwhich utilizes incoming information from the market and leads to adaptive\noptimal estimates. We robustify this algorithm step by step against additive\noutliers appearing in the observed asset prices with the rationale to better\nhandle possible peaks or missings in asset returns.\n", "versions": [{"version": "v1", "created": "Sun, 7 Apr 2013 22:10:26 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Erlwein", "Christina", ""], ["Ruckdeschel", "Peter", ""]]}, {"id": "1304.2293", "submitter": "Aurelien Latouche", "authors": "Arthur Allignol, Jan Beyersmann, Thomas Gerds, Aur\\'elien Latouche\n  (CEDRIC)", "title": "A competing risks approach for nonparametric estimation of transition\n  probabilities in a non-Markov illness-death model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competing risks model time to first event and type of first event. An example\nfrom hospital epidemiology is the incidence of hospital-acquired infection,\nwhich has to account for hospital discharge of non-infected patients as a\ncompeting risk. An illness-death model would allow to further study hospital\noutcomes of infected patients. Such a model typically relies on a Markov\nassumption. However, it is conceivable that the future course of an infected\npatient does not only depend on the time since hospital admission and current\ninfection status but also on the time since infection. We demonstrate how a\nmodified competing risks model can be used for nonparametric estimation of\ntransition probabilities when the Markov assumption is violated.\n", "versions": [{"version": "v1", "created": "Mon, 8 Apr 2013 18:21:16 GMT"}], "update_date": "2013-04-09", "authors_parsed": [["Allignol", "Arthur", "", "CEDRIC"], ["Beyersmann", "Jan", "", "CEDRIC"], ["Gerds", "Thomas", "", "CEDRIC"], ["Latouche", "Aur\u00e9lien", "", "CEDRIC"]]}, {"id": "1304.2499", "submitter": "Nicolas Dobigeon", "authors": "Yoann Altmann and Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Unsupervised Post-Nonlinear Unmixing of Hyperspectral Images Using a\n  Hamiltonian Monte Carlo Algorithm", "comments": null, "journal-ref": null, "doi": "10.1109/TIP.2014.2314022", "report-no": null, "categories": "stat.ME physics.data-an stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a nonlinear mixing model for hyperspectral image\nunmixing. The proposed model assumes that the pixel reflectances are\npost-nonlinear functions of unknown pure spectral components contaminated by an\nadditive white Gaussian noise. These nonlinear functions are approximated using\npolynomials leading to a polynomial post-nonlinear mixing model. A Bayesian\nalgorithm is proposed to estimate the parameters involved in the model yielding\nan unsupervised nonlinear unmixing algorithm. Due to the large number of\nparameters to be estimated, an efficient Hamiltonian Monte Carlo algorithm is\ninvestigated. The classical leapfrog steps of this algorithm are modified to\nhandle the parameter constraints. The performance of the unmixing strategy,\nincluding convergence and parameter tuning, is first evaluated on synthetic\ndata. Simulations conducted with real data finally show the accuracy of the\nproposed unmixing strategy for the analysis of hyperspectral images.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 09:23:20 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Altmann", "Yoann", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1304.2810", "submitter": "Tianxi Li", "authors": "Jie Cheng, Tianxi Li, Elizaveta Levina, Ji Zhu", "title": "High-dimensional Mixed Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While graphical models for continuous data (Gaussian graphical models) and\ndiscrete data (Ising models) have been extensively studied, there is little\nwork on graphical models linking both continuous and discrete variables (mixed\ndata), which are common in many scientific applications. We propose a novel\ngraphical model for mixed data, which is simple enough to be suitable for\nhigh-dimensional data, yet flexible enough to represent all possible graph\nstructures. We develop a computationally efficient regression-based algorithm\nfor fitting the model by focusing on the conditional log-likelihood of each\nvariable given the rest. The parameters have a natural group structure, and\nsparsity in the fitted graph is attained by incorporating a group lasso\npenalty, approximated by a weighted $\\ell_1$ penalty for computational\nefficiency. We demonstrate the effectiveness of our method through an extensive\nsimulation study and apply it to a music annotation data set (CAL500),\nobtaining a sparse and interpretable graphical model relating the continuous\nfeatures of the audio signal to categorical variables such as genre, emotions,\nand usage associated with particular songs. While we focus on binary discrete\nvariables, we also show that the proposed methodology can be easily extended to\ngeneral discrete variables.\n", "versions": [{"version": "v1", "created": "Tue, 9 Apr 2013 22:39:12 GMT"}, {"version": "v2", "created": "Fri, 5 Feb 2016 04:36:19 GMT"}, {"version": "v3", "created": "Fri, 19 Aug 2016 04:06:53 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Cheng", "Jie", ""], ["Li", "Tianxi", ""], ["Levina", "Elizaveta", ""], ["Zhu", "Ji", ""]]}, {"id": "1304.2828", "submitter": "Eric Bair", "authors": "Emily Colby and Eric Bair", "title": "Cross-Validation for Nonlinear Mixed Effects Models", "comments": "38 pages, 15 figures To be published in the Journal of\n  Pharmacokinetics and Pharmacodynamics", "journal-ref": "Journal of Pharmacokinetics and Pharmacodynamics, April 2013,\n  40(2): 243-252", "doi": "10.1007/s10928-013-9313-5", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-validation is frequently used for model selection in a variety of\napplications. However, it is difficult to apply cross-validation to mixed\neffects models (including nonlinear mixed effects models or NLME models) due to\nthe fact that cross-validation requires \"out-of-sample\" predictions of the\noutcome variable, which cannot be easily calculated when random effects are\npresent. We describe two novel variants of cross-validation that can be applied\nto nonlinear mixed effects models. One variant, where out-of-sample predictions\nare based on post hoc estimates of the random effects, can be used to select\nthe overall structural model. Another variant, where cross-validation seeks to\nminimize the estimated random effects rather than the estimated residuals, can\nbe used to select covariates to include in the model. We show that these\nmethods produce accurate results in a variety of simulated data sets and apply\nthem to two publicly available population pharmacokinetic data sets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 01:57:21 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Colby", "Emily", ""], ["Bair", "Eric", ""]]}, {"id": "1304.2986", "submitter": "Ryan J. Tibshirani", "authors": "Ryan J. Tibshirani", "title": "Adaptive piecewise polynomial estimation via trend filtering", "comments": "Published in at http://dx.doi.org/10.1214/13-AOS1189 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 1, 285-323", "doi": "10.1214/13-AOS1189", "report-no": "IMS-AOS-AOS1189", "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study trend filtering, a recently proposed tool of Kim et al. [SIAM Rev.\n51 (2009) 339-360] for nonparametric regression. The trend filtering estimate\nis defined as the minimizer of a penalized least squares criterion, in which\nthe penalty term sums the absolute $k$th order discrete derivatives over the\ninput points. Perhaps not surprisingly, trend filtering estimates appear to\nhave the structure of $k$th degree spline functions, with adaptively chosen\nknot points (we say ``appear'' here as trend filtering estimates are not really\nfunctions over continuous domains, and are only defined over the discrete set\nof inputs). This brings to mind comparisons to other nonparametric regression\ntools that also produce adaptive splines; in particular, we compare trend\nfiltering to smoothing splines, which penalize the sum of squared derivatives\nacross input points, and to locally adaptive regression splines [Ann. Statist.\n25 (1997) 387-413], which penalize the total variation of the $k$th derivative.\nEmpirically, we discover that trend filtering estimates adapt to the local\nlevel of smoothness much better than smoothing splines, and further, they\nexhibit a remarkable similarity to locally adaptive regression splines. We also\nprovide theoretical support for these empirical findings; most notably, we\nprove that (with the right choice of tuning parameter) the trend filtering\nestimate converges to the true underlying function at the minimax rate for\nfunctions whose $k$th derivative is of bounded variation. This is done via an\nasymptotic pairing of trend filtering and locally adaptive regression splines,\nwhich have already been shown to converge at the minimax rate [Ann. Statist. 25\n(1997) 387-413]. At the core of this argument is a new result tying together\nthe fitted values of two lasso problems that share the same outcome vector, but\nhave different predictor matrices.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 15:02:53 GMT"}, {"version": "v2", "created": "Fri, 21 Mar 2014 13:42:10 GMT"}], "update_date": "2014-03-24", "authors_parsed": [["Tibshirani", "Ryan J.", ""]]}, {"id": "1304.3073", "submitter": "Chintan Mehta", "authors": "Marc Hallin, Chintan Mehta", "title": "R-Estimation for Asymmetric Independent Component Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independent Component Analysis (ICA) recently has attracted attention in the\nstatistical literature as an alternative to elliptical models. Whereas\nk-dimensional elliptical densities depend on one single unspecified radial\ndensity, however, k-dimensional independent component distributions involve k\nunspecified component densities that for given sample size n and dimension k\nmaking statistical analysis harder. We focus here on estimating the model's\nmixing matrix. Traditional methods (FOBI, Kernel-ICA, FastICA) originating from\nthe engineering literature have consistency that requires moment conditions\nwithout achieving any type of asymptotic efficiency. When based on robust\nscatter matrices, the two-scatter methods developed by Oja, et al. (2006) and\nNordhausen, et al. (2008) enjoy better robustness features but have unclear\noptimality properties. The semiparametric approach by Chen and Bickel (2006)\nachieves semiparametric efficiency but requires estimating the k unobserved\nindependent component densities. As a reaction, an efficient\n(signed-)rank-based approach has been proposed by Ilmonen and Paindaveine\n(2011) for the case of symmetric component densities that fail to be root-n\nconsistent as soon as one of the component densities is asymmetric. In this\npaper, using ranks rather than signed ranks, we extend their approach to the\nasymmetric case and propose a one-step R-estimator for ICA mixing matrices. The\nfinite-sample performances of those estimators are investigated and compared to\nthose of existing methods under moderately large sample sizes. Particularly\ngood performances are obtained when using data-driven scores taking into\naccount the skewness and kurtosis of residuals. Finally, we show, by an\nempirical exercise, that our methods also may provide excellent results in a\ncontext such as image analysis, where the basic assumptions of ICA are quite\nunlikely to hold.\n", "versions": [{"version": "v1", "created": "Wed, 10 Apr 2013 19:56:27 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2013 21:05:26 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Hallin", "Marc", ""], ["Mehta", "Chintan", ""]]}, {"id": "1304.3206", "submitter": "Teng Zhang", "authors": "Teng Zhang, Ami Wiesel, Maria Sabrina Grec", "title": "Multivariate Generalized Gaussian Distribution: Convexity and Graphical\n  Models", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2013.2267740", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider covariance estimation in the multivariate generalized Gaussian\ndistribution (MGGD) and elliptically symmetric (ES) distribution. The maximum\nlikelihood optimization associated with this problem is non-convex, yet it has\nbeen proved that its global solution can be often computed via simple fixed\npoint iterations. Our first contribution is a new analysis of this likelihood\nbased on geodesic convexity that requires weaker assumptions. Our second\ncontribution is a generalized framework for structured covariance estimation\nunder sparsity constraints. We show that the optimizations can be formulated as\nconvex minimization as long the MGGD shape parameter is larger than half and\nthe sparsity pattern is chordal. These include, for example, maximum likelihood\nestimation of banded inverse covariances in multivariate Laplace distributions,\nwhich are associated with time varying autoregressive processes.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 06:25:28 GMT"}, {"version": "v2", "created": "Sun, 1 Sep 2013 01:29:51 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Zhang", "Teng", ""], ["Wiesel", "Ami", ""], ["Grec", "Maria Sabrina", ""]]}, {"id": "1304.3347", "submitter": "Thomas Opitz", "authors": "T. Opitz, P. Tramini, N. Molinari", "title": "Spline regression for zero-inflated models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a regression model for count data when the classical generalized\nlinear model approach is too rigid due to a high outcome of zero counts and a\nnonlinear influence of continuous covariates. Zero-Inflation is applied to take\ninto account the presence of excess zeros with separate link functions for the\nzero and the nonzero component. Nonlinearity in covariates is captured by\nspline functions based on B-splines. Our algorithm relies on maximum-likelihood\nestimation and allows for adaptive box-constrained knots, thus improving the\ngoodness of the spline fit and allowing for detection of sensitivity\nchangepoints. A simulation study substantiates the numerical stability of the\nalgorithm to infer such models. The AIC criterion is shown to serve well for\nmodel selection, in particular if nonlinearities are weak such that BIC tends\nto overly simplistic models. We fit the introduced models to real data of\nchildren's dental sanity, linking caries counts with the so-called\nBody-Mass-Index (BMI) and other socioeconomic factors. This reveals a puzzling\nnonmonotonic influence of BMI on caries counts which is yet to be explained by\nclinical experts.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 15:48:02 GMT"}], "update_date": "2013-04-12", "authors_parsed": [["Opitz", "T.", ""], ["Tramini", "P.", ""], ["Molinari", "N.", ""]]}, {"id": "1304.3378", "submitter": "James Scott", "authors": "James G. Scott, Thomas S. Shively, Stephen G. Walker", "title": "Nonparametric Bayesian testing for monotonicity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the problem of testing whether a function is monotone from\na nonparametric Bayesian perspective. Two new families of tests are\nconstructed. The first uses constrained smoothing splines, together with a\nhierarchical stochastic-process prior that explicitly controls the prior\nprobability of monotonicity. The second uses regression splines, together with\ntwo proposals for the prior over the regression coefficients. The finite-sample\nperformance of the tests is shown via simulation to improve upon existing\nfrequentist and Bayesian methods. The asymptotic properties of the Bayes factor\nfor comparing monotone versus non-monotone regression functions in a Gaussian\nmodel are also studied. Our results significantly extend those currently\navailable, which chiefly focus on determining the dimension of a parametric\nlinear model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 17:25:37 GMT"}, {"version": "v2", "created": "Sun, 1 Jun 2014 16:07:15 GMT"}], "update_date": "2014-06-03", "authors_parsed": [["Scott", "James G.", ""], ["Shively", "Thomas S.", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1304.3505", "submitter": "Yakir Berchenko", "authors": "Yakir Berchenko, Jonathan Rosenblatt and Simon D.W. Frost", "title": "Modeling and Analysing Respondent Driven Sampling as a Counting Process", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Respondent-driven sampling (RDS) is an approach to sampling design and\nanalysis which utilizes the networks of social relationships that connect\nmembers of the target population, using chain-referral methods to facilitate\nsampling. RDS typically leads to biased sampling, favoring participants with\nmany acquaintances. Naive estimates, such as the sample average, which are\nuncorrected for the sampling bias, will themselves be biased. To compensate for\nthis bias, current methodology suggests inverse-degree weighting, where the\n\"degree\" is the number of acquaintances. This stems from the fundamental RDS\nassumption that the probability of sampling an individual is proportional to\ntheir degree. Since this assumption is tenuous at best, we propose to harness\nthe additional information encapsulated in the time of recruitment, into a\nmodel-based inference framework for RDS. This information is typically\ncollected by researchers, but ignored. We adapt methods developed for inference\nin epidemic processes to estimate the population size, degree counts and\nfrequencies. While providing valuable information in themselves, these\nquantities ultimately serve to debias other estimators, such a disease's\nprevalence. A fundamental advantage of our approach is that, being model-based,\nit makes all assumptions of the data-generating process explicit. This enables\nverification of the assumptions, maximum likelihood estimation, extension with\ncovariates, and model selection. We develop asymptotic theory, proving\nconsistency and asymptotic normality properties. We further compare these\nestimators to the standard inverse-degree weighting through simulations, and\nusing real-world data. In both cases we find our estimators to outperform\ncurrent methods. The likelihood problem in the model we present is convex, and\nthus efficiently solvable. We implement these estimators in an R package,\nchords, available on CRAN.\n", "versions": [{"version": "v1", "created": "Thu, 11 Apr 2013 22:44:50 GMT"}, {"version": "v2", "created": "Tue, 18 Aug 2015 11:22:11 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Berchenko", "Yakir", ""], ["Rosenblatt", "Jonathan", ""], ["Frost", "Simon D. W.", ""]]}, {"id": "1304.3544", "submitter": "Debasish Roy", "authors": "Tara Raveendran, Debasish Roy and Ram Mohan Vasu", "title": "Iterated Gain-based Stochastic Filters for Dynamic System\n  Identification: Annealing-type Iterations and the Filter Bank", "comments": "25 pages, 7 figures (not yet published in a refereed journal or any\n  conference proceedings)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel form of nonlinear stochastic filtering employing an annealing-type\niterative update scheme, aided by the introduction of an artificial diffusion\nparameter and based on the Gaussian sum approximations of the prior and\nposterior densities, is presented. The proposed Monte Carlo filter bank\nconforms in structure to the parent nonlinear filtering (Kushner-Stratonovich)\nequation, as reflected in the additive gain-based updates, and possesses\nexcellent mixing properties enabling better explorations of the phase space of\nthe state vector. The performance of the filter bank, presently assessed\nagainst a few carefully chosen numerical examples, provide ample evidence of\nits substantively improved performance in terms of filter convergence and\nestimation accuracy vis-\\`a-vis a few other competing filters especially in\nhigher dimensional dynamic system identification problems including cases that\nmay demand estimating relatively minor variations in the parameter values from\ntheir reference states.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 06:10:42 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Raveendran", "Tara", ""], ["Roy", "Debasish", ""], ["Vasu", "Ram Mohan", ""]]}, {"id": "1304.3608", "submitter": "Daniel Oberski", "authors": "Daniel Leonard Oberski", "title": "Individual Differences in Structural Equation Model Parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals may differ in their parameter values. This article discusses a\nthree-step method of studying such differences by calculating and then modeling\n\"individual parameter contributions\", making the study of heterogeneity in\narbitrary structural equation model parameters as technically challenging as\nperforming linear regression. The proposed approach allows for a contribution\nto the study of differential error variances in survey methodology that would\nhave been difficult to make otherwise.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 11:38:36 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Oberski", "Daniel Leonard", ""]]}, {"id": "1304.3673", "submitter": "Peter Hoff", "authors": "Peter D. Hoff", "title": "Bayesian analysis of matrix data with rstiefel", "comments": "This is a vignette for the R-package \"rstiefel\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We illustrate the use of the R-package \"rstiefel\" for matrix-variate data\nanalysis in the context of two examples. The first example considers estimation\nof a reduced-rank mean matrix in the presence of normally distributed noise.\nThe second example considers the modeling of a social network of friendships\namong teenagers. Bayesian estimation for these models requires the ability to\nsimulate from the matrix-variate von Mises-Fisher distributions and the\nmatrix-variate Bingham distributions on the Stiefel manifold.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 16:28:41 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Hoff", "Peter D.", ""]]}, {"id": "1304.3676", "submitter": "Peter Hoff", "authors": "Peter D. Hoff", "title": "Comment on \"Bayesian Nonparametric Inference - Why and How\" by Mueller\n  and Mitra", "comments": "Invited discussion of \"Bayesian Nonparametric Inference - Why and\n  How\" by Mueller and Mitra, to appear in Bayesian Analysis, June 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to their great flexibility, nonparametric Bayes methods have proven to be\na valuable tool for discovering complicated patterns in data. The term\n\"nonparametric Bayes\" suggests that these methods inherit model-free operating\ncharacteristics of classical nonparametric methods, as well as coherent\nuncertainty assessments provided by Bayesian procedures. However, as the\nauthors say in the conclusion to their article, nonparametric Bayesian methods\nmay be more aptly described as \"massively parametric.\" Furthermore, I argue\nthat many of the default nonparametric Bayes procedures are only Bayesian in\nthe weakest sense of the term, and cannot be assumed to provide honest\nassessments of uncertainty merely because they carry the Bayesian label.\nHowever useful such procedures may be, we should be cautious about advertising\ndefault nonparametric Bayes procedures as either being \"assumption free\" or\nproviding descriptions of our uncertainty. If we want our nonparametric Bayes\nprocedures to have a Bayesian interpretation, we should modify default NP Bayes\nmethods to accommodate real prior information, or at the very least, carefully\nevaluate the effects of hyperparameters on posterior quantities of interest.\n", "versions": [{"version": "v1", "created": "Fri, 12 Apr 2013 16:41:55 GMT"}], "update_date": "2013-04-15", "authors_parsed": [["Hoff", "Peter D.", ""]]}, {"id": "1304.3760", "submitter": "Eric Bair", "authors": "Sheila Gaynor and Eric Bair", "title": "Identification of relevant subtypes via preweighted sparse clustering", "comments": "Version 4: 49 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG q-bio.QM stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster analysis methods are used to identify homogeneous subgroups in a data\nset. In biomedical applications, one frequently applies cluster analysis in\norder to identify biologically interesting subgroups. In particular, one may\nwish to identify subgroups that are associated with a particular outcome of\ninterest. Conventional clustering methods generally do not identify such\nsubgroups, particularly when there are a large number of high-variance features\nin the data set. Conventional methods may identify clusters associated with\nthese high-variance features when one wishes to obtain secondary clusters that\nare more interesting biologically or more strongly associated with a particular\noutcome of interest. A modification of sparse clustering can be used to\nidentify such secondary clusters or clusters associated with an outcome of\ninterest. This method correctly identifies such clusters of interest in several\nsimulation scenarios. The method is also applied to a large prospective cohort\nstudy of temporomandibular disorders and a leukemia microarray data set.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 02:15:20 GMT"}, {"version": "v2", "created": "Sat, 12 Oct 2013 14:14:13 GMT"}, {"version": "v3", "created": "Wed, 21 Sep 2016 23:59:53 GMT"}], "update_date": "2016-09-23", "authors_parsed": [["Gaynor", "Sheila", ""], ["Bair", "Eric", ""]]}, {"id": "1304.3769", "submitter": "Hung Hung", "authors": "Hung Hung, Yu-Tin Lin, Pengwen Chen, Chen-Chien Wang, Su-Yun Huang,\n  and Jung-Ying Tzeng", "title": "Detection of Gene-Gene Interactions by Multistage Sparse and Low-Rank\n  Regression", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A daunting challenge faced by modern biological sciences is finding an\nefficient and computationally feasible approach to deal with the curse of high\ndimensionality. The problem becomes even more severe when the research focus is\non interactions. To improve the performance, we propose a low-rank interaction\nmodel, where the interaction effects are modeled using a low-rank matrix. With\nparsimonious parameterization of interactions, the proposed model increases the\nstability and efficiency of statistical analysis. Built upon the low-rank\nmodel, we further propose an Extended Screen-and-Clean approach, based on the\nScreen and Clean (SC) method (Wasserman and Roeder, 2009; Wu et al., 2010), to\ndetect gene-gene interactions. In particular, the screening stage utilizes a\ncombination of a low-rank structure and a sparsity constraint in order to\nachieve higher power and higher selection-consistency probability. We\ndemonstrate the effectiveness of the method using simulations and apply the\nproposed procedure on the warfarin dosage study. The data analysis identified\nmain and interaction effects that would have been neglected using conventional\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 03:56:27 GMT"}], "update_date": "2013-04-16", "authors_parsed": [["Hung", "Hung", ""], ["Lin", "Yu-Tin", ""], ["Chen", "Pengwen", ""], ["Wang", "Chen-Chien", ""], ["Huang", "Su-Yun", ""], ["Tzeng", "Jung-Ying", ""]]}, {"id": "1304.3800", "submitter": "Luca Martino", "authors": "Luca Martino and David Luengo", "title": "Extremely efficient generation of Gamma random variables for \\alpha >= 1", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gamma distribution is well-known and widely used in many signal\nprocessing and communications applications. In this letter, a simple and\nextremely efficient accept/reject algorithm is introduced for the generation of\nindependent random variables from a Gamma distribution with any shape parameter\n\\alpha >= 1. The proposed method uses another Gamma distribution with integer\n\\alpha_p <= \\alpha, from which samples can be easily drawn, as proposal\nfunction. For this reason, the new technique attains a higher acceptance rate\n(AR) for \\alpha >= 3 than all the methods currently available in the\nliterature, with AR tends to 1 as \\alpha\\ diverges.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 11:31:37 GMT"}, {"version": "v2", "created": "Sun, 23 Jun 2013 15:01:39 GMT"}, {"version": "v3", "created": "Tue, 25 Jun 2013 23:14:41 GMT"}], "update_date": "2013-06-27", "authors_parsed": [["Martino", "Luca", ""], ["Luengo", "David", ""]]}, {"id": "1304.3838", "submitter": "Eric Bair", "authors": "Eric Bair", "title": "Identification of significant features in DNA microarray data", "comments": "35 pages, 6 figures. To be published in WIREs Computational\n  Statistics", "journal-ref": "WIREs Comp Stat, 2013, 5(4): 309-325", "doi": "10.1002/wics.1260", "report-no": null, "categories": "stat.ME q-bio.GN q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DNA microarrays are a relatively new technology that can simultaneously\nmeasure the expression level of thousands of genes. They have become an\nimportant tool for a wide variety of biological experiments. One of the most\ncommon goals of DNA microarray experiments is to identify genes associated with\nbiological processes of interest. Conventional statistical tests often produce\npoor results when applied to microarray data due to small sample sizes, noisy\ndata, and correlation among the expression levels of the genes. Thus, novel\nstatistical methods are needed to identify significant genes in DNA microarray\nexperiments. This article discusses the challenges inherent in DNA microarray\nanalysis and describes a series of statistical techniques that can be used to\novercome these challenges. The problem of multiple hypothesis testing and its\nrelation to microarray studies is also considered, along with several possible\nsolutions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 19:56:50 GMT"}], "update_date": "2013-07-02", "authors_parsed": [["Bair", "Eric", ""]]}, {"id": "1304.3839", "submitter": "Eric Bair", "authors": "Naomi Brownstein, Jianwen Cai, Gary Slade, and Eric Bair", "title": "Parameter estimation in Cox models with missing failure indicators and\n  the OPPERA study", "comments": "Version 4: 23 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a prospective cohort study, examining all participants for incidence of\nthe condition of interest may be prohibitively expensive. For example, the\n\"gold standard\" for diagnosing temporomandibular disorder (TMD) is a physical\nexamination by a trained clinician. In large studies, examining all\nparticipants in this manner is infeasible. Instead, it is common to use\nquestionnaires to screen for incidence of TMD and perform the \"gold standard\"\nexamination only on participants who screen positively. Unfortunately, some\nparticipants may leave the study before receiving the \"gold standard\"\nexamination. Within the framework of survival analysis, this results in missing\nfailure indicators. Motivated by the Orofacial Pain: Prospective Evaluation and\nRisk Assessment (OPPERA) study, a large cohort study of TMD, we propose a\nmethod for parameter estimation in survival models with missing failure\nindicators. We estimate the probability of being an incident case for those\nlacking a \"gold standard\" examination using logistic regression. These\nestimated probabilities are used to generate multiple imputations of case\nstatus for each missing examination that are combined with observed data in\nappropriate regression models. The variance introduced by the procedure is\nestimated using multiple imputation. The method can be used to estimate both\nregression coefficients in Cox proportional hazard models as well as incidence\nrates using Poisson regression. We simulate data with missing failure\nindicators and show that our method performs as well as or better than\ncompeting methods. Finally, we apply the proposed method to data from the\nOPPERA study.\n", "versions": [{"version": "v1", "created": "Sat, 13 Apr 2013 20:14:49 GMT"}, {"version": "v2", "created": "Mon, 29 Jul 2013 14:52:58 GMT"}, {"version": "v3", "created": "Tue, 24 Feb 2015 09:51:30 GMT"}, {"version": "v4", "created": "Tue, 7 Jul 2015 06:20:25 GMT"}], "update_date": "2015-07-08", "authors_parsed": [["Brownstein", "Naomi", ""], ["Cai", "Jianwen", ""], ["Slade", "Gary", ""], ["Bair", "Eric", ""]]}, {"id": "1304.3969", "submitter": "Alexandre Belloni", "authors": "Alexandre Belloni and Victor Chernozhukov and Ying Wei", "title": "Post-Selection Inference for Generalized Linear Models with Many\n  Controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME econ.EM math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers generalized linear models in the presence of many\ncontrols. We lay out a general methodology to estimate an effect of interest\nbased on the construction of an instrument that immunize against model\nselection mistakes and apply it to the case of logistic binary choice model.\nMore specifically we propose new methods for estimating and constructing\nconfidence regions for a regression parameter of primary interest $\\alpha_0$, a\nparameter in front of the regressor of interest, such as the treatment variable\nor a policy variable. These methods allow to estimate $\\alpha_0$ at the\nroot-$n$ rate when the total number $p$ of other regressors, called controls,\npotentially exceed the sample size $n$ using sparsity assumptions. The sparsity\nassumption means that there is a subset of $s<n$ controls which suffices to\naccurately approximate the nuisance part of the regression function.\nImportantly, the estimators and these resulting confidence regions are valid\nuniformly over $s$-sparse models satisfying $s^2\\log^2 p = o(n)$ and other\ntechnical conditions. These procedures do not rely on traditional consistent\nmodel selection arguments for their validity. In fact, they are robust with\nrespect to moderate model selection mistakes in variable selection. Under\nsuitable conditions, the estimators are semi-parametrically efficient in the\nsense of attaining the semi-parametric efficiency bounds for the class of\nmodels in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 02:20:57 GMT"}, {"version": "v2", "created": "Mon, 5 Aug 2013 20:27:11 GMT"}, {"version": "v3", "created": "Mon, 21 Mar 2016 15:44:23 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Wei", "Ying", ""]]}, {"id": "1304.4056", "submitter": "Bo Jiang", "authors": "Bo Jiang, Jun S. Liu", "title": "Variable selection for general index models via sliced inverse\n  regression", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1233 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2014, Vol. 42, No. 5, 1751-1786", "doi": "10.1214/14-AOS1233", "report-no": "IMS-AOS-AOS1233", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable selection, also known as feature selection in machine learning,\nplays an important role in modeling high dimensional data and is key to\ndata-driven scientific discoveries. We consider here the problem of detecting\ninfluential variables under the general index model, in which the response is\ndependent of predictors through an unknown function of one or more linear\ncombinations of them. Instead of building a predictive model of the response\ngiven combinations of predictors, we model the conditional distribution of\npredictors given the response. This inverse modeling perspective motivates us\nto propose a stepwise procedure based on likelihood-ratio tests, which is\neffective and computationally efficient in identifying important variables\nwithout specifying a parametric relationship between predictors and the\nresponse. For example, the proposed procedure is able to detect variables with\npairwise, three-way or even higher-order interactions among $p$ predictors with\na computational time of $O(p)$ instead of $O(p^k)$ (with $k$ being the highest\norder of interactions). Its excellent empirical performance in comparison with\nexisting methods is demonstrated through simulation studies as well as real\ndata examples. Consistency of the variable selection procedure when both the\nnumber of predictors and the sample size go to infinity is established.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 11:33:45 GMT"}, {"version": "v2", "created": "Fri, 1 Nov 2013 02:38:21 GMT"}, {"version": "v3", "created": "Fri, 9 May 2014 03:26:26 GMT"}, {"version": "v4", "created": "Tue, 23 Sep 2014 12:16:35 GMT"}], "update_date": "2014-09-24", "authors_parsed": [["Jiang", "Bo", ""], ["Liu", "Jun S.", ""]]}, {"id": "1304.4077", "submitter": "Pritam Ranjan", "authors": "Reshu Agarwal, Pritam Ranjan, Hugh Chipman", "title": "A new Bayesian ensemble of trees classifier for identifying multi-class\n  labels in satellite images", "comments": "31 pages, 6 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification of satellite images is a key component of many remote sensing\napplications. One of the most important products of a raw satellite image is\nthe classified map which labels the image pixels into meaningful classes.\nThough several parametric and non-parametric classifiers have been developed\nthus far, accurate labeling of the pixels still remains a challenge. In this\npaper, we propose a new reliable multiclass-classifier for identifying class\nlabels of a satellite image in remote sensing applications. The proposed\nmulticlass-classifier is a generalization of a binary classifier based on the\nflexible ensemble of regression trees model called Bayesian Additive Regression\nTrees (BART). We used three small areas from the LANDSAT 5 TM image, acquired\non August 15, 2009 (path/row: 08/29, L1T product, UTM map projection) over\nKings County, Nova Scotia, Canada to classify the land-use. Several prediction\naccuracy and uncertainty measures have been used to compare the reliability of\nthe proposed classifier with the state-of-the-art classifiers in remote\nsensing.\n", "versions": [{"version": "v1", "created": "Mon, 15 Apr 2013 12:54:52 GMT"}, {"version": "v2", "created": "Fri, 31 May 2013 16:57:33 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Agarwal", "Reshu", ""], ["Ranjan", "Pritam", ""], ["Chipman", "Hugh", ""]]}, {"id": "1304.4300", "submitter": "Mohamed Chaouch", "authors": "Mohamed Chaouch and Na\\^amane La\\\"ib", "title": "Nonparametric Multivariate L1-median Regression Estimation with\n  Functional Covariates", "comments": "34 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a nonparametric estimator is proposed for estimating the\nL1-median for multivariate conditional distribution when the covariates take\nvalues in an infinite dimensional space. The multivariate case is more\nappropriate to predict the components of a vector of random variables\nsimultaneously rather than predicting each of them separately. While estimating\nthe conditional L1-median function using the well-known Nadarya-Waston\nestimator, we establish the strong consistency of this estimator as well as the\nasymptotic normality. We also present some simulations and provide how to built\nconditional con?fidence ellipsoids for the multivariate L1-median regression in\npractice. Some numerical study in chemiometrical real data are carried out to\ncompare the multivariate L1-median regression with the vector of marginal\nmedian regression when the covariate X is a curve as well as X is a random\nvector.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 00:05:29 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Chaouch", "Mohamed", ""], ["La\u00efb", "Na\u00e2mane", ""]]}, {"id": "1304.4302", "submitter": "Seyed Hamed Alemohammad", "authors": "Seyed Hamed Alemohammad, Reza Ardakanian, Akbar Karimi", "title": "A Framework for Modelling Probabilistic Uncertainty in Rainfall Scenario\n  Analysis", "comments": "8 Pages, 7 Figures, apeared in Advances in Hydro-Science and\n  Engineering Volume VIII, Nagoya Hydraulic Research Institute for River Basin\n  Management (NHRI), pp. 472-479, Nagoya, Japan", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting future probable values of model parameters, is an essential\npre-requisite for assessing model decision reliability in an uncertain\nenvironment. Scenario Analysis is a methodology for modelling uncertainty in\nwater resources management modelling. Uncertainty if not considered\nappropriately in decision making will decrease reliability of decisions,\nespecially in long-term planning. One of the challenges in Scenario Analysis is\nhow scenarios are made. One of the most approved methods is statistical\nmodelling based on Auto-Regressive models. Stream flow future scenarios in\ndeveloped basins that human has made changes to the natural flow process could\nnot be generated directly by ARMA modelling. In this case, making scenarios for\nmonthly rainfall and using it in a water resources system model makes more\nsense. Rainfall is an ephemeral process which has zero values in some months\nwhich introduces some limitations in making use of monthly ARMA model.\nTherefore, a two stage modelling approach is adopted here which in the first\nstage yearly modelling is done. Within this yearly model three ranges are\nidentified: Dry, Normal and Wet. In the normal range yearly ARMA modelling is\nused. Dry and Wet range are considered as random processes and are modeled by\nfrequency analysis. Monthly distribution of rainfall, which is extracted from\navailable data from a moving average are considered to be deterministic and\nfixed in time. Each rainfall scenario is composed of a yearly ARMA process\nsuper-imposed by dry and wet events according to the frequency analysis. This\nmodelling framework is applied to available data from three rain-gauge stations\nin Iran. Results show this modelling approach has better consistency with\nobserved data in comparison with making use of ARMA modelling alone.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 00:14:40 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Alemohammad", "Seyed Hamed", ""], ["Ardakanian", "Reza", ""], ["Karimi", "Akbar", ""]]}, {"id": "1304.4304", "submitter": "Mohamed Chaouch", "authors": "Mohamed Chaouch and Salah Khardani", "title": "Kernel-smoothed conditional quantiles of randomly censored functional\n  stationary ergodic data", "comments": "26 pages, 4 figures. arXiv admin note: text overlap with\n  arXiv:1211.2780 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper, investigates the conditional quantile estimation of a scalar\nrandom response and a functional random covariate (i.e. valued in some\ninfinite-dimensional space) whenever {\\it functional stationary ergodic data\nwith random censorship} are considered. We introduce a kernel type estimator of\nthe conditional quantile function. We establish the strong consistency with\nrate of this estimator as well as the asymptotic normality which induces a\nconfidence interval that is usable in practice since it does not depend on any\nunknown quantity. An application to electricity peak demand interval prediction\nwith censored smart meter data is carried out to show the performance of the\nproposed estimator.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 00:25:41 GMT"}], "update_date": "2013-04-17", "authors_parsed": [["Chaouch", "Mohamed", ""], ["Khardani", "Salah", ""]]}, {"id": "1304.4564", "submitter": "M{\\aa}ns Thulin", "authors": "M{\\aa}ns Thulin", "title": "A high-dimensional two-sample test for the mean using random subspaces", "comments": null, "journal-ref": "Computational Statistics & Data Analysis, 74, 26-38 (2014)", "doi": "10.1016/j.csda.2013.12.003", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in genetics is that of testing whether a set of highly\ndependent gene expressions differ between two populations, typically in a\nhigh-dimensional setting where the data dimension is larger than the sample\nsize. Most high-dimensional tests for the equality of two mean vectors rely on\nnaive diagonal or trace estimators of the covariance matrix, ignoring\ndependencies between variables. A test recently proposed by Lopes et al. (2012)\nimplicitly incorporates dependencies by using random pseudo-projections to a\nlower-dimensional space. Their test offers higher power when the variables are\ndependent, but lacks desirable invariance properties and relies on asymptotic\np-values that are too conservative. We illustrate how a permutation approach\ncan be used to obtain p-values for the Lopes et al. test and how modifying the\ntest using random subspaces leads to a test statistic that is invariant under\nlinear transformations of the marginal distributions. The resulting test does\nnot rely on assumptions about normality or the structure of the covariance\nmatrix. We show by simulation that the new test has higher power than competing\ntests in realistic settings motivated by microarray gene expression data. We\nalso discuss the computational aspects of high-dimensional permutation tests\nand provide an efficient R implementation of the proposed test.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 19:18:43 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Thulin", "M\u00e5ns", ""]]}, {"id": "1304.4637", "submitter": "Shawn Mankad", "authors": "Runlong Tang, Moulinath Banerjee, George Michailidis, and Shawn Mankad", "title": "Two-Stage Plans for Estimating a Threshold Value of a Regression\n  Function", "comments": "35 pages (including 7 pages of supplementary material), 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates two-stage plans based on nonparametric procedures for\nestimating an inverse regression function at a given point. Specifically,\nisotonic regression is used at stage one to obtain an initial estimate followed\nby another round of isotonic regression in the vicinity of this estimate at\nstage two. It is shown that such two stage plans accelerate the convergence\nrate of one-stage procedures and are superior to existing two-stage procedures\nthat use local parametric approximations at stage two when the available budget\nis moderate and/or the regression function is 'ill-behaved'. Both Wald and\nLikelihood Ratio type confidence intervals for the threshold value of interest\nare investigated and the latter are recommended in applications due to their\nsimplicity and robustness. The developed plans are illustrated through a\ncomprehensive simulation study and an application to car fuel efficiency data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Apr 2013 22:21:58 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Tang", "Runlong", ""], ["Banerjee", "Moulinath", ""], ["Michailidis", "George", ""], ["Mankad", "Shawn", ""]]}, {"id": "1304.4654", "submitter": "Arend Voorman", "authors": "Arend Voorman, Ali Shojaie, and Daniela Witten", "title": "Graph estimation with joint additive models", "comments": "submitted to Biometrika. Method implementation available at\n  http://cran.r-project.org/web/packages/spacejam/", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been considerable interest in estimating\nconditional independence graphs in the high-dimensional setting. Most prior\nwork has assumed that the variables are multivariate Gaussian, or that the\nconditional means of the variables are linear. Unfortunately, if these\nassumptions are violated, then the resulting conditional independence estimates\ncan be inaccurate. We present a semi-parametric method, SpaCE JAM, which allows\nthe conditional means of the features to take on an arbitrary additive form. We\npresent an efficient algorithm for its computation, and prove that our\nestimator is consistent. We also extend our method to estimation of directed\ngraphs with known causal ordering. Using simulated data, we show that SpaCE JAM\nenjoys superior performance to existing methods when there are non-linear\nrelationships among the features, and is comparable to methods that assume\nmultivariate normality when the conditional means are linear. We illustrate our\nmethod on a cell-signaling data set.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 00:40:00 GMT"}, {"version": "v2", "created": "Thu, 18 Apr 2013 19:17:34 GMT"}], "update_date": "2013-04-19", "authors_parsed": [["Voorman", "Arend", ""], ["Shojaie", "Ali", ""], ["Witten", "Daniela", ""]]}, {"id": "1304.4786", "submitter": "Pedro Galeano", "authors": "Esdras Joseph, Pedro Galeano and Rosa E. Lillo", "title": "The Mahalanobis distance for functional data with applications to\n  classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a general notion of Mahalanobis distance for functional\ndata that extends the classical multivariate concept to situations where the\nobserved data are points belonging to curves generated by a stochastic process.\nMore precisely, a new semi-distance for functional observations that generalize\nthe usual Mahalanobis distance for multivariate datasets is introduced. For\nthat, the development uses a regularized square root inverse operator in\nHilbert spaces. Some of the main characteristics of the functional Mahalanobis\nsemi-distance are shown. Afterwards, new versions of several well known\nfunctional classification procedures are developed using the Mahalanobis\ndistance for functional data as a measure of proximity between functional\nobservations. The performance of several well known functional classification\nprocedures are compared with those methods used in conjunction with the\nMahalanobis distance for functional data, with positive results, through a\nMonte Carlo study and the analysis of two real data examples.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 12:13:58 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Joseph", "Esdras", ""], ["Galeano", "Pedro", ""], ["Lillo", "Rosa E.", ""]]}, {"id": "1304.4851", "submitter": "Jin Liu Jin Liu", "authors": "Jin Liu, Jian Huang, Yawei Zhang, Qing Lan, Nathaniel Rothman,\n  Tongzhang Zheng and Shuangge Ma", "title": "Integrative Analysis of Prognosis Data on Multiple Cancer Subtypes using\n  Penalization", "comments": "23 pages (main text) 17 pages (appendix), 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cancer research, profiling studies have been extensively conducted,\nsearching for genes/SNPs associated with prognosis. Cancer is a heterogeneous\ndisease. Examining similarity and difference in the genetic basis of multiple\nsubtypes of the same cancer can lead to better understanding of their\nconnections and distinctions. Classic meta-analysis approaches analyze each\nsubtype separately and then compare analysis results across subtypes.\nIntegrative analysis approaches, in contrast, analyze the raw data on multiple\nsubtypes simultaneously and can outperform meta-analysis. In this study,\nprognosis data on multiple subtypes of the same cancer are analyzed. An AFT\n(accelerated failure time) model is adopted to describe survival. The genetic\nbasis of multiple subtypes is described using the heterogeneity model, which\nallows a gene/SNP to be associated with the prognosis of some subtypes but not\nthe others. A compound penalization approach is developed to conduct gene-level\nanalysis and identify genes that contain important SNPs associated with\nprognosis. The proposed approach has an intuitive formulation and can be\nrealized using an iterative algorithm. Asymptotic properties are rigorously\nestablished. Simulation shows that the proposed approach has satisfactory\nperformance and outperforms meta-analysis using penalization. An NHL\n(non-Hodgkin lymphoma) prognosis study with SNP measurements is analyzed. Genes\nassociated with the three major subtypes, namely DLBCL, FL, and CLL/SLL, are\nidentified. The proposed approach identifies genes different from alternative\nanalysis and has reasonable prediction performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 15:11:09 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Liu", "Jin", ""], ["Huang", "Jian", ""], ["Zhang", "Yawei", ""], ["Lan", "Qing", ""], ["Rothman", "Nathaniel", ""], ["Zheng", "Tongzhang", ""], ["Ma", "Shuangge", ""]]}, {"id": "1304.4890", "submitter": "Dabao Zhang", "authors": "Yanzhu Lin, Min Zhang, Dabao Zhang", "title": "Generalized Orthogonal Components Regression for High Dimensional\n  Generalized Linear Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Here we propose an algorithm, named generalized orthogonal components\nregression (GOCRE), to explore the relationship between a categorical outcome\nand a set of massive variables. A set of orthogonal components are sequentially\nconstructed to account for the variation of the categorical outcome, and\ntogether build up a generalized linear model (GLM). This algorithm can be\nconsidered as an extension of the partial least squares (PLS) for GLMs, but\novercomes several issues of existing extensions based on iteratively reweighted\nleast squares (IRLS). First, existing extensions construct a different set of\ncomponents at each iteration and thus cannot provide a convergent set of\ncomponents. Second, existing extensions are computationally intensive because\nof repetitively constructing a full set of components. Third, although they\npursue the convergence of regression coefficients, the resultant regression\ncoefficients may still diverge especially when building logistic regression\nmodels. GOCRE instead sequentially builds up each orthogonal component upon\nconvergent construction, and simultaneously regresses against these orthogonal\ncomponents to fit the GLM. The performance of the new method is demonstrated by\nboth simulation studies and a real data example.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 17:04:10 GMT"}], "update_date": "2013-04-18", "authors_parsed": [["Lin", "Yanzhu", ""], ["Zhang", "Min", ""], ["Zhang", "Dabao", ""]]}, {"id": "1304.4920", "submitter": "Jonathan Roseblatt", "authors": "Jonathan Rosenblatt", "title": "A Practitioner's Guide to Multiple Testing Error Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is quite common in modern research, for a researcher to test many\nhypotheses. The statistical (frequentist) hypothesis testing framework, does\nnot scale with the number of hypotheses in the sense that naively performing\nmany hypothesis tests will probably yield many false findings. Indeed,\nstatistical \"significance\" is evidence for the presence of a signal within the\nnoise expected in a single test, not in a multitude. In order to protect\nhimself from an uncontrolled number of erroneous findings, a researcher has to\nconsider of the type or errors he wishes to avoid and select the adequate\nprocedure for that particular error type and data structure. A quick search of\nthe tag [multiple-comparisons] in the statistics Questions & Answers web site\nCross Validates (http://stats.stackexchange.com) demonstrates the amount of\nconfusion this task can actually cause. This was also a point made at the 2009\nMultiple Comparisons conference in Tokyo. In an attempt to offer guidance, we\nreview possible error types for multiple testing, and demonstrate them with\nsome practical examples, which clarify the formalism. Finally, we include some\nnotes on the software implementations of the methods discussed.\n  The emphasis of this manuscript is on the error-rates, and not on the\nprocedures themselves. We do try to name several procedures in this manuscript\nwhere appropriate. P-value adjustment will not be discussed as it is procedure\nspecific. I.e., it is the choice of a procedure that defines the p-value\nadjustment, and not the error rate itself. Simultaneous confidence intervals\nwill, also, not be discussed.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 19:07:05 GMT"}, {"version": "v2", "created": "Sun, 26 May 2013 12:09:04 GMT"}, {"version": "v3", "created": "Mon, 24 Jun 2013 22:54:12 GMT"}], "update_date": "2013-06-26", "authors_parsed": [["Rosenblatt", "Jonathan", ""]]}, {"id": "1304.4983", "submitter": "Qing Mai", "authors": "Qing Mai and Hui Zou", "title": "Semiparametric Sparse Discriminant Analysis", "comments": "34 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a considerable amount of work has been devoted to\ngeneralizing linear discriminant analysis to overcome its incompetence for\nhigh-dimensional classification (Witten & Tibshirani 2011, Cai & Liu 2011, Mai\net al. 2012, Fan et al. 2012). In this paper, we develop high-dimensional\nsemiparametric sparse discriminant analysis (HD-SeSDA) that generalizes the\nnormal-theory discriminant analysis in two ways: it relaxes the Gaussian\nassumptions and can handle non-polynomial (NP) dimension classification\nproblems. If the underlying Bayes rule is sparse, HD-SeSDA can estimate the\nBayes rule and select the true features simultaneously with overwhelming\nprobability, as long as the logarithm of dimension grows slower than the cube\nroot of sample size. Simulated and real examples are used to demonstrate the\nfinite sample performance of HD-SeSDA. At the core of the theory is a new\nexponential concentration bound for semiparametric Gaussian copulas, which is\nof independent interest.\n", "versions": [{"version": "v1", "created": "Wed, 17 Apr 2013 22:30:27 GMT"}, {"version": "v2", "created": "Wed, 14 Jan 2015 15:26:36 GMT"}], "update_date": "2015-01-15", "authors_parsed": [["Mai", "Qing", ""], ["Zou", "Hui", ""]]}, {"id": "1304.5453", "submitter": "Luigi Salmaso", "authors": "Livio Corain, Luigi Salmaso", "title": "A permutation approach for ranking of multivariate populations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The subject of this paper is to introduce a novel permutation-based\nnonparametric approach for the problem of ranking several multivariate\npopulations with respect to both experimental and observation studies to be\nreferred to the most useful design such as MANOVA (multivariate independent\nsamples) and MRCB (multivariate randomized complete block design, i.e.\nmultivariate dependent samples also known as repeated measures). This topic is\nnot only of theoretical interest but also have a practical relevance,\nespecially to business and industrial research where a reliable global ranking\nin terms of performance of all investigated products/prototypes is a very\nnatural goal. In fact, the need to define an appropriate ranking of items\n(products, services, teaching courses, degree programs, and so on) is very\ncommon in both experimental and observational studies within the areas of\nbusiness and industrial research.\n", "versions": [{"version": "v1", "created": "Fri, 19 Apr 2013 15:50:11 GMT"}], "update_date": "2013-04-22", "authors_parsed": [["Corain", "Livio", ""], ["Salmaso", "Luigi", ""]]}, {"id": "1304.5637", "submitter": "Hua Zhou", "authors": "Xiaoshan Li and Hua Zhou and Lexin Li", "title": "Tucker Tensor Regression and Neuroimaging Analysis", "comments": "CP decomposition; magnetic resonance image; tensor regression; Tucker\n  decomposition", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Large-scale neuroimaging studies have been collecting brain images of study\nindividuals, which take the form of two-dimensional, three-dimensional, or\nhigher dimensional arrays, also known as tensors. Addressing scientific\nquestions arising from such data demands new regression models that take\nmultidimensional arrays as covariates. Simply turning an image array into a\nlong vector causes extremely high dimensionality that compromises classical\nregression methods, and, more seriously, destroys the inherent spatial\nstructure of array data that possesses wealth of information. In this article,\nwe propose a family of generalized linear tensor regression models based upon\nthe Tucker decomposition of regression coefficient arrays. Effectively\nexploiting the low rank structure of tensor covariates brings the ultrahigh\ndimensionality to a manageable level that leads to efficient estimation. We\ndemonstrate, both numerically that the new model could provide a sound recovery\nof even high rank signals, and asymptotically that the model is consistently\nestimating the best Tucker structure approximation to the full array model in\nthe sense of Kullback-Liebler distance. The new model is also compared to a\nrecently proposed tensor regression model that relies upon an alternative\nCANDECOMP/PARAFAC (CP) decomposition.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 15:04:08 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Li", "Xiaoshan", ""], ["Zhou", "Hua", ""], ["Li", "Lexin", ""]]}, {"id": "1304.5642", "submitter": "Emily Fox", "authors": "Sivan Aldor-Noiman, Lawrence D. Brown, Emily B. Fox, and Robert A.\n  Stine", "title": "Spatio-Temporal Low Count Processes with Application to Violent Crime\n  Events", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is significant interest in being able to predict where crimes will\nhappen, for example to aid in the efficient tasking of police and other\nprotective measures. We aim to model both the temporal and spatial dependencies\noften exhibited by violent crimes in order to make such predictions. The\ntemporal variation of crimes typically follows patterns familiar in time series\nanalysis, but the spatial patterns are irregular and do not vary smoothly\nacross the area. Instead we find that spatially disjoint regions exhibit\ncorrelated crime patterns. It is this indeterminate inter-region correlation\nstructure along with the low-count, discrete nature of counts of serious crimes\nthat motivates our proposed forecasting tool. In particular, we propose to\nmodel the crime counts in each region using an integer-valued first order\nautoregressive process. We take a Bayesian nonparametric approach to flexibly\ndiscover a clustering of these region-specific time series. We then describe\nhow to account for covariates within this framework. Both approaches adjust for\nseasonality. We demonstrate our approach through an analysis of weekly reported\nviolent crimes in Washington, D.C. between 2001-2008. Our forecasts outperform\nstandard methods while additionally providing useful tools such as prediction\nintervals.\n", "versions": [{"version": "v1", "created": "Sat, 20 Apr 2013 15:46:32 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Aldor-Noiman", "Sivan", ""], ["Brown", "Lawrence D.", ""], ["Fox", "Emily B.", ""], ["Stine", "Robert A.", ""]]}, {"id": "1304.5747", "submitter": "Le-Yu Chen", "authors": "Le-Yu Chen, Sokbae Lee, Myung Jae Sung", "title": "Maximum Score Estimation of Preference Parameters for a Binary Choice\n  Model under Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops maximum score estimation of preference parameters in the\nbinary choice model under uncertainty in which the decision rule is affected by\nconditional expectations. The preference parameters are estimated in two\nstages: we estimate conditional expectations nonparametrically in the first\nstage and then the preference parameters in the second stage based on Manski\n(1975, 1985)'s maximum score estimator using the choice data and first stage\nestimates. The paper establishes consistency and derives rate of convergence of\nthe two-stage maximum score estimator. Moreover, the paper also provides\nsufficient conditions under which the two-stage estimator is asymptotically\nequivalent in distribution to the corresponding single-stage estimator that\nassumes the first stage input is known. These results are of independent\ninterest for maximum score estimation with nonparametrically generated\nregressors. The paper also presents some Monte Carlo simulation results for\nfinite-sample behavior of the two-stage estimator.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2013 15:10:28 GMT"}, {"version": "v2", "created": "Mon, 2 Dec 2013 16:24:15 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Chen", "Le-Yu", ""], ["Lee", "Sokbae", ""], ["Sung", "Myung Jae", ""]]}, {"id": "1304.5768", "submitter": "Pierre E. Jacob", "authors": "Arnaud Doucet (University of Oxford), Pierre E. Jacob (Harvard\n  University) and Sylvain Rubenthaler (Universit\\'e de Nice-Sophia Antipolis)", "title": "Derivative-Free Estimation of the Score Vector and Observed Information\n  Matrix with Application to State-Space Models", "comments": "Technical report, 43 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ionides, King et al. (see e.g. Inference for nonlinear dynamical systems,\nPNAS 103) have recently introduced an original approach to perform maximum\nlikelihood parameter estimation in state-space models which only requires being\nable to simulate the latent Markov model according to its prior distribution.\nTheir methodology relies on an approximation of the score vector for general\nstatistical models based upon an artificial posterior distribution and bypasses\nthe calculation of any derivative. We show here that this score estimator can\nbe derived from a simple application of Stein's lemma and how an additional\napplication of this lemma provides an original derivative-free estimator of the\nobserved information matrix. We establish that these estimators exhibit\nrobustness properties compared to finite difference estimators while their bias\nand variance scale as well as finite difference type estimators, including\nsimultaneous perturbations (see e.g. Spall, IEEE Trans. on Automatic Control\n37), with respect to the dimension of the parameter. For state-space models\nwhere sequential Monte Carlo computation is required, these estimators can be\nfurther improved. In this specific context, we derive original derivative-free\nestimators of the score vector and observed information matrix which are\ncomputed using sequential Monte Carlo approximations of smoothed additive\nfunctionals associated with a modified version of the original state-space\nmodel.\n", "versions": [{"version": "v1", "created": "Sun, 21 Apr 2013 17:25:25 GMT"}, {"version": "v2", "created": "Tue, 30 Apr 2013 09:19:59 GMT"}, {"version": "v3", "created": "Sun, 12 Jul 2015 17:22:13 GMT"}], "update_date": "2015-07-14", "authors_parsed": [["Doucet", "Arnaud", "", "University of Oxford"], ["Jacob", "Pierre E.", "", "Harvard\n  University"], ["Rubenthaler", "Sylvain", "", "Universit\u00e9 de Nice-Sophia Antipolis"]]}, {"id": "1304.5974", "submitter": "Kevin Xu", "authors": "Kevin S. Xu and Alfred O. Hero III", "title": "Dynamic stochastic blockmodels: Statistical models for time-evolving\n  networks", "comments": null, "journal-ref": "Proceedings of the 6th International Conference on Social\n  Computing, Behavioral-Cultural Modeling, and Prediction (2013) 201-210", "doi": "10.1007/978-3-642-37210-0_22", "report-no": null, "categories": "cs.SI cs.LG physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Significant efforts have gone into the development of statistical models for\nanalyzing data in the form of networks, such as social networks. Most existing\nwork has focused on modeling static networks, which represent either a single\ntime snapshot or an aggregate view over time. There has been recent interest in\nstatistical modeling of dynamic networks, which are observed at multiple points\nin time and offer a richer representation of many complex phenomena. In this\npaper, we propose a state-space model for dynamic networks that extends the\nwell-known stochastic blockmodel for static networks to the dynamic setting. We\nthen propose a procedure to fit the model using a modification of the extended\nKalman filter augmented with a local search. We apply the procedure to analyze\na dynamic social network of email communication.\n", "versions": [{"version": "v1", "created": "Mon, 22 Apr 2013 15:07:19 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Xu", "Kevin S.", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1304.6309", "submitter": "Jay Bartroff", "authors": "Jay Bartroff and Jinlin Song", "title": "Sequential Tests of Multiple Hypotheses Controlling Type I and II\n  Familywise Error Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the following general scenario: A scientist wishes to\nperform a battery of experiments, each generating a sequential stream of data,\nto investigate some phenomenon. The scientist would like to control the overall\nerror rate in order to draw statistically-valid conclusions from each\nexperiment, while being as efficient as possible. The between-stream data may\ndiffer in distribution and dimension but also may be highly correlated, even\nduplicated exactly in some cases. Treating each experiment as a hypothesis test\nand adopting the familywise error rate (FWER) metric, we give a procedure that\nsequentially tests each hypothesis while controlling both the type I and II\nFWERs regardless of the between-stream correlation, and only requires arbitrary\nsequential test statistics that control the error rates for a given stream in\nisolation. The proposed procedure, which we call the sequential Holm procedure\nbecause of its inspiration from Holm's (1979) seminal fixed-sample procedure,\nshows simultaneous savings in expected sample size and less conservative error\ncontrol relative to fixed sample, sequential Bonferroni, and other recently\nproposed sequential procedures in a simulation study.\n", "versions": [{"version": "v1", "created": "Tue, 23 Apr 2013 14:49:58 GMT"}, {"version": "v2", "created": "Fri, 6 Sep 2013 18:04:12 GMT"}, {"version": "v3", "created": "Thu, 8 May 2014 21:39:28 GMT"}], "update_date": "2014-05-12", "authors_parsed": [["Bartroff", "Jay", ""], ["Song", "Jinlin", ""]]}, {"id": "1304.6478", "submitter": "Miguel \\'A. Carreira-Perpi\\~n\\'an", "authors": "Miguel \\'A. Carreira-Perpi\\~n\\'an and Weiran Wang", "title": "The K-modes algorithm for clustering", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many clustering algorithms exist that estimate a cluster centroid, such as\nK-means, K-medoids or mean-shift, but no algorithm seems to exist that clusters\ndata by returning exactly K meaningful modes. We propose a natural definition\nof a K-modes objective function by combining the notions of density and cluster\nassignment. The algorithm becomes K-means and K-medoids in the limit of very\nlarge and very small scales. Computationally, it is slightly slower than\nK-means but much faster than mean-shift or K-medoids. Unlike K-means, it is\nable to find centroids that are valid patterns, truly representative of a\ncluster, even with nonconvex clusters, and appears robust to outliers and\nmisspecification of the scale and number of clusters.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 03:59:39 GMT"}], "update_date": "2013-04-25", "authors_parsed": [["Carreira-Perpi\u00f1\u00e1n", "Miguel \u00c1.", ""], ["Wang", "Weiran", ""]]}, {"id": "1304.6715", "submitter": "Lorenzo Rimoldini", "authors": "Lorenzo Rimoldini", "title": "Skewness and kurtosis unbiased by Gaussian uncertainties", "comments": "36 pages, 16 figures; v3: removed K4, M4 in captions of Figs 9-16.\n  arXiv admin note: substantial text overlap with arXiv:1304.6564", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noise is an unavoidable part of most measurements which can hinder a correct\ninterpretation of the data. Uncertainties propagate in the data analysis and\ncan lead to biased results even in basic descriptive statistics such as the\ncentral moments and cumulants. Expressions of noise-unbiased estimates of\ncentral moments and cumulants up to the fourth order are presented under the\nassumption of independent Gaussian uncertainties, for weighted and unweighted\nstatistics. These results are expected to be relevant for applications of the\nskewness and kurtosis estimators such as outlier detections, normality tests\nand in automated classification procedures. The comparison of estimators\ncorrected and not corrected for noise biases is illustrated with simulations as\na function of signal-to-noise ratio, employing different sample sizes and\nweighting schemes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Apr 2013 13:05:29 GMT"}, {"version": "v2", "created": "Sun, 28 Apr 2013 20:41:13 GMT"}, {"version": "v3", "created": "Thu, 20 Mar 2014 19:31:02 GMT"}], "update_date": "2014-06-25", "authors_parsed": [["Rimoldini", "Lorenzo", ""]]}, {"id": "1304.6949", "submitter": "Geir-Arne Fuglstad", "authors": "Geir-Arne Fuglstad, Finn Lindgren, Daniel Simpson and H{\\aa}vard Rue", "title": "Exploring a New Class of Non-stationary Spatial Gaussian Random Fields\n  with Varying Local Anisotropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian random fields (GRFs) constitute an important part of spatial\nmodelling, but can be computationally infeasible for general covariance\nstructures. An efficient approach is to specify GRFs via stochastic partial\ndifferential equations (SPDEs) and derive Gaussian Markov random field (GMRF)\napproximations of the solutions. We consider the construction of a class of\nnon-stationary GRFs with varying local anisotropy, where the local anisotropy\nis introduced by allowing the coefficients in the SPDE to vary with position.\nThis is done by using a form of diffusion equation driven by Gaussian white\nnoise with a spatially varying diffusion matrix. This allows for the\nintroduction of parameters that control the GRF by parametrizing the diffusion\nmatrix. These parameters and the GRF may be considered to be part of a\nhierarchical model and the parameters estimated in a Bayesian framework. The\nresults show that the use of an SPDE with non-constant coefficients is a\npromising way of creating non-stationary spatial GMRFs that allow for physical\ninterpretability of the parameters, although there are several remaining\nchallenges that would need to be solved before these models can be put to\ngeneral practical use.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 16:05:43 GMT"}, {"version": "v2", "created": "Fri, 25 Apr 2014 10:02:09 GMT"}], "update_date": "2016-08-11", "authors_parsed": [["Fuglstad", "Geir-Arne", ""], ["Lindgren", "Finn", ""], ["Simpson", "Daniel", ""], ["Rue", "H\u00e5vard", ""]]}, {"id": "1304.6950", "submitter": "Lutz Duembgen", "authors": "Lutz Duembgen and Ehsan Zamanzade", "title": "Inference on a Distribution Function from Ranked Set Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider independent observations $(X_1,R_1)$, $(X_2,R_2)$, \\ldots,\n$(X_n,R_n)$ with random or fixed ranks $R_i \\in \\{1,2,\\ldots,k\\}$, while\nconditional on $R_i = r$, the random variable $X_i$ has the same distribution\nas the $r$-th order statistic within a random sample of size $k$ from an\nunknown continuous distribution function $F$. Such observation schemes are\nutilized in situations in which ranking observations is much easier than\nobtaining their precise values. Two well-known special cases are ranked set\nsampling (McIntyre 1952) and judgement post-stratification (MacEachern et al.\n2004).\n  Within a general setting including unbalanced ranked set sampling we derive\nand compare the asymptotic distributions of three different estimators of the\ndistribution function $F$ as $n \\to \\infty$ with fixed $k$: The stratified\nestimator of Stokes and Sager (1988), the nonparametric maximum-likelihood\nestimator of Kvam and Samaniego (1994) and a moment-based estimator of Chen\n(2001). Our functional central limit theorems generalize and refine previous\nasymptotic analyses. In addition we discuss briefly pointwise and simultaneous\nconfidence intervals for the distribution function $F$ with guaranteed coverage\nprobability for finite sample sizes.\n  The methods are illustrated with a real data example, and the potential\nimpact of imperfect rankings is investigated in a small simulation experiment.\nAll in all, the moment-based estimator seems to offer a good compromise between\nefficiency and robustness versus imperfect ranking, in addition to\ncomputational efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 25 Apr 2013 16:06:00 GMT"}, {"version": "v2", "created": "Fri, 26 Apr 2013 04:21:54 GMT"}, {"version": "v3", "created": "Mon, 14 Oct 2013 15:05:55 GMT"}, {"version": "v4", "created": "Fri, 9 Dec 2016 13:23:42 GMT"}, {"version": "v5", "created": "Mon, 2 Jul 2018 13:25:52 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Duembgen", "Lutz", ""], ["Zamanzade", "Ehsan", ""]]}, {"id": "1304.7198", "submitter": "Chris A. J. Klaassen", "authors": "Chris A.J. Klaassen", "title": "Evidential Value in ANOVA Results in Favor of Fabrication", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Some scientific publications are under suspicion of fabrication of data.\nSince humans are bad random number generators, there might be some evidential\nvalue in favor of fabrication in the statistical results as presented in such\npapers. In line with Uri Simonsohn (2012, 2013) we study the evidential value\nof the results of an ANOVA study in favor of the hypothesis of a dependence\nstructure in the underlying data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 15:11:56 GMT"}], "update_date": "2013-04-29", "authors_parsed": [["Klaassen", "Chris A. J.", ""]]}, {"id": "1304.7406", "submitter": "Dean Eckles", "authors": "Eytan Bakshy, Dean Eckles", "title": "Uncertainty in Online Experiments with Dependent Data: An Evaluation of\n  Bootstrap Methods", "comments": "9 pages, 5 figures. This version corrects an error in one set of\n  simulations in Section 3.4 / Figure 4. All other results are unaffected", "journal-ref": "In Proceedings of the 19th ACM SIGKDD international conference on\n  Knowledge discovery and data mining (KDD '13). 2013. ACM, New York, NY, USA,\n  1303-1311", "doi": "10.1145/2487575.2488218", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online experiments exhibit dependence between users and items. For\nexample, in online advertising, observations that have a user or an ad in\ncommon are likely to be associated. Because of this, even in experiments\ninvolving millions of subjects, the difference in mean outcomes between control\nand treatment conditions can have substantial variance. Previous theoretical\nand simulation results demonstrate that not accounting for this kind of\ndependence structure can result in confidence intervals that are too narrow,\nleading to inaccurate hypothesis tests.\n  We develop a framework for understanding how dependence affects uncertainty\nin user-item experiments and evaluate how bootstrap methods that account for\ndiffering levels of dependence perform in practice. We use three real datasets\ndescribing user behaviors on Facebook - user responses to ads, search results,\nand News Feed stories - to generate data for synthetic experiments in which\nthere is no effect of the treatment on average by design. We then estimate\nempirical Type I error rates for each bootstrap method. Accounting for\ndependence within a single type of unit (i.e., within-user dependence) is often\nsufficient to get reasonable error rates. But when experiments have effects, as\none might expect in the field, accounting for multiple units with a multiway\nbootstrap can be necessary to get close to the advertised Type I error rates.\nThis work provides guidance to practitioners evaluating large-scale\nexperiments, and highlights the importance of analysis of inferential methods\nfor dependence structures common to online systems.\n", "versions": [{"version": "v1", "created": "Sat, 27 Apr 2013 20:54:41 GMT"}, {"version": "v2", "created": "Wed, 10 Jul 2013 21:47:35 GMT"}, {"version": "v3", "created": "Tue, 22 Oct 2013 20:58:51 GMT"}, {"version": "v4", "created": "Wed, 25 Oct 2017 15:31:03 GMT"}], "update_date": "2017-10-26", "authors_parsed": [["Bakshy", "Eytan", ""], ["Eckles", "Dean", ""]]}, {"id": "1304.7817", "submitter": "Benjamin Chabot-Hanowell", "authors": "Ben Hanowell", "title": "A simple hierarchical Bayesian model for simultaneous inference of\n  tournament graphs and informant error", "comments": "6 pages, 25 numbered equations, 1 algorithm description", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a hierarchical Bayesian model for simultaneous inference\nof tournament graphs and informant error. From multiple informant reports or\nmeasurement instrument outputs, the model estimates the structure of a\ncriterion (i.e., true) tournament graph with possibly tied outcomes. Tournament\ngraphs with possibly tied outcomes are graphs in which there are three possible\nstates for each unordered pair of graph nodes: node i wins and node j loses;\nnode j wins and node i loses; neither node wins (i.e., tied outcome). The model\nalso estimates the rates at which individual informants (or instruments)\nmistake the winning and losing dyad members, falsely report a tied outcome, and\nfalsely report a decisive outcome. The model was developed to infer social\ndominance structure from multiple informants' reports, but is potentially\nuseful for inferring any structure that can be characterized by a tournament\ngraph, and which is measured from multiple reports.\n", "versions": [{"version": "v1", "created": "Mon, 29 Apr 2013 22:30:39 GMT"}, {"version": "v2", "created": "Wed, 1 May 2013 02:46:46 GMT"}, {"version": "v3", "created": "Thu, 2 May 2013 03:18:50 GMT"}, {"version": "v4", "created": "Fri, 11 Oct 2013 17:43:48 GMT"}], "update_date": "2013-10-14", "authors_parsed": [["Hanowell", "Ben", ""]]}, {"id": "1304.7829", "submitter": "Wei Lin", "authors": "Wei Lin, Rui Feng, Hongzhe Li", "title": "Regularization Methods for High-Dimensional Instrumental Variables\n  Regression With an Application to Genetical Genomics", "comments": "43 pages, 4 figures, to appear in Journal of the American Statistical\n  Association (http://www.tandfonline.com/r/JASA)", "journal-ref": null, "doi": "10.1080/01621459.2014.908125", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In genetical genomics studies, it is important to jointly analyze gene\nexpression data and genetic variants in exploring their associations with\ncomplex traits, where the dimensionality of gene expressions and genetic\nvariants can both be much larger than the sample size. Motivated by such modern\napplications, we consider the problem of variable selection and estimation in\nhigh-dimensional sparse instrumental variables models. To overcome the\ndifficulty of high dimensionality and unknown optimal instruments, we propose a\ntwo-stage regularization framework for identifying and estimating important\ncovariate effects while selecting and estimating optimal instruments. The\nmethodology extends the classical two-stage least squares estimator to high\ndimensions by exploiting sparsity using sparsity-inducing penalty functions in\nboth stages. The resulting procedure is efficiently implemented by coordinate\ndescent optimization. For the representative $L_1$ regularization and a class\nof concave regularization methods, we establish estimation, prediction, and\nmodel selection properties of the two-stage regularized estimators in the\nhigh-dimensional setting where the dimensionality of covariates and instruments\nare both allowed to grow exponentially with the sample size. The practical\nperformance of the proposed method is evaluated by simulation studies and its\nusefulness is illustrated by an analysis of mouse obesity data. Supplementary\nmaterials for this article are available online.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 01:21:18 GMT"}, {"version": "v2", "created": "Tue, 18 Mar 2014 07:33:37 GMT"}], "update_date": "2014-04-15", "authors_parsed": [["Lin", "Wei", ""], ["Feng", "Rui", ""], ["Li", "Hongzhe", ""]]}, {"id": "1304.7847", "submitter": "Thomas Lee", "authors": "Randy C. S. Lai, Jan Hannig and Thomas C. M. Lee", "title": "Generalized Fiducial Inference for Ultrahigh Dimensional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the ultrahigh dimensional linear regression problem has\nattracted enormous attentions from the research community. Under the sparsity\nassumption most of the published work is devoted to the selection and\nestimation of the significant predictor variables. This paper studies a\ndifferent but fundamentally important aspect of this problem: uncertainty\nquantification for parameter estimates and model choices. To be more specific,\nthis paper proposes methods for deriving a probability density function on the\nset of all possible models, and also for constructing confidence intervals for\nthe corresponding parameters. These proposed methods are developed using the\ngeneralized fiducial methodology, which is a variant of Fisher's controversial\nfiducial idea. Theoretical properties of the proposed methods are studied, and\nin particular it is shown that statistical inference based on the proposed\nmethods will have exact asymptotic frequentist property. In terms of empirical\nperformances, the proposed methods are tested by simulation experiments and an\napplication to a real data set. Lastly this work can also be seen as an\ninteresting and successful application of Fisher's fiducial idea to an\nimportant and contemporary problem. To the best of the authors' knowledge, this\nis the first time that the fiducial idea is being applied to a so-called \"large\np small n\" problem.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 03:32:20 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Lai", "Randy C. S.", ""], ["Hannig", "Jan", ""], ["Lee", "Thomas C. M.", ""]]}, {"id": "1304.7867", "submitter": "Akifumi Notsu", "authors": "Akifumi Notsu, Osamu Komori, Shinto Eguchi", "title": "Spontaneous Clustering via Minimum \\gamma-divergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for clustering based on the local minimization of the\n\\gamma-divergence, which we call the spontaneous clustering. The greatest\nadvantage of the proposed method is that it automatically detects the number of\nclusters that adequately reflect the data structure. In contrast, exiting\nmethods such as K-means, fuzzy c-means, and model based clustering need to\nprescribe the number of clusters. We detect all the local minimum points of the\n\\gamma-divergence, which are defined as the centers of clusters. A necessary\nand sufficient condition for the \\gamma-divergence to have the local minimum\npoints is also derived in a simple setting. A simulation study and a real data\nanalysis are performed to compare our proposal with existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 04:26:24 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Notsu", "Akifumi", ""], ["Komori", "Osamu", ""], ["Eguchi", "Shinto", ""]]}, {"id": "1304.7914", "submitter": "Fabio Rapallo", "authors": "Roberto Fontana, Fabio Rapallo and Maria-Piera Rogantin", "title": "A Characterization of Saturated Designs for Factorial Experiments", "comments": "18 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study saturated fractions of factorial designs under the\nperspective of Algebraic Statistics. We define a criterion to check whether a\nfraction is saturated or not with respect to a given model. The proposed\ncriterion is based purely on combinatorial objects. Our technique is\nparticularly useful when several fractions are needed. We also show how to\ngenerate random saturated fractions with given projections, by applying the\ntheory of Markov bases for contingency tables.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 08:30:34 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Fontana", "Roberto", ""], ["Rapallo", "Fabio", ""], ["Rogantin", "Maria-Piera", ""]]}, {"id": "1304.7956", "submitter": "Maximilian Ludwig", "authors": "Maximilian Ludwig", "title": "Estimating Moving Average Processes with an improved version of Durbin's\n  Method", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper provides a simple method to estimate both univariate and\nmultivariate MA processes. Similar to Durbin's method, it rests on the\nrecursive relation between the parameters of the MA process and those of its AR\nrepresentation. This recursive relation is shown to be valid both for\ninvertible / stable and non invertible / unstable processes under the\nassumption that the process has no constant and started from zero. This makes\nthe method suitable for unit root processes, too.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 11:09:52 GMT"}, {"version": "v2", "created": "Thu, 26 Jun 2014 13:01:56 GMT"}], "update_date": "2014-06-27", "authors_parsed": [["Ludwig", "Maximilian", ""]]}, {"id": "1304.8089", "submitter": "Sonia Dias Mrs", "authors": "S\\'onia Dias and Paula Brito", "title": "Distribution and Symmetric Distribution Regression Model for\n  Interval-Valued Variables", "comments": "52 pages 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic Data Analysis works with variables for which each unit or class of\nunits takes a finite set of values/categories, an interval or a distribution\n(an histogram, for instance). When to each observation corresponds an empirical\ndistribution, we have a histogram-valued variable; it reduces to the case of an\ninterval-valued variable if each unit takes values on only one interval with\nprobability equal to one. Distribution and Symmetric Distribution is a linear\nregression model proposed for histogram-valued variables that may be\nparticularized to interval-valued variables. This model is defined for n\nexplicative variables and is based on the distributions considered within the\nintervals. In this paper we study the special case where the Uniform\ndistribution is assumed in each observed interval. As in the classical case, a\ngoodness-of-fit measure is deduced from the model. Some illustrative examples\nare presented. A simulation study allows discussing interpretations of the\nbehavior of the model for this variable type.\n", "versions": [{"version": "v1", "created": "Tue, 30 Apr 2013 17:43:11 GMT"}], "update_date": "2013-05-01", "authors_parsed": [["Dias", "S\u00f3nia", ""], ["Brito", "Paula", ""]]}]