[{"id": "1109.0152", "submitter": "Bernd Fellinghauer", "authors": "Bernd Fellinghauer, Peter B\\\"uhlmann, Martin Ryffel, Michael von Rhein\n  and Jan D. Reinhardt", "title": "Stable Graphical Model Estimation with Random Forests for Discrete,\n  Continuous, and Mixed Variables", "comments": "The authors report no conflict of interest. There was no external\n  funding", "journal-ref": null, "doi": "10.1016/j.csda.2013.02.022", "report-no": null, "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A conditional independence graph is a concise representation of pairwise\nconditional independence among many variables. Graphical Random Forests (GRaFo)\nare a novel method for estimating pairwise conditional independence\nrelationships among mixed-type, i.e. continuous and discrete, variables. The\nnumber of edges is a tuning parameter in any graphical model estimator and\nthere is no obvious number that constitutes a good choice. Stability Selection\nhelps choosing this parameter with respect to a bound on the expected number of\nfalse positives (error control).\n  The performance of GRaFo is evaluated and compared with various other methods\nfor p = 50, 100, and 200 possibly mixed-type variables while sample size is n =\n100 (n = 500 for maximum likelihood). Furthermore, GRaFo is applied to data\nfrom the Swiss Health Survey in order to evaluate how well it can reproduce the\ninterconnection of functional health components, personal, and environmental\nfactors, as hypothesized by the World Health Organization's International\nClassification of Functioning, Disability and Health (ICF). Finally, GRaFo is\nused to identify risk factors which may be associated with adverse\nneurodevelopment of children who suffer from trisomy 21 and experienced\nopen-heart surgery.\n  GRaFo performs well with mixed data and thanks to Stability Selection it\nprovides an error control mechanism for false positive selection.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 11:20:49 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2012 21:30:12 GMT"}], "update_date": "2013-04-08", "authors_parsed": [["Fellinghauer", "Bernd", ""], ["B\u00fchlmann", "Peter", ""], ["Ryffel", "Martin", ""], ["von Rhein", "Michael", ""], ["Reinhardt", "Jan D.", ""]]}, {"id": "1109.0262", "submitter": "Gail E. Potter", "authors": "Gail E. Potter, Mark S. Handcock, Ira M. Longini, Jr., M. Elizabeth\n  Halloran", "title": "Estimating within-school contact networks to understand influenza\n  transmission", "comments": "Published in at http://dx.doi.org/10.1214/11-AOAS505 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 1, 1-26", "doi": "10.1214/11-AOAS505", "report-no": "IMS-AOAS-AOAS505", "categories": "stat.ME cs.SI physics.soc-ph stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many epidemic models approximate social contact behavior by assuming random\nmixing within mixing groups (e.g., homes, schools and workplaces). The effect\nof more realistic social network structure on estimates of epidemic parameters\nis an open area of exploration. We develop a detailed statistical model to\nestimate the social contact network within a high school using friendship\nnetwork data and a survey of contact behavior. Our contact network model\nincludes classroom structure, longer durations of contacts to friends than\nnonfriends and more frequent contacts with friends, based on reports in the\ncontact survey. We performed simulation studies to explore which network\nstructures are relevant to influenza transmission. These studies yield two key\nfindings. First, we found that the friendship network structure important to\nthe transmission process can be adequately represented by a dyad-independent\nexponential random graph model (ERGM). This means that individual-level sampled\ndata is sufficient to characterize the entire friendship network. Second, we\nfound that contact behavior was adequately represented by a static rather than\ndynamic contact network.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 19:00:15 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2011 23:18:57 GMT"}, {"version": "v3", "created": "Tue, 18 Oct 2011 19:17:13 GMT"}, {"version": "v4", "created": "Thu, 15 Mar 2012 14:04:54 GMT"}], "update_date": "2012-08-27", "authors_parsed": [["Potter", "Gail E.", ""], ["Handcock", "Mark S.", ""], ["Longini,", "Ira M.", "Jr."], ["Halloran", "M. Elizabeth", ""]]}, {"id": "1109.0320", "submitter": "Tingjin Chu", "authors": "Tingjin Chu, Jun Zhu, Haonan Wang", "title": "Penalized maximum likelihood estimation and variable selection in\n  geostatistics", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS919 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 5, 2607-2625", "doi": "10.1214/11-AOS919", "report-no": "IMS-AOS-AOS919", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of selecting covariates in spatial linear models with\nGaussian process errors. Penalized maximum likelihood estimation (PMLE) that\nenables simultaneous variable selection and parameter estimation is developed\nand, for ease of computation, PMLE is approximated by one-step sparse\nestimation (OSE). To further improve computational efficiency, particularly\nwith large sample sizes, we propose penalized maximum covariance-tapered\nlikelihood estimation (PMLE$_{\\mathrm{T}}$) and its one-step sparse estimation\n(OSE$_{\\mathrm{T}}$). General forms of penalty functions with an emphasis on\nsmoothly clipped absolute deviation are used for penalized maximum likelihood.\nTheoretical properties of PMLE and OSE, as well as their approximations\nPMLE$_{\\mathrm{T}}$ and OSE$_{\\mathrm{T}}$ using covariance tapering, are\nderived, including consistency, sparsity, asymptotic normality and the oracle\nproperties. For covariance tapering, a by-product of our theoretical results is\nconsistency and asymptotic normality of maximum covariance-tapered likelihood\nestimates. Finite-sample properties of the proposed methods are demonstrated in\na simulation study and, for illustration, the methods are applied to analyze\ntwo real data sets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 22:45:31 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2012 14:58:50 GMT"}], "update_date": "2012-02-24", "authors_parsed": [["Chu", "Tingjin", ""], ["Zhu", "Jun", ""], ["Wang", "Haonan", ""]]}, {"id": "1109.0322", "submitter": "Lauren Hannah", "authors": "Lauren A. Hannah and David B. Dunson", "title": "Bayesian nonparametric multivariate convex regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, such as economics, operations research and\nreinforcement learning, one often needs to estimate a multivariate regression\nfunction f subject to a convexity constraint. For example, in sequential\ndecision processes the value of a state under optimal subsequent decisions may\nbe known to be convex or concave. We propose a new Bayesian nonparametric\nmultivariate approach based on characterizing the unknown regression function\nas the max of a random collection of unknown hyperplanes. This specification\ninduces a prior with large support in a Kullback-Leibler sense on the space of\nconvex functions, while also leading to strong posterior consistency. Although\nwe assume that f is defined over R^p, we show that this model has a convergence\nrate of log(n)^{-1} n^{-1/(d+2)} under the empirical L2 norm when f actually\nmaps a d dimensional linear subspace to R. We design an efficient reversible\njump MCMC algorithm for posterior computation and demonstrate the methods\nthrough application to value function approximation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Sep 2011 22:51:34 GMT"}], "update_date": "2011-09-05", "authors_parsed": [["Hannah", "Lauren A.", ""], ["Dunson", "David B.", ""]]}, {"id": "1109.0442", "submitter": "Christian R\\\"over", "authors": "Christian R\\\"over", "title": "A Student-t based filter for robust signal detection", "comments": "17 pages, 6 figures", "journal-ref": "Physical Review D, 84:122004, 2011", "doi": "10.1103/PhysRevD.84.122004", "report-no": "LIGO-P1100103; AEI-2011-060", "categories": "physics.data-an gr-qc stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search for gravitational-wave signals in detector data is often hampered\nby the fact that many data analysis methods are based on the theory of\nstationary Gaussian noise, while actual measurement data frequently exhibit\nclear departures from these assumptions. Deriving methods from models more\nclosely reflecting the data's properties promises to yield more sensitive\nprocedures. The commonly used matched filter is such a detection method that\nmay be derived via a Gaussian model. In this paper we propose a generalized\nmatched-filtering technique based on a Student-t distribution that is able to\naccount for heavier-tailed noise and is robust against outliers in the data. On\nthe technical side, it generalizes the matched filter's least-squares method to\nan iterative, or adaptive, variation. In a simplified Monte Carlo study we show\nthat when applied to simulated signals buried in actual interferometer noise it\nleads to a higher detection rate than the usual (\"Gaussian\") matched filter.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2011 13:45:00 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2011 12:17:50 GMT"}, {"version": "v3", "created": "Thu, 29 Dec 2011 19:54:38 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["R\u00f6ver", "Christian", ""]]}, {"id": "1109.0486", "submitter": "Bert Kappen", "authors": "Hilbert J. Kappen, Vicen\\c{c} G\\'omez", "title": "The Variational Garrote", "comments": "26 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new variational method for sparse regression\nusing $L_0$ regularization. The variational parameters appear in the\napproximate model in a way that is similar to Breiman's Garrote model. We refer\nto this method as the variational Garrote (VG). We show that the combination of\nthe variational approximation and $L_0$ regularization has the effect of making\nthe problem effectively of maximal rank even when the number of samples is\nsmall compared to the number of variables. The VG is compared numerically with\nthe Lasso method, ridge regression and the recently introduced paired mean\nfield method (PMF) (M. Titsias & M. L\\'azaro-Gredilla., NIPS 2012). Numerical\nresults show that the VG and PMF yield more accurate predictions and more\naccurately reconstruct the true model than the other methods. It is shown that\nthe VG finds correct solutions when the Lasso solution is inconsistent due to\nlarge input correlations. Globally, VG is significantly faster than PMF and\ntends to perform better as the problems become denser and in problems with\nstrongly correlated inputs. The naive implementation of the VG scales cubic\nwith the number of features. By introducing Lagrange multipliers we obtain a\ndual formulation of the problem that scales cubic in the number of samples, but\nclose to linear in the number of features.\n", "versions": [{"version": "v1", "created": "Fri, 2 Sep 2011 15:48:23 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2011 15:54:57 GMT"}, {"version": "v3", "created": "Mon, 12 Nov 2012 16:07:03 GMT"}], "update_date": "2012-11-13", "authors_parsed": [["Kappen", "Hilbert J.", ""], ["G\u00f3mez", "Vicen\u00e7", ""]]}, {"id": "1109.0701", "submitter": "Bryony J. Hill", "authors": "Bryony J. Hill, Wilfrid S. Kendall, Elke Th\\\"onnes", "title": "Fibre-generated point processes and fields of orientations", "comments": "Published in at http://dx.doi.org/10.1214/12-AOAS553 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2012, Vol. 6, No. 3, 994-1020", "doi": "10.1214/12-AOAS553", "report-no": "IMS-AOAS-AOAS553", "categories": "stat.ME stat.AP stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new approach to analyzing spatial point data\nclustered along or around a system of curves or \"fibres.\" Such data arise in\ncatalogues of galaxy locations, recorded locations of earthquakes, aerial\nimages of minefields and pore patterns on fingerprints. Finding the underlying\ncurvilinear structure of these point-pattern data sets may not only facilitate\na better understanding of how they arise but also aid reconstruction of missing\ndata. We base the space of fibres on the set of integral lines of an\norientation field. Using an empirical Bayes approach, we estimate the field of\norientations from anisotropic features of the data. We then sample from the\nposterior distribution of fibres, exploring models with different numbers of\nclusters, fitting fibres to the clusters as we proceed. The Bayesian approach\npermits inference on various properties of the clusters and associated fibres,\nand the results perform well on a number of very different curvilinear\nstructures.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2011 11:36:33 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2012 16:48:33 GMT"}, {"version": "v3", "created": "Fri, 28 Sep 2012 06:45:09 GMT"}], "update_date": "2012-10-01", "authors_parsed": [["Hill", "Bryony J.", ""], ["Kendall", "Wilfrid S.", ""], ["Th\u00f6nnes", "Elke", ""]]}, {"id": "1109.0725", "submitter": "Chris Tofallis", "authors": "Chris Tofallis", "title": "Model Building with Multiple Dependent Variables and Constraints", "comments": null, "journal-ref": "J. Royal Stat. Soc. Series D: The Statistician, (1999), 48(3),\n  371-378", "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most widely used method for finding relationships between several\nquantities is multiple regression. This however is restricted to a single\ndependent variable. We present a more general method which allows models to be\nconstructed with multiple variables on both sides of an equation and which can\nbe computed easily using a spreadsheet program. The underlying principle\n(originating from canonical correlation analysis) is that of maximising the\ncorrelation between the two sides of the model equation. This paper presents a\nfitting procedure which makes it possible to force the estimated model to\nsatisfy constraint conditions which it is required to possess, these may arise\nfrom theory, prior knowledge or be intuitively obvious. We also show that the\nleast squares approach to the problem is inadequate as it produces models which\nare not scale invariant.\n", "versions": [{"version": "v1", "created": "Sun, 4 Sep 2011 16:36:42 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Tofallis", "Chris", ""]]}, {"id": "1109.1070", "submitter": "Dylan Small", "authors": "Dylan S. Small", "title": "Mediation Analysis Without Sequential Ignorability: Using Baseline\n  Covariates Interacted with Random Assignment as Instrumental Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In randomized trials, researchers are often interested in mediation analysis\nto understand how a treatment works, in particular how much of a treatment's\neffect is mediated by an intermediated variable and how much the treatment\ndirectly affects the outcome not through the mediator. The standard regression\napproach to mediation analysis assumes sequential ignorability of the mediator,\nthat is that the mediator is effectively randomly assigned given baseline\ncovariates and the randomized treatment. Since the experiment does not\nrandomize the mediator, sequential ignorability is often not plausible. Ten\nHave et al. (2007, Biometrics), Dunn and Bentall (2007, Statistics in Medicine)\nand Albert (2008, Statistics in Medicine) presented methods that use baseline\ncovariates interacted with random assignment as instrumental variables, and do\nnot require sequential ignorability. We make two contributions to this\napproach. First, in previous work on the instrumental variable approach, it has\nbeen assumed that the direct effect of treatment and the effect of the mediator\nare constant across subjects; we allow for variation in effects across subjects\nand show what assumptions are needed to obtain consistent estimates for this\nsetting. Second, we develop a method of sensitivity analysis for violations of\nthe key assumption that the direct effect of the treatment and the effect of\nthe mediator do not depend on the baseline covariates.\n", "versions": [{"version": "v1", "created": "Tue, 6 Sep 2011 05:49:02 GMT"}, {"version": "v2", "created": "Mon, 31 Dec 2012 06:47:10 GMT"}], "update_date": "2013-01-01", "authors_parsed": [["Small", "Dylan S.", ""]]}, {"id": "1109.2279", "submitter": "James Scott", "authors": "Nicholas G. Polson and James G. Scott and Jesse Windle", "title": "The Bayesian Bridge", "comments": "Supplemental files are available from the second author's website", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Bayesian bridge estimator for regularized regression and\nclassification. Two key mixture representations for the Bayesian bridge model\nare developed: (1) a scale mixture of normals with respect to an alpha-stable\nrandom variable; and (2) a mixture of Bartlett--Fejer kernels (or triangle\ndensities) with respect to a two-component mixture of gamma random variables.\nBoth lead to MCMC methods for posterior simulation, and these methods turn out\nto have complementary domains of maximum efficiency. The first representation\nis a well known result due to West (1987), and is the better choice for\ncollinear design matrices. The second representation is new, and is more\nefficient for orthogonal problems, largely because it avoids the need to deal\nwith exponentially tilted stable random variables. It also provides insight\ninto the multimodality of the joint posterior distribution, a feature of the\nbridge model that is notably absent under ridge or lasso-type priors. We prove\na theorem that extends this representation to a wider class of densities\nrepresentable as scale mixtures of betas, and provide an explicit inversion\nformula for the mixing distribution. The connections with slice sampling and\nscale mixtures of normals are explored. On the practical side, we find that the\nBayesian bridge model outperforms its classical cousin in estimation and\nprediction across a variety of data sets, both simulated and real. We also show\nthat the MCMC for fitting the bridge model exhibits excellent mixing\nproperties, particularly for the global scale parameter. This makes for a\nfavorable contrast with analogous MCMC algorithms for other sparse Bayesian\nmodels. All methods described in this paper are implemented in the R package\nBayesBridge. An extensive set of simulation results are provided in two\nsupplemental files.\n", "versions": [{"version": "v1", "created": "Sun, 11 Sep 2011 04:57:48 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2012 21:01:16 GMT"}], "update_date": "2012-10-30", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""], ["Windle", "Jesse", ""]]}, {"id": "1109.2411", "submitter": "Kei Hirose", "authors": "Kei Hirose, Shohei Tateishi and Sadanori Konishi", "title": "Efficient algorithm to select tuning parameters in sparse regression\n  modeling with regularization", "comments": "24pages, 2figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In sparse regression modeling via regularization such as the lasso, it is\nimportant to select appropriate values of tuning parameters including\nregularization parameters. The choice of tuning parameters can be viewed as a\nmodel selection and evaluation problem. Mallows' $C_p$ type criteria may be\nused as a tuning parameter selection tool in lasso-type regularization methods,\nfor which the concept of degrees of freedom plays a key role. In the present\npaper, we propose an efficient algorithm that computes the degrees of freedom\nby extending the generalized path seeking algorithm. Our procedure allows us to\nconstruct model selection criteria for evaluating models estimated by\nregularization with a wide variety of convex and non-convex penalties. Monte\nCarlo simulations demonstrate that our methodology performs well in various\nsituations. A real data example is also given to illustrate our procedure.\n", "versions": [{"version": "v1", "created": "Mon, 12 Sep 2011 09:33:12 GMT"}, {"version": "v2", "created": "Thu, 15 Sep 2011 01:01:26 GMT"}, {"version": "v3", "created": "Wed, 4 Jan 2012 04:41:51 GMT"}], "update_date": "2012-01-05", "authors_parsed": [["Hirose", "Kei", ""], ["Tateishi", "Shohei", ""], ["Konishi", "Sadanori", ""]]}, {"id": "1109.2963", "submitter": "Cesar Comin", "authors": "Cesar H. Comin, Jo\\~ao B. Bunoro, Matheus P. Viana and Luciano da F.\n  Costa", "title": "Unveiling the Relationship Between Structure and Dynamics in Complex\n  Networks", "comments": "16 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an cs.SI nlin.CD physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last years, a great deal of attention has been focused on complex\nnetworked systems, characterized by intricate structure and dynamics. The\nlatter has been often represented in terms of overall statistics (e.g. average\nand standard deviations) of the time signals. While such approaches have led to\nmany insights, they have failed to take into account that signals at different\nparts of the system can undergo distinct evolutions, which cannot be properly\nrepresented in terms of average values. A novel framework for identifying the\nprincipal aspects of the dynamics and how it is influenced by the network\nstructure is proposed in this work. The potential of this approach is\nillustrated with respect to three important models (Integrate-and-Fire, SIS and\nKuramoto), allowing the identification of highly structured dynamics, in the\nsense that different groups of nodes not only presented specific dynamics but\nalso felt the structure of the network in different ways.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2011 01:36:42 GMT"}], "update_date": "2011-09-15", "authors_parsed": [["Comin", "Cesar H.", ""], ["Bunoro", "Jo\u00e3o B.", ""], ["Viana", "Matheus P.", ""], ["Costa", "Luciano da F.", ""]]}, {"id": "1109.2990", "submitter": "Manuel Lladser", "authors": "Manuel Lladser, Ra\\'ul Gouet, Jens Reeder", "title": "Extrapolation of Urn Models via Poissonization: Accurate Measurements of\n  the Microbial Unknown", "comments": "14 pages, 7 figures, 4 tables", "journal-ref": "PloS ONE, 6(6): e21105 (2011)", "doi": "10.1371/journal.pone.0021105", "report-no": null, "categories": "stat.ME math.PR q-bio.GN q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of high-throughput parallel methods for sequencing microbial\ncommunities is increasing our knowledge of the microbial world at an\nunprecedented rate. Though most attention has focused on determining\nlower-bounds on the alpha-diversity i.e. the total number of different species\npresent in the environment, tight bounds on this quantity may be highly\nuncertain because a small fraction of the environment could be composed of a\nvast number of different species. To better assess what remains unknown, we\npropose instead to predict the fraction of the environment that belongs to\nunsampled classes. Modeling samples as draws with replacement of colored balls\nfrom an urn with an unknown composition, and under the sole assumption that\nthere are still undiscovered species, we show that conditionally unbiased\npredictors and exact prediction intervals (of constant length in logarithmic\nscale) are possible for the fraction of the environment that belongs to\nunsampled classes. Our predictions are based on a Poissonization argument,\nwhich we have implemented in what we call the Embedding algorithm. In fixed\ni.e. non-randomized sample sizes, the algorithm leads to very accurate\npredictions on a sub-sample of the original sample. We quantify the effect of\nfixed sample sizes on our prediction intervals and test our methods and others\nfound in the literature against simulated environments, which we devise taking\ninto account datasets from a human-gut and -hand microbiota. Our methodology\napplies to any dataset that can be conceptualized as a sample with replacement\nfrom an urn. In particular, it could be applied, for example, to quantify the\nproportion of all the unseen solutions to a binding site problem in a random\nRNA pool, or to reassess the surveillance of a certain terrorist group,\npredicting the conditional probability that it deploys a new tactic in a next\nattack.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2011 04:33:07 GMT"}], "update_date": "2011-09-15", "authors_parsed": [["Lladser", "Manuel", ""], ["Gouet", "Ra\u00fal", ""], ["Reeder", "Jens", ""]]}, {"id": "1109.3142", "submitter": "Ryszard Kostecki", "authors": "Ryszard Pawe{\\l} Kostecki", "title": "On principles of inductive inference", "comments": "To appear in: Goyal P. (ed.), Proceedings of the 31th International\n  Workshop on Bayesian Inference and Maximum Entropy Methods in Science and\n  Engineering, 10-15 July 2011, Waterloo, AIP Conf. Proc., Springer, Berlin", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an intersubjective epistemic approach to foundations of\nprobability theory and statistical inference, based on relative entropy and\ncategory theory, and aimed to bypass the mathematical and conceptual problems\nof existing foundational approaches.\n", "versions": [{"version": "v1", "created": "Wed, 14 Sep 2011 17:27:49 GMT"}, {"version": "v2", "created": "Sat, 12 Nov 2011 04:59:05 GMT"}, {"version": "v3", "created": "Fri, 2 Dec 2011 15:37:54 GMT"}, {"version": "v4", "created": "Tue, 31 Jan 2012 03:35:43 GMT"}], "update_date": "2012-02-01", "authors_parsed": [["Kostecki", "Ryszard Pawe\u0142", ""]]}, {"id": "1109.3409", "submitter": "Hao Wang", "authors": "Hao Wang and Natesh S. Pillai", "title": "On a Class of Shrinkage Priors for Covariance Matrix Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible class of models based on scale mixture of uniform\ndistributions to construct shrinkage priors for covariance matrix estimation.\nThis new class of priors enjoys a number of advantages over the traditional\nscale mixture of normal priors, including its simplicity and flexibility in\ncharacterizing the prior density. We also exhibit a simple, easy to implement\nGibbs sampler for posterior simulation which leads to efficient estimation in\nhigh dimensional problems. We first discuss the theory and computational\ndetails of this new approach and then extend the basic model to a new class of\nmultivariate conditional autoregressive models for analyzing multivariate areal\ndata. The proposed spatial model flexibly characterizes both the spatial and\nthe outcome correlation structures at an appealing computational cost. Examples\nconsisting of both synthetic and real-world data show the utility of this new\nframework in terms of robust estimation as well as improved predictive\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Sep 2011 17:25:55 GMT"}, {"version": "v2", "created": "Thu, 6 Oct 2011 01:15:23 GMT"}], "update_date": "2011-10-07", "authors_parsed": [["Wang", "Hao", ""], ["Pillai", "Natesh S.", ""]]}, {"id": "1109.3829", "submitter": "Pierre E. Jacob", "authors": "Luke Bornn (Harvard University), Pierre Jacob (Universite Paris\n  Dauphine), Pierre Del Moral (INRIA Bordeaux Sud-Ouest and Universite de\n  Bordeaux), Arnaud Doucet (University of Oxford)", "title": "An Adaptive Interacting Wang-Landau Algorithm for Automatic Density\n  Exploration", "comments": "33 pages, 20 figures (the supplementary materials are included as\n  appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While statisticians are well-accustomed to performing exploratory analysis in\nthe modeling stage of an analysis, the notion of conducting preliminary\ngeneral-purpose exploratory analysis in the Monte Carlo stage (or more\ngenerally, the model-fitting stage) of an analysis is an area which we feel\ndeserves much further attention. Towards this aim, this paper proposes a\ngeneral-purpose algorithm for automatic density exploration. The proposed\nexploration algorithm combines and expands upon components from various\nadaptive Markov chain Monte Carlo methods, with the Wang-Landau algorithm at\nits heart. Additionally, the algorithm is run on interacting parallel chains --\na feature which both decreases computational cost as well as stabilizes the\nalgorithm, improving its ability to explore the density. Performance is studied\nin several applications. Through a Bayesian variable selection example, the\nauthors demonstrate the convergence gains obtained with interacting chains. The\nability of the algorithm's adaptive proposal to induce mode-jumping is\nillustrated through a trimodal density and a Bayesian mixture modeling\napplication. Lastly, through a 2D Ising model, the authors demonstrate the\nability of the algorithm to overcome the high correlations encountered in\nspatial models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Sep 2011 01:02:31 GMT"}, {"version": "v2", "created": "Wed, 25 Apr 2012 07:03:31 GMT"}, {"version": "v3", "created": "Thu, 14 Jun 2012 04:10:10 GMT"}], "update_date": "2012-06-15", "authors_parsed": [["Bornn", "Luke", "", "Harvard University"], ["Jacob", "Pierre", "", "Universite Paris\n  Dauphine"], ["Del Moral", "Pierre", "", "INRIA Bordeaux Sud-Ouest and Universite de\n  Bordeaux"], ["Doucet", "Arnaud", "", "University of Oxford"]]}, {"id": "1109.3940", "submitter": "Yuan Shi", "authors": "Yuan Shi, Yung-Kyun Noh, Fei Sha, Daniel D. Lee", "title": "Learning Discriminative Metrics via Generative Models and Kernel\n  Learning", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Metrics specifying distances between data points can be learned in a\ndiscriminative manner or from generative models. In this paper, we show how to\nunify generative and discriminative learning of metrics via a kernel learning\nframework. Specifically, we learn local metrics optimized from parametric\ngenerative models. These are then used as base kernels to construct a global\nkernel that minimizes a discriminative training criterion. We consider both\nlinear and nonlinear combinations of local metric kernels. Our empirical\nresults show that these combinations significantly improve performance on\nclassification tasks. The proposed learning algorithm is also very efficient,\nachieving order of magnitude speedup in training time compared to previous\ndiscriminative baseline methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 04:19:30 GMT"}], "update_date": "2011-09-26", "authors_parsed": [["Shi", "Yuan", ""], ["Noh", "Yung-Kyun", ""], ["Sha", "Fei", ""], ["Lee", "Daniel D.", ""]]}, {"id": "1109.3950", "submitter": "Paul Kabaila", "authors": "Paul Kabaila and Dilshani Tissera", "title": "Effect of a preliminary test of homogeneity of stratum-specific odds\n  ratios on their confidence intervals", "comments": "The title has been shortened and the exposition has been improved", "journal-ref": "Effect of a preliminary test of homogeneity of stratum-specific\n  odds ratios on their confidence intervals. Electronic Journal of Statistics,\n  6, 672-685 (2012)", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider a case-control study in which the aim is to assess the effect of a\nfactor on disease occurrence. We suppose that this factor is dichotomous. Also\nsuppose that the data consists of two strata, each stratum summarized by a\ntwo-by-two table. A commonly-proposed two-stage analysis of this type of data\nis the following. We carry out a preliminary test of homogeneity of the\nstratum-specific odds ratios. If the null hypothesis of homogeneity is accepted\nthen we find a confidence interval for the assumed common value (across strata)\nof the odds ratio. We examine the statistical properties of this two-stage\nanalysis, based on the Woolf method, on confidence intervals constructed for\nthe stratum-specific odds ratios, for large numbers of cases and controls for\neach stratum. We provide both a Monte Carlo simulation method and an elegant\nlarge-sample method for this examination. These methods are applied to obtain\nnumerical results in the context of the large numbers of cases and controls for\neach stratum that arose in a real-life dataset. In this context, we find that\nthe preliminary test of homogeneity of the stratum-specific odds ratios has a\nvery harmful effect on the coverage probabilities of these confidence\nintervals.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 06:42:03 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2012 05:16:42 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Kabaila", "Paul", ""], ["Tissera", "Dilshani", ""]]}, {"id": "1109.4166", "submitter": "Robert Erhardt", "authors": "Robert J. Erhardt, Richard L. Smith", "title": "Approximate Bayesian Computing for Spatial Extremes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical analysis of max-stable processes used to model spatial extremes\nhas been limited by the difficulty in calculating the joint likelihood\nfunction. This precludes all standard likelihood-based approaches, including\nBayesian approaches. In this paper we present a Bayesian approach through the\nuse of approximate Bayesian computing. This circumvents the need for a joint\nlikelihood function by instead relying on simulations from the (unavailable)\nlikelihood. This method is compared with an alternative approach based on the\ncomposite likelihood. We demonstrate that approximate Bayesian computing can\nresult in a lower mean square error than the composite likelihood approach when\nestimating the spatial dependence of extremes, though at an appreciably higher\ncomputational cost. We also illustrate the performance of the method with an\napplication to US temperature data to estimate the risk of crop loss due to an\nunlikely freeze event.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 20:37:21 GMT"}, {"version": "v2", "created": "Tue, 1 Nov 2011 17:13:54 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2011 17:58:27 GMT"}, {"version": "v4", "created": "Tue, 13 Dec 2011 00:13:16 GMT"}], "update_date": "2011-12-14", "authors_parsed": [["Erhardt", "Robert J.", ""], ["Smith", "Richard L.", ""]]}, {"id": "1109.4180", "submitter": "James Scott", "authors": "Nicholas G. Polson and James G. Scott", "title": "Default Bayesian analysis for multi-way tables: a data-augmentation\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a strategy for regularized estimation in multi-way\ncontingency tables, which are common in meta-analyses and multi-center clinical\ntrials. Our approach is based on data augmentation, and appeals heavily to a\nnovel class of Polya-Gamma distributions. Our main contributions are to build\nup the relevant distributional theory and to demonstrate three useful features\nof this data-augmentation scheme. First, it leads to simple EM and\nGibbs-sampling algorithms for posterior inference, circumventing the need for\nanalytic approximations, numerical integration, Metropolis--Hastings, or\nvariational methods. Second, it allows modelers much more flexibility when\nchoosing priors, which have traditionally come from the Dirichlet or\nlogistic-normal family. For example, our approach allows users to incorporate\nBayesian analogues of classical penalized-likelihood techniques (e.g. the lasso\nor bridge) in computing regularized estimates for log-odds ratios. Finally, our\ndata-augmentation scheme naturally suggests a default strategy for prior\nselection based on the logistic-Z model, which is strongly related to Jeffreys'\nprior for a binomial proportion. To illustrate the method we focus primarily on\nthe particular case of a meta-analysis/multi-center study (or a JxKxN table).\nBut the general approach encompasses many other common situations, of which we\nwill provide examples.\n", "versions": [{"version": "v1", "created": "Mon, 19 Sep 2011 22:04:04 GMT"}], "update_date": "2011-09-21", "authors_parsed": [["Polson", "Nicholas G.", ""], ["Scott", "James G.", ""]]}, {"id": "1109.4408", "submitter": "Qi Ding", "authors": "Qi Ding and Eric D. Kolaczyk", "title": "A Compressed PCA Subspace Method for Anomaly Detection in\n  High-Dimensional Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random projection is widely used as a method of dimension reduction. In\nrecent years, its combination with standard techniques of regression and\nclassification has been explored. Here we examine its use with principal\ncomponent analysis (PCA) and subspace detection methods. Specifically, we show\nthat, under appropriate conditions, with high probability the magnitude of the\nresiduals of a PCA analysis of randomly projected data behaves comparably to\nthat of the residuals of a similar PCA analysis of the original data. Our\nresults indicate the feasibility of applying subspace-based anomaly detection\nalgorithms to randomly projected data, when the data are high-dimensional but\nhave a covariance of an appropriately compressed nature. We illustrate in the\ncontext of computer network traffic anomaly detection.\n", "versions": [{"version": "v1", "created": "Tue, 20 Sep 2011 19:59:05 GMT"}, {"version": "v2", "created": "Wed, 11 Apr 2012 23:16:17 GMT"}], "update_date": "2012-04-13", "authors_parsed": [["Ding", "Qi", ""], ["Kolaczyk", "Eric D.", ""]]}, {"id": "1109.4653", "submitter": "Vilson Vieira da Silva Junior", "authors": "Vilson Vieira, Renato Fabbri, Gonzalo Travieso, Luciano da Fontoura\n  Costa", "title": "Can the evolution of music be analyzed in a quantitative manner?", "comments": "8 pages, 6 figures, added references for sections 1 and 4.C, better\n  mathematical description on section 2. New values and interpretation, now\n  considering a bootstrap method", "journal-ref": null, "doi": "10.1088/1742-5468/2012/08/P08010", "report-no": null, "categories": "physics.data-an cs.SD stat.ME", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  We propose a methodology to study music development by applying multivariate\nstatistics on composers characteristics. Seven representative composers were\nconsidered in terms of eight main musical features. Grades were assigned to\neach characteristic and their correlations were analyzed. A bootstrap method\nwas applied to simulate hundreds of artificial composers influenced by the\nseven representatives chosen. Afterwards we quantify non-numeric relations like\ndialectics, opposition and innovation. Composers differences on style and\ntechnique were represented as geometrical distances in the feature space,\nmaking it possible to quantify, for example, how much Bach and Stockhausen\ndiffer from other composers or how much Beethoven influenced Brahms. In\naddition, we compared the results with a prior investigation on philosophy.\nOpposition, strong on philosophy, was not remarkable on music. Supporting an\nobservation already considered by music theorists, strong influences were\nidentified between composers by the quantification of dialectics, implying\ninheritance and suggesting a stronger master-disciple evolution when compared\nto the philosophy analysis.\n", "versions": [{"version": "v1", "created": "Wed, 21 Sep 2011 20:43:46 GMT"}, {"version": "v2", "created": "Mon, 5 Mar 2012 00:50:09 GMT"}], "update_date": "2015-05-30", "authors_parsed": [["Vieira", "Vilson", ""], ["Fabbri", "Renato", ""], ["Travieso", "Gonzalo", ""], ["Costa", "Luciano da Fontoura", ""]]}, {"id": "1109.4706", "submitter": "Geoffrey McLachlan", "authors": "S. X. Lee and G. J. McLachlan", "title": "On the fitting of mixtures of multivariate skew t-distributions via the\n  EM algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the expectation-maximization (EM) algorithm can be applied\nexactly for the fitting of mixtures of general multivariate skew t (MST)\ndistributions, eliminating the need for computationally expensive Monte Carlo\nestimation. Finite mixtures of MST distributions have proven to be useful in\nmodelling heterogeneous data with asymmetric and heavy tail behaviour.\nRecently, they have been exploited as an effective tool for modelling flow\ncytometric data. However, without restrictions on the the characterizations of\nthe component skew t-distributions, Monte Carlo methods have been used to fit\nthese models. In this paper, we show how the EM algorithm can be implemented\nfor the iterative computation of the maximum likelihood estimates of the model\nparameters without resorting to Monte Carlo methods for mixtures with\nunrestricted MST components. The fast calculation of semi-infinite integrals on\nthe E-step of the EM algorithm is effected by noting that they can be put in\nthe form of moments of the truncated multivariate t-distribution, which\nsubsequently can be expressed in terms of the non-truncated form of the\nt-distribution function for which fast algorithms are available. We demonstrate\nthe usefulness of the proposed methodology by some applications to three real\ndata sets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2011 05:00:49 GMT"}, {"version": "v2", "created": "Wed, 5 Sep 2012 14:21:21 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Lee", "S. X.", ""], ["McLachlan", "G. J.", ""]]}, {"id": "1109.4764", "submitter": "Geoffrey McLachlan", "authors": "K. Wang, S.K. Ng, and G.J. McLachlan", "title": "Clustering of time-course gene expression profiles using normal mixture\n  models with AR(1) random effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-course gene expression data such as yeast cell cycle data may be\nperiodically expressed. To cluster such data, currently used Fourier series\napproximations of periodic gene expressions have been found not to be\nsufficiently adequate to model the complexity of the time-course data, partly\ndue to their ignoring the dependence between the expression measurements over\ntime and the correlation among gene expression profiles. We further investigate\nthe advantages and limitations of available models in the literature and\npropose a new mixture model with AR(1) random effects for the clustering of\ntime-course gene-expression profiles. Some simulations and real examples are\ngiven to demonstrate the usefulness of the proposed models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Sep 2011 10:44:51 GMT"}], "update_date": "2011-09-23", "authors_parsed": [["Wang", "K.", ""], ["Ng", "S. K.", ""], ["McLachlan", "G. J.", ""]]}, {"id": "1109.5278", "submitter": "David R. Bickel", "authors": "David R. Bickel", "title": "Controlling the degree of caution in statistical inference with the\n  Bayesian and frequentist approaches as opposite extremes", "comments": null, "journal-ref": "Bickel, D. R. (2012). Controlling the degree of caution in\n  statistical inference with the Bayesian and frequentist approaches as\n  opposite extremes. Electronic Journal of Statistics, 6, 686-709", "doi": "10.1214/12-EJS689", "report-no": null, "categories": "math.ST cs.IT math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical practice, whether a Bayesian or frequentist approach is used\nin inference depends not only on the availability of prior information but also\non the attitude taken toward partial prior information, with frequentists\ntending to be more cautious than Bayesians. The proposed framework defines that\nattitude in terms of a specified amount of caution, thereby enabling data\nanalysis at the level of caution desired and on the basis of any prior\ninformation. The caution parameter represents the attitude toward partial prior\ninformation in much the same way as a loss function represents the attitude\ntoward risk. When there is very little prior information and nonzero caution,\nthe resulting inferences correspond to those of the candidate confidence\nintervals and p-values that are most similar to the credible intervals and\nhypothesis probabilities of the specified Bayesian posterior. On the other\nhand, in the presence of a known physical distribution of the parameter,\ninferences are based only on the corresponding physical posterior. In those\nextremes of either negligible prior information or complete prior information,\ninferences do not depend on the degree of caution. Partial prior information\nbetween those two extremes leads to intermediate inferences that are more\nfrequentistic to the extent that the caution is high and more Bayesian to the\nextent that the caution is low.\n", "versions": [{"version": "v1", "created": "Sat, 24 Sep 2011 15:50:45 GMT"}], "update_date": "2012-05-02", "authors_parsed": [["Bickel", "David R.", ""]]}, {"id": "1109.6090", "submitter": "Eric Chi", "authors": "Eric C. Chi and David W. Scott", "title": "Robust Parametric Classification and Variable Selection by a Minimum\n  Distance Criterion", "comments": "41 pages, 9 figures", "journal-ref": "Journal of Computational and Graphical Statistics, 23(1):111-128,\n  2014", "doi": "10.1080/10618600.2012.737296", "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate a robust penalized logistic regression algorithm based on a\nminimum distance criterion. Influential outliers are often associated with the\nexplosion of parameter vector estimates, but in the context of standard\nlogistic regression, the bias due to outliers always causes the parameter\nvector to implode, that is shrink towards the zero vector. Thus, using\nLASSO-like penalties to perform variable selection in the presence of outliers\ncan result in missed detections of relevant covariates. We show that by\nchoosing a minimum distance criterion together with an Elastic Net penalty, we\ncan simultaneously find a parsimonious model and avoid estimation implosion\neven in the presence of many outliers in the important small $n$ large $p$\nsituation. Implementation using an MM algorithm is described and performance\nevaluated.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 04:09:37 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2012 04:29:12 GMT"}, {"version": "v3", "created": "Sat, 29 Sep 2012 23:15:18 GMT"}], "update_date": "2014-02-21", "authors_parsed": [["Chi", "Eric C.", ""], ["Scott", "David W.", ""]]}, {"id": "1109.6239", "submitter": "Monia Lupparelli", "authors": "Alberto Roverato, Monia Lupparelli and Luca La Rocca", "title": "Log-mean linear models for binary data", "comments": null, "journal-ref": "Biometrika 2013", "doi": "10.1093/biomet/ass080", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel class of models for binary data, which we call\nlog-mean linear models. The characterizing feature of these models is that they\nare specified by linear constraints on the log-mean linear parameter, defined\nas a log-linear expansion of the mean parameter of the multivariate Bernoulli\ndistribution. We show that marginal independence relationships between\nvariables can be specified by setting certain log-mean linear interactions to\nzero and, more specifically, that graphical models of marginal independence are\nlog-mean linear models. Our approach overcomes some drawbacks of the existing\nparameterizations of graphical models of marginal independence.\n", "versions": [{"version": "v1", "created": "Wed, 28 Sep 2011 15:21:40 GMT"}, {"version": "v2", "created": "Thu, 9 Feb 2012 10:31:49 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2012 13:16:55 GMT"}, {"version": "v4", "created": "Fri, 14 Dec 2012 16:45:51 GMT"}], "update_date": "2013-01-14", "authors_parsed": [["Roverato", "Alberto", ""], ["Lupparelli", "Monia", ""], ["La Rocca", "Luca", ""]]}, {"id": "1109.6405", "submitter": "P. Lahiri", "authors": "P. Lahiri, Eric Slud", "title": "Introduction", "comments": "Published in at http://dx.doi.org/10.1214/11-STS359 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2011, Vol. 26, No. 2, 161-161", "doi": "10.1214/11-STS359", "report-no": "IMS-STS-STS359", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Statistics Consortium at the University of Maryland, College Park, hosted\na two-day workshop on Bayesian Methods that Frequentists Should Know during\nApril 30--May 1, 2008. The event was co-sponsored by the Institute of\nMathematical Statistics (IMS), Office of Research and Methodology, National\nCenter for Health Statistics, Survey Research Methods Section (SRMS) of the\nAmerican Statistical Association, and Washington Statistical Society. The\nworkshop was intended to bring out the positive features of Bayesian statistics\nin solving real-life problems, including complex problems in sample surveys and\nproduction of high-quality official statistics.\n", "versions": [{"version": "v1", "created": "Thu, 29 Sep 2011 05:32:47 GMT"}], "update_date": "2011-09-30", "authors_parsed": [["Lahiri", "P.", ""], ["Slud", "Eric", ""]]}]