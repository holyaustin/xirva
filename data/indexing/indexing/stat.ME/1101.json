[{"id": "1101.0031", "submitter": "Teo Sharia", "authors": "Teo Sharia", "title": "Truncated Stochastic Approximation with Moving Bounds: Convergence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a wide class of truncated stochastic approximation\nprocedures with moving random bounds. While we believe that the proposed class\nof procedures will find its way to a wider range of applications, the main\nmotivation is to accommodate applications to parametric statistical estimation\ntheory. Our class of stochastic approximation procedures has three main\ncharacteristics: truncations with random moving bounds, a matrix valued random\nstep-size sequence, and dynamically changing random regression function. We\nestablish convergence and consider several examples to illustrate the results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Dec 2010 02:17:18 GMT"}, {"version": "v2", "created": "Wed, 12 Jan 2011 13:03:20 GMT"}, {"version": "v3", "created": "Sun, 7 Aug 2011 16:34:48 GMT"}, {"version": "v4", "created": "Wed, 2 May 2012 23:05:22 GMT"}], "update_date": "2012-05-04", "authors_parsed": [["Sharia", "Teo", ""]]}, {"id": "1101.0047", "submitter": "Xia Cui", "authors": "Xia Cui, Heng Peng, Songqiao Wen, Lixing Zhu", "title": "Component Selection in the Additive Regression Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similar to variable selection in the linear regression model, selecting\nsignificant components in the popular additive regression model is of great\ninterest. However, such components are unknown smooth functions of independent\nvariables, which are unobservable. As such, some approximation is needed. In\nthis paper, we suggest a combination of penalized regression spline\napproximation and group variable selection, called the lasso-type spline method\n(LSM), to handle this component selection problem with a diverging number of\nstrongly correlated variables in each group. It is shown that the proposed\nmethod can select significant components and estimate nonparametric additive\nfunction components simultaneously with an optimal convergence rate\nsimultaneously. To make the LSM stable in computation and able to adapt its\nestimators to the level of smoothness of the component functions, weighted\npower spline bases and projected weighted power spline bases are proposed.\nTheir performance is examined by simulation studies across two set-ups with\nindependent predictors and correlated predictors, respectively, and appears\nsuperior to the performance of competing methods. The proposed method is\nextended to a partial linear regression model analysis with real data, and\ngives reliable results.\n", "versions": [{"version": "v1", "created": "Thu, 30 Dec 2010 07:45:18 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Cui", "Xia", ""], ["Peng", "Heng", ""], ["Wen", "Songqiao", ""], ["Zhu", "Lixing", ""]]}, {"id": "1101.0240", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Zoubin Ghahramani", "title": "Generalised Wishart Processes", "comments": "14 pages, 4 figures, 1 table. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR q-fin.CP q-fin.ST stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a stochastic process with Wishart marginals: the generalised\nWishart process (GWP). It is a collection of positive semi-definite random\nmatrices indexed by any arbitrary dependent variable. We use it to model\ndynamic (e.g. time varying) covariance matrices. Unlike existing models, it can\ncapture a diverse class of covariance structures, it can easily handle missing\ndata, the dependent variable can readily include covariates other than time,\nand it scales well with dimension; there is no need for free parameters, and\noptional parameters are easy to interpret. We describe how to construct the\nGWP, introduce general procedures for inference and predictions, and show that\nit outperforms its main competitor, multivariate GARCH, even on financial data\nthat especially suits GARCH. We also show how to predict the mean of a\nmultivariate process while accounting for dynamic correlations.\n", "versions": [{"version": "v1", "created": "Fri, 31 Dec 2010 12:25:01 GMT"}], "update_date": "2011-01-04", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1101.0305", "submitter": "David R. Bickel", "authors": "David R. Bickel", "title": "Measuring support for a hypothesis about a random parameter without\n  estimating its unknown prior", "comments": "Errors in the first version were corrected, and the methodology is\n  now applied to more interesting data", "journal-ref": "D. R. Bickel, Minimax-optimal strength of statistical evidence for\n  a composite alternative hypothesis, International Statistical Review 81,\n  188-206 (2013)", "doi": "10.1111/insr.12008", "report-no": null, "categories": "math.ST cs.IT math.IT q-bio.QM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For frequentist settings in which parameter randomness represents variability\nrather than uncertainty, the ideal measure of the support for one hypothesis\nover another is the difference in the posterior and prior log odds. For\nsituations in which the prior distribution cannot be accurately estimated, that\nideal support may be replaced by another measure of support, which may be any\npredictor of the ideal support that, on a per-observation basis, is\nasymptotically unbiased. Two qualifying measures of support are defined. The\nfirst is minimax optimal with respect to the population and is equivalent to a\nparticular Bayes factor. The second is worst-sample minimax optimal and is\nequivalent to the normalized maximum likelihood. It has been extended by\nlikelihood weights for compatibility with more general models.\n  One such model is that of two independent normal samples, the standard\nsetting for gene expression microarray data analysis. Applying that model to\nproteomics data indicates that support computed from data for a single protein\ncan closely approximate the estimated difference in posterior and prior odds\nthat would be available with the data for 20 proteins. This suggests the\napplicability of random-parameter models to other situations in which the\nparameter distribution cannot be reliably estimated.\n", "versions": [{"version": "v1", "created": "Fri, 31 Dec 2010 22:32:51 GMT"}, {"version": "v2", "created": "Sat, 2 Apr 2011 20:29:05 GMT"}], "update_date": "2013-09-03", "authors_parsed": [["Bickel", "David R.", ""]]}, {"id": "1101.0391", "submitter": "Silvia Pandolfi Miss", "authors": "Francesco Bartolucci and Silvia Pandolfi", "title": "Bayesian inference for a class of latent Markov models for categorical\n  longitudinal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a Bayesian inference approach for a class of latent Markov models.\nThese models are widely used for the analysis of longitudinal categorical data,\nwhen the interest is in studying the evolution of an individual unobservable\ncharacteristic. We consider, in particular, the basic latent Markov, which does\nnot account for individual covariates, and its version that includes such\ncovariates in the measurement model. The proposed inferential approach is based\non a system of priors formulated on a transformation of the initial and\ntransition probabilities of the latent Markov chain. This system of priors is\nequivalent to one based on Dirichlet distributions. In order to draw samples\nfrom the joint posterior distribution of the parameters and the number of\nlatent states, we implement a reversible jump algorithm which alternates moves\nof Metropolis-Hastings type with moves of split/combine and birth/death types.\nThe proposed approach is illustrated through two applications based on\nlongitudinal datasets.\n", "versions": [{"version": "v1", "created": "Sun, 2 Jan 2011 09:10:20 GMT"}, {"version": "v2", "created": "Tue, 4 Jan 2011 11:58:25 GMT"}], "update_date": "2011-01-05", "authors_parsed": [["Bartolucci", "Francesco", ""], ["Pandolfi", "Silvia", ""]]}, {"id": "1101.0632", "submitter": "Doug Speed", "authors": "Doug Speed, Simon Tavar\\'e", "title": "Sparse Partitioning: Nonlinear regression with binary or tertiary\n  predictors, with application to association studies", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS411 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 2A, 873-893", "doi": "10.1214/10-AOAS411", "report-no": "IMS-AOAS-AOAS411", "categories": "q-bio.QM stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents Sparse Partitioning, a Bayesian method for identifying\npredictors that either individually or in combination with others affect a\nresponse variable. The method is designed for regression problems involving\nbinary or tertiary predictors and allows the number of predictors to exceed the\nsize of the sample, two properties which make it well suited for association\nstudies. Sparse Partitioning differs from other regression methods by placing\nno restrictions on how the predictors may influence the response. To compensate\nfor this generality, Sparse Partitioning implements a novel way of exploring\nthe model space. It searches for high posterior probability partitions of the\npredictor set, where each partition defines groups of predictors that jointly\ninfluence the response. The result is a robust method that requires no prior\nknowledge of the true predictor--response relationship. Testing on simulated\ndata suggests Sparse Partitioning will typically match the performance of an\nexisting method on a data set which obeys the existing method's model\nassumptions. When these assumptions are violated, Sparse Partitioning will\ngenerally offer superior performance.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jan 2011 00:43:09 GMT"}, {"version": "v2", "created": "Tue, 30 Aug 2011 06:11:26 GMT"}], "update_date": "2011-08-31", "authors_parsed": [["Speed", "Doug", ""], ["Tavar\u00e9", "Simon", ""]]}, {"id": "1101.0831", "submitter": "Lily Wang", "authors": "Li Wang and Suojin Wang", "title": "Nonparametric Additive Model-assisted Estimation for Survey Data", "comments": null, "journal-ref": "Journal of Multivariate Analysis, 2011", "doi": "10.1016/j.jmva.2011.03.006", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An additive model-assisted nonparametric method is investigated to estimate\nthe finite population totals of massive survey data with the aid of auxiliary\ninformation. A class of estimators is proposed to improve the precision of the\nwell known Horvitz-Thompson estimators by combining the spline and local\npolynomial smoothing methods. These estimators are calibrated, asymptotically\ndesign-unbiased, consistent, normal and robust in the sense of asymptotically\nattaining the Godambe-Joshi lower bound to the anticipated variance. A\nconsistent model selection procedure is further developed to select the\nsignificant auxiliary variables. The proposed method is sufficiently fast to\nanalyze large survey data of high dimension within seconds. The performance of\nthe proposed method is assessed empirically via simulation studies.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jan 2011 21:43:11 GMT"}], "update_date": "2019-03-19", "authors_parsed": [["Wang", "Li", ""], ["Wang", "Suojin", ""]]}, {"id": "1101.0891", "submitter": "Galit Shmueli", "authors": "Galit Shmueli", "title": "To Explain or to Predict?", "comments": "Published in at http://dx.doi.org/10.1214/10-STS330 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 3, 289-310", "doi": "10.1214/10-STS330", "report-no": "IMS-STS-STS330", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical modeling is a powerful tool for developing and testing theories\nby way of causal explanation, prediction, and description. In many disciplines\nthere is near-exclusive use of statistical modeling for causal explanation and\nthe assumption that models with high explanatory power are inherently of high\npredictive power. Conflation between explanation and prediction is common, yet\nthe distinction must be understood for progressing scientific knowledge. While\nthis distinction has been recognized in the philosophy of science, the\nstatistical literature lacks a thorough discussion of the many differences that\narise in the process of modeling for an explanatory versus a predictive goal.\nThe purpose of this article is to clarify the distinction between explanatory\nand predictive modeling, to discuss its sources, and to reveal the practical\nimplications of the distinction to each step in the modeling process.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 07:08:16 GMT"}], "update_date": "2011-01-06", "authors_parsed": [["Shmueli", "Galit", ""]]}, {"id": "1101.0899", "submitter": "Nader Ebrahimi", "authors": "Nader Ebrahimi, Ehsan S. Soofi, Refik Soyer", "title": "On the Sample Information About Parameter and Prediction", "comments": "Published in at http://dx.doi.org/10.1214/10-STS329 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 3, 348-367", "doi": "10.1214/10-STS329", "report-no": "IMS-STS-STS329", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Bayesian measure of sample information about the parameter, known as\nLindley's measure, is widely used in various problems such as developing prior\ndistributions, models for the likelihood functions and optimal designs. The\npredictive information is defined similarly and used for model selection and\noptimal designs, though to a lesser extent. The parameter and predictive\ninformation measures are proper utility functions and have been also used in\ncombination. Yet the relationship between the two measures and the effects of\nconditional dependence between the observable quantities on the Bayesian\ninformation measures remain unexplored. We address both issues. The\nrelationship between the two information measures is explored through the\ninformation provided by the sample about the parameter and prediction jointly.\nThe role of dependence is explored along with the interplay between the\ninformation measures, prior and sampling design. For the conditionally\nindependent sequence of observable quantities, decompositions of the joint\ninformation characterize Lindley's measure as the sample information about the\nparameter and prediction jointly and the predictive information as part of it.\nFor the conditionally dependent case, the joint information about parameter and\nprediction exceeds Lindley's measure by an amount due to the dependence. More\nspecific results are shown for the normal linear models and a broad subfamily\nof the exponential family. Conditionally independent samples provide relatively\nlittle information for prediction, and the gap between the parameter and\npredictive information measures grows rapidly with the sample size.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 08:20:26 GMT"}], "update_date": "2011-01-06", "authors_parsed": [["Ebrahimi", "Nader", ""], ["Soofi", "Ehsan S.", ""], ["Soyer", "Refik", ""]]}, {"id": "1101.0901", "submitter": "Vanessa Didelez", "authors": "Vanessa Didelez, Svend Kreiner, Niels Keiding", "title": "Graphical Models for Inference Under Outcome-Dependent Sampling", "comments": "Published in at http://dx.doi.org/10.1214/10-STS340 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 3, 368-387", "doi": "10.1214/10-STS340", "report-no": "IMS-STS-STS340", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider situations where data have been collected such that the sampling\ndepends on the outcome of interest and possibly further covariates, as for\ninstance in case-control studies. Graphical models represent assumptions about\nthe conditional independencies among the variables. By including a node for the\nsampling indicator, assumptions about sampling processes can be made explicit.\nWe demonstrate how to read off such graphs whether consistent estimation of the\nassociation between exposure and outcome is possible. Moreover, we give\nsufficient graphical conditions for testing and estimating the causal effect of\nexposure on outcome. The practical use is illustrated with a number of\nexamples.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 08:32:16 GMT"}], "update_date": "2011-01-06", "authors_parsed": [["Didelez", "Vanessa", ""], ["Kreiner", "Svend", ""], ["Keiding", "Niels", ""]]}, {"id": "1101.0905", "submitter": "Haim Bar", "authors": "Haim Bar, James Booth, Elizabeth Schifano, Martin T. Wells", "title": "Laplace Approximated EM Microarray Analysis: An Empirical Bayes Approach\n  for Comparative Microarray Experiments", "comments": "Published in at http://dx.doi.org/10.1214/10-STS339 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 3, 388-407", "doi": "10.1214/10-STS339", "report-no": "IMS-STS-STS339", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A two-groups mixed-effects model for the comparison of (normalized)\nmicroarray data from two treatment groups is considered. Most competing\nparametric methods that have appeared in the literature are obtained as special\ncases or by minor modification of the proposed model. Approximate maximum\nlikelihood fitting is accomplished via a fast and scalable algorithm, which we\ncall LEMMA (Laplace approximated EM Microarray Analysis). The posterior odds of\ntreatment $\\times$ gene interactions, derived from the model, involve shrinkage\nestimates of both the interactions and of the gene specific error variances.\nGenes are classified as being associated with treatment based on the posterior\nodds and the local false discovery rate (f.d.r.) with a fixed cutoff. Our\nmodel-based approach also allows one to declare the non-null status of a gene\nby controlling the false discovery rate (FDR). It is shown in a detailed\nsimulation study that the approach outperforms well-known competitors. We also\napply the proposed methodology to two previously analyzed microarray examples.\nExtensions of the proposed method to paired treatments and multiple treatments\nare also discussed.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 08:45:42 GMT"}], "update_date": "2011-01-06", "authors_parsed": [["Bar", "Haim", ""], ["Booth", "James", ""], ["Schifano", "Elizabeth", ""], ["Wells", "Martin T.", ""]]}, {"id": "1101.0912", "submitter": "Daniel Pe\\~{n}a", "authors": "Daniel Pe\\~na, Ruey S. Tsay", "title": "A Conversation with George C. Tiao", "comments": "Published in at http://dx.doi.org/10.1214/09-STS292 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 3, 408-428", "doi": "10.1214/09-STS292", "report-no": "IMS-STS-STS292", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  George C. Tiao was born in London in 1933. After graduating with a B.A. in\nEconomics from National Taiwan University in 1955 he went to the US to obtain\nan M.B.A from New York University in 1958 and a Ph.D. in Economics from the\nUniversity of Wisconsin, Madison in 1962. From 1962 to 1982 he was Assistant,\nAssociate, Professor and Bascom Professor of Statistics and Business at the\nUniversity of Wisconsin, Madison, and in the period 1973--1975 was Chairman of\nthe Department of Statistics. He moved to the Graduate School of Business at\nthe University of Chicago in 1982 and is the W. Allen Wallis Professor of\nEconometrics and Statistics (emeritus). George Tiao has played a leading role\nin the development of Bayesian Statistics, Time Series Analysis and\nEnvironmental Statistics. He is co-author, with G.E.P. Box, of Bayesian\nInference in Statistical Analysis and is the developer of a model-based\napproach to seasonal adjustment (with S. C. Hillmer), of outlier analysis in\ntime series (with I. Chang), and of new ways of vector ARMA model building\n(with R. S. Tsay). He is the author/co-author/co-editor of 7 books and over 120\narticles in refereed econometric, environmental and statistical journals and\nhas been thesis advisor of over 25 students. He is a leading figure in the\ndevelopment of Statistics in Taiwan and China and is the Founding President of\nthe International Chinese Statistical Association 1987--1988 and the Founding\nChair Editor of the journal Statistica Sinica 1988--1993. He played a leading\nrole (over the 20 year period 1979--1999) in the organization of the annual\nNBER/NSF Time Series Workshop and he was a founding member of the annual\nconference \"Making Statistics More Effective in Schools of Business\"\n1986--2006.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 09:16:35 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Pe\u00f1a", "Daniel", ""], ["Tsay", "Ruey S.", ""]]}, {"id": "1101.0935", "submitter": "A. J. van Es", "authors": "Martina Bene\\v{s}ov\\'a, Bert van Es and Peter Tegelaar", "title": "Bivariate Uniform Deconvolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a density estimator in the bivariate uniform deconvolution\nmodel. For this model we derive four inversion formulas to express the\nbivariate density that we want to estimate in terms of the bivariate density of\nthe observations. By substituting a kernel density estimator of the density of\nthe observations we then get four different estimators. Next we construct an\nasymptotically optimal convex combination of these four estimators. Expansions\nfor the bias, variance, as well as asymptotic normality, are derived. Some\nsimulated examples are presented.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 10:49:23 GMT"}, {"version": "v2", "created": "Fri, 7 Jan 2011 14:13:44 GMT"}, {"version": "v3", "created": "Wed, 8 Jun 2011 07:38:20 GMT"}], "update_date": "2011-06-09", "authors_parsed": [["Bene\u0161ov\u00e1", "Martina", ""], ["van Es", "Bert", ""], ["Tegelaar", "Peter", ""]]}, {"id": "1101.0990", "submitter": "Geert Molenberghs", "authors": "Geert Molenberghs, Geert Verbeke, Clarice G. B. Dem\\'etrio, Afr\\^anio\n  M. C. Vieira", "title": "A Family of Generalized Linear Models for Repeated Measures with Normal\n  and Conjugate Random Effects", "comments": "Published in at http://dx.doi.org/10.1214/10-STS328 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2010, Vol. 25, No. 3, 325-347", "doi": "10.1214/10-STS328", "report-no": "IMS-STS-STS328", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-Gaussian outcomes are often modeled using members of the so-called\nexponential family. Notorious members are the Bernoulli model for binary data,\nleading to logistic regression, and the Poisson model for count data, leading\nto Poisson regression. Two of the main reasons for extending this family are\n(1) the occurrence of overdispersion, meaning that the variability in the data\nis not adequately described by the models, which often exhibit a prescribed\nmean--variance link, and (2) the accommodation of hierarchical structure in the\ndata, stemming from clustering in the data which, in turn, may result from\nrepeatedly measuring the outcome, for various members of the same family, etc.\nThe first issue is dealt with through a variety of overdispersion models, such\nas, for example, the beta-binomial model for grouped binary data and the\nnegative-binomial model for counts. Clustering is often accommodated through\nthe inclusion of random subject-specific effects. Though not always, one\nconventionally assumes such random effects to be normally distributed. While\nboth of these phenomena may occur simultaneously, models combining them are\nuncommon. This paper proposes a broad class of generalized linear models\naccommodating overdispersion and clustering through two separate sets of random\neffects. We place particular emphasis on so-called conjugate random effects at\nthe level of the mean for the first aspect and normal random effects embedded\nwithin the linear predictor for the second aspect, even though our family is\nmore general. The binary, count and time-to-event cases are given particular\nemphasis. Apart from model formulation, we present an overview of estimation\nmethods, and then settle for maximum likelihood estimation with\nanalytic--numerical integration. Implications for the derivation of marginal\ncorrelations functions are discussed. The methodology is applied to data from a\nstudy in epileptic seizures, a clinical trial in toenail infection named\nonychomycosis and survival data in children with asthma.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 14:35:29 GMT"}], "update_date": "2016-08-14", "authors_parsed": [["Molenberghs", "Geert", ""], ["Verbeke", "Geert", ""], ["Dem\u00e9trio", "Clarice G. B.", ""], ["Vieira", "Afr\u00e2nio M. C.", ""]]}, {"id": "1101.1032", "submitter": "Emilio Seijo", "authors": "Emilio Seijo, Bodhisattva Sen", "title": "Change-point in stochastic design regression and the bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the consistency of different bootstrap procedures for\nconstructing confidence intervals (CIs) for the unique jump discontinuity\n(change-point) in an otherwise smooth regression function in a stochastic\ndesign setting. This problem exhibits nonstandard asymptotics and we argue that\nthe standard bootstrap procedures in regression fail to provide valid\nconfidence intervals for the change-point. We propose a version of smoothed\nbootstrap, illustrate its remarkable finite sample performance in our\nsimulation study, and prove the consistency of the procedure. The $m$ out of\n$n$ bootstrap procedure is also considered and shown to be consistent. We also\nprovide sufficient conditions for any bootstrap procedure to be consistent in\nthis scenario.\n", "versions": [{"version": "v1", "created": "Wed, 5 Jan 2011 17:17:36 GMT"}], "update_date": "2011-01-06", "authors_parsed": [["Seijo", "Emilio", ""], ["Sen", "Bodhisattva", ""]]}, {"id": "1101.1359", "submitter": "Pavel  Krivitsky", "authors": "Pavel N. Krivitsky (Department of Statistics, Pennsylvania State\n  University, University Park)", "title": "Exponential-Family Random Graph Models for Valued Networks", "comments": "42 pages, including 2 appendixes (3 pages total), 5 figures, 2\n  tables, 1 algorithm listing; a substantial revision and reorganization: major\n  changes include focus shifted to counts in particular, sections added on\n  modeling actor heterogeneity, a subsection on degeneracy, another example,\n  and an appendix on non-steepness of the CMP distribution", "journal-ref": "Electron. J. Statist. 6 (2012) 1100-1128", "doi": "10.1214/12-EJS696", "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential-family random graph models (ERGMs) provide a principled and\nflexible way to model and simulate features common in social networks, such as\npropensities for homophily, mutuality, and friend-of-a-friend triad closure,\nthrough choice of model terms (sufficient statistics). However, those ERGMs\nmodeling the more complex features have, to date, been limited to binary data:\npresence or absence of ties. Thus, analysis of valued networks, such as those\nwhere counts, measurements, or ranks are observed, has necessitated\ndichotomizing them, losing information and introducing biases.\n  In this work, we generalize ERGMs to valued networks. Focusing on modeling\ncounts, we formulate an ERGM for networks whose ties are counts and discuss\nissues that arise when moving beyond the binary case. We introduce model terms\nthat generalize and model common social network features for such data and\napply these methods to a network dataset whose values are counts of\ninteractions.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jan 2011 06:26:49 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2012 07:10:54 GMT"}], "update_date": "2012-08-01", "authors_parsed": [["Krivitsky", "Pavel N.", "", "Department of Statistics, Pennsylvania State\n  University, University Park"]]}, {"id": "1101.1438", "submitter": "Rebecca Killick", "authors": "R. Killick, P. Fearnhead and I. A. Eckley", "title": "Optimal detection of changepoints with a linear computational cost", "comments": "25 pages, 4 figures, To appear in Journal of the American Statistical\n  Association", "journal-ref": "Journal of the American Statistical Association 107(500), pp.\n  1590-1598 (2012)", "doi": "10.1080/01621459.2012.737745", "report-no": null, "categories": "stat.ME q-bio.GN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of detecting multiple changepoints in large data\nsets. Our focus is on applications where the number of changepoints will\nincrease as we collect more data: for example in genetics as we analyse larger\nregions of the genome, or in finance as we observe time-series over longer\nperiods. We consider the common approach of detecting changepoints through\nminimising a cost function over possible numbers and locations of changepoints.\nThis includes several established procedures for detecting changing points,\nsuch as penalised likelihood and minimum description length. We introduce a new\nmethod for finding the minimum of such cost functions and hence the optimal\nnumber and location of changepoints that has a computational cost which, under\nmild conditions, is linear in the number of observations. This compares\nfavourably with existing methods for the same problem whose computational cost\ncan be quadratic or even cubic. In simulation studies we show that our new\nmethod can be orders of magnitude faster than these alternative exact methods.\nWe also compare with the Binary Segmentation algorithm for identifying\nchangepoints, showing that the exactness of our approach can lead to\nsubstantial improvements in the accuracy of the inferred segmentation of the\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jan 2011 14:13:12 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2011 11:10:53 GMT"}, {"version": "v3", "created": "Tue, 9 Oct 2012 08:47:31 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Killick", "R.", ""], ["Fearnhead", "P.", ""], ["Eckley", "I. A.", ""]]}, {"id": "1101.1444", "submitter": "Tilmann Gneiting", "authors": "Tilmann Gneiting, Hana \\v{S}ev\\v{c}\\'ikov\\'a, Donald B. Percival", "title": "Estimators of Fractal Dimension: Assessing the Roughness of Time Series\n  and Spatial Data", "comments": "Published in at http://dx.doi.org/10.1214/11-STS370 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2012, Vol. 27, No. 2, 247-277", "doi": "10.1214/11-STS370", "report-no": "IMS-STS-STS370", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The fractal or Hausdorff dimension is a measure of roughness (or smoothness)\nfor time series and spatial data. The graph of a smooth, differentiable surface\nindexed in $\\mathbb{R}^d$ has topological and fractal dimension $d$. If the\nsurface is nondifferentiable and rough, the fractal dimension takes values\nbetween the topological dimension, $d$, and $d+1$. We review and assess\nestimators of fractal dimension by their large sample behavior under infill\nasymptotics, in extensive finite sample simulation studies, and in a data\nexample on arctic sea-ice profiles. For time series or line transect data,\nbox-count, Hall--Wood, semi-periodogram, discrete cosine transform and wavelet\nestimators are studied along with variation estimators with power indices 2\n(variogram) and 1 (madogram), all implemented in the R package fractaldim.\nConsidering both efficiency and robustness, we recommend the use of the\nmadogram estimator, which can be interpreted as a statistically more efficient\nversion of the Hall--Wood estimator. For two-dimensional lattice data, we\npropose robust transect estimators that use the median of variation estimates\nalong rows and columns. Generally, the link between power variations of index\n$p>0$ for stochastic processes, and the Hausdorff dimension of their sample\npaths, appears to be particularly robust and inclusive when $p=1$.\n", "versions": [{"version": "v1", "created": "Fri, 7 Jan 2011 14:26:43 GMT"}, {"version": "v2", "created": "Wed, 25 Jul 2012 08:04:38 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Gneiting", "Tilmann", ""], ["\u0160ev\u010d\u00edkov\u00e1", "Hana", ""], ["Percival", "Donald B.", ""]]}, {"id": "1101.2017", "submitter": "Emily Fox", "authors": "Emily Fox and David Dunson", "title": "Bayesian Nonparametric Covariance Regression", "comments": "33 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there is a rich literature on methods for allowing the variance in a\nunivariate regression model to vary with predictors, time and other factors,\nrelatively little has been done in the multivariate case. Our focus is on\ndeveloping a class of nonparametric covariance regression models, which allow\nan unknown p x p covariance matrix to change flexibly with predictors. The\nproposed modeling framework induces a prior on a collection of covariance\nmatrices indexed by predictors through priors for predictor-dependent loadings\nmatrices in a factor model. In particular, the predictor-dependent loadings are\ncharacterized as a sparse combination of a collection of unknown dictionary\nfunctions (e.g, Gaussian process random functions). The induced covariance is\nthen a regularized quadratic function of these dictionary elements. Our\nproposed framework leads to a highly-flexible, but computationally tractable\nformulation with simple conjugate posterior updates that can readily handle\nmissing data. Theoretical properties are discussed and the methods are\nillustrated through simulations studies and an application to the Google Flu\nTrends data.\n", "versions": [{"version": "v1", "created": "Tue, 11 Jan 2011 04:25:21 GMT"}, {"version": "v2", "created": "Tue, 8 Feb 2011 23:44:49 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Fox", "Emily", ""], ["Dunson", "David", ""]]}, {"id": "1101.2338", "submitter": "Gergely Palla", "authors": "G. Palla, P. Pollner, T. Vicsek", "title": "Rotated multifractal network generator", "comments": "Accepted for publication in JSTAT", "journal-ref": "J. Stat. Mech. (2011) P02003", "doi": "10.1088/1742-5468/2011/02/P02003", "report-no": null, "categories": "physics.data-an math-ph math.MP physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recently introduced multifractal network generator (MFNG), has been shown\nto provide a simple and flexible tool for creating random graphs with very\ndiverse features. The MFNG is based on multifractal measures embedded in 2d,\nleading also to isolated nodes, whose number is relatively low for realistic\ncases, but may become dominant in the limiting case of infinitely large network\nsizes. Here we discuss the relation between this effect and the information\ndimension for the 1d projection of the link probability measure (LPM), and\nargue that the node isolation can be avoided by a simple transformation of the\nLPM based on rotation.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jan 2011 12:19:51 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Palla", "G.", ""], ["Pollner", "P.", ""], ["Vicsek", "T.", ""]]}, {"id": "1101.2374", "submitter": "Charles Bouveyron", "authors": "Charles Bouveyron and Camille Brunet", "title": "Simultaneous model-based clustering and visualization in the Fisher\n  discriminative subspace", "comments": null, "journal-ref": "Statistics and Computing, 2011", "doi": "10.1007/s11222-011-9249-9", "report-no": null, "categories": "stat.ME stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering in high-dimensional spaces is nowadays a recurrent problem in many\nscientific domains but remains a difficult task from both the clustering\naccuracy and the result understanding points of view. This paper presents a\ndiscriminative latent mixture (DLM) model which fits the data in a latent\northonormal discriminative subspace with an intrinsic dimension lower than the\ndimension of the original space. By constraining model parameters within and\nbetween groups, a family of 12 parsimonious DLM models is exhibited which\nallows to fit onto various situations. An estimation algorithm, called the\nFisher-EM algorithm, is also proposed for estimating both the mixture\nparameters and the discriminative subspace. Experiments on simulated and real\ndatasets show that the proposed approach performs better than existing\nclustering methods while providing a useful representation of the clustered\ndata. The method is as well applied to the clustering of mass spectrometry\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 12 Jan 2011 14:40:06 GMT"}, {"version": "v2", "created": "Tue, 19 Apr 2011 08:21:44 GMT"}], "update_date": "2011-04-20", "authors_parsed": [["Bouveyron", "Charles", ""], ["Brunet", "Camille", ""]]}, {"id": "1101.2481", "submitter": "Art Owen", "authors": "Justin S. Dyer, Art B. Owen", "title": "Correct ordering in the Zipf-Poisson ensemble", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a Zipf--Poisson ensemble in which $X_i\\sim\\poi(Ni^{-\\alpha})$ for\n$\\alpha>1$ and $N>0$ and integers $i\\ge 1$. As $N\\to\\infty$ the first $n'(N)$\nrandom variables have their proper order $X_1>X_2>...>X_{n'}$ relative to each\nother, with probability tending to 1 for $n'$ up to\n$(AN/\\log(N))^{1/(\\alpha+2)}$ for an explicit constant $A(\\alpha)\\ge 3/4$. The\nrate $N^{1/(\\alpha+2)}$ cannot be achieved. The ordering of the first $n'(N)$\nentities does not preclude $X_m>X_{n'}$ for some interloping $m>n'$. The first\n$n\"$ random variables are correctly ordered exclusive of any interlopers, with\nprobability tending to 1 if $n\"\\le (BN/\\log(N))^{1/(\\alpha+2)}$ for $B<A$. For\na Zipf--Poisson model of the British National Corpus, which has a total word\ncount of $100{,}000{,}000$, our result estimates that the 72 words with the\nhighest counts are properly ordered.\n", "versions": [{"version": "v1", "created": "Thu, 13 Jan 2011 03:17:01 GMT"}], "update_date": "2011-01-14", "authors_parsed": [["Dyer", "Justin S.", ""], ["Owen", "Art B.", ""]]}, {"id": "1101.3231", "submitter": "Sean Simpson", "authors": "Sean L. Simpson", "title": "An Adjusted Likelihood Ratio Test for Separability in Unbalanced\n  Multivariate Repeated Measures Data", "comments": null, "journal-ref": "Statistical Methodology (2010) 7, 511-519", "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an adjusted likelihood ratio test of two-factor separability\n(Kronecker product structure) for unbalanced multivariate repeated measures\ndata. Here we address the particular case where the within subject correlation\nis believed to decrease exponentially in both dimensions (e.g., temporal and\nspatial dimensions). However, the test can be easily generalized to factor\nspecific matrices of any structure. A simulation study is conducted to assess\nthe inference accuracy of the proposed test. Longitudinal medical imaging data\nconcerning schizophrenia and caudate morphology illustrates the methodology.\n", "versions": [{"version": "v1", "created": "Mon, 17 Jan 2011 14:45:29 GMT"}, {"version": "v2", "created": "Tue, 1 Feb 2011 15:58:09 GMT"}], "update_date": "2011-02-02", "authors_parsed": [["Simpson", "Sean L.", ""]]}, {"id": "1101.3462", "submitter": "Nicolas Dobigeon", "authors": "Olivier Besson, Nicolas Dobigeon and Jean-Yves Tourneret", "title": "Minimum mean square distance estimation of a subspace", "comments": null, "journal-ref": null, "doi": "10.1109/TSP.2011.2166548", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of subspace estimation in a Bayesian setting. Since\nwe are operating in the Grassmann manifold, the usual approach which consists\nof minimizing the mean square error (MSE) between the true subspace $U$ and its\nestimate $\\hat{U}$ may not be adequate as the MSE is not the natural metric in\nthe Grassmann manifold. As an alternative, we propose to carry out subspace\nestimation by minimizing the mean square distance (MSD) between $U$ and its\nestimate, where the considered distance is a natural metric in the Grassmann\nmanifold, viz. the distance between the projection matrices. We show that the\nresulting estimator is no longer the posterior mean of $U$ but entails\ncomputing the principal eigenvectors of the posterior mean of $U U^{T}$.\nDerivation of the MMSD estimator is carried out in a few illustrative examples\nincluding a linear Gaussian model for the data and a Bingham or von Mises\nFisher prior distribution for $U$. In all scenarios, posterior distributions\nare derived and the MMSD estimator is obtained either analytically or\nimplemented via a Markov chain Monte Carlo simulation method. The method is\nshown to provide accurate estimates even when the number of samples is lower\nthan the dimension of $U$. An application to hyperspectral imagery is finally\ninvestigated.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jan 2011 14:33:22 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Besson", "Olivier", ""], ["Dobigeon", "Nicolas", ""], ["Tourneret", "Jean-Yves", ""]]}, {"id": "1101.3493", "submitter": "Marine Jeanmougin", "authors": "Marine Jeanmougin, Mickael Guedj, Christophe Ambroise", "title": "Defining a robust biological prior from Pathway Analysis to drive\n  Network Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-bio.MN q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring genetic networks from gene expression data is one of the most\nchallenging work in the post-genomic era, partly due to the vast space of\npossible networks and the relatively small amount of data available. In this\nfield, Gaussian Graphical Model (GGM) provides a convenient framework for the\ndiscovery of biological networks. In this paper, we propose an original\napproach for inferring gene regulation networks using a robust biological prior\non their structure in order to limit the set of candidate networks.\n  Pathways, that represent biological knowledge on the regulatory networks,\nwill be used as an informative prior knowledge to drive Network Inference. This\napproach is based on the selection of a relevant set of genes, called the\n\"molecular signature\", associated with a condition of interest (for instance,\nthe genes involved in disease development). In this context, differential\nexpression analysis is a well established strategy. However outcome signatures\nare often not consistent and show little overlap between studies. Thus, we will\ndedicate the first part of our work to the improvement of the standard process\nof biomarker identification to guarantee the robustness and reproducibility of\nthe molecular signature.\n  Our approach enables to compare the networks inferred between two conditions\nof interest (for instance case and control networks) and help along the\nbiological interpretation of results. Thus it allows to identify differential\nregulations that occur in these conditions. We illustrate the proposed approach\nby applying our method to a study of breast cancer's response to treatment.\n", "versions": [{"version": "v1", "created": "Tue, 18 Jan 2011 16:29:49 GMT"}, {"version": "v2", "created": "Tue, 17 May 2011 09:59:38 GMT"}], "update_date": "2011-05-18", "authors_parsed": [["Jeanmougin", "Marine", ""], ["Guedj", "Mickael", ""], ["Ambroise", "Christophe", ""]]}, {"id": "1101.3594", "submitter": "Donghui Yan", "authors": "Donghui Yan, Peng Gong, Aiyou Chen and Liheng Zhong", "title": "Classification under Data Contamination with Application to Remote\n  Sensing Image Mis-registration", "comments": "23 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is motivated by the problem of image mis-registration in remote\nsensing and we are interested in determining the resulting loss in the accuracy\nof pattern classification. A statistical formulation is given where we propose\nto use data contamination to model and understand the phenomenon of image\nmis-registration. This model is widely applicable to many other types of errors\nas well, for example, measurement errors and gross errors etc. The impact of\ndata contamination on classification is studied under a statistical learning\ntheoretical framework. A closed-form asymptotic bound is established for the\nresulting loss in classification accuracy, which is less than\n$\\epsilon/(1-\\epsilon)$ for data contamination of an amount of $\\epsilon$. Our\nbound is sharper than similar bounds in the domain adaptation literature and,\nunlike such bounds, it applies to classifiers with an infinite\nVapnik-Chervonekis (VC) dimension. Extensive simulations have been conducted on\nboth synthetic and real datasets under various types of data contamination,\nincluding label flipping, feature swapping and the replacement of feature\nvalues with data generated from a random source such as a Gaussian or Cauchy\ndistribution. Our simulation results show that the bound we derive is fairly\ntight.\n", "versions": [{"version": "v1", "created": "Wed, 19 Jan 2011 00:41:43 GMT"}, {"version": "v2", "created": "Thu, 5 Jan 2012 18:04:10 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Yan", "Donghui", ""], ["Gong", "Peng", ""], ["Chen", "Aiyou", ""], ["Zhong", "Liheng", ""]]}, {"id": "1101.4185", "submitter": "Yuehua Wu", "authors": "Xiaoping Shi, Yuehua Wu and Baisuo Jin", "title": "A Novel Approach for Fast Detection of Multiple Change Points in Linear\n  Models", "comments": "37 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A change point problem occurs in many statistical applications. If there\nexist change points in a model, it is harmful to make a statistical analysis\nwithout any consideration of the existence of the change points and the results\nderived from such an analysis may be misleading. There are rich literatures on\nchange point detection. Although many methods have been proposed for detecting\nmultiple change points, using these methods to find multiple change points in a\nlarge sample seems not feasible. In this article, a connection between multiple\nchange point detection and variable selection through a proper segmentation of\ndata sequence is established, and a novel approach is proposed to tackle\nmultiple change point detection problem via the following two key steps: (1)\napply the recent advances in consistent variable selection methods such as\nSCAD, adaptive LASSO and MCP to detect change points; (2) employ a refine\nprocedure to improve the accuracy of change point estimation. Five algorithms\nare hence proposed, which can detect change points with much less time and more\naccuracy compared to those in literature. In addition, an optimal segmentation\nalgorithm based on residual sum of squares is given. Our simulation study shows\nthat the proposed algorithms are computationally efficient with improved change\npoint estimation accuracy. The new approach is readily generalized to detect\nmultiple change points in other models such as generalized linear models and\nnonparametric models.\n", "versions": [{"version": "v1", "created": "Fri, 21 Jan 2011 17:15:16 GMT"}], "update_date": "2011-01-24", "authors_parsed": [["Shi", "Xiaoping", ""], ["Wu", "Yuehua", ""], ["Jin", "Baisuo", ""]]}, {"id": "1101.4331", "submitter": "Robert Strawderman", "authors": "Karen Lostritto, Robert Strawderman, Annette Molinaro", "title": "A Partitioning Deletion/Substitution/Addition Algorithm for Creating\n  Survival Risk Groups", "comments": "A revision of this paper has been accepted for publication in\n  Biometrics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurately assessing a patient's risk of a given event is essential in making\ninformed treatment decisions. One approach is to stratify patients into two or\nmore distinct risk groups with respect to a specific outcome using both\nclinical and demographic variables. Outcomes may be categorical or continuous\nin nature; important examples in cancer studies might include level of toxicity\nor time to recurrence. Recursive partitioning methods are ideal for building\nsuch risk groups. Two such methods are Classification and Regression Trees\n(CART) and a more recent competitor known as the partitioning\nDeletion/Substitution/Addition (partDSA) algorithm, both which also utilize\nloss functions (e.g. squared error for a continuous outcome) as the basis for\nbuilding, selecting and assessing predictors but differ in the manner by which\nregression trees are constructed.\n  Recently, we have shown that partDSA often outperforms CART in so-called\n\"full data\" (e.g., uncensored) settings. However, when confronted with censored\noutcome data, the loss functions used by both procedures must be modified.\nThere have been several attempts to adapt CART for right-censored data. This\narticle describes two such extensions for \\emph{partDSA} that make use of\nobserved data (i.e. possibly censored) loss functions. These observed data loss\nfunctions, constructed using inverse probability of censoring weights, are\nconsistent estimates of their uncensored counterparts provided that the\ncorresponding censoring model is correctly specified. The relative performance\nof these new methods is evaluated via simulation studies and illustrated\nthrough an analysis of clinical trial data on brain cancer patients.\n", "versions": [{"version": "v1", "created": "Sat, 22 Jan 2011 23:02:00 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2012 22:44:17 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Lostritto", "Karen", ""], ["Strawderman", "Robert", ""], ["Molinaro", "Annette", ""]]}, {"id": "1101.4368", "submitter": "Gonzalo  Garc\\'ia-Donato", "authors": "Gonzalo Garcia-Donato and Miguel Angel Martinez-Beneito", "title": "Inferences in Bayesian variable selection problems with large model\n  spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aspect of Bayesian model selection is how to deal with huge\nmodel spaces, since exhaustive enumeration of all the models entertained is\nunfeasible and inferences have to be based on the very small proportion of\nmodels visited. This is the case for the variable selection problem, with a\nmoderate to large number of possible explanatory variables being considered in\nthis paper. We review some of the strategies proposed in the literature and\nargue that inferences based on empirical frequencies via Markov Chain Monte\nCarlo sampling of the posterior distribution outperforms recently proposed\nsearching methods. We give a plausible yet very simple explanation of this\neffect, showing that estimators based on frequencies are unbiased. The results\nobtained in two illustrative examples provide strong evidence in favor of our\narguments.\n", "versions": [{"version": "v1", "created": "Sun, 23 Jan 2011 13:32:31 GMT"}], "update_date": "2011-01-25", "authors_parsed": [["Garcia-Donato", "Gonzalo", ""], ["Martinez-Beneito", "Miguel Angel", ""]]}, {"id": "1101.4577", "submitter": "Meili Baragatti", "authors": "Meili Baragatti (IML)", "title": "Bayesian Variable Selection for Probit Mixed Models Applied to Gene\n  Selection", "comments": null, "journal-ref": "Bayesian Analysis 6, 2 (2011) 209-230", "doi": "10.1214/11-BA607", "report-no": null, "categories": "stat.ME q-bio.QM stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In computational biology, gene expression datasets are characterized by very\nfew individual samples compared to a large number of measurements per sample.\nThus, it is appealing to merge these datasets in order to increase the number\nof observations and diversify the data, allowing a more reliable selection of\ngenes relevant to the biological problem. Besides, the increased size of a\nmerged dataset facilitates its re-splitting into training and validation sets.\nThis necessitates the introduction of the dataset as a random effect. In this\ncontext, extending a work of Lee et al. (2003), a method is proposed to select\nrelevant variables among tens of thousands in a probit mixed regression model,\nconsidered as part of a larger hierarchical Bayesian model. Latent variables\nare used to identify subsets of selected variables and the grouping (or\nblocking) technique of Liu (1994) is combined with a Metropolis-within-Gibbs\nalgorithm (Robert and Casella 2004). The method is applied to a merged dataset\nmade of three individual gene expression datasets, in which tens of thousands\nof measurements are available for each of several hundred human breast cancer\nsamples. Even for this large dataset comprised of around 20000 predictors, the\nmethod is shown to be efficient and feasible. As an illustration, it is used to\nselect the most important genes that characterize the estrogen receptor status\nof patients with breast cancer.\n", "versions": [{"version": "v1", "created": "Mon, 24 Jan 2011 16:08:54 GMT"}, {"version": "v2", "created": "Wed, 23 Feb 2011 06:12:55 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Baragatti", "Meili", "", "IML"]]}, {"id": "1101.4743", "submitter": "Meili Baragatti", "authors": "Meili Baragatti (IML), Agn\\`es Grimaud (IML), Denys Pommeret (IML)", "title": "Parallel Tempering with Equi-Energy Moves", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Equi-Energy Sampler (EES) introduced by Kou et al [2006] is based on a\npopulation of chains which are updated by local moves and global moves, also\ncalled equi-energy jumps. The state space is partitioned into energy rings, and\nthe current state of a chain can jump to a past state of an adjacent chain that\nhas energy level close to its level. This algorithm has been developed to\nfacilitate global moves between different chains, resulting in a good\nexploration of the state space by the target chain. This method seems to be\nmore efficient than the classical Parallel Tempering (PT) algorithm. However it\nis difficult to use in combination with a Gibbs sampler and it necessitates\nincreased storage. In this paper we propose an adaptation of this EES that\ncombines PT with the principle of swapping between chains with same levels of\nenergy. This adaptation, that we shall call Parallel Tempering with Equi-Energy\nMoves (PTEEM), keeps the original idea of the EES method while ensuring good\ntheoretical properties, and practical implementation even if combined with a\nGibbs sampler. Performances of the PTEEM algorithm are compared with those of\nthe EES and of the standard PT algorithms in the context of mixture models, and\nin a problem of identification of gene regulatory binding motifs.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jan 2011 08:08:45 GMT"}, {"version": "v2", "created": "Wed, 30 Mar 2011 11:47:44 GMT"}, {"version": "v3", "created": "Sun, 19 Jun 2011 18:51:48 GMT"}, {"version": "v4", "created": "Fri, 2 Mar 2012 12:50:26 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Baragatti", "Meili", "", "IML"], ["Grimaud", "Agn\u00e8s", "", "IML"], ["Pommeret", "Denys", "", "IML"]]}, {"id": "1101.4744", "submitter": "Jairo Cugliari", "authors": "Anestis Antoniadis (UJF), Xavier Brossat, Jairo Cugliari (LM-Orsay),\n  Jean-Michel Poggi (LM-Orsay)", "title": "Clustering functional data using wavelets", "comments": null, "journal-ref": null, "doi": "10.1142/S0219691313500033", "report-no": "RR-7515", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present two methods for detecting patterns and clusters in high\ndimensional time-dependent functional data. Our methods are based on\nwavelet-based similarity measures, since wavelets are well suited for\nidentifying highly discriminant local time and scale features. The\nmultiresolution aspect of the wavelet transform provides a time-scale\ndecomposition of the signals allowing to visualize and to cluster the\nfunctional data into homogeneous groups. For each input function, through its\nempirical orthogonal wavelet transform the first method uses the distribution\nof energy across scales generate a handy number of features that can be\nsufficient to still make the signals well distinguishable. Our new similarity\nmeasure combined with an efficient feature selection technique in the wavelet\ndomain is then used within more or less classical clustering algorithms to\neffectively differentiate among high dimensional populations. The second method\nuses dissimilarity measures between the whole time-scale representations and\nare based on wavelet-coherence tools. The clustering is then performed using a\nk-centroid algorithm starting from these dissimilarities. Practical performance\nof these methods that jointly designs both the feature selection in the wavelet\ndomain and the classification distance is demonstrated through simulations as\nwell as daily profiles of the French electricity power demand.\n", "versions": [{"version": "v1", "created": "Tue, 25 Jan 2011 08:13:18 GMT"}, {"version": "v2", "created": "Tue, 31 May 2011 12:46:40 GMT"}], "update_date": "2013-02-15", "authors_parsed": [["Antoniadis", "Anestis", "", "UJF"], ["Brossat", "Xavier", "", "LM-Orsay"], ["Cugliari", "Jairo", "", "LM-Orsay"], ["Poggi", "Jean-Michel", "", "LM-Orsay"]]}, {"id": "1101.5184", "submitter": "Marco Scutari", "authors": "Marco Scutari and Adriana Brogini", "title": "Bayesian Network Structure Learning with Permutation Tests", "comments": "13 pages, 4 figures. Presented at the Conference 'Statistics for\n  Complex Problems', Padova, June 15, 2010", "journal-ref": "Communications in Statistics - Theory and Methods 2012, 42(16-17):\n  3233-3243", "doi": "10.1080/03610926.2011.593284", "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In literature there are several studies on the performance of Bayesian\nnetwork structure learning algorithms. The focus of these studies is almost\nalways the heuristics the learning algorithms are based on, i.e. the\nmaximisation algorithms (in score-based algorithms) or the techniques for\nlearning the dependencies of each variable (in constraint-based algorithms). In\nthis paper we investigate how the use of permutation tests instead of\nparametric ones affects the performance of Bayesian network structure learning\nfrom discrete data. Shrinkage tests are also covered to provide a broad\noverview of the techniques developed in current literature.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jan 2011 00:12:18 GMT"}, {"version": "v2", "created": "Sat, 26 Mar 2011 09:41:51 GMT"}, {"version": "v3", "created": "Sun, 26 Aug 2012 18:12:19 GMT"}], "update_date": "2012-08-28", "authors_parsed": [["Scutari", "Marco", ""], ["Brogini", "Adriana", ""]]}, {"id": "1101.5463", "submitter": "Maciej Kurant", "authors": "M. Kurant, M. Gjoka, C. T. Butts, A. Markopoulou", "title": "Walking on a Graph with a Magnifying Glass: Stratified Sampling via\n  Weighted Random Walks", "comments": "To appear in SIGMETRICS 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.NI physics.soc-ph stat.ME", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Our objective is to sample the node set of a large unknown graph via\ncrawling, to accurately estimate a given metric of interest. We design a random\nwalk on an appropriately defined weighted graph that achieves high efficiency\nby preferentially crawling those nodes and edges that convey greater\ninformation regarding the target metric. Our approach begins by employing the\ntheory of stratification to find optimal node weights, for a given estimation\nproblem, under an independence sampler. While optimal under independence\nsampling, these weights may be impractical under graph crawling due to\nconstraints arising from the structure of the graph. Therefore, the edge\nweights for our random walk should be chosen so as to lead to an equilibrium\ndistribution that strikes a balance between approximating the optimal weights\nunder an independence sampler and achieving fast convergence. We propose a\nheuristic approach (stratified weighted random walk, or S-WRW) that achieves\nthis goal, while using only limited information about the graph structure and\nthe node properties. We evaluate our technique in simulation, and\nexperimentally, by collecting a sample of Facebook college users. We show that\nS-WRW requires 13-15 times fewer samples than the simple re-weighted random\nwalk (RW) to achieve the same estimation accuracy for a range of metrics.\n", "versions": [{"version": "v1", "created": "Fri, 28 Jan 2011 06:27:45 GMT"}, {"version": "v2", "created": "Mon, 28 Mar 2011 00:50:21 GMT"}], "update_date": "2011-03-29", "authors_parsed": [["Kurant", "M.", ""], ["Gjoka", "M.", ""], ["Butts", "C. T.", ""], ["Markopoulou", "A.", ""]]}, {"id": "1101.5734", "submitter": "Yilun Chen", "authors": "Yilun Chen, Alfred O. Hero III", "title": "Recursive $\\ell_{1,\\infty}$ Group lasso", "comments": "8 pages, double column, 6 figures", "journal-ref": null, "doi": "10.1109/TSP.2012.2192924", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a recursive adaptive group lasso algorithm for real-time\npenalized least squares prediction that produces a time sequence of optimal\nsparse predictor coefficient vectors. At each time index the proposed algorithm\ncomputes an exact update of the optimal $\\ell_{1,\\infty}$-penalized recursive\nleast squares (RLS) predictor. Each update minimizes a convex but\nnondifferentiable function optimization problem. We develop an online homotopy\nmethod to reduce the computational complexity. Numerical simulations\ndemonstrate that the proposed algorithm outperforms the $\\ell_1$ regularized\nRLS algorithm for a group sparse system identification problem and has lower\nimplementation complexity than direct group lasso solvers.\n", "versions": [{"version": "v1", "created": "Sat, 29 Jan 2011 23:55:49 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Chen", "Yilun", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1101.6037", "submitter": "Christian Schafer", "authors": "Christian Sch\\\"afer (CREST, CEREMADE), Nicolas Chopin (CREST, ENSAE)", "title": "Sequential Monte Carlo on large binary sampling spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.CO stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Monte Carlo algorithm is said to be adaptive if it automatically calibrates\nits current proposal distribution using past simulations. The choice of the\nparametric family that defines the set of proposal distributions is critical\nfor good performance. In this paper, we present such a parametric family for\nadaptive sampling on high-dimensional binary spaces. A practical motivation for\nthis problem is variable selection in a linear regression context. We want to\nsample from a Bayesian posterior distribution on the model space using an\nappropriate version of Sequential Monte Carlo. Raw versions of Sequential Monte\nCarlo are easily implemented using binary vectors with independent components.\nFor high-dimensional problems, however, these simple proposals do not yield\nsatisfactory results. The key to an efficient adaptive algorithm are binary\nparametric families which take correlations into account, analogously to the\nmultivariate normal distribution on continuous spaces. We provide a review of\nmodels for binary data and make one of them work in the context of Sequential\nMonte Carlo sampling. Computational studies on real life data with about a\nhundred covariates suggest that, on difficult instances, our Sequential Monte\nCarlo approach clearly outperforms standard techniques based on Markov chain\nexploration.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jan 2011 17:47:32 GMT"}, {"version": "v2", "created": "Thu, 3 Feb 2011 16:47:02 GMT"}, {"version": "v3", "created": "Wed, 2 Nov 2011 17:23:41 GMT"}, {"version": "v4", "created": "Wed, 9 Nov 2011 09:04:40 GMT"}], "update_date": "2011-11-11", "authors_parsed": [["Sch\u00e4fer", "Christian", "", "CREST, CEREMADE"], ["Chopin", "Nicolas", "", "CREST, ENSAE"]]}]