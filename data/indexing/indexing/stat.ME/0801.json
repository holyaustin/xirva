[{"id": "0801.0327", "submitter": "Kevin Bleakley", "authors": "G\\'erard Biau, Kevin Bleakley, L\\'aszl\\'o Gy\\\"orfi and Gy\\\"orgy\n  Ottucs\\'ak", "title": "Nonparametric sequential prediction of time series", "comments": "article + 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR", "license": null, "abstract": "  Time series prediction covers a vast field of every-day statistical\napplications in medical, environmental and economic domains. In this paper we\ndevelop nonparametric prediction strategies based on the combination of a set\nof 'experts' and show the universal consistency of these strategies under a\nminimum of conditions. We perform an in-depth analysis of real-world data sets\nand show that these nonparametric strategies are more flexible, faster and\ngenerally outperform ARMA methods in terms of normalized cumulative prediction\nerror.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jan 2008 02:00:15 GMT"}], "update_date": "2008-01-03", "authors_parsed": [["Biau", "G\u00e9rard", ""], ["Bleakley", "Kevin", ""], ["Gy\u00f6rfi", "L\u00e1szl\u00f3", ""], ["Ottucs\u00e1k", "Gy\u00f6rgy", ""]]}, {"id": "0801.0461", "submitter": "Shane Jensen", "authors": "Hanna M. Wallach, Shane T. Jensen, Lee Dicker and Katherine A. Heller", "title": "An Alternative Prior Process for Nonparametric Bayesian Clustering", "comments": null, "journal-ref": "Proceedings of the Thirteenth International Conference on\n  Artificial Intelligence and Statistics (AISTATS) 2010, JMLR W & CP 9, pp.\n  892-899", "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prior distributions play a crucial role in Bayesian approaches to clustering.\nTwo commonly-used prior distributions are the Dirichlet and Pitman-Yor\nprocesses. In this paper, we investigate the predictive probabilities that\nunderlie these processes, and the implicit \"rich-get-richer\" characteristic of\nthe resulting partitions. We explore an alternative prior for nonparametric\nBayesian clustering -- the uniform process -- for applications where the\n\"rich-get-richer\" property is undesirable. We also explore the cost of this\nprocess: partitions are no longer exchangeable with respect to the ordering of\nvariables. We present new asymptotic and simulation-based results for the\nclustering characteristics of the uniform process and compare these with known\nresults for the Dirichlet and Pitman-Yor processes. We compare performance on a\nreal document clustering task, demonstrating the practical advantage of the\nuniform process despite its lack of exchangeability over orderings.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2008 01:10:20 GMT"}, {"version": "v2", "created": "Fri, 15 Oct 2010 16:17:32 GMT"}], "update_date": "2010-10-18", "authors_parsed": [["Wallach", "Hanna M.", ""], ["Jensen", "Shane T.", ""], ["Dicker", "Lee", ""], ["Heller", "Katherine A.", ""]]}, {"id": "0801.0499", "submitter": "Daniel Yekutieli Dr.", "authors": "Daniel Yekutieli", "title": "Adjusted Bayesian inference for selected parameters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of providing inference from a Bayesian perspective for\nparameters selected after viewing the data. We present a Bayesian framework for\nproviding inference for selected parameters, based on the observation that\nproviding Bayesian inference for selected parameters is a truncated data\nproblem. We show that if the prior for the parameter is non-informative, or if\nthe parameter is a \"fixed\" unknown constant, then it is necessary to adjust the\nBayesian inference for selection. Our second contribution is the introduction\nof Bayesian False Discovery Rate controlling methodology,which generalizes\nexisting Bayesian FDR methods that are only defined in the two-group mixture\nmodel.We illustrate our results by applying them to simulated data and data\nfroma microarray experiment.\n", "versions": [{"version": "v1", "created": "Thu, 3 Jan 2008 10:29:09 GMT"}, {"version": "v2", "created": "Mon, 19 Jan 2009 14:12:17 GMT"}, {"version": "v3", "created": "Mon, 26 Jan 2009 08:57:26 GMT"}, {"version": "v4", "created": "Mon, 3 Aug 2009 11:04:47 GMT"}, {"version": "v5", "created": "Sun, 20 Jun 2010 13:21:43 GMT"}, {"version": "v6", "created": "Sun, 27 Mar 2011 17:51:16 GMT"}, {"version": "v7", "created": "Thu, 15 Sep 2011 03:51:07 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Yekutieli", "Daniel", ""]]}, {"id": "0801.0848", "submitter": "Nathalie Villa", "authors": "Romain Boulet (IMT), Bertrand Jouve (IMT), Fabrice Rossi (INRIA\n  Rocquencourt / INRIA Sophia Antipolis), Nathalie Villa (IMT)", "title": "Batch kernel SOM and related Laplacian methods for social network\n  analysis", "comments": null, "journal-ref": "Neurocomputing / EEG Neurocomputing (2008) A para\\^itre", "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.ML stat.TH", "license": null, "abstract": "  Large graphs are natural mathematical models for describing the structure of\nthe data in a wide variety of fields, such as web mining, social networks,\ninformation retrieval, biological networks, etc. For all these applications,\nautomatic tools are required to get a synthetic view of the graph and to reach\na good understanding of the underlying problem. In particular, discovering\ngroups of tightly connected vertices and understanding the relations between\nthose groups is very important in practice. This paper shows how a kernel\nversion of the batch Self Organizing Map can be used to achieve these goals via\nkernels derived from the Laplacian matrix of the graph, especially when it is\nused in conjunction with more classical methods based on the spectral analysis\nof the graph. The proposed method is used to explore the structure of a\nmedieval social network modeled through a weighted graph that has been directly\nbuilt from a large corpus of agrarian contracts.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jan 2008 08:28:15 GMT"}], "update_date": "2008-01-08", "authors_parsed": [["Boulet", "Romain", "", "IMT"], ["Jouve", "Bertrand", "", "IMT"], ["Rossi", "Fabrice", "", "INRIA\n  Rocquencourt / INRIA Sophia Antipolis"], ["Villa", "Nathalie", "", "IMT"]]}, {"id": "0801.0922", "submitter": "A. M. Abd Elfattah", "authors": "A. M. Abd Elfattah, O. Mohamed Marwa", "title": "Estimating of $P(Y<X)$ in the Exponential case Based on Censored Samples", "comments": "Submitted to the Electronic Journal of Statistics\n  (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics\n  (http://www.imstat.org)", "journal-ref": null, "doi": null, "report-no": "IMS-EJS-EJS_2008_166", "categories": "stat.ME", "license": null, "abstract": "  In this article, the estimation of reliability of a system is discussed\n$p(y<x)$ when strength, $X$, and stress, $Y$, are two independent exponential\ndistribution with different scale parameters when the available data are type\nII Censored sample. Different methods for estimating the reliability are\napplied. The point estimators obtained are maximum likelihood estimator,\nuniformly minimum variance unbiased estimator, and Bayesian estimators based on\nconjugate and non informative prior distributions. A comparison of the\nestimates obtained is performed. Interval estimators of the reliability are\nalso discussed.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jan 2008 08:30:01 GMT"}], "update_date": "2008-01-08", "authors_parsed": [["Elfattah", "A. M. Abd", ""], ["Marwa", "O. Mohamed", ""]]}, {"id": "0801.1265", "submitter": "Gert De Cooman", "authors": "Gert de Cooman, Erik Quaeghebeur, Enrique Miranda", "title": "Exchangeable lower previsions", "comments": "1 figure. 26 pages. Submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR math.ST stat.ME stat.TH", "license": null, "abstract": "  We extend de Finetti's (1937) notion of exchangeability to finite and\ncountable sequences of variables, when a subject's beliefs about them are\nmodelled using coherent lower previsions rather than (linear) previsions. We\nprove representation theorems in both the finite and the countable case, in\nterms of sampling without and with replacement, respectively. We also establish\na convergence result for sample means of exchangeable sequences. Finally, we\nstudy and solve the problem of exchangeable natural extension: how to find the\nmost conservative (point-wise smallest) coherent and exchangeable lower\nprevision that dominates a given lower prevision.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jan 2008 16:13:29 GMT"}], "update_date": "2008-01-09", "authors_parsed": [["de Cooman", "Gert", ""], ["Quaeghebeur", "Erik", ""], ["Miranda", "Enrique", ""]]}, {"id": "0801.1599", "submitter": "Zhibiao Zhao", "authors": "Zhibiao Zhao", "title": "Parametric and nonparametric models and methods in financial\n  econometrics", "comments": "Published in at http://dx.doi.org/10.1214/08-SS034 the Statistics\n  Surveys (http://www.i-journals.org/ss/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistics Surveys 2008, Vol. 2, 1-42", "doi": "10.1214/08-SS034", "report-no": "IMS-SS-SS_2008_34", "categories": "q-fin.ST stat.ME", "license": null, "abstract": "  Financial econometrics has become an increasingly popular research field. In\nthis paper we review a few parametric and nonparametric models and methods used\nin this area. After introducing several widely used continuous-time and\ndiscrete-time models, we study in detail dependence structures of discrete\nsamples, including Markovian property, hidden Markovian structure, contaminated\nobservations, and random samples. We then discuss several popular parametric\nand nonparametric estimation methods. To avoid model mis-specification, model\nvalidation plays a key role in financial modeling. We discuss several model\nvalidation techniques, including pseudo-likelihood ratio test, nonparametric\ncurve regression based test, residuals based test, generalized likelihood ratio\ntest, simultaneous confidence band construction, and density based test.\nFinally, we briefly touch on tools for studying large sample properties.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jan 2008 13:49:09 GMT"}, {"version": "v2", "created": "Thu, 20 Mar 2008 09:24:29 GMT"}], "update_date": "2008-12-02", "authors_parsed": [["Zhao", "Zhibiao", ""]]}, {"id": "0801.1758", "submitter": "Piero Barone", "authors": "Piero Barone", "title": "A new transform for solving the noisy complex exponentials approximation\n  problem", "comments": "42 pages, 5 figures", "journal-ref": "Journal of Approximation Theory, vol.155, pp. 1-27, 2008", "doi": "10.1016/j.jat.2008.04.007", "report-no": null, "categories": "math.ST math.NA stat.ME stat.TH", "license": null, "abstract": "  The problem of estimating a complex measure made up by a linear combination\nof Dirac distributions centered on points of the complex plane from a finite\nnumber of its complex moments affected by additive i.i.d. Gaussian noise is\nconsidered. A random measure is defined whose expectation approximates the\nunknown measure under suitable conditions. An estimator of the approximating\nmeasure is then proposed as well as a new discrete transform of the noisy\nmoments that allows to compute an estimate of the unknown measure. A small\nsimulation study is also performed to experimentally check the goodness of the\napproximations.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2008 11:30:34 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Barone", "Piero", ""]]}, {"id": "0801.1864", "submitter": "Robert Kohn", "authors": "P. Giordani and R. Kohn", "title": "Adaptive Independent Metropolis-Hastings by Fast Estimation of Mixtures\n  of Normals", "comments": "35 pages and 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.AP stat.ME", "license": null, "abstract": "  We construct an adaptive independent Metropolis-Hastings sampler that uses a\nmixture of normals as a proposal distribution. To take full advantage of the\npotential of adaptive sampling our algorithm updates the mixture of normals\nfrequently, starting early in the chain. The algorithm is built for speed and\nreliability and its sampling performance is evaluated with real and simulated\nexamples. Our article outlines conditions for adaptive sampling to hold and\ngives a readily accessible proof that under these conditions the sampling\nscheme generates iterates that converge to the target distribution.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jan 2008 22:53:12 GMT"}], "update_date": "2008-01-15", "authors_parsed": [["Giordani", "P.", ""], ["Kohn", "R.", ""]]}, {"id": "0801.2555", "submitter": "Ping Ma", "authors": "Ping Ma and Wenxuan Zhong", "title": "Penalized Clustering of Large Scale Functional Data with Multiple\n  Covariates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": null, "abstract": "  In this article, we propose a penalized clustering method for large scale\ndata with multiple covariates through a functional data approach. In the\nproposed method, responses and covariates are linked together through\nnonparametric multivariate functions (fixed effects), which have great\nflexibility in modeling a variety of function features, such as jump points,\nbranching, and periodicity. Functional ANOVA is employed to further decompose\nmultivariate functions in a reproducing kernel Hilbert space and provide\nassociated notions of main effect and interaction. Parsimonious random effects\nare used to capture various correlation structures. The mixed-effect models are\nnested under a general mixture model, in which the heterogeneity of functional\ndata is characterized. We propose a penalized Henderson's likelihood approach\nfor model-fitting and design a rejection-controlled EM algorithm for the\nestimation. Our method selects smoothing parameters through generalized\ncross-validation. Furthermore, the Bayesian confidence intervals are used to\nmeasure the clustering uncertainty. Simulation studies and real-data examples\nare presented to investigate the empirical performance of the proposed method.\nOpen-source code is available in the R package MFDA.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jan 2008 19:34:50 GMT"}], "update_date": "2008-01-17", "authors_parsed": [["Ma", "Ping", ""], ["Zhong", "Wenxuan", ""]]}, {"id": "0801.2600", "submitter": "Shota Gugushvili", "authors": "Bert van Es and Shota Gugushvili", "title": "Some thoughts on the asymptotics of the deconvolution kernel density\n  estimator", "comments": "18 pages, 8 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": null, "abstract": "  Via a simulation study we compare the finite sample performance of the\ndeconvolution kernel density estimator in the supersmooth deconvolution problem\nto its asymptotic behaviour predicted by two asymptotic normality theorems. Our\nresults indicate that for lower noise levels and moderate sample sizes the\nmatch between the asymptotic theory and the finite sample performance of the\nestimator is not satisfactory. On the other hand we show that the two\napproaches produce reasonably close results for higher noise levels. These\nobservations in turn provide additional motivation for the study of\ndeconvolution problems under the assumption that the error term variance\n$\\sigma^2\\to 0$ as the sample size $n\\to\\infty.$\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2008 02:12:17 GMT"}], "update_date": "2008-01-18", "authors_parsed": [["van Es", "Bert", ""], ["Gugushvili", "Shota", ""]]}, {"id": "0801.2748", "submitter": "Mark Kliger", "authors": "Ami Wiesel, Mark Kliger and Alfred O. Hero III", "title": "A greedy approach to sparse canonical correlation analysis", "comments": "5 pages, 3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": null, "abstract": "  We consider the problem of sparse canonical correlation analysis (CCA), i.e.,\nthe search for two linear combinations, one for each multivariate, that yield\nmaximum correlation using a specified number of variables. We propose an\nefficient numerical approximation based on a direct greedy approach which\nbounds the correlation at each stage. The method is specifically designed to\ncope with large data sets and its computational complexity depends only on the\nsparsity levels. We analyze the algorithm's performance through the tradeoff\nbetween correlation and parsimony. The results of numerical simulation suggest\nthat a significant portion of the correlation may be captured using a\nrelatively small number of variables. In addition, we examine the use of sparse\nCCA as a regularization method when the number of available samples is small\ncompared to the dimensions of the multivariates.\n", "versions": [{"version": "v1", "created": "Thu, 17 Jan 2008 18:53:07 GMT"}], "update_date": "2008-01-18", "authors_parsed": [["Wiesel", "Ami", ""], ["Kliger", "Mark", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "0801.2790", "submitter": "Mark Kliger", "authors": "Mark Kliger and Joseph M. Francos", "title": "Strongly Consistent Model Order Selection for Estimating 2-D Sinusoids\n  in Colored Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": null, "abstract": "  We consider the problem of jointly estimating the number as well as the\nparameters of two-dimensional sinusoidal signals, observed in the presence of\nan additive colored noise field. We begin by elaborating on the least squares\nestimation of 2-D sinusoidal signals, when the assumed number of sinusoids is\nincorrect. In the case where the number of sinusoidal signals is\nunder-estimated we show the almost sure convergence of the least squares\nestimates to the parameters of the dominant sinusoids. In the case where this\nnumber is over-estimated, the estimated parameter vector obtained by the least\nsquares estimator contains a sub-vector that converges almost surely to the\ncorrect parameters of the sinusoids. Based on these results, we prove the\nstrong consistency of a new model order selection rule.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jan 2008 06:56:25 GMT"}], "update_date": "2008-01-21", "authors_parsed": [["Kliger", "Mark", ""], ["Francos", "Joseph M.", ""]]}, {"id": "0801.3442", "submitter": "Qingzhao Yu", "authors": "Qingzhao Yu, Elizabeth A. Stasny, Bin Li", "title": "Bayesian models to adjust for response bias in survey data for\n  estimating rape and domestic violence rates from the NCVS", "comments": "Published in at http://dx.doi.org/10.1214/08-AOAS160 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2008, Vol. 2, No. 2, 665-686", "doi": "10.1214/08-AOAS160", "report-no": "IMS-AOAS-AOAS160", "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is difficult to accurately estimate the rates of rape and domestic\nviolence due to the sensitive nature of these crimes. There is evidence that\nbias in estimating the crime rates from survey data may arise because some\nwomen respondents are \"gagged\" in reporting some types of crimes by the use of\na telephone rather than a personal interview, and by the presence of a spouse\nduring the interview. On the other hand, as data on these crimes are collected\nevery year, it would be more efficient in data analysis if we could identify\nand make use of information from previous data. In this paper we propose a\nmodel to adjust the estimates of the rates of rape and domestic violence to\naccount for the response bias due to the \"gag\" factors. To estimate parameters\nin the model, we identify the information that is not sensitive to time and\nincorporate this into prior distributions. The strength of Bayesian estimators\nis their ability to combine information from long observational records in a\nsensible way. Within a Bayesian framework, we develop an\nExpectation-Maximization-Bayesian (EMB) algorithm for computation in analyzing\ncontingency table and we apply the jackknife to estimate the accuracy of the\nestimates. Our approach is illustrated using the yearly crime data from the\nNational Crime Victimization Survey. The illustration shows that compared with\nthe classical method, our model leads to more efficient estimation but does not\nrequire more complicated computation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Jan 2008 18:51:56 GMT"}, {"version": "v2", "created": "Mon, 28 Jul 2008 15:15:54 GMT"}], "update_date": "2008-07-28", "authors_parsed": [["Yu", "Qingzhao", ""], ["Stasny", "Elizabeth A.", ""], ["Li", "Bin", ""]]}, {"id": "0801.3751", "submitter": "D. A. S. Fraser", "authors": "M. B\\'edard, D. A. S. Fraser, A. Wong", "title": "Higher Accuracy for Bayesian and Frequentist Inference: Large Sample\n  Theory for Small Sample Likelihood", "comments": "Published in at http://dx.doi.org/10.1214/07-STS240 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 3, 301-321", "doi": "10.1214/07-STS240", "report-no": "IMS-STS-STS240", "categories": "stat.ME", "license": null, "abstract": "  Recent likelihood theory produces $p$-values that have remarkable accuracy\nand wide applicability. The calculations use familiar tools such as maximum\nlikelihood values (MLEs), observed information and parameter rescaling. The\nusual evaluation of such $p$-values is by simulations, and such simulations do\nverify that the global distribution of the $p$-values is uniform(0, 1), to high\naccuracy in repeated sampling. The derivation of the $p$-values, however,\nasserts a stronger statement, that they have a uniform(0, 1) distribution\nconditionally, given identified precision information provided by the data. We\ntake a simple regression example that involves exact precision information and\nuse large sample techniques to extract highly accurate information as to the\nstatistical position of the data point with respect to the parameter:\nspecifically, we examine various $p$-values and Bayesian posterior survivor\n$s$-values for validity. With observed data we numerically evaluate the various\n$p$-values and $s$-values, and we also record the related general formulas. We\nthen assess the numerical values for accuracy using Markov chain Monte Carlo\n(McMC) methods. We also propose some third-order likelihood-based procedures\nfor obtaining means and variances of Bayesian posterior distributions, again\nfollowed by McMC assessment. Finally we propose some adaptive McMC methods to\nimprove the simulation acceptance rates. All these methods are based on\nasymptotic analysis that derives from the effect of additional data. And the\nmethods use simple calculations based on familiar maximizing values and related\ninformations. The example illustrates the general formulas and the ease of\ncalculations, while the McMC assessments demonstrate the numerical validity of\nthe $p$-values as percentage position of a data point. The example, however, is\nvery simple and transparent, and thus gives little indication that in a wide\ngenerality of models the formulas do accurately separate information for almost\nany parameter of interest, and then do give accurate $p$-value determinations\nfrom that information. As illustration an enigmatic problem in the literature\nis discussed and simulations are recorded; various examples in the literature\nare cited.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jan 2008 13:19:50 GMT"}], "update_date": "2008-02-08", "authors_parsed": [["B\u00e9dard", "M.", ""], ["Fraser", "D. A. S.", ""], ["Wong", "A.", ""]]}, {"id": "0801.3902", "submitter": "Edward I. George", "authors": "Edward I. George", "title": "A Tribute to Ingram Olkin", "comments": "Published in at http://dx.doi.org/10.1214/07-STS250 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 3, 400-400", "doi": "10.1214/07-STS250", "report-no": "IMS-STS-STS250", "categories": "stat.ME", "license": null, "abstract": "  It is with pleasure and pride that I introduce this special section in honor\nof Ingram Olkin. This tribute is especially fitting because, among the many\nprofound and far-reaching contributions that he has made to our profession,\nIngram Olkin was the key force behind the genesis of Statistical Science. As\nput so eloquently by Morrie DeGroot [1], the founding Executive Editor of\nStatistical Science.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jan 2008 09:54:25 GMT"}], "update_date": "2008-02-08", "authors_parsed": [["George", "Edward I.", ""]]}, {"id": "0801.4065", "submitter": "Brandon Whitcher", "authors": "Volker J. Schmid, Brandon Whitcher, Anwar R. Padhani, Guang-Zhong Yang", "title": "A Semi-parametric Technique for the Quantitative Analysis of Dynamic\n  Contrast-enhanced MR Images Based on Bayesian P-splines", "comments": null, "journal-ref": "IEEE Transactions on Medical Imaging ( Volume: 28 , Issue: 6 ,\n  June 2009 ). Page(s): 789 - 798", "doi": "10.1109/TMI.2008.2007326", "report-no": null, "categories": "stat.AP physics.med-ph stat.ME", "license": null, "abstract": "  Dynamic Contrast-enhanced Magnetic Resonance Imaging (DCE-MRI) is an\nimportant tool for detecting subtle kinetic changes in cancerous tissue.\nQuantitative analysis of DCE-MRI typically involves the convolution of an\narterial input function (AIF) with a nonlinear pharmacokinetic model of the\ncontrast agent concentration. Parameters of the kinetic model are biologically\nmeaningful, but the optimization of the non-linear model has significant\ncomputational issues. In practice, convergence of the optimization algorithm is\nnot guaranteed and the accuracy of the model fitting may be compromised. To\novercome this problems, this paper proposes a semi-parametric penalized spline\nsmoothing approach, with which the AIF is convolved with a set of B-splines to\nproduce a design matrix using locally adaptive smoothing parameters based on\nBayesian penalized spline models (P-splines). It has been shown that kinetic\nparameter estimation can be obtained from the resulting deconvolved response\nfunction, which also includes the onset of contrast enhancement. Detailed\nvalidation of the method, both with simulated and in vivo data, is provided.\n", "versions": [{"version": "v1", "created": "Sat, 26 Jan 2008 08:59:24 GMT"}], "update_date": "2020-04-22", "authors_parsed": [["Schmid", "Volker J.", ""], ["Whitcher", "Brandon", ""], ["Padhani", "Anwar R.", ""], ["Yang", "Guang-Zhong", ""]]}, {"id": "0801.4207", "submitter": "Betsy Jane Becker", "authors": "Betsy Jane Becker", "title": "Multivariate Meta-Analysis: Contributions of Ingram Olkin", "comments": "Published in at http://dx.doi.org/10.1214/07-STS239 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 3, 401-406", "doi": "10.1214/07-STS239", "report-no": "IMS-STS-STS239", "categories": "stat.ME", "license": null, "abstract": "  The research on meta-analysis and particularly multivariate meta-analysis has\nbeen greatly influenced by the work of Ingram Olkin. This paper documents\nOlkin's contributions by way of citation counts and outlines several areas of\ncontribution by Olkin and his academic descendants. An academic family tree is\nprovided.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2008 08:31:14 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Becker", "Betsy Jane", ""]]}, {"id": "0801.4221", "submitter": "Barry C. Arnold", "authors": "Barry C. Arnold", "title": "Majorization: Here, There and Everywhere", "comments": "Published in at http://dx.doi.org/10.1214/0883423060000000097 the\n  Statistical Science (http://www.imstat.org/sts/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 3, 407-413", "doi": "10.1214/0883423060000000097", "report-no": "IMS-STS-STS158", "categories": "stat.ME", "license": null, "abstract": "  The appearance of Marshall and Olkin's 1979 book on inequalities with special\nemphasis on majorization generated a surge of interest in potential\napplications of majorization and Schur convexity in a broad spectrum of fields.\nAfter 25 years this continues to be the case. The present article presents a\nsampling of the diverse areas in which majorization has been found to be useful\nin the past 25 years.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2008 09:38:23 GMT"}], "update_date": "2008-02-08", "authors_parsed": [["Arnold", "Barry C.", ""]]}, {"id": "0801.4224", "submitter": "Gonzalo  Garc\\'ia-Donato", "authors": "M.J. Bayarri and G. Garc\\'ia-Donato", "title": "Generalization of Jeffreys' divergence based priors for Bayesian\n  hypothesis testing", "comments": null, "journal-ref": "Journal of the Royal Statistical Society, Series B, (2008), vol.\n  70, pp. 981--1003", "doi": null, "report-no": null, "categories": "stat.ME", "license": null, "abstract": "  In this paper we introduce objective proper prior distributions for\nhypothesis testing and model selection based on measures of divergence between\nthe competing models; we call them divergence based (DB) priors. DB priors have\nsimple forms and desirable properties, like information (finite sample)\nconsistency; often, they are similar to other existing proposals like the\nintrinsic priors; moreover, in normal linear models scenarios, they exactly\nreproduce Jeffreys-Zellner-Siow priors. Most importantly, in challenging\nscenarios such as irregular models and mixture models, the DB priors are well\ndefined and very reasonable, while alternative proposals are not. We derive\napproximations to the DB priors as well as MCMC and asymptotic expressions for\nthe associated Bayes factors.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2008 09:55:00 GMT"}], "update_date": "2009-02-27", "authors_parsed": [["Bayarri", "M. J.", ""], ["Garc\u00eda-Donato", "G.", ""]]}, {"id": "0801.4263", "submitter": "Michael Friendly", "authors": "Michael Friendly", "title": "A.-M. Guerry's Moral Statistics of France: Challenges for Multivariable\n  Spatial Analysis", "comments": "Published in at http://dx.doi.org/10.1214/07-STS241 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 3, 368-399", "doi": "10.1214/07-STS241", "report-no": "IMS-STS-STS241", "categories": "stat.ME", "license": null, "abstract": "  Andr\\'{e}-Michel Guerry's (1833) Essai sur la Statistique Morale de la France\nwas one of the foundation studies of modern social science. Guerry assembled\ndata on crimes, suicides, literacy and other ``moral statistics,'' and used\ntables and maps to analyze a variety of social issues in perhaps the first\ncomprehensive study relating such variables. Indeed, the Essai may be\nconsidered the book that launched modern empirical social science, for the\nquestions raised and the methods Guerry developed to try to answer them.\nGuerry's data consist of a large number of variables recorded for each of the\nd\\'{e}partments of France in the 1820--1830s and therefore involve both\nmultivariate and geographical aspects. In addition to historical interest,\nthese data provide the opportunity to ask how modern methods of statistics,\ngraphics, thematic cartography and geovisualization can shed further light on\nthe questions he raised. We present a variety of methods attempting to address\nGuerry's challenge for multivariate spatial statistics.\n", "versions": [{"version": "v1", "created": "Mon, 28 Jan 2008 14:29:04 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Friendly", "Michael", ""]]}, {"id": "0801.4410", "submitter": "Yuzo Maruyama", "authors": "Yuzo Maruyama, Edward I. George", "title": "Fully Bayes factors with a generalized g-prior", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS917 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 5, 2740-2765", "doi": "10.1214/11-AOS917", "report-no": "IMS-AOS-AOS917", "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the normal linear model variable selection problem, we propose selection\ncriteria based on a fully Bayes formulation with a generalization of Zellner's\n$g$-prior which allows for $p>n$. A special case of the prior formulation is\nseen to yield tractable closed forms for marginal densities and Bayes factors\nwhich reveal new model evaluation characteristics of potential interest.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2008 01:20:08 GMT"}, {"version": "v2", "created": "Mon, 20 Sep 2010 01:57:16 GMT"}, {"version": "v3", "created": "Fri, 5 Aug 2011 05:20:01 GMT"}, {"version": "v4", "created": "Thu, 23 Feb 2012 08:38:37 GMT"}], "update_date": "2012-02-24", "authors_parsed": [["Maruyama", "Yuzo", ""], ["George", "Edward I.", ""]]}, {"id": "0801.4442", "submitter": "Betsy Jane Becker", "authors": "Betsy Jane Becker, Meng-Jia Wu", "title": "The Synthesis of Regression Slopes in Meta-Analysis", "comments": "Published in at http://dx.doi.org/10.1214/07-STS243 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2007, Vol. 22, No. 3, 414-429", "doi": "10.1214/07-STS243", "report-no": "IMS-STS-STS243", "categories": "stat.ME", "license": null, "abstract": "  Research on methods of meta-analysis (the synthesis of related study results)\nhas dealt with many simple study indices, but less attention has been paid to\nthe issue of summarizing regression slopes. In part this is because of the many\ncomplications that arise when real sets of regression models are accumulated.\nWe outline the complexities involved in synthesizing slopes, describe existing\nmethods of analysis and present a multivariate generalized least squares\napproach to the synthesis of regression slopes.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jan 2008 07:58:20 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Becker", "Betsy Jane", ""], ["Wu", "Meng-Jia", ""]]}, {"id": "0801.4627", "submitter": "Ulrike Schneider", "authors": "Benedikt M. P\\\"otscher and Ulrike Schneider", "title": "On the Distribution of the Adaptive LASSO Estimator", "comments": "revised version; minor changes and some material added", "journal-ref": "J. Stat. Plann. Inference 139 (2009) 2775-2790", "doi": "10.1016/j.jspi.2009.01.003", "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the distribution of the adaptive LASSO estimator (Zou (2006)) in\nfinite samples as well as in the large-sample limit. The large-sample\ndistributions are derived both for the case where the adaptive LASSO estimator\nis tuned to perform conservative model selection as well as for the case where\nthe tuning results in consistent model selection. We show that the\nfinite-sample as well as the large-sample distributions are typically highly\nnon-normal, regardless of the choice of the tuning parameter. The uniform\nconvergence rate is also obtained, and is shown to be slower than $n^{-1/2}$ in\ncase the estimator is tuned to perform consistent model selection. In\nparticular, these results question the statistical relevance of the `oracle'\nproperty of the adaptive LASSO estimator established in Zou (2006). Moreover,\nwe also provide an impossibility result regarding the estimation of the\ndistribution function of the adaptive LASSO estimator.The theoretical results,\nwhich are obtained for a regression model with orthogonal design, are\ncomplemented by a Monte Carlo study using non-orthogonal regressors.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2008 09:57:44 GMT"}, {"version": "v2", "created": "Tue, 16 Dec 2008 12:42:12 GMT"}], "update_date": "2009-04-28", "authors_parsed": [["P\u00f6tscher", "Benedikt M.", ""], ["Schneider", "Ulrike", ""]]}, {"id": "0801.4629", "submitter": "Eric Matzner-Lober", "authors": "Pierre Andre Cornillon, Nicolas Hengartner, Eric Matzner-Lober", "title": "Recursive Bias Estimation and $L_2$ Boosting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": null, "abstract": "  This paper presents a general iterative bias correction procedure for\nregression smoothers. This bias reduction schema is shown to correspond\noperationally to the $L_2$ Boosting algorithm and provides a new statistical\ninterpretation for $L_2$ Boosting. We analyze the behavior of the Boosting\nalgorithm applied to common smoothers $S$ which we show depend on the spectrum\nof $I-S$. We present examples of common smoother for which Boosting generates a\ndivergent sequence. The statistical interpretation suggest combining algorithm\nwith an appropriate stopping rule for the iterative procedure. Finally we\nillustrate the practical finite sample performances of the iterative smoother\nvia a simulation study. simulations.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jan 2008 10:22:22 GMT"}], "update_date": "2008-01-31", "authors_parsed": [["Cornillon", "Pierre Andre", ""], ["Hengartner", "Nicolas", ""], ["Matzner-Lober", "Eric", ""]]}]