[{"id": "1311.0072", "submitter": "Arash Ali Amini", "authors": "Arash A. Amini and XuanLong Nguyen", "title": "Bayesian inference as iterated random functions with applications to\n  sequential inference in graphical models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general formalism of iterated random functions with semigroup\nproperty, under which exact and approximate Bayesian posterior updates can be\nviewed as specific instances. A convergence theory for iterated random\nfunctions is presented. As an application of the general theory we analyze\nconvergence behaviors of exact and approximate message-passing algorithms that\narise in a sequential change point detection problem formulated via a latent\nvariable directed graphical model. The sequential inference algorithm and its\nsupporting theory are illustrated by simulated examples.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 02:17:06 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Amini", "Arash A.", ""], ["Nguyen", "XuanLong", ""]]}, {"id": "1311.0081", "submitter": "Michael J Lew", "authors": "Michael J. Lew", "title": "To P or not to P: on the evidential nature of P-values and their place\n  in scientific inference", "comments": "31 pages, 9 figures and R code", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The customary use of P-values in scientific research has been attacked as\nbeing ill-conceived, and the utility of P-values has been derided. This paper\nreviews common misconceptions about P-values and their alleged deficits as\nindices of experimental evidence and, using an empirical exploration of the\nproperties of P-values, documents the intimate relationship between P-values\nand likelihood functions. It is shown that P-values quantify experimental\nevidence not by their numerical value, but through the likelihood functions\nthat they index. Many arguments against the utility of P-values are refuted and\nthe conclusion is drawn that P-values are useful indices of experimental\nevidence. The widespread use of P-values in scientific research is well\njustified by the actual properties of P-values, but those properties need to be\nmore widely understood.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 03:27:46 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Lew", "Michael J.", ""]]}, {"id": "1311.0085", "submitter": "Shizhe Chen", "authors": "Shizhe Chen, Daniela Witten, and Ali Shojaie", "title": "Selection and Estimation for Mixed Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the parameters in a pairwise graphical\nmodel in which the distribution of each node, conditioned on the others, may\nhave a different parametric form. In particular, we assume that each node's\nconditional distribution is in the exponential family. We identify restrictions\non the parameter space required for the existence of a well-defined joint\ndensity, and establish the consistency of the neighbourhood selection approach\nfor graph reconstruction in high dimensions when the true underlying graph is\nsparse. Motivated by our theoretical results, we investigate the selection of\nedges between nodes whose conditional distributions take different parametric\nforms, and show that efficiency can be gained if edge estimates obtained from\nthe regressions of particular nodes are used to reconstruct the graph. These\nresults are illustrated with examples of Gaussian, Bernoulli, Poisson and\nexponential distributions. Our theoretical findings are corroborated by\nevidence from simulation studies.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 04:32:24 GMT"}, {"version": "v2", "created": "Sun, 30 Mar 2014 05:02:50 GMT"}, {"version": "v3", "created": "Thu, 26 Jun 2014 04:25:06 GMT"}, {"version": "v4", "created": "Fri, 1 Aug 2014 21:12:38 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Chen", "Shizhe", ""], ["Witten", "Daniela", ""], ["Shojaie", "Ali", ""]]}, {"id": "1311.0096", "submitter": "Gael Martin Prof", "authors": "D. S. Poskitt, Simone D. Grose and Gael M. Martin", "title": "Higher-Order Improvements of the Sieve Bootstrap for Fractionally\n  Integrated Processes", "comments": null, "journal-ref": "Published in Journal of Econometrics, 2015. Volume 188, 94-100", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the accuracy of bootstrap-based inference in the case\nof long memory fractionally integrated processes. The re-sampling method is\nbased on the semi-parametric sieve approach, whereby the dynamics in the\nprocess used to produce the bootstrap draws are captured by an autoregressive\napproximation. Application of the sieve method to data pre-filtered by a\nsemi-parametric estimate of the long memory parameter is also explored.\nHigher-order improvements yielded by both forms of re-sampling are demonstrated\nusing Edgeworth expansions for a broad class of statistics that includes first-\nand second-order moments, the discrete Fourier transform and regression\ncoefficients. The methods are then applied to the problem of estimating the\nsampling distributions of the sample mean and of selected sample\nautocorrelation coefficients, in experimental settings. In the case of the\nsample mean, the pre-filtered version of the bootstrap is shown to avoid the\ndistinct underestimation of the sampling variance of the mean which the raw\nsieve method demonstrates in finite samples, higher order accuracy of the\nlatter notwithstanding. Pre-filtering also produces gains in terms of the\naccuracy with which the sampling distributions of the sample autocorrelations\nare reproduced, most notably in the part of the parameter space in which\nasymptotic normality does not obtain. Most importantly, the sieve bootstrap is\nshown to reproduce the (empirically infeasible) Edgeworth expansion of the\nsampling distribution of the autocorrelation coefficients, in the part of the\nparameter space in which the expansion is valid.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 06:14:14 GMT"}], "update_date": "2016-03-08", "authors_parsed": [["Poskitt", "D. S.", ""], ["Grose", "Simone D.", ""], ["Martin", "Gael M.", ""]]}, {"id": "1311.0098", "submitter": "Giovanni Petris", "authors": "Giovanni Petris", "title": "A Bayesian framework for functional time series analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces a general framework for statistical analysis of\nfunctional time series from a Bayesian perspective. The proposed approach,\nbased on an extension of the popular dynamic linear model to Banach-space\nvalued observations and states, is very flexible but also easy to implement in\nmany cases. For many kinds of data, such as continuous functions, we show how\nthe general theory of stochastic processes provides a convenient tool to\nspecify priors and transition probabilities of the model. Finally, we show how\nstandard Markov chain Monte Carlo methods for posterior simulation can be\nemployed under consistent discretizations of the data.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 06:21:28 GMT"}, {"version": "v2", "created": "Fri, 29 Nov 2013 15:46:54 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Petris", "Giovanni", ""]]}, {"id": "1311.0270", "submitter": "Marie Kratz", "authors": "Marie Kratz", "title": "There is a VaR beyond usual approximations", "comments": "33 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME q-fin.RM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Basel II and Solvency 2 both use the Value-at-Risk (VaR) as the risk measure\nto compute the Capital Requirements. In practice, to calibrate the VaR, a\nnormal approximation is often chosen for the unknown distribution of the yearly\nlog returns of financial assets. This is usually justified by the use of the\nCentral Limit Theorem (CLT), when assuming aggregation of independent and\nidentically distributed (iid) observations in the portfolio model. Such a\nchoice of modeling, in particular using light tail distributions, has proven\nduring the crisis of 2008/2009 to be an inadequate approximation when dealing\nwith the presence of extreme returns; as a consequence, it leads to a gross\nunderestimation of the risks. The main objective of our study is to obtain the\nmost accurate evaluations of the aggregated risks distribution and risk\nmeasures when working on financial or insurance data under the presence of\nheavy tail and to provide practical solutions for accurately estimating high\nquantiles of aggregated risks. We explore a new method, called Normex, to\nhandle this problem numerically as well as theoretically, based on properties\nof upper order statistics. Normex provides accurate results, only weakly\ndependent upon the sample size and the tail index. We compare it with existing\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 19:34:07 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Kratz", "Marie", ""]]}, {"id": "1311.0274", "submitter": "Adel Javanmard", "authors": "Adel Javanmard and Andrea Montanari", "title": "Nearly Optimal Sample Size in Hypothesis Testing for High-Dimensional\n  Regression", "comments": "21 pages, short version appears in Annual Allerton Conference on\n  Communication, Control and Computing, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.IT cs.LG math.IT stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of fitting the parameters of a high-dimensional\nlinear regression model. In the regime where the number of parameters $p$ is\ncomparable to or exceeds the sample size $n$, a successful approach uses an\n$\\ell_1$-penalized least squares estimator, known as Lasso. Unfortunately,\nunlike for linear estimators (e.g., ordinary least squares), no\nwell-established method exists to compute confidence intervals or p-values on\nthe basis of the Lasso estimator. Very recently, a line of work\n\\cite{javanmard2013hypothesis, confidenceJM, GBR-hypothesis} has addressed this\nproblem by constructing a debiased version of the Lasso estimator. In this\npaper, we study this approach for random design model, under the assumption\nthat a good estimator exists for the precision matrix of the design. Our\nanalysis improves over the state of the art in that it establishes nearly\noptimal \\emph{average} testing power if the sample size $n$ asymptotically\ndominates $s_0 (\\log p)^2$, with $s_0$ being the sparsity level (number of\nnon-zero coefficients). Earlier work obtains provable guarantees only for much\nlarger sample size, namely it requires $n$ to asymptotically dominate $(s_0\n\\log p)^2$.\n  In particular, for random designs with a sparse precision matrix we show that\nan estimator thereof having the required properties can be computed\nefficiently. Finally, we evaluate this approach on synthetic data and compare\nit with earlier proposals.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 19:41:42 GMT"}], "update_date": "2013-11-04", "authors_parsed": [["Javanmard", "Adel", ""], ["Montanari", "Andrea", ""]]}, {"id": "1311.0291", "submitter": "Emil Pitkin", "authors": "Emil Pitkin, Richard Berk, Lawrence Brown, Andreas Buja, Ed George,\n  Kai Zhang, Linda Zhao", "title": "Improved Precision in Estimating Average Treatment Effects", "comments": "22 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Average Treatment Effect (ATE) is a global measure of the effectiveness\nof an experimental treatment intervention. Classical methods of its estimation\neither ignore relevant covariates or do not fully exploit them. Moreover, past\nwork has considered covariates as fixed. We present a method for improving the\nprecision of the ATE estimate: the treatment and control responses are\nestimated via a regression, and information is pooled between the groups to\nproduce an asymptotically unbiased estimate; we subsequently justify the random\nX paradigm underlying the result. Standard errors are derived, and the\nestimator's performance is compared to the traditional estimator. Conditions\nunder which the regression-based estimator is preferable are detailed, and a\ndemonstration on real data is presented.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 20:06:27 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Pitkin", "Emil", ""], ["Berk", "Richard", ""], ["Brown", "Lawrence", ""], ["Buja", "Andreas", ""], ["George", "Ed", ""], ["Zhang", "Kai", ""], ["Zhao", "Linda", ""]]}, {"id": "1311.0307", "submitter": "Eric Lock", "authors": "Eric F. Lock and David B. Dunson", "title": "Shared kernel Bayesian screening", "comments": "Author version of article published in Biometrika; 23 pages, 9\n  figures", "journal-ref": "Biometrika 102(4), 829-842, 2015", "doi": "10.1093/biomet/asv032", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article concerns testing for equality of distribution between groups. We\nfocus on screening variables with shared distributional features such as common\nsupport, modes and patterns of skewness. We propose a Bayesian testing method\nusing kernel mixtures, which improves performance by borrowing information\nacross the different variables and groups through shared kernels and a common\nprobability of group differences. The inclusion of shared kernels in a finite\nmixture, with Dirichlet priors on the weights, leads to a simple framework for\ntesting that scales well for high-dimensional data. We provide closed\nasymptotic forms for the posterior probability of equivalence in two groups and\nprove consistency under model misspecification. The method is applied to DNA\nmethylation array data from a breast cancer study, and compares favorably to\ncompetitors when type I error is estimated via permutation.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 21:03:07 GMT"}, {"version": "v2", "created": "Tue, 15 Apr 2014 18:25:38 GMT"}, {"version": "v3", "created": "Wed, 17 Feb 2016 22:31:44 GMT"}], "update_date": "2016-02-19", "authors_parsed": [["Lock", "Eric F.", ""], ["Dunson", "David B.", ""]]}, {"id": "1311.0317", "submitter": "Paul McNicholas", "authors": "Brian C. Franczak, Paul D. McNicholas, Ryan P. Browne and Paula M.\n  Murray", "title": "Parsimonious Shifted Asymmetric Laplace Mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A family of parsimonious shifted asymmetric Laplace mixture models is\nintroduced. We extend the mixture of factor analyzers model to the shifted\nasymmetric Laplace distribution. Imposing constraints on the constitute parts\nof the resulting decomposed component scale matrices leads to a family of\nparsimonious models. An explicit two-stage parameter estimation procedure is\ndescribed, and the Bayesian information criterion and the integrated completed\nlikelihood are compared for model selection. This novel family of models is\napplied to real data, where it is compared to its Gaussian analogue within\nclustering and classification paradigms.\n", "versions": [{"version": "v1", "created": "Fri, 1 Nov 2013 22:14:06 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Franczak", "Brian C.", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""], ["Murray", "Paula M.", ""]]}, {"id": "1311.0343", "submitter": "Robin Willink", "authors": "R. Willink and B. D. Hall", "title": "An extension to GUM methodology: degrees-of-freedom calculations for\n  correlated multidimensional estimates", "comments": "30 pages with 2 embedded figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.data-an stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Guide to the Expression of Uncertainty in Measurement advocates the use\nof an 'effective number of degrees of freedom' for the calculation of an\ninterval of measurement uncertainty. However, it does not describe how this\nnumber is to be calculated when (i) the measurand is a vector quantity or (ii)\nwhen the errors in the estimates of the quantities defining the measurand (the\n'input quantities') are not incurred independently. An appropriate analysis for\na vector-valued measurand has been described (Metrologia 39 (2002) 361-9), and\na method for a one-dimensional measurand with dependent errors has also been\ngiven (Metrologia 44 (2007) 340-9). This paper builds on those analyses to\npresent a method for the situation where the problem is multidimensional and\ninvolves correlated errors. The result is an explicit general procedure that\nreduces to simpler procedures where appropriate. The example studied is from\nthe field of radio-frequency metrology, where measured quantities are often\ncomplex-valued and can be regarded as vectors of two elements.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 05:06:06 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Willink", "R.", ""], ["Hall", "B. D.", ""]]}, {"id": "1311.0412", "submitter": "Timothy Christensen", "authors": "Xiaohong Chen and Timothy Christensen", "title": "Optimal Uniform Convergence Rates for Sieve Nonparametric Instrumental\n  Variables Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of nonparametric regression when the regressor is\nendogenous, which is an important nonparametric instrumental variables (NPIV)\nregression in econometrics and a difficult ill-posed inverse problem with\nunknown operator in statistics. We first establish a general upper bound on the\nsup-norm (uniform) convergence rate of a sieve estimator, allowing for\nendogenous regressors and weakly dependent data. This result leads to the\noptimal sup-norm convergence rates for spline and wavelet least squares\nregression estimators under weakly dependent data and heavy-tailed error terms.\nThis upper bound also yields the sup-norm convergence rates for sieve NPIV\nestimators under i.i.d. data: the rates coincide with the known optimal\n$L^2$-norm rates for severely ill-posed problems, and are power of $\\log(n)$\nslower than the optimal $L^2$-norm rates for mildly ill-posed problems. We then\nestablish the minimax risk lower bound in sup-norm loss, which coincides with\nour upper bounds on sup-norm rates for the spline and wavelet sieve NPIV\nestimators. This sup-norm rate optimality provides another justification for\nthe wide application of sieve NPIV estimators. Useful results on\nweakly-dependent random matrices are also provided.\n", "versions": [{"version": "v1", "created": "Sat, 2 Nov 2013 21:25:13 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Chen", "Xiaohong", ""], ["Christensen", "Timothy", ""]]}, {"id": "1311.0463", "submitter": "Michio Yamamoto", "authors": "Michio Yamamoto and Yoshikazu Terada", "title": "Functional Factorial K-means Analysis", "comments": "39 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new procedure for simultaneously finding the optimal cluster structure of\nmultivariate functional objects and finding the subspace to represent the\ncluster structure is presented. The method is based on the $k$-means criterion\nfor projected functional objects on a subspace in which a cluster structure\nexists. An efficient alternating least-squares algorithm is described, and the\nproposed method is extended to a regularized method for smoothness of weight\nfunctions. To deal with the negative effect of the correlation of coefficient\nmatrix of the basis function expansion in the proposed algorithm, a two-step\napproach to the proposed method is also described. Analyses of artificial and\nreal data demonstrate that the proposed method gives correct and interpretable\nresults compared with existing methods, the functional principal component\n$k$-means (FPCK) method and tandem clustering approach. It is also shown that\nthe proposed method can be considered complementary to FPCK.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 13:19:47 GMT"}, {"version": "v2", "created": "Sun, 9 Feb 2014 03:54:41 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Yamamoto", "Michio", ""], ["Terada", "Yoshikazu", ""]]}, {"id": "1311.0524", "submitter": "Thomas Furmston", "authors": "Thomas Furmston, Stephen Hailes, A. Jennifer Morton", "title": "A Bayesian Residual-Based Test for Cointegration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cointegration is an important concept in the analysis of non-stationary\ntime-series, giving conditions under which a collection of non-stationary\nprocesses has an underlying stationary (cointegration) relationship. In this\npaper we present the first fully Bayesian residual-based test for\ncointegration, where we consider the whole space of possible cointegration\nrelationships when testing for the presence of cointegration. We first\ndemonstrate that such a test can be performed exactly in the case where the\nresidual process follows a first-order autoregressive process. We then extend\nthis test to include more complex residual processes, where we first consider a\nsuitable cointegration test-statistic and then leverage Bayesian sampling\ntechniques to perform the necessary inference. We empirically demonstrate that\nour Bayesian approach attains a superior classification accuracy than existing\napproaches, all of which use a point estimate of the cointegration relationship\nin their test. Finally, we demonstrate our approach on some real world\nfinancial time-series data.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 20:49:07 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Furmston", "Thomas", ""], ["Hailes", "Stephen", ""], ["Morton", "A. Jennifer", ""]]}, {"id": "1311.0530", "submitter": "Kostas Triantafyllopoulos", "authors": "K. Triantafyllopoulos", "title": "Multivariate stochastic volatility modelling using Wishart\n  autoregressive processes", "comments": "29 pages, 3 figures, 2 tables", "journal-ref": "Journal of Time Series Analysis, 2012, 33, 48-60", "doi": "10.1111/j.1467-9892.2011.00738.x", "report-no": null, "categories": "q-fin.CP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new multivariate stochastic volatility estimation procedure for financial\ntime series is proposed. A Wishart autoregressive process is considered for the\nvolatility precision covariance matrix, for the estimation of which a two step\nprocedure is adopted. The first step is the conditional inference on the\nautoregressive parameters and the second step is the unconditional inference,\nbased on a Newton-Raphson iterative algorithm. The proposed methodology, which\nis mostly Bayesian, is suitable for medium dimensional data and it bridges the\ngap between closed-form estimation and simulation-based estimation algorithms.\nAn example, consisting of foreign exchange rates data, illustrates the proposed\nmethodology.\n", "versions": [{"version": "v1", "created": "Sun, 3 Nov 2013 21:08:03 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Triantafyllopoulos", "K.", ""]]}, {"id": "1311.0562", "submitter": "Subhadeep Mukhopadhyay", "authors": "Emanuel Parzen, Subhadeep Mukhopadhyay", "title": "LP Mixed Data Science : Outline of Theory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the theoretical foundation of a new frontier of\nresearch-`LP Mixed Data Science'-that simultaneously extends and integrates the\npractice of traditional and novel statistical methods for nonparametric\nexploratory data modeling, and is applicable to the teaching and training of\nstatistics.\n  Statistics journals have great difficulty accepting papers unlike those\npreviously published. For statisticians with new big ideas a practical strategy\nis to publish them in many small applied studies which enables one to provide\nreferences to work of others. This essay outlines the many concepts, new\ntheory, and important algorithms of our new culture of statistical science\ncalled LP MIXED DATA SCIENCE. It provides comprehensive solutions to problems\nof data analysis and nonparametric modeling of many variables that are\ncontinuous or discrete, which does not yet have a large literature. It develops\na new modeling approach to nonparametric estimation of the multivariate copula\ndensity. We discuss the theory which we believe is very elegant (and can\nprovide a framework for United Statistical Algorithms, for traditional Small\nData methods and Big Data methods).\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 01:56:04 GMT"}, {"version": "v2", "created": "Wed, 6 Nov 2013 13:44:49 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Parzen", "Emanuel", ""], ["Mukhopadhyay", "Subhadeep", ""]]}, {"id": "1311.0634", "submitter": "Kostas Triantafyllopoulos", "authors": "K. Triantafyllopoulos", "title": "Real-time covariance estimation for the local level model", "comments": "34 pages, 3 figures, 1 table", "journal-ref": "Journal of Time Series Analysis, 2011, 32, 93-107", "doi": "10.1111/j.1467-9892.2010.00686.x", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops on-line inference for the multivariate local level model,\nwith the focus being placed on covariance estimation of the innovations. We\nassess the application of the inverse Wishart prior distribution in this\ncontext and find it too restrictive since the serial correlation structure of\nthe observation and state innovations is forced to be the same. We generalize\nthe inverse Wishart distribution to allow for a more convenient correlation\nstructure, but still retaining approximate conjugacy. We prove some relevant\nresults for the new distribution and we develop approximate Bayesian inference,\nwhich allows simultaneous forecasting of time series data and estimation of the\ncovariance of the innovations of the model. We provide results on the steady\nstate of the level of the time series, which are deployed to achieve\ncomputational savings. Using Monte Carlo experiments, we compare the proposed\nmethodology with existing estimation procedures. An example with real data\nconsisting of production data from an industrial process is given.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 10:17:43 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Triantafyllopoulos", "K.", ""]]}, {"id": "1311.0834", "submitter": "Florian Heinrichs", "authors": "Nicolai Bissantz, Holger Dette, Thimo Hildebrandt", "title": "Smooth backfitting in additive inverse regression", "comments": "Keywords: inverse regression; additive models; curse of\n  dimensionality; smooth backfitting Mathematical subject classification:\n  Primary: 62G20; Secondary 15A29 Pages: 26 Figures: 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating an additive regression function in an\ninverse regres- sion model with a convolution type operator. A smooth\nbackfitting procedure is developed and asymptotic normality of the resulting\nestimator is established. Compared to other meth- ods for the estimation in\nadditive models the new approach neither requires observations on a regular\ngrid nor the estimation of the joint density of the predictor. It is also\ndemonstrated by means of a simulation study that the backfitting estimator\noutperforms the marginal in- tegration method at least by a factor two with\nrespect to the integrated mean squared error criterion.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 20:14:17 GMT"}], "update_date": "2016-11-26", "authors_parsed": [["Bissantz", "Nicolai", ""], ["Dette", "Holger", ""], ["Hildebrandt", "Thimo", ""]]}, {"id": "1311.0835", "submitter": "Florian Heinrichs", "authors": "Ina Burghaus, Holger Dette", "title": "Optimal designs for nonlinear regression models with respect to\n  non-informative priors", "comments": "Keywords: optimal design; Bayesian optimality criteria;\n  non-informative prior; Jeffreys prior; reference prior; polynomial\n  regression; canonical moments; heteroscedasticity Pages: 21 Figures: 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In nonlinear regression models the Fisher information depends on the\nparameters of the model. Consequently, optimal designs maximizing some\nfunctional of the information matrix cannot be implemented directly but require\nsome preliminary knowledge about the unknown parameters. Bayesian optimality\ncriteria provide an attractive solution to this problem. These criteria depend\nsensitively on a reasonable specification of a prior distribution for the model\nparameters which might not be available in all applications. In this paper we\ninvestigate Bayesian optimality criteria with non-informative prior dis-\ntributions. In particular, we study the Jeffreys and the Berger-Bernardo prior\nfor which the corresponding optimality criteria are not necessarily concave.\nSeveral examples are investigated where optimal designs with respect to the new\ncriteria are calculated and compared to Bayesian optimal designs based on a\nuniform and a functional uniform prior.\n", "versions": [{"version": "v1", "created": "Mon, 4 Nov 2013 20:20:24 GMT"}], "update_date": "2013-11-05", "authors_parsed": [["Burghaus", "Ina", ""], ["Dette", "Holger", ""]]}, {"id": "1311.1039", "submitter": "Roland Langrock", "authors": "Th\\'eo Michelot, Roland Langrock, Thomas Kneib, Ruth King", "title": "Maximum penalized likelihood estimation in semiparametric\n  capture-recapture models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the semiparametric modeling of mark-recapture-recovery data where\nthe temporal and/or individual variation of model parameters is explained via\ncovariates. Typically, in such analyses a fixed (or mixed) effects parametric\nmodel is specified for the relationship between the model parameters and the\ncovariates of interest. In this paper, we discuss the modeling of the\nrelationship via the use of penalized splines, to allow for considerably more\nflexible functional forms. Corresponding models can be fitted via numerical\nmaximum penalized likelihood estimation, employing cross-validation to choose\nthe smoothing parameters in a data-driven way. Our contribution builds on and\nextends the existing literature, providing a unified inferential framework for\nsemiparametric mark-recapture-recovery models for open populations, where the\ninterest typically lies in the estimation of survival probabilities. The\napproach is applied to two real datasets, corresponding to grey herons (Ardea\nCinerea), where we model the survival probability as a function of\nenvironmental condition (a time-varying global covariate), and Soay sheep (Ovis\nAries), where we model the survival probability as a function of individual\nweight (a time-varying individual-specific covariate). The proposed\nsemiparametric approach is compared to a standard parametric (logistic)\nregression and new interesting underlying dynamics are observed in both cases.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 13:04:59 GMT"}, {"version": "v2", "created": "Wed, 20 May 2015 09:36:43 GMT"}], "update_date": "2015-05-21", "authors_parsed": [["Michelot", "Th\u00e9o", ""], ["Langrock", "Roland", ""], ["Kneib", "Thomas", ""], ["King", "Ruth", ""]]}, {"id": "1311.1189", "submitter": "Michalis Titsias", "authors": "Michalis K. Titsias, Christopher Yau, Christopher C. Holmes", "title": "Statistical Inference in Hidden Markov Models using $k$-segment\n  Constraints", "comments": "37 pages", "journal-ref": null, "doi": "10.1080/01621459.2014.998762", "report-no": null, "categories": "stat.ME cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models (HMMs) are one of the most widely used statistical\nmethods for analyzing sequence data. However, the reporting of output from HMMs\nhas largely been restricted to the presentation of the most-probable (MAP)\nhidden state sequence, found via the Viterbi algorithm, or the sequence of most\nprobable marginals using the forward-backward (F-B) algorithm. In this article,\nwe expand the amount of information we could obtain from the posterior\ndistribution of an HMM by introducing linear-time dynamic programming\nalgorithms that, we collectively call $k$-segment algorithms, that allow us to\ni) find MAP sequences, ii) compute posterior probabilities and iii) simulate\nsample paths conditional on a user specified number of segments, i.e.\ncontiguous runs in a hidden state, possibly of a particular type. We illustrate\nthe utility of these methods using simulated and real examples and highlight\nthe application of prospective and retrospective use of these methods for\nfitting HMMs or exploring existing model fits.\n", "versions": [{"version": "v1", "created": "Tue, 5 Nov 2013 20:41:17 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Titsias", "Michalis K.", ""], ["Yau", "Christopher", ""], ["Holmes", "Christopher C.", ""]]}, {"id": "1311.1292", "submitter": "Ewan Cameron Dr", "authors": "Ewan Cameron", "title": "A Generalized Savage-Dickey Ratio", "comments": "6 pages, 0 figures, submitted to EJS, comments/suggestions welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this brief research note I present a generalized version of the\nSavage-Dickey Density Ratio for representation of the Bayes factor (or marginal\nlikelihood ratio) of nested statistical models; the new version takes the form\nof a Radon-Nikodym derivative and is thus applicable to a wider family of\nprobability spaces than the original (restricted to those admitting an ordinary\nLebesgue density). A derivation is given following the measure-theoretic\nconstruction of Marin & Robert (2010), and the equivalent estimator is\ndemonstrated in application to a distributional modeling problem.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 05:59:32 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Cameron", "Ewan", ""]]}, {"id": "1311.1438", "submitter": "Yingying   Wei", "authors": "Yingying Wei and Hongkai Ji", "title": "Joint Analysis of Differential Gene Expression in Multiple Studies using\n  Correlation Motifs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard methods for detecting differential gene expression are mostly\ndesigned for analyzing a single gene expression experiment. When data from\nmultiple related gene expression studies are available, separately analyzing\neach study is not an ideal strategy as it may fail to detect important genes\nwith consistent but relatively weak differential signals in multiple studies.\nJointly modeling all data allows one to borrow information across studies to\nimprove the analysis. However, a simple concordance model, in which each gene\nis assumed to be differential in either all studies or none of the studies, is\nincapable of handling genes with study-specific differential expression. In\ncontrast, a model that naively enumerates and analyzes all possible\ndifferential patterns across all studies can deal with study-specificity and\nallow information pooling, but the complexity of its parameter space grows\nexponentially as the number of studies increases. Here we propose a\n\"correlation motif\" approach to address this dilemma. This approach\nautomatically searches for a small number of latent probability vectors called\n\"correlation motifs\" to capture the major correlation patterns among multiple\nstudies. The motifs provide the basis for sharing information among studies and\ngenes. The approach improves detection of differential expression and overcomes\nthe barrier of exponentially growing parameter space. It is capable of handling\nall possible study-specific differential patterns in a large number of studies.\nThe advantages of this new approach over existing methods are illustrated using\nboth simulated and real data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 16:24:44 GMT"}], "update_date": "2013-11-07", "authors_parsed": [["Wei", "Yingying", ""], ["Ji", "Hongkai", ""]]}, {"id": "1311.1454", "submitter": "Mark Steel", "authors": "Catalina A. Vallejos and Mark F.J. Steel", "title": "On posterior propriety for the Student-$t$ linear regression model under\n  Jeffreys priors", "comments": "minor editorial changes in this version", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.AP stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regression models with fat-tailed error terms are an increasingly popular\nchoice to obtain more robust inference to the presence of outlying\nobservations. This article focuses on Bayesian inference for the Student-$t$\nlinear regression model under objective priors that are based on the Jeffreys\nrule. Posterior propriety results presented in Fonseca et al. (2008) are\nrevisited and corrected. In particular, it is shown that the standard\nJeffreys-rule prior precludes the existence of a proper posterior distribution.\n", "versions": [{"version": "v1", "created": "Wed, 6 Nov 2013 17:34:08 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2013 11:07:02 GMT"}], "update_date": "2013-11-11", "authors_parsed": [["Vallejos", "Catalina A.", ""], ["Steel", "Mark F. J.", ""]]}, {"id": "1311.1595", "submitter": "Sokbae Lee", "authors": "Sokbae Lee, Kyungchul Song, Yoon-Jae Whang", "title": "Testing for a General Class of Functional Inequalities", "comments": "128 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a general method for testing inequality\nrestrictions on nonparametric functions. Our framework includes many\nnonparametric testing problems in a unified framework, with a number of\npossible applications in auction models, game theoretic models, wage\ninequality, and revealed preferences. Our test involves a one-sided version of\n$L_{p}$ functionals of kernel-type estimators $(1\\leq p <\\infty )$ and is easy\nto implement in general, mainly due to its recourse to the bootstrap method.\nThe bootstrap procedure is based on nonparametric bootstrap applied to\nkernel-based test statistics, with estimated \"contact sets.\" We provide\nregularity conditions under which the bootstrap test is asymptotically valid\nuniformly over a large class of distributions, including the cases that the\nlimiting distribution of the test statistic is degenerate. Our bootstrap test\nis shown to exhibit good power properties in Monte Carlo experiments, and we\nprovide a general form of the local power function. As an illustration, we\nconsider testing implications from auction theory, provide primitive conditions\nfor our test, and demonstrate its usefulness by applying our test to real data.\nWe supplement this example with the second empirical illustration in the\ncontext of wage inequality.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 08:16:52 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2013 02:09:53 GMT"}, {"version": "v3", "created": "Tue, 25 Feb 2014 04:15:43 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2015 13:59:14 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Lee", "Sokbae", ""], ["Song", "Kyungchul", ""], ["Whang", "Yoon-Jae", ""]]}, {"id": "1311.1731", "submitter": "Edoardo Airoldi", "authors": "Edoardo M Airoldi, Thiago B Costa, Stanley H Chan", "title": "Stochastic blockmodel approximation of a graphon: Theory and consistent\n  estimation", "comments": "20 pages, 4 figures, 2 algorithms. Neural Information Processing\n  Systems (NIPS), 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.LG cs.SI physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-parametric approaches for analyzing network data based on exchangeable\ngraph models (ExGM) have recently gained interest. The key object that defines\nan ExGM is often referred to as a graphon. This non-parametric perspective on\nnetwork modeling poses challenging questions on how to make inference on the\ngraphon underlying observed network data. In this paper, we propose a\ncomputationally efficient procedure to estimate a graphon from a set of\nobserved networks generated from it. This procedure is based on a stochastic\nblockmodel approximation (SBA) of the graphon. We show that, by approximating\nthe graphon with a stochastic block model, the graphon can be consistently\nestimated, that is, the estimation error vanishes as the size of the graph\napproaches infinity.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 16:20:02 GMT"}, {"version": "v2", "created": "Fri, 8 Nov 2013 04:09:51 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Airoldi", "Edoardo M", ""], ["Costa", "Thiago B", ""], ["Chan", "Stanley H", ""]]}, {"id": "1311.1797", "submitter": "Alexandre Janon", "authors": "Fabrice Gamboa (UMR CNRS 5219), Alexandre Janon (LM-Orsay, -\n  M\\'ethodes d'Analyse Stochastique des Codes et Traitements Num\\'eriques),\n  Thierry Klein (IMT), Agn\\`es Lagnoux (IMT)", "title": "Sensitivity analysis for multidimensional and functional outputs", "comments": "Fixed missing references", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X:=(X_1, \\ldots, X_p)$ be random objects (the inputs), defined on some\nprobability space $(\\Omega,{\\mathcal{F}}, \\mathbb P)$ and valued in some\nmeasurable space $E=E_1\\times\\ldots \\times E_p$. Further, let $Y:=Y = f(X_1,\n\\ldots, X_p)$ be the output. Here, $f$ is a measurable function from $E$ to\nsome Hilbert space $\\mathbb{H}$ ($\\mathbb{H}$ could be either of finite or\ninfinite dimension). In this work, we give a natural generalization of the\nSobol indices (that are classically defined when $Y\\in\\mathbb R$ ), when the\noutput belongs to $\\mathbb{H}$. These indices have very nice properties. First,\nthey are invariant. under isometry and scaling. Further they can be, as in\ndimension $1$, easily estimated by using the so-called Pick and Freeze method.\nWe investigate the asymptotic behaviour of such estimation scheme.\n", "versions": [{"version": "v1", "created": "Thu, 7 Nov 2013 20:09:54 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2013 09:42:36 GMT"}], "update_date": "2013-11-15", "authors_parsed": [["Gamboa", "Fabrice", "", "UMR CNRS 5219"], ["Janon", "Alexandre", "", "LM-Orsay, -\n  M\u00e9thodes d'Analyse Stochastique des Codes et Traitements Num\u00e9riques"], ["Klein", "Thierry", "", "IMT"], ["Lagnoux", "Agn\u00e8s", "", "IMT"]]}, {"id": "1311.2105", "submitter": "Ian Dryden", "authors": "Wen Cheng, Ian L. Dryden and Xianzheng Huang", "title": "Bayesian registration of functions and curves", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian analysis of functions and curves is considered, where warping and\nother geometrical transformations are often required for meaningful\ncomparisons. We focus on two applications involving the classification of mouse\nvertebrae shape outlines and the alignment of mass spectrometry data in\nproteomics. The functions and curves of interest are represented using the\nrecently introduced square root velocity function, which enables a warping\ninvariant elastic distance to be calculated in a straightforward manner. We\ndistinguish between various spaces of interest: the original space, the ambient\nspace after standardizing, and the quotient space after removing a group of\ntransformations. Using Gaussian process models in the ambient space and\nDirichlet priors for the warping functions, we explore Bayesian inference for\ncurves and functions. Markov chain Monte Carlo algorithms are introduced for\nsimulating from the posterior, including simulated tempering for multimodal\nposteriors. We also compare ambient and quotient space estimators for mean\nshape, and explain their frequent similarity in many practical problems using a\nLaplace approximation. A simulation study is carried out, as well as shape\nclassification of the mouse vertebra outlines and practical alignment of the\nmass spectrometry functions.\n", "versions": [{"version": "v1", "created": "Fri, 8 Nov 2013 23:27:12 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Cheng", "Wen", ""], ["Dryden", "Ian L.", ""], ["Huang", "Xianzheng", ""]]}, {"id": "1311.2422", "submitter": "Sourabh Bhattacharya", "authors": "Sabyasachi Mukhopadhyay and Sourabh Bhattacharya", "title": "Clustering Categorical Time Series into Unknown Number of Clusters: A\n  Perfect Simulation based Approach", "comments": "This is not a finished work -- simulations and real data applications\n  are pending. We will provide updates whenever they are available", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pamminger and Fruwirth-Schnatter (2010) considered a Bayesian approach to\nmodel-based clustering of categorical time series assuming a fixed number of\nclusters. But the popular methods for selecting the number of clusters, for\nexample, the Bayes Information Criterion (BIC), turned out to have severe\nproblems in the categorical time series context.\n  In this paper, we circumvent the difficulties of choosing the number of\nclusters by adopting the Bayesian semiparametric mixture model approach\nintroduced by Bhattacharya (2008), who assume that the number of clusters is a\nrandom quantity, but is bounded above by a (possibly large) number of clusters.\nWe adopt the perfect simulation approach of Mukhopadhyay and Bhattacharya\n(2012) for posterior simulation for completely solving the problems of\nconvergence of the underlying Markov chain Monte Carlo (MCMC) approach.\nImportantly, within our main perfect simulation algorithm, there arose the\nnecessity to simulate perfectly from the joint distribution of a set of\ncontinuous random variables with log-concave full conditional densities. We\npropose and develop a novel and efficient perfect simulation methodology for\njoint distributions with log-concave full conditionals. This perfect sampling\nmethodology is of independent interest as well since in a very large and\nimportant class of Bayesian applications the full conditionals turn out to be\nlog-concave.\n  We will consider application of our model and methodology to the Austrian\nwage mobility data, also analysed by Pamminger and Fruwirth-Schnatter (2010),\nand adopting the methods developed in Mukhopadhyay et al. (2011), Mukhopadhyay\net al. (2012), will obtain the posterior modes of clusterings and also the\ndesired highest posterior distribution credible regions of the posterior\ndistribution of clusterings.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 11:59:18 GMT"}], "update_date": "2013-11-12", "authors_parsed": [["Mukhopadhyay", "Sabyasachi", ""], ["Bhattacharya", "Sourabh", ""]]}, {"id": "1311.2506", "submitter": "Zhijian Wang", "authors": "Zhijian Wang, Siqian Zhu, Bin Xu", "title": "A Comment on \"Cycles and Instability in a Rock-Paper-Scissors Population\n  Game: A Continuous Time Experiment\"", "comments": "2 pages, Keywords: experiments, cycles, mixed equilibrium, discrete\n  time. JEL numbers: C72, C73, C92, D83", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors (Cason, Friedman and Hopkins, Reviews of Economics Studies, 2014)\nclaimed a result that the treatments (using simultaneous matching in discrete\ntime) replicate previous results that exhibit weak or no cycles. After correct\ntwo mathematical mistakes in their cycles tripwire algorithm, we research the\ncycles by scanning the tripwire in the full strategy space of the games and we\nfind significant cycles missed by the authors. So we suggest that, all of the\ntreatments (using simultaneous matching in discrete time) exhibit significant\ncycles.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 17:15:48 GMT"}, {"version": "v2", "created": "Sat, 16 Nov 2013 06:08:37 GMT"}, {"version": "v3", "created": "Tue, 19 Nov 2013 18:08:36 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Wang", "Zhijian", ""], ["Zhu", "Siqian", ""], ["Xu", "Bin", ""]]}, {"id": "1311.2610", "submitter": "Xiaoyue Niu", "authors": "Xiaoyue Niu and Peter D. Hoff", "title": "Joint Mean and Covariance Modeling of Multiple Health Outcome Measures", "comments": "20 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Health exams determine a patient's health status by comparing the patient's\nmeasurement with a population reference range, a 95% interval derived from a\nhomogeneous reference population. Similarly, most of the established relation\namong health problems are assumed to hold for the entire population. We use\ndata from the 2009 - 2010 National Health and Nutrition Examination Survey\n(NHANES) on four major health problems in the U.S. and apply a joint mean and\ncovariance model to study how the reference ranges and associations of those\nhealth outcomes could vary among subpopulations. We discuss guidelines for\nmodel selection and evaluation, using standard criteria such as AIC in\nconjunction with posterior predictive checks. The results from the proposed\nmodel can help identify subpopulations in which more data need to be collected\nto refine the reference range and to study the specific associations among\nthose health problems.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 21:20:54 GMT"}, {"version": "v2", "created": "Wed, 13 Nov 2013 21:45:49 GMT"}, {"version": "v3", "created": "Mon, 16 Dec 2013 20:06:49 GMT"}, {"version": "v4", "created": "Wed, 26 Mar 2014 17:31:20 GMT"}, {"version": "v5", "created": "Sat, 9 Apr 2016 01:32:38 GMT"}, {"version": "v6", "created": "Fri, 10 Nov 2017 21:20:37 GMT"}, {"version": "v7", "created": "Thu, 31 May 2018 18:00:37 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Niu", "Xiaoyue", ""], ["Hoff", "Peter D.", ""]]}, {"id": "1311.2645", "submitter": "Ivan Fernandez-Val", "authors": "Alexandre Belloni and Victor Chernozhukov and Ivan Fern\\'andez-Val and\n  Christian Hansen", "title": "Program Evaluation and Causal Inference with High-Dimensional Data", "comments": "118 pages, 3 tables, 11 figures, includes supplementary appendix.\n  This version corrects some typos in Example 2 of the published version", "journal-ref": "Econometrica, Vol. 85, No. 1 (January, 2017), 233-298", "doi": null, "report-no": null, "categories": "math.ST econ.EM stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide efficient estimators and honest confidence bands\nfor a variety of treatment effects including local average (LATE) and local\nquantile treatment effects (LQTE) in data-rich environments. We can handle very\nmany control variables, endogenous receipt of treatment, heterogeneous\ntreatment effects, and function-valued outcomes. Our framework covers the\nspecial case of exogenous receipt of treatment, either conditional on controls\nor unconditionally as in randomized control trials. In the latter case, our\napproach produces efficient estimators and honest bands for (functional)\naverage treatment effects (ATE) and quantile treatment effects (QTE). To make\ninformative inference possible, we assume that key reduced form predictive\nrelationships are approximately sparse. This assumption allows the use of\nregularization and selection methods to estimate those relations, and we\nprovide methods for post-regularization and post-selection inference that are\nuniformly valid (honest) across a wide-range of models. We show that a key\ningredient enabling honest inference is the use of orthogonal or doubly robust\nmoment conditions in estimating certain reduced form functional parameters. We\nillustrate the use of the proposed methods with an application to estimating\nthe effect of 401(k) eligibility and participation on accumulated assets.\n", "versions": [{"version": "v1", "created": "Mon, 11 Nov 2013 23:36:44 GMT"}, {"version": "v2", "created": "Mon, 30 Dec 2013 19:24:31 GMT"}, {"version": "v3", "created": "Thu, 5 Jun 2014 20:21:24 GMT"}, {"version": "v4", "created": "Sat, 9 Aug 2014 17:53:36 GMT"}, {"version": "v5", "created": "Mon, 21 Sep 2015 02:26:55 GMT"}, {"version": "v6", "created": "Fri, 18 Mar 2016 02:33:33 GMT"}, {"version": "v7", "created": "Wed, 21 Dec 2016 18:57:37 GMT"}, {"version": "v8", "created": "Fri, 5 Jan 2018 07:13:54 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Belloni", "Alexandre", ""], ["Chernozhukov", "Victor", ""], ["Fern\u00e1ndez-Val", "Ivan", ""], ["Hansen", "Christian", ""]]}, {"id": "1311.2948", "submitter": "Gen Li", "authors": "Gen Li, Andrey A. Shabalin, Ivan Rusyn, Fred A. Wright and Andrew B.\n  Nobel", "title": "An Empirical Bayes Approach for Multiple Tissue eQTL Analysis", "comments": "accepted by Biostatistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expression quantitative trait loci (eQTL) analyses, which identify genetic\nmarkers associated with the expression of a gene, are an important tool in the\nunderstanding of diseases in human and other populations. While most eQTL\nstudies to date consider the connection between genetic variation and\nexpression in a single tissue, complex, multi-tissue data sets are now being\ngenerated by the GTEx initiative. These data sets have the potential to improve\nthe findings of single tissue analyses by borrowing strength across tissues,\nand the potential to elucidate the genotypic basis of differences between\ntissues.\n  In this paper we introduce and study a multivariate hierarchical Bayesian\nmodel (MT-eQTL) for multi-tissue eQTL analysis. MT-eQTL directly models the\nvector of correlations between expression and genotype across tissues. It\nexplicitly captures patterns of variation in the presence or absence of eQTLs,\nas well as the heterogeneity of effect sizes across tissues. Moreover, the\nmodel is applicable to complex designs in which the set of donors can (i) vary\nfrom tissue to tissue, and (ii) exhibit incomplete overlap between tissues. The\nMT-eQTL model is marginally consistent, in the sense that the model for a\nsubset of tissues can be obtained from the full model via marginalization.\nFitting of the MT-eQTL model is carried out via empirical Bayes, using an\napproximate EM algorithm. Inferences concerning eQTL detection and the\nconfiguration of eQTLs across tissues are derived from adaptive thresholding of\nlocal false discovery rates, and maximum a-posteriori estimation, respectively.\nWe investigate the MT-eQTL model through a simulation study, and rigorously\nestablish the FDR control of the local FDR testing procedure under mild\nassumptions appropriate for dependent data.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 21:04:45 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2013 20:17:51 GMT"}, {"version": "v3", "created": "Mon, 28 Sep 2015 18:49:19 GMT"}, {"version": "v4", "created": "Wed, 3 Feb 2016 14:25:32 GMT"}, {"version": "v5", "created": "Thu, 7 Sep 2017 02:56:56 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Li", "Gen", ""], ["Shabalin", "Andrey A.", ""], ["Rusyn", "Ivan", ""], ["Wright", "Fred A.", ""], ["Nobel", "Andrew B.", ""]]}, {"id": "1311.2971", "submitter": "Raja Hafiz Affandi", "authors": "Raja Hafiz Affandi, Emily B. Fox, Ben Taskar", "title": "Approximate Inference in Continuous Determinantal Point Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determinantal point processes (DPPs) are random point processes well-suited\nfor modeling repulsion. In machine learning, the focus of DPP-based models has\nbeen on diverse subset selection from a discrete and finite base set. This\ndiscrete setting admits an efficient sampling algorithm based on the\neigendecomposition of the defining kernel matrix. Recently, there has been\ngrowing interest in using DPPs defined on continuous spaces. While the\ndiscrete-DPP sampler extends formally to the continuous case, computationally,\nthe steps required are not tractable in general. In this paper, we present two\nefficient DPP sampling schemes that apply to a wide range of kernel functions:\none based on low rank approximations via Nystrom and random Fourier feature\ntechniques and another based on Gibbs sampling. We demonstrate the utility of\ncontinuous DPPs in repulsive mixture modeling and synthesizing human poses\nspanning activity spaces.\n", "versions": [{"version": "v1", "created": "Tue, 12 Nov 2013 22:15:26 GMT"}], "update_date": "2013-11-14", "authors_parsed": [["Affandi", "Raja Hafiz", ""], ["Fox", "Emily B.", ""], ["Taskar", "Ben", ""]]}, {"id": "1311.2994", "submitter": "Scott Sisson", "authors": "J. Lee and Y. Fan and S. A. Sisson", "title": "Bayesian threshold selection for extremal models using measures of\n  surprise", "comments": "To appear in Computational Statistics and Data Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical extreme value theory is concerned with the use of asymptotically\nmotivated models to describe the extreme values of a process. A number of\ncommonly used models are valid for observed data that exceed some high\nthreshold. However, in practice a suitable threshold is unknown and must be\ndetermined for each analysis. While there are many threshold selection methods\nfor univariate extremes, there are relatively few that can be applied in the\nmultivariate setting. In addition, there are only a few Bayesian-based methods,\nwhich are naturally attractive in the modelling of extremes due to data\nscarcity. The use of Bayesian measures of surprise to determine suitable\nthresholds for extreme value models is proposed. Such measures quantify the\nlevel of support for the proposed extremal model and threshold, without the\nneed to specify any model alternatives. This approach is easily implemented for\nboth univariate and multivariate extremes.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 00:57:12 GMT"}, {"version": "v2", "created": "Tue, 9 Dec 2014 11:43:04 GMT"}], "update_date": "2014-12-10", "authors_parsed": [["Lee", "J.", ""], ["Fan", "Y.", ""], ["Sisson", "S. A.", ""]]}, {"id": "1311.3190", "submitter": "Amit Moscovich-Eiger", "authors": "Amit Moscovich, Boaz Nadler, Clifford Spiegelman", "title": "On the exact Berk-Jones statistics and their p-value calculation", "comments": "29 pages, 3 figures, pdflatex; Minor revision", "journal-ref": "Electronic Journal of Statistics 10 (2016) 2329-2354", "doi": "10.1214/16-EJS1172", "report-no": null, "categories": "stat.ME math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous goodness-of-fit testing is a classical problem in statistics.\nDespite having low power for detecting deviations at the tail of a\ndistribution, the most popular test is based on the Kolmogorov-Smirnov\nstatistic. While similar variance-weighted statistics, such as Anderson-Darling\nand the Higher Criticism statistic give more weight to tail deviations, as\nshown in various works, they still mishandle the extreme tails.\n  As a viable alternative, in this paper we study some of the statistical\nproperties of the exact $M_n$ statistics of Berk and Jones. We derive the\nasymptotic null distributions of $M_n, M_n^+, M_n^-$, and further prove their\nconsistency as well as asymptotic optimality for a wide range of rare-weak\nmixture models. Additionally, we present a new computationally efficient method\nto calculate $p$-values for any supremum-based one-sided statistic, including\nthe one-sided $M_n^+,M_n^-$ and $R_n^+,R_n^-$ statistics of Berk and Jones and\nthe Higher Criticism statistic. We illustrate our theoretical analysis with\nseveral finite-sample simulations.\n", "versions": [{"version": "v1", "created": "Wed, 13 Nov 2013 16:19:58 GMT"}, {"version": "v2", "created": "Sun, 18 May 2014 17:34:05 GMT"}, {"version": "v3", "created": "Thu, 2 Oct 2014 16:49:58 GMT"}, {"version": "v4", "created": "Thu, 18 Jun 2015 11:26:37 GMT"}, {"version": "v5", "created": "Thu, 24 Mar 2016 15:20:16 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Moscovich", "Amit", ""], ["Nadler", "Boaz", ""], ["Spiegelman", "Clifford", ""]]}, {"id": "1311.3350", "submitter": "Jay Bartroff", "authors": "Jay Bartroff and Jinlin Song", "title": "Sequential Tests of Multiple Hypotheses Controlling False Discovery and\n  Nondiscovery Rates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general and flexible procedure for testing multiple hypotheses\nabout sequential (or streaming) data that simultaneously controls both the\nfalse discovery rate (FDR) and false nondiscovery rate (FNR) under minimal\nassumptions about the data streams which may differ in distribution, dimension,\nand be dependent. All that is needed is a test statistic for each data stream\nthat controls the conventional type I and II error probabilities, and no\ninformation or assumptions are required about the joint distribution of the\nstatistics or data streams. The procedure can be used with sequential, group\nsequential, truncated, or other sampling schemes. The procedure is a natural\nextension of Benjamini and Hochberg's (1995) widely-used fixed sample size\nprocedure to the domain of sequential data, with the added benefit of\nsimultaneous FDR and FNR control that sequential sampling affords. We prove the\nprocedure's error control and give some tips for implementation in commonly\nencountered testing situations.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 00:22:07 GMT"}, {"version": "v2", "created": "Fri, 11 Jan 2019 17:33:03 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Bartroff", "Jay", ""], ["Song", "Jinlin", ""]]}, {"id": "1311.3576", "submitter": "Javier  Goz\\'alez", "authors": "Javier Gonz\\'alez, Ivan Vuja\\v{c}i\\'c, Ernst Wit", "title": "Reproducing kernel Hilbert space based estimation of systems of ordinary\n  differential equations", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear systems of differential equations have attracted the interest in\nfields like system biology, ecology or biochemistry, due to their flexibility\nand their ability to describe dynamical systems. Despite the importance of such\nmodels in many branches of science they have not been the focus of systematic\nstatistical analysis until recently. In this work we propose a general approach\nto estimate the parameters of systems of differential equations measured with\nnoise. Our methodology is based on the maximization of the penalized likelihood\nwhere the system of differential equations is used as a penalty. To do so, we\nuse a Reproducing Kernel Hilbert Space approach that allows to formulate the\nestimation problem as an unconstrained numeric maximization problem easy to\nsolve. The proposed method is tested with synthetically simulated data and it\nis used to estimate the unobserved transcription factor CdaR in Steptomyes\ncoelicolor using gene expression data of the genes it regulates.\n", "versions": [{"version": "v1", "created": "Thu, 14 Nov 2013 16:56:34 GMT"}, {"version": "v2", "created": "Thu, 8 May 2014 15:56:54 GMT"}], "update_date": "2014-05-09", "authors_parsed": [["Gonz\u00e1lez", "Javier", ""], ["Vuja\u010di\u0107", "Ivan", ""], ["Wit", "Ernst", ""]]}, {"id": "1311.3709", "submitter": "Noah Simon", "authors": "Noah Simon, Richard Simon", "title": "On Estimating Many Means, Selection Bias, and the Bootstrap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With recent advances in high throughput technology, researchers often find\nthemselves running a large number of hypothesis tests (thousands+) and esti-\nmating a large number of effect-sizes. Generally there is particular interest\nin those effects estimated to be most extreme. Unfortunately naive estimates of\nthese effect-sizes (even after potentially accounting for multiplicity in a\ntesting procedure) can be severely biased. In this manuscript we explore this\nbias from a frequentist perspective: we give a formal definition, and show that\nan oracle estimator using this bias dominates the naive maximum likelihood\nestimate. We give a resampling estimator to approximate this oracle, and show\nthat it works well on simulated data. We also connect this to ideas in\nempirical Bayes.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 01:33:48 GMT"}], "update_date": "2013-11-18", "authors_parsed": [["Simon", "Noah", ""], ["Simon", "Richard", ""]]}, {"id": "1311.3812", "submitter": "Kiranmoy Chatterjee Mr.", "authors": "Kiranmoy Chatterjee and Diganta Mukherjee", "title": "Approximate Bayesian Solution for Estimating Population Size from\n  Dual-record System", "comments": "20 pages and 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For Dual-record system, in the context of human population, the popular\nChandrasekar-Deming model incorporates only the time variation effect on\ncapture probabilities. How-ever, in practice population may undergo behavioral\nchange after being captured first time. In this paper we focus on the\nDual-record system model (equivalent to capture- recapture model with two\nsampling occasions) with both the time as well as behavioral response\nvariation. The relevant model suffers from identifiability problem. Two\napproaches are proposed from which approximate Bayes estimates can be obtained\nusing very simple Gibbs sampling strategies. We explore the features of our two\nproposed methods and their usages depending on the availability (or\nnon-availability) of the information on the nature of behavioral response\neffect. Extensive simulation studies are carried out to evaluate their\nperformances and compare with few available approaches. Finally, a real data\napplication is provided to the model and the methods.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 11:30:27 GMT"}, {"version": "v2", "created": "Mon, 6 Jan 2014 13:32:59 GMT"}, {"version": "v3", "created": "Tue, 28 Apr 2015 20:21:31 GMT"}], "update_date": "2015-04-30", "authors_parsed": [["Chatterjee", "Kiranmoy", ""], ["Mukherjee", "Diganta", ""]]}, {"id": "1311.3888", "submitter": "Philippe Lambert", "authors": "Philippe Lambert", "title": "Spline approximations to conditional Archimedean copula", "comments": "Key words: Conditional copula ; Archimedean copula ; B-splines", "journal-ref": "Stat (Wiley) 3 (2014) 200-217", "doi": "10.1002/sta4.55", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a flexible copula model to describe changes with a covariate in\nthe dependence structure of (conditionally exchangeable) random variables. The\nstarting point is a spline approximation to the generator of an Archimedean\ncopula. Changes in the dependence structure with a covariate $x$ are modelled\nby flexible regression of the spline coefficients on $x$. The performances and\nproperties of the spline estimate of the reference generator and the abilities\nof these conditional models to approximate conditional copulas are studied\nthrough simulations. Inference is made using Bayesian arguments with posterior\ndistributions explored using importance sampling or adaptive MCMC algorithms.\nThe modelling strategy is illustrated with two examples.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 15:48:27 GMT"}], "update_date": "2015-06-01", "authors_parsed": [["Lambert", "Philippe", ""]]}, {"id": "1311.3981", "submitter": "Xiaoquan Wen", "authors": "Xiaoquan Wen", "title": "Robust Bayesian FDR Control using Bayes Factors, with Applications to\n  Multi-tissue eQTL Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the genomic application of expression quantitative trait loci\n(eQTL) mapping, we propose a new procedure to perform simultaneous testing of\nmultiple hypotheses using Bayes factors as input test statistics. One of the\nmost significant features of this method is its robustness in controlling the\ntargeted false discovery rate (FDR) even under misspecifications of parametric\nalternative models. Moreover, the proposed procedure is highly computationally\nefficient, which is ideal for treating both complex system and big data in\ngenomic applications. We discuss the theoretical properties of the new\nprocedure and demonstrate its power and computational efficiency in\napplications of single-tissue and multi-tissue eQTL mapping.\n", "versions": [{"version": "v1", "created": "Fri, 15 Nov 2013 21:22:10 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 17:39:42 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["Wen", "Xiaoquan", ""]]}, {"id": "1311.4091", "submitter": "Madalin Guta", "authors": "Catalin Catana, Theodore Kypraios and Madalin Guta", "title": "Maximum likelihood versus likelihood-free quantum system identification\n  in the atom maser", "comments": "25 pages, 14 figures", "journal-ref": null, "doi": "10.1088/1751-8113/47/41/415302", "report-no": null, "categories": "quant-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the system identification problem of estimating a dynamical\nparameter of a Markovian quantum open system (the atom maser), by performing\ncontinuous time measurements in the system's output (outgoing atoms). Two\nestimation methods are investigated and compared. On the one hand, the maximum\nlikelihood estimator (MLE) takes into account the full measurement data and is\nasymptotically optimal in terms of its mean square error. On the other hand,\nthe `likelihood-free' method of approximate Bayesian computation (ABC) produces\nan approximation of the posterior distribution for a given set of summary\nstatistics, by sampling trajectories at different parameter values and\ncomparing them with the measurement data via chosen statistics.\n  Our analysis is performed on the atom maser model, which exhibits interesting\nfeatures such as bistability and dynamical phase transitions, and has\nconnections with the classical theory of hidden Markov processes. Building on\nprevious results which showed that atom counts are poor statistics for certain\nvalues of the Rabi angle, we apply MLE to the full measurement data and\nestimate its Fisher information. We then select several correlation statistics\nsuch as waiting times, distribution of successive identical detections, and use\nthem as input of the ABC algorithm. The resulting posterior distribution\nfollows closely the data likelihood, showing that the selected statistics\ncontain `most' statistical information about the Rabi angle.\n", "versions": [{"version": "v1", "created": "Sat, 16 Nov 2013 20:25:28 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Catana", "Catalin", ""], ["Kypraios", "Theodore", ""], ["Guta", "Madalin", ""]]}, {"id": "1311.4104", "submitter": "Joan Bruna", "authors": "Joan Bruna, St\\'ephane Mallat, Emmanuel Bacry, Jean-Fran\\c{c}ois Muzy", "title": "Intermittent process analysis with scattering moments", "comments": "Published in at http://dx.doi.org/10.1214/14-AOS1276 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 1, 323-351", "doi": "10.1214/14-AOS1276", "report-no": "IMS-AOS-AOS1276", "categories": "stat.ME math.DS stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scattering moments provide nonparametric models of random processes with\nstationary increments. They are expected values of random variables computed\nwith a nonexpansive operator, obtained by iteratively applying wavelet\ntransforms and modulus nonlinearities, which preserves the variance. First- and\nsecond-order scattering moments are shown to characterize intermittency and\nself-similarity properties of multiscale processes. Scattering moments of\nPoisson processes, fractional Brownian motions, L\\'{e}vy processes and\nmultifractal random walks are shown to have characteristic decay. The\nGeneralized Method of Simulated Moments is applied to scattering moments to\nestimate data generating models. Numerical applications are shown on financial\ntime-series and on energy dissipation of turbulent flows.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 00:40:18 GMT"}, {"version": "v2", "created": "Sun, 8 Dec 2013 17:59:31 GMT"}, {"version": "v3", "created": "Mon, 16 Mar 2015 12:50:07 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Bruna", "Joan", ""], ["Mallat", "St\u00e9phane", ""], ["Bacry", "Emmanuel", ""], ["Muzy", "Jean-Fran\u00e7ois", ""]]}, {"id": "1311.4117", "submitter": "Sinan Yildirim", "authors": "Sinan Yildirim and Sumeetpal Singh and Thomas Dean and Ajay Jasra", "title": "Parameter Estimation in Hidden Markov Models with Intractable\n  Likelihoods Using Sequential Monte Carlo", "comments": "22 pages, 8 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose sequential Monte Carlo based algorithms for maximum likelihood\nestimation of the static parameters in hidden Markov models with an intractable\nlikelihood using ideas from approximate Bayesian computation. The static\nparameter estimation algorithms are gradient based and cover both offline and\nonline estimation. We demonstrate their performance by estimating the\nparameters of three intractable models, namely the alpha-stable distribution,\ng-and-k distribution, and the stochastic volatility model with alpha-stable\nreturns, using both real and synthetic data.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 05:27:56 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Yildirim", "Sinan", ""], ["Singh", "Sumeetpal", ""], ["Dean", "Thomas", ""], ["Jasra", "Ajay", ""]]}, {"id": "1311.4175", "submitter": "Sumanta Basu", "authors": "Sumanta Basu, George Michailidis", "title": "Regularized estimation in sparse high-dimensional time series models", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1315 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 4, 1535-1567", "doi": "10.1214/15-AOS1315", "report-no": "IMS-AOS-AOS1315", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many scientific and economic problems involve the analysis of\nhigh-dimensional time series datasets. However, theoretical studies in\nhigh-dimensional statistics to date rely primarily on the assumption of\nindependent and identically distributed (i.i.d.) samples. In this work, we\nfocus on stable Gaussian processes and investigate the theoretical properties\nof $\\ell _1$-regularized estimates in two important statistical problems in the\ncontext of high-dimensional time series: (a) stochastic regression with\nserially correlated errors and (b) transition matrix estimation in vector\nautoregressive (VAR) models. We derive nonasymptotic upper bounds on the\nestimation errors of the regularized estimates and establish that consistent\nestimation under high-dimensional scaling is possible via\n$\\ell_1$-regularization for a large class of stable processes under sparsity\nconstraints. A key technical contribution of the work is to introduce a measure\nof stability for stationary processes using their spectral properties that\nprovides insight into the effect of dependence on the accuracy of the\nregularized estimates. With this proposed stability measure, we establish some\nuseful deviation bounds for dependent data, which can be used to study several\nimportant regularized estimates in a time series setting.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 16:13:31 GMT"}, {"version": "v2", "created": "Wed, 7 Jan 2015 16:30:13 GMT"}, {"version": "v3", "created": "Thu, 30 Jul 2015 09:19:14 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Basu", "Sumanta", ""], ["Michailidis", "George", ""]]}, {"id": "1311.4196", "submitter": "Andr\\'e Can\\c{c}ado", "authors": "Andr\\'e L. F. Can\\c{c}ado, Cibele Q. da-Silva and Michel F. da Silva", "title": "A spatial scan statistic for zero-inflated Poisson process", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scan statistic is widely used in spatial cluster detection applications\nof inhomogeneous Poisson processes. However, real data may present substantial\ndeparture from the underlying Poisson process. One of the possible departures\nhas to do with zero excess. Some studies point out that when applied to data\nwith excess zeros, the spatial scan statistic may produce biased inferences. In\nthis work, we develop a closed-form scan statistic for cluster detection of\nspatial zero-inflated count data. We apply our methodology to simulated and\nreal data. Our simulations revealed that the Scan-Poisson statistic steadily\ndeteriorates as the number of zeros increases, producing biased inferences. On\nthe other hand, our proposed Scan-ZIP and Scan-ZIP+EM statistics are, most of\nthe time, either superior or comparable to the Scan-Poisson statistic.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 18:25:45 GMT"}], "update_date": "2013-11-19", "authors_parsed": [["Can\u00e7ado", "Andr\u00e9 L. F.", ""], ["da-Silva", "Cibele Q.", ""], ["da Silva", "Michel F.", ""]]}, {"id": "1311.4210", "submitter": "Chen Yue", "authors": "Chen Yue, Shaojie Chen, Haris I. Sair, Raag Airan, Brian S. Caffo", "title": "Estimating a graphical intra-class correlation coefficient (GICC) using\n  multivariate probit-linear mixed models", "comments": "14 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data reproducibility is a critical issue in all scientific experiments. In\nthis manuscript, we consider the problem of quantifying the reproducibility of\ngraphical measurements. We generalize the concept of image intra-class\ncorrelation coefficient (I2C2) and propose the concept of the graphical\nintra-class correlation coefficient (GICC) for such purpose. The concept of\nGICC is based on multivariate probit-linear mixed effect models. We will\npresent a Markov Chain EM (MCEM) algorithm for estimating the GICC. Simulations\nresults with varied settings are demonstrated and our method is applied to the\nKIRBY21 test-retest dataset.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 20:03:51 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2013 04:16:44 GMT"}], "update_date": "2013-11-20", "authors_parsed": [["Yue", "Chen", ""], ["Chen", "Shaojie", ""], ["Sair", "Haris I.", ""], ["Airan", "Raag", ""], ["Caffo", "Brian S.", ""]]}, {"id": "1311.4390", "submitter": "Uwe Saint-Mont", "authors": "Uwe Saint-Mont", "title": "Randomization does not help much, comparability does", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0132102", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following Fisher, it is widely believed that randomization \"relieves the\nexperimenter from the anxiety of considering innumerable causes by which the\ndata may be disturbed.\" In particular, it is said to control for known and\nunknown nuisance factors that may considerably challenge the validity of a\nresult. Looking for quantitative advice, we study a number of straightforward,\nmathematically simple models. However, they all demonstrate that the optimism\nwith respect to randomization is wishful thinking rather than based on fact. In\nsmall to medium-sized samples, random allocation of units to treatments\ntypically yields a considerable imbalance between the groups, i.e., confounding\ndue to randomization is the rule rather than the exception.\n  In the second part of this contribution, we extend the reasoning to a number\nof traditional arguments for and against randomization. This discussion is\nrather non-technical, and at times even \"foundational\" (Frequentist vs.\nBayesian). However, its result turns out to be quite similar. While\nrandomization's contribution remains questionable, comparability contributes\nmuch to a compelling conclusion. Summing up, classical experimentation based on\nsound background theory and the systematic construction of exchangeable groups\nseems to be advisable.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 14:18:23 GMT"}, {"version": "v2", "created": "Mon, 27 Oct 2014 08:14:14 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Saint-Mont", "Uwe", ""]]}, {"id": "1311.4482", "submitter": "George Karabatsos Ph.D.", "authors": "George Karabatsos and Stephen G. Walker", "title": "A Bayesian Nonparametric Causal Model for Regression Discontinuity\n  Designs", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For non-randomized studies, the regression discontinuity design (RDD) can be\nused to identify and estimate causal effects from a \"locally-randomized\"\nsubgroup of subjects, under relatively mild conditions. However, current models\nfocus causal inferences on the impact of the treatment (versus non-treatment)\nvariable on the mean of the dependent variable, via linear regression. For\nRDDs, we propose a flexible Bayesian nonparametric regression model that can\nprovide accurate estimates of causal effects, in terms of the predictive mean,\nvariance, quantile, probability density, distribution function, or any other\nchosen function of the outcome variable. We illustrate the model through the\nanalysis of two real educational data sets, involving (resp.) a sharp RDD and a\nfuzzy RDD.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 18:34:01 GMT"}, {"version": "v2", "created": "Tue, 19 Nov 2013 15:41:05 GMT"}, {"version": "v3", "created": "Mon, 23 Jun 2014 15:36:48 GMT"}, {"version": "v4", "created": "Wed, 11 Feb 2015 18:28:17 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Karabatsos", "George", ""], ["Walker", "Stephen G.", ""]]}, {"id": "1311.4555", "submitter": "Stefan Wager", "authors": "Stefan Wager, Trevor Hastie, and Bradley Efron", "title": "Confidence Intervals for Random Forests: The Jackknife and the\n  Infinitesimal Jackknife", "comments": "To appear in Journal of Machine Learning Research (JMLR)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the variability of predictions made by bagged learners and random\nforests, and show how to estimate standard errors for these methods. Our work\nbuilds on variance estimates for bagging proposed by Efron (1992, 2012) that\nare based on the jackknife and the infinitesimal jackknife (IJ). In practice,\nbagged predictors are computed using a finite number B of bootstrap replicates,\nand working with a large B can be computationally expensive. Direct\napplications of jackknife and IJ estimators to bagging require B on the order\nof n^{1.5} bootstrap replicates to converge, where n is the size of the\ntraining set. We propose improved versions that only require B on the order of\nn replicates. Moreover, we show that the IJ estimator requires 1.7 times less\nbootstrap replicates than the jackknife to achieve a given accuracy. Finally,\nwe study the sampling distributions of the jackknife and IJ variance estimates\nthemselves. We illustrate our findings with multiple experiments and simulation\nstudies.\n", "versions": [{"version": "v1", "created": "Mon, 18 Nov 2013 21:03:31 GMT"}, {"version": "v2", "created": "Sat, 29 Mar 2014 00:06:16 GMT"}], "update_date": "2014-04-01", "authors_parsed": [["Wager", "Stefan", ""], ["Hastie", "Trevor", ""], ["Efron", "Bradley", ""]]}, {"id": "1311.5052", "submitter": "Simon Tindemans", "authors": "Simon H. Tindemans, Goran Strbac", "title": "Robust estimation of risks from small samples", "comments": "13 pages, 3 figures; supplementary information provided. A revised\n  version of this manuscript has been accepted for publication in Philosophical\n  Transactions of the Royal Society A: Mathematical, Physical and Engineering\n  Sciences", "journal-ref": "Phil. Trans. R. Soc. A 2017 375 20160299", "doi": "10.1098/rsta.2016.0299", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven risk analysis involves the inference of probability distributions\nfrom measured or simulated data. In the case of a highly reliable system, such\nas the electricity grid, the amount of relevant data is often exceedingly\nlimited, but the impact of estimation errors may be very large. This paper\npresents a robust nonparametric Bayesian method to infer possible underlying\ndistributions. The method obtains rigorous error bounds even for small samples\ntaken from ill-behaved distributions. The approach taken has a natural\ninterpretation in terms of the intervals between ordered observations, where\nallocation of probability mass across intervals is well-specified, but the\nlocation of that mass within each interval is unconstrained. This formulation\ngives rise to a straightforward computational resampling method: Bayesian\nInterval Sampling. In a comparison with common alternative approaches, it is\nshown to satisfy strict error bounds even for ill-behaved distributions.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 13:46:31 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2016 21:42:04 GMT"}, {"version": "v3", "created": "Fri, 5 May 2017 14:44:31 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Tindemans", "Simon H.", ""], ["Strbac", "Goran", ""]]}, {"id": "1311.5249", "submitter": "Paul von Hippel", "authors": "Paul von Hippel, Jamie Lynch", "title": "Efficiency Gains from Using Auxiliary Variables in Imputation", "comments": "10 pages, 2 tables, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imputation models sometimes use auxiliary variables that, though not part of\nthe planned analysis, can improve the accuracy of imputed values and the\nefficiency of point estimates. A recent article, using evidence from\nsimulations, argued that the use of auxiliary variables in imputation did not\nimprove efficiency. We review the simulation results and find that the use of\nauxiliary variables did improve efficiency; under some conditions the\nefficiency gain was equivalent to increasing the sample size by a quarter. We\ngive an example from our own research where the efficiency gained from\nauxiliary variables was equivalent to increasing the sample size by three\nquarters, and pushed some estimates from statistical insignificance to\nsignificance. For auxiliary variables to make a difference, there must be a lot\nof missing data, some estimates must be near the border of significance, and\nthe auxiliary variables must be excellent predictors of the missing values.\n", "versions": [{"version": "v1", "created": "Wed, 20 Nov 2013 22:21:47 GMT"}], "update_date": "2013-11-22", "authors_parsed": [["von Hippel", "Paul", ""], ["Lynch", "Jamie", ""]]}, {"id": "1311.5274", "submitter": "Stephen Reid", "authors": "Stephen Reid, Robert Tibshirani and Jerome Friedman", "title": "A Study of Error Variance Estimation in Lasso Regression", "comments": "30 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance estimation in the linear model when $p > n$ is a difficult problem.\nStandard least squares estimation techniques do not apply. Several variance\nestimators have been proposed in the literature, all with accompanying\nasymptotic results proving consistency and asymptotic normality under a variety\nof assumptions.\n  It is found, however, that most of these estimators suffer large biases in\nfinite samples when true underlying signals become less sparse with larger per\nelement signal strength. One estimator seems to be largely neglected in the\nliterature: a residual sum of squares based estimator using Lasso coefficients\nwith regularisation parameter selected adaptively (via cross-validation).\n  In this paper, we review several variance estimators and perform a reasonably\nextensive simulation study in an attempt to compare their finite sample\nperformance. It would seem from the results that variance estimators with\nadaptively chosen regularisation parameters perform admirably over a broad\nrange of sparsity and signal strength settings. Finally, some intial\ntheoretical analyses pertaining to these types of estimators are proposed and\ndeveloped.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 00:09:14 GMT"}, {"version": "v2", "created": "Wed, 29 Jan 2014 00:41:42 GMT"}], "update_date": "2014-01-30", "authors_parsed": [["Reid", "Stephen", ""], ["Tibshirani", "Robert", ""], ["Friedman", "Jerome", ""]]}, {"id": "1311.5312", "submitter": "Brian Kent", "authors": "Brian P. Kent, Alessandro Rinaldo, Fang-Cheng Yeh, Timothy Verstynen", "title": "Mapping Topographic Structure in White Matter Pathways with Level Set\n  Trees", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0093344", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fiber tractography on diffusion imaging data offers rich potential for\ndescribing white matter pathways in the human brain, but characterizing the\nspatial organization in these large and complex data sets remains a challenge.\nWe show that level set trees---which provide a concise representation of the\nhierarchical mode structure of probability density functions---offer a\nstatistically-principled framework for visualizing and analyzing topography in\nfiber streamlines. Using diffusion spectrum imaging data collected on\nneurologically healthy controls (N=30), we mapped white matter pathways from\nthe cortex into the striatum using a deterministic tractography algorithm that\nestimates fiber bundles as dimensionless streamlines. Level set trees were used\nfor interactive exploration of patterns in the endpoint distributions of the\nmapped fiber tracks and an efficient segmentation of the tracks that has\nempirical accuracy comparable to standard nonparametric clustering methods. We\nshow that level set trees can also be generalized to model pseudo-density\nfunctions in order to analyze a broader array of data types, including entire\nfiber streamlines. Finally, resampling methods show the reliability of the\nlevel set tree as a descriptive measure of topographic structure, illustrating\nits potential as a statistical descriptor in brain imaging analysis. These\nresults highlight the broad applicability of level set trees for visualizing\nand analyzing high-dimensional data like fiber tractography output.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 04:32:10 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Kent", "Brian P.", ""], ["Rinaldo", "Alessandro", ""], ["Yeh", "Fang-Cheng", ""], ["Verstynen", "Timothy", ""]]}, {"id": "1311.5542", "submitter": "Jianqing Fan", "authors": "Jianqing Fan, Zheng Tracy Ke, Han Liu, Lucy Xia", "title": "QUADRO: A supervised dimension reduction method via Rayleigh quotient\n  optimization", "comments": "Published at http://dx.doi.org/10.1214/14-AOS1307 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2015, Vol. 43, No. 4, 1498-1534", "doi": "10.1214/14-AOS1307", "report-no": "IMS-AOS-AOS1307", "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel Rayleigh quotient based sparse quadratic dimension\nreduction method - named QUADRO (Quadratic Dimension Reduction via Rayleigh\nOptimization) - for analyzing high- dimensional data. Unlike in the linear\nsetting where Rayleigh quotient optimization coincides with classification,\nthese two problems are very different under nonlinear settings. In this paper,\nwe clarify this difference and show that Rayleigh quotient optimization may be\nof independent scientific interests. One major challenge of Rayleigh quotient\noptimization is that the variance of quadratic statistics involves all fourth\ncross-moments of predictors, which are infeasible to compute for\nhigh-dimensional applications and may accumulate too many stochastic errors.\nThis issue is resolved by considering a family of elliptical models. Moreover,\nfor heavy-tail distributions, robust estimates of mean vectors and covariance\nmatrices are employed to guarantee uniform convergence in estimating\nnonpolynomially many parameters, even though only the fourth moments are\nassumed. Methodologically, QUADRO is based on elliptical models which allow us\nto formulate the Rayleigh quotient maximization as a convex optimization\nproblem. Computationally, we propose an efficient linearized augmented\nLagrangian method to solve the constrained optimization problem. Theoretically,\nwe provide explicit rates of convergence in terms of Rayleigh quotient under\nboth Gaussian and general elliptical models. Thorough numerical results on both\nsynthetic and real datasets are also provided to back up our theoretical\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 20:30:45 GMT"}, {"version": "v2", "created": "Tue, 23 Dec 2014 09:24:44 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2015 13:14:18 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Fan", "Jianqing", ""], ["Ke", "Zheng Tracy", ""], ["Liu", "Han", ""], ["Xia", "Lucy", ""]]}, {"id": "1311.5604", "submitter": "Jinguo Gong", "authors": "Jinguo Gong, Yadong Li, Liang Peng and Qiwei Yao", "title": "Estimation of Extreme Quantiles for Functions of Dependent Random\n  Variables", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method for estimating the extreme quantiles for a function\nof several dependent random variables. In contrast to the conventional approach\nbased on extreme value theory, we do not impose the condition that the tail of\nthe underlying distribution admits an approximate parametric form, and,\nfurthermore, our estimation makes use of the full observed data. The proposed\nmethod is semiparametric as no parametric forms are assumed on all the marginal\ndistributions. But we select appropriate bivariate copulas to model the joint\ndependence structure by taking the advantage of the recent development in\nconstructing large dimensional vine copulas. Consequently a sample quantile\nresulted from a large bootstrap sample drawn from the fitted joint distribution\nis taken as the estimator for the extreme quantile. This estimator is proved to\nbe consistent. The reliable and robust performance of the proposed method is\nfurther illustrated by simulation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Nov 2013 22:40:08 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Gong", "Jinguo", ""], ["Li", "Yadong", ""], ["Peng", "Liang", ""], ["Yao", "Qiwei", ""]]}, {"id": "1311.5625", "submitter": "Haolei Weng", "authors": "Haolei Weng, Yang Feng, Xingye Qiao", "title": "Regularization after retention in ultrahigh dimensional linear\n  regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ultrahigh dimensional setting, independence screening has been both\ntheoretically and empirically proved a useful variable selection framework with\nlow computation cost. In this work, we propose a two-step framework by using\nmarginal information in a different perspective from independence screening. In\nparticular, we retain significant variables rather than screening out\nirrelevant ones. The new method is shown to be model selection consistent in\nthe ultrahigh dimensional linear regression model. To improve the finite sample\nperformance, we then introduce a three-step version and characterize its\nasymptotic behavior. Simulations and real data analysis show advantages of our\nmethod over independence screening and its iterative variants in certain\nregimes.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 00:22:21 GMT"}, {"version": "v2", "created": "Tue, 3 Jun 2014 06:36:15 GMT"}, {"version": "v3", "created": "Thu, 10 Aug 2017 14:46:26 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Weng", "Haolei", ""], ["Feng", "Yang", ""], ["Qiao", "Xingye", ""]]}, {"id": "1311.5655", "submitter": "Nanny Wermuth", "authors": "N. Wermuth G.M. Marchetti P. Zwiernik", "title": "Binary distributions of concentric rings", "comments": "12 pages, 1 figure, 8 tables", "journal-ref": "Journal of Multivariate Analysis 130 (2014) 252--260", "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce families of jointly symmetric, binary distributions that are\ngenerated over directed star graphs whose nodes represent variables and whose\nedges indicate positive dependences. The families are parametrized in terms of\na single parameter. It is an outstanding feature of these distributions that\njoint probabilities relate to evenly-spaced concentric rings. Kronecker product\ncharacterizations make them computationally attractive for a large number of\nvariables. We study the behaviour of different measures of dependence and\nderive maximum likelihood estimates when all nodes are observed and when the\ninner node is hidden.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 05:18:51 GMT"}, {"version": "v2", "created": "Tue, 29 Jul 2014 17:02:19 GMT"}], "update_date": "2014-07-30", "authors_parsed": [["Zwiernik", "N. Wermuth G. M. Marchetti P.", ""]]}, {"id": "1311.5727", "submitter": "Jonathan Jaeger", "authors": "Gianluca Frasso, Jonathan Jaeger, Philippe Lambert", "title": "Estimation and approximation in multidimensional dynamics", "comments": "17 pages, 5 figures. http://hdl.handle.net/2268/158563", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Differential equations (DEs) are commonly used to describe dynamic systems\nevolving in one (ordinary differential equations or ODEs) or in more than one\ndimensions (partial differential equations or PDEs). In real data applications\nthe parameters involved in the DE models are usually unknown and need to be\nestimated from the available measurements together with the state function. In\nthis paper, we present frequentist and Bayesian approaches for the joint\nestimation of the parameters and of the state functions involved in PDEs. We\nalso propose two strategies to include differential (initial and/or boundary)\nconditions in the estimation procedure. We evaluate the performances of the\nproposed strategy on simulated and real data applications.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 12:02:54 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Frasso", "Gianluca", ""], ["Jaeger", "Jonathan", ""], ["Lambert", "Philippe", ""]]}, {"id": "1311.5769", "submitter": "Chris Greenman", "authors": "Roshan A, Jones PH and Greenman CD", "title": "An Exact, Time-Independent Approach to Clone Size Distributions in\n  Normal and Mutated Cells", "comments": "18 Pages; 6 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological tools such as genetic lineage tracing, three dimensional confocal\nmicroscopy and next generation DNA sequencing are providing new ways to\nquantify the distribution of clones of normal and mutated cells.\nPopulation-wide clone size distributions in vivo are complicated by multiple\ncell types, and overlapping birth and death processes. This has led to the\nincreased need for mathematically informed models to understand their\nbiological significance. Standard approaches usually require knowledge of\nclonal age. We show that modelling on clone size independent of time is an\nalternative method that offers certain analytical advantages; it can help\nparameterize these models, and obtain distributions for counts of mutated or\nproliferating cells, for example. When applied to a general birth-death process\ncommon in epithelial progenitors this takes the form of a gamblers ruin\nproblem, the solution of which relates to counting Motzkin lattice paths.\nApplying this approach to mutational processes, an alternative, exact,\nformulation of the classic Luria Delbruck problem emerges. This approach can be\nextended beyond neutral models of mutant clonal evolution, and also describe\nsome distributions relating to sub-clones within a tumour. The approaches above\nare generally applicable to any Markovian branching process where the dynamics\nof different \"coloured\" daughter branches are of interest.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 14:51:39 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["A", "Roshan", ""], ["PH", "Jones", ""], ["CD", "Greenman", ""]]}, {"id": "1311.5777", "submitter": "Paul Jenkins", "authors": "Paul A. Jenkins", "title": "Exact simulation of the sample paths of a diffusion with a finite\n  entrance boundary", "comments": "19 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.PR stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusion processes arise in many fields, and so simulating the path of a\ndiffusion is an important problem. It is usually necessary to make some sort of\napproximation via model-discretization, but a recently introduced class of\nalgorithms, known as the exact algorithm and based on retrospective rejection\nsampling ideas, obviate the need for such discretization. In this paper I\nextend the exact algorithm to apply to a class of diffusions with a finite\nentrance boundary. The key innovation is that for these models the Bessel\nprocess is a more suitable candidate process than the more usually chosen\nBrownian motion. The algorithm is illustrated by an application to a general\ndiffusion model of population growth, where it simulates paths efficiently,\nwhile previous algorithms are impracticable.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 15:16:15 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Jenkins", "Paul A.", ""]]}, {"id": "1311.5828", "submitter": "Gerard Keogh Dr.", "authors": "Gerard Keogh", "title": "The Splice Bootstrap", "comments": "16 pages and 2 large appendix tables with simulation results", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new bootstrap method to compute predictive intervals\nfor nonlinear autoregressive time series model forecast. This method we call\nthe splice boobstrap as it involves splicing the last p values of a given\nseries to a suitably simulated series. This ensures that each simulated series\nwill have the same set of p time series values in common, a necessary\nrequirement for computing conditional predictive intervals. Using simulation\nstudies we show the methods gives 90% intervals intervals that are similar to\nthose expected from theory for simple linear and SETAR model driven by normal\nand non-normal noise. Furthermore, we apply the method to some economic data\nand demonstrate the intervals compare favourably with cross-validation based\nintervals.\n", "versions": [{"version": "v1", "created": "Fri, 22 Nov 2013 17:43:41 GMT"}], "update_date": "2013-11-25", "authors_parsed": [["Keogh", "Gerard", ""]]}, {"id": "1311.6000", "submitter": "Christian P. Robert", "authors": "Jeong Eun Lee (Auckland University of Technology) and Christian P.\n  Robert (Universite Paris-Dauphine and University of Warwick)", "title": "Importance sampling schemes for evidence approximation in mixture models", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The marginal likelihood is a central tool for drawing Bayesian inference\nabout the number of components in mixture models. It is often approximated\nsince the exact form is unavailable. A bias in the approximation may be due to\nan incomplete exploration by a simulated Markov chain (e.g., a Gibbs sequence)\nof the collection of posterior modes, a phenomenon also known as lack of label\nswitching, as all possible label permutations must be simulated by a chain in\norder to converge and hence overcome the bias. In an importance sampling\napproach, imposing label switching to the importance function results in an\nexponential increase of the computational cost with the number of components.\nIn this paper, two importance sampling schemes are proposed through choices for\nthe importance function; a MLE proposal and a Rao-Blackwellised importance\nfunction. The second scheme is called dual importance sampling. We demonstrate\nthat this dual importance sampling is a valid estimator of the evidence and\nmoreover show that the statistical efficiency of estimates increases. To reduce\nthe induced high demand in computation, the original importance function is\napproximated but a suitable approximation can produce an estimate with the same\nprecision and with reduced computational workload.\n", "versions": [{"version": "v1", "created": "Sat, 23 Nov 2013 14:01:31 GMT"}, {"version": "v2", "created": "Thu, 13 Nov 2014 11:02:54 GMT"}], "update_date": "2014-11-14", "authors_parsed": [["Lee", "Jeong Eun", "", "Auckland University of Technology"], ["Robert", "Christian P.", "", "Universite Paris-Dauphine and University of Warwick"]]}, {"id": "1311.6186", "submitter": "Zhao Ren", "authors": "Mengjie Chen, Chao Gao, Zhao Ren, Harrison H. Zhou", "title": "Sparse CCA via Precision Adjusted Iterative Thresholding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse Canonical Correlation Analysis (CCA) has received considerable\nattention in high-dimensional data analysis to study the relationship between\ntwo sets of random variables. However, there has been remarkably little\ntheoretical statistical foundation on sparse CCA in high-dimensional settings\ndespite active methodological and applied research activities. In this paper,\nwe introduce an elementary sufficient and necessary characterization such that\nthe solution of CCA is indeed sparse, propose a computationally efficient\nprocedure, called CAPIT, to estimate the canonical directions, and show that\nthe procedure is rate-optimal under various assumptions on nuisance parameters.\nThe procedure is applied to a breast cancer dataset from The Cancer Genome\nAtlas project. We identify methylation probes that are associated with genes,\nwhich have been previously characterized as prognosis signatures of the\nmetastasis of breast cancer.\n", "versions": [{"version": "v1", "created": "Sun, 24 Nov 2013 23:50:20 GMT"}], "update_date": "2013-11-26", "authors_parsed": [["Chen", "Mengjie", ""], ["Gao", "Chao", ""], ["Ren", "Zhao", ""], ["Zhou", "Harrison H.", ""]]}, {"id": "1311.6238", "submitter": "Jason D. Lee", "authors": "Jason D. Lee, Dennis L. Sun, Yuekai Sun, Jonathan E. Taylor", "title": "Exact post-selection inference, with application to the lasso", "comments": "Published at http://dx.doi.org/10.1214/15-AOS1371 in the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2016, Vol. 44, No. 3, 907-927", "doi": "10.1214/15-AOS1371", "report-no": "IMS-AOS-AOS1371", "categories": "math.ST stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a general approach to valid inference after model selection. At\nthe core of our framework is a result that characterizes the distribution of a\npost-selection estimator conditioned on the selection event. We specialize the\napproach to model selection by the lasso to form valid confidence intervals for\nthe selected coefficients and test whether all relevant variables have been\nincluded in the model.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 09:01:08 GMT"}, {"version": "v2", "created": "Wed, 5 Feb 2014 20:15:48 GMT"}, {"version": "v3", "created": "Thu, 13 Feb 2014 00:56:16 GMT"}, {"version": "v4", "created": "Fri, 14 Feb 2014 03:06:38 GMT"}, {"version": "v5", "created": "Wed, 14 Jan 2015 02:52:11 GMT"}, {"version": "v6", "created": "Mon, 10 Aug 2015 23:57:55 GMT"}, {"version": "v7", "created": "Tue, 16 Feb 2016 09:02:58 GMT"}, {"version": "v8", "created": "Tue, 3 May 2016 07:48:17 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Lee", "Jason D.", ""], ["Sun", "Dennis L.", ""], ["Sun", "Yuekai", ""], ["Taylor", "Jonathan E.", ""]]}, {"id": "1311.6311", "submitter": "Ioannis Kosmidis", "authors": "Ioannis Kosmidis", "title": "Bias in parametric estimation: reduction and useful side-effects", "comments": null, "journal-ref": "WIREs.Compu.Stat. 6 (2014) 185-196", "doi": "10.1002/wics.1296", "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bias of an estimator is defined as the difference of its expected value\nfrom the parameter to be estimated, where the expectation is with respect to\nthe model. Loosely speaking, small bias reflects the desire that if an\nexperiment is repeated indefinitely then the average of all the resultant\nestimates will be close to the parameter value that is estimated. The current\npaper is a review of the still-expanding repository of methods that have been\ndeveloped to reduce bias in the estimation of parametric models. The review\nprovides a unifying framework where all those methods are seen as attempts to\napproximate the solution of a simple estimating equation. Of particular focus\nis the maximum likelihood estimator, which despite being asymptotically\nunbiased under the usual regularity conditions, has finite-sample bias that can\nresult in significant loss of performance of standard inferential procedures.\nAn informal comparison of the methods is made revealing some useful practical\nside-effects in the estimation of popular models in practice including: i)\nshrinkage of the estimators in binomial and multinomial regression models that\nguarantees finiteness even in cases of data separation where the maximum\nlikelihood estimator is infinite, and ii) inferential benefits for models that\nrequire the estimation of dispersion or precision parameters.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 14:10:49 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Kosmidis", "Ioannis", ""]]}, {"id": "1311.6360", "submitter": "Dennis Wei", "authors": "Dennis Wei and Alfred O. Hero III", "title": "Performance Guarantees for Adaptive Estimation of Sparse Signals", "comments": "34 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT math.IT stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies adaptive sensing for estimating the nonzero amplitudes of\na sparse signal with the aim of providing analytical guarantees on the\nperformance gain due to adaptive resource allocation. We consider a previously\nproposed optimal two-stage policy for allocating sensing resources. For\npositive powers q, we derive tight upper bounds on the mean qth-power error\nresulting from the optimal two-stage policy and corresponding lower bounds on\nthe improvement over non-adaptive uniform sensing. It is shown that the\nadaptation gain is related to the detectability of nonzero signal components as\ncharacterized by Chernoff coefficients, thus quantifying analytically the\ndependence on the sparsity level of the signal, the signal-to-noise ratio, and\nthe sensing resource budget. For fixed sparsity levels and increasing\nsignal-to-noise ratio or sensing budget, we obtain the rate of convergence to\noracle performance and the rate at which the fraction of resources spent on the\nfirst exploratory stage decreases to zero. For a vanishing fraction of nonzero\ncomponents, the gain increases without bound as a function of signal-to-noise\nratio and sensing budget. Numerical simulations demonstrate that the bounds on\nadaptation gain are quite tight in non-asymptotic regimes as well.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 16:43:26 GMT"}, {"version": "v2", "created": "Mon, 4 Aug 2014 16:57:37 GMT"}], "update_date": "2014-08-05", "authors_parsed": [["Wei", "Dennis", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1311.6403", "submitter": "Lutz Duembgen", "authors": "Lutz Duembgen and Kaspar Rufibach and Dominic Schuhmacher", "title": "Maximum-Likelihood Estimation of a Log-Concave Density based on Censored\n  Data", "comments": null, "journal-ref": "Electronic Journal of Statistics 8 (2014), 1405-1437", "doi": "10.1214/14-EJS930", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider nonparametric maximum-likelihood estimation of a log-concave\ndensity in case of interval-censored, right-censored and binned data. We allow\nfor the possibility of a subprobability density with an additional mass at\n$+\\infty$, which is estimated simultaneously. The existence of the estimator is\nproved under mild conditions and various theoretical aspects are given, such as\ncertain shape and consistency properties. An EM algorithm is proposed for the\napproximate computation of the estimator and its performance is illustrated in\ntwo examples.\n", "versions": [{"version": "v1", "created": "Mon, 25 Nov 2013 18:47:27 GMT"}, {"version": "v2", "created": "Thu, 28 Nov 2013 16:01:19 GMT"}, {"version": "v3", "created": "Mon, 28 Jul 2014 20:25:55 GMT"}], "update_date": "2014-08-15", "authors_parsed": [["Duembgen", "Lutz", ""], ["Rufibach", "Kaspar", ""], ["Schuhmacher", "Dominic", ""]]}, {"id": "1311.6530", "submitter": "Cristina Tortora Dr", "authors": "Cristina Tortora, Paul D. McNicholas and Ryan P. Browne", "title": "A Mixture of Generalized Hyperbolic Factor Analyzers", "comments": null, "journal-ref": null, "doi": "10.1007/s11634-015-0204-z", "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based clustering imposes a finite mixture modelling structure on data\nfor clustering. Finite mixture models assume that the population is a convex\ncombination of a finite number of densities, the distribution within each\npopulation is a basic assumption of each particular model. Among all\ndistributions that have been tried, the generalized hyperbolic distribution has\nthe advantage that is a generalization of several other methods, such as the\nGaussian distribution, the skew t-distribution, etc. With specific parameters,\nit can represent either a symmetric or a skewed distribution. While its\ninherent flexibility is an advantage in many ways, it means the estimation of\nmore parameters than its special and limiting cases. The aim of this work is to\npropose a mixture of generalized hyperbolic factor analyzers to introduce\nparsimony and extend the method to high dimensional data. This work can be seen\nas an extension of the mixture of factor analyzers model to generalized\nhyperbolic mixtures. The performance of our generalized hyperbolic factor\nanalyzers is illustrated on real data, where it performs favourably compared to\nits Gaussian analogue.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 00:59:26 GMT"}, {"version": "v2", "created": "Mon, 15 Dec 2014 15:03:49 GMT"}, {"version": "v3", "created": "Fri, 22 May 2015 14:19:46 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Tortora", "Cristina", ""], ["McNicholas", "Paul D.", ""], ["Browne", "Ryan P.", ""]]}, {"id": "1311.6732", "submitter": "Daniel Takahashi", "authors": "Andr\\'e Fujita, Daniel Y. Takahashi, Alexandre G. Patriota, Jo\\~ao R.\n  Sato", "title": "A statistical test to identify differences in clustering structures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical inference on functional magnetic resonance imaging (fMRI) data is\nan important task in brain imaging. One major hypothesis is that the presence\nor not of a psychiatric disorder can be explained by the differential\nclustering of neurons in the brain. In view of this fact, it is clearly of\ninterest to address the question of whether the properties of the clusters have\nchanged between groups of patients and controls. The normal method of\napproaching group differences in brain imaging is to carry out a voxel-wise\nunivariate analysis for a difference between the mean group responses using an\nappropriate test (e.g. a t-test) and to assemble the resulting \"significantly\ndifferent voxels\" into clusters, testing again at cluster level. In this\napproach of course, the primary voxel-level test is blind to any cluster\nstructure. Direct assessments of differences between groups (or reproducibility\nwithin groups) at the cluster level have been rare in brain imaging. For this\nreason, we introduce a novel statistical test called ANOCVA - ANalysis Of\nCluster structure Variability, which statistically tests whether two or more\npopulations are equally clustered using specific features. The proposed method\nallows us to compare the clustering structure of multiple groups\nsimultaneously, and also to identify features that contribute to the\ndifferential clustering. We illustrate the performance of ANOCVA through\nsimulations and an application to an fMRI data set composed of children with\nADHD and controls. Results show that there are several differences in the\nbrain's clustering structure between them, corroborating the hypothesis in the\nliterature. Furthermore, we identified some brain regions previously not\ndescribed, generating new hypothesis to be tested empirically.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 16:38:00 GMT"}], "update_date": "2013-11-27", "authors_parsed": [["Fujita", "Andr\u00e9", ""], ["Takahashi", "Daniel Y.", ""], ["Patriota", "Alexandre G.", ""], ["Sato", "Jo\u00e3o R.", ""]]}, {"id": "1311.6756", "submitter": "Carlo Berzuini Professor", "authors": "Carlo Berzuini (Centre for Biostatistics, University of Manchester),\n  A. Philip Dawid (Statistical Laboratory, Centre for Mathematical Sciences,\n  University of Cambridge)", "title": "Stochastic Mechanistic Interaction", "comments": "28 pages, 4 figures", "journal-ref": "Biometrika (2016) 103 (1): 89-102", "doi": "10.1093/biomet/asv072", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a fully probabilistic formulation of the notion of mechanistic\ninteraction (interaction in some fundamental mechanistic sense) between the\neffects of putative (possibly continuous) causal factors A and B on a binary\noutcome variable Y indicating 'survival' vs 'failure'. We define mechanistic\ninteraction in terms of departure from a generalized 'noisy OR' model, under\nwhich the multiplicative causal effect of A (resp., B) on the probability of\nfailure cannot be enhanced by manipulating B (resp., A). We present conditions\nunder which mechanistic interaction in the above sense can be assessed via\nsimple tests on excess risk or superadditivity, in a possibly retrospective\nregime of observation. These conditions are defined in terms of generalized\nconditional independence relationships (generalised because they may involve\nnon-stochastic 'regime indicators') that can often be checked on a graphical\nrepresentation of the problem. Inference about mechanistic interaction between\ndirect, or path-specific, causal effects can be accommodated in the proposed\nframework. The method is illustrated with the aid of a study in experimental\npsychology.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 17:55:16 GMT"}, {"version": "v2", "created": "Sat, 14 Jun 2014 14:00:20 GMT"}, {"version": "v3", "created": "Wed, 19 Nov 2014 14:43:01 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Berzuini", "Carlo", "", "Centre for Biostatistics, University of Manchester"], ["Dawid", "A. Philip", "", "Statistical Laboratory, Centre for Mathematical Sciences,\n  University of Cambridge"]]}, {"id": "1311.6844", "submitter": "Jelena Markovic", "authors": "Jelena Markovic and Lie Wang", "title": "Estimating the Ratio of Two Functions in a Nonparametric Regression\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to measurement noise, a common problem in in various fields is how to\nestimate the ratio of two functions. We consider this problem of estimating the\nratio of two functions in a nonparametric regression model. Assuming the noise\nis normally distributed, this is equivalent to estimating the ratio of the\nmeans of two normally distributed random variables. We identified a consistent\nestimator that gives the mean squared loss of order $O(1/n)$ ($n$ is the sample\nsize) when conditioned on a highly probable event. We also present our result\napplied to both the real data from EAPS and on simulated data, confirming our\ntheoretical results.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 23:19:35 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Markovic", "Jelena", ""], ["Wang", "Lie", ""]]}, {"id": "1311.6849", "submitter": "Bodhisattva Sen", "authors": "Bodhisattva Sen and Mary Meyer", "title": "Testing against a linear regression model using ideas from\n  shape-restricted estimation", "comments": "38 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A formal likelihood ratio hypothesis test for the validity of a parametric\nregression function is proposed, using a large-dimensional, nonparametric\ndouble cone alternative. For example, the test against a constant function uses\nthe alternative of increasing or decreasing regression functions, and the test\nagainst a linear function uses the convex or concave alternative. The proposed\ntest is exact, unbiased and the critical value is easily computed. The power of\nthe test increases to one as the sample size increases, under very mild\nassumptions -- even when the alternative is mis-specified. That is, the power\nof the test converges to one for any true regression function that deviates (in\na non-degenerate way) from the parametric null hypothesis. We also formulate\ntests for the linear versus partial linear model, and consider the special case\nof the additive model. Simulations show that our procedure behaves well\nconsistently when compared with other methods. Although the alternative fit is\nnon-parametric, no tuning parameters are involved.\n", "versions": [{"version": "v1", "created": "Tue, 26 Nov 2013 23:31:26 GMT"}, {"version": "v2", "created": "Fri, 27 Jun 2014 00:58:41 GMT"}], "update_date": "2014-06-30", "authors_parsed": [["Sen", "Bodhisattva", ""], ["Meyer", "Mary", ""]]}, {"id": "1311.6976", "submitter": "Bjarne {\\O}rum Fruergaard", "authors": "Bjarne {\\O}rum Fruergaard, Toke Jansen Hansen, Lars Kai Hansen", "title": "Dimensionality reduction for click-through rate prediction: Dense versus\n  sparse representation", "comments": "Presented at the Probabilistic Models for Big Data workshop at NIPS\n  2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.LG stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online advertising, display ads are increasingly being placed based on\nreal-time auctions where the advertiser who wins gets to serve the ad. This is\ncalled real-time bidding (RTB). In RTB, auctions have very tight time\nconstraints on the order of 100ms. Therefore mechanisms for bidding\nintelligently such as clickthrough rate prediction need to be sufficiently\nfast. In this work, we propose to use dimensionality reduction of the\nuser-website interaction graph in order to produce simplified features of users\nand websites that can be used as predictors of clickthrough rate. We\ndemonstrate that the Infinite Relational Model (IRM) as a dimensionality\nreduction offers comparable predictive performance to conventional\ndimensionality reduction schemes, while achieving the most economical usage of\nfeatures and fastest computations at run-time. For applications such as\nreal-time bidding, where fast database I/O and few computations are key to\nsuccess, we thus recommend using IRM based features as predictors to exploit\nthe recommender effects from bipartite graphs.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 14:19:21 GMT"}, {"version": "v2", "created": "Tue, 13 May 2014 09:21:28 GMT"}], "update_date": "2014-05-14", "authors_parsed": [["Fruergaard", "Bjarne \u00d8rum", ""], ["Hansen", "Toke Jansen", ""], ["Hansen", "Lars Kai", ""]]}, {"id": "1311.7065", "submitter": "Martin Weidner", "authors": "Ivan Fernandez-Val and Martin Weidner", "title": "Individual and Time Effects in Nonlinear Panel Models with Large N, T", "comments": "84 pages, 10 tables, includes supplementary appendix; Relative to the\n  published version we corrected some expressions in Theorem 4.2 here", "journal-ref": "Journal of Econometrics, Volume 192, Issue 1, May 2016, Pages\n  291-312", "doi": null, "report-no": null, "categories": "stat.ME econ.EM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We derive fixed effects estimators of parameters and average partial effects\nin (possibly dynamic) nonlinear panel data models with individual and time\neffects. They cover logit, probit, ordered probit, Poisson and Tobit models\nthat are important for many empirical applications in micro and macroeconomics.\nOur estimators use analytical and jackknife bias corrections to deal with the\nincidental parameter problem, and are asymptotically unbiased under asymptotic\nsequences where $N/T$ converges to a constant. We develop inference methods and\nshow that they perform well in numerical examples.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 18:37:37 GMT"}, {"version": "v2", "created": "Sun, 13 Jul 2014 15:25:20 GMT"}, {"version": "v3", "created": "Thu, 2 Apr 2015 03:17:11 GMT"}, {"version": "v4", "created": "Mon, 30 Nov 2015 21:49:06 GMT"}, {"version": "v5", "created": "Tue, 18 Dec 2018 15:48:18 GMT"}], "update_date": "2018-12-19", "authors_parsed": [["Fernandez-Val", "Ivan", ""], ["Weidner", "Martin", ""]]}, {"id": "1311.7122", "submitter": "Yingying  Wei", "authors": "Yingying Wei and Hongkai Ji", "title": "A Survival Copula Mixture Model for Comparing Two Genomic Rank Lists", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analyses of high-throughput genomic data often lead to ranked lists of\ngenomic loci. How to characterize concordant signals between two rank lists is\na common problem with many applications. One example is measuring the\nreproducibility between two replicate experiments. Another is to characterize\nthe interaction and co-binding between two transcription factors (TF) based on\nthe overlap between their binding sites. As an exploratory tool, the simple\nVenn diagram approach can be used to show the common loci between two lists.\nHowever, this approach does not account for changes in overlap with decreasing\nranks, which may contain useful information for studying similarities or\ndissimilarities of the two lists. The recently proposed irreproducible\ndiscovery rate (IDR) approach compares two rank lists using a copula mixture\nmodel. This model considers the rank correlation between two lists. However, it\nonly analyzes the genomic loci that appear in both lists, thereby only\nmeasuring signal concordance in the overlapping set of the two lists. When two\nlists have little overlap but loci in their overlapping set have high\nconcordance in terms of rank, the original IDR approach may misleadingly claim\nthat the two rank lists are highly reproducible when they are indeed not. In\nthis article, we propose to address the various issues above by translating the\nproblem into a bivariate survival problem. A survival copula mixture model is\ndeveloped to characterize concordant signals in two rank lists. The\neffectiveness of this approach is demonstrated using both simulations and real\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 27 Nov 2013 20:48:03 GMT"}], "update_date": "2013-11-28", "authors_parsed": [["Wei", "Yingying", ""], ["Ji", "Hongkai", ""]]}, {"id": "1311.7320", "submitter": "Maurizio Filippone", "authors": "Maurizio Filippone", "title": "Bayesian Inference for Gaussian Process Classifiers with Annealing and\n  Pseudo-Marginal MCMC", "comments": "6 pages, 2 figures, 1 table - to appear in ICPR 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods have revolutionized the fields of pattern recognition and\nmachine learning. Their success, however, critically depends on the choice of\nkernel parameters. Using Gaussian process (GP) classification as a working\nexample, this paper focuses on Bayesian inference of covariance (kernel)\nparameters using Markov chain Monte Carlo (MCMC) methods. The motivation is\nthat, compared to standard optimization of kernel parameters, they have been\nsystematically demonstrated to be superior in quantifying uncertainty in\npredictions. Recently, the Pseudo-Marginal MCMC approach has been proposed as a\npractical inference tool for GP models. In particular, it amounts in replacing\nthe analytically intractable marginal likelihood by an unbiased estimate\nobtainable by approximate methods and importance sampling. After discussing the\npotential drawbacks in employing importance sampling, this paper proposes the\napplication of annealed importance sampling. The results empirically\ndemonstrate that compared to importance sampling, annealed importance sampling\ncan reduce the variance of the estimate of the marginal likelihood\nexponentially in the number of data at a computational cost that scales only\npolynomially. The results on real data demonstrate that employing annealed\nimportance sampling in the Pseudo-Marginal MCMC approach represents a step\nforward in the development of fully automated exact inference engines for GP\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 28 Nov 2013 14:03:17 GMT"}, {"version": "v2", "created": "Mon, 26 May 2014 14:59:29 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Filippone", "Maurizio", ""]]}, {"id": "1311.7455", "submitter": "Jian Huang", "authors": "Jian Huang, Shuangge Ma, Cun-Hui Zhang and Yong Zhou", "title": "Semi-Penalized Inference with Direct False Discovery Rate Control in\n  High-Dimensions", "comments": "35 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method, semi-penalized inference with direct false discovery\nrate control (SPIDR), for variable selection and confidence interval\nconstruction in high-dimensional linear regression. SPIDR first uses a\nsemi-penalized approach to constructing estimators of the regression\ncoefficients. We show that the SPIDR estimator is ideal in the sense that it\nequals an ideal least squares estimator with high probability under a sparsity\nand other suitable conditions. Consequently, the SPIDR estimator is\nasymptotically normal. Based on this distributional result, SPIDR determines\nthe selection rule by directly controlling false discovery rate. This provides\nan explicit assessment of the selection error. This also naturally leads to\nconfidence intervals for the selected coefficients with a proper confidence\nstatement. We conduct simulation studies to evaluate its finite sample\nperformance and demonstrate its application on a breast cancer gene expression\ndata set. Our simulation studies and data example suggest that SPIDR is a\nuseful method for high-dimensional statistical inference in practice.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 01:28:52 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Huang", "Jian", ""], ["Ma", "Shuangge", ""], ["Zhang", "Cun-Hui", ""], ["Zhou", "Yong", ""]]}, {"id": "1311.7513", "submitter": "Philip Dawid", "authors": "Philip Dawid, Monica Musio, Stephen E. Fienberg", "title": "From Statistical Evidence to Evidence of Causality", "comments": "27 pages, 1 table, 9 figures. This is a fairly substantial revision\n  of version 1", "journal-ref": "Bayesian Analysis, Volume 11, Number 3 (2016), 725-752", "doi": "10.1214/15-BA968", "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While statisticians and quantitative social scientists typically study the\n\"effects of causes\" (EoC), Lawyers and the Courts are more concerned with\nunderstanding the \"causes of effects\" (CoE). EoC can be addressed using\nexperimental design and statistical analysis, but it is less clear how to\nincorporate statistical or epidemiological evidence into CoE reasoning, as\nmight be required for a case at Law. Some form of counterfactual reasoning,\nsuch as the \"potential outcomes\" approach championed by Rubin, appears\nunavoidable, but this typically yields \"answers\" that are sensitive to\narbitrary and untestable assumptions. We must therefore recognise that a CoE\nquestion simply might not have a well-determined answer. It is nevertheless\npossible to use statistical data to set bounds within which any answer must\nlie. With less than perfect data these bounds will themselves be uncertain,\nleading to a compounding of different kinds of uncertainty. Still further care\nis required in the presence of possible confounding factors. In addition, even\nidentifying the relevant \"counterfactual contrast\" may be a matter of Policy as\nmuch as of Science. Defining the question is as non-trivial a task as finding a\nroute towards an answer. This paper develops some technical elaborations of\nthese philosophical points, and illustrates them with an analysis of a case\nstudy in child protection.\n  Keywords: benfluorex, causes of effects, counterfactual, child protection,\neffects of causes, Fre'chet bound, potential outcome, probability of causation\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 10:17:54 GMT"}, {"version": "v2", "created": "Sat, 25 Oct 2014 20:39:36 GMT"}], "update_date": "2020-04-28", "authors_parsed": [["Dawid", "Philip", ""], ["Musio", "Monica", ""], ["Fienberg", "Stephen E.", ""]]}, {"id": "1311.7582", "submitter": "Antonio Canale", "authors": "Antonio Canale and Bruno Scarpa", "title": "Bayesian nonparametric location-scale-shape mixtures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete mixture models are one of the most successful approaches for density\nestimation. Under a Bayesian nonparametric framework, Dirichlet process\nlocation-scale mixture of Gaussian kernels is the golden standard, both having\nnice theoretical properties and computational tractability. In this paper we\nexplore the use of the skew-normal kernel, which can naturally accommodate\nseveral degrees of skewness by the use of a third parameter. The choice of this\nkernel function allows us to formulate nonparametric location-scale-shape\nmixture prior with large support and good performance in different\napplications. Asymptotically, we show that this modelling framework is\nconsistent in frequentist sense. Efficient Gibbs sampling algorithms are also\ndiscussed and the performance of the methods are tested through simulations and\napplications to galaxy velocity and fertility data. Extensions to accommodate\ndiscrete data are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 14:50:36 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Canale", "Antonio", ""], ["Scarpa", "Bruno", ""]]}, {"id": "1311.7650", "submitter": "Mikhail Langovoy", "authors": "Mikhail Langovoy and Michael Habeck and Bernhard Schoelkopf", "title": "Adaptive nonparametric detection in cryo-electron microscopy", "comments": "Proceedings of the 58-th World Statistical Congress (2011)", "journal-ref": "Proceedings of the 58-th World Statistical Congress (2011),\n  Session: High Dimensional Data, pp. 4456 - 4461", "doi": null, "report-no": null, "categories": "stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cryo-electron microscopy (cryo-EM) is an emerging experimental method to\ncharacterize the structure of large biomolecular assemblies. Single particle\ncryo-EM records 2D images (so-called micrographs) of projections of the\nthree-dimensional particle, which need to be processed to obtain the\nthree-dimensional reconstruction. A crucial step in the reconstruction process\nis particle picking which involves detection of particles in noisy 2D\nmicrographs with low signal-to-noise ratios of typically 1:10 or even lower.\nTypically, each picture contains a large number of particles, and particles\nhave unknown irregular and nonconvex shapes.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 18:05:50 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langovoy", "Mikhail", ""], ["Habeck", "Michael", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1311.7656", "submitter": "Mikhail Langovoy", "authors": "Mikhail Langovoy and Suvrit Sra", "title": "Statistical estimation for optimization problems on graphs", "comments": "Paper for the NIPS Workshop on Discrete Optimization for Machine\n  Learning (DISCML) (2011): Uncertainty, Generalization and Feedback", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.DM math.OC stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large graphs abound in machine learning, data mining, and several related\nareas. A useful step towards analyzing such graphs is that of obtaining certain\nsummary statistics - e.g., or the expected length of a shortest path between\ntwo nodes, or the expected weight of a minimum spanning tree of the graph, etc.\nThese statistics provide insight into the structure of a graph, and they can\nhelp predict global properties of a graph. Motivated thus, we propose to study\nstatistical properties of structured subgraphs (of a given graph), in\nparticular, to estimate the expected objective function value of a\ncombinatorial optimization problem over these subgraphs. The general task is\nvery difficult, if not unsolvable; so for concreteness we describe a more\nspecific statistical estimation problem based on spanning trees. We hope that\nour position paper encourages others to also study other types of graphical\nstructures for which one can prove nontrivial statistical estimates.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 18:21:13 GMT"}], "update_date": "2013-12-02", "authors_parsed": [["Langovoy", "Mikhail", ""], ["Sra", "Suvrit", ""]]}]