[{"id": "1004.0209", "submitter": "Genevera Allen", "authors": "Genevera I. Allen and Robert Tibshirani", "title": "Inference with Transposable Data: Modeling the Effects of Row and Column\n  Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of large-scale inference on the row or column\nvariables of data in the form of a matrix. Often this data is transposable,\nmeaning that both the row variables and column variables are of potential\ninterest. An example of this scenario is detecting significant genes in\nmicroarrays when the samples or arrays may be dependent due to underlying\nrelationships. We study the effect of both row and column correlations on\ncommonly used test-statistics, null distributions, and multiple testing\nprocedures, by explicitly modeling the covariances with the matrix-variate\nnormal distribution. Using this model, we give both theoretical and simulation\nresults revealing the problems associated with using standard statistical\nmethodology on transposable data. We solve these problems by estimating the row\nand column covariances simultaneously, with transposable regularized covariance\nmodels, and de-correlating or sphering the data as a pre-processing step. Under\nreasonable assumptions, our method gives test statistics that follow the scaled\ntheoretical null distribution and are approximately independent. Simulations\nbased on various models with structured and observed covariances from real\nmicroarray data reveal that our method offers substantial improvements in two\nareas: 1) increased statistical power and 2) correct estimation of false\ndiscovery rates.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2010 19:19:31 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Allen", "Genevera I.", ""], ["Tibshirani", "Robert", ""]]}, {"id": "1004.0482", "submitter": "Zhen Pang", "authors": "Zhen Pang and Liugen Xue", "title": "Estimation for Single-Index mixed models with Longitudinal data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a single-index mixed model with longitudinal data.\nA new set of estimating equations is proposed to estimate the single-index\ncoefficient. The link function is estimated by using the local linear\nsmoothing. Asymptotic normality is established for the proposed estimators.\nAlso, the estimator of the link function achieves optimal convergence rates;\nand the estimators of variance components have root-$n$ consistency. These\nresults facilitate the construction of confidence regions/intervals and\nhypothesis testing for the parameters of interest. Some simulations and an\napplication to real data are included.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2010 02:36:16 GMT"}], "update_date": "2010-04-06", "authors_parsed": [["Pang", "Zhen", ""], ["Xue", "Liugen", ""]]}, {"id": "1004.0678", "submitter": "Christopher P. Saunders", "authors": "Christopher P. Saunders, Linda J. Davis, Andrea C. Lamas, John J.\n  Miller, Donald T. Gantz", "title": "Construction and evaluation of classifiers for forensic document\n  analysis", "comments": "Published in at http://dx.doi.org/10.1214/10-AOAS379 the Annals of\n  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of\n  Mathematical Statistics (http://www.imstat.org)", "journal-ref": "Annals of Applied Statistics 2011, Vol. 5, No. 1, 381-399", "doi": "10.1214/10-AOAS379", "report-no": "IMS-AOAS-AOAS379", "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study we illustrate a statistical approach to questioned document\nexamination. Specifically, we consider the construction of three classifiers\nthat predict the writer of a sample document based on categorical data. To\nevaluate these classifiers, we use a data set with a large number of writers\nand a small number of writing samples per writer. Since the resulting\nclassifiers were found to have near perfect accuracy using leave-one-out\ncross-validation, we propose a novel Bayesian-based cross-validation method for\nevaluating the classifiers.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2010 18:39:27 GMT"}, {"version": "v2", "created": "Tue, 28 Jun 2011 05:28:50 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Saunders", "Christopher P.", ""], ["Davis", "Linda J.", ""], ["Lamas", "Andrea C.", ""], ["Miller", "John J.", ""], ["Gantz", "Donald T.", ""]]}, {"id": "1004.0911", "submitter": "Jalmar Manuel  Farfan Carrasco", "authors": "Jalmar M.F. Carrasco, Silvia L.P. Ferrari, Gauss M. Cordeiro", "title": "A New Generalized Kumaraswamy Distribution", "comments": "Beta distribution; Continuous proportions; Generalized Kumaraswamy\n  distribution; Kumaraswamy distribution; Maximum likelihood; McDonald\n  Distribution; Moments.", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new five-parameter continuous distribution which generalizes the\nKumaraswamy and the beta distributions as well as some other well-known\ndistributions is proposed and studied. The model has as special cases new four-\nand three-parameter distributions on the standard unit interval. Moments, mean\ndeviations, R\\'enyi's entropy and the moments of order statistics are obtained\nfor the new generalized Kumaraswamy distribution. The score function is given\nand estimation is performed by maximum likelihood. Hypothesis testing is also\ndiscussed. A data set is used to illustrate an application of the proposed\ndistribution.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2010 17:51:35 GMT"}], "update_date": "2010-04-07", "authors_parsed": [["Carrasco", "Jalmar M. F.", ""], ["Ferrari", "Silvia L. P.", ""], ["Cordeiro", "Gauss M.", ""]]}, {"id": "1004.1112", "submitter": "Dennis Prangle", "authors": "Paul Fearnhead and Dennis Prangle", "title": "Constructing Summary Statistics for Approximate Bayesian Computation:\n  Semi-automatic ABC", "comments": "v2: Revised in response to reviewer comments, adding more examples\n  and a method for inference from multiple data sources", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern statistical applications involve inference for complex stochastic\nmodels, where it is easy to simulate from the models, but impossible to\ncalculate likelihoods. Approximate Bayesian computation (ABC) is a method of\ninference for such models. It replaces calculation of the likelihood by a step\nwhich involves simulating artificial data for different parameter values, and\ncomparing summary statistics of the simulated data to summary statistics of the\nobserved data. Here we show how to construct appropriate summary statistics for\nABC in a semi-automatic manner. We aim for summary statistics which will enable\ninference about certain parameters of interest to be as accurate as possible.\nTheoretical results show that optimal summary statistics are the posterior\nmeans of the parameters. While these cannot be calculated analytically, we use\nan extra stage of simulation to estimate how the posterior means vary as a\nfunction of the data; and then use these estimates of our summary statistics\nwithin ABC. Empirical results show that our approach is a robust method for\nchoosing summary statistics, that can result in substantially more accurate ABC\nanalyses than the ad-hoc choices of summary statistics proposed in the\nliterature. We also demonstrate advantages over two alternative methods of\nsimulation-based inference.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2010 15:32:49 GMT"}, {"version": "v2", "created": "Wed, 13 Apr 2011 19:34:21 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Fearnhead", "Paul", ""], ["Prangle", "Dennis", ""]]}, {"id": "1004.1729", "submitter": "Maciej Kurant", "authors": "Maciej Kurant, Athina Markopoulou, Patrick Thiran", "title": "On the bias of BFS", "comments": "9 pages", "journal-ref": "International Teletraffic Congress (ITC 22), 2010", "doi": null, "report-no": null, "categories": "cs.DM cs.DS cs.NI cs.SI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Breadth First Search (BFS) and other graph traversal techniques are widely\nused for measuring large unknown graphs, such as online social networks. It has\nbeen empirically observed that an incomplete BFS is biased toward high degree\nnodes. In contrast to more studied sampling techniques, such as random walks,\nthe precise bias of BFS has not been characterized to date. In this paper, we\nquantify the degree bias of BFS sampling. In particular, we calculate the node\ndegree distribution expected to be observed by BFS as a function of the\nfraction of covered nodes, in a random graph $RG(p_k)$ with a given degree\ndistribution $p_k$. Furthermore, we also show that, for $RG(p_k)$, all commonly\nused graph traversal techniques (BFS, DFS, Forest Fire, and Snowball Sampling)\nlead to the same bias, and we show how to correct for this bias. To give a\nbroader perspective, we compare this class of exploration techniques to random\nwalks that are well-studied and easier to analyze. Next, we study by simulation\nthe effect of graph properties not captured directly by our model. We find that\nthe bias gets amplified in graphs with strong positive assortativity. Finally,\nwe demonstrate the above results by sampling the Facebook social network, and\nwe provide some practical guidelines for graph sampling in practice.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2010 17:36:43 GMT"}], "update_date": "2011-02-23", "authors_parsed": [["Kurant", "Maciej", ""], ["Markopoulou", "Athina", ""], ["Thiran", "Patrick", ""]]}, {"id": "1004.1876", "submitter": "Satoshi Aoki", "authors": "Satoshi Aoki and Akimichi Takemura", "title": "Design and analysis of fractional factorial experiments from the\n  viewpoint of computational algebraic statistics", "comments": "16 pages", "journal-ref": "Journal of Statistical Theory and Practice, Vol. 6 (2012), No. 1,\n  147--161,", "doi": "10.1080/15598608.2012.647556", "report-no": null, "categories": "stat.ME stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give an expository review of applications of computational algebraic\nstatistics to design and analysis of fractional factorial experiments based on\nour recent works. For the purpose of design, the techniques of Gr\\\"obner bases\nand indicator functions allow us to treat fractional factorial designs without\ndistinction between regular designs and non-regular designs. For the purpose of\nanalysis of data from fractional factorial designs, the techniques of Markov\nbases allow us to handle discrete observations. Thus the approach of\ncomputational algebraic statistics greatly enlarges the scope of fractional\nfactorial designs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2010 05:50:08 GMT"}], "update_date": "2012-04-09", "authors_parsed": [["Aoki", "Satoshi", ""], ["Takemura", "Akimichi", ""]]}, {"id": "1004.2138", "submitter": "Clifford Lam", "authors": "Clifford Lam, Qiwei Yao and Neil Bathia", "title": "Estimation for Latent Factor Models for High-Dimensional Time Series", "comments": "35 pages article, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with the dimension reduction for high-dimensional time\nseries based on common factors. In particular we allow the dimension of time\nseries $p$ to be as large as, or even larger than, the sample size $n$. The\nestimation for the factor loading matrix and the factor process itself is\ncarried out via an eigenanalysis for a $p\\times p$ non-negative definite\nmatrix. We show that when all the factors are strong in the sense that the norm\nof each column in the factor loading matrix is of the order $p^{1/2}$, the\nestimator for the factor loading matrix, as well as the resulting estimator for\nthe precision matrix of the original $p$-variant time series, are weakly\nconsistent in $L_2$-norm with the convergence rates independent of $p$. This\nresult exhibits clearly that the `curse' is canceled out by the `blessings' in\ndimensionality. We also establish the asymptotic properties of the estimation\nwhen not all factors are strong. For the latter case, a two-step estimation\nprocedure is preferred accordingly to the asymptotic theory. The proposed\nmethods together with their asymptotic properties are further illustrated in a\nsimulation study. An application to a real data set is also reported.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 10:16:29 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2010 16:58:41 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2010 15:39:16 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Lam", "Clifford", ""], ["Yao", "Qiwei", ""], ["Bathia", "Neil", ""]]}, {"id": "1004.2287", "submitter": "Vivian Viallon", "authors": "Vivian Viallon, Onureena Banerjee, Gregoire Rey, Eric Jougla, Joel\n  Coste", "title": "An empirical comparative study of approximate methods for binary\n  graphical models; application to the search of associations among causes of\n  death in French death certificates", "comments": "29 pages, 4 figures.", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Looking for associations among multiple variables is a topical issue in\nstatistics due to the increasing amount of data encountered in biology,\nmedicine and many other domains involving statistical applications. Graphical\nmodels have recently gained popularity for this purpose in the statistical\nliterature. Following the ideas of the LASSO procedure designed for the linear\nregression framework, recent developments dealing with graphical model\nselection have been based on $\\ell_1$-penalization. In the binary case,\nhowever, exact inference is generally very slow or even intractable because of\nthe form of the so-called log-partition function. Various approximate methods\nhave recently been proposed in the literature and the main objective of this\npaper is to compare them. Through an extensive simulation study, we show that a\nsimple modification of a method relying on a Gaussian approximation achieves\ngood performance and is very fast. We present a real application in which we\nsearch for associations among causes of death recorded on French death\ncertificates.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2010 23:21:54 GMT"}], "update_date": "2010-04-15", "authors_parsed": [["Viallon", "Vivian", ""], ["Banerjee", "Onureena", ""], ["Rey", "Gregoire", ""], ["Jougla", "Eric", ""], ["Coste", "Joel", ""]]}, {"id": "1004.2548", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters, Mario V. W\\\"uthrich, Pavel V. Shevchenko", "title": "Chain ladder method: Bayesian bootstrap versus classical bootstrap", "comments": null, "journal-ref": "Insurance: Mathematics and Economics (2010)", "doi": "10.1016/j.insmatheco.2010.03.007", "report-no": null, "categories": "q-fin.CP q-fin.RM stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intention of this paper is to estimate a Bayesian distribution-free chain\nladder (DFCL) model using approximate Bayesian computation (ABC) methodology.\nWe demonstrate how to estimate quantities of interest in claims reserving and\ncompare the estimates to those obtained from classical and credibility\napproaches. In this context, a novel numerical procedure utilising Markov chain\nMonte Carlo (MCMC), ABC and a Bayesian bootstrap procedure was developed in a\ntruly distribution-free setting. The ABC methodology arises because we work in\na distribution-free setting in which we make no parametric assumptions, meaning\nwe can not evaluate the likelihood point-wise or in this case simulate directly\nfrom the likelihood model. The use of a bootstrap procedure allows us to\ngenerate samples from the intractable likelihood without the requirement of\ndistributional assumptions, this is crucial to the ABC framework. The developed\nmethodology is used to obtain the empirical distribution of the DFCL model\nparameters and the predictive distribution of the outstanding loss liabilities\nconditional on the observed claims. We then estimate predictive Bayesian\ncapital estimates, the Value at Risk (VaR) and the mean square error of\nprediction (MSEP). The latter is compared with the classical bootstrap and\ncredibility methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2010 04:48:39 GMT"}], "update_date": "2010-04-16", "authors_parsed": [["Peters", "Gareth W.", ""], ["W\u00fcthrich", "Mario V.", ""], ["Shevchenko", "Pavel V.", ""]]}, {"id": "1004.2868", "submitter": "Torsten Ensslin", "authors": "Torsten A. Ensslin and Cornelius Weig", "title": "Inference with minimal Gibbs free energy in information field theory", "comments": "14 pages", "journal-ref": null, "doi": "10.1103/PhysRevE.82.051112", "report-no": "J-MPA2648e", "categories": "astro-ph.IM cs.IT hep-th math.IT physics.data-an stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-linear and non-Gaussian signal inference problems are difficult to\ntackle. Renormalization techniques permit us to construct good estimators for\nthe posterior signal mean within information field theory (IFT), but the\napproximations and assumptions made are not very obvious. Here we introduce the\nsimple concept of minimal Gibbs free energy to IFT, and show that previous\nrenormalization results emerge naturally. They can be understood as being the\nGaussian approximation to the full posterior probability, which has maximal\ncross information with it. We derive optimized estimators for three\napplications, to illustrate the usage of the framework: (i) reconstruction of a\nlog-normal signal from Poissonian data with background counts and point spread\nfunction, as it is needed for gamma ray astronomy and for cosmography using\nphotometric galaxy redshifts, (ii) inference of a Gaussian signal with unknown\nspectrum and (iii) inference of a Poissonian log-normal signal with unknown\nspectrum, the combination of (i) and (ii). Finally we explain how Gaussian\nknowledge states constructed by the minimal Gibbs free energy principle at\ndifferent temperatures can be combined into a more accurate surrogate of the\nnon-Gaussian posterior.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2010 15:43:22 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2010 23:50:28 GMT"}, {"version": "v3", "created": "Tue, 31 Aug 2010 15:31:11 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Ensslin", "Torsten A.", ""], ["Weig", "Cornelius", ""]]}, {"id": "1004.2910", "submitter": "Matthew Harrison", "authors": "Matthew T. Harrison", "title": "Conservative Hypothesis Tests and Confidence Intervals using Importance\n  Sampling", "comments": "26 pages, 3 figures, 3 tables [significant rewrite of version 1,\n  including additional examples, title change]", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling is a common technique for Monte Carlo approximation,\nincluding Monte Carlo approximation of p-values. Here it is shown that a simple\ncorrection of the usual importance sampling p-values creates valid p-values,\nmeaning that a hypothesis test created by rejecting the null when the p-value\nis <= alpha will also have a type I error rate <= alpha. This correction uses\nthe importance weight of the original observation, which gives valuable\ndiagnostic information under the null hypothesis. Using the corrected p-values\ncan be crucial for multiple testing and also in problems where evaluating the\naccuracy of importance sampling approximations is difficult. Inverting the\ncorrected p-values provides a useful way to create Monte Carlo confidence\nintervals that maintain the nominal significance level and use only a single\nMonte Carlo sample. Several applications are described, including accelerated\nmultiple testing for a large neurophysiological dataset and exact conditional\ninference for a logistic regression model with nuisance parameters.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2010 19:27:10 GMT"}, {"version": "v2", "created": "Fri, 8 Apr 2011 21:01:11 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Harrison", "Matthew T.", ""]]}, {"id": "1004.2995", "submitter": "Yiyuan She", "authors": "Florentina Bunea, Yiyuan She, Marten H. Wegkamp", "title": "Optimal selection of reduced rank estimators of high-dimensional\n  matrices", "comments": "Published in at http://dx.doi.org/10.1214/11-AOS876 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org) (some typos corrected)", "journal-ref": "Annals of Statistics 2011, Vol. 39, No. 2, 1282-1309", "doi": "10.1214/11-AOS876", "report-no": "IMS-AOS-AOS876", "categories": "math.ST stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new criterion, the Rank Selection Criterion (RSC), for\nselecting the optimal reduced rank estimator of the coefficient matrix in\nmultivariate response regression models. The corresponding RSC estimator\nminimizes the Frobenius norm of the fit plus a regularization term proportional\nto the number of parameters in the reduced rank model. The rank of the RSC\nestimator provides a consistent estimator of the rank of the coefficient\nmatrix; in general, the rank of our estimator is a consistent estimate of the\neffective rank, which we define to be the number of singular values of the\ntarget matrix that are appropriately large. The consistency results are valid\nnot only in the classic asymptotic regime, when $n$, the number of responses,\nand $p$, the number of predictors, stay bounded, and $m$, the number of\nobservations, grows, but also when either, or both, $n$ and $p$ grow, possibly\nmuch faster than $m$. We establish minimax optimal bounds on the mean squared\nerrors of our estimators. Our finite sample performance bounds for the RSC\nestimator show that it achieves the optimal balance between the approximation\nerror and the penalty term. Furthermore, our procedure has very low\ncomputational complexity, linear in the number of candidate models, making it\nparticularly appealing for large scale problems. We contrast our estimator with\nthe nuclear norm penalized least squares (NNP) estimator, which has an\ninherently higher computational complexity than RSC, for multivariate\nregression models. We show that NNP has estimation properties similar to those\nof RSC, albeit under stronger conditions. However, it is not as parsimonious as\nRSC. We offer a simple correction of the NNP estimator which leads to\nconsistent rank estimation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2010 04:18:27 GMT"}, {"version": "v2", "created": "Tue, 5 Oct 2010 15:51:15 GMT"}, {"version": "v3", "created": "Thu, 19 May 2011 10:06:31 GMT"}, {"version": "v4", "created": "Mon, 17 Oct 2011 02:03:06 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["Bunea", "Florentina", ""], ["She", "Yiyuan", ""], ["Wegkamp", "Marten H.", ""]]}, {"id": "1004.3476", "submitter": "Cosma Rohilla Shalizi", "authors": "Shinsuke Koyama, Lucia Castellanos P\\'erez-Bolde, Cosma Rohilla\n  Shalizi, Robert E. Kass", "title": "Approximate Methods for State-Space Models", "comments": "31 pages, 4 figures. Different pagination from journal version due to\n  incompatible style files but same content; the supplemental file for the\n  journal appears here as appendices B--E.", "journal-ref": "Journal of the American Statistical Association, volume 105, 2010,\n  pp. 170--180", "doi": "10.1198/jasa.2009.tm08326", "report-no": null, "categories": "stat.ME physics.data-an q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-space models provide an important body of techniques for analyzing\ntime-series, but their use requires estimating unobserved states. The optimal\nestimate of the state is its conditional expectation given the observation\nhistories, and computing this expectation is hard when there are\nnonlinearities. Existing filtering methods, including sequential Monte Carlo,\ntend to be either inaccurate or slow. In this paper, we study a nonlinear\nfilter for nonlinear/non-Gaussian state-space models, which uses Laplace's\nmethod, an asymptotic series expansion, to approximate the state's conditional\nmean and variance, together with a Gaussian conditional distribution. This {\\em\nLaplace-Gaussian filter} (LGF) gives fast, recursive, deterministic state\nestimates, with an error which is set by the stochastic characteristics of the\nmodel and is, we show, stable over time. We illustrate the estimation ability\nof the LGF by applying it to the problem of neural decoding and compare it to\nsequential Monte Carlo both in simulations and with real data. We find that the\nLGF can deliver superior results in a small fraction of the computing time.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2010 15:19:45 GMT"}], "update_date": "2010-04-21", "authors_parsed": [["Koyama", "Shinsuke", ""], ["P\u00e9rez-Bolde", "Lucia Castellanos", ""], ["Shalizi", "Cosma Rohilla", ""], ["Kass", "Robert E.", ""]]}, {"id": "1004.3717", "submitter": "Natalia Bahamonde", "authors": "Natalia Bahamonde, Paul Doukhan and Eric Moulines", "title": "Estimation of the autocovariance function with missing observations", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel estimator of the autocorrelation function in presence of\nmissing observations. We establish the consistency, the asymptotic normality,\nand we derive deviation bounds for various classes of weakly dependent\nstationary time series, including causal or non causal models. In addition, we\nintroduce a modified version periodogram defined from these autocorrelation\nestimators and derive asymptotic distribution of linear functionals of this\nestimator.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 14:46:45 GMT"}], "update_date": "2010-04-22", "authors_parsed": [["Bahamonde", "Natalia", ""], ["Doukhan", "Paul", ""], ["Moulines", "Eric", ""]]}, {"id": "1004.3726", "submitter": "Valerie Monbet", "authors": "Dominique Drouet Mari and Valerie Monbet", "title": "Using a priori knowledge to construct copulas", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our purpose is to model the dependence between two random variables, taking\ninto account a priori knowledge on these variables. For example, in many\napplications (oceanography, finance...), there exists an order relation between\nthe two variables; when one takes high values, the other cannot take low\nvalues, but the contrary is possible. The dependence for the high values of the\ntwo variables is, therefore, not symmetric.\n  However a minimal dependence also exists: low values of one variable are\nassociated with low values of the other variable. The dependence can also be\nextreme for the maxima or the minima of the two variables. In this paper, we\nconstruct step by step asymmetric copulas with asymptotic minimal dependence,\nand with or without asymptotic maximal dependence, using mixture variables to\nget at first asymmetric dependence and then minimal dependence. We fit these\nmodels to a real dataset of sea states and compare them using Likelihood Ratio\nTests when they are nested, and BIC- criterion (Bayesian Information criterion)\notherwise.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 15:24:58 GMT"}], "update_date": "2010-04-22", "authors_parsed": [["Mari", "Dominique Drouet", ""], ["Monbet", "Valerie", ""]]}, {"id": "1004.3782", "submitter": "Ping Li", "authors": "Ping Li", "title": "On Practical Algorithms for Entropy Estimation and the Improved Sample\n  Complexity of Compressed Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Estimating the p-th frequency moment of data stream is a very heavily studied\nproblem. The problem is actually trivial when p = 1, assuming the strict\nTurnstile model. The sample complexity of our proposed algorithm is essentially\nO(1) near p=1. This is a very large improvement over the previously believed\nO(1/eps^2) bound. The proposed algorithm makes the long-standing problem of\nentropy estimation an easy task, as verified by the experiments included in the\nappendix.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2010 19:55:55 GMT"}], "update_date": "2015-03-14", "authors_parsed": [["Li", "Ping", ""]]}, {"id": "1004.3830", "submitter": "Gareth Peters Dr", "authors": "Gareth W. Peters and Balakrishnan Kannan and Ben Lasscock and Chris\n  Mellen", "title": "Model Selection and Adaptive Markov chain Monte Carlo for Bayesian\n  Cointegrated VAR model", "comments": "to appear journal Bayesian Analysis", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP q-fin.PM q-fin.ST stat.CO stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops a matrix-variate adaptive Markov chain Monte Carlo (MCMC)\nmethodology for Bayesian Cointegrated Vector Auto Regressions (CVAR). We\nreplace the popular approach to sampling Bayesian CVAR models, involving griddy\nGibbs, with an automated efficient alternative, based on the Adaptive\nMetropolis algorithm of Roberts and Rosenthal, (2009). Developing the adaptive\nMCMC framework for Bayesian CVAR models allows for efficient estimation of\nposterior parameters in significantly higher dimensional CVAR series than\npreviously possible with existing griddy Gibbs samplers. For a n-dimensional\nCVAR series, the matrix-variate posterior is in dimension $3n^2 + n$, with\nsignificant correlation present between the blocks of matrix random variables.\nWe also treat the rank of the CVAR model as a random variable and perform joint\ninference on the rank and model parameters. This is achieved with a Bayesian\nposterior distribution defined over both the rank and the CVAR model\nparameters, and inference is made via Bayes Factor analysis of rank.\nPractically the adaptive sampler also aids in the development of automated\nBayesian cointegration models for algorithmic trading systems considering\ninstruments made up of several assets, such as currency baskets. Previously the\nliterature on financial applications of CVAR trading models typically only\nconsiders pairs trading (n=2) due to the computational cost of the griddy\nGibbs. We are able to extend under our adaptive framework to $n >> 2$ and\ndemonstrate an example with n = 10, resulting in a posterior distribution with\nparameters up to dimension 310. By also considering the rank as a random\nquantity we can ensure our resulting trading models are able to adjust to\npotentially time varying market conditions in a coherent statistical framework.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 02:26:17 GMT"}], "update_date": "2010-04-23", "authors_parsed": [["Peters", "Gareth W.", ""], ["Kannan", "Balakrishnan", ""], ["Lasscock", "Ben", ""], ["Mellen", "Chris", ""]]}, {"id": "1004.3871", "submitter": "Umberto Picchini", "authors": "Umberto Picchini and Susanne Ditlevsen", "title": "Practical Estimation of High Dimensional Stochastic Differential\n  Mixed-Effects Models", "comments": "Forthcoming in \"Computational Statistics & Data Analysis\"", "journal-ref": "Computational Statistics & Data Analysis, 2011, Volume 55, Issue\n  3, pages 1426-1444", "doi": "10.1016/j.csda.2010.10.003", "report-no": null, "categories": "stat.CO math.DS stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic differential equations (SDEs) are established tools to model\nphysical phenomena whose dynamics are affected by random noise. By estimating\nparameters of an SDE intrinsic randomness of a system around its drift can be\nidentified and separated from the drift itself. When it is of interest to model\ndynamics within a given population, i.e. to model simultaneously the\nperformance of several experiments or subjects, mixed-effects modelling allows\nfor the distinction of between and within experiment variability. A framework\nto model dynamics within a population using SDEs is proposed, representing\nsimultaneously several sources of variation: variability between experiments\nusing a mixed-effects approach and stochasticity in the individual dynamics\nusing SDEs. These \"stochastic differential mixed-effects models\" have\napplications in e.g. pharmacokinetics/pharmacodynamics and biomedical\nmodelling. A parameter estimation method is proposed and computational\nguidelines for an efficient implementation are given. Finally the method is\nevaluated using simulations from standard models like the two-dimensional\nOrnstein-Uhlenbeck (OU) and the square root models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 09:23:58 GMT"}, {"version": "v2", "created": "Sun, 3 Oct 2010 15:17:39 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Picchini", "Umberto", ""], ["Ditlevsen", "Susanne", ""]]}, {"id": "1004.4027", "submitter": "Robert B. Gramacy", "authors": "Robert B. Gramacy and Herbert K. H. Lee", "title": "Optimization Under Unknown Constraints", "comments": "19 pages, 8 figures, Valencia discussion paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of complex functions, such as the output of computer simulators,\nis a difficult task that has received much attention in the literature. A less\nstudied problem is that of optimization under unknown constraints, i.e., when\nthe simulator must be invoked both to determine the typical real-valued\nresponse and to determine if a constraint has been violated, either for\nphysical or policy reasons. We develop a statistical approach based on Gaussian\nprocesses and Bayesian learning to both approximate the unknown function and\nestimate the probability of meeting the constraints. A new integrated\nimprovement criterion is proposed to recognize that responses from inputs that\nviolate the constraint may still be informative about the function, and thus\ncould potentially be useful in the optimization. The new criterion is\nillustrated on synthetic data, and on a motivating optimization problem from\nhealth care policy.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2010 23:36:58 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2010 16:39:50 GMT"}], "update_date": "2010-07-06", "authors_parsed": [["Gramacy", "Robert B.", ""], ["Lee", "Herbert K. H.", ""]]}, {"id": "1004.4391", "submitter": "Andrey Novikov", "authors": "Andrey Novikov, Petr Novikov", "title": "Locally most powerful sequential tests of a simple hypothesis vs.\n  One-sided alternatives for independent observations", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Let $X_1,X_2,..., X_n,...$ be a stochastic process with independent values\nwhose distribution $P_\\theta$ depends on an unknown parameter $\\theta$,\n$\\theta\\in\\Theta$, where $\\Theta$ is an open subset of the real line. The\nproblem of testing $H_0:$ $\\theta=\\theta_0$ vs. a composite alternative $H_1:$\n$\\theta>\\theta_0$ is considered, where $\\theta_0\\in\\Theta$ is a fixed value of\nthe parameter. The main objective of this work is the characterization of the\nstructure of the locally most powerful (in the sense of Berk) sequential tests\nin this problem.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2010 23:46:22 GMT"}], "update_date": "2010-04-27", "authors_parsed": [["Novikov", "Andrey", ""], ["Novikov", "Petr", ""]]}, {"id": "1004.4522", "submitter": "Ma{\\l}gorzata Snarska", "authors": "Ma{\\l}gorzata Snarska", "title": "Toy Model for Large Non-Symmetric Random Matrices", "comments": "5 pages, 3 figures, Proceedings of the 3rd Polish Symposium on Econo-\n  and Sociophysics, Wroclaw 2007,", "journal-ref": "Acta Physica Polonica A, 2008 Vol.114 Issue 3 p.555 - 559", "doi": null, "report-no": null, "categories": "physics.data-an econ.GN q-fin.EC q-fin.ST stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-symmetric rectangular correlation matrices occur in many problems in\neconomics. We test the method of extracting statistically meaningful\ncorrelations between input and output variables of large dimensionality and\nbuild a toy model for artificially included correlations in large random time\nseries.The results are then applied to analysis of polish macroeconomic data\nand can be used as an alternative to classical cointegration approach.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2010 13:30:58 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Snarska", "Ma\u0142gorzata", ""]]}, {"id": "1004.4956", "submitter": "Yingying Li", "authors": "Jianqing Fan, Yingying Li, Ke Yu", "title": "Vast Volatility Matrix Estimation using High Frequency Data for\n  Portfolio Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.PM math.ST q-fin.ST stat.AP stat.ME stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Portfolio allocation with gross-exposure constraint is an effective method to\nincrease the efficiency and stability of selected portfolios among a vast pool\nof assets, as demonstrated in Fan et al (2008). The required high-dimensional\nvolatility matrix can be estimated by using high frequency financial data. This\nenables us to better adapt to the local volatilities and local correlations\namong vast number of assets and to increase significantly the sample size for\nestimating the volatility matrix. This paper studies the volatility matrix\nestimation using high-dimensional high-frequency data from the perspective of\nportfolio selection. Specifically, we propose the use of \"pairwise-refresh\ntime\" and \"all-refresh time\" methods proposed by Barndorff-Nielsen et al (2008)\nfor estimation of vast covariance matrix and compare their merits in the\nportfolio selection. We also establish the concentration inequalities of the\nestimates, which guarantee desirable properties of the estimated volatility\nmatrix in vast asset allocation with gross exposure constraints. Extensive\nnumerical studies are made via carefully designed simulations. Comparing with\nthe methods based on low frequency daily data, our methods can capture the most\nrecent trend of the time varying volatility and correlation, hence provide more\naccurate guidance for the portfolio allocation in the next time period. The\nadvantage of using high-frequency data is significant in our simulation and\nempirical studies, which consist of 50 simulated assets and 30 constituent\nstocks of Dow Jones Industrial Average index.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2010 06:11:20 GMT"}], "update_date": "2010-04-29", "authors_parsed": [["Fan", "Jianqing", ""], ["Li", "Yingying", ""], ["Yu", "Ke", ""]]}, {"id": "1004.5074", "submitter": "Christian P. Robert", "authors": "Christian P. Robert", "title": "Evidence and Evolution: A Review", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST q-bio.PE stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  \"Evidence and Evolution: the Logic behind the Science\" was published in 2008\nby Elliott Sober. It examines the philosophical foundations of the statistical\narguments used to evaluate hypotheses in evolutionary biology, based on simple\nexamples and likelihood ratios. The difficulty with reading the book from a\nstatistician's perspective is the reluctance of the author to engage into model\nbuilding and even less into parameter estimation. The first chapter nonetheless\nconstitutes a splendid coverage of the most common statistical approaches to\ntesting and model comparison, even though the advocation of the Akaike\ninformation criterion against Bayesian alternatives is rather forceful. The\nbook also covers an examination of the \"intelligent design\" arguments against\nthe Darwinian evolution theory, predictably if unnecessarily resorting to\nPopperian arguments to correctly argue that the creationist perspective fails\nto predict anything. The following chapters cover the more relevant issues of\nassessing selection versus drift and of testing for the presence of a common\nancestor. While remaining a philosophy treatise, Evidence and Evolution is\nwritten in a way that is accessible to laymen, if rather unusual from a\nstatistician viewpoint, and the insight about testing issues gained from\nEvidence and Evolution makes it a worthwhile read.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2010 16:13:44 GMT"}], "update_date": "2010-06-23", "authors_parsed": [["Robert", "Christian P.", ""]]}, {"id": "1004.5178", "submitter": "Ning Hao", "authors": "Jianqing Fan, Shaojun Guo, Ning Hao", "title": "Variance Estimation Using Refitted Cross-validation in Ultrahigh\n  Dimensional Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variance estimation is a fundamental problem in statistical modeling. In\nultrahigh dimensional linear regressions where the dimensionality is much\nlarger than sample size, traditional variance estimation techniques are not\napplicable. Recent advances on variable selection in ultrahigh dimensional\nlinear regressions make this problem accessible. One of the major problems in\nultrahigh dimensional regression is the high spurious correlation between the\nunobserved realized noise and some of the predictors. As a result, the realized\nnoises are actually predicted when extra irrelevant variables are selected,\nleading to serious underestimate of the noise level. In this paper, we propose\na two-stage refitted procedure via a data splitting technique, called refitted\ncross-validation (RCV), to attenuate the influence of irrelevant variables with\nhigh spurious correlations. Our asymptotic results show that the resulting\nprocedure performs as well as the oracle estimator, which knows in advance the\nmean regression function. The simulation studies lend further support to our\ntheoretical claims. The naive two-stage estimator which fits the selected\nvariables in the first stage and the plug-in one stage estimators using LASSO\nand SCAD are also studied and compared. Their performances can be improved by\nthe proposed RCV method.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 03:43:50 GMT"}, {"version": "v2", "created": "Fri, 24 Dec 2010 20:07:49 GMT"}], "update_date": "2010-12-27", "authors_parsed": [["Fan", "Jianqing", ""], ["Guo", "Shaojun", ""], ["Hao", "Ning", ""]]}, {"id": "1004.5225", "submitter": "Gergely Palla", "authors": "G. Palla, L. Lovasz, T. Vicsek", "title": "Multifractal Network Generator", "comments": "Preprint. Final version appeared in PNAS.", "journal-ref": "Proc. Natl. Acad. Sci. USA 107: 7640-7645, (2010)", "doi": "10.1073/pnas.0912983107", "report-no": null, "categories": "physics.data-an math-ph math.MP physics.soc-ph stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to constructing networks with realistic features.\nOur method, in spite of its conceptual simplicity (it has only two parameters)\nis capable of generating a wide variety of network types with prescribed\nstatistical properties, e.g., with degree- or clustering coefficient\ndistributions of various, very different forms. In turn, these graphs can be\nused to test hypotheses, or, as models of actual data. The method is based on a\nmapping between suitably chosen singular measures defined on the unit square\nand sparse infinite networks. Such a mapping has the great potential of\nallowing for graph theoretical results for a variety of network topologies. The\nmain idea of our approach is to go to the infinite limit of the singular\nmeasure and the size of the corresponding graph simultaneously. A very unique\nfeature of this construction is that the complexity of the generated network is\nincreasing with the size. We present analytic expressions derived from the\nparameters of the -- to be iterated-- initial generating measure for such major\ncharacteristics of graphs as their degree, clustering coefficient and\nassortativity coefficient distributions. The optimal parameters of the\ngenerating measure are determined from a simple simulated annealing process.\nThus, the present work provides a tool for researchers from a variety of fields\n(such as biology, computer science, biology, or complex systems) enabling them\nto create a versatile model of their network data.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 09:19:12 GMT"}], "update_date": "2010-04-30", "authors_parsed": [["Palla", "G.", ""], ["Lovasz", "L.", ""], ["Vicsek", "T.", ""]]}, {"id": "1004.5300", "submitter": "Etienne Roquain", "authors": "Kyung In Kim, Etienne Roquain (PMA), Mark Van De Wiel", "title": "Spatial clustering of array CGH features in combination with\n  hierarchical multiple testing", "comments": null, "journal-ref": "Statistical Applications in Genetics and Molecular Biology (2010)\n  Vol. 9 : Iss. 1, Article 40", "doi": "10.2202/1544-6115.1532", "report-no": null, "categories": "stat.AP stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach for clustering DNA features using array CGH data\nfrom multiple tumor samples. We distinguish data-collapsing: joining contiguous\nDNA clones or probes with extremely similar data into regions, from clustering:\njoining contiguous, correlated regions based on a maximum likelihood principle.\nThe model-based clustering algorithm accounts for the apparent spatial patterns\nin the data. We evaluate the randomness of the clustering result by a cluster\nstability score in combination with cross-validation. Moreover, we argue that\nthe clustering really captures spatial genomic dependency by showing that\ncoincidental clustering of independent regions is very unlikely. Using the\nregion and cluster information, we combine testing of these for association\nwith a clinical variable in an hierarchical multiple testing approach. This\nallows for interpreting the significance of both regions and clusters while\ncontrolling the Family-Wise Error Rate simultaneously. We prove that in the\ncontext of permutation tests and permutation-invariant clusters it is allowed\nto perform clustering and testing on the same data set. Our procedures are\nillustrated on two cancer data sets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 14:24:13 GMT"}, {"version": "v2", "created": "Sun, 19 Dec 2010 16:44:06 GMT"}], "update_date": "2010-12-21", "authors_parsed": [["Kim", "Kyung In", "", "PMA"], ["Roquain", "Etienne", "", "PMA"], ["Van De Wiel", "Mark", ""]]}, {"id": "1004.5328", "submitter": "Pavel  Krivitsky", "authors": "Pavel N. Krivitsky (1 and 2), Mark S. Handcock (3), and Martina Morris\n  (4) ((1) Department of Statistics and iLab, Carnegie Mellon University,\n  Pittsburgh, USA, (2) Institute for Systems and Robotics, Instituto Superior\n  T\\'ecnico, Lisbon, Portugal, (3) Department of Statistics, University of\n  California at Los Angeles, Los Angeles, USA, (4) Department of Sociology and\n  Department of Statistics, University of Washington, Seattle, USA)", "title": "Adjusting for Network Size and Composition Effects in Exponential-Family\n  Random Graph Models", "comments": "37 pages, 2 figures, 5 tables; notation revised and clarified, some\n  sections (particularly 4.3 and 5) made more rigorous, some derivations moved\n  into the appendix, typos fixed, some wording changed", "journal-ref": "Statistical Methodology 8 (2011) 319-339", "doi": "10.1016/j.stamet.2011.01.005", "report-no": null, "categories": "stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exponential-family random graph models (ERGMs) provide a principled way to\nmodel and simulate features common in human social networks, such as\npropensities for homophily and friend-of-a-friend triad closure. We show that,\nwithout adjustment, ERGMs preserve density as network size increases. Density\ninvariance is often not appropriate for social networks. We suggest a simple\nmodification based on an offset which instead preserves the mean degree and\naccommodates changes in network composition asymptotically. We demonstrate that\nthis approach allows ERGMs to be applied to the important situation of\negocentrically sampled data. We analyze data from the National Health and\nSocial Life Survey (NHSLS).\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 15:49:57 GMT"}, {"version": "v2", "created": "Mon, 27 Dec 2010 10:35:23 GMT"}], "update_date": "2012-01-09", "authors_parsed": [["Krivitsky", "Pavel N.", "", "1 and 2"], ["Handcock", "Mark S.", ""], ["Morris", "Martina", ""]]}, {"id": "1004.5408", "submitter": "Adom Giffin", "authors": "Adom Giffin", "title": "Approximation for a Toy Defective Ising Model", "comments": "Presented at the 29th International Workshop on Bayesian Inference\n  and Maximum Entropy Methods in Science and Engineering, Oxford, MS, July\n  5-10, 2009. 10 pages, 0 figures", "journal-ref": "A. Giffin, \"Approximation For A Toy Defective Ising Model\",\n  Bayesian Inference and Maximum Entropy Methods in Science and Engineering,\n  ed. by P. M. Goggans, C. Y. Chan, AIP Conf. Proc. 1193 79 (2009).", "doi": "10.1063/1.3275649", "report-no": null, "categories": "cond-mat.stat-mech cond-mat.mtrl-sci stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been previously shown that one can use the ME methodology (Caticha\nGiffin 2006) to reproduce a mean field solution for a simple fluid (Tseng\n2004). One could easily use the case of a simple ferromagnetic material as\nwell. The drawback to the mean field approach is that one must assume that all\natoms must all act the same. The problem becomes more tractable when the agents\nare only allowed to interact with their nearest neighbors and can be in only\ntwo possible states. The easiest case being an Ising model. The purpose of this\npaper is to illustrate the use of the ME method as an approximation tool. The\npaper show a simple case to compare with the traditional mean field approach.\nThen we show two examples that lie outside of traditional methodologies. These\ncases explore a ferromagnetic material with defects. The main result is that\nregardless of the case, the ME method provides good approximations for each\ncase which would not otherwise be possible or at least well justified.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2010 21:41:52 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Giffin", "Adom", ""]]}, {"id": "1004.5538", "submitter": "Francois Orieux", "authors": "Francois Orieux, Jean-Francois Giovannelli, Thomas Rodet", "title": "Bayesian estimation of regularization and PSF parameters for Wiener-Hunt\n  deconvolution", "comments": null, "journal-ref": null, "doi": "10.1364/JOSAA.27.001593", "report-no": null, "categories": "stat.CO cs.CV physics.data-an stat.ME", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper tackles the problem of image deconvolution with joint estimation\nof PSF parameters and hyperparameters. Within a Bayesian framework, the\nsolution is inferred via a global a posteriori law for unknown parameters and\nobject. The estimate is chosen as the posterior mean, numerically calculated by\nmeans of a Monte-Carlo Markov chain algorithm. The estimates are efficiently\ncomputed in the Fourier domain and the effectiveness of the method is shown on\nsimulated examples. Results show precise estimates for PSF parameters and\nhyperparameters as well as precise image estimates including restoration of\nhigh-frequencies and spatial details, within a global and coherent approach.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2010 14:23:46 GMT"}], "update_date": "2015-05-18", "authors_parsed": [["Orieux", "Francois", ""], ["Giovannelli", "Jean-Francois", ""], ["Rodet", "Thomas", ""]]}]